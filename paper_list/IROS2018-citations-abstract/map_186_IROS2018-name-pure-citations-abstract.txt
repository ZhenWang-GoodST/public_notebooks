total paper: 186
Title: Geometric-based Line Segment Tracking for HDR Stereo Sequences
Key Words: compressed sensing  convex programming  image matching  image segmentation  image sequences  stereo image processing  video signal processing  robust tracking  art techniques  appearance-based methods  High Dynamic Range environments  HDR stereo sequences  geometric-based line segment tracking  stereo streams  appearance-based matching techniques  video sequences  Image segmentation  Lighting  Tracking  Feature extraction  Video sequences  Motion segmentation  Simultaneous localization and mapping 
Abstract: In this work, we propose a purely geometrical approach for the robust matching of line segments for challenging stereo streams with severe illumination changes or High Dynamic Range (HDR) environments. To that purpose, we exploit the univocal nature of the matching problem, i.e. every observation must be corresponded with a single feature or not corresponded at all. We state the problem as a sparse, convex, ℓ1-minimization of the matching vector regularized by the geometric constraints. This formulation allows for the robust tracking of line segments along sequences where traditional appearance-based matching techniques tend to fail due to dynamic changes in illumination conditions. Moreover, the proposed matching algorithm also results in a considerable speed-up of previous state of the art techniques making it suitable for real-time applications such as Visual Odometry (VO). This, of course, comes at expense of a slightly lower number of matches in comparison with appearance-based methods, and also limits its application to continuous video sequences, as it is rather constrained to small pose increments between consecutive frames. We validate the claimed advantages by first evaluating the matching performance in challenging video sequences, and then testing the method in a benchmarked point and line based VO algorithm.


Title: A modular framework for model-based visual tracking using edge, texture and depth features
Key Words: feature extraction  image colour analysis  image sensors  object tracking  modular framework  confidence index  multiple vision sensors  depth map  textured points  edge points  real-time model-based visual tracker  depth features  model-based visual tracking using edge  Cameras  Visualization  Solid modeling  Image edge detection  Three-dimensional displays  Sensors  Robustness 
Abstract: We present in this paper a modular real-time model-based visual tracker. It is able to fuse different types of measurement, that is, edge points, textured points, and depth map, provided by one or multiple vision sensors. A confidence index is also proposed for determining if the outputs of the tracker are reliable or not. As expected, experimental results show that the more various measurements are combined, the more accurate and robust is the tracker. The corresponding C++ source code is available for the community in the ViSP library.


Title: FSG: A statistical approach to line detection via fast segments grouping
Key Words: feature extraction  image segmentation  robot vision  fast segments grouping  line segment detection algorithms  segment grouping methods  vanishing points detection  statistical approach  high level robot localization task  plausible line candidates  robust line detection algorithm  FSG  low textured scenes  visual robotic tasks  line extraction  Image segmentation  Probabilistic logic  Estimation  Simultaneous localization and mapping  Task analysis  Detection algorithms 
Abstract: Line extraction is a preliminary step in various visual robotic tasks performed in low textured scenes such as city and indoor settings. Several efficient line segment detection algorithms such as LSD and EDLines have recently emerged. However, the state of the art segment grouping methods are not robust enough or not amenable for detecting lines in real-time. In this paper we present FSG, a fast and robust line detection algorithm. It is based on two independent components. A proposer that greedily cluster segments suggesting plausible line candidates and a probabilistic model that decides if a group of segments is an actual line. In the experiments we show that our procedure is more robust and faster than the best methods in the literature and achieves state-of-the art performance in a high level robot localization task such as vanishing points detection.


Title: Optimized Contrast Enhancements to Improve Robustness of Visual Tracking in a SLAM Relocalisation Context
Key Words: cameras  feature extraction  image colour analysis  image enhancement  image representation  mobile robots  robot vision  SLAM (robots)  video signal processing  optimized contrast enhancements  visual tracking  SLAM relocalisation context  indirect SLAM techniques  robotics community  feature points  multilayered image representation  contrast enhanced version  tracking process  detection  matching  dynamic contrast enhancements  dynamic light changing conditions  ORB-SLAM  light changed condition  reference video  Mutual information  Lighting  Robustness  Simultaneous localization and mapping  Cameras  Entropy  Visualization 
Abstract: Robustness of indirect SLAM techniques to light changing conditions remains a central issue in the robotics community. With the change in the illumination of a scene, feature points are either not extracted properly due to low contrasts, or not matched due to large differences in descriptors. In this paper, we propose a multi-layered image representation (MLI) in which each layer holds a contrast enhanced version of the current image in the tracking process in order to improve detection and matching. We show how Mutual Information can be used to compute dynamic contrast enhancements on each layer. We demonstrate how this approach dramatically improves the robustness in dynamic light changing conditions on both synthetic and real environments compared to default ORB-SLAM. This work focalises on the specific case of SLAM relocalisation in which a first pass on a reference video constructs a map, and a second pass with a light changed condition relocalizes the camera in the map.


Title: Weighted Total Least Squares based Online Calibration Method for RSS based Localization
Key Words: calibration  distance measurement  error compensation  least squares approximations  regression analysis  linear regression model  partial input elements  weighted total least squares techniques  WTLS techniques  received signal strength based localization algorithm  RSS-to-distance based localization algorithm  improved online model-based calibration approach  distance estimation  error compensation  Calibration  Linear regression  Estimation  Shadow mapping  Measurement uncertainty  Taylor series  Convergence 
Abstract: In the received signal strength (RSS) based localization, a model-based calibration approach has been usually done by relating RSS-to-distance among anchor nodes. In this paper, an improved calibration method is proposed. For that purpose, RSS and estimated distance between any pairs of an-chor/unknown nodes is considered under the linear regression model. Unfortunately in this model, partial input elements are erroneous due to the inaccurate localization of unknown nodes. To obtain its solution under consideration of such an error, the weighted total least squares (WTLS) techniques are employed here. With the help of the WTLS techniques, several errors involved in the model can be effectively compensated. To show the efficiency of the proposed calibration, it is combined with several localization algorithms and its performance is verified by various simulations. The results show that the proposed calibration can give a very similar localization performance to that of each localization algorithm when true model parameters are known.


Title: LIPS: LiDAR-Inertial 3D Plane SLAM
Key Words: graph theory  image representation  mobile robots  optical radar  optimisation  robot vision  SLAM (robots)  inertial preintegratation measurement  LiDAR-inertial 3D plane SLAM  simultaneous localization and mapping  singularity free plane factor  closest point plane representation  Simultaneous localization and mapping  Three-dimensional displays  Laser radar  Optimization  Lips 
Abstract: This paper presents the formalization of the closest point plane representation and an analysis of its incorporation in 3D indoor simultaneous localization and mapping (SLAM). We present a singularity free plane factor leveraging the closest point plane representation, and demonstrate its fusion with inertial preintegratation measurements in a graph-based optimization framework. The resulting LiDAR-inertial 3D plane SLAM (LIPS) system is validated both on a custom made LiDAR simulator and on a real-world experiment.


Title: Scan Similarity-based Pose Graph Construction method for Graph SLAM
Key Words: graph theory  mobile robots  pose estimation  robot vision  SLAM (robots)  scan similarity-based pose graph construction method  constructed graph  loop closure detection method  real world dataset  benchmark dataset  odometry estimation process  error accumulation phenomenon  pose graph SLAM  scan similarity computation method  graph accuracy  high quality graph  Simultaneous localization and mapping  Lasers  Estimation  Heuristic algorithms  Optimization 
Abstract: Scan similarity-based pose graph construction method for graph SLAM is proposed. To perform delicate pose graph SLAM, front-end that constructs a graph as well as back-end that optimizes the constructed graph is an important task. Generally, there is an error accumulation phenomenon during the odometry estimation process. This paper focuses on the method of creating a high quality graph by suggesting ways to improve the graph accuracy since the accumulated errors in the graph might degrade the performance of the entire graph SLAM. We deal with one of our previous works, dynamic keyframe selection technique, based on scan similarity computation method more precisely and suggest a loop closure detection method by exploiting previously proposed 2-D laser scan descriptor. To verify objective performance of the proposed method, the experimental results of the odometry estimation are shown by using the benchmark dataset and the real world dataset. Additionally, results of the pose graph SLAM are shown for the real world dataset which include the loop clorues.


Title: Egocentric Spatial Memory
Key Words: learning (artificial intelligence)  mobile robots  neurophysiology  recurrent neural nets  robot vision  place recognition  robotic control  3D virtual mazes  deep learning based mapping system  ESM network  external memory  recurrent neural network  spatially extended environment  2D global maps  integrated deep neural network architecture  egocentric perspective  spatial information  memory system  egocentric spatial memory  Computer architecture  Cameras  Navigation  Microprocessors  Sensors  Task analysis  Motion measurement 
Abstract: Egocentric spatial memory (ESM) defines a memory system with encoding, storing, recognizing and recalling the spatial information about the environment from an egocentric perspective. We introduce an integrated deep neural network architecture for modeling ESM. It learns to estimate the occupancy state of the world and progressively construct top-down 2D global maps from egocentric views in a spatially extended environment. During the exploration, our proposed ESM model updates belief of the global map based on local observations using a recurrent neural network. It also augments the local mapping with a novel external memory to encode and store latent representations of the visited places over longterm exploration in large environments which enables agents to perform place recognition and hence, loop closure. Our proposed ESM network contributes in the following aspects: (1) without feature engineering, our model predicts free space based on egocentric views efficiently in an end-to-end manner; (2) different from other deep learning-based mapping system, ESMN deals with continuous actions and states which is vitally important for robotic control in real applications. In the experiments, we demonstrate its accurate and robust global mapping capacities in 3D virtual mazes and realistic indoor environments by comparing with several competitive baselines.


Title: Predicting Objective Function Change in Pose-Graph Optimization
Key Words: graph theory  optimisation  SLAM (robots)  outlier detection  robust online incremental SLAM applications  graph pruning  information-theoretic metrics  pose-graph optimization scheme  Linear programming  Optimization  Simultaneous localization and mapping  Measurement errors  Noise measurement  Reliability 
Abstract: Robust online incremental SLAM applications require metrics to evaluate the impact of current measurements. Despite its prevalence in graph pruning, information-theoretic metrics solely are insufficient to detect outliers. The optimal value of the objective function is a better choice to detect outliers but cannot be computed unless the problem is solved. In this paper, we show how the objective function change can be predicted in an incremental pose-graph optimization scheme, without actually solving the problem. The predicted objective function change can be used to guide online decisions or detect outliers. Experiments validate the accuracy of the predicted objective function, and an application to outlier detection is also provided, showing its advantages over M-estimators.


Title: Efficient Long-term Mapping in Dynamic Environments
Key Words: graph theory  mobile robots  robot vision  SLAM (robots)  mapping problem  longterm SLAM datasets  graph coherency  intra-session loop closure detections  out-dated nodes  graph complexity  nonstatic entities  merging procedure  efficient ICP-based alignment  up-to-date state  2D point cloud data  local maps  graph SLAM paradigm  multiple mapping sessions  single mapping sessions  SLAM system  autonomous robots  dynamic environments  long-term robot operation  Simultaneous localization and mapping  Cloud computing  Three-dimensional displays  Two dimensional displays  Merging  Optimization 
Abstract: As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.


Title: Localization of Classified Objects in SLAM using Nonparametric Statistics and Clustering
Key Words: feature extraction  learning (artificial intelligence)  mobile robots  nonparametric statistics  object detection  pattern clustering  robot vision  SLAM (robots)  statistical analysis  nonparametric statistical approach  data association  mapping process  object detection  machine learning  semantic information  nonparametric statistics  classified objects  locating objects  SLAM  unsupervised clustering method  detected objects  Simultaneous localization and mapping  Semantics  Object detection  Cameras  Three-dimensional displays 
Abstract: Traditional Simultaneous Localization and Mapping (SLAM) approaches build maps based on points, lines or planes. These maps visually resemble the environment but without any semantic or information about the objects in the environment. Recent advancements in machine learning have made object detection highly accurate and reliable with large set of objects. Object detection can effectively help SLAM to incorporate semantics in the mapping process. One of the main obstacles is data association between detected objects over time. We demonstrate a nonparametric statistical approach to solve the data association between detected objects over consecutive frames. Then we use an unsupervised clustering method to identify the existence of objects in the map. The complete process can be run in parallel with SLAM. The performance of our algorithm is demonstrated on several public datasets, which shows promising results in locating objects in SLAM.


Title: Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation
Key Words: image segmentation  probability  SLAM (robots)  stereo image processing  SLAM framework  NYUv2 dataset  computational efficiency  frame-wise segmentation result  computationally intensive stages  segmentation label  updating class probabilities  processing components  geometric-based segmentation method  geometric-based incremental segmentation  Semantics  Three-dimensional displays  Image segmentation  Simultaneous localization and mapping  Cameras  Real-time systems  Two dimensional displays 
Abstract: We propose an efficient and scalable method for incrementally building a dense, semantically annotated 3D map in real-time. The proposed method assigns class probabilities to each region, not each element (e.g., surfel and voxel), of the 3D map which is built up through a robust SLAM framework and incrementally segmented with a geometric-based segmentation method. Differently from all other approaches, our method has a capability of running at over 30Hz while performing all processing components, including SLAM, segmentation, 2D recognition, and updating class probabilities of each segmentation label at every incoming frame, thanks to the high efficiency that characterizes the computationally intensive stages of our framework. By utilizing a specifically designed CNN to improve the frame-wise segmentation result, we can also achieve high accuracy. We validate our method on the NYUv2 dataset by comparing with the state of the art in terms of accuracy and computational efficiency, and by means of an analysis in terms of time and space complexity.


Title: Semantic Monocular SLAM for Highly Dynamic Environments
Key Words: cameras  feature extraction  image motion analysis  image sequences  mobile robots  object detection  object tracking  pose estimation  probability  robot vision  SLAM (robots)  static environment  semantic monocular SLAM framework  semantic information  explicit probabilistic model  dynamic environments  Virtual KITTI  Synthia datasets  pose estimation  Semantics  Simultaneous localization and mapping  Feature extraction  Dynamics  Cameras  Pose estimation  Probabilistic logic 
Abstract: Recent advances in monocular SLAM have enabled real-time capable systems which run robustly under the assumption of a static environment, but fail in presence of dynamic scene changes and motion, since they lack an explicit dynamic outlier handling. We propose a semantic monocular SLAM framework designed to deal with highly dynamic environments, combining feature-based and direct approaches to achieve robustness under challenging conditions. The proposed approach exploits semantic information extracted from the scene within an explicit probabilistic model, which maximizes the probability for both tracking and mapping to rely on those scene parts that do not present a relative motion with respect to the camera. We show more stable pose estimation in dynamic environments and comparable performance to the state of the art on static sequences on the Virtual KITTI and Synthia datasets.


Title: Discrete Configuration Space Methods for Determining Modular Connector Area of Acceptance in Higher Dimensions
Key Words: collision avoidance  discrete systems  mobile robots  discrete configuration space methods  physical connectors  docking process  robotic control systems  automatic control systems  robotic self-reconfiguration  air-to-air refueling  configuration space obstacle model  modular connector area of acceptance  self-aligning geometry  Meyer's flooding algorithm  Connectors  Geometry  Robots  Contacts  Three-dimensional displays  Two dimensional displays  Image segmentation 
Abstract: Physical connectors with self-aligning geometry aid in the docking process for many robotic and automatic control systems such as robotic self-reconfiguration and air-to-air refueling. This self-aligning geometry provides a wider range of acceptable error tolerance in relative pose between the two rigid objects, increasing successful docking chances. We present a new method for computing the error range (or area of acceptance) for a pair of rigid connector objects with self-aligning geometry capable of higher dimensional analysis which was previously limited to three. The method is based on the configuration space obstacle model, which gives us a representation of the space of contact states between the two objects. Using an approach direction as analogous to gravity, and assuming the target docked configuration is stable, the set of misaligned points that lead to docking is the target configuration's watershed for an arbitrarily dimensioned configuration space obstacle. It is well known that the watershed of a height map on a discrete grid can be found using any number of algorithms from image segmentation. We present an implementation based on Meyer's flooding algorithm to determine this watershed and measure the AA for simple connectors in 2D and 3D. Results are presented for systems including unconstrained motion in SE(2) and motion constrained to four dimensions (ie. x,y,z,pitch) in SE(3).


Title: Towards vision-based manipulation of plastic materials
Key Words: deformation  manipulators  object tracking  plastic products  robot vision  vision-based manipulation  plastic materials  object deformation  visual tracking  visual error  deformable objects  kinetic sand shaping  Task analysis  Robots  Shape  Plastics  Visualization  Deformable models  Strain  Manipulation  visual servoing  human studies  learning 
Abstract: This paper represents a step towards vision-based manipulation of plastic materials. Manipulating deformable objects is made challenging by: 1) the absence of a model for the object deformation, 2) the inherent difficulty of visual tracking of deformable objects, 3) the difficulty in defining a visual error and 4) the difficulty in generating control inputs to minimise the visual error. We propose a novel representation of the task of manipulating deformable objects. In this preliminary case study, the shaping of kinetic sand, we assume a finite set of actions: pushing, tapping and incising. We consider that these action types affect only a subset of the state, i.e., their effect does not affect the entire state of the system (specialized actions). We report the results of a user study to validate these hypotheses and release the recorded dataset. The actions (pushing, tapping and incising) are clearly adopted during the task, although it is clear that 1) participants use also mixed actions and 2) actions' effects can marginally affect the entire state, requesting a relaxation of our specialized actions hypothesis. Moreover, we compute task errors and corresponding control inputs (in the image space) using image processing. Finally, we show how machine learning can be applied to infer the mapping from error to action on the data extracted from the user study.


Title: Generating Adaptive Attending Behaviors using User State Classification and Deep Reinforcement Learning
Key Words: behavioural sciences computing  gradient methods  learning (artificial intelligence)  mobile robots  pattern classification  deep deterministic policy gradient  user state classification  deep reinforcement learning  user information  adaptive attending behavior generation  DDPG  mobile robots  Legged locomotion  Reinforcement learning  Two dimensional displays  Cameras  Acceleration  Robot sensing systems 
Abstract: This paper describes a method of generating attending behaviors adaptively to the user state. The method classifies the user state based on user information such as the relative position and the orientation. For each classified state, the method executes the corresponding policy for behavior generation, which has been trained using a deep reinforcement learning, namely DDPG (deep deterministic policy gradient). We use as a state space of DDPG a distance-transformed local map with person information, and define reward functions suitable for respective user states. We conducted attending experiments both in a simulated and a real environment to show the effectiveness of the proposed method.


Title: Real-Time Edge Template Tracking via Homography Estimation
Key Words: edge detection  feature extraction  image sampling  image sequences  image texture  target tracking  transforms  video signal processing  homography estimation  planar edge templates  homography transformations  sampled edge pixels  Lucas-Kanade-like algorithm  low textured targets  edge template tracking  nonLambertian objects  video sequences  Image edge detection  Target tracking  Real-time systems  Video sequences  Cost function  Transforms 
Abstract: In this paper, we propose a novel real-time method for tracking planar edge templates. This method tracks an edge template by estimating its homography transformations with respect to the sampled edge pixels detected from the incoming frames. Particularly, we define a cost function based on a new feature map of the to-be-tracked edge template and optimize it by a Lucas-Kanade-like algorithm. The feature map is defined as the fourth root of the distance transform. Our method operates on just edges so that it is good at tracking those low textured targets, such as hollow targets (mug rim), thin targets (cable, ring) and non-Lambertian objects (disc). We validate and compare our method with four other methods on five newly collected real-world video sequences. The results achieves the lowest overall average error (1.58 pixels) and also outperforms others in terms of success rate. The per frame processing time of about 30 ms proves that our method is acceptable in realtime applications. The code and dataset are publicly available at: http://webdocs.cs.ualberta.ca/~xuebin/.


Title: Robust Model-Predictive Deformation Control of a Soft Object by Using a Flexible Continuum Robot
Key Words: deformation  Jacobian matrices  manipulators  mobile robots  nonlinear control systems  predictive control  robust control  surgery  robust model-predictive deformation control  soft object  flexible continuum robot  soft tissues  prediction horizon-based controller  Strain  Jacobian matrices  Force  Deformable models  Uncertainty  End effectors 
Abstract: Flexible continuum robots have exhibited unique advantages in working in an unstructured environment. Many applications require robots to actively control the deformation of soft objects, such as soft tissues in surgery. Thus, this study presents a robust model-predictive deformation control of a soft object using a flexible continuum robot. A linear approximation model for mapping from actuation space of a continuum robot to deformation space of a soft object is established. Jacobian matrix is estimated online by using a robust Geman-McClure estimator. Then, the deformation of the soft object is regulated by using a prediction horizon-based controller with exponential weighting for model uncertainty. The proposed control approach is effective in manipulating a soft object with a flexible continuum robot that is in contact with obstacles.


Title: City-Scale Road Audit System using Deep Learning
Key Words: Global Positioning System  image segmentation  learning (artificial intelligence)  roads  traffic engineering computing  image tagging  GPS  multistep deep learning  road networks  city-scale road audit system  label hierarchy  road defects  semantic segmentation  Roads  Image segmentation  Semantics  Deep learning  Global Positioning System  Cameras  Real-time systems 
Abstract: Road networks in cities are massive and is a critical component of mobility. Fast response to defects, that can occur not only due to regular wear and tear but also because of extreme events like storms, is essential. Hence there is a need for an automated system that is quick, scalable and cost-effective for gathering information about defects. We propose a system for city-scale road audit, using some of the most recent developments in deep learning and semantic segmentation. For building and benchmarking the system, we curated a dataset which has annotations required for road defects. However, many of the labels required for road audit have high ambiguity which we overcome by proposing a label hierarchy. We also propose a multi-step deep learning model that segments the road, subdivide the road further into defects, tags the frame for each defect and finally localizes the defects on a map gathered using GPS. We analyze and evaluate the models on image tagging as well as segmentation at different levels of the label hierarchy.


Title: A Multi-Position Joint Particle Filtering Method for Vehicle Localization in Urban Area
Key Words: distance measurement  image matching  mobile robots  particle filtering (numerical methods)  path planning  probability  robot vision  flexible multiposition joint particle filtering  position error  anchor point  curving roads  ego-trajectory  probabilistic filtering method  flexible road map  long range navigation  error accumulation  visual odometry  traditional visual localization methods  autonomous vehicles  robust localization  urban area  vehicle localization  dense parallel road branches  Roads  Trajectory  Filtering  Urban areas  Wheels  Simultaneous localization and mapping  Navigation 
Abstract: Robust localization is a prerequisite for autonomous vehicles. Traditional visual localization methods like visual odometry suffer error accumulation on long range navigation. In this paper, a flexible road map based probabilistic filtering method is proposed to tackle this problem. To effectively match the ego-trajectory to various curving roads in map, a new representation based on anchor point (AP) which captures the main curving points on the trajectory is presented. Based on APs of the map and trajectory, a flexible Multi-Position Joint Particle Filtering (MPJPF) framework is proposed to correct the position error. The method features the capability of adaptively estimating a series of APs jointly and only updates the estimation at situations with low uncertainty. It explicitly avoids the drawbacks of obliging to determine the current position at large uncertain situations such as dense parallel road branches. The experiments carried out on KITTI benchmark demonstrate our success.


Title: Joint Ego-motion Estimation Using a Laser Scanner and a Monocular Camera Through Relative Orientation Estimation and 1-DoF ICP
Key Words: automobiles  cameras  iterative methods  laser ranging  mobile robots  motion estimation  optical scanners  pose estimation  sensor fusion  SLAM (robots)  joint ego-motion estimation  laser scanner  monocular camera  autonomous vehicles  SLAM algorithms  sensor suite  laser range finder  3D point clouds  iterative closest point problem  sensor modality  orientation estimation  autonomous cars  pose estimation  autonomous robots  1-DoF ICP  data association  Cameras  Iterative closest point algorithm  Lasers  Three-dimensional displays  Robot vision systems  Image color analysis 
Abstract: Pose estimation and mapping are key capabilities of most autonomous vehicles and thus a number of localization and SLAM algorithms have been developed in the past. Autonomous robots and cars are typically equipped with multiple sensors. Often, the sensor suite includes a camera and a laser range finder. In this paper, we consider the problem of incremental ego-motion estimation, using both, a monocular camera and a laser range finder jointly. We propose a new algorithm, that exploits the advantages of both sensors-the ability of cameras to determine orientations well and the ability of laser range finders to estimate the scale and to directly obtain 3D point clouds. Our approach estimates the 5 degrees of freedom relative orientation from image pairs through feature point correspondences and formulates the remaining scale estimation as a new variant of the iterative closest point problem with only one degree of freedom. We furthermore exploit the camera information in a new way to constrain the data association between laser point clouds. The experiments presented in this paper suggest that our approach is able to accurately estimate the ego-motion of a vehicle and that we obtain more accurate frame-to-frame alignments than with one sensor modality alone.


Title: LandmarkBoost: Efficient visualContext Classifiers for Robust Localization
Key Words: image capture  image classification  image matching  image retrieval  nearest neighbour methods  pose estimation  search problems  stereo image processing  metric pose retrieval algorithms  image plane  state-of-the-art descriptor matching methods  visualContext classifiers  binary descriptors  robust localization  Landmark-Boost  boosting framework  contextual information  landmark observations  boosted classifier  landmark classification task  2D-3D matching methods  visual context  mobile platforms  nearest neighbor search  reliable pose retrieval algorithms  Visualization  Feature extraction  Measurement  Robots  Three-dimensional displays  Pose estimation  Context modeling 
Abstract: The growing popularity of autonomous systems creates a need for reliable and efficient metric pose retrieval algorithms. Currently used approaches tend to rely on nearest neighbor search of binary descriptors to perform the 2D-3D matching and guarantee realtime capabilities on mobile platforms. These methods struggle, however, with the growing size of the map, changes in viewpoint or appearance, and visual aliasing present in the environment. The rigidly defined descriptor patterns only capture a limited neighborhood of the keypoint and completely ignore the overall visual context. We propose LandmarkBoost - an approach that, in contrast to the conventional 2D-3D matching methods, casts the search problem as a landmark classification task. We use a boosted classifier to classify landmark observations and directly obtain correspondences as classifier scores. We also introduce a formulation of visual context that is flexible, efficient to compute, and can capture relationships in the entire image plane. The original binary descriptors are augmented with contextual information and informative features are selected by the boosting framework. Through detailed experiments, we evaluate the retrieval quality and performance of Landmark-Boost, demonstrating that it outperforms common state-of-the-art descriptor matching methods.


Title: Embedding Temporally Consistent Depth Recovery for Real-time Dense Mapping in Visual-inertial Odometry
Key Words: distance measurement  image reconstruction  interpolation  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  real-time dense mapping  visual-inertial odometry  dense scene information  fast self-localization  VIO-based SLAM systems  VIO depth estimations  subspace-based stabilization scheme  temporal consistency  edge-preserving depth interpolation  simultaneous localization and mapping  learning-based methods  embedding temporally consistent depth recovery  Simultaneous localization and mapping  Real-time systems  Feature extraction  Pipelines  Interpolation  Three-dimensional displays 
Abstract: Dense mapping is always the desire of simultaneous localization and mapping (SLAM), especially for the applications that require fast and dense scene information. Visual-inertial odometry (VIO) is a light-weight and effective solution to fast self-localization. However, VIO-based SLAM systems have difficulty in providing dense mapping results due to the spatial sparsity and temporal instability of the VIO depth estimations. Although there have been great efforts on real-time mapping and depth recovery from sparse measurements, the existing solutions for VIO-based SLAM still fail to preserve sufficient geometry details in their results. In this paper, we propose to embed depth recovery into VIO-based SLAM for real-time dense mapping. In the proposed method, we present a subspace-based stabilization scheme to maintain the temporal consistency and design a hierarchical pipeline for edge-preserving depth interpolation to reduce the computational burden. Numerous experiments demonstrate that our method can achieve an accuracy improvement of up to 49.1 cm compared to state-of-the-art learning-based methods for depth recovery and reconstruct sufficient geometric details in dense mapping when only 0.07% depth samples are available. Since a simple CPU implementation of our method already runs at 10-20 fps, we believe our method is very favorable for practical SLAM systems with critical computational requirements.


Title: Perception Based Locomotion System for a Humanoid Robot with Adaptive Footstep Compensation under Task Constraints
Key Words: adaptive control  collision avoidance  humanoid robots  interpolation  legged locomotion  path planning  task constraints  humanoid robot  adaptive footstep compensation  adaptive locomotion system  local error correction  perception based locomotion system  locomotion error  locomotion planning  point cloud  environmental measurements  plane estimation  space interpolation  collision avoidance  laser scans  Humanoid robots  Task analysis  Planning  Foot  Three-dimensional displays  Estimation 
Abstract: In order to accurately reach a target position while executing a task which imposes occlusion or constraints of the posture, a humanoid robot requires an adaptive locomotion system, which can comprehensively integrate localization, environmental mapping, global locomotion planning and local error correction. In this paper, we propose a method of constructing a perception based locomotion system for a humanoid robot. The major contribution of this paper is solving a problem of the locomotion error caused by the task constraints, by locally compensating footsteps and assessing the need for global footstep re-planning online based on environmental measurements. The proposed system provides an accurate and dense ground point cloud, called HeightField, using plane estimation and space interpolation, and obstacle point cloud for frequent collision avoidance by accumulating laser scans. This environmental perception enables a humanoid robot to plan footsteps globally even in the situation where the sight of the robot is limited and compensate footsteps while estimating landing state during locomotion online with the localization result. We evaluated the practicality of the proposed system by applying it to our humanoid robot carrying a heavy object in a construction site and confirmed that the proposed system contributed to improved locomotion abilities of a humanoid robot engaging in heavy-duty or dangerous tasks.


Title: Learning How Pedestrians Navigate: A Deep Inverse Reinforcement Learning Approach
Key Words: collision avoidance  feature extraction  human-robot interaction  learning (artificial intelligence)  mobile robots  navigation  neural nets  trajectory control  mobile robots  human robot interaction  robot navigation algorithms  human navigation behaviors  maximum entropy deep inverse reinforcement learning  nonlinear reward function  deep neural network approximation  social affinity map  human motion trajectories  learned reward function  natural social navigation behaviors  deep inverse reinforcement learning approach  pedestrian trajectories  MEDIRL algorithm  feature extraction  collision avoidance  pedestrians navigation  Navigation  Robots  Trajectory  Reinforcement learning  Collision avoidance  Neural networks  Entropy 
Abstract: Humans and mobile robots will be increasingly cohabiting in the same environments, which has lead to an increase in studies on human robot interaction (HRI). One important topic in these studies is the development of robot navigation algorithms that are socially compliant to humans navigating in the same space. In this paper, we present a method to learn human navigation behaviors using maximum entropy deep inverse reinforcement learning (MEDIRL). We use a large open dataset of pedestrian trajectories collected in an uncontrolled environment as the expert demonstrations. Human navigation behaviors are captured by a nonlinear reward function through deep neural network (DNN) approximation. The developed MEDIRL algorithm takes feature inputs including social affinity map (SAM) that are extracted from human motion trajectories. We perform simulation experiments using the learned reward function, and the performance is evaluated comparing it with the real measured pedestrian trajectories in the dataset. The evaluation results show that the proposed method has acceptable prediction accuracy compared to other state-of-the-art methods, and it can generate pedestrian trajectories similar to real human trajectories with natural social navigation behaviors such as collision avoidance, leader-follower, and split-and-rejoin.


Title: Deep Semantic Lane Segmentation for Mapless Driving
Key Words: automobiles  feature extraction  image colour analysis  image segmentation  mobile robots  neural nets  object detection  path planning  road traffic  robot vision  deep semantic lane segmentation  autonomous driving systems  automated cars  sensor system  urban scenarios  deep neural network  lane semantics  road scene  mapless autonomous driving  street scenes  RGB images  lane detection  cityscapes dataset  Roads  Semantics  Neural networks  Image segmentation  Autonomous vehicles  Three-dimensional displays  Pipelines 
Abstract: In autonomous driving systems a strong relation to highly accurate maps is taken to be inevitable, although street scenes change frequently. However, a preferable system would be to equip the automated cars with a sensor system that is able to navigate urban scenarios without an accurate map. We present a novel pipeline using a deep neural network to detect lane semantics and topology given RGB images. On the basis of this classification, the information about the road scene can be extracted just from the sensor setup supporting mapless autonomous driving. In addition to superseding the huge effort of creating and maintaining highly accurate maps, our system reduces the need for precise localization. Using an extended Cityscapes dataset, we show accurate ego lane detection including lane semantics on challenging scenarios for autonomous driving.


Title: PRISM: Pose Registration for Integrated Semantic Mapping
Key Words: mobile robots  multi-robot systems  navigation  pose estimation  service robots  SLAM (robots)  computer science department  modern SLAM algorithms  map data  tedious manual process  automatically generated maps  PRISM  semantic markup  pose registration  integrated semantic  robotics applications  hotel  room service  hospital  medication  patient  UT Austin  autonomous mobile robots  BWIBots  building-wide intelligence project  Robots  Semantics  Three-dimensional displays  Cameras  Two dimensional displays  Computational modeling  Navigation 
Abstract: Many robotics applications involve navigating to positions specified in terms of their semantic significance. A robot operating in a hotel may need to deliver room service to a named room. In a hospital, it may need to deliver medication to a patient's room. The Building-Wide Intelligence Project at UT Austin has been developing a fleet of autonomous mobile robots, called BWIBots, which perform tasks in the computer science department. Tasks include guiding a person, delivering a message, or bringing an object to a location such as an office, lecture hall, or classroom. The process of constructing a map that a robot can use for navigation has been simplified by modern SLAM algorithms. The attachment of semantics to map data, however, remains a tedious manual process of labeling locations in otherwise automatically generated maps. This paper introduces a system called PRISM to automate a step in this process by enabling a robot to localize door signs - a semantic markup intended to aid the human occupants of a building - and to annotate these locations in its map.


Title: Semantic Mapping with Simultaneous Object Detection and Localization
Key Words: image sensors  mobile robots  object detection  particle filtering (numerical methods)  pose estimation  semantic mapping problem  CT-Map method  six degree-of-freedom pose  pose estimation  RGB-D sensor  Michigan progress fetch robot  particle filtering algorithm  CRF  conditional random field  contextual temporal mapping  object localization  object detection  Semantics  Object detection  Context modeling  Three-dimensional displays  Pose estimation  Simultaneous localization and mapping 
Abstract: We present a filtering-based method for semantic mapping to simultaneously detect objects and localize their 6 degree-of-freedom pose. For our method, called Contextual Temporal Mapping (or CT-Map), we represent the semantic map as a belief over object classes and poses across an observed scene. Inference for the semantic mapping problem is then modeled in the form of a Conditional Random Field (CRF). CT-Map is a CRF that considers two forms of relationship potentials to account for contextual relations between objects and temporal consistency of object poses, as well as a measurement potential on observations. A particle filtering algorithm is then proposed to perform inference in the CT-Map model. We demonstrate the efficacy of the CT-Map method with a Michigan Progress Fetch robot equipped with a RGB-D sensor. Our results demonstrate that the particle filtering based inference of CT-Map provides improved object detection and pose estimation with respect to baseline methods that treat observations as independent samples of a scene.


Title: Image-Based Visual Servoing Controller for Multirotor Aerial Robots Using Deep Reinforcement Learning
Key Words: aerospace computing  aerospace robotics  aircraft control  control engineering computing  gradient methods  helicopters  learning (artificial intelligence)  mobile robots  robot vision  visual servoing  deep reinforcement learning algorithm  deep deterministic policy gradients  image-based visual servoing controller  IBVS policy  linear velocity commands  multirotor aerial robots  simulated flight scenarios  Gazebo-based simulation scenario  RL-IBVS controller  Visual servoing  Reinforcement learning  Unmanned aerial vehicles  Task analysis  Detectors  Cameras 
Abstract: In this paper, we propose a novel Image-Based Visual Servoing (IBVS) controller for multirotor aerial robots based on a recent deep reinforcement learning algorithm named Deep Deterministic Policy Gradients (DDPG). The proposed RL-IBVS controller is successfully trained in a Gazebo-based simulation scenario in order to learn the appropriate IBVS policy for directly mapping a state, based on errors in the image, to the linear velocity commands of the aerial robot. A thorough validation of the proposed controller has been conducted in simulated and real flight scenarios, demonstrating outstanding capabilities in object following applications. Moreover, we conduct a detailed comparison of the RL-IBVS controller with respect to classic and partitioned IBVS approaches.


Title: C-blox: A Scalable and Consistent TSDF-based Dense Mapping Approach
Key Words: autonomous aerial vehicles  image reconstruction  image sensors  robot vision  SLAM (robots)  truncated signed distance field  TSDF subvolumes  lightweight micro aerial vehicle  scalable maps  map growth  bundle adjustment  feature-based camera tracking  dense 3D mapping  map consistency  delayed loop closure  accumulated camera tracking error  precise dense 3D maps  higher level decision making  robotic platforms  consistent dense map  Cameras  Simultaneous localization and mapping  Image reconstruction  Three-dimensional displays  Robot vision systems 
Abstract: In many applications, maintaining a consistent dense map of the environment is key to enabling robotic platforms to perform higher level decision making. Several works have addressed the challenge of creating precise dense 3D maps from visual sensors providing depth information. However, during operation over longer missions, reconstructions can easily become inconsistent due to accumulated camera tracking error and delayed loop closure. Without explicitly addressing the problem of map consistency, recovery from such distortions tends to be difficult. We present a novel system for dense 3D mapping which addresses the challenge of building consistent maps while dealing with scalability. Central to our approach is the representation of the environment as a collection of overlapping Truncated Signed Distance Field (TSDF) subvolumes. These subvolumes are localized through feature-based camera tracking and bundle adjustment. Our main contribution is a pipeline for identifying stable regions in the map, and to fuse the contributing subvolumes. This approach allows us to reduce map growth while still maintaining consistency. We demonstrate the proposed system on a publicly available dataset and simulation engine, and demonstrate the efficacy of the proposed approach for building consistent and scalable maps. Finally we demonstrate our approach running in real-time onboard a lightweight Micro Aerial Vehicle (MAV).


Title: Stereo Visual Odometry and Semantics based Localization of Aerial Robots in Indoor Environments
Key Words: distance measurement  image colour analysis  image segmentation  indoor environment  learning (artificial intelligence)  mobile robots  neural nets  object detection  particle filtering (numerical methods)  pose estimation  robot vision  SLAM (robots)  stereo image processing  indoor environments  particle filter localization approach  semantic information  mini-aerial robots  stereo VO algorithm  semantic measurements  pre-trained deep learning based object detector  3D point clouds  visual SLAM approach  stereo visual odometry  semantics based localization  DL  RGB spectrum  drift free pose estimation  Semantics  Three-dimensional displays  Unmanned aerial vehicles  Robots  Atmospheric measurements  Particle measurements  Prediction algorithms 
Abstract: In this paper we propose a particle filter localization approach, based on stereo visual odometry (VO) and semantic information from indoor environments, for mini-aerial robots. The prediction stage of the particle filter is performed using the 3D pose of the aerial robot estimated by the stereo VO algorithm. This predicted 3D pose is updated using inertial as well as semantic measurements. The algorithm processes semantic measurements in two phases; firstly, a pre-trained deep learning (DL) based object detector is used for real time object detections in the RGB spectrum. Secondly, from the corresponding 3D point clouds of the detected objects, we segment their dominant horizontal plane and estimate their relative position, also augmenting a prior map with new detections. The augmented map is then used in order to obtain a drift free pose estimate of the aerial robot. We validate our approach in several real flight experiments where we compare it against ground truth and a state of the art visual SLAM approach.


Title: Laser-Based Reactive Navigation for Multirotor Aerial Robots using Deep Reinforcement Learning
Key Words: autonomous aerial vehicles  collision avoidance  learning (artificial intelligence)  mobile robots  traditional motion planning algorithms  precise maps  fast reactive navigation algorithm  multirotor aerial robots  2D-laser range measurements  Gazebo-based simulation scenario  artificial potential field formulation  laser-based reactive navigation  collision avoidance capabilities  reactive navigation behavior  deep reinforcement learning  dynamic obstacles  static obstacles  Navigation  Robots  Unmanned aerial vehicles  Lasers  Heuristic algorithms  Reinforcement learning  Planning 
Abstract: Navigation in unknown indoor environments with fast collision avoidance capabilities is an ongoing research topic. Traditional motion planning algorithms rely on precise maps of the environment, where re-adapting a generated path can be highly demanding in terms of computational cost. In this paper, we present a fast reactive navigation algorithm using Deep Reinforcement Learning applied to multi rotor aerial robots. Taking as input the 2D-laser range measurements and the relative position of the aerial robot with respect to the desired goal, the proposed algorithm is successfully trained in a Gazebo-based simulation scenario by adopting an artificial potential field formulation. A thorough evaluation of the trained agent has been carried out both in simulated and real indoor scenarios, showing the appropriate reactive navigation behavior of the agent in the presence of static and dynamic obstacles.


Title: Drone Detection Using Depth Maps
Key Words: autonomous aerial vehicles  collision avoidance  image sensors  learning (artificial intelligence)  mobile robots  object detection  static obstacle avoidance  dynamic objects  field-of-view requirements  on-board small UAVs  relative altitude  azimuth  depth map-based approach  collision avoidance  depth map sequences  unmanned aerial vehicle navigation  collision-free path planning  FOV  deep learning-based drone detection model  sensing technologies  3D localization  Drones  Cameras  Three-dimensional displays  Atmospheric modeling  Sensors  Neural networks  Two dimensional displays 
Abstract: Obstacle avoidance is a key feature for safe Unmanned Aerial Vehicle (UAV) navigation. While solutions have been proposed for static obstacle avoidance, systems enabling avoidance of dynamic objects, such as drones, are hard to implement due to the detection range and field-of-view (FOV) requirements, as well as the constraints for integrating such systems on-board small UAVs. In this work, a dataset of 6k synthetic depth maps of drones has been generated and used to train a state-of-the-art deep learning-based drone detection model. While many sensing technologies can only provide relative altitude and azimuth of an obstacle, our depth map-based approach enables full 3D localization of the obstacle. This is extremely useful for collision avoidance, as 3D localization of detected drones is key to perform efficient collision-free path planning. The proposed detection technique has been validated in several real depth map sequences, with multiple types of drones flying at up to 2 m/s, achieving an average precision of 98.7 %, an average recall of 74.7 % and a record detection range of 9.5 meters.


Title: Towards View-Invariant Intersection Recognition from Videos using Deep Network Ensembles
Key Words: data mining  image recognition  learning (artificial intelligence)  neural nets  object recognition  video signal processing  recognition accuracy  road segments  LSTM based Siamese style deep network  meeting point  deep network ensembles  videos  view-invariant intersection recognition  video recognition  Videos  Trajectory  Visualization  Task analysis  Roads  Image recognition  Training 
Abstract: This paper strives to answer the following question: Is it possible to recognize an intersection when seen from different road segments that constitute the intersection? An intersection or a junction typically is a meeting point of three or four road segments. Its recognition from a road segment that is transverse to or 180 degrees apart from its previous sighting is an extremely challenging and yet a very relevant problem to be addressed from the point of view of both autonomous driving as well as loop detection. This paper formulates this as a problem of video recognition and proposes a novel LSTM based Siamese style deep network for video recognition. For what is indeed a challenging problem and the limited annotated dataset available we show competitive results of recognizing intersections when approached from diverse viewpoints or road segments. Specifically, we tabulate effective recognition accuracy even as the approaches to the intersection being compared are disparate both in terms of viewpoints and weather/illumination conditions. We show competitive results on both synthetic yet highly realistic data mined from the gaming platform GTA as well as on real world data made available through Mapillary.


Title: Tree Species Identification from Bark Images Using Convolutional Neural Networks
Key Words: feature extraction  forestry  geophysical image processing  image classification  learning (artificial intelligence)  neural nets  vegetation mapping  bark images  tree individual number  high-resolution bark images  species recognition  tree diameters  tree bark species classification  standard vision problems  deep learning  forestry related tasks  convolutional neural networks  tree species identification  Vegetation  Forestry  Deep learning  Feature extraction  Training  Cameras  Task analysis 
Abstract: Tree species identification using bark images is a challenging problem that could prove useful for many forestry related tasks. However, while the recent progress in deep learning showed impressive results on standard vision problems, a lack of datasets prevented its use on tree bark species classification. In this work, we present, and make publicly available, a novel dataset called BarkNet 1.0 containing more than 23,000 high-resolution bark images from 23 different tree species over a wide range of tree diameters. With it, we demonstrate the feasibility of species recognition through bark images, using deep learning. More specifically, we obtain an accuracy of 93.88% on single crop, and an accuracy of 97.81% using a majority voting approach on all of the images of a tree. We also empirically demonstrate that, for a fixed number of images, it is better to maximize the number of tree individuals in the training database, thus directing future data collection efforts.


Title: UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation
Key Words: cameras  distance measurement  feature extraction  image reconstruction  image sequences  learning (artificial intelligence)  motion estimation  pose estimation  stereo image processing  unsupervised learning  unsupervised deep network  ego-motion estimation  unsupervised visual odometry system  monocular view  objective function  temporally alligned sequences  monocular images  disparity-based depth estimation network  dense depth map  UnDEMoN  binocular stereo image pairs  temporal reconstruction losses  pose estimation network  6DoF camera pose estimation  Image reconstruction  Cameras  Pose estimation  Training  Meters  Linear programming 
Abstract: This paper presents a deep network based unsupervised visual odometry system for 6-DoF camera pose estimation and finding dense depth map for its monocular view. The proposed network is trained using unlabeled binocular stereo image pairs and is shown to provide superior performance in depth and ego-motion estimation compared to the existing state-of-the-art. This is achieved by introducing a novel objective function and training the network using temporally alligned sequences of monocular images. The objective function is based on the Charbonnier penalty applied to spatial and bi-directional temporal reconstruction losses. The overall novelty of the approach lies in the fact that the proposed deep framework combines a disparity-based depth estimation network with a pose estimation network to obtain absolute scale-aware 6-DoF camera pose and superior depth map. According to our knowledge, such a framework with complete unsupervised end-to-end learning has not been tried so far, making it a novel contribution in the field. The effectiveness of the approach is demonstrated through performance comparison with the state-of-the-art methods on KITTI driving dataset.


Title: Compact & Comprehensive Canonical Appearances Discovered Autonomously
Key Words: decision making  image representation  image sensors  learning (artificial intelligence)  mobile robots  path planning  robot vision  exploration approach  autonomous ground robot  depth sensor  bubble space representation  exploration path length  topological mapping  canonical appearances  appearance-based learning  Robot sensing systems  Decision making  Robot kinematics  Cognition  Lasers  Measurement 
Abstract: This paper presents an exploration approach for discovering canonical appearances in unknown environments using an autonomous ground robot equipped with a depth sensor. This approach is based on the previously proposed two-stage algorithm that alternates between local and global decision making for efficient topological mapping based on bubble space representation. Differing from it, the approach aims to identify vantage viewpoints with characterizing views for subsequent appearance-based learning as well as achieving complete coverage. This is demonstrated by a series of experiments using an outdoor benchmark data set including a comparative study with evaluation metrics including the exploration path length and number of canonical appearances discovered.


Title: Deep Learning for Exploration and Recovery of Uncharted and Dynamic Targets from UAV-like Vision
Key Words: autonomous aerial vehicles  convolutional neural nets  image classification  learning (artificial intelligence)  mobile robots  path planning  probability  random processes  robot vision  target tracking  online search tasks  multitarget environments  dynamic targets  UAV-like vision  deep learning  dynamic search  strategic explorational agency  single deep network  navigational actions  dual-stream classification paradigm  sensory processing  agent location  static evolutions  dynamic evolutions  probabilistic placement  fully random target walks  herd-inspired behaviours  dual-stream architecture  unmanned aerial vehicle  convolutional neural network  multitarget behaviour classes  optimal navigational decision samples  long term map memory  Navigation  Robot sensing systems  Task analysis  History  Visualization  Vehicle dynamics  Reinforcement learning 
Abstract: This paper discusses deep learning for solving static and dynamic search and recovery tasks - such as the retrieval of all instances of actively moving targets - based on partial-view Unmanned Aerial Vehicle (UAV)-like sensing. In particular, we demonstrate that abstracted tactic and strategic explorational agency can be implemented effectively via a single deep network that optimises in unity: the mapping of sensory inputs and positional history towards navigational actions. We propose a dual-stream classification paradigm that integrates one Convolutional Neural Network (CNN) for sensory processing with a second one for interpreting an evolving longterm map memory. In order to learn effective search behaviours given agent location and agent-centric sensory inputs, we train this design against 400k+ optimal navigational decision samples from each set of static and dynamic evolutions for different multi-target behaviour classes. We quantify recovery performance across an extensive range of scenarios; including probabilistic placement and dynamics, as well as fully random target walks and herd-inspired behaviours. Detailed results comparisons show that our design can outperform naive, independent stream and off-the-shelf DRQN solutions. We conclude that the proposed dual-stream architecture can provide a unified, rationally motivated and effective architecture for solving online search tasks in dynamic, multi-target environments. With this paper we publish3 key source code and associated models.


Title: Information Sparsification in Visual-Inertial Odometry
Key Words: computational complexity  distance measurement  graph theory  mobile robots  SLAM (robots)  information sparsification  tightly couple visual measurements  inertial measurements  fixed-lag visual-inertial odometry framework  bound computational complexity  fixed-lag smoothers  densely connected linear  information-theoretic perspective  dense marginalization step  information content  nonlinear factor graph  information loss  information sparsity  VIO methods  EuRoC visual-inertial dataset  structural similarity  nonlinearity  computational complexity  Optimization  Markov processes  Microsoft Windows  Computational complexity  Cameras  Simultaneous localization and mapping  Visualization 
Abstract: In this paper, we present a novel approach to tightly couple visual and inertial measurements in a fixed-lag visual-inertial odometry (VIO) framework using information sparsification. To bound computational complexity, fixed-lag smoothers typically marginalize out variables, but consequently introduce a densely connected linear prior which significantly deteriorates accuracy and efficiency. Current state-of-the-art approaches account for the issue by selectively discarding measurements and marginalizing additional variables. However, such strategies are sub-optimal from an information-theoretic perspective. Instead, our approach performs a dense marginalization step and preserves the information content of the dense prior. Our method sparsifies the dense prior with a nonlinear factor graph by minimizing the information loss. The resulting factor graph maintains information sparsity, structural similarity, and nonlinearity. To validate our approach, we conduct real-time drone tests and perform comparisons to current state-of-the-art fixed-lag VIO methods in the EuRoC visual-inertial dataset. The experimental results show that the proposed method achieves competitive and superior accuracy in almost all trials. We include a detailed run-time analysis to demonstrate that the proposed algorithm is suitable for real-time applications.


Title: Towards Robust Visual Odometry with a Multi-Camera System
Key Words: cameras  distance measurement  image sampling  minimisation  photometry  pose estimation  position measurement  stereo image processing  robust visual odometry algorithm  robust VO algorithm  current pose tracker estimation  photometric error minimisation  plane-sweeping stereo cameras  near-infrared illumination  NIR illumination  single stereo configuration  multicamera setup  sliding window optimizer  sampled feature points  local mapper  multicamera system  Cameras  Tracking  Lighting  Visual odometry  Robot vision systems  Robustness  Simultaneous localization and mapping 
Abstract: We present a visual odometry (VO) algorithm for a multi-camera system and robust operation in challenging environments. Our algorithm consists of a pose tracker and a local mapper. The tracker estimates the current pose by minimizing photometric errors between the most recent keyframe and the current frame. The mapper initializes the depths of all sampled feature points using plane-sweeping stereo. To reduce pose drift, a sliding window optimizer is used to refine poses and structure jointly. Our formulation is flexible enough to support an arbitrary number of stereo cameras. We evaluate our algorithm thoroughly on five datasets. The datasets were captured in different conditions: daytime, night-time with near-infrared (NIR) illumination and nighttime without NIR illumination. Experimental results show that a multi-camera setup makes the VO more robust to challenging environments, especially night-time conditions, in which a single stereo configuration fails easily due to the lack of features.


Title: Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition
Key Words: entropy  feature extraction  geometry  image matching  image recognition  learning (artificial intelligence)  mobile robots  octrees  optical radar  robot vision  unsupervised learning  Generative Adversarial Network  adversarial feature  place recognition  global geometry map  Conditional Entropy Reduction module  unsupervised place feature  local 2D maps  dynamic octree mapping module  core modules  LiDAR inputs  end-to-end feature  geometry matching  traditional methods  LiDAR-based place recognition  unsupervised feature learning  feature size  place recognition task  North Campus Long-Term LiDAR dataset  feature learning process  place feature learning  Octrees  Laser radar  Task analysis  Decoding  Simultaneous localization and mapping  Generative adversarial networks 
Abstract: Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.


Title: DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments
Key Words: mobile robots  object detection  path planning  robot vision  SLAM (robots)  high-dynamic environments  ORB-SLAM2  dense semantic octo-tree map  dynamic objects  DS-SLAM combines semantic segmentation network  dense semantic map creation  local mapping  robust semantic visual SLAM  impressed SLAM systems  Semantics  Simultaneous localization and mapping  Image segmentation  Feature extraction  Heuristic algorithms  Three-dimensional displays  Optical flow 
Abstract: Simultaneous Localization and Mapping (SLAM) is considered to be a fundamental capability for intelligent mobile robots. Over the past decades, many impressed SLAM systems have been developed and achieved good performance under certain circumstances. However, some problems are still not well solved, for example, how to tackle the moving objects in the dynamic environments, how to make the robots truly understand the surroundings and accomplish advanced tasks. In this paper, a robust semantic visual SLAM towards dynamic environments named DS-SLAM is proposed. Five threads run in parallel in DS-SLAM: tracking, semantic segmentation, local mapping, loop closing and dense semantic map creation. DS-SLAM combines semantic segmentation network with moving consistency check method to reduce the impact of dynamic objects, and thus the localization accuracy is highly improved in dynamic environments. Meanwhile, a dense semantic octo-tree map is produced, which could be employed for high-level tasks. We conduct experiments both on TUM RGB-D dataset and in real-world environment. The results demonstrate the absolute trajectory accuracy in DS-SLAM can be improved one order of magnitude compared with ORB-SLAM2. It is one of the state-of-the-art SLAM systems in high-dynamic environments.


Title: A robust pose graph approach for city scale LiDAR mapping
Key Words: graph theory  image filtering  image reconstruction  Kalman filters  mobile robots  nonlinear filters  optical radar  optimisation  pose estimation  radar imaging  robot vision  SLAM (robots)  map quality  quantitative experimental results  robust optimization strategy  systematical initialization bias  factor graph  refined structure  urban environments  multitask acquisitions  scan-matching factors  graph optimization  cumulative drift  city scale LiDAR mapping  robust pose graph approach  Optimization  Three-dimensional displays  Laser radar  Global Positioning System  Feature extraction  Sensors  Urban areas 
Abstract: This paper presents a method for reconstructing globally consistent 3D High-Definition (HD) maps at city scale. Current approaches for eliminating cumulative drift are mainly based on the pose graph optimization under the constraint of scan-matching factors. The misaligned edges in the graph may have negative impacts on the results. To address this problem and further handle inconsistency caused by multi-task acquisitions in urban environments, we introduce a refined structure of the factor graph considering systematical initialization bias, where the scan-matching factors are twice validated through a novel classifier and a robust optimization strategy. In addition, we incorporate a multi-hypothesis extended Kalman filter (MH-EKF) to remove dynamic objects. Quantitative experimental results demonstrate that the proposed method outperforms state-of-the-art techniques in terms of map quality.


Title: Good Feature Selection for Least Squares Pose Optimization in VO/VSLAM
Key Words: computational complexity  control engineering computing  feature extraction  least squares approximations  optimisation  pose estimation  robot vision  SLAM (robots)  least squares pose optimization  pose estimation  pose tracking  NP-hard Max-logDet problem  feature selection  VO-VSLAM  integrating Max-logDet feature selection  Feature extraction  Optimization  Pose estimation  Simultaneous localization and mapping  Measurement uncertainty  Approximation algorithms 
Abstract: This paper aims to select features that contribute most to the pose estimation in VO/VSLAM. Unlike existing feature selection works that are focused on efficiency only, our method significantly improves the accuracy of pose tracking, while introducing little overhead. By studying the impact of feature selection towards least squares pose optimization, we demonstrate the applicability of improving accuracy via good feature selection. To that end, we introduce the Max-logDet metric to guide the feature selection, which is connected to the conditioning of least squares pose optimization problem. We then describe an efficient algorithm for approximately solving the NP-hard Max-logDet problem. Integrating Max-logDet feature selection into a state-of-the-art visual SLAM system leads to accuracy improvements with low overhead, as demonstrated via evaluation on a public benchmark.


Title: HMAPs - Hybrid Height- Voxel Maps for Environment Representation
Key Words: mobile robots  optical radar  path planning  robot vision  SLAM (robots)  2.5D representation  Microsoft Kinect One  SLAM approach  complex elements  Velodyne VLP-16 LiDAR  updated grid representation  complex environments  reliable method  occupied space  free space  HVoxel  height-voxel elements  3D point-clouds  mobile robot  grid-based mapping approach  environment representation  hybrid height- voxel maps  HMAP  Two dimensional displays  Three-dimensional displays  Simultaneous localization and mapping  Pipelines  Ray tracing  Planning  Indexing 
Abstract: This paper presents a hybrid 3D-like grid-based mapping approach, that we called HMAP, used as a reliable and efficient 3D representation of the environment surrounding a mobile robot. Considering 3D point-clouds as input data, the proposed mapping approach addresses the representation of height-voxel (HVoxel) elements inside the HMAP, where free and occupied space is modeled through HVoxels, resulting in a reliable method for 3D representation. The proposed method corrects some of the problems inherent to the representation of complex environments based on 2D and 2.5D representations, while keeping an updated grid representation. Additionally, we also propose a complete pipeline for SLAM based on HMAPs. Indoor and outdoor experiments were carried out to validate the proposed representation using data from a Microsoft Kinect One (indoor) and a Velodyne VLP-16 LiDAR (outdoor). The obtained results show that HMAPs can provide a more detailed view of complex elements in a scene when compared to a classic 2.5D representation. Moreover, validation of the proposed SLAM approach was carried out in an outdoor dataset with promising results, which lay a foundation for further research in the topic.


Title: Magnetic- Visual Sensor Fusion-based Dense 3D Reconstruction and Localization for Endoscopic Capsule Robots
Key Words: biomedical optical imaging  cameras  endoscopes  image fusion  image reconstruction  medical image processing  medical robotics  robot vision  visual sensor fusion-based dense 3D reconstruction  real-time 3D reconstruction  actively controlled capsule endoscopic robots  minimally invasive diagnostic technology  therapeutic technology  gastrointestinal tract  intraoperative map fusion approach  actively controlled endoscopic capsule robot applications  magnetic vision-based localization  nonrigid deformations  frame-to-model map fusion  ex-vivo porcine stomach models  root mean square surface reconstruction errors  endoscopic camera  Magnetic resonance imaging  Robot sensing systems  Magnetic separation  Three-dimensional displays  Cameras  Endoscopes 
Abstract: Reliable and real-time 3D reconstruction and localization functionality is a crucial prerequisite for the navigation of actively controlled capsule endoscopic robots as an emerging, minimally invasive diagnostic and therapeutic technology for use in the gastrointestinal (GI) tract. In this study, we propose a fully dense, non-rigidly deformable, strictly real-time, intraoperative map fusion approach for actively controlled endoscopic capsule robot applications which combines magnetic and vision-based localization, with non-rigid deformations based frame-to-model map fusion. The performance of the proposed method is evaluated using four different ex-vivo porcine stomach models. Across different trajectories of varying speed and complexity, and four different endoscopic cameras, the root mean square surface reconstruction errors vary from 1.58 to 2.17 cm.


Title: Active Range and Bearing-based Radiation Source Localization
Key Words: cameras  image sensors  radioactive sources  static step size  radiation mapping approach  active source localization approach  adaptive step size  localization time  3D radiation source localization  bearing sensor  Compton gamma camera  image radiation  source locations  active source localization framework  Fisher Information  bearing-based radiation source localization  passive source localization  size 0.26 m  Sensors  Cameras  Photonics  Three-dimensional displays  Image sensors  Position measurement  Two dimensional displays 
Abstract: 3D radiation source localization is a common task across applications such as decommissioning, disaster response, and security, but traditional count-based sensors struggle to efficiently disambiguate between symmetries in sensor, source, and environment configurations. Recent works have demonstrated successful passive source localization using a bearing sensor called the Compton gamma camera that can image radiation. This paper first presents an approach to mapping the spatial distribution of radiation with a gamma camera to estimate source locations. An active source localization framework is then developed that greedily selects new waypoints that maximize the Fisher Information provided by the camera's range and bearing observations for source localization. Finally the common assumption of a static step size in between waypoints is relaxed to allow step sizes to adapt online to the observed information. The proposed radiation mapping approach is evaluated in 5×4 m2 and 14×6 m2 laboratory environments, where multiple point sources were localized to within an average of 0.26 m or 0.6% of the environment dimensions. The active source localization approach is evaluated in simulation and an adaptive step size yields a 27% decrease in the localization time and a 16% decrease in the distance traveled to localize a source in a 15×15×15 m3 environment.


Title: Design of an Autonomous Robot for Mapping, Navigation, and Manipulation in Underground Mines
Key Words: cameras  inertial systems  manipulators  mining  mobile robots  robot vision  sensors  autonomous driving  autonomous robot  manipulation  underground mines  dangerous working environment  harsh environment  robot design  underground objects  manipulating objects  robotic arm  robust four wheeled platform  depth cameras  inertial measurement unit  autonomous navigation  Robot sensing systems  Manipulators  Cameras  Navigation  Mobile robots 
Abstract: Underground mines are a dangerous working environment and, therefore, robots could help putting less humans at risk. Traditional robots, sensors, and software often do not work reliably underground due to the harsh environment. This paper analyzes requirements and presents a robot design capable of navigating autonomously underground and manipulating objects with a robotic arm. The robot's base is a robust four wheeled platform powered by electric motors and able to withstand the harsh environment. It is equipped with color and depth cameras, lighting, laser scanners, an inertial measurement unit, and a robotic arm. We conducted two experiments testing mapping and autonomous navigation. Mapping a 75 meters long route including a loop closure results in a map that qualitatively matches the original map to a good extent. Testing autonomous driving on a previously created map of a second, straight, 150 meters long route was also successful. However, without loop closure, rotation errors cause apparent deviations in the created map. These first experiments showed the robot's operability underground.


Title: Deep Q-Learning for Dry Stacking Irregular Objects
Key Words: learning (artificial intelligence)  manipulators  motion control  neurocontrollers  state-action pairs  Q-network  robot arm  generated stacking plans  physical constraints  geometric constraints  action space  deep neural network  learned Q-function  reinforcement learning algorithm  local geometric considerations  reinforcement learning approach  dry stacking irregular objects  deep Q-learning  Stacking  Buildings  Robots  Planning  Shape  Stability analysis  Reinforcement learning 
Abstract: We propose a reinforcement learning approach for automatically building dry stacked (i.e. no mortar) structures with irregular objects. Stacking irregular objects is a challenging problem since each assembly action can be drawn from a continuous space of poses for an object, and several local geometric and physical considerations strongly affect the stability. To tackle this challenge, we concentrate on a simplified 2D version of the problem. We present a reinforcement learning algorithm based on deep Q-learning, where the learned Q-function, which maps state-action pairs into expected long-term rewards, is represented by a deep neural network. As the action space is continuous the Q-network is trained by sampling a finite number of actions that consider both geometric and physical constraints to approximate the target Q-values, Experiments show that the proposed method outperforms previous heuristics-based planning, leading to super construction with objects containing a significant amount of variations. We validate the generated stacking plans by executing them using a robot arm and manufactured, irregular objects.


Title: Indoor Mapping and Localization for Pedestrians using Opportunistic Sensing with Smartphones
Key Words: Bayes methods  Gaussian processes  indoor radio  mobile computing  mobile robots  optimisation  particle filtering (numerical methods)  path planning  radionavigation  regression analysis  SLAM (robots)  smart phones  wireless LAN  Gaussian Processes Regression  real-time localization  GPR variance map  pseudowall constraints  magnetic fields  globally consistent trajectories  opportunistic magnetic headings  WiFi signal similarity validation  magnetic sequence matching  loop-closure constraints  pedestrian dead-reckoning  motion constraints  GraphSLAM front-end  signal maps  Bayesian filtering-based online localization  GraphSLAM-based offline mapping  ambient indoor environments  low-cost indoor mapping  indoor localization  smartphone  size 2.3 m  size 3.41 m  Wireless fidelity  Trajectory  Smart phones  Simultaneous localization and mapping  Ground penetrating radar  Legged locomotion 
Abstract: Indoor localization for pedestrians has gained increasing popularity among the rich body of literature for the last decade. In this paper, a low-cost indoor mapping and localization solution is proposed using the opportunistic signals from ambient indoor environments with a smartphone. It is composed of GraphSLAM-based offline mapping and Bayesian filtering-based online localization using generated signal maps. The GraphSLAM front-end is constructed by motion constraints from pedestrian dead-reckoning (PDR), loop-closure constraints identified by magnetic sequence matching with WiFi signal similarity validation, and observation constraints from opportunistic magnetic headings after error rejection. Globally consistent trajectories are created by graph optimization, after which signal maps (e.g., WiFi, magnetic fields, lights) are generated by Gaussian Processes Regression (GPR) for later localization. We propose to use the pseudo-wall constraints from the GPR variance map of magnetic fields and the lights measurements as observations for particle filtering. The proposed method is evaluated on several datasets collected from both the in-compass office buildings and outside public areas. Real-time localization is demonstrated on a smartphone in an office building covering 2000 square meters with the 50- and 90-percentile accuracies being 2.30 m and 3.41 m, respectively.


Title: Navigation without localisation: reliable teach and repeat based on the convergence theorem
Key Words: calibration  cameras  mobile robots  navigation  path planning  robot vision  velocity control  mobile robot  taught path  learned velocities  camera information  position error model  mathematical proof  camera calibration  navigation system  mathematical model  explicit localisation  teach-and-repeat navigation scenarios  teach-and-repeat visual navigation  Robot kinematics  Navigation  Cameras  Robot vision systems  Simultaneous localization and mapping  Feature extraction 
Abstract: We present a novel concept for teach-and-repeat visual navigation. The proposed concept is based on a mathematical model, which indicates that in teach-and-repeat navigation scenarios, mobile robots do not need to perform explicit localisation. Rather than that, a mobile robot which repeats a previously taught path can simply “replay” the learned velocities, while using its camera information only to correct its heading relative to the intended path. To support our claim, we establish a position error model of a robot, which traverses a taught path by only correcting its heading. Then, we outline a mathematical proof which shows that this position error does not diverge over time. Based on the insights from the model, we present a simple monocular teach-and-repeat navigation method. The method is computationally efficient, it does not require camera calibration, and it can learn and autonomously traverse arbitrarily-shaped paths. In a series of experiments, we demonstrate that the method can reliably guide mobile robots in realistic indoor and outdoor conditions, and can cope with imperfect odometry, landmark deficiency, illumination variations and naturally-occurring environment changes. Furthermore, we provide the navigation system and the datasets gathered at www.github.com/gestom/stroll_bearnav.


Title: Accurate Mix-Norm-Based Scan Matching
Key Words: expectation-maximisation algorithm  image matching  learning (artificial intelligence)  least squares approximations  mobile robots  optimisation  pose estimation  exponential power distributions  convergence characteristic  mix-norm-based scan matching  robust objective function design  MoEP-based residual error model  EM-like algorithm  likelihood field model  iteratively reweighted least squares phase  LFM  IRLS  on-line parameter learning  MiNoM optimization  mobile robotics  Iterative closest point algorithm  Linear programming  Gaussian distribution  Standards  Convergence  Optimization  Heuristic algorithms 
Abstract: Highly accurate mapping and localization is of prime importance for mobile robotics, and its core lies in efficient scan matching. Previous research are focusing on designing a robust objective function and the residual error distribution is often ignored or simply assumed as unitary or mixture of simple distributions. In this paper, a mixture of exponential power (MoEP) distributions is proposed to approximate the residual error distribution. The objective function induced by MoEP-based residual error modelling ensembles a mix-norm-based scan matching (MiNoM), which enhances the matching accuracy and convergence characteristic. Both the parameters of transformation (rotation and translation) and residual error distribution are estimated efficiently via an EM-like algorithm. The optimization of MiNoM is iteratively achieved via two phases: An on-line parameter learning (OPL) phase to learn residual error distribution for better representation according to the likelihood field model (LFM), and an iteratively reweighted least squares (IRLS) phase to attain transformation for accuracy and efficiency. Extensive experimental results validate that the proposed MiNoM out-performs several state-of-the-art scan matching algorithms in both convergence characteristic and matching accuracy.


Title: StreetMap - Mapping and Localization on Ground Planes using a Downward Facing Camera
Key Words: cameras  feature extraction  image filtering  image texture  mobile robots  robot vision  rectilinear textures  indoor tiling  ground plane texture  globally consistent map  complete working pipeline  absolute localization  indoor tiles  general texture  ground textures  mobile robot  downward facing camera  Cameras  Robot vision systems  Feature extraction  Robot kinematics  Slabs 
Abstract: This paper describes a system to map a ground-plane, and to subsequently use the map for localization of a mobile robot. The robot has a downward-facing camera, and works on a variety of ground textures including general texture like tarmac, man-made designs like carpet, and rectilinear textures like indoor tiles or outdoor slabs. Such textures provide a basis for measuring relative motion (i.e. computer mouse functionality). But the goal here is the more challenging one of absolute localization. The paper describes a complete working pipeline to build a globally consistent map of a given ground-plane and subsequently to localize within this map at real-time. Two algorithms are described. The first is a feature-based approach which is general to any ground plane texture. The second algorithm takes advantage of the extra constraints available for common rectilinear textures like indoor tiling, paving slabs, and laid brickwork. Quantitative and qualitative experimental results are shown for mapping and localization on a variety of ground-planes.


Title: The TUM VI Benchmark for Evaluating Visual-Inertial Odometry
Key Words: augmented reality  calibration  cameras  distance measurement  image capture  image sensors  image sequences  mobile robots  optical tracking  pose estimation  robot vision  SLAM (robots)  synchronisation  visual-inertial odometry  photometric calibration  motion capture system  IMU measurements  pose ground truth  inertial measurements  vision sensors  augmented reality  SLAM methods  visual odometry  IMU sensors  camera images  TUM VI benchmark  frequency 20.0 Hz  frequency 200.0 Hz  frequency 120.0 Hz  Cameras  Calibration  Simultaneous localization and mapping  Benchmark testing  Visual odometry  Time measurement 
Abstract: Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20 Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120 Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data is publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.


Title: Scale-Robust Localization Using General Object Landmarks
Key Words: distance measurement  feature extraction  learning (artificial intelligence)  mobile robots  object detection  robot vision  SLAM (robots)  deep-learning-based object features  KITTI Odometry benchmark  outdoor images  scale-robust localization  visual localization  robotic mapping applications  object landmarks  SIFT point-features  Visualization  Measurement  Simultaneous localization and mapping  Robustness  Databases  Search problems 
Abstract: Visual localization under large changes in scale is an important capability in many robotic mapping applications, such as localizing at low altitudes in maps built at high altitudes, or performing loop closure over long distances. Existing approaches, however, are robust only up to about a 3× difference in scale between map and query images. We propose a novel combination of deep-learning-based object features and state-of-the-art SIFT point-features that yields improved robustness to scale change. This technique is training-free and class-agnostic, and in principle can be deployed in any environment out-of-the-box. We evaluate the proposed technique on the KITTI Odometry benchmark and on a novel dataset of outdoor images exhibiting changes in visual scale of 7× and greater, which we have released to the public. Our technique consistently outperforms localization using either SIFT features or the proposed object features alone, achieving both greater accuracy and much lower failure rates under large changes in scale.


Title: Invariant smoothing on Lie Groups
Key Words: estimation theory  Kalman filters  Lie groups  linearisation techniques  optimisation  robot vision  SLAM (robots)  smoothing methods  linearizations  invariant Kalman filtering  robot localization  posteriori estimator  nonlinear smoothing methods  group-affine observation systems  Lie groups  invariant smoothing  Smoothing methods  Manifolds  Simultaneous localization and mapping  Kalman filters  Random variables  Estimation  Robot localization 
Abstract: In this paper we propose a (non-linear) smoothing algorithm for group-affine observation systems, a recently introduced class of estimation problems on Lie groups that bear a particular structure. As most non-linear smoothing methods, the proposed algorithm is based on a maximum a posteriori estimator, determined by optimization. But owing to the specific properties of the considered class of problems, the involved linearizations are proved to have a form of independence with respect to the current estimates, leveraged to avoid (partially or sometimes totally) the need to relinearize. The method is validated on a robot localization example, both in simulations and on real experimental data.


Title: A Combined RGB and Depth Descriptor for SLAM with Humanoids
Key Words: cameras  feature extraction  humanoid robots  image colour analysis  mobile robots  pose estimation  robot vision  SLAM (robots)  feature tracking  codebooks  reproducibility  humanoid robots  visual simultaneous localization  depth descriptor  ORB-SLAM  visual SLAM system  track features  DLab  RGB-D camera  Nao humanoid  binary descriptor  FAB-MAP  place recognition module  Simultaneous localization and mapping  Image color analysis  Cameras  Three-dimensional displays  Humanoid robots  Visualization 
Abstract: In this paper, we present a visual simultaneous localization and mapping (SLAM) system for humanoid robots. We introduce a new binary descriptor called DLab that exploits the combined information of color, depth, and intensity to achieve robustness with respect to uniqueness, reproducibility, and stability. We use DLab within ORB-SLAM, where we replaced the place recognition module with a modification of FAB-MAP that works with newly built codebooks using our binary descriptor. In experiments carried out in simulation and with a real Nao humanoid equipped with an RGB-D camera, we show that DLab has a superior performance in comparison to other descriptors. The application to feature tracking and place recognition reveal that the new descriptor is able to reliably track features even in sequences with seriously blurred images and that it has a higher percentage of correctly identified similar images. As a result, our new visual SLAM system has a lower absolute trajectory error in comparison to ORB-SLAM and is able to accurately track the robot's trajectory.


Title: Neural-Network-Controlled Spring Mass Template for Humanoid Running
Key Words: humanoid robots  interpolation  legged locomotion  neurocontrollers  pendulums  robot dynamics  springs (mechanical)  table lookup  lookup tables  data-driven approach  deep neural network  simulation data  SLIP model  humanoid robot  whole-body model  QP-based inverse dynamics controller  WALK-MAN robot  robust running motions  neural-network-controlled spring mass template  humanoid running  legged robots  model-based approaches  whole-body robot  controlled SLIP-like behaviors  online incompatibility  interpolations  spring-loaded inverted pendulum model  Legged locomotion  Neural networks  Data models  Training  Biological system modeling  Robot kinematics 
Abstract: To generate dynamic motions such as hopping and running on legged robots, model-based approaches are usually used to embed the well studied spring-loaded inverted pendulum (SLIP) model into the whole-body robot. In producing controlled SLIP-like behaviors, existing methods either suffer from online incompatibility or resort to classical interpolations based on lookup tables. Alternatively, this paper presents the application of a data-driven approach which obviates the need for solving the inverse of the running return map online. Specifically, a deep neural network is trained offline with a large amount of simulation data based on the SLIP model to learn its dynamics. The trained network is applied online to generate reference foot placements for the humanoid robot. The references are then mapped to the whole-body model through a QP-based inverse dynamics controller. Simulation experiments on the WALK-MAN robot are conducted to evaluate the effectiveness of the proposed approach in generating bio-inspired and robust running motions.


Title: Quadruped Locomotion Control Based on Two Bipeds Jointly Carrying Model
Key Words: gait analysis  legged locomotion  motion control  robot dynamics  quadruped locomotion control  novel gait planning  control framework  quadruped robot  rear ends  joint torques  support legs  bipedal sub-robots  quadruped body forces  bipedal torso forces  operating modes  virtual forces  support leg torques  gait generators  gait parameters  hind legs  Legged locomotion  Robot kinematics  Torso  Radio frequency  Hip  Mathematical model 
Abstract: A novel gait planning and control framework was developed for quadruped locomotion of a robot. It modeled the quadruped robot as two bipeds carrying the body from the front and rear ends. We first mapped the relationship between the joint torques of support legs and the torso forces of the bipedal sub-robots. Then the equations describing the relationship between the quadruped body forces and the bipedal torso forces under various operating modes of the robot were deduced and solved. Virtual forces were generated on the quadruped body to manipulate its velocity and orientation. Then these virtual forces were distributed to the front and hind sub-robots to generate support leg torques. The state machines and gait generators for the two bipedal sub-robots were designed individually, resulting in the decoupling of the gait parameters in the front legs and hind legs. The effectiveness of the controller was validated through dynamic simulations.


Title: An Investigation of 2nd-Order Fixed Point SLIP Behavior
Key Words: approximation theory  collision avoidance  friction  legged locomotion  nonlinear control systems  pendulums  1st-order fixed points  2nd-order fixed points  2nd-order fixed point SLIP behavior  analytical stance phase approximation  friction cone constraints  obstacle avoidance  SLIP model behavior analysis  nondimensional leg stiffness  numerical return map search scheme  nondimensional SLIP model  Legged locomotion  Springs  Friction  Mathematical model  Numerical models  Trajectory 
Abstract: This paper introduces alternative behaviors described by the SLIP model when it is subject to a range of initial conditions. A non-dimensional SLIP model and a numerical return map search scheme are used to determine fixed points as a function of non-dimensional leg stiffness and vertical displacement under friction constraints. A SLIP model behavior analysis is performed, using an analytical stance phase approximation, by diverging from the fixed points, i.e. by increasing/decreasing initial horizontal velocity, and/or touchdown angle. The results show that beyond the regular fixed points, the SLIP model performs an alternative, stable behavior that repeats itself every two cycles of motion. We call these 2nd-order fixed points and the regular ones 1st-order fixed points. A numerical simulation scheme was developed to investigate 2nd-order fixed points for a wide range of horizontal velocities and touchdown angles. Results show that 2nd-order fixed points respecting the friction cone constraints exist that can lead to a number of different behaviors such as high jumps, obstacle avoidance of different heights, or backward motion.


Title: Cost of Transport Estimation for Legged Robot Based on Terrain Features Inference from Aerial Scan
Key Words: feature extraction  inference mechanisms  learning (artificial intelligence)  legged locomotion  motion control  path planning  robot vision  terrain mapping  multilegged robot  crawled terrain  hexapod robot  legged robot  terrain features inference  aerial scan  robot locomotion  incremental learning  geometrical data  visual data  terrain learning  extraterrestrial missions  robot deployment  robot motion planning  cost of transport estimation  terrain descriptors  mechanical properties  Robots  Feature extraction  Image color analysis  Estimation  Unmanned aerial vehicles  Three-dimensional displays  Visualization 
Abstract: The effectiveness of the robot locomotion can be measured using the cost of transport (CoT) which represents the amount of energy that is needed for traversing from one place to another. Terrains excerpt different mechanical properties when crawled by a multi-legged robot, and thus different values of the CoT. It is therefore desirable to estimate the CoT in advance and plan the robot motion accordingly. However, the CoT might not be known prior the robot deployment, e.g., in extraterrestrial missions; hence, a robot has to learn different terrains as it crawls through the environment incrementally. In this work, we focus on estimating the CoT from visual and geometrical data of the crawled terrain. A thorough analysis of different terrain descriptors within the context of incremental learning is presented to select the best performing approach. We report on the achieved results and experimental verification of the selected approaches with a real hexapod robot crawling over six different terrains.


Title: DNN-based Speech Recognition System dealing with Motor State as Auxiliary Information of DNN for Head Shaking Robot
Key Words: acoustic signal processing  neural nets  robots  signal denoising  speech processing  speech recognition  motor state  head shaking robot  deep neural network  acoustic modeling  speech recognition algorithm  feature mapping model  acoustic model  phoneme recognition  feature enhancement model  speech recognition system  auxiliary information  DNN  background noise suppression  Robots  Speech recognition  Speech enhancement  Noise measurement  Feature extraction  Mel frequency cepstral coefficient 
Abstract: In this paper, a deep neural network (DNN) based integrated background noise suppression and acoustic modeling for speech recognition proposed in which on/off state of the motor for the head shaking robot is employed as the relevant auxiliary information of the DNN input. Since the motor sound being generated when the robot is moving or shaking its head severely degrades the performance of the speech recognition accuracy, we propose to use the motor on/off state as additional information when designing the DNN-based recognition system. Our speech recognition algorithm consists of two parts including the feature mapping model for feature enhancement and the acoustic model for phoneme recognition. As for the feature mapping, the stacked DNN is designed for the precise feature enhancement such that the lower DNN and upper DNN are trained separately and combined after which the motor state is plugged into both the lower DNN and upper DNN in addition to the input noisy speech. Then, the acoustic model is trained upon the feature enhancement model in which the motor state is again used as the augmented feature. The proposed technique to suppress the acoustic and motor noises was evaluated in term of the phoneme error rate (PER) and showed a significant improvement over the conventional system.


Title: Multibeam Data Processing for Underwater Mapping
Key Words: image segmentation  oceanographic techniques  sonar  sonar detection  sonar imaging  underwater vehicles  balanced trade-off  underwater mapping literature  underwater mapping literature  local thresholding techniques  subsea structures  multibeam data processing  DIDSON imaging sonar  map accuracy  sonar-based underwater mapping  sonar artifacts  range measurements  occupied regions  free regions  received acoustic echos  sonars output  underwater mapping platforms  primary sensor  multibeam sonars  Sonar measurements  Robot sensing systems  Acoustic beams  Image segmentation  Mathematical model  Acoustics 
Abstract: From archaeology to the inspection of subsea structures, underwater mapping has become critical to many applications. Because of the balanced trade-off between range and resolution, multibeam sonars are often used as the primary sensor in underwater mapping platforms. These sonars output an image representing the intensity of the received acoustic echos over space, which must be classified into free and occupied regions before range measurements are determined and spatially registered. Most classifiers found in the underwater mapping literature use local thresholding techniques, which are highly sensitive to noise, outliers, and sonar artifacts typically found in these images. In this paper we present an overview of some of the techniques developed in the scope of our work on sonar-based underwater mapping, with the aim of improving map accuracy through better segmentation performance. We also provide experimental results using data collected with a DIDSON imaging sonar that show that these techniques improve both segmentation accuracy and robustness to outliers.


Title: A Deformable Spiral Based Algorithm to Smooth Coverage Path Planning for Marine Growth Removal
Key Words: autonomous underwater vehicles  bridges (structures)  inspection  multi-robot systems  path planning  underwater structures  DSCPP  smooth paths  spiral path  popular boustrophedon-based coverage approach  intervention autonomous underwater vehicle  deformable spiral coverage path planning algorithm  smooth coverage path planning  deformable spiral-based algorithm  Spirals  Cleaning  Path planning  Fatigue  Underwater structures  Manipulators  Poles and towers 
Abstract: Marine growths that flourish on the surfaces of underwater structures, such as bridge pylons, make the inspection and maintenance of these structures challenging. A robotic solution, using an Intervention Autonomous Underwater Vehicle (I-AUV), is developed for removing marine growth. This paper presents a Deformable Spiral Coverage Path Planning (DSCPP) algorithm for marine growth removal. DSCPP generates smooth paths to prevent damage to the surfaces of the structures and to avoid frequent or aggressive decelerations and accelerations due to sharp turns. DSCPP generates a spiral path within a circle and analytically maps the path to a minimum bounding rectangle which encompasses an area of a surface with marine growth. It aims to achieve a spiral path with minimal length while preventing missed areas of coverage. Several case studies are presented to validate the algorithm. Comparison results show that DSCPP outperforms the popular boustrophedon-based coverage approach when considering the requirements for the application under consideration.


Title: Emotional Bodily Expressions for Culturally Competent Robots through Long Term Human-Robot Interaction
Key Words: emotion recognition  human-robot interaction  learning (artificial intelligence)  multiculture society  incremental learning model  habitual emotional behaviors  social robot  emotional bodily expressions  imitated robot motions  cultural background  culturally competent robots  long term human-robot interaction  Robot kinematics  Neurons  Self-organizing feature maps  Trajectory  Training  Collision avoidance 
Abstract: Generating emotional bodily expressions for culturally competent robots has been gaining increased attention to enhance the engagement and empathy between robots and humans in a multi-culture society. In this paper, we propose an incremental learning model for selecting the user's representative or habitual emotional behaviors which place emphasis on individual users' cultural traits identified through long term interaction. Furthermore, a transformation model is proposed to convert the obtained emotional behaviors into a specific robot's motion space. To validate the proposed approach, the models were evaluated by two example scenarios of interaction. The experimental results confirmed that the proposed approach endows a social robot with the capability to learn emotional behaviors from individual users, and to generate its emotional bodily expressions. It was also verified that the imitated robot motions are rated emotionally acceptable by the demonstrator and recognizable by the subjects from the same cultural background with the demonstrator.


Title: Pose Estimation and Map Formation with Spiking Neural Networks: towards Neuromorphic SLAM
Key Words: mixed analogue-digital integrated circuits  mobile robots  neural nets  neurophysiology  pose estimation  SLAM (robots)  pose estimation  spiking neural networks  neuromorphic SLAM  biologically inspired neuronal path integration  mobile robot  neuronal map formation architecture  simultaneous localization and mapping  mixed signal analog-digital neuromorphic hardware  ultra low-power neuromorphic hardware  robotic vehicle simulation  on-board plasticity  Neurons  Neuromorphics  Collision avoidance  Simultaneous localization and mapping  Synapses 
Abstract: In this paper, we investigate the use of ultra low-power, mixed signal analog/digital neuromorphic hardware for implementation of biologically inspired neuronal path integration and map formation for a mobile robot. We perform spiking network simulations of the developed architecture, interfaced to a simulated robotic vehicle. We then port the neuronal map formation architecture on two connected neuromorphic devices, one of which features on-board plasticity, and demonstrate the feasibility of a neuromorphic realization of simultaneous localization and mapping (SLAM).


Title: Precise Localization in High-Definition Road Maps for Urban Regions
Key Words: cameras  image resolution  Kalman filters  nonlinear filters  road vehicles  satellite navigation  stereo image processing  traffic engineering computing  high-resolution road maps  road borders  Unscented Kalman Filter  narrow urban roads  highly automated driving  precise localization  high-definition road maps  sensor specific feature layers  stereo camera  vehicle odometry  low-cost GNSS module  size 5.0 km  size 0.08 m  Roads  Global navigation satellite system  Simultaneous localization and mapping  Semantics  Urban areas  Receivers 
Abstract: The future of automated driving in urban areas will most probably rely on highly accurate road maps. However, the necessary precision of a localization in such maps has so far only been reached using extra, sensor specific feature layers for localization. In this paper we want to show that it is possible to achieve sufficient accuracy without a separate localization layer. Instead, elements are used that are already contained in high-resolution road maps, such as markings and road borders. For this, we introduce a modular approach in which detections from different detection algorithms are associated with elements in the map and then fused to an absolute pose using an Unscented Kalman Filter. We evaluate our approach using a sensor setup that employs a stereo camera, vehicle odometry and a low-cost GNSS module on a 5km test route covering both narrow urban roads and multi-lane main roads under varying weather conditions. The results show that this approach is capable to be used for highly automated driving, showing an accuracy of 0.08m in typical road scenarios and a is available 98% of the time.


Title: Virtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments
Key Words: graph theory  image reconstruction  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  3D scene reconstructions  virtual occupancy grid map  mobile robots  VOG-map  submap-based pose graph SLAM  underwater SLAM system  path planning  free space information  Simultaneous localization and mapping  Three-dimensional displays  Path planning  Robot kinematics  Casting 
Abstract: In this paper, we propose a mapping approach that constructs a globally deformable virtual occupancy grid map (VOG-map) based on local submaps. Such a representation allows pose graph SLAM systems to correct globally accumulated drift via loop closures while maintaining free space information for the purpose of path planning. We demonstrate use of such a representation for implementing an underwater SLAM system in which the robot actively plans paths to generate accurate 3D scene reconstructions. We evaluate performance on simulated as well as real-world experiments. Our work furthers capabilities of mobile robots actively mapping and exploring unstructured, three dimensional environments.


Title: Decentralized Localization Framework using Heterogeneous Map-matchings
Key Words: decentralised control  mobile robots  road vehicles  sensor fusion  stability  stochastic processes  decentralized localization framework  heterogeneous map-matchings  system stability  localization methods  map matchings  stochastic situational analysis model  heterogeneous map-matching sources  dissimilar sensors  fusion methods  multienvironment sensors  single environmental sensor  autonomous driving applications  robust real-time localization  Roads  Laser radar  Three-dimensional displays  Cameras  Feature extraction  Sensor fusion 
Abstract: Highly accurate and robust real-time localization is an essential technique for various autonomous driving applications. Numerous localization methods have been proposed that combine various types of sensors, including an environmental sensor, IMU and GPS. However, the usage of a single environmental sensor is rather fragile. Although the use of multi-environment sensors is a better alternative, fusion methods from previous studies have not adequately compensated for shortcomings in dissimilar sensors or have not considered errors in the pre-built map. In this paper, we propose a decentralized localization framework using heterogeneous map-matching sources. Decentralized localization performs two independent map-matchings and integrates them with a stochastic situational analysis model. By applying a stochastic model, the reliability of the two map matchings is collected and system stability is verified. A number of experiments with autonomous vehicles within the actual driving environment have shown that combining multiple map-matching sources ensures more robust results than the use of a single environmental sensor.


Title: LDSO: Direct Sparse Odometry with Loop Closure
Key Words: feature extraction  graph theory  mobile robots  optimisation  pose estimation  robot vision  SLAM (robots)  intensity gradient  DSO sliding window optimization  Sim(3) relative pose constraints  image pixel  loop closure detection  monocular visual SLAM system  Direct Sparse Odometry  state-of-the-art feature-based systems  pose-graph optimization  modified point selection strategy  relative poses  co-visibility graph  3D geometric error terms  conventional feature-based bag-of-words approach  loop closure candidates  tracking frontend  corner features  LDSO  featureless areas  Optimization  Feature extraction  Microsoft Windows  Simultaneous localization and mapping  Cameras  Bundle adjustment  Robustness 
Abstract: In this paper we present an extension of Direct Sparse Odometry (DSO) [1] to a monocular visual SLAM system with loop closure detection and pose-graph optimization (LDSO). As a direct technique, DSO can utilize any image pixel with sufficient intensity gradient, which makes it robust even in featureless areas. LDSO retains this robustness, while at the same time ensuring repeatability of some of these points by favoring corner features in the tracking frontend. This repeatability allows to reliably detect loop closure candidates with a conventional feature-based bag-of-words (BoW) approach. Loop closure candidates are verified geometrically and Sim(3) relative pose constraints are estimated by jointly minimizing 2D and 3D geometric error terms. These constraints are fused with a co-visibility graph of relative poses extracted from DSO's sliding window optimization. Our evaluation on publicly available datasets demonstrates that the modified point selection strategy retains the tracking accuracy and robustness, and the integrated pose-graph optimization significantly reduces the accumulated rotation-, translation- and scale-drift, resulting in an overall performance comparable to state-of-the-art feature-based systems, even without global bundle adjustment.


Title: Data-Driven Discrete Planning for Targeted Hopping of Compliantly Actuated Robotic Legs
Key Words: actuators  elasticity  legged locomotion  mobile robots  motion control  path planning  robot dynamics  planar hopping leg prototype validate  hopping trials  data-driven manner  serial elastic actuation  planar leg  discrete-time planning problem  simple controller structure  time-continuous trajectories  considerable real-time problems  fast locomotion  motion planning  compliantly actuated robotic legs  targeted hopping  data-driven discrete planning  Legged locomotion  Planning  Springs  Switches  Hardware 
Abstract: Motion planning for fast locomotion of compliantly actuated robotic legs is generally considered to be a challenging issue, posing considerable real-time problems. This is at least the case if time-continuous trajectories need to be generated online. In this paper we take advantage of a simple controller structure, which reduces the motion planning to a discrete-time planning problem, in which only a small set of input parameters need to be determined for each step. We show that for a planar leg with serial elastic actuation, hopping on a ground with stairs of irregular length and height can be planned online, based on a parameter mapping which has been learned in a data-driven manner by performing hopping trials with an adaptive exploration algorithm to evenly sample the parameter space. Experiments on a planar hopping leg prototype validate the approach.


Title: A Synergetic Voluntary Control for Exoskeleton based on Spinal Cord Mapping of Peripheral Bioelectric Activity
Key Words: bioelectric phenomena  electromyography  gait analysis  injuries  matrix decomposition  medical robotics  neurophysiology  patient rehabilitation  synergetic voluntary control  spinal cord mapping  peripheral bioelectric activity  voluntary motion intention  control method  exoskeleton robot control  voluntary lower limb muscle activities  spinal cord injury  muscle synergy  walking motion  spinal cord map level  reliable cord levels  unreliable spinal cord levels  maximally voluntary locomotion  whole-body muscle activity  intended lower limb muscle activity  spinal cord activity  walking rehabilitation  nonnegative matrix factorization  transformation matrix  hybrid assistive limb  walking experiments  Muscles  Legged locomotion  Spinal cord  Exoskeletons  Robot kinematics  Estimation 
Abstract: Walking rehabilitation must be performed based on voluntary motion intention, and for this purpose, the development of a control method for an exoskeleton robot based on voluntary intention is investigated. This study proposes a method of exoskeleton robot control to estimate the voluntary lower limb muscle activities lost after a spinal cord injury (SCI). This method is based on the spinal cord mapping of the remaining muscle activities and its matching to the one obtained from healthy participants considering the muscle synergy of the whole body during the walking motion. By implementing the matching procedure at the spinal cord map level and incorporating information of reliable and unreliable spinal cord levels based on a diagnosis, the method has the potential to provide a maximally voluntary locomotion for people with SCI. We report an analysis of the synergy of the whole-body muscle activity during walking and its spinal cord mapping using non-negative matrix factorization and the computation of the transformation matrix to estimate the intended lower limb muscle activity from the remaining spinal cord activity. The implementation of the proposed method using the right leg of the hybrid assistive limb and walking experiments with a healthy participant are also reported.


Title: Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots
Key Words: human-robot interaction  learning (artificial intelligence)  mobile robots  neural nets  object detection  object recognition  service robots  robust object recognition  symbiotic deep learning  mobile service robot  human environments  symbiotic autonomy approach  HHELP  RGB camera  onboard tablet  object detection  deep neural network  domestic environment  YOLOv2 neural network  bootstrap YOLOv2  CMU  Monarch Mbot  ISR-Lisbon  Neural networks  Labeling  Training  Symbiosis  Service robots  Object recognition 
Abstract: Despite the recent success of state-of-the-art deep learning algorithms in object recognition, when these are deployed as-is on a mobile service robot, we observed that they failed to recognize many objects in real human environments. In this paper, we introduce a learning algorithm in which robots address this flaw by asking humans for help, also known as a symbiotic autonomy approach. In particular, we bootstrap YOLOv2, a state-of-the-art deep neural network and train a new neural network, that we call HHELP, using only data collected from human help. Using an RGB camera and an onboard tablet, the robot proactively seeks human input to assist it in labeling surrounding objects. Pepper, located at CMU, and Monarch Mbot, located at ISR-Lisbon, were the service robots that we used to validate the proposed approach. We conducted a study in a realistic domestic environment over the course of 20 days with 6 research participants. To improve object detection, we used the two neural networks, YOLOv2 + HHELP, in parallel. Following this methodology, the robot was able to detect twice the number of objects compared to the initial YOLOv2 neural network, and achieved a higher mAP (mean Average Precision) score. Using the learning algorithm the robot also collected data about where an object was located and to whom it belonged to by asking humans. This enabled us to explore a future use case where robots can search for a specific person's object. We view the contribution of this work to be relevant for service robots in general, in addition to Pepper, and Mbot.


Title: People as Sensors: Imputing Maps from Human Actions
Key Words: collision avoidance  driver information systems  mobile robots  pedestrians  road vehicles  human actions  autonomous vehicles  pedestrian detection  collision avoidance  map estimation  human driving experiments  landmark-based mapping approaches  agents actions  Random variables  Estimation  Automobiles  Computational modeling  Intelligent sensors 
Abstract: Despite growing attention in autonomy, there are still many open problems, including how autonomous vehicles will interact and communicate with other agents, such as human drivers and pedestrians. Unlike most approaches that focus on pedestrian detection and planning for collision avoidance, this paper considers modeling the interaction between human drivers and pedestrians and how it might influence map estimation, as a proxy for detection. We take a mapping inspired approach and incorporate people as sensors into mapping frameworks. By taking advantage of other agents' actions, we demonstrate how we can impute portions of the map that would otherwise be occluded. We evaluate our framework in human driving experiments and on real-world data, using occupancy grids and landmark-based mapping approaches. Our approach significantly improves overall environment awareness and outperforms standard mapping techniques.


Title: Uncertainty-based Online Mapping and Motion Planning for Marine Robotics Guidance
Key Words: autonomous underwater vehicles  path planning  probability  robot dynamics  vehicle dynamics  uncertainty-based framework  online computation constraints  motion planning  marine robotics guidance  robotic systems  safe path  underwater environments  autonomous vehicles  probabilistic safety  online mapping  Uncertainty  Safety  Planning  Probabilistic logic  Robot sensing systems  Vehicle dynamics 
Abstract: In real-world robotics, motion planning remains to be an open challenge. Not only robotic systems are required to move through unexplored environments, but also their manoeuvrability is constrained by their dynamics and often suffer from uncertainty. One approach to overcome this problem is to incrementally map the surroundings while, simultaneously, planning a safe and feasible path to a desired goal. This is especially critical in underwater environments, where autonomous vehicles must deal with both motion and environment uncertainties. In order to cope with these constraints, this work proposes an uncertainty-based framework for mapping and planning3 feasible motions online with probabilistic safety-guarantees. The proposed approach deals with the motion, probabilistic safety, and online computation constraints by (i) incrementally representing the environment as a collection of local maps, and (ii) iteratively (re)planning kinodynamically-feasible and probabilistically-safe paths to goal. The proposed framework is evaluated on the Sparus II, a nonholonomic torpedo-shaped AUV, by conducting simulated and real-world trials, thus proving the efficacy of the method and its suitability even for systems with limited on-board computational power.


Title: MAP - A Mobile Agile Printer Robot for on-site Construction
Key Words: construction  legged locomotion  mobile robots  robot kinematics  service robots  three-dimensional printing  wheels  outdoors construction site  3D printing large structures  omnidirectional robot capable  Mobile Agile Printer construction robot  on-site construction  concurrent on-site operations  low 3D printing trajectory deviations  construction robots  mobile platform  high-DoF 3D printing system  MAP  Three-dimensional printing  Wheels  Legged locomotion  Printers 
Abstract: In this paper, we present a Mobile Agile Printer (MAP) construction robot; a highly agile, 4-legged, omnidirectional robot capable of 3D printing large structures. To overcome dynamic challenges when operating within an outdoors construction site, MAP incorporates a high-DoF 3D printing system connected to a mobile platform with novel features designed to enable disturbance rejection and live adaption to the robot's pose. In doing so, we demonstrate the benefits of designing construction robots with a focus on agility, a compact working volume and ability to operate within a potentially unlimited workspace. Performance tests were conducted showing smooth omni-directional motion as a key requirement for maintaining low 3D printing trajectory deviations over a large volume. In doing so, we show that MAP has the ability to construct in new ways more sensitive to its environment, context and concurrent on-site operations.


Title: Kinematic Morphing Networks for Manipulation Skill Transfer
Key Words: affine transforms  image morphing  iterative methods  manipulators  motion control  neural nets  robot vision  kinematic model  robot motions  robot skill  manipulation skill transfer  kinematic morphing networks  affine transformations  map depth image observations  deep neural network  Kinematics  Three-dimensional displays  Robot sensing systems  Prototypes  Neural networks  Solid modeling 
Abstract: The transfer of a robot skill between different geometric environments is non-trivial since a wide variety of environments exists, sensor observations as well as robot motions are high-dimensional, and the environment might only be partially observed. We consider the problem of extracting a low-dimensional description of the manipulated environment in form of a kinematic model. This allows us to transfer a skill by defining a policy on a prototype model and morphing the observed environment to this prototype. A deep neural network is used to map depth image observations of the environment to morphing parameter, which include transformations and configurations of the prototype model. Using the concatenation property of affine transformations and the ability to convert point clouds to depth images allows to apply the network in an iterative manner. The network is trained on data generated in a simulator and on augmented data that is created with its own predictions. The algorithm is evaluated on different tasks, where it is shown that iterative predictions lead to a higher accuracy than one-step predictions.


Title: Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correction
Key Words: accelerometers  calibration  cameras  distance measurement  gyroscopes  inertial navigation  learning (artificial intelligence)  mobile robots  neural nets  pose estimation  robot vision  SLAM (robots)  vision-aided absolute trajectory estimation  unsupervised deep network  online error correction  unsupervised deep neural network approach  RGB-D imagery  inertial measurements  Visual-Inertial-Odometry Learner  inertial measurement unit intrinsic parameters  white noise  extrinsic calibration  camera  IMU measurements  hypothesis trajectories  scaled image projection errors  visual odometry  visual simultaneous localization  KITTI Odometry dataset  competitive odometry performance  visual-inertial odometry  Cameras  Jacobian matrices  Image reconstruction  Trajectory  Simultaneous localization and mapping  Training 
Abstract: Adstract- We present an unsupervised deep neural network approach to the fusion of RGB-D imagery with inertial measurements for absolute trajectory estimation. Our network, dubbed the Visual-Inertial-Odometry Learner (VIOLearner), learns to perform visual-inertial odometry (VIO) without inertial measurement unit (IMU) intrinsic parameters (corresponding to gyroscope and accelerometer bias or white noise) or the extrinsic calibration between an IMU and camera. The network learns to integrate IMU measurements and generate hypothesis trajectories which are then corrected online according to the Jacobians of scaled image projection errors with respect to a spatial grid of pixel coordinates. We evaluate our network against state-of-the-art (SOA) visual-inertial odometry, visual odometry, and visual simultaneous localization and mapping (VSLAM) approaches on the KITTI Odometry dataset [1] and demonstrate competitive odometry performance.


Title: Distributed Deep Reinforcement Learning based Indoor Visual Navigation
Key Words: indoor environment  indoor navigation  learning (artificial intelligence)  mobile robots  object detection  path planning  robot vision  complicated environment scene  motor control command  navigation task  large-scale indoor complex environment  pre-constructed map  indoor environment  complex spatial perception possible  indoor space  complex navigation path  aforementioned large-scale environment  real environments  distributed deep reinforcement learning based indoor visual navigation  Navigation  Visualization  Task analysis  Training  Reinforcement learning  Robots  Indoor environments  deep reinforcement learning  visual navigation 
Abstract: Recently, as the rise of deep reinforcement learning, it not only can help the robot to convert the complicated environment scene to motor control command directly but also can accomplish the navigation task properly. In this paper, we propose a novel structure, where the objective is to achieve navigation in large-scale indoor complex environment without pre-constructed map. Generally, it requires good understanding of such indoor environment to make complex spatial perception possible, especially when the indoor space consists of many walls and doors which might block the view of robot leading to complex navigation path. By the proposed distributed deep reinforcement learning in different local regions, our method can achieve indoor visual navigation in the aforementioned large-scale environment without extra map information and human instruction. In the experiments, we validate our proposed method by conducting highly promising navigation tasks both in simulation and real environments.


Title: Efficient Map Representations for Multi-Dimensional Normal Distributions Transforms
Key Words: mobile robots  Monte Carlo methods  normal distribution  probability  robot vision  stereo image processing  transforms  indoor environments  outdoor environments  driving flying robots  fast approach  accurate approach  indexed kd-trees  free space  occupancy probabilities  map consistency  large-scale environments  mapping efficiency  efficient map representations  3D map representations  static environments  dynamic environments  multidimensional normal distributions transforms  3D normal distributions transform mapping  Three-dimensional displays  Robot sensing systems  Gaussian distribution  Two dimensional displays  Task analysis  Transforms 
Abstract: Efficient 2D and 3D map representations of both static and dynamic, indoor and outdoor environments are crucial for navigation of driving and flying robots. In this paper, we propose a fast and accurate approach for 2D and 3D Normal Distributions Transform (NDT) mapping based on indexed kd-trees. Similar to other approaches, we also model free space, which allows us to obtain occupancy probabilities. Additionally, we provide optional visibility based updates to enhance map consistency in case of noisy data, e.g. from stereo cameras. Unlike other available implementations, our approach is natively applicable to large-scale environments and in real-time, because our maps are able to grow dynamically. This also offers applicability to exploration tasks. To evaluate our approach, we present experimental results on publicly available datasets and discuss the mapping efficiency in terms of accuracy, runtime and memory management. As an exemplary use case, we apply our maps to Monte Carlo Localization on a well-known large-scale dataset.


Title: Modeling and Control of an Articulated Tail for Maneuvering a Reduced Degree of Freedom Legged Robot
Key Words: actuators  feedback  hardware-in-the loop simulation  legged locomotion  linearisation techniques  motion control  robot dynamics  leg mechanisms  quadruped robot  dynamic tail motions  robotic system design  outer loop controller  articulated tail mechanism  inner loop controller  tail prototype  dynamic modeling control  articulated robotic tail  maneuvering  legged robotic systems  reduced degree of freedom legged robot  hardware-in-the-loop experiments  quadruped platform simulation  feedback linearization maps  Legged locomotion  Robot kinematics  Foot  Dynamics  Task analysis  Manipulators 
Abstract: This paper presents dynamic modeling and control of an articulated robotic tail to maneuver and stabilize a reduced degree-of-freedom (DOF) quadruped robot. Conventional legged robotic systems consist of leg mechanisms that provide simultaneous propulsion, maneuvering and stabilization. However, in nature animals have been observed to utilize their tails to assist the legs in multiple tasks. Similarly, by incorporating an articulated tail onboard a quadruped robot, dynamic tail motions can be used to aid maneuvering. Therefore, tail implementation can potentially lead to simplifications in design and control of the legged robot since the legs will be responsible for only propulsion tasks. In this paper, a robotic system design consisting of an articulated tail and quadruped robot system is presented. Dynamic models are derived to analyze an optimal tail mass and length ratio to enhance inertial adjustment applications and develop an outer loop controller to plan tail trajectories for desired maneuvering applications. Results of analytical optimization are corroborated with measured data from biological animals. To decouple the dynamics of the articulated tail mechanism an inner loop controller using feedback linearization maps the desired behavior to the actuator inputs. This approach is validated using hardware-in-the-loop experiments with tail prototype in conjunction with simulated quadruped platform. Results demonstrate the capabilities of the articulated tail in enabling precise left and right turning (maneuvering).


Title: A Framework for Robot Grasp Transferring with Non-rigid Transformation
Key Words: collision avoidance  control engineering computing  dexterous manipulators  grippers  learning (artificial intelligence)  optimisation  path planning  orientation search  collision avoidance  grasp generation  dexterous tasks execution  online planning  grasp planning  robot grasp transferring  task requirements  robot reachability  grasp robustness  nonrigid transformation  human demonstration  analytic approach  Task analysis  Robots  Grasping  Planning  Collision avoidance  Grippers  Databases 
Abstract: Grasp planning is essential for robots to execute dexterous tasks. Solving the optimal grasps for various objects online, however, is challenging due to the heavy computation load during exhaustive sampling, and the difficulties to consider task requirements. This paper proposes a framework to combine analytic approach with learning for efficient grasp generation. The example grasps are taught by human demonstration and mapped to similar objects by a non-rigid transformation. The mapped grasps are evaluated analytically and refined by an orientation search to improve the grasp robustness and robot reachability. The proposed approach is able to plan high-quality grasps, avoid collision, satisfy task requirements, and achieve efficient online planning. The effectiveness of the proposed method is verified by a series of experiments.


Title: Deep Reinforcement Learning to Acquire Navigation Skills for Wheel-Legged Robots in Complex Environments
Key Words: learning (artificial intelligence)  legged locomotion  path planning  robot vision  wheels  navigation skills  navigation behaviors  action policies training  height-map image observations  motor commands  dynamic environments  mobile robot navigation  complex environments  deep reinforcement learning  wheel-legged robots  Training  Task analysis  Navigation  Mobile robots  Trajectory  Robot sensing systems 
Abstract: Mobile robot navigation in complex and dynamic environments is a challenging but important problem. Reinforcement learning approaches fail to solve these tasks efficiently due to reward sparsities, temporal complexities and high-dimensionality of sensorimotor spaces which are inherent in such problems. We present a novel approach to train action policies to acquire navigation skills for wheel-legged robots using deep reinforcement learning. The policy maps height-map image observations to motor commands to navigate to a target position while avoiding obstacles. We propose to acquire the multifaceted navigation skill by learning and exploiting a number of manageable navigation behaviors. We also introduce a domain randomization technique to improve the versatility of the training samples. We demonstrate experimentally a significant improvement in terms of data-efficiency, success rate, robustness against irrelevant sensory data, and also the quality of the maneuver skills.


Title: Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network
Key Words: convolutional neural nets  learning (artificial intelligence)  object detection  robot vision  statistical analysis  support vector machines  patched convolutional neural network  semantic-aware patch-level convolutional neural network  statistical features  multiclass support vector machine  deep learning framework  robotic applications  vision systems  shadow detection methods  Image color analysis  Image edge detection  Support vector machines  Robots  Image segmentation  Training  Time complexity 
Abstract: In recent years, various shadow detection methods from a single image have been proposed and used in vision systems; however, most of them are not appropriate for the robotic applications due to the expensive time complexity. This paper introduces a fast shadow detection method using a deep learning framework, with a time cost that is appropriate for robotic applications. In our solution, we first obtain a shadow prior map with the help of multi-class support vector machine using statistical features. Then, we use a semantic-aware patch-level Convolutional Neural Network that efficiently trains on shadow examples by combining the original image and the shadow prior map. Experiments on benchmark datasets demonstrate the proposed method significantly decreases the time complexity of shadow detection, by one or two orders of magnitude compared with state-of-the-art methods, without losing accuracy.


Title: Robotic Subsurface Pipeline Mapping with a Ground-penetrating Radar and a Camera
Key Words: buried object detection  geophysical image processing  geophysical techniques  ground penetrating radar  image reconstruction  maximum likelihood estimation  pipelines  radar detection  radar imaging  robot vision  pipeline groups  hyperbola response  GPR sensing process  Ground Penetrating Radar scans  subsurface pipeline mapping method  robotic subsurface pipeline mapping  subsurface pipes  representative pipeline configurations  maximum likelihood estimation  J-Linkage method  hyperbolas  GPR scans  mapping outputs  visual simultaneous localization  nonperpendicular angles  general scanning  size 4.69 cm  Ground penetrating radar  Pipelines  Cameras  Three-dimensional displays  Trajectory  Robot sensing systems 
Abstract: We propose a novel subsurface pipeline mapping method by fusing Ground Penetrating Radar (GPR) scans and camera images. To facilitate the simultaneous detection of multiple pipelines, we model the GPR sensing process and prove hyperbola response for general scanning with non-perpendicular angles. Furthermore, we fuse visual simultaneous localization and mapping outputs, encoder readings with GPR scans to classify hyperbolas into different pipeline groups. We extensively apply the J-Linkage method and maximum likelihood estimation to improve algorithm robustness and accuracy. As the result, we optimally estimate the radii and locations of all pipelines. We have implemented our method and tested it in physical experiments with representative pipeline configurations. The results show that our method successfully reconstructs all subsurface pipes. Moreover, the average localization error is 4.69cm.


Title: Mobile Robot Localization Considering Class of Sensor Observations
Key Words: collision avoidance  mobile robots  localization robustness  environment dynamics  robots  sensor observations  mapped obstacles  observation model  unmapped obstacles  real-world mobile robot navigation competition  mobile robot localization  Robot sensing systems  Mathematical model  Robustness  Hidden Markov models  Mobile robots  Collision avoidance 
Abstract: Localization robustness against environment dynamics is significant for robots to achieve autonomous navigation in unmodified environments. A basic method of improving the robustness of a robot is considering the sensor observations obtained from mapped obstacles and using them for localizing the robot's pose. This study proposes an observation model that considers the class of sensor observations, where “class” categorizes the sensor observations as those obtained from mapped and unmapped obstacles. In the proposed approach, the robot's pose and the class are estimated simultaneously. As a result, the robot's pose can be localized using the sensor observations obtained only from mapped obstacles. First, we evaluated the performance of the proposed approach using simulations. Further, we tested the proposed approach in a real-world mobile robot navigation competition, called “Tsukuba Challenge,” held in Japan. The robustness and effectiveness of the proposed approach against environment dynamics were verified from the experimental results.


Title: Octree map based on sparse point cloud and heuristic probability distribution for labeled images
Key Words: calibration  cameras  convolutional neural nets  image recognition  object recognition  octrees  probability  stereo image processing  semantic octree maps  probabilistic octree framework  single lidar scans  octree map building algorithm  labeled lidar scan  camera-lidar calibration parameters  convolutional neural network  accurate driving maneuvers  automated vehicle  urban roads  labeled images  heuristic probability distribution  sparse point cloud  Three-dimensional displays  Semantics  Laser radar  Uncertainty  Octrees  Cameras  Buildings 
Abstract: To navigate through urban roads, an automated vehicle must be able to perceive and recognize objects in a three-dimensional environment. A high level contextual understanding of the surroundings is necessary to execute accurate driving maneuvers. This paper presents a novel approach to build three dimensional semantic octree maps from lidar scans and the output of a convolutional neural network (CNN) to obtain the labels of the environment. We present a heuristic method to associate uncertainties to the labels from the images based on a combination of the labels themselves, score maps retrieved by the CNN and the raw images. These uncertainties and the camera-lidar calibration parameters for multiple cameras are considered in the projection of the labels and their uncertainties into the point cloud. Every labeled lidar scan works as an input to an octree map building algorithm that calculates and updates the label probabilities of the voxels in the map. This paper also presents a qualitative and quantitative evaluation of accuracy, analyzing projection in single lidar scans and complete maps built with our probabilistic octree framework.


Title: Human-in-the-loop Augmented Mapping
Key Words: inertial systems  mobile robots  operating systems (computers)  optical radar  path planning  robot programming  user interfaces  2D map building  user interface  human map augmentation  LIDAR  Gmapping ROS package  Unity software  online editing capabilities  user-friendly system  traditional offline post processing  real-time human augmented mapping system  human-in-the-loop  mapping errors  Two dimensional displays  Laser radar  Simultaneous localization and mapping  Three-dimensional displays  Corporate acquisitions 
Abstract: In this paper we develop a real-time human augmented mapping system. This approach replaces the traditional offline post processing of maps by a user-friendly system allowing for online editing capabilities. A wide number of applications that acquire accurate mapping of the environment could benefit from such a solution. The proposed framework consists of two main parts: 2D map building using LIDAR, encoders, and IMU; and a user interface for human map augmentation. The first part is built over Gmapping ROS package, while the second is developed in Unity software. Realworld experiments validated the ability of our system to correct for sensor noise and various mapping errors, thus increasing the accuracy of the obtained maps without additional computational costs.


Title: A B-Spline Mapping Framework for Long-Term Autonomous Operations
Key Words: image representation  image sensors  mobile robots  navigation  path planning  robot vision  SLAM (robots)  splines (mathematics)  landmark-based maps  robotics community  high frequency sensor  B-spline curves  B-spline maps  mapping algorithm  2D B-spline mapping framework  outdoor long-term autonomous operations  simultaneous localization and mapping  SLAM algorithm  software-in-the-loop simulations  Splines (mathematics)  Simultaneous localization and mapping  Three-dimensional displays  Robot kinematics  Two dimensional displays 
Abstract: This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.


Title: Building Dense Reflectance Maps of Indoor Environments Using an RGB-D Camera
Key Words: brightness  cameras  image colour analysis  image reconstruction  lighting conditions  light emitters  high dynamic range radiosity estimation  reflectance estimate  diffuse reflectance  specific lighting condition  colored models  extensive progress  RGB-D cameras  dense surface geometry  robotic applications  indoor environments  building dense reflectance maps  Cameras  Lighting  Image reconstruction  Robots  Geometry  Indoor environments  Surface treatment 
Abstract: The ability to build models of the environment is an essential prerequisite for many robotic applications. In recent years, mapping of dense surface geometry using RGB-D cameras has seen extensive progress. Many approaches build colored models, typically directly using the intensity values provided by the camera. Unfortunately, these intensities are inherently affected by illumination. Therefore, the resulting maps only represent the environment for one specific lighting condition. To overcome this limitation, we propose to build reflectance maps that are invariant against changes in lighting. Our approach estimates the diffuse reflectance of a surface by recovering its radiosity and the corresponding irradiance. As imperfections in this process can significantly degrade the reflectance estimate, we remove outliers in the high dynamic range radiosity estimation and propose a method to refine the reflectance estimate. Our system implements the whole pipeline for offline reconstruction of dense reflectance maps including the segmentation of light emitters in the scene. We demonstrate the applicability of our approach in real-world experiments under varying lighting conditions.


Title: 3D Underground Mapping with a Mobile Robot and a GPR Antenna
Key Words: feature extraction  ground penetrating radar  image matching  image segmentation  mobile robots  radar imaging  underground robotic applications  image processing techniques  subsurface 3D map  Ground Penetrating Radar  construction services  automatic subsurface mapping  GPR antenna  mobile robot  underground mapping  Ground penetrating radar  Three-dimensional displays  Feature extraction  Mobile antennas  Mobile robots  Antenna measurements 
Abstract: Automatic subsurface mapping is essential in the construction services, as it is anticipated to become the main operational environment of the future robots to be realized in the respective domain. Towards this direction, the paper at hand, introduces for the first time herein, an integrated framework for subsurface mapping by exploiting a surface operating mobile robot with a Ground Penetrating Radar (GPR). The mobile robot tows the GPR antenna, which is mounted on a specifically designed trailer, and is utilized as the mean to cover the surface area, while at the same time the antenna scans the subsurface by emitting electromagnetic pulses. The gathered data are processed for the construction of a subsurface 3D map. Specifically, image processing techniques, that involve background segmentation, HOG [1] feature extraction, hypothesis verification and matching are applied on the 2D radargram (B-Scan) for the detection of the salient points that correspond to buried utilities. By employing the pulse propagation velocity into the subsurface and the soil utilities, the salient points are expressed in world coordinates and used for the composition of the 3D subsurface map. Our method has been evaluated on a real test site, accompanied by ground-truth annotation data of experts and revealed remarkable performance, exhibiting not only the feasibility of underground mapping but also the capacity to obtain exploitable results for underground robotic applications.


Title: Adaptive Baseline Monocular Dense Mapping with Inter-Frame Depth Propagation
Key Words: image matching  image reconstruction  image sequences  stereo image processing  monocular dense mapping methods  frame-to-frame propagated depth filter  wide-baseline observations  sequential input images  adaptive baseline matching cost computation  sequential depth estimation  multibaseline observations  separate multiview stereo problems  image sequence  inter-frame depth propagation  adaptive baseline monocular dense mapping  Estimation  Cameras  Probabilistic logic  Adaptive systems  Image sequences  Real-time systems  Robot vision systems 
Abstract: State-of-the-art monocular dense mapping methods usually divide the image sequence into several separate multi-view stereo problems thus have limited utilization of the information in multi-baseline observations and sequential depth estimations. In this paper, two core contributions are proposed to improve the mapping performance by exploiting the information. The first is an adaptive baseline matching cost computation that uses the sequential input images to provide each pixel with wide-baseline observations. The second is a frame-to-frame propagated depth filter which integrates the sequential depth estimation of the same physical point in a robust probabilistic manner. Two contributions are integrated into a monocular dense mapping system that generates the depth maps in real-time for both pinhole and fisheye cameras. Our system is fully parallelized and can run at more than 25 fps on a Nvidia Jetson TX2. We compare our work with state-of-the-art methods on the public dataset. Onboard UAV mapping and handhold experiments are also used to demonstrate the performance of our method. For the benefit of the community, we make the implementation open source.


Title: Real Time Incremental Foveal Texture Mapping for Autonomous Vehicles
Key Words: cameras  computer vision  image reconstruction  image resolution  image texture  mesh generation  mobile robots  optical radar  robot vision  scan matching techniques  end-to-end real time framework  real time incremental foveal texture mapping  real time incremental foveal texture mapping  precise localization  detailed map  urban environment  high resolution graphics grade  texture mapping error  texture error  output map  computation time  ray-filtering  sparse input LIDAR scan  high resolution 3D  camera image information  pose-refinement procedure  color texture  coherent 3D surface  computer games  background map  planning algorithms  virtual test bed  autonomous vehicles  navigation  Three-dimensional displays  Laser radar  Cameras  Real-time systems  Image color analysis  Global Positioning System 
Abstract: We propose an end-to-end real time framework to generate high resolution graphics grade textured 3D map of urban environment. The generated detailed map finds its application in the precise localization and navigation of autonomous vehicles. It can also serve as a virtual test bed for various vision and planning algorithms as well as a background map in the computer games. In this paper, we focus on two important issues: (i) incrementally generating a map with coherent 3D surface, in real time and (ii) preserving the quality of color texture. To handle the above issues, firstly, we perform a pose-refinement procedure which leverages camera image information, Delaunay triangulation and existing scan matching techniques to produce high resolution 3D map from the sparse input LIDAR scan. This 3D map is then texturized and accumulated by using a novel technique of ray-filtering which handles occlusion and inconsistencies in pose-refinement. Further, inspired by human fovea, we introduce foveal-processing which significantly reduces the computation time and also assists ray-filtering to maintain consistency in color texture and coherency in 3D surface of the output map. Moreover, we also introduce texture error (TE) and mean texture mapping error (MTME), which provides quantitative measure of texturing and overall quality of the textured maps.


Title: Directional Grid Maps: Modeling Multimodal Angular Uncertainty in Dynamic Environments
Key Words: collision avoidance  human-robot interaction  mobile robots  optical radar  path planning  probability  directional grid maps  occupancy map  mobile robot  robotic arm  static environments  dynamic objects  safer navigation  human-robot interaction  directional statistics  robotic mapping  model circular data  angular motion  probability measure-field  angular variations  indoor environments  outdoor environments  dynamic environments  grid maps  multimodal angular uncertainty  Vehicle dynamics  Robot sensing systems  Data models  Uncertainty  Navigation 
Abstract: Robots often have to deal with the challenges of operating in dynamic and sometimes unpredictable environments. Although an occupancy map of the environment is sufficient for navigation of a mobile robot or manipulation tasks with a robotic arm in static environments, robots operating in dynamic environments demand richer information to improve robustness, efficiency, and safety. For instance, in path planning, it is important to know the direction of motion of dynamic objects at various locations of the environment for safer navigation or human-robot interaction. In this paper, we introduce directional statistics into robotic mapping to model circular data. Primarily, in collateral to occupancy grid maps, we propose directional grid maps to represent the location-wide long-term angular motion of the environment. Being highly representative, this defines a probability measure-field over the longitude-latitude space rather than a scalar-field or a vector-field. Withal, we further demonstrate how the same theory can be used to model angular variations in the spatial domain, temporal domain, and spatiotemporal domain. We carried out a series of experiments to validate the proposed models using a variety of robots having different sensors such as RGB cameras and LiDARs on simulated and real-world settings in both indoor and outdoor environments.


Title: Robust LIDAR Localization for Autonomous Driving in Rain
Key Words: feature extraction  mobile robots  optical radar  particle filtering (numerical methods)  stereo image processing  traffic engineering computing  3D LIDAR scans  histogram filter  particle filter  posterior distributions  vehicle poses  complex urban environments  fair weather  rainy weather  robust LIDAR localization  autonomous driving  map-based localization method  rainy conditions  ground reflectivity features  vertical features extraction  Feature extraction  Three-dimensional displays  Laser radar  Histograms  Rain  Measurement by laser beam  Two dimensional displays 
Abstract: This paper introduces a map-based localization method aiming to increase robustness in rainy conditions. This method utilizes two types of features: ground reflectivity features and vertical features extracted from 3D LIDAR scans and builds vehicle pose belief with two filters: a histogram filter and a particle filter. The posterior distributions from the two filters are integrated to estimate vehicle poses. This method exploits advantages of both features and filters, compensating respective weakness to deal with complex urban environments. Testing was performed in the fair and rainy weather. Road test results prove robustness and reliability of the proposed method.


Title: Move Base Flex A Highly Flexible Navigation Framework for Mobile Robots
Key Words: mobile robots  motion control  navigation  path planning  MBF  path planning  motion control  robot tasks  complex navigation tasks  Move Base Flex  highly flexible navigation framework  modular navigation  map-independent navigation  open-source navigation  Navigation  Robots  Computer architecture  Task analysis  Flexible printed circuits  Servers  Planning 
Abstract: We present Move Base Flex (MBF), a highly flexible, modular, map-independent, open-source navigation framework for use in ROS. MBF provides modular actions for executing plugins for path planning, motion control, and recovery. These actions define interfaces for external executives to allow highly flexible navigation strategies, which can be intertwined with other robot tasks. MBF has been successfully deployed in a professional setting at customer facilities to control robots in highly dynamic environments. We compare MBF with the well-known move_base and present the architecture as well as different deployment approaches, including how MBF can be used with different executives to perform complex navigation tasks interleaved with other robot operations.


Title: PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization
Key Words: feature extraction  mobile robots  optical radar  path planning  local views  sliding window fashion  matching current  old features  map representation  local maps  off-road environments  single localization failure  distinctive features  coined PoseMap  dynamic environments  robotic systems  long-term localization  multienvironment 3D LiDAR localization  frequency 8.0 Hz  time 18.0 month  Simultaneous localization and mapping  Three-dimensional displays  Laser radar  Optimization  Feature extraction 
Abstract: Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.


Title: Identifying Driver Behaviors Using Trajectory Features for Vehicle Navigation
Key Words: automobiles  behavioural sciences computing  driver information systems  feature extraction  Internet  mobile robots  vehicle trajectories  autonomous vehicles  car trajectories  data-driven mapping  vehicle navigation simulation system  driver behavior identification  Web-based user study  Trajectory  Navigation  Automobiles  Feature extraction  Measurement  Acceleration 
Abstract: We present a novel approach to automatically identify driver behaviors from vehicle trajectories and use them for safe navigation of autonomous vehicles. We propose a novel set of features that can be easily extracted from car trajectories. We derive a data-driven mapping between these features and six driver behaviors using an elaborate web-based user study. We also compute a summarized score indicating a level of awareness that is needed while driving next to other vehicles. We also incorporate our algorithm into a vehicle navigation simulation system and demonstrate its benefits in terms of safer realtime navigation, while driving next to aggressive or dangerous drivers.


Title: Interactive Robotic Manipulation of Elastic Objects
Key Words: collision avoidance  elastic deformation  finite element analysis  force control  manipulators  robot kinematics  sensitivity analysis  simulation  interactive simulation-based control methodology  interactive robotic manipulation  finite element method  sensitivity analysis  mathematical model  robots configuration  collision avoidance  elastic deformation objects  quasistatic assumption  Robots  Computational modeling  Shape  Collision avoidance  Mathematical model  Strain  Finite element analysis 
Abstract: In this paper, we address the challenge of robotic manipulation of elastically deforming objects. To this end, we model elastic objects using the Finite Element Method. Through a quasi-static assumption, we leverage sensitivity analysis to mathematically model how changes in the robot's configuration affect the deformed shape of the object being manipulated. This enables an interactive, simulation-based control methodology, wherein user-specified deformations for the elastic objects are automatically mapped to joint angle commands. The optimization formulation we introduce is general, operates directly within a robot's workspace and can readily incorporate joint limits as well as collision avoidance between the links. We validate our control methodology on a YuMi® IRB 14000, which we use to manipulate a variety of elastic objects.


Title: Domain Randomization and Generative Models for Robotic Grasping
Key Words: grippers  learning (artificial intelligence)  neural nets  planning (artificial intelligence)  probability  domain randomization  generative models  deep learning-based robotic grasping  significant progress thanks  algorithmic improvements  increased data availability  state-of-the-art models  unique object instances  result generalization  novel data generation pipeline  deep neural network  successful grasps  autoregressive grasp planning model  probability distribution  possible grasps  sample grasps  test time  model architecture  unseen realistic objects  random objects  real-world grasp  random simulated objects  Grasping  Training  Data models  Computational modeling  Robot sensing systems  Neural networks 
Abstract: Deep learning-based robotic grasping has made significant progress thanks to algorithmic improvements and increased data availability. However, state-of-the-art models are often trained on as few as hundreds or thousands of unique object instances, and as a result generalization can be a challenge. In this work, we explore a novel data generation pipeline for training a deep neural network to perform grasp planning that applies the idea of domain randomization to object synthesis. We generate millions of unique, unrealistic procedurally generated objects, and train a deep neural network to perform grasp planning on these objects. Since the distribution of successful grasps for a given object can be highly multimodal, we propose an autoregressive grasp planning model that maps sensor inputs of a scene to a probability distribution over possible grasps. This model allows us to sample grasps efficiently at test time (or avoid sampling entirely). We evaluate our model architecture and data generation pipeline in simulation and the real world. We find we can achieve a >90% success rate on previously unseen realistic objects at test time in simulation despite having only been trained on random objects. We also demonstrate an 80% success rate on real-world grasp attempts despite having only been trained on random simulated objects.


Title: Planning Hand-Arm Grasping Motions with Human-Like Appearance
Key Words: humanoid robots  manipulator kinematics  motion control  path planning  planning hand-arm grasping motions  hand-arm robotic systems  grasping actions  coordinated movements  robotic arm  anthropomorphic mechanical hand  human movements  human hand synergies  planning phase  motion planning  state-of-the-art planning algorithm  human-like appearance  search space  sampling-based planner  Planning  Grasping  Robot kinematics  Trajectory  Complexity theory  Manipulators 
Abstract: This paper addresses the problem of obtaining human-like motions on hand-arm robotic systems performing grasping actions. The focus is set on the coordinated movements of the robotic arm and the anthropomorphic mechanical hand, with which the arm is equipped. For this, human movements performing different grasps are captured and mapped to the robot in order to compute the human hand synergies. These synergies are used to both obtain human-like movements and to reduce the complexity of the planning phase by reducing the dimension of the search space. In addition, the paper proposes a sampling-based planner, which guides the motion planning following the synergies and considering different types of grasps. The introduced approach is tested in an application example and thoroughly compared with a state-of-the-art planning algorithm, obtaining better results.


Title: Robust Exploration with Multiple Hypothesis Data Association
Key Words: image fusion  mobile robots  robot vision  SLAM (robots)  target tracking  tree searching  joint compatibility branch  simultaneous localization and mapping  map accuracy  diverse hypotheses  multiple hypothesis tracking  robust back-ends  catastrophic failure  single false positive assignment  rich features  autonomous exploration  SLAM  ambiguous data association problem  multiple hypothesis data association  robust exploration  Simultaneous localization and mapping  Trajectory  Noise measurement  State estimation  Optimization  Measurement uncertainty 
Abstract: We study the ambiguous data association problem confronting simultaneous localization and mapping (SLAM), specifically for the autonomous exploration of environments lacking rich features. In such environments, a single false positive assignment might lead to catastrophic failure, which even robust back-ends may be unable to resolve. Inspired by multiple hypothesis tracking, we present a novel approach to effectively manage multiple hypotheses (MH) of data association inherited from traditional joint compatibility branch and bound (JCBB), which entails the generation, ordering and elimination of hypotheses. We analyze the performance of MHJCBB in two particular situations, one applying it to SLAM over a predefined trajectory and the other showing its applicability in exploring unknown environments. Statistical results demonstrate that MHJCBB's maintenance of diverse hypotheses under ambiguous conditions significantly improves map accuracy.


Title: Reactive Collision Avoidance Using Real-Time Local Gaussian Mixture Model Maps
Key Words: cameras  collision avoidance  Gaussian processes  geometry  helicopters  mobile robots  probability  trajectory control  collision avoidance  discrete map  GMM local mapping algorithm  gaussian mixture model maps  robots  CPU  quadrotor navigation  depth camera processing  time-parameterized trajectory  geometric properties  probabilistic approach  cluttered environments  Trajectory  Collision avoidance  Robot sensing systems  Real-time systems  Current measurement  Gaussian mixture model 
Abstract: In unknown, cluttered environments, robots require online real-time mapping and collision checking in order to navigate robustly. Discrete map representations are inefficient for collision checking as they are expensive in terms of memory and computation. This paper takes a probabilistic approach to local mapping by representing the environment as a Gaussian Mixture Model (GMM) and leverages its geometric properties to enable efficient collision checking given a time-parameterized trajectory. In contrast to current discretization-based methods, a GMM preserves geometric coverage of the environment without losing representation accuracy with varying map resolutions. We introduce a novel GMM local mapping algorithm that can be used with a single depth camera processed on a single CPU, and provide algorithms for collision avoidance given arbitrary trajectory representations. Finally, we provide experimentation results demonstrating safety, efficiency, and data coverage for real-time collision avoidance with a quadrotor navigating in a cluttered environment.


Title: Grid-Based Motion Planning Using Advanced Motions for Hexapod Robots
Key Words: graph theory  legged locomotion  motion control  path planning  grid-based motion planning  advanced motions  hexapod robots  motion planning framework  chimney walking  robot motion  hierarchical planning framework  custom-designed Corin hexapod  environment surfaces  Legged locomotion  Planning  Trajectory  Collision avoidance  Robot motion 
Abstract: This paper presents the motion planning framework for a hexapod, based on advanced motions, for accessing challenging spaces, namely narrow pathways and large holes, both of which are surrounded by walls. The advanced motions, wall and chimney walking, utilise environment surfaces that are perpendicular to the ground plane to support the robot motion. Such techniques have not yet been studied in the literature. The hierarchical planning framework proposed here is an extension to existing approaches which have only considered ground walking where foothold contacts are confined to the ground plane. During the pre-processing phase of the 2.5D grid map, the motion primitives employed are assessed for each cell and stacked to the graph if valid. The A* algorithm is then used to find a path to the goal position. Following that, the path is post-processed to smoothen the motions and generate a continuous path. Footholds are then selected along the path. The framework has been evaluated in simulation on the custom-designed Corin hexapod. The resulting path enables access to areas that are previously thought to be inaccessible and reduces the travelling distance compared to previous studies.


Title: A Variational Feature Encoding Method of 3D Object for Probabilistic Semantic SLAM
Key Words: Bayes methods  belief networks  feature extraction  maximum likelihood estimation  object recognition  probability  robot vision  SLAM (robots)  object recognition methods  true generative model  semantic simultaneous localization and mapping  maximum likelihood estimation  shape retrieval  Bayesian inference  Bayesian networks  approximated distributions  variational auto-encoder  complex distributions  observation likelihood  tractable distributions  3D object shapes  view-independent loop closure  object shape  range sensor  mobile robot  complex probability distribution  probabilistic observation model  high-level semantic features  complex 3D objects  probabilistic semantic SLAM  variational feature encoding method  Shape  Simultaneous localization and mapping  Three-dimensional displays  Semantics  Solid modeling  Bayes methods  Probabilistic logic 
Abstract: This paper presents a feature encoding method of complex 3D objects for high-level semantic features. Recent approaches to object recognition methods become important for semantic simultaneous localization and mapping (SLAM). However, there is a lack of consideration of the probabilistic observation model for 3D objects, as the shape of a 3D object basically follows a complex probability distribution. Furthermore, since the mobile robot equipped with a range sensor observes only a single view, much information of the object shape is discarded. These limitations are the major obstacles to semantic SLAM and view-independent loop closure using 3D object shapes as features. In order to enable the numerical analysis for the Bayesian inference, we approximate the true observation model of 3D objects to tractable distributions. Since the observation likelihood can be obtained from the generative model, we formulate the true generative model for 3D object with the Bayesian networks. To capture these complex distributions, we apply a variational auto-encoder. To analyze the approximated distributions and encoded features, we perform classification with maximum likelihood estimation and shape retrieval.


Title: Scale Correct Monocular Visual Odometry Using a LiDAR Altimeter
Key Words: distance measurement  image sequences  mobile robots  optical radar  robot vision  SLAM (robots)  stereo image processing  stereo visual SLAM  monocular vision  inherent scale ambiguity  LiDAR altimeter  scale correct monocular visual odometry  RGB-D methods  scale drift  keyframe basis  scale constraint  mapping algorithm  keyframe based tracking  Visual Odometry method  laser altimeter  range data  exploration vehicles  power requirements  computational load  metrological accuracy  RGB-D sensors  3D LiDARs  metric references  sensory sources  Cameras  Laser radar  Measurement  Three-dimensional displays  Visual odometry  Visualization  Sensors 
Abstract: The inherent scale ambiguity in monocular vision is a well known issue that forces the integration of other sensory sources to obtain metric references. However, 2D or 3D LiDARs and RGB-D sensors, while guaranteeing metrological accuracy, impose a non negligible burden both in terms of computational load and power requirements limiting the feasibility of being implemented on small exploration vehicles. This paper presents a scale aware monocular Visual Odometry framework that fuses range data from a laser altimeter in order to recover and maintain a correct metric scale. The proposed Visual Odometry method consists of a keyframe based tracking and mapping algorithm using optical flow where range data serves as a scale constraint on a keyframe to keyframe basis. An optimization backend based on iSAM2 is employed in order to refine the trajectory and map estimates eliminating the scale drift without the need of performing loop closures. We demonstrate that our algorithm can obtain very similar performances to state of the art stereo visual SLAM and RGB-D methods.


Title: Robust Visual-Inertial State Estimation with Multiple Odometries and Efficient Mapping on an MAV with Ultra-Wide FOV Stereo Vision
Key Words: autonomous aerial vehicles  cameras  distance measurement  estimation theory  image fusion  image sensors  inertial navigation  motion estimation  motion measurement  state estimation  stereo image processing  visual perception  wide-angle stereo cameras  multicopter system  inertial measurement unit  virtual pinhole cameras  independent visual odometry  vision system  sensor fusion  robust visual-inertial state estimation  ultrawide FOV stereo vision  MAV  IMU  robust visual-inertial navigation  omnidirectional 3D mapping pipeline experiment  field of view  synthesized pinhole stereo systems  motion estimation fusion  image processing  multiVO approach  Cameras  Distortion  Image resolution  Computational modeling  Navigation  Visual odometry  Hardware 
Abstract: The here presented flying system uses two pairs of wide-angle stereo cameras and maps a large area of interest in a short amount of time. We present a multicopter system equipped with two pairs of wide-angle stereo cameras and an inertial measurement unit (IMU) for robust visual-inertial navigation and time-efficient omni-directional 3D mapping. The four cameras cover a 240 degree stereo field of view (FOV) vertically, which makes the system also suitable for cramped and confined environments like caves. In our approach, we synthesize eight virtual pinhole cameras from four wide-angle cameras. Each of the resulting four synthesized pinhole stereo systems provides input to an independent visual odometry (VO). Subsequently, the four individual motion estimates are fused with data from an IMU, based on their consistency with the state estimation. We describe the configuration and image processing of the vision system as well as the sensor fusion and mapping pipeline on board the MAV. We demonstrate the robustness of our multi-VO approach for visual-inertial navigation and present results of a 3D-mapping experiment.


Title: Learning Hardware Dynamics Model from Experiments for Locomotion Optimization
Key Words: control engineering computing  learning (artificial intelligence)  legged locomotion  motion control  optimisation  pendulums  robot dynamics  locomotion optimization  hardware compatibility  hardware-compatible motion plan  linear inverted pendulum  ZMP  hardware dynamics model learning  zero moment point  LIP  center of mass  quadruped  Hardware  Optimization  Dynamics  Legged locomotion  Data models  Solid modeling 
Abstract: The hardware compatibility of legged locomotion is often illustrated by Zero Moment Point (ZMP) that has been extensively studied for decades. One of the most popular models for computing the ZMP is the linear inverted pendulum (LIP) model that expresses ZMP as a linear function of the center of mass(COM) and its acceleration. In the real world, however, it may not accurately predict the true ZMP of hardware due to various reasons such as unmodeled dynamics and differences between simulation model and hardware. In this paper, we aim to improve the theoretical ZMP model by learning the real hardware dynamics from experimental data. We first optimize the motion plan using the theoretical ZMP model and collect COP data by executing the motion on a force plate. We then train a new ZMP model that maps the motion plan variable to the actual ZMP and use the learned model for finding a new hardware-compatible motion plan. Through various locomotion tasks of a quadruped, we demonstrate that motions planned for the learned ZMP model are compatible on hardware when those for the theoretical ZMP model are not. Furthermore, experiments using ZMP models with different complexities reveal that overly complex models may suffer from over-fitting even though they can potentially represent more complex, unmodeled dynamics.


Title: ArthroSLAM: Multi-Sensor Robust Visual Localization for Minimally Invasive Orthopedic Surgery
Key Words: biomedical optical imaging  cameras  endoscopes  image sensors  Kalman filters  medical image processing  medical robotics  orthopaedics  SLAM (robots)  surgery  image feedback  ArthroSLAM  Simultaneous Localisation and Mapping system  SLAM system  external camera  robotic arm  minimally invasive arthroscopic surgery  minimally invasive orthopedic surgery  robotic orthopedic surgical assistant  knee section  human cadaver knee joint  Extended Kalman Filter framework  arthroscope holder  intraarticular space  Cameras  Robot vision systems  Visualization  Reliability 
Abstract: Minimally invasive arthroscopic surgery is a very challenging procedure that requires the manipulation of instruments in limited intraarticular space using distorted and sometimes uninformative images. Localizing the arthroscope reliably and at all times w.r.t. surrounding tissue is of fundamental importance to prevent unintended injury to patients. However, even highly-trained surgeons can struggle to localize the arthro-scope using poor image feedback. In this paper, we propose and demonstrate for the first time a visual Simultaneous Localisation and Mapping (SLAM) system, termed ArthroSLAM, capable of robustly and reliably localizing an arthroscope inside a human knee joint. The proposed system fuses the information obtained from the arthroscope, an external camera mounted on an arthroscope holder, and the odometry of a robotic arm manipulating the scope, in an Extended Kalman Filter framework. Also for the first time, we implement five alternative strategies for localization and compare them to our method in a realistic setup with a human cadaver knee joint. ArthroSLAM is shown to outperform the alternative strategies under various challenging conditions, localizing reliably and at all times with a mean Relative Pose Error of up to 1.4mm and 0.7°. Additional experiments conducted with degraded odometry data also validate the robustness of the method. An initial evaluation of the sparse map of a knee section computed by our method exhibits good morphological agreement. All results suggest that ArthroSLAM is a viable component for the robotic orthopedic surgical assistant of the future.


Title: Underwater Surveying via Bearing Only Cooperative Localization
Key Words: mobile robots  path planning  remotely operated vehicles  underwater vehicles  bearing only cooperative localization  aerial ground vehicles  underwater domain  robotic applications  cave mapping  marine archeology surveying  fresh water  South Carolina  visibility conditions  depth sensors  magnetic sensors  inertial sensors  Florida  Barbados  Cameras  Springs  Robot kinematics  Lakes  Robot sensing systems 
Abstract: Bearing only cooperative localization has been used successfully on aerial and ground vehicles. In this paper we present an extension of the approach to the underwater domain. The focus is on adapting the technique to handle the challenging visibility conditions underwater. Furthermore, data from inertial, magnetic, and depth sensors are utilized to improve the robustness of the estimation. In addition to robotic applications, the presented technique can be used for cave mapping and for marine archeology surveying, both by human divers. Experimental results from different environments, including a fresh water, low visibility, lake in South Carolina; a cavern in Florida; and coral reefs in Barbados during the day and during the night, validate the robustness and the accuracy of the proposed approach.


Title: Semi-Supervised SLAM: Leveraging Low-Cost Sensors on Underground Autonomous Vehicles for Position Tracking
Key Words: cameras  learning (artificial intelligence)  mining  mining industry  mobile robots  object tracking  robot vision  SLAM (robots)  ORB-SLAM2  ground map locations  deep learning  position tracking  operational underground mining vehicles  single camera localization  map creation  mine environment  mining companies  underground environment  SemiSupervised SLAM  underground autonomous vehicles  low-cost sensors  Simultaneous localization and mapping  Cameras  Measurement  Grounding  Visual odometry  Lighting 
Abstract: This work presents Semi-Supervised SLAM - a method for developing a map suitable for coarse localization within an underground environment with minimal human intervention, with system characteristics driven by real-world requirements of major mining companies. This work leverages existing information common within a mining environment - namely a surveyed mine map - which is used to sparsely ground map locations within the mine environment, increasing map accuracy and allowing localization within a global frame. Map creation utilizes a low cost camera sensor and minimal user information to produce a map which can be used for single camera localization within a mining environment. We evaluate the localization capabilities of the proposed approach in depth by performing data collection on operational underground mining vehicles within an active underground mine and by simulating occlusions common to the environment such as dust and water. The proposed system is capable of producing maps which have an average localization error 2.5 times smaller than the next best performing method ORB-SLAM2, comparable localization performance to a state-of-the-art deep learning approach (which is not a feasible solution due to both compute and training requirements) and is robust to simulated environmental obscurants.


Title: Interaction-Aware Probabilistic Behavior Prediction in Urban Environments
Key Words: Bayes methods  belief networks  control engineering computing  driver information systems  inference mechanisms  Markov processes  mobile robots  Monte Carlo methods  probability  road vehicles  traffic engineering computing  combinatorial scene developments  road layouts  future scenes  probabilistic forward simulation  sequential Monte Carlo inference  single agents  context-dependent motion models  complete scene  dynamic Bayesian network  probabilistic prediction framework  mutual interaction  traffic rules  road-geometry  route intentions  traffic participants  urban scenarios  complex scenarios  autonomous driving  urban environments  interaction-aware probabilistic behavior prediction  interaction-unaware physics  real-world scenarios  Trajectory  Estimation  Vehicles  Probabilistic logic  Hidden Markov models  Predictive models  Bayes methods 
Abstract: Planning for autonomous driving in complex, urban scenarios requires accurate prediction of the trajectories of surrounding traffic participants. Their future behavior depends on their route intentions, the road-geometry, traffic rules and mutual interaction, resulting in interdependencies between their trajectories. We present a probabilistic prediction framework based on a dynamic Bayesian network, which represents the state of the complete scene including all agents and respects the aforementioned dependencies. We propose Markovian, context-dependent motion models to define the interaction-aware behavior of drivers. At first, the state of the dynamic Bayesian network is estimated over time by tracking the single agents via sequential Monte Carlo inference. Secondly, we perform a probabilistic forward simulation of the network's estimated belief state to generate the different combinatorial scene developments. This provides the corresponding trajectories for the set of possible, future scenes. Our framework can handle various road layouts and number of traffic participants. We evaluate the approach in online simulations and real-world scenarios. It is shown that our interaction-aware prediction outperforms interaction-unaware physics- and map-based approaches.


Title: Persistent Anytime Learning of Objects from Unseen Classes
Key Words: image classification  random forests  random forest classifier  semantic mapping  object classification  standard offline methods  incremental approach  robotic applications  data samples  Training  Vegetation  Robots  Semantics  Standards  Training data  Uncertainty  Learning and Adaptive Systems  Object Detection  Segmentation and Categorization  Online Learning 
Abstract: We present a fast and very effective method for object classification that is particularly suited for robotic applications such as grasping and semantic mapping. Our approach is based on a Random Forest classifier that can be trained incrementally. This has the major benefit that semantic information from new data samples can be incorporated without retraining the entire model. Even if new samples from a previously unseen class are presented, our method is able to perform efficient updates and learn a sustainable representation for this new class. Further features of our method include a very fast and memory-efficient implementation, as well as the ability to interrupt the learning process at any time without a significant performance degradation. Experiments on benchmark data for robotic applications show the clear benefits of our incremental approach and its competitiveness with standard offline methods in terms of classification accuracy.


Title: Flamen − 7 DOF Robotic Arm to Manipulate a Spanish Fan
Key Words: control engineering computing  fans  manipulators  mobile robots  motion control  position control  7-DOF robotic arm  Flamen  Flamenco dancers  manipulation  traditional fan  Spanish fan  Fans  Manipulators  Robot kinematics  Actuators  Grasping  Cameras  Actuation  Automation  Background subtraction  Contour Detection  Coordinate extraction  Filtering  Mapping  Masking  Robotic Arm  Spanish Fan 
Abstract: A Spanish fan is a hand held traditional fan which is used as an accessory and also by Flamenco dancers. The manipulation of the fan is quite difficult as it involves dynamic motion which includes opening, flapping and closing the fan along a pivotal point. The key points include the motion to be quick and the fan to be opened to the maximum degree possible without human intervention. A robotic arm with 7 Degrees of Freedom (DOF) is used to manipulate the autonomous motion. The fan placed on the table is localized and detected using a camera by background subtraction, masking and filtering; post which the contour of the fan is detected. The pixels obtained is then transformed into real life coordinates. The Dynamixel motors then traverses to the coordinates of the fan's position to grasp, open, flap, close and put the fan down.


Title: Video Motion Capture from the Part Confidence Maps of Multi-Camera Images by Spatiotemporal Filtering Using the Human Skeletal Model
Key Words: cameras  image filtering  image motion analysis  image reconstruction  image sequences  spatiotemporal phenomena  video signal processing  video motion capture  part confidence maps  inverted motions  two-time inverse kinematics computations  human skeleton  human motion analysis  human motion data  spatiotemporal filter  camera image  human skeletal model  spatiotemporal filtering  multicamera images  Phase change materials  Three-dimensional displays  Cameras  Computational modeling  Optical imaging  Adaptive optics  Spatiotemporal phenomena 
Abstract: This paper discusses video motion capture, namely, 3D reconstruction of human motion from multi-camera images. After the Part Confidence Maps are computed from each camera image, the proposed spatiotemporal filter is applied to deliver the human motion data with accuracy and smoothness for human motion analysis. The spatiotemporal filter uses the human skeleton and mixes temporal smoothing in two-time inverse kinematics computations. The experimental results show that the mean per joint position error was 26.1mm for regular motions and 38.8mm for inverted motions.


Title: Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning
Key Words: convolutional neural nets  end effectors  learning (artificial intelligence)  motion control  neurocontrollers  learning synergies  self-supervised deep reinforcement learning  cluttered objects  pushing movements  model-free deep reinforcement learning  fully convolutional networks  end-effector orientations  Q-learning framework  pushing motions  grasping success rates  picking efficiencies  skilled robotic manipulation  grasping  prehensile action  pixel-wise sampling  Grasping  Training  Three-dimensional displays  Reinforcement learning  Planning  Manipulators 
Abstract: Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end-effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors even amid challenging cases of tightly packed clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu/


Title: The Socially Invisible Robot Navigation in the Social World Using Robot Entitativity
Key Words: human-robot interaction  multi-robot systems  navigation  path planning  simulated robot-human interaction scenarios  entitative robots  strong emotional reactions  socially invisible robot navigation  robot entitativity  data-driven algorithm  navigational algorithms  trajectory computation  multirobot systems  Trajectory  Navigation  Psychology  Computational modeling  Surveillance  Robot kinematics 
Abstract: We present a real-time, data-driven algorithm to enhance the social-invisibility of robots within crowds. Our approach is based on prior psychological research, which reveals that people notice and-importantly-react negatively to groups of social actors when they have high entitativity, moving in a tight group with similar appearances and trajectories. In order to evaluate that behavior, we performed a user study to develop navigational algorithms that minimize entitativity. This study establishes mapping between emotional reactions and multi-robot trajectories and appearances, and further generalizes the finding across various environmental conditions. We demonstrate the applicability of our entitativity modeling for trajectory computation for active surveillance and dynamic intervention in simulated robot-human interaction scenarios. Our approach empirically shows that various levels of entitative robots can be used to both avoid and influence pedestrians while not eliciting strong emotional reactions, giving multi-robot systems socially-invisibility.


Title: Probabilistic Collision Threat Assessment for Autonomous Driving at Road Intersections Inclusive of Vehicles in Violation of Traffic Rules
Key Words: belief networks  collision avoidance  decision making  mobile robots  probability  road safety  road traffic  road vehicles  traffic rules violation  vehicles road intersections inclusive  Bayesian networks  time window filtering  decision-making  in-vehicle testing  nonviolation vehicles  closed urban test road  violation vehicles  autonomous vehicle  probabilistic collision threat assessment algorithm  autonomous driving  Roads  Reliability  Principal component analysis  Probabilistic logic  Prediction algorithms  Autonomous vehicles 
Abstract: In this paper, we propose a probabilistic collision threat assessment algorithm for autonomous driving at road intersections that assesses a given traffic situation at an intersection reliably and robustly for an autonomous vehicle to cross the intersection safely, even in the face of violation vehicles (that is, vehicles in violation of traffic rules at the intersection). To this end, the proposed algorithm employs a detailed digital map to predict future paths of observed vehicles and then utilizes the predicted future paths to identify potential threats (vehicles) and potential collision areas, regardless of whether observed vehicles are obeying traffic rules at the intersection. Next, by means of Bayesian networks and time window filtering under an independent and distributed reasoning structure, it assesses the potential threats regarding the possibility of collision reliably and robustly, even under uncertain and incomplete noise data. Then, it has been tested and evaluated through in-vehicle testing on a closed urban test road under traffic conditions inclusive of non-violation and violation vehicles. In-vehicle testing results show that the performance of the proposed algorithm is sufficiently reliable to be used in decision-making for autonomous driving at intersections in terms of reliability and robustness, even in the face of violation vehicles.


Title: Search-Based Optimal Motion Planning for Automated Driving
Key Words: mobile robots  optimisation  path planning  road vehicles  search problems  trajectory control  automated driving  fast motion planning  robust motion planning  real-time computation  urban conditions  convenient geometrical representation  search space  driving constraints  classical path planning approach  exact cost-to-go map  optimal motion trajectory  time horizons  fast driving conditions  slow driving conditions  search-based optimal motion planning  Planning  Vehicle dynamics  Trajectory  Dynamics  Roads  Search problems  Automation  motion planning  automated driving  lane change  multi-lane driving  traffic lights  A* search  MPC 
Abstract: This paper presents a framework for fast and robust motion planning designed to facilitate automated driving. The framework allows for real-time computation even for horizons of several hundred meters and thus enabling automated driving in urban conditions. This is achieved through several features. Firstly, a convenient geometrical representation of both the search space and driving constraints enables the use of classical path planning approach. Thus, a wide variety of constraints can be tackled simultaneously (other vehicles, traffic lights, etc.). Secondly, an exact cost-to-go map, obtained by solving a relaxed problem, is then used by A*-based algorithm with model predictive flavour in order to compute the optimal motion trajectory. The algorithm takes into account both distance and time horizons. The approach is validated within a simulation study with realistic traffic scenarios. We demonstrate the capability of the algorithm to devise plans both in fast and slow driving conditions, even when full stop is required.


Title: Visual Vehicle Tracking Through Noise and Occlusions Using Crowd-Sourced Maps
Key Words: image motion analysis  image reconstruction  image segmentation  object detection  object tracking  traffic engineering computing  video signal processing  video surveillance  camera phones  performed city-scale structure-from-motion  high-accuracy localisation  unsupervised motion prediction  real-time visual tracking pipeline  monocular camera  large-scale datasets  New York City  perception system  large-scale crowd-sourced maps  visual vehicle tracking  location-specific method  Automobiles  Trajectory  Tracking  Three-dimensional displays  Cameras  Pipelines  Hidden Markov models 
Abstract: We present a location-specific method to visually track the positions of observed vehicles based on large-scale crowd-sourced maps. We equipped a large fleet of cars that drive around cities with camera phones mounted on the dashboard, and performed city-scale structure-from-motion to accurately reconstruct the trajectories taken by the vehicles. We show that these data can be used to first create a system enabling high-accuracy localisation, and then to accurately predict the future motion of newly observed cars in the camera view. As a basis for the method we use a recently proposed system [1] for unsupervised motion prediction and extend it to a real-time visual tracking pipeline which can track vehicles through noise and extended occlusions using only a monocular camera. The system is tested using two large-scale datasets of San Francisco and New York City containing millions of frames. We demonstrate the performance of the system in a variety of traffic, time, and weather conditions. The presented system requires no manual annotation or knowledge of road infrastructure. To our knowledge, this is the first time a perception system based on a large-scale crowd-sourced maps has been evaluated at this scale.


Title: Online Adaptation of Robot Pushing Control to Object Properties
Key Words: learning (artificial intelligence)  manipulators  mobile robots  path planning  unknown objects  data-driven approach  local inverse models  robot-object interaction  push manipulation  object behaviour  maximum a posteriori estimation  pushing objects  holonomic mobile robot base  diverse object set  learned inverse models  object properties  online adaptation  robot pushing control  robotic scenarios  real-world environments  MAP  Inverse problems  Adaptation models  Robot kinematics  Friction  Feedforward systems  Task analysis 
Abstract: Pushing is a common task in robotic scenarios. In real-world environments, robots need to manipulate various unknown objects without previous experience. We propose a data-driven approach for learning local inverse models of robot-object interaction for push manipulation. The robot makes observations of the object behaviour on the fly and adapts its movement direction. The proposed model is probabilistic, and we update it using maximum a posteriori (MAP) estimation. We test our method by pushing objects with a holonomic mobile robot base. Validation of results over a diverse object set demonstrates a high degree of robustness and a high success rate in pushing objects towards a fixed target and along a path compared to previous methods. Moreover, based on learned inverse models, the robot can learn object properties and distinguish between different object behaviours when they are pushed from different sides.


Title: Optimal Time Allocation for Quadrotor Trajectory Generation
Key Words: autonomous aerial vehicles  convex programming  helicopters  mobile robots  optimal control  polynomials  robot dynamics  trajectory control  optimal time allocation  quadrotor flights  quadrotor trajectory generation problem  spatial trajectory  time optimization  polynomial trajectories  quadrotor platform  kinodynamic limits  autonomous flights  open-source ROS-package  temporal trajectory  convex program  mapping function  Trajectory  Resource management  Safety  Optimization  Acceleration  Time-domain analysis  Shape 
Abstract: In this paper, we present a framework to do optimal time allocation for quadrotor trajectory generation. Using this method, we can generate minimum-time piecewise polynomial trajectories for quadrotor flights. We decouple the quadrotor trajectory generation problem into two folds. Firstly we generate a smooth and safe curve which is parameterized by a virtual variable. This curve named spatial trajectory is independent of time and has fixed spatial properties. Then a mapping function which decides how the quadrotor moves along the spatial trajectory respecting kinodynamic limits is found by minimizing total trajectory time. The mapping function maps the virtual variable to time is named temporal trajectory. We formulate the minimum-time temporal trajectory generation problem as a convex program which can be efficiently solved. We show that the proposed method can corporate with various types of previous trajectory generation method to obtain the optimal time allocation. The proposed method is integrated into a customized light-weight quadrotor platform and is validated by presenting autonomous flights in indoor and outdoor environments. We release our code for time optimization as an open-source ros-package.


Title: LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain
Key Words: embedded systems  feature extraction  image segmentation  optical radar  optimisation  pose estimation  robot vision  SLAM (robots)  SLAM framework  edge features  feature extraction  point cloud segmentation  lightweight and ground-optimized lidar odometry  real-time six degree-of-freedom pose estimation  low-power embedded system  ground plane  two-step Levenberg-Marquardt optimization method  optimization steps  ground vehicles  LeGO-LOAM  Feature extraction  Three-dimensional displays  Laser radar  Image segmentation  Pose estimation  Real-time systems  Iterative closest point algorithm 
Abstract: We propose a lightweight and ground-optimized lidar odometry and mapping method, LeGO-LOAM, for realtime six degree-of-freedom pose estimation with ground vehicles. LeGO-LOAM is lightweight, as it can achieve realtime pose estimation on a low-power embedded system. LeGO-LOAM is ground-optimized, as it leverages the presence of a ground plane in its segmentation and optimization steps. We first apply point cloud segmentation to filter out noise, and feature extraction to obtain distinctive planar and edge features. A two-step Levenberg-Marquardt optimization method then uses the planar and edge features to solve different components of the six degree-of-freedom transformation across consecutive scans. We compare the performance of LeGO-LOAM with a state-of-the-art method, LOAM, using datasets gathered from variable-terrain environments with ground vehicles, and show that LeGO-LOAM achieves similar or better accuracy with reduced computational expense. We also integrate LeGO-LOAM into a SLAM framework to eliminate the pose estimation error caused by drift, which is tested using the KITTI dataset.


Title: Hallucinating Robots: Inferring Obstacle Distances from Partial Laser Measurements
Key Words: collision avoidance  distance measurement  learning (artificial intelligence)  mobile robots  neural nets  optical scanners  hallucinating robots  obstacle distances  partial laser measurements  mobile robots  2D laser scanners  glass panels  richer sensor readings  RGBD sensors  raw 2D laser data  raw 2D laser distances  partial 2D laser readings  Lasers  Two dimensional displays  Measurement by laser beam  Robot sensing systems  Neural networks 
Abstract: Many mobile robots rely on 2D laser scanners for localization, mapping, and navigation. However, those sensors are unable to correctly provide distance to obstacles such as glass panels and tables whose actual occupancy is invisible at the height the sensor is measuring. In this work, instead of estimating the distance to obstacles from richer sensor readings such as 3D lasers or RGBD sensors, we present a method to estimate the distance directly from raw 2D laser data. To learn a mapping from raw 2D laser distances to obstacle distances we frame the problem as a learning task and train a neural network formed as an autoencoder. A novel configuration of network hyperparameters is proposed for the task at hand and is quantitatively validated on a test set. Finally, we qualitatively demonstrate in real time on a Care-O-bot 4 that the trained network can successfully infer obstacle distances from partial 2D laser readings.


Title: Laser Map Aided Visual Inertial Localization in Changing Environment
Key Words: cameras  geometry  optical radar  optimisation  robot vision  SLAM (robots)  map optimization  changing environment  bi-directional tasks  LiDAR-built map  online visual inertial odometry system  laser map aided visual inertial localization  geometry information  crossmodal data association  multisession laser  Visualization  Lasers  Bundle adjustment  Laser radar  Robots  Cameras 
Abstract: Long-term visual localization in outdoor environment is a challenging problem, especially faced with the cross-seasonal, bi-directional tasks and changing environment. In this paper we propose a novel visual inertial localization framework that localizes against the LiDAR-built map. Based on the geometry information of the laser map, a hybrid bundle adjustment framework is proposed, which estimates the poses of the cameras with respect to the prior laser map as well as optimizes the state variables of the online visual inertial odometry system simultaneously. For more accurate crossmodal data association, the laser map is optimized using multisession laser and visual data to extract the salient and stable subset for visual localization. To validate the efficiency of the proposed method, we collect data in south part of our campus in different seasons, along the same and opposite-direction route. In all sessions of localization data, our proposed method gives satisfactory results, and shows the superiority of the hybrid bundle adjustment and map optimization1.


Title: Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map
Key Words: feature extraction  optical radar  robot vision  SLAM (robots)  stereo image processing  simultaneous localization and mapping  scan context performance  Light Detection and Ranging scans  visual scenes  two-phase search algorithm  3D LiDAR scans  loop-detection invariant  nonhistogram-based global descriptor  global localization  diverse sensors  dense 3D maps  structural information  diverse feature detectors  3D point cloud map  place recognition  Three-dimensional displays  Sensors  Laser radar  Histograms  Shape  Visualization  Encoding 
Abstract: Compared to diverse feature detectors and descriptors used for visual scenes, describing a place using structural information is relatively less reported. Recent advances in simultaneous localization and mapping (SLAM) provides dense 3D maps of the environment and the localization is proposed by diverse sensors. Toward the global localization based on the structural information, we propose Scan Context, a non-histogram-based global descriptor from 3D Light Detection and Ranging (LiDAR) scans. Unlike previously reported methods, the proposed approach directly records a 3D structure of a visible space from a sensor and does not rely on a histogram or on prior training. In addition, this approach proposes the use of a similarity score to calculate the distance between two scan contexts and also a two-phase search algorithm to efficiently detect a loop. Scan context and its search algorithm make loop-detection invariant to LiDAR viewpoint changes so that loops can be detected in places such as reverse revisit and corner. Scan context performance has been evaluated via various benchmark datasets of 3D LiDAR scans, and the proposed method shows a sufficiently improved performance.


Title: Safe Motion Planning for Steerable Needles Using Cost Maps Automatically Extracted from Pulmonary Images
Key Words: blood vessels  cancer  computerised tomography  feature extraction  lung  medical image processing  needles  lung nodule biopsy  steerable needles  bronchoscope  bronchial tubes  blood vessels  safe motion planning  motion planning approach  lung pleura  pulmonary CT images  cost map  lung periphery  lung nodules  needle biopsy  lung cancer  Needles  Lung  Biomedical imaging  Planning  Biopsy  Computed tomography  Blood vessels 
Abstract: Lung cancer is the deadliest form of cancer, and early diagnosis is critical to favorable survival rates. Definitive diagnosis of lung cancer typically requires needle biopsy. Common lung nodule biopsy approaches either carry significant risk or are incapable of accessing large regions of the lung, such as in the periphery. Deploying a steerable needle from a bronchoscope and steering through the lung allows for safe biopsy while improving the accessibility of lung nodules in the lung periphery. In this work, we present a method for extracting a cost map automatically from pulmonary CT images, and utilizing the cost map to efficiently plan safe motions for a steerable needle through the lung. The cost map encodes obstacles that should be avoided, such as the lung pleura, bronchial tubes, and large blood vessels, and additionally formulates a cost for the rest of the lung which corresponds to an approximate likelihood that a blood vessel exists at each location in the anatomy. We then present a motion planning approach that utilizes the cost map to generate paths that minimize accumulated cost while safely reaching a goal location in the lung.


Title: Development and Evaluation of an Intuitive Flexible Interface for Teleoperating Soft Growing Robots
Key Words: bending  mobile robots  path planning  service robots  telerobotics  user interfaces  intuitive flexible interface  teleoperating soft growing robots  robotic systems design  tip-extending  navigation  disaster scenarios  intuitive human control  intuitively map human bending  shape information  command mappings  developed interface  commercially available interfaces  virtual task scenarios  shape mapping  vine robot rolls  Shape  Robot sensing systems  Robot kinematics  Three-dimensional displays  Kinematics  Current measurement 
Abstract: Mobility by growth is a new paradigm in robotic systems design and their applications in the real world. Soft, tip-extending, or “growing”, robots have potential applications including inspection and navigation in disaster scenarios. However, due to their growing capability, such robots create unique challenges for intuitive human control. In this paper, a new flexible interface is proposed to intuitively map human bending commands into movements of the growing robot while providing shape information of the robot in order to improve situational awareness. Several command mappings are proposed, and a subjective study was conducted to assess the intuitiveness of the developed interface and mappings compared with other commercially available interfaces. The interfaces were evaluated using four metrics in two virtual task scenarios. The proposed interface with shape mapping performed better than the other interfaces, especially when the vine robot rolls over unintentionally during complex tasks.


Title: Visual-Inertial Teach and Repeat Powered by Google Tango
Key Words: automatic optical inspection  autonomous aerial vehicles  collision avoidance  control engineering computing  Global Positioning System  mobile robots  pose estimation  robot vision  trajectory control  human operator  visual inspection task  autonomous aerial vehicle  Google Tango visual-inertial mapping framework  pose estimates  GPS-denied environments  inspection points  feature-based localization map  industrial facilities  multicopters  visual-inertial teach  hedge maze  Robots  Inspection  Task analysis  Collision avoidance  Google  Autonomous systems  Visualization 
Abstract: Many industrial facilities require periodic visual inspections. Often the points of interest are out of reach or in potentially hazardous environment. Multi-copters are ideal platforms to automate this expensive and tedious task. This video presents a system that enables a human operator to teach a visual inspection task to an autonomous aerial vehicle by simply demonstrating the task using a tablet. The system employs the Google Tango visual-inertial mapping framework as the only source of pose estimates, thus enabling operation in GPS-denied environments. In a first step the operator records the desired inspection path using the tablet. Inspection points are automatically inserted if the operator pauses, holding a viewpoint. The mapping framework then computes a feature-based localization map, which is shared with the robot. After take-off, the robot estimates its pose based on this map and plans a smooth trajectory through the way points defined by the operator. Furthermore, the system is able to track the global pose of other robots or the operator, localized in the same map, and follow them in real-time, while avoiding collision. This was demonstrated in the second part of the video, where the robot is following the operator in real-time through a hedge maze.


Title: Learning Forward and Inverse Kinematics Maps Efficiently
Key Words: actuators  control engineering computing  elasticity  learning (artificial intelligence)  manipulator kinematics  nonparametric statistics  learning forward kinematics maps  exploratory learning approaches  action-outcome sampling  omnielastic manipulators  rigid manipulators  inverse kinematics mappings  nonparametric models  elastic discretely-actuated robots  rigid discretely-actuated robots  tailored parametric models  data-efficiency  Manipulators  Kinematics  Solid modeling  Elasticity  Analytical models  Strain 
Abstract: When learning forward and inverse kinematics maps of manipulators, usually little attention is paid to data-efficiency, i.e., the accuracy gained per action-outcome sample. This paper examines properties of popular (online) learning techniques and demonstrates that - regardless of the employed exploration strategy - the structure of kinematics mappings does not allow for a practically viable trade-off between the number of samples and the resulting approximation error for manipulators with more than a few DoFs - unless tailored parametric models are employed. We discuss suitable choices for these parametric models for both rigid and elastic discretely-actuated robots and compare their data -efficiency to that of popular exploratory learning approaches relying on non-parametric models. Our theoretical considerations are confirmed by various experimental results for inverse kinematics mappings of rigid and omnielastic manipulators.


Title: Classification of Hanging Garments Using Learned Features Extracted from 3D Point Clouds
Key Words: clothing  computer graphics  control engineering computing  convolutional neural nets  feature extraction  image classification  manipulators  neurocontrollers  robot vision  service robots  support vector machines  3D objects  feature vector extraction  t-shirts  hanging garments classification  3D point clouds  SVM  generalized convolution operation  single global feature vector  convolutional neural network  depth maps  robotic arm  hanging state  robotic manipulation  garment category  Clothing  Three-dimensional displays  Feature extraction  Robot sensing systems  Convolution  Image reconstruction 
Abstract: The presented work deals with classification of garment categories including pants, shorts, shirts, T-shirts and towels. The knowledge of the garment category is crucial for its robotic manipulation. Our work focuses particularly on garments being held in a hanging state by a robotic arm. The input of our method is a set of depth maps taken from different viewpoints around the garment. The depths are fused into a single 3D point cloud. The cloud is fed into a convolutional neural network that transforms it into a single global feature vector. The network utilizes a generalized convolution operation defined over the local neighborhood of a point. It can deal with permutations of the input points. It was trained on a large dataset of common 3D objects. The extracted feature vector is classified with SVM trained on smaller datasets of garments. The proposed method was evaluated on publicly available data and compared to the original methods, achieving competitive performance and better generalization capability.


Title: Should We Compete or Should We Cooperate? Applying Game Theory to Task Allocation in Drone Swarms
Key Words: game theory  preferred task allocations  competitive algorithm  game theoretical algorithms  described scenario  relevant question  partial information  disaster area  drone swarms  task allocation  game theory  Task analysis  Robots  Resource management  Games  Drones  Nash equilibrium  Genetic algorithms 
Abstract: Let's imagine a swarm of drones that has to visit some locations and build a map in a disaster area. Let's assume the drones only can communicate to their neighbors and manage partial information of the mission. A relevant question in this scenario is “Should the robots compete or should they cooperate?”. This work analyzes the described scenario to answer this question. Two game theoretical algorithms have been developed: one competitive and another cooperative. The competitive algorithm poses games among each drone and its neighbors and searches the Nash Equilibrium. The cooperative one defines electoral systems that allow the drones to vote their preferred task allocations for their neighbors. Both algorithms are extensively tested in multiple scenarios with different features. After the experiments the question can be answered “The robots should cooperate!”.


Title: Any-Time Trajectory Planning for Safe Emergency Landing
Key Words: aerospace components  aerospace engineering  aircraft control  aircraft landing guidance  path planning  trajectory control  landing site selection  safest emergency landing trajectory  multiple landing sites  any-time property  time trajectory planning  safe emergency landing  critical situation  human pilots  landing trajectories  aircraft  Trajectory  Aircraft  Planning  Turning  Drag  Atmospheric modeling  Force 
Abstract: Loss of thrust is a critical situation for human pilots of fixed-wing aircraft which force them to select a landing site in the nearby range and perform an emergency landing. The time for the landing site selection is limited by the actual altitude of the aircraft, and it may be fatal if the correct decision is not chosen fast enough. Therefore, we propose a novel RRT* -based planning algorithm for finding the safest emergency landing trajectory towards a given set of possible landing sites. Multiple landing sites are evaluated simultaneously during the flight even before any mechanical issue occurs, and the roadmap of possible landing trajectories is updated permanently. Thus, the proposed algorithm has the any-time property and provides the best emergency landing trajectory almost instantly.


Title: Joint 3D Proposal Generation and Object Detection from View Aggregation
Key Words: image classification  image colour analysis  image fusion  mobile robots  neural nets  object detection  optical radar  radar detection  regression analysis  road vehicle radar  robot vision  high resolution feature maps  reliable 3D object proposals  multiple object classes  category classification  second stage detection network  AVOD  KITTI 3D object detection  autonomous vehicles  3D bounding box regression  multimodal feature fusion  RPN  region proposal network  RGB images  LIDAR point clouds  neural network architecture  autonomous driving scenarios  Aggregate View Object Detection network  joint 3D proposal generation  Three-dimensional displays  Feature extraction  Proposals  Computer architecture  Agriculture  Object detection  Two dimensional displays 
Abstract: We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is available at: https://github.com/kujason/avod.


Title: TSSD: Temporal Single-Shot Detector Based on Attention and LSTM
Key Words: feature extraction  object detection  robot vision  video signal processing  convolutional long short-term memory  creative temporal analysis unit  multiscale feature maps  high-level ConvLSTM unit  pyramidal feature hierarchy  attention mechanism  real-time online approaches  video detection task  robotic vision  rich temporal information  temporal object detection  temporal single-shot detector  developed TSSD  attention-aware features  scale suppression  background suppression  ConvLSTM-based attention  attention-based ConvLSTM  Feature extraction  Detectors  Robots  Task analysis  Visualization  Lenses  Proposals 
Abstract: Temporal object detection has attracted significant attention, but most popular methods can not leverage the rich temporal information in video or robotic vision. Although many different algorithms have been developed for video detection task, real-time online approaches are frequently deficient. In this paper, based on attention mechanism and convolutional long short-term memory (ConvLSTM), we propose a temporal single-shot detector (TSSD)for robotic vision. Distinct from previous methods, we aim to temporally integrate pyramidal feature hierarchy using ConvLSTM, and design a novel structure including a high-level ConvLSTM unit as well as a low-level one (HL-LSTM)for multi-scale feature maps. Moreover, we develop a creative temporal analysis unit, namely, ConvLSTM-based attention and attention-based ConvLSTM (A&CL), in which the ConvLSTM-based attention is specially tailored for background suppression and scale suppression while the attention-based ConvLSTM temporally integrates attention-aware features. Finally, our method is evaluated on ImageNet VID dataset. Extensive comparisons on detection performance confirm the superiority of the proposed approach, and the developed TSSD achieves a considerably enhanced accuracy vs. speed trade-off, i.e., 64.8% mAP vs. 27 FPS.


Title: Obstacle Detection for USVs by Joint Stereo-View Semantic Segmentation
Key Words: cameras  collision avoidance  control engineering computing  convolutional neural nets  edge detection  image segmentation  mobile robots  remotely operated vehicles  robot vision  stereo image processing  water edge  stereo extensions  joint stereo-view semantic segmentation  unmanned surface vehicles  scene semantic segmentation problem  single-view model  consistent class labels assignment  monocular CNN  class-label posterior map  stereo-based obstacle detection  Semantics  Image segmentation  Cameras  Image edge detection  Sea surface  Graphical models  Three-dimensional displays 
Abstract: We propose a stereo-based obstacle detection approach for unmanned surface vehicles. Obstacle detection is cast as a scene semantic segmentation problem in which pixels are assigned a probability of belonging to water or non-water regions. We extend a single-view model to a stereo system by adding a constraint which prefers consistent class labels assignment to pixels in the left and right camera images corresponding to the same parts of a 3D scene. Our approach jointly fits a semantic model to both images, leading to an improved class-label posterior map from which obstacles and water edge are extracted. In overall F-measure, our approach outperforms the current state-of-the-art monocular approach by 0.495, a monocular CNN by 0.798 and their stereo extensions by 0.059 and 0.515, respectively on the task of obstacle detection while running real-time on a single CPU.


Title: Stereo Camera Localization in 3D LiDAR Maps
Key Words: cameras  Global Positioning System  image matching  image reconstruction  mobile robots  optical radar  pose estimation  robot vision  SLAM (robots)  stereo image processing  stereo disparity map  average localization error  stereo camera localization  Global Positioning System  3D LiDAR maps  simultaneous localization and mapping techniques  SLAM techniques  3D light detection and ranging sensors  visual positioning algorithm  GPS signal  visual tracking  six degree of freedom  DOF  camera pose estimation  KITTI dataset  Cameras  Three-dimensional displays  Laser radar  Simultaneous localization and mapping  Visualization  Global Positioning System 
Abstract: As simultaneous localization and mapping (SLAM) techniques have flourished with the advent of 3D Light Detection and Ranging (LiDAR) sensors, accurate 3D maps are readily available. Many researchers turn their attention to localization in a previously acquired 3D map. In this paper, we propose a novel and lightweight camera-only visual positioning algorithm that involves localization within prior 3D LiDAR maps. We aim to achieve the consumer level global positioning system (GPS) accuracy using vision within the urban environment, where GPS signal is unreliable. Via exploiting a stereo camera, depth from the stereo disparity map is matched with 3D LiDAR maps. A full six degree of freedom (DOF) camera pose is estimated via minimizing depth residual. Powered by visual tracking that provides a good initial guess for the localization, the proposed depth residual is successfully applied for camera pose estimation. Our method runs online, as the average localization error is comparable to ones resulting from state-of-the-art approaches. We validate the proposed method as a stand-alone localizer using KITTI dataset and as a module in the SLAM framework using our own dataset.


Title: Vision-Based Terrain Classification and Solar Irradiance Mapping for Solar-Powered Robotics
Key Words: cameras  energy harvesting  feature extraction  Haar transforms  image classification  image colour analysis  image texture  mobile robots  neural nets  robot vision  solar power  terrain mapping  wavelet transforms  outdoor mobile robots  feature extraction  visual-spectrum images  on-board camera  Haar wavelet transform  color information  textural information  ANN  high dynamic range imagery  energy consumption  traversability criteria  energy harvesting capabilities  vision-based artificial neural network  sequential methodology  solar irradiance map  terrain classes  solar-powered mobile robots  real-time terrain classification  solar irradiance mapping  vision-based terrain classification  Image color analysis  Feature extraction  Neural networks  Training  Image segmentation  Wavelet transforms  Sensors  Field Robotics  Image Processing  Solar Mapping  Terrain Classification  Solar Robotics 
Abstract: This paper examines techniques for real-time terrain classification and solar irradiance mapping for outdoor, solar-powered mobile robots using a vision-based Artificial Neural Network (ANN). This process is completed sequentially. First, terrain classification is completed by extracting key features from visual-spectrum images captured from an on-board camera using Haar wavelet transform to identify both color and textural information. These features are then classified using an ANN to identify grass, concrete, asphalt, gravel, and mulch. Using the terrain classes, the image is then analyzed using concepts from high dynamic range imagery to establish the solar irradiance map of the area. In this way, our sequential methodology presented allows unmanned vehicles to classify the terrain and map the irradiance of a given area with no prior knowledge. Whereas, the terrain classification can be used in determining energy consumption or traversability criteria and the irradiance map can be used to estimate the energy harvesting capabilities.


Title: Towards Real-Time Unsupervised Monocular Depth Estimation on CPU
Key Words: embedded systems  estimation theory  feature extraction  image reconstruction  image sensors  learning (artificial intelligence)  microprocessor chips  mobile robots  object detection  robot vision  stereo image processing  robotic navigation  autonomous navigation  deep learning  low-power constraints  embedded system  single input image  image reconstruction problem  KITTI image  depth map  CPU  unsupervised monocular depth estimation  features extraction  time 1.7 s  frequency 8.0 Hz  frequency 40.0 Hz  Estimation  Feature extraction  Computer architecture  Training  Decoding  Image resolution  Real-time systems 
Abstract: Unsupervised depth estimation from a single image is a very attractive technique with several implications in robotic, autonomous navigation, augmented reality and so on. This topic represents a very challenging task and the advent of deep learning enabled to tackle this problem with excellent results. However, these architectures are extremely deep and complex. Thus, real-time performance can be achieved only by leveraging power-hungry GPUs that do not allow to infer depth maps in application fields characterized by low-power constraints. To tackle this issue, in this paper we propose a novel architecture capable to quickly infer an accurate depth map on a CPU, even of an embedded system, using a pyramid of features extracted from a single input image. Similarly to state-of-the-art, we train our network in an unsupervised manner casting depth estimation as an image reconstruction problem. Extensive experimental results on the KITTI dataset show that compared to the top performing approach our network has similar accuracy but a much lower complexity (about 6% of parameters) enabling to infer a depth map for a KITTI image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard CPU. Moreover, by trading accuracy for efficiency, our network allows to infer maps at about 2 Hz and 40 Hz respectively, still being more accurate than most state-of-the-art slower methods. To the best of our knowledge, it is the first method enabling such performance on CPUs paving the way for effective deployment of unsupervised monocular depth estimation even on embedded systems.


Title: Adversarial Learning-Based On-Line Anomaly Monitoring for Assured Autonomy
Key Words: learning (artificial intelligence)  object detection  remotely operated vehicles  indoor environments  Udacity dataset  image conditioned energy based generative adversarial network  on-line monitoring framework  assured autonomy  Adversarial Learning-Based On-Line Anomaly Monitoring  autonomous ground vehicle  sensor data  action condition video prediction framework  anomalous actuator commands  proper actuator commands  generative adversarial network  SFAM  system-focused anomaly detection  CFAM  controller-focused anomaly detection  sensor inputs  unmanned ground vehicle  learning-based control systems  Generators  Convolution  Actuators  Monitoring  Anomaly detection  Robot sensing systems  Computer architecture 
Abstract: The paper proposes an on-line monitoring framework for continuous real-time safety/security in learning-based control systems (specifically application to a unmanned ground vehicle). We monitor validity of mappings from sensor inputs to actuator commands, controller-focused anomaly detection (CFAM), and from actuator commands to sensor inputs, system-focused anomaly detection (SFAM). CFAM is an image conditioned energy based generative adversarial network (EBGAN) in which the energy based discriminator distinguishes between proper and anomalous actuator commands. SFAM is based on an action condition video prediction framework to detect anomalies between predicted and observed temporal evolution of sensor data. We demonstrate the effectiveness of the approach on our autonomous ground vehicle for indoor environments and on Udacity dataset for outdoor environments.


Title: DROAN - Disparity-Space Representation for Obstacle Avoidance: Enabling Wire Mapping & Avoidance
Key Words: collision avoidance  graph theory  image segmentation  image sensors  mobile robots  motion control  neural nets  robot vision  stereo image processing  multiple disparity images  C-space expansion  disparity space representation  generic obstacles  wire pixels  confidence map  semantic segmentation paradigm  convolutional neural network  monocular wire detection  generic obstacle avoidance  robust autonomous aerial vehicles  depth estimation  DROAN - disparity-space representation  Wires  Robot sensing systems  Three-dimensional displays  Cameras  Trajectory  Uncertainty 
Abstract: Wire detection, depth estimation and avoidance is one of the hardest challenges towards the ubiquitous presence of robust autonomous aerial vehicles. We present an approach and a system which tackles these three challenges along with generic obstacle avoidance as well. First, we perform monocular wire detection using a convolutional neural network under the semantic segmentation paradigm, and obtain a confidence map of wire pixels. Along with this, we also use a binocular stereo pair to detect other generic obstacles. We represent wires and generic obstacles using a disparity space representation and do a C-space expansion by using a non-linear sensor model we develop. Occupancy inference for collision checking is performed by maintaining a pose graph over multiple disparity images. For avoidance of wire and generic obstacles, we use a precomputed trajectory library, which is evaluated in an online fashion in accordance to a cost function over proximity to the goal. We follow this trajectory with a path tracking controller. Finally, we demonstrate the effectiveness of our proposed method in simulation for wire mapping, and on hardware by multiple runs for both wire and generic obstacle avoidance.


Title: Appearance-Based Along-Route Localization for Planetary Missions
Key Words: image matching  image registration  image sequences  mobile robots  object recognition  planetary rovers  robot vision  SLAM (robots)  image preprocessing steps  recognition framework SeqSLAM  appearance-based along-route localization algorithm  planetary missions  direct sequence-based approach  Moon-analogue mission  planetary rover  image similarity metrics wrt  translational viewpoint differences  rotational viewpoint differences  route traversal conditions  matching locations  flexible mechanism  frame matches  homing mechanism  autonomous navigation  real-time localization  individual frames  image sequences  robust place recognition  Navigation  Lighting  Cameras  Moon  Simultaneous localization and mapping  Visualization  mobile robotics  field robotics  place-recognition  autonomous route navigation 
Abstract: We propose an appearance-based along-route localization algorithm that relies on robust place recognition by matching image sequences instead of individual frames. Our approach extends state of the art place recognition framework SeqSLAM in several aspects to realize real-time localization along routes for autonomous navigation. First, our method is online in that we only rely on the recently observed image frames. Second, we provide a homing mechanism based on rotations computed from frame matches. And third, we use a more flexible mechanism to search for matching locations, not restricting the search to straight lines in the cost matrix as in SeqSLAM, but allowing for a wide variety of route traversal conditions such as varying velocities or rotational and translational viewpoint differences. We investigate different image preprocessing steps as well as image similarity metrics wrt. their influence on illumination and viewpoint invariance for a more robust place recognition. On a new challenging dataset, recorded in real world experiments with a planetary rover, in the course of a Moon-analogue mission on Sicily's Mount Etna, we show the feasibility of our direct, sequence-based approach to along-route localization.


Title: A Monocular Indoor Localiser Based on an Extended Kalman Filter and Edge Images from a Convolutional Neural Network
Key Words: cameras  convolutional neural nets  edge detection  image fusion  Kalman filters  mobile robots  nonlinear filters  pose estimation  robot vision  SLAM (robots)  camera location estimation  extended Kalman filter  6 DOF pose estimation  visual simultaneous localisation-and-mapping algorithms  prebuilt map  ground plane edge image extraction  motion model  unsigned distance function  indoor environment  monocular images  monocular indoor localiser  EKF framework  CNN  convolutional neural network  Image edge detection  Cameras  Robot vision systems  Feature extraction  Convolution  Image segmentation 
Abstract: The main contribution of this paper is an extended Kalman filter (EKF)based algorithm for estimating the 6 DOF pose of a camera using monocular images of an indoor environment. In contrast to popular visual simultaneous localisation and mapping algorithms, the technique proposed relies on a pre-built map represented as an unsigned distance function of the ground plane edges. Images from the camera are processed using a Convolutional Neural Network (CNN)to extract a ground plane edge image. Pixels that belong to these edges are used in the observation equation of the EKF to estimate the camera location. Use of the CNN makes it possible to extract ground plane edges under significant changes to scene illumination. The EKF framework lends itself to use of a suitable motion model, fusing information from any other sensors such as wheel encoders or inertial measurement units, if available, and rejecting spurious observations. A series of experiments are presented to demonstrate the effectiveness of the proposed technique.


Title: Automated Map Reading: Image Based Localisation in 2-D Maps Using Binary Semantic Descriptors
Key Words: cartography  image classification  image matching  image representation  visual databases  compact binary descriptors  localisation accuracy  2-D map  location tagged descriptors  descriptor estimates  human-system interaction  human map reading  variable imaging conditions  semantic features  image database matching  2-D cartographic map  semantic matching  binary semantic descriptors  image based localisation  automated map reading  Semantics  Buildings  Junctions  Roads  Databases  Feature extraction  Meters 
Abstract: We describe a novel approach to image based localisation in urban environments which uses semantic matching between images and a 2-D cartographic map. This contrasts with the majority of existing approaches which use image to image database matching. We use highly compact binary descriptors to represent locations, indicating the presence or not of semantic features, which significantly increases scalability and has the potential for greater invariance to variable imaging conditions. The approach is also more akin to human map reading, making it better suited to human-system interaction. In this initial study we use semantic features relating to buildings and road junctions in discrete viewing directions. CNN classifiers are used to detect the features in images and we match descriptor estimates with location tagged descriptors derived from the 2-D map to give localisation. The descriptors are not sufficiently discriminative on their own, but when concatenated sequentially along a route, their combination becomes highly distinctive and allows localisation even when using non-perfect classifiers. Performance is further improved by taking into account left or right turns over a route. Experimental results obtained using Google StreetView and OpenStreetMap data show that the approach has considerable potential, achieving localisation accuracy of around 85% using routes corresponding to approximately 200 meters.


Title: Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality
Key Words: augmented reality  calibration  cameras  human-robot interaction  image registration  image sensors  inspection  mobile robots  robot vision  SLAM (robots)  stereo image processing  mixed-reality headsets  headset orientation  structure inspection  marker-free self-localization  onboard depth sensor  simple point cloud registration  camera image  inspection information  joint point cloud and image-based localization  JPIL  human-robot interaction  time 20.0 min  Three-dimensional displays  Headphones  Inspection  Cameras  Virtual reality  Solid modeling  Robot sensing systems 
Abstract: This paper introduces a method of structure inspection using mixed-reality headsets to reduce the human effort in reporting accurate inspection information such as fault locations in 3D coordinates. Prior to every inspection, the headset needs to be localized. While external pose estimation and fiducial marker based localization would require setup, maintenance, and manual calibration; marker-free self-localization can be achieved using the onboard depth sensor and camera. However, due to limited depth sensor range of portable mixed-reality headsets like Microsoft HoloLens, localization based on simple point cloud registration (sPCR) would require extensive mapping of the environment. Also, localization based on camera image would face same issues as stereo ambiguities and hence depends on viewpoint. We thus introduce a novel approach to Joint Point Cloud and Image-based Localization (JPIL) for mixed-reality headsets that uses visual cues and headset orientation to register small, partially overlapped point clouds and save significant manual labor and time in environment mapping. Our empirical results compared to sPCR show average 10 fold reduction of required overlap surface area that could potentially save on average 20 minutes per inspection. JPIL is not only restricted to inspection tasks but also can be essential in enabling intuitive human-robot interaction for spatial mapping and scene understanding in conjunction with other agents like autonomous robotic systems that are increasingly being deployed in outdoor environments for applications like structural inspection.


Title: Probabilistic Dense Reconstruction from a Moving Camera
Key Words: cameras  image colour analysis  image reconstruction  image sequences  probability  SLAM (robots)  stereo image processing  TUM RGB-D SLAM  ICL-NUIM dataset  spatial correlations  visual scale changes  insufficient parallaxes  motion stereo  spatial stereo  single monocular camera  online dense reconstruction  probabilistic approach  moving camera  probabilistic dense reconstruction  outdoor experiments  dense 3D models  inlier probability expectations  depth estimates  probabilistic scheme  monocular depth estimation  temporal correlations  Image reconstruction  Cameras  Estimation  Visualization  Robot vision systems  Probabilistic logic  Simultaneous localization and mapping 
Abstract: This paper presents a probabilistic approach for online dense reconstruction using a single monocular camera moving through the environment. Compared to spatial stereo, depth estimation from motion stereo is challenging due to insufficient parallaxes, visual scale changes, pose errors, etc. We utilize both the spatial and temporal correlations of consecutive depth estimates to increase the robustness and accuracy of monocular depth estimation. An online, recursive, probabilistic scheme to compute depth estimates, with corresponding covariances and inlier probability expectations, is proposed in this work. We integrate the obtained depth hypotheses into dense 3D models in an uncertainty-aware way. We show the effectiveness and efficiency of our proposed approach by comparing it with state-of-the-art methods in the TUM RGB-D SLAM & ICL-NUIM dataset. Online indoor and outdoor experiments are also presented for performance demonstration.


Title: Summarizing Large Scale 3D Mesh
Key Words: mesh generation  mobile robots  robot vision  SLAM (robots)  stereo image processing  vision-based summarizing process  HD 3D maps  large-scale 3D map  semantic information  geometric information  photometric information  autonomous navigation  semantic mapping  3D sensor devices  Three-dimensional displays  Navigation  Entropy  Semantics  Visualization  Optimization  Robots 
Abstract: Recent progress in 3D sensor devices and in semantic mapping allows to build very rich HD 3D maps very useful for autonomous navigation and localization. However, these maps are particularly huge and require important memory capabilities as well computational resources. In this paper, we propose a new method for summarizing a 3D map (Mesh)as a set of compact spheres in order to facilitate its use by systems with limited resources (smartphones, robots, UAVs,...). This vision-based summarizing process is applied in a fully automatic way using jointly photometric, geometric and semantic information of the studied environment. The main contribution of this research is to provide a very compact map that maximizes the significance of its content while maintaining the full visibility of the environment. Experimental results in summarizing large-scale 3D map demonstrate the feasibility of our approach and evaluate the performance of the algorithm.


Title: Deep Sequential Models for Sampling-Based Planning
Key Words: collision avoidance  computational geometry  learning (artificial intelligence)  mobile robots  multi-agent systems  path planning  sampling methods  deep sequential models  sequence model  sampling-based planner  efficient plans  planner state  neural-network-based models  fewer rejected samples  multiagent environments  graphical models  Hidden Markov models  Computational modeling  Planning  Adaptation models  Space exploration  Uncertainty  Sensors 
Abstract: We demonstrate how a sequence model and a sampling-based planner can influence each other to produce efficient plans and how such a model can automatically learn to take advantage of observations of the environment. Sampling-based planners such as RRT generally know nothing of their environments even if they have traversed similar spaces many times. A sequence model, such as an HMM or LSTM, guides the search for good paths. The resulting model, called DeRRT*, observes the state of the planner and the local environment to bias the next move and next planner state. The neural-network-based models avoid manual feature engineering by co-training a convolutional network which processes map features and observations from sensors. We incorporate this sequence model in a manner that combines its likelihood with the existing bias for searching large unexplored Voronoi regions. This leads to more efficient trajectories with fewer rejected samples even in difficult domains such as when escaping bug traps. This model can also be used for dimensionality reduction in multi-agent environments with dynamic obstacles. Instead of planning in a high-dimensional space that includes the configurations of the other agents, we plan in a low-dimensional subspace relying on the sequence model to bias samples using the observed behavior of the other agents. The techniques presented here are general, include both graphical models and deep learning approaches, and can be adapted to a range of planners.


Title: A Topology-Based Path Similarity Metric and its Application to Sampling-Based Motion Planning
Key Words: mobile robots  path planning  sampling methods  topology  homotopic similarity  homotopy equivalence  naive application  local planning  sampling-based motion planning  topologically distinct portions  topologically distinct paths  robotic motion planning  homotopy classes  topology-based path similarity metric  path deformation roadmaps  multiple homotopically distinct paths  Measurement  Planning  Strain  Algorithms  Merging  Manipulators 
Abstract: Many applications of robotic motion planning benefit from considering multiple homotopically distinct paths rather than a single path from start to goal. However, determining whether paths represent different homotopy classes can be difficult to compute. We propose metrics for efficiently approximating the homotopic similarity of two paths are, instead of verifying homotopy equivalence directly. We propose two metrics: (1) a naive application of local planning, a common subroutine of sampling-based motion planning, and (2) a novel approach that reasons about the topologically distinct portions of the workspace that a path visits. We present three applications of our metric to demonstrate its use and effectiveness: extracting topologically distinct paths from an existing roadmap, comparing paths for robot manipulators, and improving the computational efficiency of an existing sampling-based method, Path Deformation Roadmaps (PDRs), by over two orders of magnitude. We explore the trade-off between quality and computational efficiency in the proposed metrics.


Title: Real-Time Motion Planning in Changing Environments Using Topology-Based Encoding of Past Knowledge
Key Words: collision avoidance  encoding  graph theory  mobile robots  reachability analysis  topology  approximate Reeb graph  BKPIECE algorithms  topology-based encoding  trajectory planning  complex environments  DRM-connect algorithm  dynamic reachability maps  lazy collision checking  fallback strategy  RRT-connect algorithm  sparser roadmaps  motion planning  changing environments  Task analysis  Trajectory  Planning  Heuristic algorithms  Robots  Topology  Maintenance engineering 
Abstract: Trajectory planning and replanning in complex environments often reuses very little information from the previous solutions. This is particularly evident when the motion is repeated multiple times with only a limited amount of variation between each run. To address this issue, we propose the DRM-connect algorithm, a combination of dynamic reachability maps (DRM) with lazy collision checking and a fallback strategy based on the RRT-connect algorithm which is used to repair the roadmap through further exploration. This fallback allows us to use much sparser roadmaps. Furthermore, we investigate using an approximate Reeb graph to capture the topology-persistent features of the past solutions of the problem utilising this sparsity. We evaluate DRM-connect with a Reeb graph on reaching tasks, and we compare it to state-of-the-art methods. We show that the proposed method outperforms both RRT-connect and BKPIECE algorithms in the number of collision checks required and we show that our method has the potential to scale to systems with higher number degrees of freedom.


Title: Light-Weight Object Detection and Decision Making via Approximate Computing in Resource-Constrained Mobile Robots
Key Words: decision making  Markov processes  mobile robots  object detection  path planning  robot vision  light-weight object detection  approximate computing  resource-constrained mobile robots  autonomous flights  indoor environments  point clouds  computer vision algorithms  mobile autonomous platforms  video data  decision making  geometric maps  Markov decision process framework  Object detection  Proposals  Roads  Support vector machines  Computer vision  Cameras 
Abstract: Most of the current solutions for autonomous flights in indoor environments rely on purely geometric maps (e.g., point clouds). There has been, however, a growing interest in supplementing such maps with semantic information (e.g., object detections) using computer vision algorithms. Unfortunately, there is a disconnect between the relatively heavy computational requirements of these computer vision solutions, and the limited computation capacity available on mobile autonomous platforms. In this paper, we propose to bridge this gap with a novel Markov Decision Process framework that adapts the parameters of the vision algorithms to the incoming video data rather than fixing them a priori. As a concrete example, we test our framework on a object detection and tracking task, showing significant benefits in terms of energy consumption without considerable loss in accuracy, using a combination of publicly available and novel datasets.


Title: SOS: Stereo Matching in O(1) with Slanted Support Windows
Key Words: cameras  computer vision  image matching  image reconstruction  image texture  stereo image processing  stereo matching  slanted support windows  computer vision  triangulation-based depth cameras  structured light systems  active research topic  trade-off accuracy  fronto-parallel assumptions  search space  active stereo configuration  local methods  PatchMatch Stereo  computational cost  local smoothness  entire stereo pipeline  high quality depth maps  Microsoft Windows  Cameras  Image reconstruction  Three-dimensional displays  Correlation  Image resolution  Optimization 
Abstract: Depth cameras have accelerated research in many areas of computer vision. Most triangulation-based depth cameras, whether structured light systems like the Kinect or active (assisted) stereo systems, are based on the principle of stereo matching. Depth from stereo is an active research topic dating back 30 years. Despite recent advances, algorithms usually trade-off accuracy for speed. In particular, efficient methods rely on fronto-parallel assumptions to reduce the search space and keep computation low. We present SOS (Slanted O(1) Stereo), the first algorithm capable of leveraging slanted support windows without sacrificing speed or accuracy. We use an active stereo configuration, where an illuminator textures the scene. Under this setting, local methods - such as PatchMatch Stereo - obtain state of the art results by jointly estimating disparities and slant, but at a large computational cost. We observe that these methods typically exploit local smoothness to simplify their initialization strategies. Our key insight is that local smoothness can in fact be used to amortize the computation not only within initialization, but across the entire stereo pipeline. Building on these insights, we propose a novel hierarchical initialization that is able to efficiently perform search over disparity and slants. We then show how this structure can be leveraged to provide high quality depth maps. Extensive quantitative evaluations demonstrate that the proposed technique yields significantly more precise results than current state of the art, but at a fraction of the computational cost. Our prototype implementation runs at 4000 fps on modern GPU architectures.


Title: Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry
Key Words: cameras  distance measurement  feature extraction  image colour analysis  image segmentation  pose estimation  cylinder and plane extraction  pose optimization residuals  probabilistic RGB-D odometry framework  curve-aware deteriorates performance  plane extraction approach  single CPU core  organized point clouds  cylinder segments  CAPE  visual odometry  Histograms  Eigenvalues and eigenfunctions  Cameras  Image segmentation  Three-dimensional displays  Probabilistic logic  Principal component analysis 
Abstract: This paper presents CAPE, a method to extract planes and cylinder segments from organized point clouds, which processes 640 × 480 depth images on a single CPU core at an average of 300 Hz, by operating on a grid of planar cells. While, compared to state-of-the-art plane extraction, the latency of CAPE is more consistent and 4-10 times faster, depending on the scene, we also demonstrate empirically that applying CAPE to visual odometry can improve trajectory estimation on scenes made of cylindrical surfaces (e.g. tunnels), whereas using a plane extraction approach that is not curve-aware deteriorates performance on these scenes. To use these geometric primitives in visual odometry, we propose extending a probabilistic RGB-D odometry framework based on points, lines and planes to cylinder primitives. Following this framework, CAPE runs on fused depth maps and the parameters of cylinders are modelled probabilistically to account for uncertainty and weight accordingly the pose optimization residuals.


Title: Incremental Object Database: Building 3D Models from Multiple Partial Observations
Key Words: feature extraction  image colour analysis  image reconstruction  image representation  image segmentation  mobile agents  object detection  solid modelling  multiple partial observations  incremental object database  indoor scenes  merged models  object model  observed instances  segmented RGB-D images  global segmentation map  3D models  mobile agent  Image segmentation  Databases  Three-dimensional displays  GSM  Shape  Image reconstruction  Solid modeling 
Abstract: Collecting 3D object data sets involves a large amount of manual work and is time consuming. Getting complete models of objects either requires a 3D scanner that covers all the surfaces of an object or one needs to rotate it to completely observe it. We present a system that incrementally builds a database of objects as a mobile agent traverses a scene. Our approach requires no prior knowledge of the shapes present in the scene. Object-like segments are extracted from a global segmentation map, which is built online using the input of segmented RGB-D images. These segments are stored in a database, matched among each other, and merged with other previously observed instances. This allows us to create and improve object models on the fly and to use these merged models to reconstruct also unobserved parts of the scene. The database contains each (potentially merged) object model only once, together with a set of poses where it was observed. We evaluate our pipeline with one public dataset, and on a newly created Google Tango dataset containing four indoor scenes with some of the objects appearing multiple times, both within and across scenes.


Title: Submap-Based Pose-Graph Visual SLAM: A Robust Visual Exploration and Localization System* The work in this paper is supported by the National Natural Science Foundation of China (61603103, 61673125), the Natural Science Foundation of Guangdong of China (2016A030310293), and the Major Scientific and Technological Special Project of Guangdong of China (2016B090910003).
Key Words: graph theory  mean square error methods  pose estimation  robot vision  SLAM (robots)  VSLAM algorithms  robust visual exploration  visual simultaneous localization and mapping  submap-based pose-graph visual SLAM  robust exploration  visual front-end  submap-based VSLAM system  Image edge detection  Optimization  Robustness  Visualization  Merging  Robots  Tracking  Monocular VSLAM  Submap-based Backend  Robustness 
Abstract: For VSLAM (Visual Simultaneous Localization and Mapping), localization is a challenging task, especially for some challenging situations: textureless frames, motion blur, etc. To build a robust exploration and localization system in a given space, a submap-based VSLAM system is proposed in this paper. Our system uses a submap back-end and a visual front-end. The main advantage of our system is its robustness with respect to tracking failure, a common problem in current VSLAM algorithms. The robustness of our system is compared with the state-of-the-art in terms of average tracking percentage. The precision of our system is also evaluated in terms of ATE (absolute trajectory error) RMSE (root mean square error) comparing the state-of-the-art. The ability of our system in solving the “kidnapped” problem is demonstrated. Our system can improve the robustness of visual localization in challenging situations.


Title: Learning Monocular Visual Odometry with Dense 3D Mapping from Dense 3D Flow
Key Words: distance measurement  Gaussian processes  image reconstruction  learning (artificial intelligence)  mobile robots  motion estimation  neural nets  pose estimation  robot vision  SLAM (robots)  stereo image processing  learning monocular visual odometry  monocular SLAM  simultaneous localization  neural network  dual-stream L-VO network  6DOF relative pose  bivariate Gaussian modeling  KITTI odometry  visual SLAM system  dense 2D flow  fully deep learning approach  dense 3D flow  dense 3D mapping  Three-dimensional displays  Simultaneous localization and mapping  Visual odometry  Two dimensional displays  Deep learning  Cameras  Training 
Abstract: This paper introduces a fully deep learning approach to monocular SLAM, which can perform simultaneous localization using a neural network for learning visual odometry (L-VO) and dense 3D mapping. Dense 2D flow and a depth image are generated from monocular images by sub-networks, which are then used by a 3D flow associated layer in the L-VO network to generate dense 3D flow. Given this 3D flow, the dual-stream L-VO network can then predict the 6DOF relative pose and furthermore reconstruct the vehicle trajectory. In order to learn the correlation between motion directions, the Bivariate Gaussian modeling is employed in the loss function. The L-VO network achieves an overall performance of 2.68 % for average translational error and 0.0143°/m for average rotational error on the KITTI odometry benchmark. Moreover, the learned depth is leveraged to generate a dense 3D map. As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.


Title: Unit Quaternion-Based Parameterization for Point Features in Visual Navigation
Key Words: covariance matrices  geometry  image representation  recursive estimation  SLAM (robots)  point features  visual navigation  Cartesian 3D representation  homogeneous points  error state  unit-quaternion error covariance  initial feature observations  initial error-states  unit quaternion-based representation  unit quaternion-based parameterization  initial infinite depth uncertainty  Quaternions  Cameras  Simultaneous localization and mapping  Visualization  Navigation  Convergence  Three-dimensional displays 
Abstract: In this paper, we propose to use unit quaternions to represent point features in visual navigation. Contrary to the Cartesian 3D representation, the unit quaternion can well represent features at both large and small distances from the camera without suffering from convergence problems. Contrary to inverse-depth, homogeneous points, or anchored homogeneous points, the unit quaternion has error state of minimum dimension of three. In contrast to prior representations, the proposed method does not need to approximate an initial infinite depth uncertainty. In fact, the unit-quaternion error covariance can be initialized from the initial feature observations without prior information, and the initial error-states are not only bounded, but the bound is identical for all scene geometries. To the best of our knowledge, this is the first time bearing-only recursive estimation (in covariance form) of point features has been possible without using measurements to initialize error covariance. The proposed unit quaternion-based representation is validated on numerical examples.


Title: Improving Repeatability of Experiments by Automatic Evaluation of SLAM Algorithms
Key Words: robot vision  SLAM (robots)  SLAM algorithms  robotics  repeatability  Simultaneous Localization And Mapping  Simultaneous localization and mapping  Buildings  Measurement  Data models  Lasers 
Abstract: The development of good experimental methodologies for robotics takes often inspiration from general principles of experimental practice. Repeatability prescribes that experiments should involve several trials in order to guarantee that results are not achieved by chance, but are systematic, and statistically significant trends can be identified. In this paper, we propose an approach to improve the repeatability of experiments performed in robotics. In particular, we focus on the domain of SLAM (Simultaneous Localization And Mapping) and we introduce a system that exploits simulations to generate a large number of test data on which SLAM algorithms are automatically evaluated in order to obtain consistent results, according to the principle of repeatability.


Title: Guaranteed Coverage with a Blind Unreliable Robot
Key Words: graph theory  mobile robots  path planning  guaranteed coverage  blind unreliable robot  coverage planning  simple mobile robot  heuristic algorithm  specially-constructed graph  Robot sensing systems  Robot kinematics  Planning  Navigation  Computational modeling 
Abstract: We consider the problem of coverage planning for a particular type of very simple mobile robot. The robot must be able to translate in a commanded direction (specified in a global reference frame), with bounded error on the motion direction, until reaching the environment boundary. The objective, for a given environment map, is to generate a sequence of motions that is guaranteed to cover as large a portion of that environment as possible, in spite of the severe limits on the robot's sensing and actuation abilities. We show how to model the knowledge available to this kind of robot about its own position within the environment, show how to compute the region whose coverage can be guaranteed for a given plan, and characterize regions whose coverage cannot be guaranteed by any plan. We also describe a heuristic algorithm that generates coverage plans for this robot, based on a search across a specially-constructed graph. Simulation results demonstrate the effectiveness of the approach.


Title: Multi-Layer Coverage Path Planner for Autonomous Structural Inspection of High-Rise Structures
Key Words: autonomous aerial vehicles  inspection  path planning  structural engineering  travelling salesman problems  autonomous structural inspection  high-rise structures  buildings  towers  unmanned aerial vehicle  multi-layer coverage path planner  3D coverage path planning  traveling salesman problem  Inspection  Three-dimensional displays  Path planning  Planning  Unmanned aerial vehicles  Solid modeling  Spirals 
Abstract: In this paper, a novel 3D coverage path planning method, which is efficient and practical for inspection of high-rise structures such as buildings or towers, using an unmanned aerial vehicle (UAV) is presented. Our approach basically focuses on developing a model-based path planner for structural inspection with a prior map, which is opposite to a non-model based exploration. The proposed method uses a volumetric map which is made before the path planning. With the map, the whole structure is divided into several layers for efficient path planning. Firstly, in each layer, a set of the normal vectors of the center point of every voxel is calculated, and then the opposing vectors become viewpoints. Due to too many viewpoints and an overlapped inspection surface, we down-sample them with a voxel grid filter. Then, the shortest tour connecting the reduced viewpoints must be computed with the Traveling Salesman Problem (TSP) solver. Lastly, all the paths in each layer are combined to form the complete path. The results are verified using simulations with a rotary wing UAV and compared with other state-of-the-art algorithm. It is proven that our method performs much better for structural inspection with respect to computation time as well as the coverage completeness.


Title: Down the CLiFF: Flow-Aware Trajectory Planning Under Motion Pattern Uncertainty
Key Words: mobile robots  path planning  flow-aware tralatory planning  motion pattern uncertainty  flow-aware trajectory  dynamic environments  flow model uncertainty  flow-aware planning  statistical model  map flow patterns  biasing functions  RRT* planning algorithm  CLiFF-map model  flow-compliant trajectories  flow motion patterns  Trajectory  Robots  Planning  Cost function  Uncertainty  Vehicle dynamics  Aerospace electronics 
Abstract: In this paper we address the problem of flow-aware trajectory planning in dynamic environments considering flow model uncertainty. Flow-aware planning aims to plan trajectories that adhere to existing flow motion patterns in the environment, with the goal to make robots more efficient, less intrusive and safer. We use a statistical model called CLiFF-map that can map flow patterns for both continuous media and discrete objects. We propose novel cost and biasing functions for an RRT* planning algorithm, which exploits all the information available in the CLiFF-map model, including uncertainties due to flow variability or partial observability. Qualitatively, a benefit of our approach is that it can also be tuned to yield trajectories with different qualities such as exploratory or cautious, depending on application requirements. Quantitatively, we demonstrate that our approach produces more flow-compliant trajectories, compared to two baselines.


Title: Online Foot-Strike Detection Using Inertial Measurements for Multi-Legged Walking Robots
Key Words: accelerometers  gait analysis  legged locomotion  motion control  position control  terrain mapping  interrupt mode  hexapod walking robot  inertial measurements  multilegged walking robots  proprioceptive terrain sensing  terrain irregularities  inertial data  online foot strike detection  foot strike event detector  data processing  accelerometers  terrain traversal  Legged locomotion  Robot sensing systems  Accelerometers  Servomotors  Reliability 
Abstract: Proprioceptive terrain sensing is essential for rough terrain traversal because it helps legged robots to negotiate individual steps by reacting to terrain irregularities. In this work, we propose to utilize inertial data in the detection of the contact between the leg and the terrain during the stride phase of the leg. We show that relatively cheap accelerometers can be utilized to reliably detect a foot-strike, and thus allow the robot to crawl irregular terrains. The continuous data processing is compared with the interrupt mode in which data are provided only around the foot-strike event. The interrupt mode exhibits significantly better performance, and it also supports generalization of the foot-strike event detector learned from data collected in slow locomotion to faster locomotion where the signals slightly change. The proposed solution is experimentally validated using a real hexapod walking robot for which the walking speed has been improved in comparison to the previous adaptive motion gait based on a force threshold-based position controller for the foot-strike detection.


Title: An Everyday Robotic System that Maintains Local Rules Using Semantic Map Based on Long-Term Episodic Memory
Key Words: mobile robots  path planning  robot agent  local-rule-aware home assistive tasks  semantic map  long-term episodic memory  home environments  global society  probabilistic object localization map  Fetch robots  semantic common knowledge  PR2 robot  robotic system  time 41.0 d  Task analysis  Probabilistic logic  Semantics  Planning  Robot sensing systems  Solid modeling  Service Robots  Learning and Adaptive Systems  Big Data in Robotics and Automation 
Abstract: To enable robots to work on real home environments, they have to not only consider common knowledge in the global society, but also be aware of existing rules there. Since such “local rules” are not describable beforehand, robot agents must acquire them through their lives after deployment. To achieve this, we developed a framework that a) lets robots record long-term episodic memories in their deployed environments, b) autonomously builds probabilistic object localization map as structurization of logged data and c) make adapted task plans based on the map. We equipped our framework on PR2 and Fetch robots operating and recording episodic memory for 41 days with semantic common knowledge of the environment. We also conducted demonstrations in which a PR2 robot tidied up a room, showing that the robot agent can successfully plan and execute local-rule-aware home assistive tasks by using our proposed framework.


Title: Registering Reconstructions of the Two Sides of Fruit Tree Rows
Key Words: agricultural products  feature extraction  image reconstruction  image registration  three dimensionalreconstructions  reconstruction registeration  partial reconstructions  side-views  measuring traits  yield mapping  orchard rows  fruit tree  Image reconstruction  Three-dimensional displays  Vegetation  Principal component analysis  Shape  Semantics  Cameras 
Abstract: We consider the problem of building accurate three dimensional (3D)reconstructions of orchard rows. This problem arises in many applications including yield mapping and measuring traits (e.g. trunk diameters)for phenotyping. While 3D reconstructions of side views can be obtained using standard methods, merging the two side-views is difficult due to the lack of overlap between the two partial reconstructions. We present a novel method that utilizes global features to constrain the solution. Specifically, we use information from the silhouettes and the ground plane for alignment. The method is evaluated using multiple simulated and real datasets. For additional information and demonstration of experimental results please see https://www.youtube.com/watch?v=6mGMF2gFv4M.


Title: Design of an Autonomous Precision Pollination Robot
Key Words: greenhouses  mobile robots  motion control  path planning  precision robotic pollination systems  natural pollinators  uniformity  human population  ongoing development  autonomous robot  BrambleBee  ecology  visual perception  robust autonomous pollination system  autonomous precision pollination robot  Cameras  End effectors  Agriculture  Robot vision systems  Three-dimensional displays 
Abstract: Precision robotic pollination systems can not only fill the gap of declining natural pollinators, but can also surpass them in efficiency and uniformity, helping to feed the fast-growing human population on Earth. This paper presents the design and ongoing development of an autonomous robot named “BrambleBee”, which aims at pollinating bramble plants in a greenhouse environment. Partially inspired by the ecology and behavior of bees, BrambleBee employs state-of-the-art localization and mapping, visual perception, path planning, motion control, and manipulation techniques to create an efficient and robust autonomous pollination system.


Title: HERO: Accelerating Autonomous Robotic Tasks with FPGA
Key Words: convolutional neural nets  field programmable gate arrays  mobile robots  path planning  SLAM (robots)  motion planning tasks  HERO platform  CNN inference  autonomous robotic tasks  Heterogeneous Extensible Robot Open platform  OpenCL programming  SLAM  convolutional neural network inference  FPGA acceleration  heterogeneous computing  simultaneous localization and mapping  VGG-16  ResNet-50  Field programmable gate arrays  Kernel  Acceleration  Simultaneous localization and mapping  Task analysis  Planning 
Abstract: The Heterogeneous Extensible Robot Open (HERO) platform is designed for autonomous robotic research. While bringing in the flexible computational capacities by CPU and FPGA, it addresses the challenges of heterogeneous computing by embracing OpenCL programming. We propose heterogeneous computing approaches for three fundamental robotic tasks: simultaneous localization and mapping (SLAM), motion planning and convolutional neural network (CNN) inference. With FPGA acceleration, the SLAM and motion planning tasks are performed 2-4 times faster on the HERO platform against fine-tuned software implementation. For CNN inference, it can process 20-30 images per second with the network of VGG-16 or ResNet-50. We expect the open platform and the developing experiences shared in this paper can facilitate future robotic research, especially for those compute intensive tasks of perception, movement and manipulation.


Title: Quadtree-Accelerated Real-Time Monocular Dense Mapping
Key Words: autonomous aerial vehicles  image fusion  image motion analysis  image reconstruction  image resolution  mobile robots  path planning  quadtrees  robot vision  stereo image processing  real-time monocular dense mapping  truncated signed distance function  dense 3D maps  resolution depth maps  pixels  dynamic belief propagation  pixel selection  depth map  intensity image  quadtree structure  single localized moving camera  high-quality dense depth maps  robotic navigation  Cameras  Three-dimensional displays  Belief propagation  Estimation  Optimization  Real-time systems  Image resolution 
Abstract: In this paper, we propose a novel mapping method for robotic navigation. High-quality dense depth maps are estimated and fused into 3D reconstructions in real-time using a single localized moving camera. The quadtree structure of the intensity image is used to reduce the computation burden by estimating the depth map in multiple resolutions. Both the quadtree-based pixel selection and the dynamic belief propagation are proposed to speed up the mapping process: pixels are selected and optimized with the computation resource according to their levels in the quadtree. Solved depth estimations are further interpolated and fused temporally into full resolution depth maps and fused into dense 3D maps using truncated signed distance function (TSDF). We compare our method with other state-of-the-art methods using the public datasets. Onboard UAV autonomous flight is also used to further prove the usability and efficiency of our method on portable devices. For the benefit of the community, the implementation is also released as open source at https://github.com/HKUST-Aerial-Robotics/open_quadtree_mapping.


Title: NDVI Point Cloud Generator Tool Using Low-Cost RGB-D Sensor
Key Words: cameras  geophysical image processing  image colour analysis  image sensors  vegetation mapping  vegetation index estimation  Microsoft Kinect V2  vegetation monitoring purposes  ROS point cloud generation tools  active IR camera  active RGB-D sensor technology  RGB camera  NDVI point cloud generator tool  3D NDVI maps  Conferences  Intelligent robots 
Abstract: In this manuscript, a NDVI point cloud generator tool based on low-cost active RGB-D sensor is presented. Taking advantage of currently available ROS point cloud generation tools and RGB-D sensor technology (like Microsoft Kinect), that includes an inbuilt active IR camera and a RGB camera, 3D NDVI maps can be quickly and easily generated for vegetation monitoring purposes. When using low-cost sensors for vegetation index estimation, it is necessary to apply a rigorous methodology for extracting reliable information. In this paper, the methodology for NDVI generation using a low-cost sensor as well as experiments to evaluate its performance is presented. The experiments performed show that it is possible to obtain a reliable NDVI point cloud from a Kinect V2.


Title: PCAOT: A Manhattan Point Cloud Registration Method Towards Large Rotation and Small Overlap
Key Words: computational geometry  image registration  iterative methods  principal component analysis  Manhattan world assumption  transformation estimation  overlap estimation  ICP  rotation angle  PCAOT  Manhattan point cloud registration method  robot mapping  iterative closest point  robot localization  principal coordinate alignment with overlap tuning  3D cuboid  Three-dimensional displays  Estimation  Tuning  Iterative closest point algorithm  Mathematical model  Filtering  Task analysis 
Abstract: Point cloud registration is a popular research topic and has been widely used in many tasks, such as robot mapping and localization. It is a challenging problem when the overlap is small, or the rotation is large. The problem has not been well solved by existing methods such as the iterative closest point (ICP) and its variants. In this paper, a novel method named principal coordinate alignment with overlap tuning (PCAOT) is proposed based on the Manhattan world assumption. It solves two key problems together, the transformation estimation and the overlap estimation. The overlap is represented by a 3D cuboid and the transformation is computed only within the overlap region. Instead of finding point correspondence as in traditional methods, we estimate the rotation by principal coordinates alignment, which is faster and less sensitive than ICP and its variants to small overlaps and large rotations. Evaluations demonstrate that our method achieves much better results than the ICP and its variants when the overlap ratio is smaller than 50%, or the rotation angle is larger than 60°. Especially, it is effective when the overlap ratio is less than 30%, or the rotation angle is larger than 90°.


Title: Minimal Construct: Efficient Shortest Path Finding for Mobile Robots in Polygonal Maps
Key Words: collision avoidance  graph theory  mobile robots  navigation  mobile robots  polygonal maps  navigational software  shortest collision-free path  Dijkstra algorithm  visibility graph algorithm  minimal construct  visibility graph-based shortest path algorithms  shortest path finding  A* algorithm  Navigation  Heuristic algorithms  Mobile robots  Software  Complexity theory  Standards 
Abstract: With the advent of polygonal maps finding their way into the navigational software of mobile robots, the Visibility Graph can be used to search for the shortest collision-free path. The nature of the Visibility Graph-based shortest path algorithms is such that first the entire graph is computed in a relatively time-consuming manner. Then, the graph can be searched efficiently any number of times for varying start and target state combinations with the A* or the Dijkstra algorithm. However, real-world environments are typically too dynamic for a map to remain valid for a long time. With the goal of obtaining the shortest path quickly in an ever changing environment, we introduce a rapid path finding algorithm-Minimal Construct-that discovers only a necessary portion of the Visibility Graph around the obstacles that actually get in the way. Collision tests are computed only for lines that seem heuristically promising. This way, shortest paths can be found much faster than with a state-of-the-art Visibility Graph algorithm and as our experiments show, even grid-based A* searches are outperformed in most cases with the added benefit of smoother and shorter paths.


Title: Dynamic Modelling and Motion Planning for the Nonprehensile Manipulation and Locomotion Tasks of the Quadruped Rsbot*This work is supported by the project of Robotics Innovation Based on Advanced Materials under Ritsumeikan Global Innovation Research Organization (R-GIRO)
Key Words: legged locomotion  manipulator dynamics  motion control  path planning  Drive Mode  Inchworm Mode  Scoot Mode  universal model  dynamic equation  contact force constraints  system state variables  system state paths  robot motions  nonprehensile manipulation  locomotion tasks  quadruped robot  dynamic modelling  motion planning method  state acceleration constraints  Robot kinematics  Legged locomotion  Force  Friction  Mathematical model  Planning 
Abstract: This paper presents the dynamic modelling and motion planning method for a quadruped robot that uses its legs for nonprehensile manipulation as well as locomotion. Three different working modes named Drive Mode, Inchworm Mode and Scoot Mode are proposed to enable the robot to move forward together with the object. We firstly introduce a universal model for these modes and deduce its dynamic equation. Then the contact force constraints are combined and mapped to the system state variables. Based on the acquired state acceleration constraints, the motion planning problem can be solved by designing system state paths in the phase space. After that, we described the mathematical problems within the three working modes and generate the robot motions accordingly. Finally, experimental results obtained through simulations and physical tests are reported to demonstrate the effectiveness of our method.


Title: Quotient-Space Motion Planning
Key Words: mobile robots  motion control  path planning  quotient-space motion planning  OMPL  robot  Quotient-space roadMap Planner  roadmap-based motion planning algorithm  nested quotient-space decomposition  open motion planning library  Planning  Manipulators  Runtime  Visualization  Probabilistic logic  Manifolds 
Abstract: A motion planning algorithm computes the motion of a robot by computing a path through its configuration space. To improve the runtime of motion planning algorithms, we propose to nest robots in each other, creating a nested quotient-space decomposition of the configuration space. Based on this decomposition we define a new roadmap-based motion planning algorithm called the Quotient-space roadMap Planner (QMP). The algorithm starts growing a graph on the lowest dimensional quotient space, switches to the next quotient space once a valid path has been found, and keeps updating the graphs on each quotient space simultaneously until a valid path in the configuration space has been found. We show that this algorithm is probabilistically complete and outperforms a set of state-of-the-art algorithms implemented in the open motion planning library (OMPL).


Title: Computing a Collision-Free Path Using the Monogenic Scale Space
Key Words: collision avoidance  Laplace equations  mobile robots  multi-robot systems  position control  static obstacles  dynamic obstacles  mobile robot  safe path  goal position  Laplace equation  collision-free path  rectangular bounded domain  monogenic scale space  environment map  nonconvex environments  functionalities  Kernel  Laplace equations  Mathematical model  Mobile robots  Magnetic domains  Magnetic resonance imaging 
Abstract: Mobile robots have been used for various purposes with different functionalities which require them to freely move in environments containing both static and dynamic obstacles to accomplish given tasks. One of the most relevant capabilities in terms of navigating a mobile robot in such an environment is to find a safe path to a goal position. This paper shows that there exists an accurate solution to the Laplace equation which allows finding a collision-free path and that it can be efficiently calculated for a rectangular bounded domain such as a map which is represented as an image. This is accomplished by the use of the monogenic scale space resulting in a vector field which describes the attracting and repelling forces from the obstacles and the goal. The method is shown to work in reasonably convex domains and by the use of tessellation of the environment map for non-convex environments.


Title: Perception-Driven Sparse Graphs for Optimal Motion Planning
Key Words: collision avoidance  graph theory  mobile robots  optimal control  optimisation  robot vision  trajectory control  motion plan generation  planning subproblem  mapping subproblem  optimal motion planning  perception-driven sparse graphs  optimal trajectory  plan graphs  visual sensors  Planning  Trajectory  Robot sensing systems  Collision avoidance  Heuristic algorithms 
Abstract: Most existing motion planning algorithms assume that a map (of some quality) is fully determined prior to generating a motion plan. In many emerging applications of robotics, e.g., fast-moving agile aerial robots with constrained embedded computational platforms and visual sensors, dense maps of the world are not immediately available, and they are computationally expensive to construct. We propose a new algorithm for generating plan graphs which couples the perception and motion planning processes for computational efficiency. In a nutshell, the proposed algorithm iteratively switches between the planning sub-problem and the mapping sub-problem, each updating based on the other until a valid trajectory is found. The resulting trajectory retains a provable property of providing an optimal trajectory with respect to the full (unmapped) environment, while utilizing only a fraction of the sensing data in computational experiments.


Title: Seeing the Wood for the Trees: Reliable Localization in Urban and Natural Environments
Key Words: feature extraction  geophysical image processing  image matching  image segmentation  robot vision  vegetation mapping  reliable localization  urban environments  natural environments  current state-of-the-art global approaches  structure-poor vegetated areas  orchards  environments clutter  repeatable extraction  distinctive landmarks  natural forests  tree trunks  foliage intertwines  planar structure  place recognition  feature extraction module segments  reliable object-sized segments  heavy clutter  foliage-heavy forest  urban scenarios  random forest  shape descriptor  Feature extraction  Vegetation  Three-dimensional displays  Forestry  Reliability  Clutter  Hidden Markov models 
Abstract: In this work we introduce Natural Segmentation and Matching (NSM), an algorithm for reliable localization, using laser, in both urban and natural environments. Current state-of-the-art global approaches do not generalize well to structure-poor vegetated areas such as forests or orchards. In these environments clutter and perceptual aliasing prevents repeatable extraction of distinctive landmarks between different test runs. In natural forests, tree trunks are not distinctive, foliage intertwines and there is a complete lack of planar structure. In this paper we propose a method for place recognition which uses a more involved feature extraction process which is better suited to this type of environment. First, a feature extraction module segments stable and reliable object-sized segments from a point cloud despite the presence of heavy clutter or tree foliage. Second, repeatable oriented key poses are extracted and matched with a reliable shape descriptor using a Random Forest to estimate the current sensor's position within the target map. We present qualitative and quantitative evaluation on three datasets from different environments - the KITTI benchmark, a parkland scene and a foliage-heavy forest. The experiments show how our approach can achieve place recognition in woodlands while also outperforming current state-of-the-art approaches in urban scenarios without specific tuning.


Title: A Novel Autonomous Robot for Greenhouse Applications
Key Words: agriculture  cameras  greenhouses  mobile robots  robot vision  autonomous robot  low-cost 3D camera  greenhouse headland  greenhouse heating system  agricultural robot  greenhouse applications  Green products  Rails  Mobile robots  Tools  Wheels  Task analysis 
Abstract: This paper presents a novel agricultural robot for greenhouse applications. In many greenhouses, including the greenhouse used in this work, sets of pipes run along the floor between plant rows. These pipes are components of the greenhouse heating system, and doubles as rails for trolleys used by workers. A flat surface separates the start of each rail set at the greenhouse headland. If a robot is to autonomously drive along plant rows, and also be able to move from one set of rails to the next, it must be able to locomote both on rails and on flat surfaces. This puts requirements on mechanical design and navigation, as the robot must cope with two very different operational environments. The robot presented in this paper has been designed to overcome these challenges and allows for autonomous operation both in open environments and on rails by using only low-cost sensors. The robot is assembled using a modular system created by the authors and tested in a greenhouse during ordinary operation. Using the robot, we map the environment and automatically determine the starting point of each rail in the map. We also show how we are able to identify rails and estimate the robots pose relative to theses using only a low-cost 3D camera. When a rail is located, the robot makes the transition from floor to rail and travels along the row of plants before it moves to the next rail set which it has identified in the map. The robot is used for UV treatment of cucumber plants.


Title: π-SoC: Heterogeneous SoC Architecture for Visual Inertial SLAM Applications
Key Words: energy consumption  mobile computing  mobile robots  optimisation  SLAM (robots)  system-on-chip  visual inertial SLAM applications  autonomous vehicles  robotics  core technologies  battery-powered mobile devices  energy budget  energy consumption  energy efficiency  visual inertial SLAM workloads  60 FPS performance  heterogeneous SoC architecture  simultaneous localization and mapping  hardware accelerator  IO interface  memory hierarchy  Simultaneous localization and mapping  Feature extraction  Three-dimensional displays  Instruction sets  Power demand  Graphics processing units  Computer architecture 
Abstract: In recent years, we have observed a clear trend in the rapid rise of autonomous vehicles and robotics. One of the core technologies enabling these applications, Simultaneous Localization And Mapping (SLAM), imposes two main challenges: first, these workloads are computationally intensive and they often have real-time requirements; second, these workloads run on battery-powered mobile devices with limited energy budget. Hence, performance should be improved while simultaneously reducing energy consumption, two rather contradicting goals by conventional wisdom. Previous attempts to optimize SLAM performance and energy efficiency usually involve optimizing one function and fail to approach the problem systematically. In this paper, we first study the characteristics of visual inertial SLAM workloads on existing heterogeneous SoCs. Then based on the initial findings, we propose π-SoC, a heterogeneous SoC design that systematically optimize the IO interface, the memory hierarchy, as well as the the hardware accelerator. We implemented this system on a Xilinx Zynq UltraScale MPSoC and was able to deliver over 60 FPS performance with average power less than 5 W.


Title: GPU-Accelerated Next-Best-View Coverage of Articulated Scenes
Key Words: embedded systems  graphics processing units  mobile robots  path planning  rendering (computer graphics)  robot vision  costmap computation  path planning  simulation  viewpoint candidates  multiple device classes  multiGPU servers  utility map  robots  complex articulated scenes  GPU-accelerated next-best-view coverage  next-best-view algorithms  mapping tasks  articulated environments  obstructed areas  degrees of freedom  embedded devices  next-best-view approach  embedded systems  graphics processing units  OpenGL  Graphics processing units  Task analysis  Cameras  Robot sensing systems  Planning  Solid modeling 
Abstract: Next-best-view algorithms are commonly used for covering known scenes, for example in search, maintenance, and mapping tasks. In this paper, we consider the problem of planning a strategy for covering articulated environments where the robot also has to manipulate objects to inspect obstructed areas. This problem is particularly challenging due to the many degrees of freedom resulting from the articulation. We propose to exploit graphics processing units present in many embedded devices to parallelize the computations of a greedy next-best-view approach. We implemented algorithms for costmap computation, path planning, as well as simulation and evaluation of viewpoint candidates in OpenGL for Embedded Systems and benchmarked the implementations on multiple device classes ranging from smartphones to multi-GPU servers. We introduce a heuristic for estimating a utility map from images rendered with strategically placed spherical cameras and show in simulation experiments that robots can successfully explore complex articulated scenes with our system.


Title: Lane Marking Quality Assessment for Autonomous Driving
Key Words: cameras  Global Positioning System  image processing  optical radar  road vehicles  traffic engineering computing  road curvature  background road surfaces  dual-modal algorithm  lane marking quality assessment  autonomous driving  future transportation systems  inspection vehicle  frontal-view camera  global positioning system receiver  light detection and ranging system  LIDAR  Roads  Measurement  Laser radar  Cameras  Shape  Global Positioning System  Three-dimensional displays 
Abstract: Measuring the quality of roads and ensuring they are ready for autonomous driving is important for future transportation systems. Here we focus on developing metrics and algorithms to assess lane marking (LM)qualities from an egocentric view of an inspection vehicle equipped with a global positioning system (GPS)receiver, a frontal-view camera, and a light detection and ranging (LIDAR)system. We propose three quality metrics for LMs: correctness, shape, and visibility. The correctness metric measures the divergence between the expected LMs based on prior map inputs and the actual sensor inputs. The shape metric evaluates smoothness in road curvature and width range. The visibility metric evaluates the contrast between LMs and background road surfaces. We propose a dual-modal algorithm to compute these metrics. We have implemented the algorithms and tested them under KITTI dataset. The results show that our metrics can successfully detect LM anomalies in all testing scenarios.


Title: P-CAP: Pre-Computed Alternative Paths to Enable Aggressive Aerial Maneuvers in Cluttered Environments
Key Words: autonomous aerial vehicles  collision avoidance  data structures  graph theory  mobile robots  navigation  probability  search problems  p-CAP  pre-computed alternative paths  cluttered environments  fast autonomous flight  autonomous navigation  complex environment  continuous heuristic search  k-connected grid  probabilistic scheme  onboard sensors  prior map information  data structure  flight experiments  unstructured environments  aggressive aerial maneuvers  graph  forests-like environments  obstacles avoidance  Navigation  Sensors  Switches  Collision avoidance  Forestry  Gold  Planning 
Abstract: We propose a novel method to enable fast autonomous flight in cluttered environments. Typically, autonomous navigation through a complex environment requires a continuous heuristic search on a graph generated by a k-connected grid or a probabilistic scheme. As the vehicle progresses, modification of the graph with data from onboard sensors is expensive as is search on the graph, especially if the paths must be kino-dynamically feasible. We suggest that computation needed to find safe paths during fast flight can be greatly reduced if we precompute and carefully arrange a dense set of alternative paths before the flight. Any prior map information can be used to prune the alternative paths to come up with a data structure that enables very fast online computation to deal with obstacles that are not on the map but only detected by onboard sensors. To test this idea, we have conducted a large number of flight experiments in structured (large industrial facilities) and unstructured (forests-like) environments. We show that even in the most unstructured environments, this method enables flight at a speed up to 10m/s while avoiding obstacles detected from onboard sensors.


Title: Sparse 3D Topological Graphs for Micro-Aerial Vehicle Planning
Key Words: autonomous aerial vehicles  computational geometry  graph theory  image colour analysis  mobile robots  path planning  topology  Euclidean signed distance field  3D generalized Voronoi diagram  RGB-D sensing  global planning  skeleton diagram  topological information  noisy sensor data  sparse map representations  compact map representations  MAV  microaerial vehicle planning  sparse 3D topological graphs  Planning  Three-dimensional displays  Two dimensional displays  Skeleton  Robot sensing systems  Topology 
Abstract: Micro-Aerial Vehicles (MAVs) have the advantage of moving freely in 3D space. However, creating compact and sparse map representations that can be efficiently used for planning for such robots is still an open problem. In this paper, we take maps built from noisy sensor data and construct a sparse graph containing topological information that can be used for 3D planning. We use a Euclidean Signed Distance Field, extract a 3D Generalized Voronoi Diagram (GVD), and obtain a thin skeleton diagram representing the topological structure of the environment. We then convert this skeleton diagram into a sparse graph, which we show is resistant to noise and changes in resolution. We demonstrate global planning over this graph, and the orders of magnitude speed-up it offers over other common planning methods. We validate our planning algorithm in real maps built onboard an MAV, using RGB-D sensing.


Title: Persistent Monitoring with Refueling on a Terrain Using a Team of Aerial and Ground Robots
Key Words: aerospace robotics  integer programming  linear programming  multi-robot systems  path planning  tree searching  terrain  persistent monitoring  heterogeneous team  aerial robots  ground robots  MILP formulation  branch-and-cut framework  separation algorithm  Fuels  Monitoring  Unmanned aerial vehicles  Routing  Robot sensing systems  Kernel 
Abstract: There are many applications such as surveillance and mapping that require persistent monitoring of terrains. In this work, we consider a heterogeneous team of aerial and ground robots that are tasked with monitoring a terrain along a given path. Both types of robots are equipped with cameras that can monitor the terrain within their fields-of-view. We also consider the ability of the aerial robots to land occasionally on the terrain to recharge. The objective is to find a path for all the robots to reduce the time required. Determining optimal routes for the robots is a challenging problem because of constrained visibility due to the terrain and fuel limitations of the robots. We devise an MILP formulation for the problem using a 1.5 dimensional representation model. A branch-and-cut framework is used to implement the MILP and involves the design of a separation algorithm to compute valid inequalities. We report results from extensive simulations and proof-of-concept field experiments to show the efficacy of our approach.


Title: Human Gaze Following for Human-Robot Interaction
Key Words: control engineering computing  face recognition  gaze tracking  human-robot interaction  learning (artificial intelligence)  single 2D image  human gaze predictions  human-robot interaction tasks  fluent interactions  mutual gaze prediction  gaze heat map statistics  referential gaze  deep learning approach  human gaze fixations  Task analysis  Real-time systems  Head  Cameras  Robot vision systems  Videos 
Abstract: Gaze provides subtle informative cues to aid fluent interactions among people. Incorporating human gaze predictions can signify how engaged a person is while interacting with a robot and allow the robot to predict a human's intentions or goals. We propose a novel approach to predict human gaze fixations relevant for human-robot interaction tasks-both referential and mutual gaze-in real time on a robot. We use a deep learning approach which tracks a human's gaze from a robot's perspective in real time. The approach builds on prior work which uses a deep network to predict the referential gaze of a person from a single 2D image. Our work uses an interpretable part of the network, a gaze heat map, and incorporates contextual task knowledge such as location of relevant objects, to predict referential gaze. We find that the gaze heat map statistics also capture differences between mutual and referential gaze conditions, which we use to predict whether a person is facing the robot's camera or not. We highlight the challenges of following a person's gaze on a robot in real time and show improved performance for referential gaze and mutual gaze prediction.


Title: Map-based Deep Imitation Learning for Obstacle Avoidance
Key Words: collision avoidance  decision making  inference mechanisms  learning (artificial intelligence)  mobile robots  optimisation  robot vision  SLAM (robots)  mobile robots  deep imitation learning algorithm  egocentric local occupancy maps  fast feed-forward inferences  policy robustness  optimal decision making  obstacle avoidance policy  map-based deep imitation learning  value iteration networks  near-optimal continuous action commands  planning-based scenarios  Collision avoidance  Robot kinematics  Mobile robots  Training  Neural networks  Trajectory 
Abstract: Making an optimal decision to avoid obstacles while heading to the goal is one of the fundamental challenges for mobile robots equipped with limited computational resources. In this paper, we present a deep imitation learning algorithm that develops a computationally efficient obstacle avoidance policy based on egocentric local occupancy maps. The trained model embedded with a variant of the value iteration networks is able to provide near-optimal continuous action commands through fast feed-forward inferences and generalize well to unseen planning-based scenarios. To improve the policy robustness, we augment the training data set with artificially generated maps, which effectively alleviates the shortage of catastrophic samples in normal demonstrations. Extensive experiments on a Segway robot show the effectiveness of the proposed approach in terms of solution optimality, robustness as well as computation time.


Title: Wireframe Mapping for Resource-Constrained Robots
Key Words: mobile robots  path planning  simulation mapping  wireframe mapping  resource-constrained robots  wireframe representation  particle filter  sparse wireframe map structure  map representation  wireframe structure  occupancy grid map  discrete map errors  Simultaneous localization and mapping  Uncertainty  Two dimensional displays  Geometry  Navigation 
Abstract: This paper presents a novel wireframe map structure for resource-constrained robots operating in a rectilinear 2D environment. The wireframe representation compactly represents geometry, in addition to transient situations such as occlusions and boundaries of unexplored regions. We formulate a particle filter to suit this sparse wireframe map structure. Functions for calculating the likelihood of scans, merging wireframes, and resampling are developed to accommodate this map representation. The wireframe structure with the particle filter allows for severe discrete map errors to be corrected, leading to accurate maps with small storage requirements. We show in a simulation study that the algorithm attains a map of an environment with 1 % error, compared to an occupancy grid map obtained with GMapping which attained 23% error with the same storage requirements. A simulation mapping a large environment demonstrates the algorithms scalability.


Title: Improving Trajectory Optimization Using a Roadmap Framework
Key Words: mobile robots  optimisation  path planning  sampling methods  trajectory control  trajectory optimization process  sampling-based planners  motion planning system  multiquery roadmap  sparse roadmap framework  optimization-based motion planners  Planning  Trajectory optimization  Robots  Dynamics  Task analysis  Collision avoidance 
Abstract: We present an evaluation of several representative sampling-based and optimization-based motion planners, and then introduce an integrated motion planning system which incorporates recent advances in trajectory optimization into a sparse roadmap framework. Through experiments in 4 common application scenarios with 5000 test cases each, we show that optimization-based or sampling-based planners alone are not effective for realistic problems where fast planning times are required. To the best of our knowledge, this is the first work that presents such a systematic and comprehensive evaluation of state-of-the-art motion planners, which are based on a significant amount of experiments. We then combine different stand-alone planners with trajectory optimization. The results show that the combination of our sparse roadmap and trajectory optimization provides superior performance over other standard sampling-based planners' combinations. By using a multi-query roadmap instead of generating completely new trajectories for each planning problem, our approach allows for extensions such as persistent control policy information associated with a trajectory across planning problems. Also, the sub-optimality resulting from the sparsity of roadmap, as well as the unexpected disturbances from the environment, can both be overcome by the real-time trajectory optimization process.


