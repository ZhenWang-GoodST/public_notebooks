total paper: 62
Title: Optimized Contrast Enhancements to Improve Robustness of Visual Tracking in a SLAM Relocalisation Context
Key Words: cameras  feature extraction  image colour analysis  image enhancement  image representation  mobile robots  robot vision  SLAM (robots)  video signal processing  optimized contrast enhancements  visual tracking  SLAM relocalisation context  indirect SLAM techniques  robotics community  feature points  multilayered image representation  contrast enhanced version  tracking process  detection  matching  dynamic contrast enhancements  dynamic light changing conditions  ORB-SLAM  light changed condition  reference video  Mutual information  Lighting  Robustness  Simultaneous localization and mapping  Cameras  Entropy  Visualization 
Abstract: Robustness of indirect SLAM techniques to light changing conditions remains a central issue in the robotics community. With the change in the illumination of a scene, feature points are either not extracted properly due to low contrasts, or not matched due to large differences in descriptors. In this paper, we propose a multi-layered image representation (MLI) in which each layer holds a contrast enhanced version of the current image in the tracking process in order to improve detection and matching. We show how Mutual Information can be used to compute dynamic contrast enhancements on each layer. We demonstrate how this approach dramatically improves the robustness in dynamic light changing conditions on both synthetic and real environments compared to default ORB-SLAM. This work focalises on the specific case of SLAM relocalisation in which a first pass on a reference video constructs a map, and a second pass with a light changed condition relocalizes the camera in the map.


Title: Key-frame Selection for Multi-robot Simultaneous Localization and Tracking in Robot Soccer Field
Key Words: entropy  mobile robots  multi-robot systems  robot vision  SLAM (robots)  traditional key-frame selection algorithms  temporal relationship  spatial relationship  pre-defined field  information entropy  selection ratio  key-frames  localization results  robot soccer field  optical images  extensive computation resources  key-frame selection algorithm  multiple robots simultaneous localization  multirobot soccer games  Entropy  Robot sensing systems  Object detection  Sports  Cameras  Legged locomotion 
Abstract: Optical images provide rich features but require extensive computation resources to process for SLAM. When there are limited computation resources on the robots, it becomes a heavy burden to process the images in real-time. This paper presents the design and implementation of key-frame selection algorithm for multiple robots simultaneous localization and tracking on the multi-robot soccer games which have pre-defined field and objects. Compared to traditional key-frame selection algorithms, this work makes use of the temporal and spatial relationship among objects on the pre-defined field to compute the information entropy. The selection ratio can be adjusted by two parameters: entropy threshold and the maximum moving distance. The experimental results show that the developed method can effectively detect the change of scene using selected key-frames. And comparing with the localization results using all the images, using less than 20% of all images after walking 11,203mm it only increase up to 0.87% trajectory errors.


Title: LIPS: LiDAR-Inertial 3D Plane SLAM
Key Words: graph theory  image representation  mobile robots  optical radar  optimisation  robot vision  SLAM (robots)  inertial preintegratation measurement  LiDAR-inertial 3D plane SLAM  simultaneous localization and mapping  singularity free plane factor  closest point plane representation  Simultaneous localization and mapping  Three-dimensional displays  Laser radar  Optimization  Lips 
Abstract: This paper presents the formalization of the closest point plane representation and an analysis of its incorporation in 3D indoor simultaneous localization and mapping (SLAM). We present a singularity free plane factor leveraging the closest point plane representation, and demonstrate its fusion with inertial preintegratation measurements in a graph-based optimization framework. The resulting LiDAR-inertial 3D plane SLAM (LIPS) system is validated both on a custom made LiDAR simulator and on a real-world experiment.


Title: Scan Similarity-based Pose Graph Construction method for Graph SLAM
Key Words: graph theory  mobile robots  pose estimation  robot vision  SLAM (robots)  scan similarity-based pose graph construction method  constructed graph  loop closure detection method  real world dataset  benchmark dataset  odometry estimation process  error accumulation phenomenon  pose graph SLAM  scan similarity computation method  graph accuracy  high quality graph  Simultaneous localization and mapping  Lasers  Estimation  Heuristic algorithms  Optimization 
Abstract: Scan similarity-based pose graph construction method for graph SLAM is proposed. To perform delicate pose graph SLAM, front-end that constructs a graph as well as back-end that optimizes the constructed graph is an important task. Generally, there is an error accumulation phenomenon during the odometry estimation process. This paper focuses on the method of creating a high quality graph by suggesting ways to improve the graph accuracy since the accumulated errors in the graph might degrade the performance of the entire graph SLAM. We deal with one of our previous works, dynamic keyframe selection technique, based on scan similarity computation method more precisely and suggest a loop closure detection method by exploiting previously proposed 2-D laser scan descriptor. To verify objective performance of the proposed method, the experimental results of the odometry estimation are shown by using the benchmark dataset and the real world dataset. Additionally, results of the pose graph SLAM are shown for the real world dataset which include the loop clorues.


Title: Predicting Objective Function Change in Pose-Graph Optimization
Key Words: graph theory  optimisation  SLAM (robots)  outlier detection  robust online incremental SLAM applications  graph pruning  information-theoretic metrics  pose-graph optimization scheme  Linear programming  Optimization  Simultaneous localization and mapping  Measurement errors  Noise measurement  Reliability 
Abstract: Robust online incremental SLAM applications require metrics to evaluate the impact of current measurements. Despite its prevalence in graph pruning, information-theoretic metrics solely are insufficient to detect outliers. The optimal value of the objective function is a better choice to detect outliers but cannot be computed unless the problem is solved. In this paper, we show how the objective function change can be predicted in an incremental pose-graph optimization scheme, without actually solving the problem. The predicted objective function change can be used to guide online decisions or detect outliers. Experiments validate the accuracy of the predicted objective function, and an application to outlier detection is also provided, showing its advantages over M-estimators.


Title: Efficient Long-term Mapping in Dynamic Environments
Key Words: graph theory  mobile robots  robot vision  SLAM (robots)  mapping problem  longterm SLAM datasets  graph coherency  intra-session loop closure detections  out-dated nodes  graph complexity  nonstatic entities  merging procedure  efficient ICP-based alignment  up-to-date state  2D point cloud data  local maps  graph SLAM paradigm  multiple mapping sessions  single mapping sessions  SLAM system  autonomous robots  dynamic environments  long-term robot operation  Simultaneous localization and mapping  Cloud computing  Three-dimensional displays  Two dimensional displays  Merging  Optimization 
Abstract: As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.


Title: Localization of Classified Objects in SLAM using Nonparametric Statistics and Clustering
Key Words: feature extraction  learning (artificial intelligence)  mobile robots  nonparametric statistics  object detection  pattern clustering  robot vision  SLAM (robots)  statistical analysis  nonparametric statistical approach  data association  mapping process  object detection  machine learning  semantic information  nonparametric statistics  classified objects  locating objects  SLAM  unsupervised clustering method  detected objects  Simultaneous localization and mapping  Semantics  Object detection  Cameras  Three-dimensional displays 
Abstract: Traditional Simultaneous Localization and Mapping (SLAM) approaches build maps based on points, lines or planes. These maps visually resemble the environment but without any semantic or information about the objects in the environment. Recent advancements in machine learning have made object detection highly accurate and reliable with large set of objects. Object detection can effectively help SLAM to incorporate semantics in the mapping process. One of the main obstacles is data association between detected objects over time. We demonstrate a nonparametric statistical approach to solve the data association between detected objects over consecutive frames. Then we use an unsupervised clustering method to identify the existence of objects in the map. The complete process can be run in parallel with SLAM. The performance of our algorithm is demonstrated on several public datasets, which shows promising results in locating objects in SLAM.


Title: Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation
Key Words: image segmentation  probability  SLAM (robots)  stereo image processing  SLAM framework  NYUv2 dataset  computational efficiency  frame-wise segmentation result  computationally intensive stages  segmentation label  updating class probabilities  processing components  geometric-based segmentation method  geometric-based incremental segmentation  Semantics  Three-dimensional displays  Image segmentation  Simultaneous localization and mapping  Cameras  Real-time systems  Two dimensional displays 
Abstract: We propose an efficient and scalable method for incrementally building a dense, semantically annotated 3D map in real-time. The proposed method assigns class probabilities to each region, not each element (e.g., surfel and voxel), of the 3D map which is built up through a robust SLAM framework and incrementally segmented with a geometric-based segmentation method. Differently from all other approaches, our method has a capability of running at over 30Hz while performing all processing components, including SLAM, segmentation, 2D recognition, and updating class probabilities of each segmentation label at every incoming frame, thanks to the high efficiency that characterizes the computationally intensive stages of our framework. By utilizing a specifically designed CNN to improve the frame-wise segmentation result, we can also achieve high accuracy. We validate our method on the NYUv2 dataset by comparing with the state of the art in terms of accuracy and computational efficiency, and by means of an analysis in terms of time and space complexity.


Title: Semantic Monocular SLAM for Highly Dynamic Environments
Key Words: cameras  feature extraction  image motion analysis  image sequences  mobile robots  object detection  object tracking  pose estimation  probability  robot vision  SLAM (robots)  static environment  semantic monocular SLAM framework  semantic information  explicit probabilistic model  dynamic environments  Virtual KITTI  Synthia datasets  pose estimation  Semantics  Simultaneous localization and mapping  Feature extraction  Dynamics  Cameras  Pose estimation  Probabilistic logic 
Abstract: Recent advances in monocular SLAM have enabled real-time capable systems which run robustly under the assumption of a static environment, but fail in presence of dynamic scene changes and motion, since they lack an explicit dynamic outlier handling. We propose a semantic monocular SLAM framework designed to deal with highly dynamic environments, combining feature-based and direct approaches to achieve robustness under challenging conditions. The proposed approach exploits semantic information extracted from the scene within an explicit probabilistic model, which maximizes the probability for both tracking and mapping to rely on those scene parts that do not present a relative motion with respect to the camera. We show more stable pose estimation in dynamic environments and comparable performance to the state of the art on static sequences on the Virtual KITTI and Synthia datasets.


Title: Unscented Kalman Filter on Lie Groups for Visual Inertial Odometry
Key Words: distance measurement  Kalman filters  Lie groups  nonlinear filters  SLAM (robots)  state estimation  stereo image processing  unscented Kalman filter  Lie groups  visual information  inertial measurements  state estimation  robust estimation  computational efficiency  low-cost aerial vehicles  processor power  innovative filter  stereo visual inertial odometry building  invariant filtering theory  computational complexity  stereo multistate constraint Kalman filter  EuRoC dataset  MAV outdoor dataset  Cameras  Kalman filters  Visualization  Computational modeling  Uncertainty  Robustness  Noise measurement  Lie groups  unscented Kalman filter  visual inertial odometry  aerial vehicle  localization 
Abstract: Fusing visual information with inertial measurements for state estimation has aroused major interests in recent years. However, combining a robust estimation with computational efficiency remains challenging, specifically for low-cost aerial vehicles in which the quality of the sensors and the processor power are constrained by size, weight and cost. In this paper, we present an innovative filter for stereo visual inertial odometry building on: (i) the recently introduced stereo multistate constraint Kalman filter; (ii) the invariant filtering theory; and (iii) the unscented Kalman filter (UKF) on Lie groups. Our solution combines accuracy, robustness and versatility of the UKF. We then compare our approach to state-of-art solutions in terms of accuracy, robustness and computational complexity on the EuRoC dataset and a challenging MAV outdoor dataset.


Title: Joint Ego-motion Estimation Using a Laser Scanner and a Monocular Camera Through Relative Orientation Estimation and 1-DoF ICP
Key Words: automobiles  cameras  iterative methods  laser ranging  mobile robots  motion estimation  optical scanners  pose estimation  sensor fusion  SLAM (robots)  joint ego-motion estimation  laser scanner  monocular camera  autonomous vehicles  SLAM algorithms  sensor suite  laser range finder  3D point clouds  iterative closest point problem  sensor modality  orientation estimation  autonomous cars  pose estimation  autonomous robots  1-DoF ICP  data association  Cameras  Iterative closest point algorithm  Lasers  Three-dimensional displays  Robot vision systems  Image color analysis 
Abstract: Pose estimation and mapping are key capabilities of most autonomous vehicles and thus a number of localization and SLAM algorithms have been developed in the past. Autonomous robots and cars are typically equipped with multiple sensors. Often, the sensor suite includes a camera and a laser range finder. In this paper, we consider the problem of incremental ego-motion estimation, using both, a monocular camera and a laser range finder jointly. We propose a new algorithm, that exploits the advantages of both sensors-the ability of cameras to determine orientations well and the ability of laser range finders to estimate the scale and to directly obtain 3D point clouds. Our approach estimates the 5 degrees of freedom relative orientation from image pairs through feature point correspondences and formulates the remaining scale estimation as a new variant of the iterative closest point problem with only one degree of freedom. We furthermore exploit the camera information in a new way to constrain the data association between laser point clouds. The experiments presented in this paper suggest that our approach is able to accurately estimate the ego-motion of a vehicle and that we obtain more accurate frame-to-frame alignments than with one sensor modality alone.


Title: Embedding Temporally Consistent Depth Recovery for Real-time Dense Mapping in Visual-inertial Odometry
Key Words: distance measurement  image reconstruction  interpolation  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  real-time dense mapping  visual-inertial odometry  dense scene information  fast self-localization  VIO-based SLAM systems  VIO depth estimations  subspace-based stabilization scheme  temporal consistency  edge-preserving depth interpolation  simultaneous localization and mapping  learning-based methods  embedding temporally consistent depth recovery  Simultaneous localization and mapping  Real-time systems  Feature extraction  Pipelines  Interpolation  Three-dimensional displays 
Abstract: Dense mapping is always the desire of simultaneous localization and mapping (SLAM), especially for the applications that require fast and dense scene information. Visual-inertial odometry (VIO) is a light-weight and effective solution to fast self-localization. However, VIO-based SLAM systems have difficulty in providing dense mapping results due to the spatial sparsity and temporal instability of the VIO depth estimations. Although there have been great efforts on real-time mapping and depth recovery from sparse measurements, the existing solutions for VIO-based SLAM still fail to preserve sufficient geometry details in their results. In this paper, we propose to embed depth recovery into VIO-based SLAM for real-time dense mapping. In the proposed method, we present a subspace-based stabilization scheme to maintain the temporal consistency and design a hierarchical pipeline for edge-preserving depth interpolation to reduce the computational burden. Numerous experiments demonstrate that our method can achieve an accuracy improvement of up to 49.1 cm compared to state-of-the-art learning-based methods for depth recovery and reconstruct sufficient geometric details in dense mapping when only 0.07% depth samples are available. Since a simple CPU implementation of our method already runs at 10-20 fps, we believe our method is very favorable for practical SLAM systems with critical computational requirements.


Title: Autonomous Localization, Navigation and Haustral Fold Detection for Robotic Endoscopy
Key Words: biological organs  biomedical optical imaging  cancer  endoscopes  medical image processing  medical robotics  surgery  autonomous localization  Haustral fold detection  robotic endoscopy  capsule endoscopes  minimally invasive devices  gastrointestinal abnormalities  colorectal cancer  real-time navigation system  observational devices  autonomous navigation  single minimally invasive device  vision system  autonomous lumen center tracking  haustral fold identification  multiple haustral folds  robotic endoscope platform  active simulator  real-time localization  center tracking algorithm  colonoscopy  in vivo video  surgical tools  mobility system  Endoscopes  Robot sensing systems  Colon  Navigation  Wheels  In vivo 
Abstract: Capsule endoscopes have gained popularity over the last decade as minimally invasive devices for diagnosing gastrointestinal abnormalities such as colorectal cancer. While this technology offers a less invasive and more convenient alternative to traditional scopes, these capsules are only able to provide observational capabilities due to their passive nature. With the addition of a reliable mobility system and a real-time navigation system, capsule endoscopes could transform from observational devices into active surgical tools, offering biopsy and therapeutic capabilities and even autonomous navigation in a single minimally invasive device. In this work, a vision system is developed to allow for autonomous lumen center tracking and haustral fold identification and tracking during colonoscopy. This system is tested for its ability to accurately identify and track multiple haustral folds across many frames in both simulated and in vivo video, and the lumen center tracking is tested onboard a robotic endoscope platform (REP) within an active simulator to demonstrate autonomous navigation. In addition, real-time localization is demonstrated using open source ORB-SLAM2. The vision system successfully identified 95.6% of Haustral folds in simulator frames and 70.6% in in vivo frames and false positives occurred in less than 1% of frames. The center tracking algorithm showed in vivo center estimates within a mean error of 6.6% of physician estimates and allowed for the REP to traverse 2 m of the active simulator in 6 minutes without intervention.


Title: PRISM: Pose Registration for Integrated Semantic Mapping
Key Words: mobile robots  multi-robot systems  navigation  pose estimation  service robots  SLAM (robots)  computer science department  modern SLAM algorithms  map data  tedious manual process  automatically generated maps  PRISM  semantic markup  pose registration  integrated semantic  robotics applications  hotel  room service  hospital  medication  patient  UT Austin  autonomous mobile robots  BWIBots  building-wide intelligence project  Robots  Semantics  Three-dimensional displays  Cameras  Two dimensional displays  Computational modeling  Navigation 
Abstract: Many robotics applications involve navigating to positions specified in terms of their semantic significance. A robot operating in a hotel may need to deliver room service to a named room. In a hospital, it may need to deliver medication to a patient's room. The Building-Wide Intelligence Project at UT Austin has been developing a fleet of autonomous mobile robots, called BWIBots, which perform tasks in the computer science department. Tasks include guiding a person, delivering a message, or bringing an object to a location such as an office, lecture hall, or classroom. The process of constructing a map that a robot can use for navigation has been simplified by modern SLAM algorithms. The attachment of semantics to map data, however, remains a tedious manual process of labeling locations in otherwise automatically generated maps. This paper introduces a system called PRISM to automate a step in this process by enabling a robot to localize door signs - a semantic markup intended to aid the human occupants of a building - and to annotate these locations in its map.


Title: C-blox: A Scalable and Consistent TSDF-based Dense Mapping Approach
Key Words: autonomous aerial vehicles  image reconstruction  image sensors  robot vision  SLAM (robots)  truncated signed distance field  TSDF subvolumes  lightweight micro aerial vehicle  scalable maps  map growth  bundle adjustment  feature-based camera tracking  dense 3D mapping  map consistency  delayed loop closure  accumulated camera tracking error  precise dense 3D maps  higher level decision making  robotic platforms  consistent dense map  Cameras  Simultaneous localization and mapping  Image reconstruction  Three-dimensional displays  Robot vision systems 
Abstract: In many applications, maintaining a consistent dense map of the environment is key to enabling robotic platforms to perform higher level decision making. Several works have addressed the challenge of creating precise dense 3D maps from visual sensors providing depth information. However, during operation over longer missions, reconstructions can easily become inconsistent due to accumulated camera tracking error and delayed loop closure. Without explicitly addressing the problem of map consistency, recovery from such distortions tends to be difficult. We present a novel system for dense 3D mapping which addresses the challenge of building consistent maps while dealing with scalability. Central to our approach is the representation of the environment as a collection of overlapping Truncated Signed Distance Field (TSDF) subvolumes. These subvolumes are localized through feature-based camera tracking and bundle adjustment. Our main contribution is a pipeline for identifying stable regions in the map, and to fuse the contributing subvolumes. This approach allows us to reduce map growth while still maintaining consistency. We demonstrate the proposed system on a publicly available dataset and simulation engine, and demonstrate the efficacy of the proposed approach for building consistent and scalable maps. Finally we demonstrate our approach running in real-time onboard a lightweight Micro Aerial Vehicle (MAV).


Title: Stereo Visual Odometry and Semantics based Localization of Aerial Robots in Indoor Environments
Key Words: distance measurement  image colour analysis  image segmentation  indoor environment  learning (artificial intelligence)  mobile robots  neural nets  object detection  particle filtering (numerical methods)  pose estimation  robot vision  SLAM (robots)  stereo image processing  indoor environments  particle filter localization approach  semantic information  mini-aerial robots  stereo VO algorithm  semantic measurements  pre-trained deep learning based object detector  3D point clouds  visual SLAM approach  stereo visual odometry  semantics based localization  DL  RGB spectrum  drift free pose estimation  Semantics  Three-dimensional displays  Unmanned aerial vehicles  Robots  Atmospheric measurements  Particle measurements  Prediction algorithms 
Abstract: In this paper we propose a particle filter localization approach, based on stereo visual odometry (VO) and semantic information from indoor environments, for mini-aerial robots. The prediction stage of the particle filter is performed using the 3D pose of the aerial robot estimated by the stereo VO algorithm. This predicted 3D pose is updated using inertial as well as semantic measurements. The algorithm processes semantic measurements in two phases; firstly, a pre-trained deep learning (DL) based object detector is used for real time object detections in the RGB spectrum. Secondly, from the corresponding 3D point clouds of the detected objects, we segment their dominant horizontal plane and estimate their relative position, also augmenting a prior map with new detections. The augmented map is then used in order to obtain a drift free pose estimate of the aerial robot. We validate our approach in several real flight experiments where we compare it against ground truth and a state of the art visual SLAM approach.


Title: Information Sparsification in Visual-Inertial Odometry
Key Words: computational complexity  distance measurement  graph theory  mobile robots  SLAM (robots)  information sparsification  tightly couple visual measurements  inertial measurements  fixed-lag visual-inertial odometry framework  bound computational complexity  fixed-lag smoothers  densely connected linear  information-theoretic perspective  dense marginalization step  information content  nonlinear factor graph  information loss  information sparsity  VIO methods  EuRoC visual-inertial dataset  structural similarity  nonlinearity  computational complexity  Optimization  Markov processes  Microsoft Windows  Computational complexity  Cameras  Simultaneous localization and mapping  Visualization 
Abstract: In this paper, we present a novel approach to tightly couple visual and inertial measurements in a fixed-lag visual-inertial odometry (VIO) framework using information sparsification. To bound computational complexity, fixed-lag smoothers typically marginalize out variables, but consequently introduce a densely connected linear prior which significantly deteriorates accuracy and efficiency. Current state-of-the-art approaches account for the issue by selectively discarding measurements and marginalizing additional variables. However, such strategies are sub-optimal from an information-theoretic perspective. Instead, our approach performs a dense marginalization step and preserves the information content of the dense prior. Our method sparsifies the dense prior with a nonlinear factor graph by minimizing the information loss. The resulting factor graph maintains information sparsity, structural similarity, and nonlinearity. To validate our approach, we conduct real-time drone tests and perform comparisons to current state-of-the-art fixed-lag VIO methods in the EuRoC visual-inertial dataset. The experimental results show that the proposed method achieves competitive and superior accuracy in almost all trials. We include a detailed run-time analysis to demonstrate that the proposed algorithm is suitable for real-time applications.


Title: DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments
Key Words: mobile robots  object detection  path planning  robot vision  SLAM (robots)  high-dynamic environments  ORB-SLAM2  dense semantic octo-tree map  dynamic objects  DS-SLAM combines semantic segmentation network  dense semantic map creation  local mapping  robust semantic visual SLAM  impressed SLAM systems  Semantics  Simultaneous localization and mapping  Image segmentation  Feature extraction  Heuristic algorithms  Three-dimensional displays  Optical flow 
Abstract: Simultaneous Localization and Mapping (SLAM) is considered to be a fundamental capability for intelligent mobile robots. Over the past decades, many impressed SLAM systems have been developed and achieved good performance under certain circumstances. However, some problems are still not well solved, for example, how to tackle the moving objects in the dynamic environments, how to make the robots truly understand the surroundings and accomplish advanced tasks. In this paper, a robust semantic visual SLAM towards dynamic environments named DS-SLAM is proposed. Five threads run in parallel in DS-SLAM: tracking, semantic segmentation, local mapping, loop closing and dense semantic map creation. DS-SLAM combines semantic segmentation network with moving consistency check method to reduce the impact of dynamic objects, and thus the localization accuracy is highly improved in dynamic environments. Meanwhile, a dense semantic octo-tree map is produced, which could be employed for high-level tasks. We conduct experiments both on TUM RGB-D dataset and in real-world environment. The results demonstrate the absolute trajectory accuracy in DS-SLAM can be improved one order of magnitude compared with ORB-SLAM2. It is one of the state-of-the-art SLAM systems in high-dynamic environments.


Title: A robust pose graph approach for city scale LiDAR mapping
Key Words: graph theory  image filtering  image reconstruction  Kalman filters  mobile robots  nonlinear filters  optical radar  optimisation  pose estimation  radar imaging  robot vision  SLAM (robots)  map quality  quantitative experimental results  robust optimization strategy  systematical initialization bias  factor graph  refined structure  urban environments  multitask acquisitions  scan-matching factors  graph optimization  cumulative drift  city scale LiDAR mapping  robust pose graph approach  Optimization  Three-dimensional displays  Laser radar  Global Positioning System  Feature extraction  Sensors  Urban areas 
Abstract: This paper presents a method for reconstructing globally consistent 3D High-Definition (HD) maps at city scale. Current approaches for eliminating cumulative drift are mainly based on the pose graph optimization under the constraint of scan-matching factors. The misaligned edges in the graph may have negative impacts on the results. To address this problem and further handle inconsistency caused by multi-task acquisitions in urban environments, we introduce a refined structure of the factor graph considering systematical initialization bias, where the scan-matching factors are twice validated through a novel classifier and a robust optimization strategy. In addition, we incorporate a multi-hypothesis extended Kalman filter (MH-EKF) to remove dynamic objects. Quantitative experimental results demonstrate that the proposed method outperforms state-of-the-art techniques in terms of map quality.


Title: Good Feature Selection for Least Squares Pose Optimization in VO/VSLAM
Key Words: computational complexity  control engineering computing  feature extraction  least squares approximations  optimisation  pose estimation  robot vision  SLAM (robots)  least squares pose optimization  pose estimation  pose tracking  NP-hard Max-logDet problem  feature selection  VO-VSLAM  integrating Max-logDet feature selection  Feature extraction  Optimization  Pose estimation  Simultaneous localization and mapping  Measurement uncertainty  Approximation algorithms 
Abstract: This paper aims to select features that contribute most to the pose estimation in VO/VSLAM. Unlike existing feature selection works that are focused on efficiency only, our method significantly improves the accuracy of pose tracking, while introducing little overhead. By studying the impact of feature selection towards least squares pose optimization, we demonstrate the applicability of improving accuracy via good feature selection. To that end, we introduce the Max-logDet metric to guide the feature selection, which is connected to the conditioning of least squares pose optimization problem. We then describe an efficient algorithm for approximately solving the NP-hard Max-logDet problem. Integrating Max-logDet feature selection into a state-of-the-art visual SLAM system leads to accuracy improvements with low overhead, as demonstrated via evaluation on a public benchmark.


Title: HMAPs - Hybrid Height- Voxel Maps for Environment Representation
Key Words: mobile robots  optical radar  path planning  robot vision  SLAM (robots)  2.5D representation  Microsoft Kinect One  SLAM approach  complex elements  Velodyne VLP-16 LiDAR  updated grid representation  complex environments  reliable method  occupied space  free space  HVoxel  height-voxel elements  3D point-clouds  mobile robot  grid-based mapping approach  environment representation  hybrid height- voxel maps  HMAP  Two dimensional displays  Three-dimensional displays  Simultaneous localization and mapping  Pipelines  Ray tracing  Planning  Indexing 
Abstract: This paper presents a hybrid 3D-like grid-based mapping approach, that we called HMAP, used as a reliable and efficient 3D representation of the environment surrounding a mobile robot. Considering 3D point-clouds as input data, the proposed mapping approach addresses the representation of height-voxel (HVoxel) elements inside the HMAP, where free and occupied space is modeled through HVoxels, resulting in a reliable method for 3D representation. The proposed method corrects some of the problems inherent to the representation of complex environments based on 2D and 2.5D representations, while keeping an updated grid representation. Additionally, we also propose a complete pipeline for SLAM based on HMAPs. Indoor and outdoor experiments were carried out to validate the proposed representation using data from a Microsoft Kinect One (indoor) and a Velodyne VLP-16 LiDAR (outdoor). The obtained results show that HMAPs can provide a more detailed view of complex elements in a scene when compared to a classic 2.5D representation. Moreover, validation of the proposed SLAM approach was carried out in an outdoor dataset with promising results, which lay a foundation for further research in the topic.


Title: Indoor Mapping and Localization for Pedestrians using Opportunistic Sensing with Smartphones
Key Words: Bayes methods  Gaussian processes  indoor radio  mobile computing  mobile robots  optimisation  particle filtering (numerical methods)  path planning  radionavigation  regression analysis  SLAM (robots)  smart phones  wireless LAN  Gaussian Processes Regression  real-time localization  GPR variance map  pseudowall constraints  magnetic fields  globally consistent trajectories  opportunistic magnetic headings  WiFi signal similarity validation  magnetic sequence matching  loop-closure constraints  pedestrian dead-reckoning  motion constraints  GraphSLAM front-end  signal maps  Bayesian filtering-based online localization  GraphSLAM-based offline mapping  ambient indoor environments  low-cost indoor mapping  indoor localization  smartphone  size 2.3 m  size 3.41 m  Wireless fidelity  Trajectory  Smart phones  Simultaneous localization and mapping  Ground penetrating radar  Legged locomotion 
Abstract: Indoor localization for pedestrians has gained increasing popularity among the rich body of literature for the last decade. In this paper, a low-cost indoor mapping and localization solution is proposed using the opportunistic signals from ambient indoor environments with a smartphone. It is composed of GraphSLAM-based offline mapping and Bayesian filtering-based online localization using generated signal maps. The GraphSLAM front-end is constructed by motion constraints from pedestrian dead-reckoning (PDR), loop-closure constraints identified by magnetic sequence matching with WiFi signal similarity validation, and observation constraints from opportunistic magnetic headings after error rejection. Globally consistent trajectories are created by graph optimization, after which signal maps (e.g., WiFi, magnetic fields, lights) are generated by Gaussian Processes Regression (GPR) for later localization. We propose to use the pseudo-wall constraints from the GPR variance map of magnetic fields and the lights measurements as observations for particle filtering. The proposed method is evaluated on several datasets collected from both the in-compass office buildings and outside public areas. Real-time localization is demonstrated on a smartphone in an office building covering 2000 square meters with the 50- and 90-percentile accuracies being 2.30 m and 3.41 m, respectively.


Title: The TUM VI Benchmark for Evaluating Visual-Inertial Odometry
Key Words: augmented reality  calibration  cameras  distance measurement  image capture  image sensors  image sequences  mobile robots  optical tracking  pose estimation  robot vision  SLAM (robots)  synchronisation  visual-inertial odometry  photometric calibration  motion capture system  IMU measurements  pose ground truth  inertial measurements  vision sensors  augmented reality  SLAM methods  visual odometry  IMU sensors  camera images  TUM VI benchmark  frequency 20.0 Hz  frequency 200.0 Hz  frequency 120.0 Hz  Cameras  Calibration  Simultaneous localization and mapping  Benchmark testing  Visual odometry  Time measurement 
Abstract: Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20 Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120 Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data is publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.


Title: Scale-Robust Localization Using General Object Landmarks
Key Words: distance measurement  feature extraction  learning (artificial intelligence)  mobile robots  object detection  robot vision  SLAM (robots)  deep-learning-based object features  KITTI Odometry benchmark  outdoor images  scale-robust localization  visual localization  robotic mapping applications  object landmarks  SIFT point-features  Visualization  Measurement  Simultaneous localization and mapping  Robustness  Databases  Search problems 
Abstract: Visual localization under large changes in scale is an important capability in many robotic mapping applications, such as localizing at low altitudes in maps built at high altitudes, or performing loop closure over long distances. Existing approaches, however, are robust only up to about a 3× difference in scale between map and query images. We propose a novel combination of deep-learning-based object features and state-of-the-art SIFT point-features that yields improved robustness to scale change. This technique is training-free and class-agnostic, and in principle can be deployed in any environment out-of-the-box. We evaluate the proposed technique on the KITTI Odometry benchmark and on a novel dataset of outdoor images exhibiting changes in visual scale of 7× and greater, which we have released to the public. Our technique consistently outperforms localization using either SIFT features or the proposed object features alone, achieving both greater accuracy and much lower failure rates under large changes in scale.


Title: Invariant smoothing on Lie Groups
Key Words: estimation theory  Kalman filters  Lie groups  linearisation techniques  optimisation  robot vision  SLAM (robots)  smoothing methods  linearizations  invariant Kalman filtering  robot localization  posteriori estimator  nonlinear smoothing methods  group-affine observation systems  Lie groups  invariant smoothing  Smoothing methods  Manifolds  Simultaneous localization and mapping  Kalman filters  Random variables  Estimation  Robot localization 
Abstract: In this paper we propose a (non-linear) smoothing algorithm for group-affine observation systems, a recently introduced class of estimation problems on Lie groups that bear a particular structure. As most non-linear smoothing methods, the proposed algorithm is based on a maximum a posteriori estimator, determined by optimization. But owing to the specific properties of the considered class of problems, the involved linearizations are proved to have a form of independence with respect to the current estimates, leveraged to avoid (partially or sometimes totally) the need to relinearize. The method is validated on a robot localization example, both in simulations and on real experimental data.


Title: A Combined RGB and Depth Descriptor for SLAM with Humanoids
Key Words: cameras  feature extraction  humanoid robots  image colour analysis  mobile robots  pose estimation  robot vision  SLAM (robots)  feature tracking  codebooks  reproducibility  humanoid robots  visual simultaneous localization  depth descriptor  ORB-SLAM  visual SLAM system  track features  DLab  RGB-D camera  Nao humanoid  binary descriptor  FAB-MAP  place recognition module  Simultaneous localization and mapping  Image color analysis  Cameras  Three-dimensional displays  Humanoid robots  Visualization 
Abstract: In this paper, we present a visual simultaneous localization and mapping (SLAM) system for humanoid robots. We introduce a new binary descriptor called DLab that exploits the combined information of color, depth, and intensity to achieve robustness with respect to uniqueness, reproducibility, and stability. We use DLab within ORB-SLAM, where we replaced the place recognition module with a modification of FAB-MAP that works with newly built codebooks using our binary descriptor. In experiments carried out in simulation and with a real Nao humanoid equipped with an RGB-D camera, we show that DLab has a superior performance in comparison to other descriptors. The application to feature tracking and place recognition reveal that the new descriptor is able to reliably track features even in sequences with seriously blurred images and that it has a higher percentage of correctly identified similar images. As a result, our new visual SLAM system has a lower absolute trajectory error in comparison to ORB-SLAM and is able to accurately track the robot's trajectory.


Title: Pose Estimation and Map Formation with Spiking Neural Networks: towards Neuromorphic SLAM
Key Words: mixed analogue-digital integrated circuits  mobile robots  neural nets  neurophysiology  pose estimation  SLAM (robots)  pose estimation  spiking neural networks  neuromorphic SLAM  biologically inspired neuronal path integration  mobile robot  neuronal map formation architecture  simultaneous localization and mapping  mixed signal analog-digital neuromorphic hardware  ultra low-power neuromorphic hardware  robotic vehicle simulation  on-board plasticity  Neurons  Neuromorphics  Collision avoidance  Simultaneous localization and mapping  Synapses 
Abstract: In this paper, we investigate the use of ultra low-power, mixed signal analog/digital neuromorphic hardware for implementation of biologically inspired neuronal path integration and map formation for a mobile robot. We perform spiking network simulations of the developed architecture, interfaced to a simulated robotic vehicle. We then port the neuronal map formation architecture on two connected neuromorphic devices, one of which features on-board plasticity, and demonstrate the feasibility of a neuromorphic realization of simultaneous localization and mapping (SLAM).


Title: Virtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments
Key Words: graph theory  image reconstruction  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  3D scene reconstructions  virtual occupancy grid map  mobile robots  VOG-map  submap-based pose graph SLAM  underwater SLAM system  path planning  free space information  Simultaneous localization and mapping  Three-dimensional displays  Path planning  Robot kinematics  Casting 
Abstract: In this paper, we propose a mapping approach that constructs a globally deformable virtual occupancy grid map (VOG-map) based on local submaps. Such a representation allows pose graph SLAM systems to correct globally accumulated drift via loop closures while maintaining free space information for the purpose of path planning. We demonstrate use of such a representation for implementing an underwater SLAM system in which the robot actively plans paths to generate accurate 3D scene reconstructions. We evaluate performance on simulated as well as real-world experiments. Our work furthers capabilities of mobile robots actively mapping and exploring unstructured, three dimensional environments.


Title: LDSO: Direct Sparse Odometry with Loop Closure
Key Words: feature extraction  graph theory  mobile robots  optimisation  pose estimation  robot vision  SLAM (robots)  intensity gradient  DSO sliding window optimization  Sim(3) relative pose constraints  image pixel  loop closure detection  monocular visual SLAM system  Direct Sparse Odometry  state-of-the-art feature-based systems  pose-graph optimization  modified point selection strategy  relative poses  co-visibility graph  3D geometric error terms  conventional feature-based bag-of-words approach  loop closure candidates  tracking frontend  corner features  LDSO  featureless areas  Optimization  Feature extraction  Microsoft Windows  Simultaneous localization and mapping  Cameras  Bundle adjustment  Robustness 
Abstract: In this paper we present an extension of Direct Sparse Odometry (DSO) [1] to a monocular visual SLAM system with loop closure detection and pose-graph optimization (LDSO). As a direct technique, DSO can utilize any image pixel with sufficient intensity gradient, which makes it robust even in featureless areas. LDSO retains this robustness, while at the same time ensuring repeatability of some of these points by favoring corner features in the tracking frontend. This repeatability allows to reliably detect loop closure candidates with a conventional feature-based bag-of-words (BoW) approach. Loop closure candidates are verified geometrically and Sim(3) relative pose constraints are estimated by jointly minimizing 2D and 3D geometric error terms. These constraints are fused with a co-visibility graph of relative poses extracted from DSO's sliding window optimization. Our evaluation on publicly available datasets demonstrates that the modified point selection strategy retains the tracking accuracy and robustness, and the integrated pose-graph optimization significantly reduces the accumulated rotation-, translation- and scale-drift, resulting in an overall performance comparable to state-of-the-art feature-based systems, even without global bundle adjustment.


Title: Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correction
Key Words: accelerometers  calibration  cameras  distance measurement  gyroscopes  inertial navigation  learning (artificial intelligence)  mobile robots  neural nets  pose estimation  robot vision  SLAM (robots)  vision-aided absolute trajectory estimation  unsupervised deep network  online error correction  unsupervised deep neural network approach  RGB-D imagery  inertial measurements  Visual-Inertial-Odometry Learner  inertial measurement unit intrinsic parameters  white noise  extrinsic calibration  camera  IMU measurements  hypothesis trajectories  scaled image projection errors  visual odometry  visual simultaneous localization  KITTI Odometry dataset  competitive odometry performance  visual-inertial odometry  Cameras  Jacobian matrices  Image reconstruction  Trajectory  Simultaneous localization and mapping  Training 
Abstract: Adstract- We present an unsupervised deep neural network approach to the fusion of RGB-D imagery with inertial measurements for absolute trajectory estimation. Our network, dubbed the Visual-Inertial-Odometry Learner (VIOLearner), learns to perform visual-inertial odometry (VIO) without inertial measurement unit (IMU) intrinsic parameters (corresponding to gyroscope and accelerometer bias or white noise) or the extrinsic calibration between an IMU and camera. The network learns to integrate IMU measurements and generate hypothesis trajectories which are then corrected online according to the Jacobians of scaled image projection errors with respect to a spatial grid of pixel coordinates. We evaluate our network against state-of-the-art (SOA) visual-inertial odometry, visual odometry, and visual simultaneous localization and mapping (VSLAM) approaches on the KITTI Odometry dataset [1] and demonstrate competitive odometry performance.


Title: A B-Spline Mapping Framework for Long-Term Autonomous Operations
Key Words: image representation  image sensors  mobile robots  navigation  path planning  robot vision  SLAM (robots)  splines (mathematics)  landmark-based maps  robotics community  high frequency sensor  B-spline curves  B-spline maps  mapping algorithm  2D B-spline mapping framework  outdoor long-term autonomous operations  simultaneous localization and mapping  SLAM algorithm  software-in-the-loop simulations  Splines (mathematics)  Simultaneous localization and mapping  Three-dimensional displays  Robot kinematics  Two dimensional displays 
Abstract: This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.


Title: Robust Exploration with Multiple Hypothesis Data Association
Key Words: image fusion  mobile robots  robot vision  SLAM (robots)  target tracking  tree searching  joint compatibility branch  simultaneous localization and mapping  map accuracy  diverse hypotheses  multiple hypothesis tracking  robust back-ends  catastrophic failure  single false positive assignment  rich features  autonomous exploration  SLAM  ambiguous data association problem  multiple hypothesis data association  robust exploration  Simultaneous localization and mapping  Trajectory  Noise measurement  State estimation  Optimization  Measurement uncertainty 
Abstract: We study the ambiguous data association problem confronting simultaneous localization and mapping (SLAM), specifically for the autonomous exploration of environments lacking rich features. In such environments, a single false positive assignment might lead to catastrophic failure, which even robust back-ends may be unable to resolve. Inspired by multiple hypothesis tracking, we present a novel approach to effectively manage multiple hypotheses (MH) of data association inherited from traditional joint compatibility branch and bound (JCBB), which entails the generation, ordering and elimination of hypotheses. We analyze the performance of MHJCBB in two particular situations, one applying it to SLAM over a predefined trajectory and the other showing its applicability in exploring unknown environments. Statistical results demonstrate that MHJCBB's maintenance of diverse hypotheses under ambiguous conditions significantly improves map accuracy.


Title: A Variational Feature Encoding Method of 3D Object for Probabilistic Semantic SLAM
Key Words: Bayes methods  belief networks  feature extraction  maximum likelihood estimation  object recognition  probability  robot vision  SLAM (robots)  object recognition methods  true generative model  semantic simultaneous localization and mapping  maximum likelihood estimation  shape retrieval  Bayesian inference  Bayesian networks  approximated distributions  variational auto-encoder  complex distributions  observation likelihood  tractable distributions  3D object shapes  view-independent loop closure  object shape  range sensor  mobile robot  complex probability distribution  probabilistic observation model  high-level semantic features  complex 3D objects  probabilistic semantic SLAM  variational feature encoding method  Shape  Simultaneous localization and mapping  Three-dimensional displays  Semantics  Solid modeling  Bayes methods  Probabilistic logic 
Abstract: This paper presents a feature encoding method of complex 3D objects for high-level semantic features. Recent approaches to object recognition methods become important for semantic simultaneous localization and mapping (SLAM). However, there is a lack of consideration of the probabilistic observation model for 3D objects, as the shape of a 3D object basically follows a complex probability distribution. Furthermore, since the mobile robot equipped with a range sensor observes only a single view, much information of the object shape is discarded. These limitations are the major obstacles to semantic SLAM and view-independent loop closure using 3D object shapes as features. In order to enable the numerical analysis for the Bayesian inference, we approximate the true observation model of 3D objects to tractable distributions. Since the observation likelihood can be obtained from the generative model, we formulate the true generative model for 3D object with the Bayesian networks. To capture these complex distributions, we apply a variational auto-encoder. To analyze the approximated distributions and encoded features, we perform classification with maximum likelihood estimation and shape retrieval.


Title: Online Temporal Calibration for Monocular Visual-Inertial Systems
Key Words: calibration  cameras  inertial systems  motion estimation  optimisation  robot vision  sensor fusion  SLAM (robots)  monocular visual-inertial systems  accurate state estimation  intelligent applications  robot navigation  autonomous driving  virtual reality  augmented reality  visual fusion  inertial fusion  sensor fusion  visual measurements  inertial measurements  IMU states  SLAM system  feature-based optimization frameworks  Cameras  Sensors  Delays  Calibration  Visualization  Clocks  Three-dimensional displays 
Abstract: Accurate state estimation is a fundamental module for various intelligent applications, such as robot navigation, autonomous driving, virtual and augmented reality. Visual and inertial fusion is a popular technology for 6-DOF state estimation in recent years. Time instants at which different sensors' measurements are recorded are of crucial importance to the system's robustness and accuracy. In practice, timestamps of each sensor typically suffer from triggering and transmission delays, leading to temporal misalignment (time offsets) among different sensors. Such temporal offset dramatically influences the performance of sensor fusion. To this end, we propose an online approach for calibrating temporal offset between visual and inertial measurements. Our approach achieves temporal offset calibration by jointly optimizing time offset, camera and IMU states, as well as feature locations in a SLAM system. Furthermore, the approach is a general model, which can be easily employed in several feature-based optimization frameworks. Simulation and experimental results demonstrate the high accuracy of our calibration approach even compared with other state-of-art offline tools. The VIO comparison against other methods proves that the online temporal calibration significantly benefits visual-inertial systems. The source code of temporal calibration is integrated into our public project, VINS-Mono1.


Title: Trifo-VIO: Robust and Efficient Stereo Visual Inertial Odometry Using Points and Lines
Key Words: computer graphics  distance measurement  filtering theory  graph theory  image matching  Kalman filters  mobile robots  nonlinear filters  pose estimation  robot vision  SLAM (robots)  stereo image processing  efficient stereo Visual Inertial Odometry  stereo VIO system  line features  system robustness  point features  low-texture environment  lightweight filtering-based loop closing technique  global bundle adjustment  graph optimization  current sliding window  Trifo Ironsides dataset  visual-inertial dataset  high-quality synchronized stereo camera  Cameras  Optimization  Visualization  Feature extraction  Image edge detection  Three-dimensional displays  Robot sensing systems 
Abstract: In this paper, we present the Trifo Visual Inertial Odometry (Trifo-VIO), a tightly-coupled filtering-based stereo VIO system using both points and lines. Line features help improve system robustness in challenging scenarios when point features cannot be reliably detected or tracked, e.g. low-texture environment or lighting change. In addition, we propose a novel lightweight filtering-based loop closing technique to reduce accumulated drift without global bundle adjustment or pose graph optimization. We formulate loop closure as EKF updates to optimally relocate the current sliding window maintained by the filter to past keyframes. We also present the Trifo Ironsides dataset, a new visual-inertial dataset, featuring high-quality synchronized stereo camera and IMU data from the Ironsides sensor [3] with various motion types and textures and millimeter-accuracy groundtruth. To validate the performance of the proposed system, we conduct extensive comparison with state-of-the-art approaches (OKVIS, VINS-MONO and S-MSCKF) using both the public EuRoC dataset and the Trifo Ironsides dataset.


Title: Scale Correct Monocular Visual Odometry Using a LiDAR Altimeter
Key Words: distance measurement  image sequences  mobile robots  optical radar  robot vision  SLAM (robots)  stereo image processing  stereo visual SLAM  monocular vision  inherent scale ambiguity  LiDAR altimeter  scale correct monocular visual odometry  RGB-D methods  scale drift  keyframe basis  scale constraint  mapping algorithm  keyframe based tracking  Visual Odometry method  laser altimeter  range data  exploration vehicles  power requirements  computational load  metrological accuracy  RGB-D sensors  3D LiDARs  metric references  sensory sources  Cameras  Laser radar  Measurement  Three-dimensional displays  Visual odometry  Visualization  Sensors 
Abstract: The inherent scale ambiguity in monocular vision is a well known issue that forces the integration of other sensory sources to obtain metric references. However, 2D or 3D LiDARs and RGB-D sensors, while guaranteeing metrological accuracy, impose a non negligible burden both in terms of computational load and power requirements limiting the feasibility of being implemented on small exploration vehicles. This paper presents a scale aware monocular Visual Odometry framework that fuses range data from a laser altimeter in order to recover and maintain a correct metric scale. The proposed Visual Odometry method consists of a keyframe based tracking and mapping algorithm using optical flow where range data serves as a scale constraint on a keyframe to keyframe basis. An optimization backend based on iSAM2 is employed in order to refine the trajectory and map estimates eliminating the scale drift without the need of performing loop closures. We demonstrate that our algorithm can obtain very similar performances to state of the art stereo visual SLAM and RGB-D methods.


Title: ArthroSLAM: Multi-Sensor Robust Visual Localization for Minimally Invasive Orthopedic Surgery
Key Words: biomedical optical imaging  cameras  endoscopes  image sensors  Kalman filters  medical image processing  medical robotics  orthopaedics  SLAM (robots)  surgery  image feedback  ArthroSLAM  Simultaneous Localisation and Mapping system  SLAM system  external camera  robotic arm  minimally invasive arthroscopic surgery  minimally invasive orthopedic surgery  robotic orthopedic surgical assistant  knee section  human cadaver knee joint  Extended Kalman Filter framework  arthroscope holder  intraarticular space  Cameras  Robot vision systems  Visualization  Reliability 
Abstract: Minimally invasive arthroscopic surgery is a very challenging procedure that requires the manipulation of instruments in limited intraarticular space using distorted and sometimes uninformative images. Localizing the arthroscope reliably and at all times w.r.t. surrounding tissue is of fundamental importance to prevent unintended injury to patients. However, even highly-trained surgeons can struggle to localize the arthro-scope using poor image feedback. In this paper, we propose and demonstrate for the first time a visual Simultaneous Localisation and Mapping (SLAM) system, termed ArthroSLAM, capable of robustly and reliably localizing an arthroscope inside a human knee joint. The proposed system fuses the information obtained from the arthroscope, an external camera mounted on an arthroscope holder, and the odometry of a robotic arm manipulating the scope, in an Extended Kalman Filter framework. Also for the first time, we implement five alternative strategies for localization and compare them to our method in a realistic setup with a human cadaver knee joint. ArthroSLAM is shown to outperform the alternative strategies under various challenging conditions, localizing reliably and at all times with a mean Relative Pose Error of up to 1.4mm and 0.7°. Additional experiments conducted with degraded odometry data also validate the robustness of the method. An initial evaluation of the sparse map of a knee section computed by our method exhibits good morphological agreement. All results suggest that ArthroSLAM is a viable component for the robotic orthopedic surgical assistant of the future.


Title: Robot Identification and Localization with Pointing Gestures
Key Words: distance measurement  gesture recognition  mobile robots  multi-robot systems  pose estimation  robot vision  SLAM (robots)  mobile robot  multirobot scenarios  robot identification and localization  gesture pointing  robot odometry frame  inertial measurement unit  IMU  Robot sensing systems  Robot kinematics  Solid modeling  Manipulators  Drones  Three-dimensional displays 
Abstract: We propose a novel approach to establish the relative pose of a mobile robot with respect to an operator that wants to interact with it; we focus on scenarios in which the robot is in the same environment as the operator, and is visible to them. The approach is based on comparing the trajectory of the robot, which is known in the robot's odometry frame, to the motion of the arm of the operator, who, for a short time, keeps pointing at the robot they want to interact with. In multi-robot scenarios, the same approach can be used to simultaneously identify which robot the operator wants to interact with. The main advantage over alternatives is that our system only relies on the robot's odometry, on a wearable inertial measurement unit (IMU), and, crucially, on the operator's own perception. We experimentally show the feasibility of our approach using real-world robots.


Title: Semi-Supervised SLAM: Leveraging Low-Cost Sensors on Underground Autonomous Vehicles for Position Tracking
Key Words: cameras  learning (artificial intelligence)  mining  mining industry  mobile robots  object tracking  robot vision  SLAM (robots)  ORB-SLAM2  ground map locations  deep learning  position tracking  operational underground mining vehicles  single camera localization  map creation  mine environment  mining companies  underground environment  SemiSupervised SLAM  underground autonomous vehicles  low-cost sensors  Simultaneous localization and mapping  Cameras  Measurement  Grounding  Visual odometry  Lighting 
Abstract: This work presents Semi-Supervised SLAM - a method for developing a map suitable for coarse localization within an underground environment with minimal human intervention, with system characteristics driven by real-world requirements of major mining companies. This work leverages existing information common within a mining environment - namely a surveyed mine map - which is used to sparsely ground map locations within the mine environment, increasing map accuracy and allowing localization within a global frame. Map creation utilizes a low cost camera sensor and minimal user information to produce a map which can be used for single camera localization within a mining environment. We evaluate the localization capabilities of the proposed approach in depth by performing data collection on operational underground mining vehicles within an active underground mine and by simulating occlusions common to the environment such as dust and water. The proposed system is capable of producing maps which have an average localization error 2.5 times smaller than the next best performing method ORB-SLAM2, comparable localization performance to a state-of-the-art deep learning approach (which is not a feasible solution due to both compute and training requirements) and is robust to simulated environmental obscurants.


Title: LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain
Key Words: embedded systems  feature extraction  image segmentation  optical radar  optimisation  pose estimation  robot vision  SLAM (robots)  SLAM framework  edge features  feature extraction  point cloud segmentation  lightweight and ground-optimized lidar odometry  real-time six degree-of-freedom pose estimation  low-power embedded system  ground plane  two-step Levenberg-Marquardt optimization method  optimization steps  ground vehicles  LeGO-LOAM  Feature extraction  Three-dimensional displays  Laser radar  Image segmentation  Pose estimation  Real-time systems  Iterative closest point algorithm 
Abstract: We propose a lightweight and ground-optimized lidar odometry and mapping method, LeGO-LOAM, for realtime six degree-of-freedom pose estimation with ground vehicles. LeGO-LOAM is lightweight, as it can achieve realtime pose estimation on a low-power embedded system. LeGO-LOAM is ground-optimized, as it leverages the presence of a ground plane in its segmentation and optimization steps. We first apply point cloud segmentation to filter out noise, and feature extraction to obtain distinctive planar and edge features. A two-step Levenberg-Marquardt optimization method then uses the planar and edge features to solve different components of the six degree-of-freedom transformation across consecutive scans. We compare the performance of LeGO-LOAM with a state-of-the-art method, LOAM, using datasets gathered from variable-terrain environments with ground vehicles, and show that LeGO-LOAM achieves similar or better accuracy with reduced computational expense. We also integrate LeGO-LOAM into a SLAM framework to eliminate the pose estimation error caused by drift, which is tested using the KITTI dataset.


Title: Learning a Local Feature Descriptor for 3D LiDAR Scans
Key Words: convolutional neural nets  feature extraction  image matching  image representation  learning (artificial intelligence)  robot vision  SLAM (robots)  learned feature descriptor  3D local descriptors  local feature descriptor  3D LiDAR scans  robust data association  scan alignment algorithms  handcrafted feature descriptors  metric learning network  local surface patches  convolutional neural network  ground-truth correspondences  SLAM system  CNN  Siamese network  Three-dimensional displays  Measurement  Laser radar  Feature extraction  Streaming media  Task analysis  Gray-scale 
Abstract: Robust data association is necessary for virtually every SLAM system and finding corresponding points is typically a preprocessing step for scan alignment algorithms. Traditionally, handcrafted feature descriptors were used for these problems but recently learned descriptors have been shown to perform more robustly. In this work, we propose a local feature descriptor for 3D LiDAR scans. The descriptor is learned using a Convolutional Neural Network (CNN). Our proposed architecture consists of a Siamese network for learning a feature descriptor and a metric learning network for matching the descriptors. We also present a method for estimating local surface patches and obtaining ground-truth correspondences. In extensive experiments, we compare our learned feature descriptor with existing 3D local descriptors and report highly competitive results for multiple experiments in terms of matching accuracy and computation time.


Title: Laser Map Aided Visual Inertial Localization in Changing Environment
Key Words: cameras  geometry  optical radar  optimisation  robot vision  SLAM (robots)  map optimization  changing environment  bi-directional tasks  LiDAR-built map  online visual inertial odometry system  laser map aided visual inertial localization  geometry information  crossmodal data association  multisession laser  Visualization  Lasers  Bundle adjustment  Laser radar  Robots  Cameras 
Abstract: Long-term visual localization in outdoor environment is a challenging problem, especially faced with the cross-seasonal, bi-directional tasks and changing environment. In this paper we propose a novel visual inertial localization framework that localizes against the LiDAR-built map. Based on the geometry information of the laser map, a hybrid bundle adjustment framework is proposed, which estimates the poses of the cameras with respect to the prior laser map as well as optimizes the state variables of the online visual inertial odometry system simultaneously. For more accurate crossmodal data association, the laser map is optimized using multisession laser and visual data to extract the salient and stable subset for visual localization. To validate the efficiency of the proposed method, we collect data in south part of our campus in different seasons, along the same and opposite-direction route. In all sessions of localization data, our proposed method gives satisfactory results, and shows the superiority of the hybrid bundle adjustment and map optimization1.


Title: Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map
Key Words: feature extraction  optical radar  robot vision  SLAM (robots)  stereo image processing  simultaneous localization and mapping  scan context performance  Light Detection and Ranging scans  visual scenes  two-phase search algorithm  3D LiDAR scans  loop-detection invariant  nonhistogram-based global descriptor  global localization  diverse sensors  dense 3D maps  structural information  diverse feature detectors  3D point cloud map  place recognition  Three-dimensional displays  Sensors  Laser radar  Histograms  Shape  Visualization  Encoding 
Abstract: Compared to diverse feature detectors and descriptors used for visual scenes, describing a place using structural information is relatively less reported. Recent advances in simultaneous localization and mapping (SLAM) provides dense 3D maps of the environment and the localization is proposed by diverse sensors. Toward the global localization based on the structural information, we propose Scan Context, a non-histogram-based global descriptor from 3D Light Detection and Ranging (LiDAR) scans. Unlike previously reported methods, the proposed approach directly records a 3D structure of a visible space from a sensor and does not rely on a histogram or on prior training. In addition, this approach proposes the use of a similarity score to calculate the distance between two scan contexts and also a two-phase search algorithm to efficiently detect a loop. Scan context and its search algorithm make loop-detection invariant to LiDAR viewpoint changes so that loops can be detected in places such as reverse revisit and corner. Scan context performance has been evaluated via various benchmark datasets of 3D LiDAR scans, and the proposed method shows a sufficiently improved performance.


Title: Keyframe-Based Photometric Online Calibration and Color Correction
Key Words: calibration  cameras  computer vision  image colour analysis  image motion analysis  image texture  photometry  pose estimation  splines (mathematics)  exposure estimation  thin plate splines  meshing algorithms  camera view poses estimation  sparse visual SLAM  gamma curve  structure-from-motion system  textured meshes  camera response function  sparsely sampled scene points  sixth-order polynomial  TPS  camera view  illumination  uniformly illuminated surfaces  global-shutter color cameras  real-time online vignetting  constant intensity  photoconsistency  computer vision algorithms  vignetting function  color correction  keyframe-based photometric online calibration  Cameras  Calibration  Splines (mathematics)  Image color analysis  Lighting  Estimation  Visualization 
Abstract: Finding the parameters of a vignetting function for a camera currently involves the acquisition of several images in a given scene under very controlled lighting conditions, a cumbersome and error-prone task where the end result can only be confirmed visually. Many computer vision algorithms assume photoconsistency, constant intensity between scene points in different images, and tend to perform poorly if this assumption is violated. We present a real-time online vignetting and response calibration with additional exposure estimation for global-shutter color cameras. Our method does not require uniformly illuminated surfaces, known texture or specific geometry. The only assumptions are that the camera is moving, the illumination is static and reflections are Lambertian. Our method estimates the camera view poses by sparse visual SLAM and models the vignetting function by a small number of thin plate splines (TPS) together with a sixth-order polynomial to provide a dense estimation of attenuation from sparsely sampled scene points. The camera response function (CRF) is jointly modeled by a TPS and a Gamma curve. We evaluate our approach on synthetic datasets and in real-world scenarios with reference data from a Structure-from-Motion (SfM) system. We show clear visual improvement on textured meshes without the need for extensive meshing algorithms. A useful calibration is obtained from a few keyframes which makes an on-the-fly deployment conceivable.


Title: Stereo Camera Localization in 3D LiDAR Maps
Key Words: cameras  Global Positioning System  image matching  image reconstruction  mobile robots  optical radar  pose estimation  robot vision  SLAM (robots)  stereo image processing  stereo disparity map  average localization error  stereo camera localization  Global Positioning System  3D LiDAR maps  simultaneous localization and mapping techniques  SLAM techniques  3D light detection and ranging sensors  visual positioning algorithm  GPS signal  visual tracking  six degree of freedom  DOF  camera pose estimation  KITTI dataset  Cameras  Three-dimensional displays  Laser radar  Simultaneous localization and mapping  Visualization  Global Positioning System 
Abstract: As simultaneous localization and mapping (SLAM) techniques have flourished with the advent of 3D Light Detection and Ranging (LiDAR) sensors, accurate 3D maps are readily available. Many researchers turn their attention to localization in a previously acquired 3D map. In this paper, we propose a novel and lightweight camera-only visual positioning algorithm that involves localization within prior 3D LiDAR maps. We aim to achieve the consumer level global positioning system (GPS) accuracy using vision within the urban environment, where GPS signal is unreliable. Via exploiting a stereo camera, depth from the stereo disparity map is matched with 3D LiDAR maps. A full six degree of freedom (DOF) camera pose is estimated via minimizing depth residual. Powered by visual tracking that provides a good initial guess for the localization, the proposed depth residual is successfully applied for camera pose estimation. Our method runs online, as the average localization error is comparable to ones resulting from state-of-the-art approaches. We validate the proposed method as a stand-alone localizer using KITTI dataset and as a module in the SLAM framework using our own dataset.


Title: Robocentric Visual-Inertial Odometry
Key Words: distance measurement  inertial navigation  Kalman filters  mobile robots  Monte Carlo methods  motion estimation  nonlinear filters  position measurement  SLAM (robots)  robocentric EKF-based VINS  standard world-centric frameworks  R-VIO  real-world experiments  state-of-the-art VINS  robocentric visual-inertial odometry  visual-inertial navigation systems  consistent localization  challenging environments  monocular vision  moving local frame  standard world-centric VINS  global gravity vector  multistate constraint Kalman filter framework  visual-inertial odometry algorithm  global pose  high-accuracy relative motion  robocentric formulation  Robot sensing systems  Three-dimensional displays  Computational efficiency  Navigation  Standards  Gravity  Quaternions 
Abstract: In this paper, we propose a novel robocentric formulation of visual-inertial navigation systems (VINS)within a multi-state constraint Kalman filter (MSCKF)framework and develop an efficient, lightweight, robocentric visual-inertial odometry (R-VIO)algorithm for consistent localization in challenging environments using only monocular vision. The key idea of the proposed approach is to deliberately reformulate the 3D VINS with respect to a moving local frame (i.e., robocentric), rather than a fixed global frame of reference as in the standard world-centric VINS, and instead utilize high-accuracy relative motion estimates for global pose update. As an immediate advantage of using this robocentric formulation, the proposed R-VIO can start from an arbitrary pose, without the need to align its orientation with the global gravity vector. More importantly, we analytically show that the proposed robocentric EKF-based VINS does not undergo the observability mismatch issue as in the standard world-centric frameworks which was identified as the main cause of inconsistency of estimation. The proposed R-VIO is extensively tested through both Monte Carlo simulations and real-world experiments using different sensor platforms in different environments and shown to achieve competitive performance with the state-of-the-art VINS algorithms in terms of consistency, accuracy and efficiency.


Title: Appearance-Based Along-Route Localization for Planetary Missions
Key Words: image matching  image registration  image sequences  mobile robots  object recognition  planetary rovers  robot vision  SLAM (robots)  image preprocessing steps  recognition framework SeqSLAM  appearance-based along-route localization algorithm  planetary missions  direct sequence-based approach  Moon-analogue mission  planetary rover  image similarity metrics wrt  translational viewpoint differences  rotational viewpoint differences  route traversal conditions  matching locations  flexible mechanism  frame matches  homing mechanism  autonomous navigation  real-time localization  individual frames  image sequences  robust place recognition  Navigation  Lighting  Cameras  Moon  Simultaneous localization and mapping  Visualization  mobile robotics  field robotics  place-recognition  autonomous route navigation 
Abstract: We propose an appearance-based along-route localization algorithm that relies on robust place recognition by matching image sequences instead of individual frames. Our approach extends state of the art place recognition framework SeqSLAM in several aspects to realize real-time localization along routes for autonomous navigation. First, our method is online in that we only rely on the recently observed image frames. Second, we provide a homing mechanism based on rotations computed from frame matches. And third, we use a more flexible mechanism to search for matching locations, not restricting the search to straight lines in the cost matrix as in SeqSLAM, but allowing for a wide variety of route traversal conditions such as varying velocities or rotational and translational viewpoint differences. We investigate different image preprocessing steps as well as image similarity metrics wrt. their influence on illumination and viewpoint invariance for a more robust place recognition. On a new challenging dataset, recorded in real world experiments with a planetary rover, in the course of a Moon-analogue mission on Sicily's Mount Etna, we show the feasibility of our direct, sequence-based approach to along-route localization.


Title: A Monocular Indoor Localiser Based on an Extended Kalman Filter and Edge Images from a Convolutional Neural Network
Key Words: cameras  convolutional neural nets  edge detection  image fusion  Kalman filters  mobile robots  nonlinear filters  pose estimation  robot vision  SLAM (robots)  camera location estimation  extended Kalman filter  6 DOF pose estimation  visual simultaneous localisation-and-mapping algorithms  prebuilt map  ground plane edge image extraction  motion model  unsigned distance function  indoor environment  monocular images  monocular indoor localiser  EKF framework  CNN  convolutional neural network  Image edge detection  Cameras  Robot vision systems  Feature extraction  Convolution  Image segmentation 
Abstract: The main contribution of this paper is an extended Kalman filter (EKF)based algorithm for estimating the 6 DOF pose of a camera using monocular images of an indoor environment. In contrast to popular visual simultaneous localisation and mapping algorithms, the technique proposed relies on a pre-built map represented as an unsigned distance function of the ground plane edges. Images from the camera are processed using a Convolutional Neural Network (CNN)to extract a ground plane edge image. Pixels that belong to these edges are used in the observation equation of the EKF to estimate the camera location. Use of the CNN makes it possible to extract ground plane edges under significant changes to scene illumination. The EKF framework lends itself to use of a suitable motion model, fusing information from any other sensors such as wheel encoders or inertial measurement units, if available, and rejecting spurious observations. A series of experiments are presented to demonstrate the effectiveness of the proposed technique.


Title: Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality
Key Words: augmented reality  calibration  cameras  human-robot interaction  image registration  image sensors  inspection  mobile robots  robot vision  SLAM (robots)  stereo image processing  mixed-reality headsets  headset orientation  structure inspection  marker-free self-localization  onboard depth sensor  simple point cloud registration  camera image  inspection information  joint point cloud and image-based localization  JPIL  human-robot interaction  time 20.0 min  Three-dimensional displays  Headphones  Inspection  Cameras  Virtual reality  Solid modeling  Robot sensing systems 
Abstract: This paper introduces a method of structure inspection using mixed-reality headsets to reduce the human effort in reporting accurate inspection information such as fault locations in 3D coordinates. Prior to every inspection, the headset needs to be localized. While external pose estimation and fiducial marker based localization would require setup, maintenance, and manual calibration; marker-free self-localization can be achieved using the onboard depth sensor and camera. However, due to limited depth sensor range of portable mixed-reality headsets like Microsoft HoloLens, localization based on simple point cloud registration (sPCR) would require extensive mapping of the environment. Also, localization based on camera image would face same issues as stereo ambiguities and hence depends on viewpoint. We thus introduce a novel approach to Joint Point Cloud and Image-based Localization (JPIL) for mixed-reality headsets that uses visual cues and headset orientation to register small, partially overlapped point clouds and save significant manual labor and time in environment mapping. Our empirical results compared to sPCR show average 10 fold reduction of required overlap surface area that could potentially save on average 20 minutes per inspection. JPIL is not only restricted to inspection tasks but also can be essential in enabling intuitive human-robot interaction for spatial mapping and scene understanding in conjunction with other agents like autonomous robotic systems that are increasingly being deployed in outdoor environments for applications like structural inspection.


Title: Probabilistic Dense Reconstruction from a Moving Camera
Key Words: cameras  image colour analysis  image reconstruction  image sequences  probability  SLAM (robots)  stereo image processing  TUM RGB-D SLAM  ICL-NUIM dataset  spatial correlations  visual scale changes  insufficient parallaxes  motion stereo  spatial stereo  single monocular camera  online dense reconstruction  probabilistic approach  moving camera  probabilistic dense reconstruction  outdoor experiments  dense 3D models  inlier probability expectations  depth estimates  probabilistic scheme  monocular depth estimation  temporal correlations  Image reconstruction  Cameras  Estimation  Visualization  Robot vision systems  Probabilistic logic  Simultaneous localization and mapping 
Abstract: This paper presents a probabilistic approach for online dense reconstruction using a single monocular camera moving through the environment. Compared to spatial stereo, depth estimation from motion stereo is challenging due to insufficient parallaxes, visual scale changes, pose errors, etc. We utilize both the spatial and temporal correlations of consecutive depth estimates to increase the robustness and accuracy of monocular depth estimation. An online, recursive, probabilistic scheme to compute depth estimates, with corresponding covariances and inlier probability expectations, is proposed in this work. We integrate the obtained depth hypotheses into dense 3D models in an uncertainty-aware way. We show the effectiveness and efficiency of our proposed approach by comparing it with state-of-the-art methods in the TUM RGB-D SLAM & ICL-NUIM dataset. Online indoor and outdoor experiments are also presented for performance demonstration.


Title: Summarizing Large Scale 3D Mesh
Key Words: mesh generation  mobile robots  robot vision  SLAM (robots)  stereo image processing  vision-based summarizing process  HD 3D maps  large-scale 3D map  semantic information  geometric information  photometric information  autonomous navigation  semantic mapping  3D sensor devices  Three-dimensional displays  Navigation  Entropy  Semantics  Visualization  Optimization  Robots 
Abstract: Recent progress in 3D sensor devices and in semantic mapping allows to build very rich HD 3D maps very useful for autonomous navigation and localization. However, these maps are particularly huge and require important memory capabilities as well computational resources. In this paper, we propose a new method for summarizing a 3D map (Mesh)as a set of compact spheres in order to facilitate its use by systems with limited resources (smartphones, robots, UAVs,...). This vision-based summarizing process is applied in a fully automatic way using jointly photometric, geometric and semantic information of the studied environment. The main contribution of this research is to provide a very compact map that maximizes the significance of its content while maintaining the full visibility of the environment. Experimental results in summarizing large-scale 3D map demonstrate the feasibility of our approach and evaluate the performance of the algorithm.


Title: Submap-Based Pose-Graph Visual SLAM: A Robust Visual Exploration and Localization System* The work in this paper is supported by the National Natural Science Foundation of China (61603103, 61673125), the Natural Science Foundation of Guangdong of China (2016A030310293), and the Major Scientific and Technological Special Project of Guangdong of China (2016B090910003).
Key Words: graph theory  mean square error methods  pose estimation  robot vision  SLAM (robots)  VSLAM algorithms  robust visual exploration  visual simultaneous localization and mapping  submap-based pose-graph visual SLAM  robust exploration  visual front-end  submap-based VSLAM system  Image edge detection  Optimization  Robustness  Visualization  Merging  Robots  Tracking  Monocular VSLAM  Submap-based Backend  Robustness 
Abstract: For VSLAM (Visual Simultaneous Localization and Mapping), localization is a challenging task, especially for some challenging situations: textureless frames, motion blur, etc. To build a robust exploration and localization system in a given space, a submap-based VSLAM system is proposed in this paper. Our system uses a submap back-end and a visual front-end. The main advantage of our system is its robustness with respect to tracking failure, a common problem in current VSLAM algorithms. The robustness of our system is compared with the state-of-the-art in terms of average tracking percentage. The precision of our system is also evaluated in terms of ATE (absolute trajectory error) RMSE (root mean square error) comparing the state-of-the-art. The ability of our system in solving the “kidnapped” problem is demonstrated. Our system can improve the robustness of visual localization in challenging situations.


Title: Learning Monocular Visual Odometry with Dense 3D Mapping from Dense 3D Flow
Key Words: distance measurement  Gaussian processes  image reconstruction  learning (artificial intelligence)  mobile robots  motion estimation  neural nets  pose estimation  robot vision  SLAM (robots)  stereo image processing  learning monocular visual odometry  monocular SLAM  simultaneous localization  neural network  dual-stream L-VO network  6DOF relative pose  bivariate Gaussian modeling  KITTI odometry  visual SLAM system  dense 2D flow  fully deep learning approach  dense 3D flow  dense 3D mapping  Three-dimensional displays  Simultaneous localization and mapping  Visual odometry  Two dimensional displays  Deep learning  Cameras  Training 
Abstract: This paper introduces a fully deep learning approach to monocular SLAM, which can perform simultaneous localization using a neural network for learning visual odometry (L-VO) and dense 3D mapping. Dense 2D flow and a depth image are generated from monocular images by sub-networks, which are then used by a 3D flow associated layer in the L-VO network to generate dense 3D flow. Given this 3D flow, the dual-stream L-VO network can then predict the 6DOF relative pose and furthermore reconstruct the vehicle trajectory. In order to learn the correlation between motion directions, the Bivariate Gaussian modeling is employed in the loss function. The L-VO network achieves an overall performance of 2.68 % for average translational error and 0.0143°/m for average rotational error on the KITTI odometry benchmark. Moreover, the learned depth is leveraged to generate a dense 3D map. As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.


Title: Unit Quaternion-Based Parameterization for Point Features in Visual Navigation
Key Words: covariance matrices  geometry  image representation  recursive estimation  SLAM (robots)  point features  visual navigation  Cartesian 3D representation  homogeneous points  error state  unit-quaternion error covariance  initial feature observations  initial error-states  unit quaternion-based representation  unit quaternion-based parameterization  initial infinite depth uncertainty  Quaternions  Cameras  Simultaneous localization and mapping  Visualization  Navigation  Convergence  Three-dimensional displays 
Abstract: In this paper, we propose to use unit quaternions to represent point features in visual navigation. Contrary to the Cartesian 3D representation, the unit quaternion can well represent features at both large and small distances from the camera without suffering from convergence problems. Contrary to inverse-depth, homogeneous points, or anchored homogeneous points, the unit quaternion has error state of minimum dimension of three. In contrast to prior representations, the proposed method does not need to approximate an initial infinite depth uncertainty. In fact, the unit-quaternion error covariance can be initialized from the initial feature observations without prior information, and the initial error-states are not only bounded, but the bound is identical for all scene geometries. To the best of our knowledge, this is the first time bearing-only recursive estimation (in covariance form) of point features has been possible without using measurements to initialize error covariance. The proposed unit quaternion-based representation is validated on numerical examples.


Title: Improving Repeatability of Experiments by Automatic Evaluation of SLAM Algorithms
Key Words: robot vision  SLAM (robots)  SLAM algorithms  robotics  repeatability  Simultaneous Localization And Mapping  Simultaneous localization and mapping  Buildings  Measurement  Data models  Lasers 
Abstract: The development of good experimental methodologies for robotics takes often inspiration from general principles of experimental practice. Repeatability prescribes that experiments should involve several trials in order to guarantee that results are not achieved by chance, but are systematic, and statistically significant trends can be identified. In this paper, we propose an approach to improve the repeatability of experiments performed in robotics. In particular, we focus on the domain of SLAM (Simultaneous Localization And Mapping) and we introduce a system that exploits simulations to generate a large number of test data on which SLAM algorithms are automatically evaluated in order to obtain consistent results, according to the principle of repeatability.


Title: OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions
Key Words: image recognition  mobile robots  navigation  object recognition  public domain software  robot vision  SLAM (robots)  open source toolbox  changing conditions  traversed route  inclement conditions  navigating robots  robotic systems  environmental conditions  fully open-source toolbox  open access  source code  visual place recognition problem  open source platform  OpenSeqSLAM2.0  Visualization  Robots  Tools  Open source software  Search methods  Trajectory  Heuristic algorithms 
Abstract: Visually recognising a traversed route - regardless of whether seen during the day or night, in clear or inclement conditions, or in summer or winter - is an important capability for navigating robots. Since SeqSLAM was introduced in 2012, a large body of work has followed exploring how robotic systems can use the algorithm to meet the challenges posed by navigation in changing environmental conditions. The following paper describes OpenSeqSLAM2.0, a fully open-source toolbox for visual place recognition under changing conditions. Beyond the benefits of open access to the source code, OpenSeqSLAM2.0 provides a number of tools to facilitate exploration of the visual place recognition problem and interactive parameter tuning. Using the new open source platform, it is shown for the first time how comprehensive parameter characterisations provide new insights into many of the system components previously presented in ad hoc ways and provide users with a guide to what system component options should be used under what circumstances and why.


Title: HERO: Accelerating Autonomous Robotic Tasks with FPGA
Key Words: convolutional neural nets  field programmable gate arrays  mobile robots  path planning  SLAM (robots)  motion planning tasks  HERO platform  CNN inference  autonomous robotic tasks  Heterogeneous Extensible Robot Open platform  OpenCL programming  SLAM  convolutional neural network inference  FPGA acceleration  heterogeneous computing  simultaneous localization and mapping  VGG-16  ResNet-50  Field programmable gate arrays  Kernel  Acceleration  Simultaneous localization and mapping  Task analysis  Planning 
Abstract: The Heterogeneous Extensible Robot Open (HERO) platform is designed for autonomous robotic research. While bringing in the flexible computational capacities by CPU and FPGA, it addresses the challenges of heterogeneous computing by embracing OpenCL programming. We propose heterogeneous computing approaches for three fundamental robotic tasks: simultaneous localization and mapping (SLAM), motion planning and convolutional neural network (CNN) inference. With FPGA acceleration, the SLAM and motion planning tasks are performed 2-4 times faster on the HERO platform against fine-tuned software implementation. For CNN inference, it can process 20-30 images per second with the network of VGG-16 or ResNet-50. We expect the open platform and the developing experiences shared in this paper can facilitate future robotic research, especially for those compute intensive tasks of perception, movement and manipulation.


Title: Methods for Autonomous Wristband Placement with a Search-and-Rescue Aerial Manipulator
Key Words: aerospace control  autonomous aerial vehicles  convolutional neural nets  grippers  image colour analysis  learning (artificial intelligence)  manipulators  path planning  rescue robots  robot vision  SLAM (robots)  autonomous wristband placement  robotic system  automatic wristband placement  remote sensor readings  continuous health monitoring  unmanned aerial manipulator  automatic wrist detection  RGB-D camera  convolutional neural network  Faster R-CNN  passive detachable gripper  VGG-16 neural network  target localization  trajectory planning  machine learning  parallel delta manipulator  search-and-rescue aerial manipulator  search and rescue operations  unmanned aerial vehicles  Manipulators  Wrist  Cameras  Grippers  Robot kinematics  Robot sensing systems 
Abstract: A new robotic system for Search And Rescue (SAR) operations based on the automatic wristband placement on the victims' arm, which may provide identification, beaconing and remote sensor readings for continuous health monitoring. This paper focuses on the development of the automatic target localization and the device placement using an unmanned aerial manipulator. The automatic wrist detection and localization system uses an RGB-D camera and a convolutional neural network based on the region faster method (Faster R-CNN). A lightweight parallel delta manipulator with a large workspace has been built, and a new design of a wristband in the form of a passive detachable gripper, is presented, which, under contact, automatically attaches to the human, while disengages from the manipulator. A new trajectory planning method has been used to minimize the torques caused by the external forces during contact, which cause attitude perturbations. Experiments have been done to evaluate the machine learning method for detection and location, and for the assessment of the performance of the trajectory planning method. The results show how the VGG-16 neural network provides a detection accuracy of 67.99%. Moreover, simulation experiments have been done to show that the new trajectories minimize the perturbations to the aerial platform.


Title: Intelligent Robotic IoT System (IRIS)Testbed
Key Words: Global Positioning System  intelligent robots  IP networks  middleware  mobile radio  mobile robots  multi-robot systems  personal area networks  protocols  robot vision  SLAM (robots)  wireless LAN  IPv6 network stack  individual robots  system implementation details  Intelligent robotic IoT system  modular source testbed  portable source testbed  open-source testbed  robotic wireless network research  IRIS  Time Difference of Arrival localization system  Time Difference of Arrival localization system  static global positioning system  multirobot testbeds  multirobot testbeds  Programmable Wireless Communication Stack  scalable source testbed  lightweight publish-subscribe overlay protocol  ROMANO  modular architecture  Iris recognition  Iris  Robot kinematics  Protocols  Transceivers  Ultrasonic imaging 
Abstract: We present the Intelligent Robotic IoT System (IRIS), a modular, portable, scalable, and open-source testbed for robotic wireless network research. There are two key features that separate IRIS from most of the state-of-the-art multi-robot testbeds. (1)Portability: IRIS does not require a costly static global positioning system such as a VICON system nor time-intensive vision-based SLAM for its operation. Designed with an inexpensive Time Difference of Arrival (TDoA)localization system with centimeter level accuracy, the IRIS testbed can be deployed in an arbitrary uncontrolled environment in a matter of minutes. (2)Programmable Wireless Communication Stack: IRIS comes with a modular programmable low-power IEEE 802.15.4 radio and IPv6 network stack on each node. For the ease of administrative control and communication, we also developed a lightweight publish-subscribe overlay protocol called ROMANO that is used for bootstrapping the robots (also referred to as the IRISbots), collecting statistics, and direct control of individual robots, if needed. We detail the modular architecture of the IRIS testbed design along with the system implementation details and localization performance statistics.


Title: π-SoC: Heterogeneous SoC Architecture for Visual Inertial SLAM Applications
Key Words: energy consumption  mobile computing  mobile robots  optimisation  SLAM (robots)  system-on-chip  visual inertial SLAM applications  autonomous vehicles  robotics  core technologies  battery-powered mobile devices  energy budget  energy consumption  energy efficiency  visual inertial SLAM workloads  60 FPS performance  heterogeneous SoC architecture  simultaneous localization and mapping  hardware accelerator  IO interface  memory hierarchy  Simultaneous localization and mapping  Feature extraction  Three-dimensional displays  Instruction sets  Power demand  Graphics processing units  Computer architecture 
Abstract: In recent years, we have observed a clear trend in the rapid rise of autonomous vehicles and robotics. One of the core technologies enabling these applications, Simultaneous Localization And Mapping (SLAM), imposes two main challenges: first, these workloads are computationally intensive and they often have real-time requirements; second, these workloads run on battery-powered mobile devices with limited energy budget. Hence, performance should be improved while simultaneously reducing energy consumption, two rather contradicting goals by conventional wisdom. Previous attempts to optimize SLAM performance and energy efficiency usually involve optimizing one function and fail to approach the problem systematically. In this paper, we first study the characteristics of visual inertial SLAM workloads on existing heterogeneous SoCs. Then based on the initial findings, we propose π-SoC, a heterogeneous SoC design that systematically optimize the IO interface, the memory hierarchy, as well as the the hardware accelerator. We implemented this system on a Xilinx Zynq UltraScale MPSoC and was able to deliver over 60 FPS performance with average power less than 5 W.


Title: The Earth Ain't Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera
Key Words: cameras  image reconstruction  pose estimation  road vehicles  SLAM (robots)  stereo image processing  traffic engineering computing  local bundle-adjustment like procedure  3D pose  semantic cues  moving ego vehicle  shape estimation  plain roads  monocular localization demonstrations  autonomous driving systems  traffic participants  moving camera  monocular reconstruction  arbitrarily-shaped roads  monocular object localization  road plane configurations  local ground plane  local planar patches  monocular camera  Shape  Roads  Three-dimensional displays  Cameras  Image reconstruction  Automobiles  Surface reconstruction 
Abstract: Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but monocular localization demonstrations have been confined to plain roads. We demonstrate - to the best of our knowledge - the first results for monocular object localization and shape estimation on surfaces that are non-coplanar with the moving ego vehicle mounted with a monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the 3D pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands. We also demonstrate that our approach transfers from synthetic to real data, without any hyperparameter-/fine-tuning. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.


Title: Map-based Deep Imitation Learning for Obstacle Avoidance
Key Words: collision avoidance  decision making  inference mechanisms  learning (artificial intelligence)  mobile robots  optimisation  robot vision  SLAM (robots)  mobile robots  deep imitation learning algorithm  egocentric local occupancy maps  fast feed-forward inferences  policy robustness  optimal decision making  obstacle avoidance policy  map-based deep imitation learning  value iteration networks  near-optimal continuous action commands  planning-based scenarios  Collision avoidance  Robot kinematics  Mobile robots  Training  Neural networks  Trajectory 
Abstract: Making an optimal decision to avoid obstacles while heading to the goal is one of the fundamental challenges for mobile robots equipped with limited computational resources. In this paper, we present a deep imitation learning algorithm that develops a computationally efficient obstacle avoidance policy based on egocentric local occupancy maps. The trained model embedded with a variant of the value iteration networks is able to provide near-optimal continuous action commands through fast feed-forward inferences and generalize well to unseen planning-based scenarios. To improve the policy robustness, we augment the training data set with artificially generated maps, which effectively alleviates the shortage of catastrophic samples in normal demonstrations. Extensive experiments on a Segway robot show the effectiveness of the proposed approach in terms of solution optimality, robustness as well as computation time.


