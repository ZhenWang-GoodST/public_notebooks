total paper: 100
Title: Geometric-based Line Segment Tracking for HDR Stereo Sequences
Key Words: compressed sensing  convex programming  image matching  image segmentation  image sequences  stereo image processing  video signal processing  robust tracking  art techniques  appearance-based methods  High Dynamic Range environments  HDR stereo sequences  geometric-based line segment tracking  stereo streams  appearance-based matching techniques  video sequences  Image segmentation  Lighting  Tracking  Feature extraction  Video sequences  Motion segmentation  Simultaneous localization and mapping 
Abstract: In this work, we propose a purely geometrical approach for the robust matching of line segments for challenging stereo streams with severe illumination changes or High Dynamic Range (HDR) environments. To that purpose, we exploit the univocal nature of the matching problem, i.e. every observation must be corresponded with a single feature or not corresponded at all. We state the problem as a sparse, convex, ℓ1-minimization of the matching vector regularized by the geometric constraints. This formulation allows for the robust tracking of line segments along sequences where traditional appearance-based matching techniques tend to fail due to dynamic changes in illumination conditions. Moreover, the proposed matching algorithm also results in a considerable speed-up of previous state of the art techniques making it suitable for real-time applications such as Visual Odometry (VO). This, of course, comes at expense of a slightly lower number of matches in comparison with appearance-based methods, and also limits its application to continuous video sequences, as it is rather constrained to small pose increments between consecutive frames. We validate the claimed advantages by first evaluating the matching performance in challenging video sequences, and then testing the method in a benchmarked point and line based VO algorithm.


Title: FSG: A statistical approach to line detection via fast segments grouping
Key Words: feature extraction  image segmentation  robot vision  fast segments grouping  line segment detection algorithms  segment grouping methods  vanishing points detection  statistical approach  high level robot localization task  plausible line candidates  robust line detection algorithm  FSG  low textured scenes  visual robotic tasks  line extraction  Image segmentation  Probabilistic logic  Estimation  Simultaneous localization and mapping  Task analysis  Detection algorithms 
Abstract: Line extraction is a preliminary step in various visual robotic tasks performed in low textured scenes such as city and indoor settings. Several efficient line segment detection algorithms such as LSD and EDLines have recently emerged. However, the state of the art segment grouping methods are not robust enough or not amenable for detecting lines in real-time. In this paper we present FSG, a fast and robust line detection algorithm. It is based on two independent components. A proposer that greedily cluster segments suggesting plausible line candidates and a probabilistic model that decides if a group of segments is an actual line. In the experiments we show that our procedure is more robust and faster than the best methods in the literature and achieves state-of-the art performance in a high level robot localization task such as vanishing points detection.


Title: Optimized Contrast Enhancements to Improve Robustness of Visual Tracking in a SLAM Relocalisation Context
Key Words: cameras  feature extraction  image colour analysis  image enhancement  image representation  mobile robots  robot vision  SLAM (robots)  video signal processing  optimized contrast enhancements  visual tracking  SLAM relocalisation context  indirect SLAM techniques  robotics community  feature points  multilayered image representation  contrast enhanced version  tracking process  detection  matching  dynamic contrast enhancements  dynamic light changing conditions  ORB-SLAM  light changed condition  reference video  Mutual information  Lighting  Robustness  Simultaneous localization and mapping  Cameras  Entropy  Visualization 
Abstract: Robustness of indirect SLAM techniques to light changing conditions remains a central issue in the robotics community. With the change in the illumination of a scene, feature points are either not extracted properly due to low contrasts, or not matched due to large differences in descriptors. In this paper, we propose a multi-layered image representation (MLI) in which each layer holds a contrast enhanced version of the current image in the tracking process in order to improve detection and matching. We show how Mutual Information can be used to compute dynamic contrast enhancements on each layer. We demonstrate how this approach dramatically improves the robustness in dynamic light changing conditions on both synthetic and real environments compared to default ORB-SLAM. This work focalises on the specific case of SLAM relocalisation in which a first pass on a reference video constructs a map, and a second pass with a light changed condition relocalizes the camera in the map.


Title: Key-frame Selection for Multi-robot Simultaneous Localization and Tracking in Robot Soccer Field
Key Words: entropy  mobile robots  multi-robot systems  robot vision  SLAM (robots)  traditional key-frame selection algorithms  temporal relationship  spatial relationship  pre-defined field  information entropy  selection ratio  key-frames  localization results  robot soccer field  optical images  extensive computation resources  key-frame selection algorithm  multiple robots simultaneous localization  multirobot soccer games  Entropy  Robot sensing systems  Object detection  Sports  Cameras  Legged locomotion 
Abstract: Optical images provide rich features but require extensive computation resources to process for SLAM. When there are limited computation resources on the robots, it becomes a heavy burden to process the images in real-time. This paper presents the design and implementation of key-frame selection algorithm for multiple robots simultaneous localization and tracking on the multi-robot soccer games which have pre-defined field and objects. Compared to traditional key-frame selection algorithms, this work makes use of the temporal and spatial relationship among objects on the pre-defined field to compute the information entropy. The selection ratio can be adjusted by two parameters: entropy threshold and the maximum moving distance. The experimental results show that the developed method can effectively detect the change of scene using selected key-frames. And comparing with the localization results using all the images, using less than 20% of all images after walking 11,203mm it only increase up to 0.87% trajectory errors.


Title: LIPS: LiDAR-Inertial 3D Plane SLAM
Key Words: graph theory  image representation  mobile robots  optical radar  optimisation  robot vision  SLAM (robots)  inertial preintegratation measurement  LiDAR-inertial 3D plane SLAM  simultaneous localization and mapping  singularity free plane factor  closest point plane representation  Simultaneous localization and mapping  Three-dimensional displays  Laser radar  Optimization  Lips 
Abstract: This paper presents the formalization of the closest point plane representation and an analysis of its incorporation in 3D indoor simultaneous localization and mapping (SLAM). We present a singularity free plane factor leveraging the closest point plane representation, and demonstrate its fusion with inertial preintegratation measurements in a graph-based optimization framework. The resulting LiDAR-inertial 3D plane SLAM (LIPS) system is validated both on a custom made LiDAR simulator and on a real-world experiment.


Title: Scan Similarity-based Pose Graph Construction method for Graph SLAM
Key Words: graph theory  mobile robots  pose estimation  robot vision  SLAM (robots)  scan similarity-based pose graph construction method  constructed graph  loop closure detection method  real world dataset  benchmark dataset  odometry estimation process  error accumulation phenomenon  pose graph SLAM  scan similarity computation method  graph accuracy  high quality graph  Simultaneous localization and mapping  Lasers  Estimation  Heuristic algorithms  Optimization 
Abstract: Scan similarity-based pose graph construction method for graph SLAM is proposed. To perform delicate pose graph SLAM, front-end that constructs a graph as well as back-end that optimizes the constructed graph is an important task. Generally, there is an error accumulation phenomenon during the odometry estimation process. This paper focuses on the method of creating a high quality graph by suggesting ways to improve the graph accuracy since the accumulated errors in the graph might degrade the performance of the entire graph SLAM. We deal with one of our previous works, dynamic keyframe selection technique, based on scan similarity computation method more precisely and suggest a loop closure detection method by exploiting previously proposed 2-D laser scan descriptor. To verify objective performance of the proposed method, the experimental results of the odometry estimation are shown by using the benchmark dataset and the real world dataset. Additionally, results of the pose graph SLAM are shown for the real world dataset which include the loop clorues.


Title: Predicting Objective Function Change in Pose-Graph Optimization
Key Words: graph theory  optimisation  SLAM (robots)  outlier detection  robust online incremental SLAM applications  graph pruning  information-theoretic metrics  pose-graph optimization scheme  Linear programming  Optimization  Simultaneous localization and mapping  Measurement errors  Noise measurement  Reliability 
Abstract: Robust online incremental SLAM applications require metrics to evaluate the impact of current measurements. Despite its prevalence in graph pruning, information-theoretic metrics solely are insufficient to detect outliers. The optimal value of the objective function is a better choice to detect outliers but cannot be computed unless the problem is solved. In this paper, we show how the objective function change can be predicted in an incremental pose-graph optimization scheme, without actually solving the problem. The predicted objective function change can be used to guide online decisions or detect outliers. Experiments validate the accuracy of the predicted objective function, and an application to outlier detection is also provided, showing its advantages over M-estimators.


Title: Efficient Long-term Mapping in Dynamic Environments
Key Words: graph theory  mobile robots  robot vision  SLAM (robots)  mapping problem  longterm SLAM datasets  graph coherency  intra-session loop closure detections  out-dated nodes  graph complexity  nonstatic entities  merging procedure  efficient ICP-based alignment  up-to-date state  2D point cloud data  local maps  graph SLAM paradigm  multiple mapping sessions  single mapping sessions  SLAM system  autonomous robots  dynamic environments  long-term robot operation  Simultaneous localization and mapping  Cloud computing  Three-dimensional displays  Two dimensional displays  Merging  Optimization 
Abstract: As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.


Title: Localization of Classified Objects in SLAM using Nonparametric Statistics and Clustering
Key Words: feature extraction  learning (artificial intelligence)  mobile robots  nonparametric statistics  object detection  pattern clustering  robot vision  SLAM (robots)  statistical analysis  nonparametric statistical approach  data association  mapping process  object detection  machine learning  semantic information  nonparametric statistics  classified objects  locating objects  SLAM  unsupervised clustering method  detected objects  Simultaneous localization and mapping  Semantics  Object detection  Cameras  Three-dimensional displays 
Abstract: Traditional Simultaneous Localization and Mapping (SLAM) approaches build maps based on points, lines or planes. These maps visually resemble the environment but without any semantic or information about the objects in the environment. Recent advancements in machine learning have made object detection highly accurate and reliable with large set of objects. Object detection can effectively help SLAM to incorporate semantics in the mapping process. One of the main obstacles is data association between detected objects over time. We demonstrate a nonparametric statistical approach to solve the data association between detected objects over consecutive frames. Then we use an unsupervised clustering method to identify the existence of objects in the map. The complete process can be run in parallel with SLAM. The performance of our algorithm is demonstrated on several public datasets, which shows promising results in locating objects in SLAM.


Title: Fast Kinodynamic Bipedal Locomotion Planning with Moving Obstacles
Key Words: collision avoidance  humanoid robots  legged locomotion  motion control  pendulums  robot dynamics  robot kinematics  wheels  moving obstacles  bipedal robot  complex environments  footstep planning algorithms  footstep locations  biped dynamics  temporal duration  dynamically consistent description  PSP  collision-free route  nonholonomic wheeled robots  kinematic constraints  bipedal motion  body dynamic walking behavior  3D physics-based simulation  linear inverted pendulum model dynamics  dynamic constraints  kinodynamic bipedal locomotion planning  sampling-based kino-dynamic planning  LIPM  phase space planner  steering method  Planning  Heuristic algorithms  Robot kinematics  Legged locomotion  Collision avoidance 
Abstract: In this paper, we present a sampling-based kino-dynamic planning framework for a bipedal robot in complex environments. Unlike other footstep planning algorithms which typically plan footstep locations and the biped dynamics in separate steps, we handle both simultaneously. Three primary advantages of this approach are (1) the ability to differentiate alternate routes while selecting footstep locations based on the temporal duration of the route as determined by the Linear Inverted Pendulum Model (LIPM) dynamics, (2) the ability to perform collision checking through time so that collisions with moving obstacles are prevented without avoiding their entire trajectory, and (3) the ability to specify a minimum forward velocity for the biped. To generate a dynamically consistent description of the walking behavior, we exploit the Phase Space Planner (PSP) [1] [2]. To plan a collision-free route toward the goal, we adapt planning strategies from non-holonomic wheeled robots to gather a sequence of inputs for the PSP. This allows us to efficiently approximate dynamic and kinematic constraints on bipedal motion, to apply a sampling-based planning algorithm such as RRT or RRT*, and to use the Dubin's path [3] as the steering method to connect two points in the configuration space. The results of the algorithm are sent to a Whole Body Controller [1] to generate full body dynamic walking behavior. Our planning algorithm is tested in a 3D physics-based simulation of the humanoid robot Valkyrie.


Title: Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation
Key Words: image segmentation  probability  SLAM (robots)  stereo image processing  SLAM framework  NYUv2 dataset  computational efficiency  frame-wise segmentation result  computationally intensive stages  segmentation label  updating class probabilities  processing components  geometric-based segmentation method  geometric-based incremental segmentation  Semantics  Three-dimensional displays  Image segmentation  Simultaneous localization and mapping  Cameras  Real-time systems  Two dimensional displays 
Abstract: We propose an efficient and scalable method for incrementally building a dense, semantically annotated 3D map in real-time. The proposed method assigns class probabilities to each region, not each element (e.g., surfel and voxel), of the 3D map which is built up through a robust SLAM framework and incrementally segmented with a geometric-based segmentation method. Differently from all other approaches, our method has a capability of running at over 30Hz while performing all processing components, including SLAM, segmentation, 2D recognition, and updating class probabilities of each segmentation label at every incoming frame, thanks to the high efficiency that characterizes the computationally intensive stages of our framework. By utilizing a specifically designed CNN to improve the frame-wise segmentation result, we can also achieve high accuracy. We validate our method on the NYUv2 dataset by comparing with the state of the art in terms of accuracy and computational efficiency, and by means of an analysis in terms of time and space complexity.


Title: Semantic Monocular SLAM for Highly Dynamic Environments
Key Words: cameras  feature extraction  image motion analysis  image sequences  mobile robots  object detection  object tracking  pose estimation  probability  robot vision  SLAM (robots)  static environment  semantic monocular SLAM framework  semantic information  explicit probabilistic model  dynamic environments  Virtual KITTI  Synthia datasets  pose estimation  Semantics  Simultaneous localization and mapping  Feature extraction  Dynamics  Cameras  Pose estimation  Probabilistic logic 
Abstract: Recent advances in monocular SLAM have enabled real-time capable systems which run robustly under the assumption of a static environment, but fail in presence of dynamic scene changes and motion, since they lack an explicit dynamic outlier handling. We propose a semantic monocular SLAM framework designed to deal with highly dynamic environments, combining feature-based and direct approaches to achieve robustness under challenging conditions. The proposed approach exploits semantic information extracted from the scene within an explicit probabilistic model, which maximizes the probability for both tracking and mapping to rely on those scene parts that do not present a relative motion with respect to the camera. We show more stable pose estimation in dynamic environments and comparable performance to the state of the art on static sequences on the Virtual KITTI and Synthia datasets.


Title: Regularizing Reinforcement Learning with State Abstraction
Key Words: learning (artificial intelligence)  optimisation  pattern clustering  deep reinforcement learning performance  optimal sub-policies  state space clustering  hierarchical reinforcement learning algorithm  near-optimal policy  state cluster  abstract state  continuous action reinforcement learning  similar optimal action  discrete reinforcement  state abstraction  learned policy  Reinforcement learning  Complexity theory  Convergence  Shape  Clustering algorithms  Task analysis  Partitioning algorithms 
Abstract: State abstraction in a discrete reinforcement learning setting clusters states sharing a similar optimal action to yield an easier to solve decision process. In this paper, we generalize the concept of state abstraction to continuous action reinforcement learning by defining an abstract state as a state cluster over which a near-optimal policy of simple shape exists. We propose a hierarchical reinforcement learning algorithm that is able to simultaneously find the state space clustering and the optimal sub-policies in each cluster. The main advantage of the proposed framework is to provide a straightforward way of regularizing reinforcement learning by controlling the behavioral complexity of the learned policy. We apply our algorithm on several benchmark tasks and a robot tactile manipulation task and show that we can match state-of-the-art deep reinforcement learning performance by combining a small number of linear policies.


Title: A Multi-Position Joint Particle Filtering Method for Vehicle Localization in Urban Area
Key Words: distance measurement  image matching  mobile robots  particle filtering (numerical methods)  path planning  probability  robot vision  flexible multiposition joint particle filtering  position error  anchor point  curving roads  ego-trajectory  probabilistic filtering method  flexible road map  long range navigation  error accumulation  visual odometry  traditional visual localization methods  autonomous vehicles  robust localization  urban area  vehicle localization  dense parallel road branches  Roads  Trajectory  Filtering  Urban areas  Wheels  Simultaneous localization and mapping  Navigation 
Abstract: Robust localization is a prerequisite for autonomous vehicles. Traditional visual localization methods like visual odometry suffer error accumulation on long range navigation. In this paper, a flexible road map based probabilistic filtering method is proposed to tackle this problem. To effectively match the ego-trajectory to various curving roads in map, a new representation based on anchor point (AP) which captures the main curving points on the trajectory is presented. Based on APs of the map and trajectory, a flexible Multi-Position Joint Particle Filtering (MPJPF) framework is proposed to correct the position error. The method features the capability of adaptively estimating a series of APs jointly and only updates the estimation at situations with low uncertainty. It explicitly avoids the drawbacks of obliging to determine the current position at large uncertain situations such as dense parallel road branches. The experiments carried out on KITTI benchmark demonstrate our success.


Title: Embedding Temporally Consistent Depth Recovery for Real-time Dense Mapping in Visual-inertial Odometry
Key Words: distance measurement  image reconstruction  interpolation  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  real-time dense mapping  visual-inertial odometry  dense scene information  fast self-localization  VIO-based SLAM systems  VIO depth estimations  subspace-based stabilization scheme  temporal consistency  edge-preserving depth interpolation  simultaneous localization and mapping  learning-based methods  embedding temporally consistent depth recovery  Simultaneous localization and mapping  Real-time systems  Feature extraction  Pipelines  Interpolation  Three-dimensional displays 
Abstract: Dense mapping is always the desire of simultaneous localization and mapping (SLAM), especially for the applications that require fast and dense scene information. Visual-inertial odometry (VIO) is a light-weight and effective solution to fast self-localization. However, VIO-based SLAM systems have difficulty in providing dense mapping results due to the spatial sparsity and temporal instability of the VIO depth estimations. Although there have been great efforts on real-time mapping and depth recovery from sparse measurements, the existing solutions for VIO-based SLAM still fail to preserve sufficient geometry details in their results. In this paper, we propose to embed depth recovery into VIO-based SLAM for real-time dense mapping. In the proposed method, we present a subspace-based stabilization scheme to maintain the temporal consistency and design a hierarchical pipeline for edge-preserving depth interpolation to reduce the computational burden. Numerous experiments demonstrate that our method can achieve an accuracy improvement of up to 49.1 cm compared to state-of-the-art learning-based methods for depth recovery and reconstruct sufficient geometric details in dense mapping when only 0.07% depth samples are available. Since a simple CPU implementation of our method already runs at 10-20 fps, we believe our method is very favorable for practical SLAM systems with critical computational requirements.


Title: Towards Minimal Intervention Control with Competing Constraints
Key Words: control engineering computing  control system synthesis  learning (artificial intelligence)  linear quadratic control  optimal control  robot programming  trajectory control  task execution  trajectory constraints  information-theory  finite horizon linear quadratic regulator  Cartesian space  pure trajectory generation  imitation learning algorithms  simulated robot  robot null space  optimal control  minimal intervention control strategy  Aerospace electronics  Trajectory  Null space  Task analysis  Probabilistic logic  End effectors 
Abstract: As many imitation learning algorithms focus on pure trajectory generation in either Cartesian space or joint space, the problem of considering competing trajectory constraints from both spaces still presents several challenges. In particular, when perturbations are applied to the robot, the underlying controller should take into account the importance of each space for the task execution, and compute the control effort accordingly. However, no such controller formulation exists. In this paper, we provide a minimal intervention control strategy that simultaneously addresses the problems of optimal control and competing constraints between Cartesian and joint spaces. In light of the inconsistency between Cartesian and joint constraints, we exploit the robot null space from an information-theory perspective so as to reduce the corresponding conflict. An optimal solution to the aforementioned controller is derived and furthermore a connection to the classical finite horizon linear quadratic regulator (LQR) is provided. Finally, a writing task in a simulated robot verifies the effectiveness of our approach.


Title: Semantic Mapping with Simultaneous Object Detection and Localization
Key Words: image sensors  mobile robots  object detection  particle filtering (numerical methods)  pose estimation  semantic mapping problem  CT-Map method  six degree-of-freedom pose  pose estimation  RGB-D sensor  Michigan progress fetch robot  particle filtering algorithm  CRF  conditional random field  contextual temporal mapping  object localization  object detection  Semantics  Object detection  Context modeling  Three-dimensional displays  Pose estimation  Simultaneous localization and mapping 
Abstract: We present a filtering-based method for semantic mapping to simultaneously detect objects and localize their 6 degree-of-freedom pose. For our method, called Contextual Temporal Mapping (or CT-Map), we represent the semantic map as a belief over object classes and poses across an observed scene. Inference for the semantic mapping problem is then modeled in the form of a Conditional Random Field (CRF). CT-Map is a CRF that considers two forms of relationship potentials to account for contextual relations between objects and temporal consistency of object poses, as well as a measurement potential on observations. A particle filtering algorithm is then proposed to perform inference in the CT-Map model. We demonstrate the efficacy of the CT-Map method with a Michigan Progress Fetch robot equipped with a RGB-D sensor. Our results demonstrate that the particle filtering based inference of CT-Map provides improved object detection and pose estimation with respect to baseline methods that treat observations as independent samples of a scene.


Title: C-blox: A Scalable and Consistent TSDF-based Dense Mapping Approach
Key Words: autonomous aerial vehicles  image reconstruction  image sensors  robot vision  SLAM (robots)  truncated signed distance field  TSDF subvolumes  lightweight micro aerial vehicle  scalable maps  map growth  bundle adjustment  feature-based camera tracking  dense 3D mapping  map consistency  delayed loop closure  accumulated camera tracking error  precise dense 3D maps  higher level decision making  robotic platforms  consistent dense map  Cameras  Simultaneous localization and mapping  Image reconstruction  Three-dimensional displays  Robot vision systems 
Abstract: In many applications, maintaining a consistent dense map of the environment is key to enabling robotic platforms to perform higher level decision making. Several works have addressed the challenge of creating precise dense 3D maps from visual sensors providing depth information. However, during operation over longer missions, reconstructions can easily become inconsistent due to accumulated camera tracking error and delayed loop closure. Without explicitly addressing the problem of map consistency, recovery from such distortions tends to be difficult. We present a novel system for dense 3D mapping which addresses the challenge of building consistent maps while dealing with scalability. Central to our approach is the representation of the environment as a collection of overlapping Truncated Signed Distance Field (TSDF) subvolumes. These subvolumes are localized through feature-based camera tracking and bundle adjustment. Our main contribution is a pipeline for identifying stable regions in the map, and to fuse the contributing subvolumes. This approach allows us to reduce map growth while still maintaining consistency. We demonstrate the proposed system on a publicly available dataset and simulation engine, and demonstrate the efficacy of the proposed approach for building consistent and scalable maps. Finally we demonstrate our approach running in real-time onboard a lightweight Micro Aerial Vehicle (MAV).


Title: Information Sparsification in Visual-Inertial Odometry
Key Words: computational complexity  distance measurement  graph theory  mobile robots  SLAM (robots)  information sparsification  tightly couple visual measurements  inertial measurements  fixed-lag visual-inertial odometry framework  bound computational complexity  fixed-lag smoothers  densely connected linear  information-theoretic perspective  dense marginalization step  information content  nonlinear factor graph  information loss  information sparsity  VIO methods  EuRoC visual-inertial dataset  structural similarity  nonlinearity  computational complexity  Optimization  Markov processes  Microsoft Windows  Computational complexity  Cameras  Simultaneous localization and mapping  Visualization 
Abstract: In this paper, we present a novel approach to tightly couple visual and inertial measurements in a fixed-lag visual-inertial odometry (VIO) framework using information sparsification. To bound computational complexity, fixed-lag smoothers typically marginalize out variables, but consequently introduce a densely connected linear prior which significantly deteriorates accuracy and efficiency. Current state-of-the-art approaches account for the issue by selectively discarding measurements and marginalizing additional variables. However, such strategies are sub-optimal from an information-theoretic perspective. Instead, our approach performs a dense marginalization step and preserves the information content of the dense prior. Our method sparsifies the dense prior with a nonlinear factor graph by minimizing the information loss. The resulting factor graph maintains information sparsity, structural similarity, and nonlinearity. To validate our approach, we conduct real-time drone tests and perform comparisons to current state-of-the-art fixed-lag VIO methods in the EuRoC visual-inertial dataset. The experimental results show that the proposed method achieves competitive and superior accuracy in almost all trials. We include a detailed run-time analysis to demonstrate that the proposed algorithm is suitable for real-time applications.


Title: Towards Robust Visual Odometry with a Multi-Camera System
Key Words: cameras  distance measurement  image sampling  minimisation  photometry  pose estimation  position measurement  stereo image processing  robust visual odometry algorithm  robust VO algorithm  current pose tracker estimation  photometric error minimisation  plane-sweeping stereo cameras  near-infrared illumination  NIR illumination  single stereo configuration  multicamera setup  sliding window optimizer  sampled feature points  local mapper  multicamera system  Cameras  Tracking  Lighting  Visual odometry  Robot vision systems  Robustness  Simultaneous localization and mapping 
Abstract: We present a visual odometry (VO) algorithm for a multi-camera system and robust operation in challenging environments. Our algorithm consists of a pose tracker and a local mapper. The tracker estimates the current pose by minimizing photometric errors between the most recent keyframe and the current frame. The mapper initializes the depths of all sampled feature points using plane-sweeping stereo. To reduce pose drift, a sliding window optimizer is used to refine poses and structure jointly. Our formulation is flexible enough to support an arbitrary number of stereo cameras. We evaluate our algorithm thoroughly on five datasets. The datasets were captured in different conditions: daytime, night-time with near-infrared (NIR) illumination and nighttime without NIR illumination. Experimental results show that a multi-camera setup makes the VO more robust to challenging environments, especially night-time conditions, in which a single stereo configuration fails easily due to the lack of features.


Title: Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition
Key Words: entropy  feature extraction  geometry  image matching  image recognition  learning (artificial intelligence)  mobile robots  octrees  optical radar  robot vision  unsupervised learning  Generative Adversarial Network  adversarial feature  place recognition  global geometry map  Conditional Entropy Reduction module  unsupervised place feature  local 2D maps  dynamic octree mapping module  core modules  LiDAR inputs  end-to-end feature  geometry matching  traditional methods  LiDAR-based place recognition  unsupervised feature learning  feature size  place recognition task  North Campus Long-Term LiDAR dataset  feature learning process  place feature learning  Octrees  Laser radar  Task analysis  Decoding  Simultaneous localization and mapping  Generative adversarial networks 
Abstract: Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.


Title: DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments
Key Words: mobile robots  object detection  path planning  robot vision  SLAM (robots)  high-dynamic environments  ORB-SLAM2  dense semantic octo-tree map  dynamic objects  DS-SLAM combines semantic segmentation network  dense semantic map creation  local mapping  robust semantic visual SLAM  impressed SLAM systems  Semantics  Simultaneous localization and mapping  Image segmentation  Feature extraction  Heuristic algorithms  Three-dimensional displays  Optical flow 
Abstract: Simultaneous Localization and Mapping (SLAM) is considered to be a fundamental capability for intelligent mobile robots. Over the past decades, many impressed SLAM systems have been developed and achieved good performance under certain circumstances. However, some problems are still not well solved, for example, how to tackle the moving objects in the dynamic environments, how to make the robots truly understand the surroundings and accomplish advanced tasks. In this paper, a robust semantic visual SLAM towards dynamic environments named DS-SLAM is proposed. Five threads run in parallel in DS-SLAM: tracking, semantic segmentation, local mapping, loop closing and dense semantic map creation. DS-SLAM combines semantic segmentation network with moving consistency check method to reduce the impact of dynamic objects, and thus the localization accuracy is highly improved in dynamic environments. Meanwhile, a dense semantic octo-tree map is produced, which could be employed for high-level tasks. We conduct experiments both on TUM RGB-D dataset and in real-world environment. The results demonstrate the absolute trajectory accuracy in DS-SLAM can be improved one order of magnitude compared with ORB-SLAM2. It is one of the state-of-the-art SLAM systems in high-dynamic environments.


Title: Good Feature Selection for Least Squares Pose Optimization in VO/VSLAM
Key Words: computational complexity  control engineering computing  feature extraction  least squares approximations  optimisation  pose estimation  robot vision  SLAM (robots)  least squares pose optimization  pose estimation  pose tracking  NP-hard Max-logDet problem  feature selection  VO-VSLAM  integrating Max-logDet feature selection  Feature extraction  Optimization  Pose estimation  Simultaneous localization and mapping  Measurement uncertainty  Approximation algorithms 
Abstract: This paper aims to select features that contribute most to the pose estimation in VO/VSLAM. Unlike existing feature selection works that are focused on efficiency only, our method significantly improves the accuracy of pose tracking, while introducing little overhead. By studying the impact of feature selection towards least squares pose optimization, we demonstrate the applicability of improving accuracy via good feature selection. To that end, we introduce the Max-logDet metric to guide the feature selection, which is connected to the conditioning of least squares pose optimization problem. We then describe an efficient algorithm for approximately solving the NP-hard Max-logDet problem. Integrating Max-logDet feature selection into a state-of-the-art visual SLAM system leads to accuracy improvements with low overhead, as demonstrated via evaluation on a public benchmark.


Title: HMAPs - Hybrid Height- Voxel Maps for Environment Representation
Key Words: mobile robots  optical radar  path planning  robot vision  SLAM (robots)  2.5D representation  Microsoft Kinect One  SLAM approach  complex elements  Velodyne VLP-16 LiDAR  updated grid representation  complex environments  reliable method  occupied space  free space  HVoxel  height-voxel elements  3D point-clouds  mobile robot  grid-based mapping approach  environment representation  hybrid height- voxel maps  HMAP  Two dimensional displays  Three-dimensional displays  Simultaneous localization and mapping  Pipelines  Ray tracing  Planning  Indexing 
Abstract: This paper presents a hybrid 3D-like grid-based mapping approach, that we called HMAP, used as a reliable and efficient 3D representation of the environment surrounding a mobile robot. Considering 3D point-clouds as input data, the proposed mapping approach addresses the representation of height-voxel (HVoxel) elements inside the HMAP, where free and occupied space is modeled through HVoxels, resulting in a reliable method for 3D representation. The proposed method corrects some of the problems inherent to the representation of complex environments based on 2D and 2.5D representations, while keeping an updated grid representation. Additionally, we also propose a complete pipeline for SLAM based on HMAPs. Indoor and outdoor experiments were carried out to validate the proposed representation using data from a Microsoft Kinect One (indoor) and a Velodyne VLP-16 LiDAR (outdoor). The obtained results show that HMAPs can provide a more detailed view of complex elements in a scene when compared to a classic 2.5D representation. Moreover, validation of the proposed SLAM approach was carried out in an outdoor dataset with promising results, which lay a foundation for further research in the topic.


Title: Indoor Mapping and Localization for Pedestrians using Opportunistic Sensing with Smartphones
Key Words: Bayes methods  Gaussian processes  indoor radio  mobile computing  mobile robots  optimisation  particle filtering (numerical methods)  path planning  radionavigation  regression analysis  SLAM (robots)  smart phones  wireless LAN  Gaussian Processes Regression  real-time localization  GPR variance map  pseudowall constraints  magnetic fields  globally consistent trajectories  opportunistic magnetic headings  WiFi signal similarity validation  magnetic sequence matching  loop-closure constraints  pedestrian dead-reckoning  motion constraints  GraphSLAM front-end  signal maps  Bayesian filtering-based online localization  GraphSLAM-based offline mapping  ambient indoor environments  low-cost indoor mapping  indoor localization  smartphone  size 2.3 m  size 3.41 m  Wireless fidelity  Trajectory  Smart phones  Simultaneous localization and mapping  Ground penetrating radar  Legged locomotion 
Abstract: Indoor localization for pedestrians has gained increasing popularity among the rich body of literature for the last decade. In this paper, a low-cost indoor mapping and localization solution is proposed using the opportunistic signals from ambient indoor environments with a smartphone. It is composed of GraphSLAM-based offline mapping and Bayesian filtering-based online localization using generated signal maps. The GraphSLAM front-end is constructed by motion constraints from pedestrian dead-reckoning (PDR), loop-closure constraints identified by magnetic sequence matching with WiFi signal similarity validation, and observation constraints from opportunistic magnetic headings after error rejection. Globally consistent trajectories are created by graph optimization, after which signal maps (e.g., WiFi, magnetic fields, lights) are generated by Gaussian Processes Regression (GPR) for later localization. We propose to use the pseudo-wall constraints from the GPR variance map of magnetic fields and the lights measurements as observations for particle filtering. The proposed method is evaluated on several datasets collected from both the in-compass office buildings and outside public areas. Real-time localization is demonstrated on a smartphone in an office building covering 2000 square meters with the 50- and 90-percentile accuracies being 2.30 m and 3.41 m, respectively.


Title: Navigation without localisation: reliable teach and repeat based on the convergence theorem
Key Words: calibration  cameras  mobile robots  navigation  path planning  robot vision  velocity control  mobile robot  taught path  learned velocities  camera information  position error model  mathematical proof  camera calibration  navigation system  mathematical model  explicit localisation  teach-and-repeat navigation scenarios  teach-and-repeat visual navigation  Robot kinematics  Navigation  Cameras  Robot vision systems  Simultaneous localization and mapping  Feature extraction 
Abstract: We present a novel concept for teach-and-repeat visual navigation. The proposed concept is based on a mathematical model, which indicates that in teach-and-repeat navigation scenarios, mobile robots do not need to perform explicit localisation. Rather than that, a mobile robot which repeats a previously taught path can simply “replay” the learned velocities, while using its camera information only to correct its heading relative to the intended path. To support our claim, we establish a position error model of a robot, which traverses a taught path by only correcting its heading. Then, we outline a mathematical proof which shows that this position error does not diverge over time. Based on the insights from the model, we present a simple monocular teach-and-repeat navigation method. The method is computationally efficient, it does not require camera calibration, and it can learn and autonomously traverse arbitrarily-shaped paths. In a series of experiments, we demonstrate that the method can reliably guide mobile robots in realistic indoor and outdoor conditions, and can cope with imperfect odometry, landmark deficiency, illumination variations and naturally-occurring environment changes. Furthermore, we provide the navigation system and the datasets gathered at www.github.com/gestom/stroll_bearnav.


Title: The TUM VI Benchmark for Evaluating Visual-Inertial Odometry
Key Words: augmented reality  calibration  cameras  distance measurement  image capture  image sensors  image sequences  mobile robots  optical tracking  pose estimation  robot vision  SLAM (robots)  synchronisation  visual-inertial odometry  photometric calibration  motion capture system  IMU measurements  pose ground truth  inertial measurements  vision sensors  augmented reality  SLAM methods  visual odometry  IMU sensors  camera images  TUM VI benchmark  frequency 20.0 Hz  frequency 200.0 Hz  frequency 120.0 Hz  Cameras  Calibration  Simultaneous localization and mapping  Benchmark testing  Visual odometry  Time measurement 
Abstract: Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20 Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120 Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data is publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.


Title: Scale-Robust Localization Using General Object Landmarks
Key Words: distance measurement  feature extraction  learning (artificial intelligence)  mobile robots  object detection  robot vision  SLAM (robots)  deep-learning-based object features  KITTI Odometry benchmark  outdoor images  scale-robust localization  visual localization  robotic mapping applications  object landmarks  SIFT point-features  Visualization  Measurement  Simultaneous localization and mapping  Robustness  Databases  Search problems 
Abstract: Visual localization under large changes in scale is an important capability in many robotic mapping applications, such as localizing at low altitudes in maps built at high altitudes, or performing loop closure over long distances. Existing approaches, however, are robust only up to about a 3× difference in scale between map and query images. We propose a novel combination of deep-learning-based object features and state-of-the-art SIFT point-features that yields improved robustness to scale change. This technique is training-free and class-agnostic, and in principle can be deployed in any environment out-of-the-box. We evaluate the proposed technique on the KITTI Odometry benchmark and on a novel dataset of outdoor images exhibiting changes in visual scale of 7× and greater, which we have released to the public. Our technique consistently outperforms localization using either SIFT features or the proposed object features alone, achieving both greater accuracy and much lower failure rates under large changes in scale.


Title: Invariant smoothing on Lie Groups
Key Words: estimation theory  Kalman filters  Lie groups  linearisation techniques  optimisation  robot vision  SLAM (robots)  smoothing methods  linearizations  invariant Kalman filtering  robot localization  posteriori estimator  nonlinear smoothing methods  group-affine observation systems  Lie groups  invariant smoothing  Smoothing methods  Manifolds  Simultaneous localization and mapping  Kalman filters  Random variables  Estimation  Robot localization 
Abstract: In this paper we propose a (non-linear) smoothing algorithm for group-affine observation systems, a recently introduced class of estimation problems on Lie groups that bear a particular structure. As most non-linear smoothing methods, the proposed algorithm is based on a maximum a posteriori estimator, determined by optimization. But owing to the specific properties of the considered class of problems, the involved linearizations are proved to have a form of independence with respect to the current estimates, leveraged to avoid (partially or sometimes totally) the need to relinearize. The method is validated on a robot localization example, both in simulations and on real experimental data.


Title: A Combined RGB and Depth Descriptor for SLAM with Humanoids
Key Words: cameras  feature extraction  humanoid robots  image colour analysis  mobile robots  pose estimation  robot vision  SLAM (robots)  feature tracking  codebooks  reproducibility  humanoid robots  visual simultaneous localization  depth descriptor  ORB-SLAM  visual SLAM system  track features  DLab  RGB-D camera  Nao humanoid  binary descriptor  FAB-MAP  place recognition module  Simultaneous localization and mapping  Image color analysis  Cameras  Three-dimensional displays  Humanoid robots  Visualization 
Abstract: In this paper, we present a visual simultaneous localization and mapping (SLAM) system for humanoid robots. We introduce a new binary descriptor called DLab that exploits the combined information of color, depth, and intensity to achieve robustness with respect to uniqueness, reproducibility, and stability. We use DLab within ORB-SLAM, where we replaced the place recognition module with a modification of FAB-MAP that works with newly built codebooks using our binary descriptor. In experiments carried out in simulation and with a real Nao humanoid equipped with an RGB-D camera, we show that DLab has a superior performance in comparison to other descriptors. The application to feature tracking and place recognition reveal that the new descriptor is able to reliably track features even in sequences with seriously blurred images and that it has a higher percentage of correctly identified similar images. As a result, our new visual SLAM system has a lower absolute trajectory error in comparison to ORB-SLAM and is able to accurately track the robot's trajectory.


Title: Contact Force Control of an Aerial Manipulator in Pressing an Emergency Switch Process
Key Words: aerospace robotics  aircraft control  autonomous aerial vehicles  force control  manipulators  position control  springs (mechanical)  vibration control  emergency switch process  dangerous work situation  industrial leakage accidents  flexible robot  small robot  aerial manipulator system  hexa-rotor UAV  UAV platform  hover flight  impedance control algorithm  force-sensorless contact force control method  one-DOF manipulator  spring-mass-damper system model  Manipulators  Force  Contacts  Attitude control  Force control  Pressing 
Abstract: The dangerous work situation in industrial leakage accidents urgently needs a flexible and small robot to help workers perform operations and to protect them from being injured. An aerial manipulator system consisting of a hexa-rotor UAV and a one-DOF manipulator is developed, and is used to press an emergency switch to shut off machinery in an emergency. In practical application, an aerial manipulator usually performs contact operations as the UAV platform is in hover flight. The hovering UAV acting as a spring-mass-damper system is firstly proved. Then, based on the derived spring-mass-damper system model and the impedance control algorithm, the force-sensorless contact force control method is presented. That is, the force is indirectly controlled through controlling the UAV's position error and pitch angle simultaneously. The practical operation experiment of pressing an emergency button shows that the proposed method is able to control the contact force as the aerial manipulator interacts with the external environment.


Title: Pose Estimation and Map Formation with Spiking Neural Networks: towards Neuromorphic SLAM
Key Words: mixed analogue-digital integrated circuits  mobile robots  neural nets  neurophysiology  pose estimation  SLAM (robots)  pose estimation  spiking neural networks  neuromorphic SLAM  biologically inspired neuronal path integration  mobile robot  neuronal map formation architecture  simultaneous localization and mapping  mixed signal analog-digital neuromorphic hardware  ultra low-power neuromorphic hardware  robotic vehicle simulation  on-board plasticity  Neurons  Neuromorphics  Collision avoidance  Simultaneous localization and mapping  Synapses 
Abstract: In this paper, we investigate the use of ultra low-power, mixed signal analog/digital neuromorphic hardware for implementation of biologically inspired neuronal path integration and map formation for a mobile robot. We perform spiking network simulations of the developed architecture, interfaced to a simulated robotic vehicle. We then port the neuronal map formation architecture on two connected neuromorphic devices, one of which features on-board plasticity, and demonstrate the feasibility of a neuromorphic realization of simultaneous localization and mapping (SLAM).


Title: Precise Localization in High-Definition Road Maps for Urban Regions
Key Words: cameras  image resolution  Kalman filters  nonlinear filters  road vehicles  satellite navigation  stereo image processing  traffic engineering computing  high-resolution road maps  road borders  Unscented Kalman Filter  narrow urban roads  highly automated driving  precise localization  high-definition road maps  sensor specific feature layers  stereo camera  vehicle odometry  low-cost GNSS module  size 5.0 km  size 0.08 m  Roads  Global navigation satellite system  Simultaneous localization and mapping  Semantics  Urban areas  Receivers 
Abstract: The future of automated driving in urban areas will most probably rely on highly accurate road maps. However, the necessary precision of a localization in such maps has so far only been reached using extra, sensor specific feature layers for localization. In this paper we want to show that it is possible to achieve sufficient accuracy without a separate localization layer. Instead, elements are used that are already contained in high-resolution road maps, such as markings and road borders. For this, we introduce a modular approach in which detections from different detection algorithms are associated with elements in the map and then fused to an absolute pose using an Unscented Kalman Filter. We evaluate our approach using a sensor setup that employs a stereo camera, vehicle odometry and a low-cost GNSS module on a 5km test route covering both narrow urban roads and multi-lane main roads under varying weather conditions. The results show that this approach is capable to be used for highly automated driving, showing an accuracy of 0.08m in typical road scenarios and a is available 98% of the time.


Title: Virtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments
Key Words: graph theory  image reconstruction  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  3D scene reconstructions  virtual occupancy grid map  mobile robots  VOG-map  submap-based pose graph SLAM  underwater SLAM system  path planning  free space information  Simultaneous localization and mapping  Three-dimensional displays  Path planning  Robot kinematics  Casting 
Abstract: In this paper, we propose a mapping approach that constructs a globally deformable virtual occupancy grid map (VOG-map) based on local submaps. Such a representation allows pose graph SLAM systems to correct globally accumulated drift via loop closures while maintaining free space information for the purpose of path planning. We demonstrate use of such a representation for implementing an underwater SLAM system in which the robot actively plans paths to generate accurate 3D scene reconstructions. We evaluate performance on simulated as well as real-world experiments. Our work furthers capabilities of mobile robots actively mapping and exploring unstructured, three dimensional environments.


Title: LDSO: Direct Sparse Odometry with Loop Closure
Key Words: feature extraction  graph theory  mobile robots  optimisation  pose estimation  robot vision  SLAM (robots)  intensity gradient  DSO sliding window optimization  Sim(3) relative pose constraints  image pixel  loop closure detection  monocular visual SLAM system  Direct Sparse Odometry  state-of-the-art feature-based systems  pose-graph optimization  modified point selection strategy  relative poses  co-visibility graph  3D geometric error terms  conventional feature-based bag-of-words approach  loop closure candidates  tracking frontend  corner features  LDSO  featureless areas  Optimization  Feature extraction  Microsoft Windows  Simultaneous localization and mapping  Cameras  Bundle adjustment  Robustness 
Abstract: In this paper we present an extension of Direct Sparse Odometry (DSO) [1] to a monocular visual SLAM system with loop closure detection and pose-graph optimization (LDSO). As a direct technique, DSO can utilize any image pixel with sufficient intensity gradient, which makes it robust even in featureless areas. LDSO retains this robustness, while at the same time ensuring repeatability of some of these points by favoring corner features in the tracking frontend. This repeatability allows to reliably detect loop closure candidates with a conventional feature-based bag-of-words (BoW) approach. Loop closure candidates are verified geometrically and Sim(3) relative pose constraints are estimated by jointly minimizing 2D and 3D geometric error terms. These constraints are fused with a co-visibility graph of relative poses extracted from DSO's sliding window optimization. Our evaluation on publicly available datasets demonstrates that the modified point selection strategy retains the tracking accuracy and robustness, and the integrated pose-graph optimization significantly reduces the accumulated rotation-, translation- and scale-drift, resulting in an overall performance comparable to state-of-the-art feature-based systems, even without global bundle adjustment.


Title: Uncertainty-based Online Mapping and Motion Planning for Marine Robotics Guidance
Key Words: autonomous underwater vehicles  path planning  probability  robot dynamics  vehicle dynamics  uncertainty-based framework  online computation constraints  motion planning  marine robotics guidance  robotic systems  safe path  underwater environments  autonomous vehicles  probabilistic safety  online mapping  Uncertainty  Safety  Planning  Probabilistic logic  Robot sensing systems  Vehicle dynamics 
Abstract: In real-world robotics, motion planning remains to be an open challenge. Not only robotic systems are required to move through unexplored environments, but also their manoeuvrability is constrained by their dynamics and often suffer from uncertainty. One approach to overcome this problem is to incrementally map the surroundings while, simultaneously, planning a safe and feasible path to a desired goal. This is especially critical in underwater environments, where autonomous vehicles must deal with both motion and environment uncertainties. In order to cope with these constraints, this work proposes an uncertainty-based framework for mapping and planning3 feasible motions online with probabilistic safety-guarantees. The proposed approach deals with the motion, probabilistic safety, and online computation constraints by (i) incrementally representing the environment as a collection of local maps, and (ii) iteratively (re)planning kinodynamically-feasible and probabilistically-safe paths to goal. The proposed framework is evaluated on the Sparus II, a nonholonomic torpedo-shaped AUV, by conducting simulated and real-world trials, thus proving the efficacy of the method and its suitability even for systems with limited on-board computational power.


Title: Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correction
Key Words: accelerometers  calibration  cameras  distance measurement  gyroscopes  inertial navigation  learning (artificial intelligence)  mobile robots  neural nets  pose estimation  robot vision  SLAM (robots)  vision-aided absolute trajectory estimation  unsupervised deep network  online error correction  unsupervised deep neural network approach  RGB-D imagery  inertial measurements  Visual-Inertial-Odometry Learner  inertial measurement unit intrinsic parameters  white noise  extrinsic calibration  camera  IMU measurements  hypothesis trajectories  scaled image projection errors  visual odometry  visual simultaneous localization  KITTI Odometry dataset  competitive odometry performance  visual-inertial odometry  Cameras  Jacobian matrices  Image reconstruction  Trajectory  Simultaneous localization and mapping  Training 
Abstract: Adstract- We present an unsupervised deep neural network approach to the fusion of RGB-D imagery with inertial measurements for absolute trajectory estimation. Our network, dubbed the Visual-Inertial-Odometry Learner (VIOLearner), learns to perform visual-inertial odometry (VIO) without inertial measurement unit (IMU) intrinsic parameters (corresponding to gyroscope and accelerometer bias or white noise) or the extrinsic calibration between an IMU and camera. The network learns to integrate IMU measurements and generate hypothesis trajectories which are then corrected online according to the Jacobians of scaled image projection errors with respect to a spatial grid of pixel coordinates. We evaluate our network against state-of-the-art (SOA) visual-inertial odometry, visual odometry, and visual simultaneous localization and mapping (VSLAM) approaches on the KITTI Odometry dataset [1] and demonstrate competitive odometry performance.


Title: Modeling and Control of an Articulated Tail for Maneuvering a Reduced Degree of Freedom Legged Robot
Key Words: actuators  feedback  hardware-in-the loop simulation  legged locomotion  linearisation techniques  motion control  robot dynamics  leg mechanisms  quadruped robot  dynamic tail motions  robotic system design  outer loop controller  articulated tail mechanism  inner loop controller  tail prototype  dynamic modeling control  articulated robotic tail  maneuvering  legged robotic systems  reduced degree of freedom legged robot  hardware-in-the-loop experiments  quadruped platform simulation  feedback linearization maps  Legged locomotion  Robot kinematics  Foot  Dynamics  Task analysis  Manipulators 
Abstract: This paper presents dynamic modeling and control of an articulated robotic tail to maneuver and stabilize a reduced degree-of-freedom (DOF) quadruped robot. Conventional legged robotic systems consist of leg mechanisms that provide simultaneous propulsion, maneuvering and stabilization. However, in nature animals have been observed to utilize their tails to assist the legs in multiple tasks. Similarly, by incorporating an articulated tail onboard a quadruped robot, dynamic tail motions can be used to aid maneuvering. Therefore, tail implementation can potentially lead to simplifications in design and control of the legged robot since the legs will be responsible for only propulsion tasks. In this paper, a robotic system design consisting of an articulated tail and quadruped robot system is presented. Dynamic models are derived to analyze an optimal tail mass and length ratio to enhance inertial adjustment applications and develop an outer loop controller to plan tail trajectories for desired maneuvering applications. Results of analytical optimization are corroborated with measured data from biological animals. To decouple the dynamics of the articulated tail mechanism an inner loop controller using feedback linearization maps the desired behavior to the actuator inputs. This approach is validated using hardware-in-the-loop experiments with tail prototype in conjunction with simulated quadruped platform. Results demonstrate the capabilities of the articulated tail in enabling precise left and right turning (maneuvering).


Title: Muscle Activation Source Model-based sEMG Signal Decomposition and Recognition of Interface Rotation
Key Words: biomechanics  electromyography  medical signal processing  motion estimation  skin  muscle structures  muscle activation source model-based sEMG signal decomposition  sEMG interface rotation  muscle activation signals  surface electromyography signals  muscle activation extraction  hand motion estimation  rotation recognition  inertial measurement unit  Electrodes  Muscles  Mathematical model  Signal resolution  Electromyography  Conductivity  Motion estimation 
Abstract: Muscle activation signals are measured from the skin surface as surface electromyography (EMG) signals that contain information on human intentions; therefore, they are widely used in various robotics applications owing to their usability. However, selective muscle activation extraction is difficult because of the complexity of muscle structures. This study investigated muscle activation source model-based sEMG signal decomposition that considers the anatomical factors of muscle structures. The main advantage of the proposed model-based signal decomposition is that sEMG interface rotation can be recognized by comparing source parameters identified before and after rotation. To assess the performance of the proposed model-based decomposition method, hand motion estimation and rotation recognition were conducted. Additionally, two-dimensional simultaneous control was conducted with an inertial measurement unit to verify the usability of the proposed model. The results indicate that the proposed model decomposes an sEMG signal based on motion with good performance and demonstrate feasibility of motion estimation independent of sEMG interface rotation.


Title: Augmenting Physical Simulators with Stochastic Neural Networks: Case Study of Planar Pushing and Bouncing
Key Words: Gaussian processes  learning (artificial intelligence)  neural nets  state estimation  robot state estimation  planar pushing  ball bouncing  analytical rigid-body simulator  model uncertainty  symbolic simulators  stochastic neural networks  generalizable physical simulator  universal uncertainty estimates  analytical learned simulators  Gaussian processes  object trajectories  Analytical models  Predictive models  Physics  Data models  Uncertainty  Engines  Neural networks 
Abstract: An efficient, generalizable physical simulator with universal uncertainty estimates has wide applications in robot state estimation, planning, and control. In this paper, we build such a simulator for two scenarios, planar pushing and ball bouncing, by augmenting an analytical rigid-body simulator with a neural network that learns to model uncertainty as residuals. Combining symbolic, deterministic simulators with learnable, stochastic neural nets provides us with expressiveness, efficiency, and generalizability simultaneously. Our model outperforms both purely analytical and purely learned simulators consistently on real, standard benchmarks. Compared with methods that model uncertainty using Gaussian processes, our model runs much faster, generalizes better to new object shapes, and is able to characterize the complex distribution of object trajectories.


Title: Robotic Subsurface Pipeline Mapping with a Ground-penetrating Radar and a Camera
Key Words: buried object detection  geophysical image processing  geophysical techniques  ground penetrating radar  image reconstruction  maximum likelihood estimation  pipelines  radar detection  radar imaging  robot vision  pipeline groups  hyperbola response  GPR sensing process  Ground Penetrating Radar scans  subsurface pipeline mapping method  robotic subsurface pipeline mapping  subsurface pipes  representative pipeline configurations  maximum likelihood estimation  J-Linkage method  hyperbolas  GPR scans  mapping outputs  visual simultaneous localization  nonperpendicular angles  general scanning  size 4.69 cm  Ground penetrating radar  Pipelines  Cameras  Three-dimensional displays  Trajectory  Robot sensing systems 
Abstract: We propose a novel subsurface pipeline mapping method by fusing Ground Penetrating Radar (GPR) scans and camera images. To facilitate the simultaneous detection of multiple pipelines, we model the GPR sensing process and prove hyperbola response for general scanning with non-perpendicular angles. Furthermore, we fuse visual simultaneous localization and mapping outputs, encoder readings with GPR scans to classify hyperbolas into different pipeline groups. We extensively apply the J-Linkage method and maximum likelihood estimation to improve algorithm robustness and accuracy. As the result, we optimally estimate the radii and locations of all pipelines. We have implemented our method and tested it in physical experiments with representative pipeline configurations. The results show that our method successfully reconstructs all subsurface pipes. Moreover, the average localization error is 4.69cm.


Title: Mobile Robot Localization Considering Class of Sensor Observations
Key Words: collision avoidance  mobile robots  localization robustness  environment dynamics  robots  sensor observations  mapped obstacles  observation model  unmapped obstacles  real-world mobile robot navigation competition  mobile robot localization  Robot sensing systems  Mathematical model  Robustness  Hidden Markov models  Mobile robots  Collision avoidance 
Abstract: Localization robustness against environment dynamics is significant for robots to achieve autonomous navigation in unmodified environments. A basic method of improving the robustness of a robot is considering the sensor observations obtained from mapped obstacles and using them for localizing the robot's pose. This study proposes an observation model that considers the class of sensor observations, where “class” categorizes the sensor observations as those obtained from mapped and unmapped obstacles. In the proposed approach, the robot's pose and the class are estimated simultaneously. As a result, the robot's pose can be localized using the sensor observations obtained only from mapped obstacles. First, we evaluated the performance of the proposed approach using simulations. Further, we tested the proposed approach in a real-world mobile robot navigation competition, called “Tsukuba Challenge,” held in Japan. The robustness and effectiveness of the proposed approach against environment dynamics were verified from the experimental results.


Title: Human-in-the-loop Augmented Mapping
Key Words: inertial systems  mobile robots  operating systems (computers)  optical radar  path planning  robot programming  user interfaces  2D map building  user interface  human map augmentation  LIDAR  Gmapping ROS package  Unity software  online editing capabilities  user-friendly system  traditional offline post processing  real-time human augmented mapping system  human-in-the-loop  mapping errors  Two dimensional displays  Laser radar  Simultaneous localization and mapping  Three-dimensional displays  Corporate acquisitions 
Abstract: In this paper we develop a real-time human augmented mapping system. This approach replaces the traditional offline post processing of maps by a user-friendly system allowing for online editing capabilities. A wide number of applications that acquire accurate mapping of the environment could benefit from such a solution. The proposed framework consists of two main parts: 2D map building using LIDAR, encoders, and IMU; and a user interface for human map augmentation. The first part is built over Gmapping ROS package, while the second is developed in Unity software. Realworld experiments validated the ability of our system to correct for sensor noise and various mapping errors, thus increasing the accuracy of the obtained maps without additional computational costs.


Title: A B-Spline Mapping Framework for Long-Term Autonomous Operations
Key Words: image representation  image sensors  mobile robots  navigation  path planning  robot vision  SLAM (robots)  splines (mathematics)  landmark-based maps  robotics community  high frequency sensor  B-spline curves  B-spline maps  mapping algorithm  2D B-spline mapping framework  outdoor long-term autonomous operations  simultaneous localization and mapping  SLAM algorithm  software-in-the-loop simulations  Splines (mathematics)  Simultaneous localization and mapping  Three-dimensional displays  Robot kinematics  Two dimensional displays 
Abstract: This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.


Title: PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization
Key Words: feature extraction  mobile robots  optical radar  path planning  local views  sliding window fashion  matching current  old features  map representation  local maps  off-road environments  single localization failure  distinctive features  coined PoseMap  dynamic environments  robotic systems  long-term localization  multienvironment 3D LiDAR localization  frequency 8.0 Hz  time 18.0 month  Simultaneous localization and mapping  Three-dimensional displays  Laser radar  Optimization  Feature extraction 
Abstract: Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.


Title: Preliminary Evaluation of Null-Space Dynamic Process Model Identification with Application to Cooperative Navigation of Underwater Vehicles
Key Words: least squares approximations  marine communication  parameter estimation  position control  underwater vehicles  vehicle dynamics  UV model parameters  control-surface parameters  thruster-model parameters  preliminary evaluation  null-space dynamic process model identification  underactuated underwater vehicle  control-input parameters  UV nonlinear plant-model parameters  nonlinear model identification  underwater communication  cooperative navigation  null-space least-squares parameter identification method  Navigation  Vehicle dynamics  Acoustics  Underwater vehicles  Heuristic algorithms  Kinematics  Kalman filters 
Abstract: This paper reports a method and preliminary evaluation of a novel null-space least-squares parameter identification method for a fully nonlinear second -order 6-degree-of-freedom (DOF) dynamic process model of an underactuated underwater vehicle (UV) for which both the model parameters and the control-input parameters are unknown. This paper further reports the application of the identified plant models in combined underwater communication and navigation (cooperative navigation) of UVs. We report an approach to model identification that simultaneously identifies 6-DOF UV nonlinear plant-model parameters, control-surface parameters, and thruster-model parameters. We believe this approach is suitable for identifying plant model parameters from data obtained in full-scale experimental trials of UVs in controlled motion. The reported approach to nonlinear model identification of UVs is evaluated in simulation studies. The resulting identified UV plant models are further evaluated in simulated cooperative navigation missions of the UV that are representative of high-precision survey missions. To the best of our knowledge, this paper reports the first method to identify 6-DOF UV model parameters, control-surface parameters, and thruster-model parameters simultaneously.


Title: Robust Exploration with Multiple Hypothesis Data Association
Key Words: image fusion  mobile robots  robot vision  SLAM (robots)  target tracking  tree searching  joint compatibility branch  simultaneous localization and mapping  map accuracy  diverse hypotheses  multiple hypothesis tracking  robust back-ends  catastrophic failure  single false positive assignment  rich features  autonomous exploration  SLAM  ambiguous data association problem  multiple hypothesis data association  robust exploration  Simultaneous localization and mapping  Trajectory  Noise measurement  State estimation  Optimization  Measurement uncertainty 
Abstract: We study the ambiguous data association problem confronting simultaneous localization and mapping (SLAM), specifically for the autonomous exploration of environments lacking rich features. In such environments, a single false positive assignment might lead to catastrophic failure, which even robust back-ends may be unable to resolve. Inspired by multiple hypothesis tracking, we present a novel approach to effectively manage multiple hypotheses (MH) of data association inherited from traditional joint compatibility branch and bound (JCBB), which entails the generation, ordering and elimination of hypotheses. We analyze the performance of MHJCBB in two particular situations, one applying it to SLAM over a predefined trajectory and the other showing its applicability in exploring unknown environments. Statistical results demonstrate that MHJCBB's maintenance of diverse hypotheses under ambiguous conditions significantly improves map accuracy.


Title: Simultaneous Task Allocation and Planning Under Uncertainty
Key Words: control engineering computing  formal verification  iterative methods  Markov processes  mobile robots  multi-robot systems  operating systems (computers)  path planning  resource allocation  robot programming  temporal logic  simultaneous task allocation  uncertain environments  individual robot behaviour  linear temporal logic  multirobot policies  simultaneous task planning  Markov decision processes  formal verification  multirobot operating systems  Task analysis  Planning  Robot kinematics  Resource management  Uncertainty  Probabilistic logic 
Abstract: We propose novel techniques for task allocation and planning in multi-robot systems operating in uncertain environments. Task allocation is performed simultaneously with planning, which provides more detailed information about individual robot behaviour, but also exploits independence between tasks to do so efficiently. We use Markov decision processes to model robot behaviour and linear temporal logic to specify tasks and safety constraints. Building upon techniques and tools from formal verification, we show how to generate a sequence of multi-robot policies, iteratively refining them to reallocate tasks if individual robots fail, and providing probabilistic guarantees on the performance (and safe operation) of the team of robots under the resulting policy. We implement our approach and evaluate it on a benchmark multi-robot example.


Title: Strategic-Tactical Planning for Autonomous Underwater Vehicles over Long Horizons
Key Words: autonomous underwater vehicles  control engineering computing  mobile robots  planning (artificial intelligence)  robot dynamics  vehicle dynamics  strategic-tactical planning  autonomous underwater vehicles  long horizons  persistent autonomy  AI Planners  long-term autonomous behaviour  abstraction planning techniques  two-level hierarchical structure  hierarchical decompositions  Task analysis  Planning  Manifolds  Batteries  Inspection  Robots  Valves 
Abstract: In challenging environments where human intervention is expensive, robust and persistent autonomy is a key requirement. AI Planners can efficiently construct plans to achieve this long-term autonomous behaviour. However, in plans which are expected to last over days, or even weeks, the size of the state-space becomes too large for current planners to solve as a single problem. These problems are well-suited to decomposition and abstraction planning techniques. We present a novel approach in the context of persistent autonomy in autonomous underwater vehicles, in which tasks are complex and diverse and plans cannot be precomputed. Our approach performs a decomposition into a two-level hierarchical structure, which dynamically constructs planning problems at the upper level of the hierarchy using solution plans from the lower level. Solution plans are then executed and monitored simultaneously at both levels. We evaluate the approach, showing that compared to strictly top-down hierarchical decompositions, our approach leads to more robust solution plans of higher quality.


Title: A Variational Feature Encoding Method of 3D Object for Probabilistic Semantic SLAM
Key Words: Bayes methods  belief networks  feature extraction  maximum likelihood estimation  object recognition  probability  robot vision  SLAM (robots)  object recognition methods  true generative model  semantic simultaneous localization and mapping  maximum likelihood estimation  shape retrieval  Bayesian inference  Bayesian networks  approximated distributions  variational auto-encoder  complex distributions  observation likelihood  tractable distributions  3D object shapes  view-independent loop closure  object shape  range sensor  mobile robot  complex probability distribution  probabilistic observation model  high-level semantic features  complex 3D objects  probabilistic semantic SLAM  variational feature encoding method  Shape  Simultaneous localization and mapping  Three-dimensional displays  Semantics  Solid modeling  Bayes methods  Probabilistic logic 
Abstract: This paper presents a feature encoding method of complex 3D objects for high-level semantic features. Recent approaches to object recognition methods become important for semantic simultaneous localization and mapping (SLAM). However, there is a lack of consideration of the probabilistic observation model for 3D objects, as the shape of a 3D object basically follows a complex probability distribution. Furthermore, since the mobile robot equipped with a range sensor observes only a single view, much information of the object shape is discarded. These limitations are the major obstacles to semantic SLAM and view-independent loop closure using 3D object shapes as features. In order to enable the numerical analysis for the Bayesian inference, we approximate the true observation model of 3D objects to tractable distributions. Since the observation likelihood can be obtained from the generative model, we formulate the true generative model for 3D object with the Bayesian networks. To capture these complex distributions, we apply a variational auto-encoder. To analyze the approximated distributions and encoded features, we perform classification with maximum likelihood estimation and shape retrieval.


Title: Robust Sensor Fusion with Self-Tuning Mixture Models
Key Words: adaptive control  control system synthesis  expectation-maximisation algorithm  Gaussian processes  least squares approximations  mixture models  nonlinear control systems  optimisation  robots  robust control  self-adjusting systems  sensor fusion  state estimation  robust sensor fusion  self-tuning mixture models  nonlinear state estimation  robotics  robust cost functions  nonGaussian error models  environmental changes  ageing  error distribution  state estimation process  Gaussian mixture  sensor model  standard state estimation  implicit expectation-maximization approach  distribution parameters  self-tuning algorithm  least-squares optimization framework  parameter tuning  Estimation  Robot sensing systems  Optimization  Tuning  Biological system modeling  Heuristic algorithms  Standards 
Abstract: A fundamental problem of non-linear state estimation in robotics is the violation of assumptions about the sensors' error distribution. State of the art approaches reduce the impact of these violations with robust cost functions or predefined non-Gaussian error models. Both require extensive parameter tuning and fail if the sensors' error characteristic changes over time, due to environmental changes, ageing or sensor malfunctions. We demonstrate how the error distribution itself can be part of the state estimation process. Based on an efficient approximation of a Gaussian mixture, we optimize the sensor model simultaneously during the standard state estimation. Due to an implicit expectation-maximization approach, we achieve a fast convergence without prior knowledge of the true distribution parameters. We implement this self-tuning algorithm in a least-squares optimization framework and demonstrate its real time capability on a real world dataset for satellite localization of a driving vehicle. The resulting estimation quality is superior to previous robust algorithms.


Title: ArthroSLAM: Multi-Sensor Robust Visual Localization for Minimally Invasive Orthopedic Surgery
Key Words: biomedical optical imaging  cameras  endoscopes  image sensors  Kalman filters  medical image processing  medical robotics  orthopaedics  SLAM (robots)  surgery  image feedback  ArthroSLAM  Simultaneous Localisation and Mapping system  SLAM system  external camera  robotic arm  minimally invasive arthroscopic surgery  minimally invasive orthopedic surgery  robotic orthopedic surgical assistant  knee section  human cadaver knee joint  Extended Kalman Filter framework  arthroscope holder  intraarticular space  Cameras  Robot vision systems  Visualization  Reliability 
Abstract: Minimally invasive arthroscopic surgery is a very challenging procedure that requires the manipulation of instruments in limited intraarticular space using distorted and sometimes uninformative images. Localizing the arthroscope reliably and at all times w.r.t. surrounding tissue is of fundamental importance to prevent unintended injury to patients. However, even highly-trained surgeons can struggle to localize the arthro-scope using poor image feedback. In this paper, we propose and demonstrate for the first time a visual Simultaneous Localisation and Mapping (SLAM) system, termed ArthroSLAM, capable of robustly and reliably localizing an arthroscope inside a human knee joint. The proposed system fuses the information obtained from the arthroscope, an external camera mounted on an arthroscope holder, and the odometry of a robotic arm manipulating the scope, in an Extended Kalman Filter framework. Also for the first time, we implement five alternative strategies for localization and compare them to our method in a realistic setup with a human cadaver knee joint. ArthroSLAM is shown to outperform the alternative strategies under various challenging conditions, localizing reliably and at all times with a mean Relative Pose Error of up to 1.4mm and 0.7°. Additional experiments conducted with degraded odometry data also validate the robustness of the method. An initial evaluation of the sparse map of a knee section computed by our method exhibits good morphological agreement. All results suggest that ArthroSLAM is a viable component for the robotic orthopedic surgical assistant of the future.


Title: Robot Identification and Localization with Pointing Gestures
Key Words: distance measurement  gesture recognition  mobile robots  multi-robot systems  pose estimation  robot vision  SLAM (robots)  mobile robot  multirobot scenarios  robot identification and localization  gesture pointing  robot odometry frame  inertial measurement unit  IMU  Robot sensing systems  Robot kinematics  Solid modeling  Manipulators  Drones  Three-dimensional displays 
Abstract: We propose a novel approach to establish the relative pose of a mobile robot with respect to an operator that wants to interact with it; we focus on scenarios in which the robot is in the same environment as the operator, and is visible to them. The approach is based on comparing the trajectory of the robot, which is known in the robot's odometry frame, to the motion of the arm of the operator, who, for a short time, keeps pointing at the robot they want to interact with. In multi-robot scenarios, the same approach can be used to simultaneously identify which robot the operator wants to interact with. The main advantage over alternatives is that our system only relies on the robot's odometry, on a wearable inertial measurement unit (IMU), and, crucially, on the operator's own perception. We experimentally show the feasibility of our approach using real-world robots.


Title: Multimotion Visual Odometry (MVO): Simultaneous Estimation of Camera and Third-Party Motions
Key Words: cameras  computer vision  image motion analysis  image segmentation  image sensors  image sequences  motion estimation  object detection  object tracking  stereo image processing  dynamic scene  multimotion visual odometry pipeline  MVO  dynamic objects  motion capture system  simultaneous estimation  third-party motions  computer vision  previous work  moving camera  largely static environment  segment  tracking-by-detection  motion constraints  planar motion  SE motion  scene flow  unconstrained motions  camera motions  object tracking  stereo/RGB-D camera  multimodal visual odometry pipeline  Cameras  Motion segmentation  Tracking  Dynamics  Trajectory  Estimation  Image segmentation 
Abstract: Estimating motion from images is a well-studied problem in computer vision and robotics. Previous work has developed techniques to estimate the motion of a moving camera in a largely static environment (e.g., visual odometry) and to segment or track motions in a dynamic scene using known camera motions (e.g., multiple object tracking). It is more challenging to estimate the unknown motion of the camera and the dynamic scene simultaneously. Most previous work requires a priori object models (e.g., tracking-by-detection), motion constraints (e.g., planar motion), or fails to estimate the full SE (3) motions of the scene (e.g., scene flow). While these approaches work well in specific application domains, they are not generalizable to unconstrained motions. This paper extends the traditional visual odometry (VO) pipeline to estimate the full SE (3) motion of both a stereo/RGB-D camera and the dynamic scene. This multimotion visual odometry (MVO) pipeline requires no a priori knowledge of the environment or the dynamic objects. Its performance is evaluated on a real-world dynamic dataset with ground truth for all motions from a motion capture system.


Title: Semi-Supervised SLAM: Leveraging Low-Cost Sensors on Underground Autonomous Vehicles for Position Tracking
Key Words: cameras  learning (artificial intelligence)  mining  mining industry  mobile robots  object tracking  robot vision  SLAM (robots)  ORB-SLAM2  ground map locations  deep learning  position tracking  operational underground mining vehicles  single camera localization  map creation  mine environment  mining companies  underground environment  SemiSupervised SLAM  underground autonomous vehicles  low-cost sensors  Simultaneous localization and mapping  Cameras  Measurement  Grounding  Visual odometry  Lighting 
Abstract: This work presents Semi-Supervised SLAM - a method for developing a map suitable for coarse localization within an underground environment with minimal human intervention, with system characteristics driven by real-world requirements of major mining companies. This work leverages existing information common within a mining environment - namely a surveyed mine map - which is used to sparsely ground map locations within the mine environment, increasing map accuracy and allowing localization within a global frame. Map creation utilizes a low cost camera sensor and minimal user information to produce a map which can be used for single camera localization within a mining environment. We evaluate the localization capabilities of the proposed approach in depth by performing data collection on operational underground mining vehicles within an active underground mine and by simulating occlusions common to the environment such as dust and water. The proposed system is capable of producing maps which have an average localization error 2.5 times smaller than the next best performing method ORB-SLAM2, comparable localization performance to a state-of-the-art deep learning approach (which is not a feasible solution due to both compute and training requirements) and is robust to simulated environmental obscurants.


Title: Development of Wide Angle Fovea Lens for High-Definition Imager Over 3 Mega Pixels
Key Words: image sensors  lenses  photodetectors  robot vision  stereo image processing  visual perception  WAF lens  wide angle fovea lens  high-definition imager  autonomous robot  vehicle supersensing vision system  robotic vision  field of view  FOV  high-resolution photosensitive imaging chip  stereo vision system  optical performance  aspherical surface  projection testing  Lenses  Prototypes  Spatial resolution  Cameras  Robots  Optical imaging 
Abstract: This paper presents a high-quality wide-angle fovea lens, i.e., the WAF lens, for the autonomous robot's and vehicle's super-sensing vision system. The WAF lens is well-known in the field of robotic vision with respect to its unique design concept, biologically-inspired from a visual system of the primates. The WAF lens achieves the following two conflicting properties in imaging simultaneously: (1) wide field of view (FOV) and (2) high magnification factor (although only the central FOV achieves it partially). In this paper, the authors designs the WAF lens for the high-resolution photosensitive imaging chip more than 3M pixels. For this design, we decide the following targets on the assumption of applying this WAF lens for the stereo vision system: (1) The WAF lens can measure a very far distance over 100m ahead from the imager accurately. (2) The WAF lens can observe approximately 100-degree wide FOV on the same time. We produce a prototype of this WAF lens with much higher optical performance than our previous developments. The compound system of the prototype includes four aspherical surfaces in its front part to project enough bright images so that the WAF lens is available not only at daytime but also in dark situations at night. The authors experiment and demonstrate the projection tests using the prototype, and discuss about the results as the inspection of this challenging development.


Title: Vision-Based State Estimation and Trajectory Tracking Control of Car-Like Mobile Robots with Wheel Skidding and Slipping
Key Words: automobiles  control system synthesis  estimation theory  Lyapunov methods  mobile robots  motion control  robot vision  stability  state estimation  trajectory control  wheels  car-like mobile robots  wheel slipping  Lyapunov method  system stability  visual estimation algorithm  vision-based approach  wheel skidding  trajectory tracking control  vision-based state estimation  Mobile robots  Wheels  Perturbation methods  Visualization  Estimation  Trajectory tracking 
Abstract: Most existing trajectory tracking controllers are based on non-skidding and non-slipping assumptions, also assume that full states are accessible, which is unrealistic for real-world applications due to tire-road interaction. This paper presents a novel vision-based approach to achieve high performance tracking control of a Car-Like Mobile Robot (CLMR) with wheel skidding and slippage. A visual estimation algorithm is proposed to provide reliable position, velocity, skidding and slipping information to close the control loop. The stability of the proposed system can be guaranteed by Lyapunov method since the position tracking error and the estimation error converge to zero simultaneously. Simulation is made to validate the effectiveness of the developed controller in the presence of skidding and slipping with online visual estimator.


Title: LiDAR-Based Object Tracking and Shape Estimation Using Polylines and Free-Space Information
Key Words: image reconstruction  image segmentation  object detection  object tracking  optical radar  traffic engineering computing  tracking framework targets  simultaneous estimation  free-space information  accurate dynamic estimates  consistent shape reconstructions  polylines  reliable object perception  automated driving  precise contour measurements  object geometry  bounding boxes  public traffic  box assumption  object contours  object poses  2D polylines  tracking systems  Shape  Estimation  Laser modes  Measurement by laser beam  Radar tracking  Geometry  Shape measurement 
Abstract: Reliable object perception is a vital requirement for automated driving. Despite the availability of precise contour measurements, most state-of-the-art tracking systems still represent object geometry as bounding boxes. However, there are objects operating in public traffic for which the box assumption is highly inappropriate. We therefore propose to represent object contours using 2D polylines. Taking into account the mutual dependence of object poses and shape, our tracking framework targets at a simultaneous estimation of both states. Moreover, we propose to augment scan segments with free-space information at their boundaries and show how this knowledge can be incorporated into the tracking framework and beyond. Evaluation with real scan data shows that our method produces accurate dynamic estimates and consistent shape reconstructions.


Title: Search-Based Optimal Motion Planning for Automated Driving
Key Words: mobile robots  optimisation  path planning  road vehicles  search problems  trajectory control  automated driving  fast motion planning  robust motion planning  real-time computation  urban conditions  convenient geometrical representation  search space  driving constraints  classical path planning approach  exact cost-to-go map  optimal motion trajectory  time horizons  fast driving conditions  slow driving conditions  search-based optimal motion planning  Planning  Vehicle dynamics  Trajectory  Dynamics  Roads  Search problems  Automation  motion planning  automated driving  lane change  multi-lane driving  traffic lights  A* search  MPC 
Abstract: This paper presents a framework for fast and robust motion planning designed to facilitate automated driving. The framework allows for real-time computation even for horizons of several hundred meters and thus enabling automated driving in urban conditions. This is achieved through several features. Firstly, a convenient geometrical representation of both the search space and driving constraints enables the use of classical path planning approach. Thus, a wide variety of constraints can be tackled simultaneously (other vehicles, traffic lights, etc.). Secondly, an exact cost-to-go map, obtained by solving a relaxed problem, is then used by A*-based algorithm with model predictive flavour in order to compute the optimal motion trajectory. The algorithm takes into account both distance and time horizons. The approach is validated within a simulation study with realistic traffic scenarios. We demonstrate the capability of the algorithm to devise plans both in fast and slow driving conditions, even when full stop is required.


Title: Laser Map Aided Visual Inertial Localization in Changing Environment
Key Words: cameras  geometry  optical radar  optimisation  robot vision  SLAM (robots)  map optimization  changing environment  bi-directional tasks  LiDAR-built map  online visual inertial odometry system  laser map aided visual inertial localization  geometry information  crossmodal data association  multisession laser  Visualization  Lasers  Bundle adjustment  Laser radar  Robots  Cameras 
Abstract: Long-term visual localization in outdoor environment is a challenging problem, especially faced with the cross-seasonal, bi-directional tasks and changing environment. In this paper we propose a novel visual inertial localization framework that localizes against the LiDAR-built map. Based on the geometry information of the laser map, a hybrid bundle adjustment framework is proposed, which estimates the poses of the cameras with respect to the prior laser map as well as optimizes the state variables of the online visual inertial odometry system simultaneously. For more accurate crossmodal data association, the laser map is optimized using multisession laser and visual data to extract the salient and stable subset for visual localization. To validate the efficiency of the proposed method, we collect data in south part of our campus in different seasons, along the same and opposite-direction route. In all sessions of localization data, our proposed method gives satisfactory results, and shows the superiority of the hybrid bundle adjustment and map optimization1.


Title: Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map
Key Words: feature extraction  optical radar  robot vision  SLAM (robots)  stereo image processing  simultaneous localization and mapping  scan context performance  Light Detection and Ranging scans  visual scenes  two-phase search algorithm  3D LiDAR scans  loop-detection invariant  nonhistogram-based global descriptor  global localization  diverse sensors  dense 3D maps  structural information  diverse feature detectors  3D point cloud map  place recognition  Three-dimensional displays  Sensors  Laser radar  Histograms  Shape  Visualization  Encoding 
Abstract: Compared to diverse feature detectors and descriptors used for visual scenes, describing a place using structural information is relatively less reported. Recent advances in simultaneous localization and mapping (SLAM) provides dense 3D maps of the environment and the localization is proposed by diverse sensors. Toward the global localization based on the structural information, we propose Scan Context, a non-histogram-based global descriptor from 3D Light Detection and Ranging (LiDAR) scans. Unlike previously reported methods, the proposed approach directly records a 3D structure of a visible space from a sensor and does not rely on a histogram or on prior training. In addition, this approach proposes the use of a similarity score to calculate the distance between two scan contexts and also a two-phase search algorithm to efficiently detect a loop. Scan context and its search algorithm make loop-detection invariant to LiDAR viewpoint changes so that loops can be detected in places such as reverse revisit and corner. Scan context performance has been evaluated via various benchmark datasets of 3D LiDAR scans, and the proposed method shows a sufficiently improved performance.


Title: Determining Effective Swarm Sizes for Multi-Job Type Missions
Key Words: multi-agent systems  multi-robot systems  optimisation  particle swarm optimisation  queueing theory  sensitivity analysis  vehicle routing  sensitivity analysis  M/M/k/k queuing model  swarm search and service mission  SSS mission  swarm sizes  DVR  dynamic vehicle routing  multijob type missions  multiagent framework  balancing vehicle allocation  human operators  Robot sensing systems  Routing  Time factors  Planning  Task analysis 
Abstract: Swarm search and service (SSS) missions require large swarms to simultaneously search an area while servicing jobs as they are encountered. Jobs must be immediately serviced and can be one of several different job types - each requiring a different service time and number of vehicles to complete its service successfully. After jobs are serviced, vehicles are returned to the swarm and become available for reallocation. As part of SSS mission planning, human operators must determine the number of vehicles needed to achieve this balance. The complexities associated with balancing vehicle allocation to multiple as yet unknown tasks with returning vehicles makes this extremely difficult for humans. Previous work assumes that all system jobs are known ahead of time or that vehicles move independently of each other in a multi-agent framework. We present a dynamic vehicle routing (DVR) framework whose policies optimally allocate vehicles as jobs arrive. By incorporating time constraints into the DVR framework, an M/M/k/k queuing model can be used to evaluate overall steady state system performance for a given swarm size. Using these estimates, operators can rapidly compare system performance across different configurations, leading to more effective choices for swarm size. A sensitivity analysis is performed and its results are compared with the model, illustrating the appropriateness of our method to problems of plausible scale and complexity.


Title: Development and validation of MRI compatible pediatric surgical robot with modular tooling for bone biopsy
Key Words: biomedical MRI  bone  medical robotics  paediatrics  phantoms  surgery  tumours  PSR-BBT  cortical bone phantoms  cancellous bone phantoms  MRI testing  T1-FFE  T2-FFE  MR-guided robotic surgery  modular Tooling  magnetic resonance imaging  MR-compatible tools  surgical accuracy  Pediatric Surgery Robot platform  modular tool interface  Bone Biopsy Tooling  modified titanium bone biopsy needle  joint Cartesian level control  MRI compatible pediatric surgical robot  lesion  tumor  5-DOF robot  Philips Achieva 3.0T MRI bore  surgical preplanning  control interface  Cartesian level control  axial force  signal-to-noise ratio variation  geometric distortion  magnetic flux density 3 T  Biopsy  Bones  Magnetic resonance imaging  Robots  Surgery  Signal to noise ratio  Tools 
Abstract: In clinical practice, magnetic resonance imaging (MRI) is used to locate a lesion/tumor for bone biopsy in children. However, there is a lack of MR-compatible tools that can be used simultaneously during imaging and biopsy while maintaining surgical accuracy and safety. The Pediatric Surgery Robot (PSR) platform is a 5-DOF robot with a modular tool interface. For the case of bone biopsy, a Bone Biopsy Tooling (BBT) is attached. It is designed to fit within a Philips Achieva 3.0T MRI bore and carry a modified titanium bone biopsy needle. A surgical pre-planning and control interface has been developed for joint and Cartesian level control. The PSR-BBT has demonstrated 1.65 +/- 1.77 mm accuracy in Cartesian control in free space. The PSR-BBT can generate 12.46 +/- 0.32 N of axial force while drilling at a speed of 30 rpm, which is sufficient for cortical and cancellous bone phantoms. Under MRI testing (T1-FFE, T1-SE, T2-FFE and T2-TSE scans), the system demonstrated less than 33% signal-to-noise ratio variation while drilling and a 0.46% geometric distortion while powered on without significantly impacting MRI guidance in situ. These results show that the PSR-BBT can allow the user to simultaneously image and perform the biopsy and presents the PSR as a viable platform for MR-guided robotic surgery.


Title: Rolling-Joint Design Optimization for Tendon Driven Snake-Like Surgical Robots
Key Words: manipulators  medical robotics  surgery  tendon driven snake-like surgical robots  intra-luminal procedures  flexibility  serial rolling-joints  base architecture  joint angle range  tendons  rolling-joint design optimization  optimized joints  Tendons  Tools  Navigation  Robot sensing systems  Mathematical model  Three-dimensional displays 
Abstract: The use of snake-like robots for surgery is a popular choice for intra-luminal procedures. In practice, the requirements for strength, flexibility and accuracy are difficult to be satisfied simultaneously. This paper presents a computational approach for optimizing the design of a snake-like robot using serial rolling-joints and tendons as the base architecture. The method optimizes the design in terms of joint angle range and tendon placement to prevent the tendons and joints from colliding during bending motion. The resulting optimized joints were manufactured using 3D printing. The robot was characterized in terms of workspace, dexterity, precision and manipulation forces. The results show a repeatability as low as 0.9mm and manipulation forces of up to 5.6N.


Title: Towards an Automatic Spasticity Assessment by Means of Collaborative Robots
Key Words: biomechanics  brain  medical disorders  medical robotics  muscle  neurophysiology  patient rehabilitation  shear modulus  viscoelasticity  exaggerated stretch reflexes  upper motor neuron syndrome  collaborative robots  noninvasive biomechanical modelling  automatic spasticity assessment  muscle control disorder  muscle tone  upper limb joints  patient rehabilitation  7-DOF Rosen kinematics  nonlinear state  Hills force-velocity relation  rigidity  viscoelasticity  extensibility  thixotropy  passive movement response  Collaboration  Biological system modeling  Muscles  Brain modeling  Biomechanics  Intelligent robots  Collaborative robotics  Movement capture  Rehabilitation  Spasticity  Upper limb modelling 
Abstract: Summary form only given. Robotics can play a significant role in the rehabilitation of patients with spasticity by improving their early diagnosis and reducing the costs associated with care. Spasticity is a muscle control disorder characterized by an increase in muscle tone with exaggerated stretch reflexes, as one component of the upper motor neuron syndrome. Furthermore, spasticity is present in other pathologies, such as cerebral palsy, spina bifida, brain stroke among others. This video shows the ongoing research on developing a platform for the modelling and the assessment of spasticity using collaborative robots as clinical tool. Our aim is to develop methods for non-invasive biomechanical modelling of upper limbs joints using 7-DOF Rosen Kinematics [1], mixed with a non-linear state of Hills force-velocity relation [2], improved by introducing new parameters such as rigidity, viscoelasticity, extensibility and thixotropy. After a learning phase performed by the therapist, the robot replicates the trajectories required to perform the assessment. The video also describes the detailed analysis of passive movement response (force/torque and position/velocity)of the limb. These parameters will be used to determine the degree of spasticity of patients in a fast and objective manner, while simultaneously developing new clinical scales, such as a modified version of Ashworth [3].


Title: PAMPC: Perception-Aware Model Predictive Control for Quadrotors
Key Words: autonomous aerial vehicles  helicopters  mobile robots  nonlinear programming  path planning  predictive control  robot vision  PAMPC  lighting conditions  visual-inertial odometry pipeline  low-power ARM computer  nonlinear optimization problem  action objective  numerical optimization  perception-aware model predictive control framework  model-based optimization framework  perception objective  quadrotor  motion planning  Cameras  Trajectory  Optimization  Robot vision systems  Predictive control  Planning 
Abstract: We present the first perception-aware model predictive control framework for quadrotors that unifies control and planning with respect to action and perception objectives. Our framework leverages numerical optimization to compute trajectories that satisfy the system dynamics and require control inputs within the limits of the platform. Simultaneously, it optimizes perception objectives for robust and reliable sensing by maximizing the visibility of a point of interest and minimizing its velocity in the image plane. Considering both perception and action objectives for motion planning and control is challenging due to the possible conflicts arising from their respective requirements. For example, for a quadrotor to track a reference trajectory, it needs to rotate to align its thrust with the direction of the desired acceleration. However, the perception objective might require to minimize such rotation to maximize the visibility of a point of interest. A model-based optimization framework, able to consider both perception and action objectives and couple them through the system dynamics, is therefore necessary. Our perception-aware model predictive control framework works in a receding-horizon fashion by iteratively solving a non-linear optimization problem. It is capable of running in real-time, fully onboard our lightweight, small-scale quadrotor using a low-power ARM computer, together with a visual-inertial odometry pipeline. We validate our approach in experiments demonstrating (I) the conflict between perception and action objectives, and (II) improved behavior in extremely challenging lighting conditions.


Title: Learning to Fly by MySelf: A Self-Supervised CNN-Based Approach for Autonomous Navigation
Key Words: autonomous aerial vehicles  collision avoidance  convolutional neural nets  feature extraction  indoor navigation  learning (artificial intelligence)  learning systems  mobile robots  motion control  neurocontrollers  regression analysis  robot vision  sensor fusion  velocity control  indoor flights  unmanned aerial vehicles  civilian applications  indoor-flight dataset  agent distance-to-collision prediction  drone safe deployment  on-board monocular camera  external sensors  spatio-temporal feature extraction  static appearance information  motion information  robot distance estimation  linear velocity  navigation policy learning  real-distance labels  raw visual input  regression CNN  real-time obstacle avoidance  indoor robot navigation  autonomous navigation methods  UAV  self-supervised CNN-based approach  navigation policy  Robots  Navigation  Sensors  Drones  Cameras  Task analysis  Trajectory 
Abstract: Nowadays, Unmanned Aerial Vehicles (UAVs)are becoming increasingly popular facilitated by their extensive availability. Autonomous navigation methods can act as an enabler for the safe deployment of drones on a wide range of real-world civilian applications. In this work, we introduce a self-supervised CNN-based approach for indoor robot navigation. Our method addresses the problem of real-time obstacle avoidance, by employing a regression CNN that predicts the agent's distance-to-collision in view of the raw visual input of its on-board monocular camera. The proposed CNN is trained on our custom indoor-flight dataset which is collected and annotated with real-distance labels, in a self-supervised manner using external sensors mounted on an UAV. By simultaneously processing the current and previous input frame, the proposed CNN extracts spatio-temporal features that encapsulate both static appearance and motion information to estimate the robot's distance to its closest obstacle towards multiple directions. These predictions are used to modulate the yaw and linear velocity of the UAV, in order to navigate autonomously and avoid collisions. Experimental evaluation demonstrates that the proposed approach learns a navigation policy that achieves high accuracy on real-world indoor flights, outperforming previously proposed methods from the literature.


Title: Hands and Faces, Fast: Mono-Camera User Detection Robust Enough to Directly Control a UAV in Flight
Key Words: autonomous aerial vehicles  cameras  control engineering computing  convolutional neural nets  face recognition  feature extraction  gesture recognition  human-robot interaction  image colour analysis  image segmentation  object detection  robust control  video signal processing  YOLOv2 deep convolutional neural network  hand-and-face detector  gestural human-UAV interface  robust control  hand-labelled videos  face-engagement  robust sensor front-end  gray-scale images  robust real-time system  mono-camera user detection  human-robot interaction  human-UAV interaction experiments  Detectors  Feature extraction  Proposals  Training  Object detection  Face detection  Cameras 
Abstract: We present a robust real-time system for simultaneous detection of hands and faces in RGB and gray-scale images, and a novel dataset used for training. Our goal is to provide a robust sensor front-end suitable for real-time human-robot interaction using face-engagement and gestures. Using hand-labelled videos obtained from real human-UAV interaction experiments, we re-trained the YOLOv2 Deep Convolutional Neural Network to detect only hands and faces. This model was then used to automatically label several much larger third-party datasets. After manual correction of these results, we modified and re-trained the model on all this labelled data. We obtain qualitatively good detection results at 60Hz on a commodity GPU: our simultaneous hand-and-face detector gives state of the art accuracy and speed in a hand detection benchmark and competitive results in a face detection benchmark. To demonstrate its effectiveness for human-robot interaction we describe its use as the input to a simple but practical gestural human-UAV interface for entertainment or industrial applications. All software, training and test data are freely available.


Title: Angle-Encoded Swarm Optimization for UAV Formation Path Planning
Key Words: autonomous aerial vehicles  collision avoidance  mobile robots  multi-robot systems  particle swarm optimisation  angle-encoded particle swarm optimization  3DR solo drones  mission planner  Internet-of- Things  UAV formation path planning  triangular formation maintenance  swarm convergence  multiple-objective optimisation algorithm  unmanned aerial vehicles  feasible path planning technique  Trajectory  Collision avoidance  Unmanned aerial vehicles  Task analysis  Cost function  Shape  Quadcopter  θ-PSO  path planning  loT  triangular formation  collision avoidance 
Abstract: This paper presents a novel and feasible path planning technique for a group of unmanned aerial vehicles (DAVs) conducting surface inspection of infrastructure. The ultimate goal is to minimise the travel distance of DAVs while simultaneously avoid obstacles, and maintain altitude constraints as well as the shape of the UAV formation. A multiple-objective optimisation algorithm, called the Angle-encoded Particle Swarm Optimization (θ- PSO) algorithm, is proposed to accelerate the swarm convergence with angular velocity and position being used for the location of particles. The whole formation is modelled as a virtual rigid body and controlled to maintain a desired geometric shape among the paths created while the centroid of the group follows a pre-determined trajectory. Based on the testbed of 3DR Solo drones equipped with a proprietary Mission Planner, and the Internet-of- Things (loT) for multi-directional transmission and reception of data between the DAV s, extensive experiments have been conducted for triangular formation maintenance along a monorail bridge. The results obtained confirm the feasibility and effectiveness of the proposed approach.


Title: An Integrated Localization-Navigation Scheme for Distance-Based Docking of UAVs
Key Words: adaptive estimation  autonomous aerial vehicles  convergence  image sequences  invariance  mobile robots  navigation  path planning  position control  single landmark  unmanned aerial vehicles  GPS-less environment  optical flow sensors  ultra-wideband ranging sensors  discrete-time LaSalle invariance principle  UAV  distance-based docking problem  integrated localization-navigation scheme  asymptotic docking  delicate control scheme  relative position  nonlinear adaptive estimation scheme  bounded velocity  discrete-time integrators  navigation tasks  relative localization  integrated estimation-control scheme  arbitrarily unknown position  Navigation  Convergence  Distance measurement  Adaptive estimation  Estimation  Task analysis  Optical sensors 
Abstract: In this paper we study the distance-based docking problem of unmanned aerial vehicles (UAVs) by using a single landmark placed at an arbitrarily unknown position. To solve the problem, we propose an integrated estimation-control scheme to simultaneously achieve the relative localization and navigation tasks for discrete-time integrators under bounded velocity: a nonlinear adaptive estimation scheme to estimate the relative position to the landmark, and a delicate control scheme to ensure both the convergence of the estimation and the asymptotic docking at the given landmark. A rigorous proof of convergence is provided by invoking the discrete-time LaSalle's invariance principle, and we also validate our theoretical findings on quadcopters equipped with ultra-wideband ranging sensors and optical flow sensors in a GPS-less environment.


Title: SCALAR - Simultaneous Calibration of 2D Laser and Robot's Kinematic Parameters Using Three Planar Constraints
Key Words: calibration  industrial robots  laser ranging  position control  robot kinematics  calibration approaches  calibration parameters  geometric planar constraints  2D Laser Range Finder  6-DoF robot  calibration method  laser tracker  expensive external measurement system  calibrations  robot accuracy  industrial robots  kinematic parameters  simultaneous calibration  SCALAR  robot system  Robot kinematics  Calibration  Cameras  Measurement by laser beam  Kinematics  Robot vision systems 
Abstract: Industrial robots are increasingly used in various applications where the robot accuracy becomes very important, hence calibrations of the robot's kinematic parameters and the measurement system's extrinsic parameters are required. However, the existing calibration approaches are either too cumbersome or require another expensive external measurement system such as laser tracker or measurement spinarm. In this paper, we propose SCALAR, a calibration method to simultaneously improve the kinematic parameters of a 6-DoF robot and the extrinsic parameters of a 2D Laser Range Finder (LRF) that is attached to the robot. Three flat planes are placed around the robot, and for each plane the robot moves to several poses such that the LRF's ray intersect the respective plane. Geometric planar constraints are then used to optimize the calibration parameters using Levenberg-Marquardt nonlinear optimization algorithm. We demonstrate through simulations that SCALAR can reduce the average position and orientation errors of the robot system from 14.6 mm and 4.05° to 0.09 mm and 0.02°.


Title: Any-Time Trajectory Planning for Safe Emergency Landing
Key Words: aerospace components  aerospace engineering  aircraft control  aircraft landing guidance  path planning  trajectory control  landing site selection  safest emergency landing trajectory  multiple landing sites  any-time property  time trajectory planning  safe emergency landing  critical situation  human pilots  landing trajectories  aircraft  Trajectory  Aircraft  Planning  Turning  Drag  Atmospheric modeling  Force 
Abstract: Loss of thrust is a critical situation for human pilots of fixed-wing aircraft which force them to select a landing site in the nearby range and perform an emergency landing. The time for the landing site selection is limited by the actual altitude of the aircraft, and it may be fatal if the correct decision is not chosen fast enough. Therefore, we propose a novel RRT* -based planning algorithm for finding the safest emergency landing trajectory towards a given set of possible landing sites. Multiple landing sites are evaluated simultaneously during the flight even before any mechanical issue occurs, and the roadmap of possible landing trajectories is updated permanently. Thus, the proposed algorithm has the any-time property and provides the best emergency landing trajectory almost instantly.


Title: Stereo Camera Localization in 3D LiDAR Maps
Key Words: cameras  Global Positioning System  image matching  image reconstruction  mobile robots  optical radar  pose estimation  robot vision  SLAM (robots)  stereo image processing  stereo disparity map  average localization error  stereo camera localization  Global Positioning System  3D LiDAR maps  simultaneous localization and mapping techniques  SLAM techniques  3D light detection and ranging sensors  visual positioning algorithm  GPS signal  visual tracking  six degree of freedom  DOF  camera pose estimation  KITTI dataset  Cameras  Three-dimensional displays  Laser radar  Simultaneous localization and mapping  Visualization  Global Positioning System 
Abstract: As simultaneous localization and mapping (SLAM) techniques have flourished with the advent of 3D Light Detection and Ranging (LiDAR) sensors, accurate 3D maps are readily available. Many researchers turn their attention to localization in a previously acquired 3D map. In this paper, we propose a novel and lightweight camera-only visual positioning algorithm that involves localization within prior 3D LiDAR maps. We aim to achieve the consumer level global positioning system (GPS) accuracy using vision within the urban environment, where GPS signal is unreliable. Via exploiting a stereo camera, depth from the stereo disparity map is matched with 3D LiDAR maps. A full six degree of freedom (DOF) camera pose is estimated via minimizing depth residual. Powered by visual tracking that provides a good initial guess for the localization, the proposed depth residual is successfully applied for camera pose estimation. Our method runs online, as the average localization error is comparable to ones resulting from state-of-the-art approaches. We validate the proposed method as a stand-alone localizer using KITTI dataset and as a module in the SLAM framework using our own dataset.


Title: Modelling an Actuated Large Deformation Soft Continuum Robot Surface Undergoing External Forces Using a Lumped-Mass Approacb* Research supported by UK Engineering and Physical Sciences Research Council (EPSRC).
Key Words: compliant mechanisms  continuum mechanics  finite element analysis  manipulator dynamics  shear modulus  large deformation continuum surfaces  soft continuum robotic arms  3D integrated surface-arm model  lumped-mass methodology  soft robotics  Mathematical model  Robots  Load modeling  Deformable models  Strain  Actuators  Springs 
Abstract: Precise actuation of continuum surfaces in combination with continuum robotic arms that undergo large deformation is of high interest in soft robotics but of limited model-based study to date. This work develops this area towards enabling the robust design and control of large deformation continuum surfaces (LDCS) across multiple industrial applications in the healthcare, aerospace, manufacturing, and automotive domains. It introduces an actuation based dynamic model of LDCSs to accurately determine their deflection due to application of concentrated external forces while maintaining many physical characteristics and constraints on actuation elements and surface structure such as gravity, inertia, damping, elasticity, and interactive forces between actuators and LDCS. Using the lumped-mass methodology, a 3D integrated surface-arm model is developed, simulated and then validated experimentally where a pair of parallel arms are attached to the surface to actuate and deform it. The surface is then simultaneously subjected to a concentrated constant external force at its top center between the two arms. Comparing measured displacements between the experimental and modelling results over actuation time yielded the maximum error is less than 1% of the length of the surface's side at its final deflected profile despite the limited number of nodes (masses) used in the LDCS model while it is exposed to a significant external force.


Title: A Novel Monocular-Based Navigation Approach for UAV Autonomous Transmission-Line Inspection
Key Words: autonomous aerial vehicles  control engineering computing  image registration  inspection  mobile robots  neural nets  object detection  path planning  poles and towers  power overhead lines  robot vision  UAV autonomous navigation approach  pan-tilt monocular-based navigation scheme  neural network  homography matrix  distance variation  point set registration model  tower detection  overhead transmission lines  UAV autonomous transmission-line inspection  Poles and towers  Inspection  Navigation  Power transmission lines  Kernel  Cameras  Safety 
Abstract: This paper proposes a unique and robust UAV autonomous navigation approach along one side of overhead transmission lines for inspection. To this end, we establish a perspective model and develop a novel Pan/Tilt monocular-based navigation scheme. Simultaneously, the following three key issues are addressed. First, to locate the effective landmark - transmission tower timely and reliably, we customize a neural network for tower detection and combine it with a fast and smooth tracking. Second, to provide UAV with a robust and precise heading, we detect the transmission lines and compute and optimize their vanishing point. Third, to keep a safe distance from transmission lines, we optimize a homography matrix to restore the parallel nature of transmission lines and perceive the distance variation by a point set registration model. Finally, by the designed UAV platform, we test the whole system in a real-world transmission-line inspection scenario under different weather condition and achieve an encouraging result. Our approach provides great flexibility for refined inspection and effectively improves inspection safety.


Title: Energy-Efficient Trajectory Generation for a Hexarotor with Dual- Tilting Propellers
Key Words: autonomous aerial vehicles  matrix algebra  optimal control  propellers  trajectory control  hexarotor  maneuverability  control allocation matrix  brushless motors  angular accelerations  optimal control problem  underactuation degree  dual- tilting propellers  energy-efficient trajectory generation  Propellers  Trajectory  Brushless motors  Batteries  Silicon  Force  Resource management 
Abstract: In this paper, we consider a non-conventional hexarotor whose propellers can be simultaneously tilted about two orthogonal axes: in this way, its underactuation degree can be easily adapted to the task at hand. For a given tilt profile, the minimum-energy trajectory between two prescribed boundary states is explicitly determined by solving an optimal control problem with respect to the angular accelerations of the six brushless motors. We also perform, for the first time, a systematic study of the singularities of the control allocation matrix of the hexarotor, showing the presence of subtle singular configurations that should be carefully avoided in the design phase. Numerical experiments conducted with the FAST-Hex platform illustrate the theory and delineate the pros and cons of dual-tilting paradigm in terms of maneuverability and energy efficiency.


Title: Appearance-Based Along-Route Localization for Planetary Missions
Key Words: image matching  image registration  image sequences  mobile robots  object recognition  planetary rovers  robot vision  SLAM (robots)  image preprocessing steps  recognition framework SeqSLAM  appearance-based along-route localization algorithm  planetary missions  direct sequence-based approach  Moon-analogue mission  planetary rover  image similarity metrics wrt  translational viewpoint differences  rotational viewpoint differences  route traversal conditions  matching locations  flexible mechanism  frame matches  homing mechanism  autonomous navigation  real-time localization  individual frames  image sequences  robust place recognition  Navigation  Lighting  Cameras  Moon  Simultaneous localization and mapping  Visualization  mobile robotics  field robotics  place-recognition  autonomous route navigation 
Abstract: We propose an appearance-based along-route localization algorithm that relies on robust place recognition by matching image sequences instead of individual frames. Our approach extends state of the art place recognition framework SeqSLAM in several aspects to realize real-time localization along routes for autonomous navigation. First, our method is online in that we only rely on the recently observed image frames. Second, we provide a homing mechanism based on rotations computed from frame matches. And third, we use a more flexible mechanism to search for matching locations, not restricting the search to straight lines in the cost matrix as in SeqSLAM, but allowing for a wide variety of route traversal conditions such as varying velocities or rotational and translational viewpoint differences. We investigate different image preprocessing steps as well as image similarity metrics wrt. their influence on illumination and viewpoint invariance for a more robust place recognition. On a new challenging dataset, recorded in real world experiments with a planetary rover, in the course of a Moon-analogue mission on Sicily's Mount Etna, we show the feasibility of our direct, sequence-based approach to along-route localization.


Title: A Monocular Indoor Localiser Based on an Extended Kalman Filter and Edge Images from a Convolutional Neural Network
Key Words: cameras  convolutional neural nets  edge detection  image fusion  Kalman filters  mobile robots  nonlinear filters  pose estimation  robot vision  SLAM (robots)  camera location estimation  extended Kalman filter  6 DOF pose estimation  visual simultaneous localisation-and-mapping algorithms  prebuilt map  ground plane edge image extraction  motion model  unsigned distance function  indoor environment  monocular images  monocular indoor localiser  EKF framework  CNN  convolutional neural network  Image edge detection  Cameras  Robot vision systems  Feature extraction  Convolution  Image segmentation 
Abstract: The main contribution of this paper is an extended Kalman filter (EKF)based algorithm for estimating the 6 DOF pose of a camera using monocular images of an indoor environment. In contrast to popular visual simultaneous localisation and mapping algorithms, the technique proposed relies on a pre-built map represented as an unsigned distance function of the ground plane edges. Images from the camera are processed using a Convolutional Neural Network (CNN)to extract a ground plane edge image. Pixels that belong to these edges are used in the observation equation of the EKF to estimate the camera location. Use of the CNN makes it possible to extract ground plane edges under significant changes to scene illumination. The EKF framework lends itself to use of a suitable motion model, fusing information from any other sensors such as wheel encoders or inertial measurement units, if available, and rejecting spurious observations. A series of experiments are presented to demonstrate the effectiveness of the proposed technique.


Title: Probabilistic Dense Reconstruction from a Moving Camera
Key Words: cameras  image colour analysis  image reconstruction  image sequences  probability  SLAM (robots)  stereo image processing  TUM RGB-D SLAM  ICL-NUIM dataset  spatial correlations  visual scale changes  insufficient parallaxes  motion stereo  spatial stereo  single monocular camera  online dense reconstruction  probabilistic approach  moving camera  probabilistic dense reconstruction  outdoor experiments  dense 3D models  inlier probability expectations  depth estimates  probabilistic scheme  monocular depth estimation  temporal correlations  Image reconstruction  Cameras  Estimation  Visualization  Robot vision systems  Probabilistic logic  Simultaneous localization and mapping 
Abstract: This paper presents a probabilistic approach for online dense reconstruction using a single monocular camera moving through the environment. Compared to spatial stereo, depth estimation from motion stereo is challenging due to insufficient parallaxes, visual scale changes, pose errors, etc. We utilize both the spatial and temporal correlations of consecutive depth estimates to increase the robustness and accuracy of monocular depth estimation. An online, recursive, probabilistic scheme to compute depth estimates, with corresponding covariances and inlier probability expectations, is proposed in this work. We integrate the obtained depth hypotheses into dense 3D models in an uncertainty-aware way. We show the effectiveness and efficiency of our proposed approach by comparing it with state-of-the-art methods in the TUM RGB-D SLAM & ICL-NUIM dataset. Online indoor and outdoor experiments are also presented for performance demonstration.


Title: Probabilistic Learning of Torque Controllers from Kinematic and Force Constraints
Key Words: control engineering computing  force control  Gaussian distribution  learning (artificial intelligence)  manipulators  motion control  position control  torque control  kinematic constraints  task representation  task space  Gaussian distributions  7- DoF torque-controlled manipulators  joint space  torque control commands  operational configuration space  force constraints  probabilistic learning  Task analysis  Torque  Aerospace electronics  Probabilistic logic  Force  End effectors 
Abstract: When learning skills from demonstrations, one is often required to think in advance about the appropriate task representation (usually in either operational or configuration space). We here propose a probabilistic approach for simultaneously learning and synthesizing torque control commands which take into account task space, joint space and force constraints. We treat the problem by considering different torque controllers acting on the robot, whose relevance is learned probabilistically from demonstrations. This information is used to combine the controllers by exploiting the properties of Gaussian distributions, generating new torque commands that satisfy the important features of the task. We validate the approach in two experimental scenarios using 7- DoF torque-controlled manipulators, with tasks that require the consideration of different controllers to be properly executed.


Title: Simultaneous End-User Programming of Goals and Actions for Robotic Shelf Organization
Key Words: human-robot interaction  inference mechanisms  learning (artificial intelligence)  mobile robots  teaching  user interfaces  teaching strategies  end-user programming  learning task  human teachers  fetch mobile manipulator  warehouses  robotic shelf organization  online user study  goal inference approach  system implementation  grocery store shelf images  Task analysis  Programming  Manipulators  Shape  Packaging  Education 
Abstract: Arrangement of items on shelves in stores or warehouses is a tedious, repetitive task that can be feasible for robots to perform. The diversity of products that are available in stores and the different setups and preferences of each store makes pre-programming a robot for this task extremely challenging. Instead, our work argues for enabling end-users to customize the robot to their specific objects and setup at deployment time by programming it themselves. To that end, this paper contributes (i) a task representation for shelf arrangements based on a large dataset of grocery store shelf images, (ii) a method for inferring goal configurations from user inputs including demonstrations and direct parameter specifications, and (iii) a system implementation of the proposed approach that allows simultaneously learning task goals and actions. We evaluate our goal inference approach with ten different teaching strategies that combine alternative user inputs in different ways on the large dataset of grocery configurations, as well as with real human teachers through an online user study (N=32). We evaluate our full system implemented on a Fetch mobile manipulator on eight benchmark tasks that demonstrate end-to-end programming and execution of shelf arrangement tasks.


Title: Adaptive Admittance Control in Task-Priority Framework for Contact Force Control in Autonomous Underwater Floating Manipulation* This work is part of a project titled “Force/position control system to enable compliant manipulation from a floating I-AUV”, which received funding from the European Union's Horizon 2020 research and innovation programme, under the Marie Sklodowska-Curie grant agreement no. 750063.
Key Words: autonomous underwater vehicles  cooperative systems  end effectors  force control  manipulator dynamics  manipulator kinematics  manipulators  mobile robots  position control  task-priority kinematic control algorithm  custom force control strategy  impedance control  TP algorithm  inequality tasks  singular configurations  force control part  impedance concept  exerted force  stable contact  control architecture  adaptive admittance control  task-priority framework  contact force control  underwater vehicle-manipulator system  end-effector configuration  floating-base manipulation  Force control  Task analysis  Force  Impedance  Robots  Heuristic algorithms  Inspection 
Abstract: This paper presents a control architecture for an underwater vehicle-manipulator system (UVMS) to enable simultaneous tracking of end-effector configuration and contact force during floating-base manipulation. The main feature of the architecture is its combination of a task-priority (TP) kinematic control algorithm with a custom force control strategy, based on impedance (admittance) control. The TP algorithm used in the work includes recent treatment of equality and inequality tasks as well as original concepts to handle operation in singular configurations of the system. In the force control part the impedance concept is extended to allow for direct control over the value of exerted force and torque. Additional feed-forward signal is used to ensure stable contact. The performance of the control architecture is demonstrated by experiments in a test tank, with GIRONA500 I-AUV performing pipe inspection.


Title: Distributed Pressure Sensing for Enabling Self-Aware Autonomous Aerial Vehicles
Key Words: aerodynamics  aerospace components  autonomous aerial vehicles  pressure control  pressure sensors  state estimation  wind tunnels  pressure sensors  NASA Langley Research Center  distributed algorithm  wind tunnel  commercial air transportation  self-aware autonomous aerial vehicles  lattice-based subcomponents  14-foot wingspan  skin panels  flexible aerostructure  aerodynamic state estimation  modular distributed pressure sensing skin  autonomous systems  adaptable self-state estimation  robust self-state estimation  autonomous aerial transportation  pressure distribution  Robot sensing systems  Pressure sensors  Skin  Aerodynamics  NASA 
Abstract: Autonomous aerial transportation will be a fixture of future robotic societies, simultaneously requiring more stringent safety requirements and fewer resources for characterization than current commercial air transportation. More robust, adaptable, self-state estimation will be necessary to create such autonomous systems. We present a modular, scalable, distributed pressure sensing skin for aerodynamic state estimation of a large, flexible aerostructure. This skin used a network of 22 nodes that performed in situ computation and communication of data collected from 74 pressure sensors, which were embedded into the skin panels of an ultra-lightweight 14-foot wingspan made from commutable, lattice-based subcomponents, and tested at NASA Langley Research Center's 14X22 wind tunnel. The density of the pressure sensors allowed for the use of a novel distributed algorithm to generate estimates of the wing lift contribution that were more accurate than the direct integration of the pressure distribution over the wing surface.


Title: Coping with Context Change in Open-Ended Object Recognition without Explicit Context Information
Key Words: learning (artificial intelligence)  mobile robots  object recognition  object category learning  learning recognition  evaluation approaches  recognition approaches  multicontext scenarios  recognition methods  unconstrained human environments  autonomous robots  object categories  human-centric environment  explicit context information  open-ended object recognition  context change  Robots  Visualization  Protocols  Histograms  Context modeling  Three-dimensional displays  Training 
Abstract: To deploy a robot in a human-centric environment, it is important that the robot is able to continuously acquire and update object categories while working in the environment. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent or interleaved fashion. One of the main challenges in unconstrained human environments is to cope with the effects of context change. This paper presents two main contributions: (i) an approach for evaluating open-ended object category learning and recognition methods in multi-context scenarios; (ii) evaluation of different object category learning and recognition approaches regarding their ability to cope with the effects of context change. Off-line evaluation approaches such as cross-validation do not comply with the simultaneous nature of learning and recognition. A teaching protocol, supporting context change, was therefore designed and used in this work for experimental evaluation. Seven learning and recognition approaches were evaluated and compared using the protocol. The best performance, in terms of number of learned categories, was obtained with a recently proposed local variant of Latent Dirichlet Allocation (LDA), closely followed by a Bag-of-Words (BoW) approach. In terms of adaptability, i.e. coping with context change, the best result was obtained with BoW, immediately followed by the local LDA variant.


Title: Submap-Based Pose-Graph Visual SLAM: A Robust Visual Exploration and Localization System* The work in this paper is supported by the National Natural Science Foundation of China (61603103, 61673125), the Natural Science Foundation of Guangdong of China (2016A030310293), and the Major Scientific and Technological Special Project of Guangdong of China (2016B090910003).
Key Words: graph theory  mean square error methods  pose estimation  robot vision  SLAM (robots)  VSLAM algorithms  robust visual exploration  visual simultaneous localization and mapping  submap-based pose-graph visual SLAM  robust exploration  visual front-end  submap-based VSLAM system  Image edge detection  Optimization  Robustness  Visualization  Merging  Robots  Tracking  Monocular VSLAM  Submap-based Backend  Robustness 
Abstract: For VSLAM (Visual Simultaneous Localization and Mapping), localization is a challenging task, especially for some challenging situations: textureless frames, motion blur, etc. To build a robust exploration and localization system in a given space, a submap-based VSLAM system is proposed in this paper. Our system uses a submap back-end and a visual front-end. The main advantage of our system is its robustness with respect to tracking failure, a common problem in current VSLAM algorithms. The robustness of our system is compared with the state-of-the-art in terms of average tracking percentage. The precision of our system is also evaluated in terms of ATE (absolute trajectory error) RMSE (root mean square error) comparing the state-of-the-art. The ability of our system in solving the “kidnapped” problem is demonstrated. Our system can improve the robustness of visual localization in challenging situations.


Title: Learning Monocular Visual Odometry with Dense 3D Mapping from Dense 3D Flow
Key Words: distance measurement  Gaussian processes  image reconstruction  learning (artificial intelligence)  mobile robots  motion estimation  neural nets  pose estimation  robot vision  SLAM (robots)  stereo image processing  learning monocular visual odometry  monocular SLAM  simultaneous localization  neural network  dual-stream L-VO network  6DOF relative pose  bivariate Gaussian modeling  KITTI odometry  visual SLAM system  dense 2D flow  fully deep learning approach  dense 3D flow  dense 3D mapping  Three-dimensional displays  Simultaneous localization and mapping  Visual odometry  Two dimensional displays  Deep learning  Cameras  Training 
Abstract: This paper introduces a fully deep learning approach to monocular SLAM, which can perform simultaneous localization using a neural network for learning visual odometry (L-VO) and dense 3D mapping. Dense 2D flow and a depth image are generated from monocular images by sub-networks, which are then used by a 3D flow associated layer in the L-VO network to generate dense 3D flow. Given this 3D flow, the dual-stream L-VO network can then predict the 6DOF relative pose and furthermore reconstruct the vehicle trajectory. In order to learn the correlation between motion directions, the Bivariate Gaussian modeling is employed in the loss function. The L-VO network achieves an overall performance of 2.68 % for average translational error and 0.0143°/m for average rotational error on the KITTI odometry benchmark. Moreover, the learned depth is leveraged to generate a dense 3D map. As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.


Title: Unit Quaternion-Based Parameterization for Point Features in Visual Navigation
Key Words: covariance matrices  geometry  image representation  recursive estimation  SLAM (robots)  point features  visual navigation  Cartesian 3D representation  homogeneous points  error state  unit-quaternion error covariance  initial feature observations  initial error-states  unit quaternion-based representation  unit quaternion-based parameterization  initial infinite depth uncertainty  Quaternions  Cameras  Simultaneous localization and mapping  Visualization  Navigation  Convergence  Three-dimensional displays 
Abstract: In this paper, we propose to use unit quaternions to represent point features in visual navigation. Contrary to the Cartesian 3D representation, the unit quaternion can well represent features at both large and small distances from the camera without suffering from convergence problems. Contrary to inverse-depth, homogeneous points, or anchored homogeneous points, the unit quaternion has error state of minimum dimension of three. In contrast to prior representations, the proposed method does not need to approximate an initial infinite depth uncertainty. In fact, the unit-quaternion error covariance can be initialized from the initial feature observations without prior information, and the initial error-states are not only bounded, but the bound is identical for all scene geometries. To the best of our knowledge, this is the first time bearing-only recursive estimation (in covariance form) of point features has been possible without using measurements to initialize error covariance. The proposed unit quaternion-based representation is validated on numerical examples.


Title: A Bayesian Framework for Simultaneous Robot Localization and Target Detection and Engagement
Key Words: Bayes methods  image fusion  image sensors  Kalman filters  mobile robots  nonlinear filters  object detection  probability  remotely operated vehicles  robot vision  mobile robot  multistage Bayesian approaches  multistage localization approach  global coordinate frame  multistage target observation approach  target engagement  multiple sensors  Bayesian framework  simultaneous robot localization  sensors on-board  target detection  associated detection probability  extended Kalman filter  unmanned ground vehicle  Robot kinematics  Robot sensing systems  Bayes methods  Mobile robots  Uncertainty 
Abstract: This paper presents a framework for engaging a target while approaching it from a long distance, using observation from sensors on-board a mobile robot. The proposed framework consists of two multi-stage Bayesian approaches to reliably detect and accurately engage with the target under uncertainties. The multi-stage localization approach localizes the robot and the target in a global coordinate frame. Their locations are estimated sequentially when the robot is at a long distance from the target, whereas they are localized simultaneously when the target is in the close vicinity. In the multi-stage target observation approach, a level of confidence and the associated probability of detection of the sensor are defined to make the target detectable in maximal occasions. This allows the extended Kalman filter to be implemented for the target engagement. The proposed framework was implemented on an unmanned ground vehicle equipped with multiple sensors. Results show the effectiveness of the proposed framework in solving real-world problems.


Title: Improving Repeatability of Experiments by Automatic Evaluation of SLAM Algorithms
Key Words: robot vision  SLAM (robots)  SLAM algorithms  robotics  repeatability  Simultaneous Localization And Mapping  Simultaneous localization and mapping  Buildings  Measurement  Data models  Lasers 
Abstract: The development of good experimental methodologies for robotics takes often inspiration from general principles of experimental practice. Repeatability prescribes that experiments should involve several trials in order to guarantee that results are not achieved by chance, but are systematic, and statistically significant trends can be identified. In this paper, we propose an approach to improve the repeatability of experiments performed in robotics. In particular, we focus on the domain of SLAM (Simultaneous Localization And Mapping) and we introduce a system that exploits simulations to generate a large number of test data on which SLAM algorithms are automatically evaluated in order to obtain consistent results, according to the principle of repeatability.


Title: Robust Humanoid Control Using a QP Solver with Integral Gains
Key Words: humanoid robots  Lyapunov methods  quadratic programming  robot dynamics  robust control  torque control  low-frequency bounded disturbances  Lyapunov-stable torque control  dynamical model  dynamic constraints  QP solver  kinetic joint friction  robust humanoid control  torque controlled humanoid robots  multiobjective weighted tasks  optimal dynamically-feasible reference  exponential convergence  joint torque feedback  nonmodelled torque bias  quadratic programming  HRP-5P robot  Torque  Humanoid robots  Convergence  Acceleration  Torque control  Task analysis  Robust control  Torque control  Passivity  Quadratic programming  Humanoid robots 
Abstract: We propose a control framework for torque controlled humanoid robots that efficiently minimizes the tracking error in a Quadratic Programming (QP)formulated as multiobjective weighted tasks with constraints. It results in an optimal dynamically-feasible reference that can be tracked robustly, with exponential convergence, without joint torque feedback, in the presence of non modelled torque bias and low-frequency bounded disturbances. This is achieved by introducing integral gains in a Lyapunov-stable torque control, which exploit the passivity properties of the dynamical model of the robot and their effect on the dynamic constraints of the QP solver. The robustness of this framework is demonstrated in simulation by commanding our robot, the HRP-5P, to achieve simultaneously several objectives in the configuration and the Cartesian spaces, in the presence of non-modeled static and kinetic joint friction, as well as an uncertain torque scale.


Title: C-MPDM: Continuously-Parameterized Risk-Aware MPDM by Quickly Discovering Contextual Policies
Key Words: decision making  gradient methods  operations research  optimisation  continuously-parameterized risk-aware MPDM  on-line forward roll-out process  computational cost  continuous-valued parameters  iterative gradient-based algorithm  multipolicy decision making systems  social environment  promising policy parameters  contextual policies  Robots  Cost function  Real-time systems  Trajectory  Decision making  Backpropagation 
Abstract: Risk-aware Multi-Policy Decision Making (MPDM)is a powerful framework for reliable navigation in a dynamic social environment where rather than evaluating individual trajectories, a “library” of policies (reactive controllers)is evaluated by anticipating potentially dangerous future outcomes using an on-line forward roll-out process. There is a core tension in Multi-Policy Decision Making (MPDM)systems - it is desirable to add more policies to the system for flexibility in finding good policies, however, this increases computational cost. As a result, MPDM was limited to small (perhaps 5-10)discrete policies - a significant performance bottleneck. In this paper, we radically enhance the expressivity of MPDM by allowing policies to have continuous-valued parameters, while simultaneously satisfying real-time constraints by quickly discovering promising policy parameters through a novel iterative gradient-based algorithm. Our evaluation includes results from extensive simulation and real-world experiments in semi-crowded environments.


Title: HERO: Accelerating Autonomous Robotic Tasks with FPGA
Key Words: convolutional neural nets  field programmable gate arrays  mobile robots  path planning  SLAM (robots)  motion planning tasks  HERO platform  CNN inference  autonomous robotic tasks  Heterogeneous Extensible Robot Open platform  OpenCL programming  SLAM  convolutional neural network inference  FPGA acceleration  heterogeneous computing  simultaneous localization and mapping  VGG-16  ResNet-50  Field programmable gate arrays  Kernel  Acceleration  Simultaneous localization and mapping  Task analysis  Planning 
Abstract: The Heterogeneous Extensible Robot Open (HERO) platform is designed for autonomous robotic research. While bringing in the flexible computational capacities by CPU and FPGA, it addresses the challenges of heterogeneous computing by embracing OpenCL programming. We propose heterogeneous computing approaches for three fundamental robotic tasks: simultaneous localization and mapping (SLAM), motion planning and convolutional neural network (CNN) inference. With FPGA acceleration, the SLAM and motion planning tasks are performed 2-4 times faster on the HERO platform against fine-tuned software implementation. For CNN inference, it can process 20-30 images per second with the network of VGG-16 or ResNet-50. We expect the open platform and the developing experiences shared in this paper can facilitate future robotic research, especially for those compute intensive tasks of perception, movement and manipulation.


Title: The Deformable Quad-Rotor Enabled and Wasp-Pedal-Carrying Inspired Aerial Gripper
Key Words: aerodynamics  aircraft control  autonomous aerial vehicles  controllability  deformation  grippers  helicopters  mobile robots  stability  aerial gripper design  REMS  quadrotor body  quadrotor deformation  rigid elements based morphing structure  aerodynamic flow  wasp grasping behavior  stability  unmanned aerial vehicles  Grippers  Grasping  Strain  Payloads  Rotors  Manipulators  Gravity 
Abstract: The paper presents the development of a novel deformable quad-rotor enabled aerial gripper. The mechanism of our deformable quad-rotor is based on simultaneous expansion or contraction of the quad-rotor body, which is generated by controlling a rigid elements based morphing structure (REMS). Such deformation results in a highly deformable quad-rotor that can not only perform morphological adaptation in response to environmental changes and obstacles, but also improve the flight performance by contracting to facilitate the agility/maneuverability or by expanding to enhance the stability. Meanwhile, inspired by the wasp grasping behavior, such controllable expansion and contraction from the REMS ingeniously enable a new function of aerial gripper. In this paper, we start to detail the mechanism and design of the REMS based deformable quad-rotor, then present the quad-rotor deformation enabled aerial gripper design, its dynamics modeling, the grasping function and analysis. The simulation was conducted in order to graphically show the elicited aerodynamic flow situation during expansion or contraction of the quad-rotor with and without carrying payload. Experiments were further implemented to validate the grasping function of the gripper and the flight performance of the quad-rotor. Finally, two case studies on the new aerial gripper were performed. All results demonstrate the excellent performance of the deformable quad-rotor enabled aerial gripper, that is, it has the advantages of both flight maneuverability and grasping capability during performing tasks.


Title: Quotient-Space Motion Planning
Key Words: mobile robots  motion control  path planning  quotient-space motion planning  OMPL  robot  Quotient-space roadMap Planner  roadmap-based motion planning algorithm  nested quotient-space decomposition  open motion planning library  Planning  Manipulators  Runtime  Visualization  Probabilistic logic  Manifolds 
Abstract: A motion planning algorithm computes the motion of a robot by computing a path through its configuration space. To improve the runtime of motion planning algorithms, we propose to nest robots in each other, creating a nested quotient-space decomposition of the configuration space. Based on this decomposition we define a new roadmap-based motion planning algorithm called the Quotient-space roadMap Planner (QMP). The algorithm starts growing a graph on the lowest dimensional quotient space, switches to the next quotient space once a valid path has been found, and keeps updating the graphs on each quotient space simultaneously until a valid path in the configuration space has been found. We show that this algorithm is probabilistically complete and outperforms a set of state-of-the-art algorithms implemented in the open motion planning library (OMPL).


Title: Joint Stem Detection and Crop-Weed Classification for Plant-Specific Treatment in Precision Farming
Key Words: agricultural robots  agriculture  agrochemicals  control engineering computing  convolutional neural nets  crops  image classification  image segmentation  learning (artificial intelligence)  robot vision  spraying  crop production  robots  mechanical treatments  class-wise stem detection  conventional weed control  spraying process  convolutional network  farming process  agrochemicals  environmental impact  pixel-wise semantic segmentation  Agriculture  Decoding  Image segmentation  Feature extraction  Semantics  Task analysis  Robots 
Abstract: Applying agrochemicals is the default procedure for conventional weed control in crop production, but has negative impacts on the environment. Robots have the potential to treat every plant in the field individually and thus can reduce the required use of such chemicals. To achieve that, robots need the ability to identify crops and weeds in the field and must additionally select effective treatments. While certain types of weed can be treated mechanically, other types need to be treated by (selective) spraying. In this paper, we present an approach that provides the necessary information for effective plant-specific treatment. It outputs the stem location for weeds, which allows for mechanical treatments, and the covered area of the weed for selective spraying. Our approach uses an end-to-end trainable fully convolutional network that simultaneously estimates stem positions as well as the covered area of crops and weeds. It jointly learns the class-wise stem detection and the pixel-wise semantic segmentation. Experimental evaluations on different real-world datasets show that our approach is able to reliably solve this problem. Compared to state-of-the-art approaches, our approach not only substantially improves the stem detection accuracy, i.e., distinguishing crop and weed stems, but also provides an improvement in the semantic segmentation performance.


Title: π-SoC: Heterogeneous SoC Architecture for Visual Inertial SLAM Applications
Key Words: energy consumption  mobile computing  mobile robots  optimisation  SLAM (robots)  system-on-chip  visual inertial SLAM applications  autonomous vehicles  robotics  core technologies  battery-powered mobile devices  energy budget  energy consumption  energy efficiency  visual inertial SLAM workloads  60 FPS performance  heterogeneous SoC architecture  simultaneous localization and mapping  hardware accelerator  IO interface  memory hierarchy  Simultaneous localization and mapping  Feature extraction  Three-dimensional displays  Instruction sets  Power demand  Graphics processing units  Computer architecture 
Abstract: In recent years, we have observed a clear trend in the rapid rise of autonomous vehicles and robotics. One of the core technologies enabling these applications, Simultaneous Localization And Mapping (SLAM), imposes two main challenges: first, these workloads are computationally intensive and they often have real-time requirements; second, these workloads run on battery-powered mobile devices with limited energy budget. Hence, performance should be improved while simultaneously reducing energy consumption, two rather contradicting goals by conventional wisdom. Previous attempts to optimize SLAM performance and energy efficiency usually involve optimizing one function and fail to approach the problem systematically. In this paper, we first study the characteristics of visual inertial SLAM workloads on existing heterogeneous SoCs. Then based on the initial findings, we propose π-SoC, a heterogeneous SoC design that systematically optimize the IO interface, the memory hierarchy, as well as the the hardware accelerator. We implemented this system on a Xilinx Zynq UltraScale MPSoC and was able to deliver over 60 FPS performance with average power less than 5 W.


Title: Smooth Point-to-Point Trajectory Planning in $SE$ (3)with Self-Collision and Joint Constraints Avoidance
Key Words: collision avoidance  end effectors  manipulator dynamics  manipulator kinematics  smooth point-to-point trajectory planning  joint constraints avoidance  serial robotic structures  time-optimal SE(3) trajectory  robot end-effector  4th order dynamics flexible joint robots  self-collision avoidance  point-to-point trajectory planner  Trajectory  End effectors  Interpolation  Planning  Collision avoidance 
Abstract: In this paper we introduce a novel point-to-point trajectory planner for serial robotic structures that combines the ability to avoid self-collisions and to respect motion constraints, while complying with the requirement of being C4 continuous. The latter property makes our approach also suited for 4th order dynamics flexible joint robots, which gained significant practical relevance recently. In particular, we address the problem of generating a smooth, kinematically nearly time-optimal SE(3) trajectory while simultaneously avoiding potential collisions of the robot end-effector with its base as well as respecting the Cartesian unreachable states induced by the joint limits of the proximal kinematic structure.


Title: The Earth Ain't Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera
Key Words: cameras  image reconstruction  pose estimation  road vehicles  SLAM (robots)  stereo image processing  traffic engineering computing  local bundle-adjustment like procedure  3D pose  semantic cues  moving ego vehicle  shape estimation  plain roads  monocular localization demonstrations  autonomous driving systems  traffic participants  moving camera  monocular reconstruction  arbitrarily-shaped roads  monocular object localization  road plane configurations  local ground plane  local planar patches  monocular camera  Shape  Roads  Three-dimensional displays  Cameras  Image reconstruction  Automobiles  Surface reconstruction 
Abstract: Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but monocular localization demonstrations have been confined to plain roads. We demonstrate - to the best of our knowledge - the first results for monocular object localization and shape estimation on surfaces that are non-coplanar with the moving ego vehicle mounted with a monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the 3D pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands. We also demonstrate that our approach transfers from synthetic to real data, without any hyperparameter-/fine-tuning. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.


Title: Lightweight Collision Avoidance for Resource-Constrained Robots
Key Words: collision avoidance  control system synthesis  mobile robots  motion control  resource-constrained robot  controller design  lightweight collision avoidance strategy  embedded control  reference control input  dynamic environment  low level motion control  on-board sensors  vehicle  physical robots  low computational requirements  Collision avoidance  Robot sensing systems  Navigation  Vehicle dynamics 
Abstract: One of the safest and most reliable strategies for vehicle's collision avoidance is embedded control at low level to guarantee safe motion in all situations using on-board sensors. In this paper, we propose a novel lightweight collision avoidance strategy that can be implemented as a low level motion control to achieve safe motion while simultaneously tracking the robot's reference control input. This strategy is designed to be general so that it can be easily integrated with most control designs, with the primary target of resource-constrained robot swarms that act in real-time, dynamic environments. The main advantages of our approach are a very simple structure and low computational requirements. We verified the effectiveness of the proposed collision avoidance strategy through two simulated scenarios and with physical robots. We believe our design can be directly used in many areas, such as autonomous driving, intelligent transportation and planetary exploration.


Title: Wireframe Mapping for Resource-Constrained Robots
Key Words: mobile robots  path planning  simulation mapping  wireframe mapping  resource-constrained robots  wireframe representation  particle filter  sparse wireframe map structure  map representation  wireframe structure  occupancy grid map  discrete map errors  Simultaneous localization and mapping  Uncertainty  Two dimensional displays  Geometry  Navigation 
Abstract: This paper presents a novel wireframe map structure for resource-constrained robots operating in a rectilinear 2D environment. The wireframe representation compactly represents geometry, in addition to transient situations such as occlusions and boundaries of unexplored regions. We formulate a particle filter to suit this sparse wireframe map structure. Functions for calculating the likelihood of scans, merging wireframes, and resampling are developed to accommodate this map representation. The wireframe structure with the particle filter allows for severe discrete map errors to be corrected, leading to accurate maps with small storage requirements. We show in a simulation study that the algorithm attains a map of an environment with 1 % error, compared to an occupancy grid map obtained with GMapping which attained 23% error with the same storage requirements. A simulation mapping a large environment demonstrates the algorithms scalability.


