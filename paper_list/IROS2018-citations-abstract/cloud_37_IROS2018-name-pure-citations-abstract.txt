total paper: 37
Title: Efficient Long-term Mapping in Dynamic Environments
Key Words: graph theory  mobile robots  robot vision  SLAM (robots)  mapping problem  longterm SLAM datasets  graph coherency  intra-session loop closure detections  out-dated nodes  graph complexity  nonstatic entities  merging procedure  efficient ICP-based alignment  up-to-date state  2D point cloud data  local maps  graph SLAM paradigm  multiple mapping sessions  single mapping sessions  SLAM system  autonomous robots  dynamic environments  long-term robot operation  Simultaneous localization and mapping  Cloud computing  Three-dimensional displays  Two dimensional displays  Merging  Optimization 
Abstract: As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.


Title: Cloud services for robotic nurses? Assessing legal and ethical issues in the use of cloud services for healthcare robots
Key Words: cloud computing  ethical aspects  health care  legislation  medical robotics  mobile robots  security of data  cyber- aspects  data protection requirements  data security  healthcare cloud robotics  ethical issues  legal issues  robotic nurses  Cloud computing  Medical services  Robot kinematics  Robot sensing systems  Law 
Abstract: This paper explores ethical and legal implications arising from the intertwinement of cloud services, healthcare and robotics. It closes an existing gap in the literature by highlighting the distinctive ethical and legal concerns associated with the inter-dependence of the cyber- and the physical aspects of healthcare cloud robotics. The identified core concerns include uncertainties with regard to data protection requirements; distributed responsibilities for unintended harm; achievement of transparency and consent for cloud robot services especially for vulnerable robot users; secondary uses of cloud data derived from robot activities; data security; and wider social issues. The paper aims to raise awareness and stimulate reflection of the legal and ethical impacts on different stakeholders arising from the use of cloud services in healthcare robotics. We show that due to the complexity of these concerns the design and implementation of such robots in healthcare requires an interdisciplinary development and impact assessment process. In light of legal requirements and ethical responsibilities towards end-users and other stakeholders, we draw practical considerations for engineers developing cloud services for robots in healthcare.


Title: Capturing Deformations of Interacting Non-rigid Objects Using RGB-D Data
Key Words: computational geometry  finite element analysis  image colour analysis  image registration  image segmentation  image sequences  segmented point clouds  collision detection  joint registration framework  RGB-D sensor  point cloud data  elastic deformations  RGB-D data  interacting nonrigid objects  FEM elastic model  geometrical point-to-point correspondences  ICP algorithm  rigid transformations  RGB images  visual segmentation  Strain  Three-dimensional displays  Finite element analysis  Deformable models  Computational modeling  Collision avoidance  Visualization 
Abstract: This paper presents a method for tracking multiple interacting deformable objects undergoing rigid motions, elastic deformations and contacts, using image and point cloud data provided by an RGB-D sensor. A joint registration frame-work is proposed, based on physical Finite Element Method (FEM) elastic and interaction models. It first relies on a visual segmentation of the considered objects in the RGB images. The different segmented point clouds are then processed to estimate rigid transformations with on an ICP algorithm, and to determine geometrical point-to-point correspondences with the meshes. External forces resulting from these correspondences and between the current and the rigidly transformed mesh can then be derived. It provides both non-rigid and rigid data cues. A classical collision detection and response model is also integrated, giving contact forces between the objects. The deformations of the objects are estimated by solving a dynamic system balancing these external and contact forces with the internal or regularization forces computed through the FEM elastic model. This approach has been here tested on different scenarios involving two or three interacting deformable objects of various shapes, with promising results.


Title: A Series Elastic Tactile Sensing Array for Tactile Exploration of Deformable and Rigid Objects
Key Words: elasticity  manipulators  position control  tactile sensors  deformable objects  rigid objects  series elastic elements  sixteen compliant sensing elements  position-controlled robot manipulator  series elastic tactile array  contact location  tactile arrays  multiple sensing elements  vision-based sensors  robotic systems  tactile sensing arrays  tactile exploration  series elastic tactile sensing array  Magnetic sensors  Pins  Tactile sensors  Saturation magnetization 
Abstract: Tactile sensing arrays are used to detect contacts of robotic systems with the environment. They are particularly useful for scenarios in which vision-based sensors cannot be used. Thanks to the presence of multiple sensing elements, tactile arrays also provide spatial information about the contact location. In this work, we present our series elastic tactile array to enable tactile exploration for position-controlled robot manipulators. Sixteen compliant sensing elements are arranged as a 4×4 array. This allows the position-controlled robot to explore objects via palpation. Tactile sensing was accomplished by measuring the change of the magnetic field caused by neodymium magnets embedded into the series elastic elements. We demonstrate the efficacy of our sensor with two sets of experiments involving physical interaction scenarios. Firstly, we show that the sensor can be used to differentiate between rigid and deformable objects. Secondly, we show that point clouds of objects can be generated quickly with our sensor module attached to a position-controlled robot manipulator as an end-effector.


Title: Joint Ego-motion Estimation Using a Laser Scanner and a Monocular Camera Through Relative Orientation Estimation and 1-DoF ICP
Key Words: automobiles  cameras  iterative methods  laser ranging  mobile robots  motion estimation  optical scanners  pose estimation  sensor fusion  SLAM (robots)  joint ego-motion estimation  laser scanner  monocular camera  autonomous vehicles  SLAM algorithms  sensor suite  laser range finder  3D point clouds  iterative closest point problem  sensor modality  orientation estimation  autonomous cars  pose estimation  autonomous robots  1-DoF ICP  data association  Cameras  Iterative closest point algorithm  Lasers  Three-dimensional displays  Robot vision systems  Image color analysis 
Abstract: Pose estimation and mapping are key capabilities of most autonomous vehicles and thus a number of localization and SLAM algorithms have been developed in the past. Autonomous robots and cars are typically equipped with multiple sensors. Often, the sensor suite includes a camera and a laser range finder. In this paper, we consider the problem of incremental ego-motion estimation, using both, a monocular camera and a laser range finder jointly. We propose a new algorithm, that exploits the advantages of both sensors-the ability of cameras to determine orientations well and the ability of laser range finders to estimate the scale and to directly obtain 3D point clouds. Our approach estimates the 5 degrees of freedom relative orientation from image pairs through feature point correspondences and formulates the remaining scale estimation as a new variant of the iterative closest point problem with only one degree of freedom. We furthermore exploit the camera information in a new way to constrain the data association between laser point clouds. The experiments presented in this paper suggest that our approach is able to accurately estimate the ego-motion of a vehicle and that we obtain more accurate frame-to-frame alignments than with one sensor modality alone.


Title: Perception Based Locomotion System for a Humanoid Robot with Adaptive Footstep Compensation under Task Constraints
Key Words: adaptive control  collision avoidance  humanoid robots  interpolation  legged locomotion  path planning  task constraints  humanoid robot  adaptive footstep compensation  adaptive locomotion system  local error correction  perception based locomotion system  locomotion error  locomotion planning  point cloud  environmental measurements  plane estimation  space interpolation  collision avoidance  laser scans  Humanoid robots  Task analysis  Planning  Foot  Three-dimensional displays  Estimation 
Abstract: In order to accurately reach a target position while executing a task which imposes occlusion or constraints of the posture, a humanoid robot requires an adaptive locomotion system, which can comprehensively integrate localization, environmental mapping, global locomotion planning and local error correction. In this paper, we propose a method of constructing a perception based locomotion system for a humanoid robot. The major contribution of this paper is solving a problem of the locomotion error caused by the task constraints, by locally compensating footsteps and assessing the need for global footstep re-planning online based on environmental measurements. The proposed system provides an accurate and dense ground point cloud, called HeightField, using plane estimation and space interpolation, and obstacle point cloud for frequent collision avoidance by accumulating laser scans. This environmental perception enables a humanoid robot to plan footsteps globally even in the situation where the sight of the robot is limited and compensate footsteps while estimating landing state during locomotion online with the localization result. We evaluated the practicality of the proposed system by applying it to our humanoid robot carrying a heavy object in a construction site and confirmed that the proposed system contributed to improved locomotion abilities of a humanoid robot engaging in heavy-duty or dangerous tasks.


Title: Group emotion recognition strategies for entertainment robots
Key Words: affective computing  cloud computing  emotion recognition  face recognition  humanoid robots  mobile robots  face API  human perceptions  assistive robotics  emotion API  Microsoft Azure cognitive services  Waseda entertainment robots  computer science  Ekman's extended Big Six emotional model  group emotion recognition strategies  affective computing  cloud-computing based solution  facial expression analysis  Face  Emotion recognition  Entertainment industry  Cameras  Humanoid robots  Mood  humanoid robot  entertainment robot  assistive robotics  emotion recognition 
Abstract: In this paper, a system to determine the emotion of a group of people via facial expression analysis is proposed for the Waseda Entertainment Robots. General models and standard methods for emotion definition and recognition are briefly described, as well as strategies for computing the group global emotion, knowing the individual emotions of group members. This work is based on Ekman's extended “Big Six” emotional model, popular in Computer Science and Affective Computing. Emotion recognition via facial expression analysis is performed with a cloud-computing based solution, using Microsoft Azure Cognitive services. First, the performances of both the Face API to detect faces, and Emotion API, to compute emotion via face expression analysis, are tested. After that, a solution to compute the emotion of a group of people has been implemented and its performances compared to human perceptions. This work presents concepts and strategies which can be generalized for applications within the scope of assistive robotics and, more broadly, affective computing, wherever it will be necessary to determine the emotion of a group of people.


Title: Stereo Visual Odometry and Semantics based Localization of Aerial Robots in Indoor Environments
Key Words: distance measurement  image colour analysis  image segmentation  indoor environment  learning (artificial intelligence)  mobile robots  neural nets  object detection  particle filtering (numerical methods)  pose estimation  robot vision  SLAM (robots)  stereo image processing  indoor environments  particle filter localization approach  semantic information  mini-aerial robots  stereo VO algorithm  semantic measurements  pre-trained deep learning based object detector  3D point clouds  visual SLAM approach  stereo visual odometry  semantics based localization  DL  RGB spectrum  drift free pose estimation  Semantics  Three-dimensional displays  Unmanned aerial vehicles  Robots  Atmospheric measurements  Particle measurements  Prediction algorithms 
Abstract: In this paper we propose a particle filter localization approach, based on stereo visual odometry (VO) and semantic information from indoor environments, for mini-aerial robots. The prediction stage of the particle filter is performed using the 3D pose of the aerial robot estimated by the stereo VO algorithm. This predicted 3D pose is updated using inertial as well as semantic measurements. The algorithm processes semantic measurements in two phases; firstly, a pre-trained deep learning (DL) based object detector is used for real time object detections in the RGB spectrum. Secondly, from the corresponding 3D point clouds of the detected objects, we segment their dominant horizontal plane and estimate their relative position, also augmenting a prior map with new detections. The augmented map is then used in order to obtain a drift free pose estimate of the aerial robot. We validate our approach in several real flight experiments where we compare it against ground truth and a state of the art visual SLAM approach.


Title: Conceptualization of Object Compositions Using Persistent Homology
Key Words: image segmentation  learning (artificial intelligence)  object recognition  shape recognition  topology  topological shape analysis  shape commonalities  spatial topology analysis  point cloud segment constellations  description space  object segment decompositions  persistent homology  Shape  Three-dimensional displays  Visualization  Topology  Dictionaries  Prototypes  Training 
Abstract: A topological shape analysis is proposed and utilized to learn concepts that reflect shape commonalities. Our approach is two-fold: i) a spatial topology analysis of point cloud segment constellations within objects. Therein constellations are decomposed and described in an hierarchical manner - from single segments to segment groups until a single group reflects an entire object. ii) a topology analysis of the description space in which segment decompositions are exposed in. Inspired by Persistent Homology, hidden groups of shape commonalities are revealed from object segment decompositions. Experiments show that extracted persistent groups of commonalities can represent semantically meaningful shape concepts. We also show the generalization capability of the proposed approach considering samples of external datasets.


Title: CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks
Key Words: calibration  cameras  image processing  image sensors  learning (artificial intelligence)  optical radar  extrinsic calibration parameters  underlying geometric problem  photometric consistency  geometric consistency  camera calibration matrix K  LiDAR point cloud  calibration efforts  rigid body transformation  geometrically supervised deep network capable  calibration targets  calibration techniques  meaningful data  sensor rig  3D LiDAR  3D spatial transformer networks  geometrically supervised extrinsic calibration  Calibration  Three-dimensional displays  Cameras  Laser radar  Robot sensing systems  Training  Two dimensional displays 
Abstract: 3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a geometrically supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet.


Title: HMAPs - Hybrid Height- Voxel Maps for Environment Representation
Key Words: mobile robots  optical radar  path planning  robot vision  SLAM (robots)  2.5D representation  Microsoft Kinect One  SLAM approach  complex elements  Velodyne VLP-16 LiDAR  updated grid representation  complex environments  reliable method  occupied space  free space  HVoxel  height-voxel elements  3D point-clouds  mobile robot  grid-based mapping approach  environment representation  hybrid height- voxel maps  HMAP  Two dimensional displays  Three-dimensional displays  Simultaneous localization and mapping  Pipelines  Ray tracing  Planning  Indexing 
Abstract: This paper presents a hybrid 3D-like grid-based mapping approach, that we called HMAP, used as a reliable and efficient 3D representation of the environment surrounding a mobile robot. Considering 3D point-clouds as input data, the proposed mapping approach addresses the representation of height-voxel (HVoxel) elements inside the HMAP, where free and occupied space is modeled through HVoxels, resulting in a reliable method for 3D representation. The proposed method corrects some of the problems inherent to the representation of complex environments based on 2D and 2.5D representations, while keeping an updated grid representation. Additionally, we also propose a complete pipeline for SLAM based on HMAPs. Indoor and outdoor experiments were carried out to validate the proposed representation using data from a Microsoft Kinect One (indoor) and a Velodyne VLP-16 LiDAR (outdoor). The obtained results show that HMAPs can provide a more detailed view of complex elements in a scene when compared to a classic 2.5D representation. Moreover, validation of the proposed SLAM approach was carried out in an outdoor dataset with promising results, which lay a foundation for further research in the topic.


Title: Robust Generalized Point Cloud Registration with Expectation Maximization Considering Anisotropic Positional Uncertainties
Key Words: expectation-maximisation algorithm  Gaussian distribution  image registration  matrix algebra  optimisation  anisotropic positional uncertainties  E-step  correspondence probabilities  M-step  transformation matrix  constrained optimization problem  expectation conditional maximization framework  multivariate Gaussian distribution  positional error  generalized point cloud registration problem  computer-assisted surgery  medical robotics  robust generalized point cloud registration  Three-dimensional displays  Hidden Markov models  Covariance matrices  Surgery  Optimization  Mixture models  Linear programming 
Abstract: Alignment of two point clouds is an essential problem in medical robotics and computer-assisted surgery. In this paper, we first formally formulate the generalized point cloud registration problem in a probabilistic manner. Specifically, not only positional but also the orientational information are incorporated into registration. Notably, the positional error is assumed to obey a multivariate Gaussian distribution to accommodate anisotropic cases. Expectation conditional maximization framework is utilized to solve the problem. In E-step, the correspondence probabilities between points in two generalized point clouds are computed. In M -step, the constrained optimization problem with respect to the transformation matrix is re-formulated as an unconstrained one. Extensive experiments are conducted to compare the proposed algorithm with the state-of-the-art registration methods. The experimental results demonstrate the algorithm's robustness to noise and outliers, fast convergence speed.


Title: UX 1 system design - A robotic system for underwater mining exploration
Key Words: cameras  control system synthesis  innovation management  mining  mobile robots  robot vision  sonar  underwater vehicles  UX 1 system design  underwater mining exploration  UX-1 underwater mine exploration robotic system  UNEXMIN project  international innovation action  EU H2020 program  flooded underground mines  UX-1 robot prototype  recovery system  post-processing computational infrastructure  spherical robot  rotating laser line structured light systems  comprehensive mine model  robot design  UV-light  natural gamma-ray detector  multi-spectral camera  electro-conductivity  magnetic field sensors  high resolution imagery  Robot sensing systems  Sonar  Cameras  Payloads  Three-dimensional displays 
Abstract: This paper describes the UX-1 underwater mine exploration robotic system under development in the context of the UNEXMIN project. UNEXMIN is an international innovation action funded under the EU H2020 program, aiming to develop new technologies and services allowing the exploration of flooded underground mines. The system is comprised by the UX-1 robot prototype, launch and recovery system, command and control subsystem and a data management and post-processing computational infrastructure. The UX-1 robot is a small spherical robot equipped with a multibeam sonar, five digital cameras and rotating laser line structured light systems. It is capable of obtaining an accurate point cloud of the surrounding environment along with high resolution imagery. A set of mineralogy, water parameters and geophysical sensors was also developed in order to obtain a more comprehensive mine model. These comprise a multi-spectral camera, electro-conductivity, pH, magnetic field sensors, a subbottom sonar, total natural gamma-ray detector, UV-light for fluorescent observation and a water sampling unit. The design of the system is presented along with the robot design. Some preliminary results are also presented and discussed.


Title: Finding safe 3D robot grasps through efficient haptic exploration with unscented Bayesian optimization and collision penalty
Key Words: approximation theory  Bayes methods  collision avoidance  grippers  haptic interfaces  Kalman filters  mobile robots  optimisation  path planning  unscented Bayesian optimization  novel collision penalty  exploration steps  safe 3D robot grasps  efficient haptic exploration  robust grasping  accurate models  known objects  approximate models  familiar objects  partial point clouds  unknown objects  sensing inaccuracies  local exploration  grasp execution  3D haptic exploration strategy  Grasping  Three-dimensional displays  Optimization  Robot sensing systems  Bayes methods 
Abstract: Robust grasping is a major, and still unsolved, problem in robotics. Information about the 3D shape of an object can be obtained either from prior knowledge (e.g., accurate models of known objects or approximate models of familiar objects) or real-time sensing (e.g., partial point clouds of unknown objects) and can be used to identify good potential grasps. However, due to modeling and sensing inaccuracies, local exploration is often needed to refine such grasps and successfully apply them in the real world. The recently proposed unscented Bayesian optimization technique can make such exploration safer by selecting grasps that are robust to uncertainty in the input space (e.g., inaccuracies in the grasp execution). Extending our previous work on 2D optimization, in this paper we propose a 3D haptic exploration strategy that combines unscented Bayesian optimization with a novel collision penalty heuristic to find safe grasps in a very efficient way: while by augmenting the search-space to 3D we are able to find better grasps, the collision penalty heuristic allows us to do so without increasing the number of exploration steps.


Title: Kinematic Morphing Networks for Manipulation Skill Transfer
Key Words: affine transforms  image morphing  iterative methods  manipulators  motion control  neural nets  robot vision  kinematic model  robot motions  robot skill  manipulation skill transfer  kinematic morphing networks  affine transformations  map depth image observations  deep neural network  Kinematics  Three-dimensional displays  Robot sensing systems  Prototypes  Neural networks  Solid modeling 
Abstract: The transfer of a robot skill between different geometric environments is non-trivial since a wide variety of environments exists, sensor observations as well as robot motions are high-dimensional, and the environment might only be partially observed. We consider the problem of extracting a low-dimensional description of the manipulated environment in form of a kinematic model. This allows us to transfer a skill by defining a policy on a prototype model and morphing the observed environment to this prototype. A deep neural network is used to map depth image observations of the environment to morphing parameter, which include transformations and configurations of the prototype model. Using the concatenation property of affine transformations and the ability to convert point clouds to depth images allows to apply the network in an iterative manner. The network is trained on data generated in a simulator and on augmented data that is created with its own predictions. The algorithm is evaluated on different tasks, where it is shown that iterative predictions lead to a higher accuracy than one-step predictions.


Title: Model-free and learning-free grasping by Local Contact Moment matching
Key Words: dexterous manipulators  grippers  image matching  learning (artificial intelligence)  path planning  robot vision  local contact moment matching  LoCoMo metric  grasp planners  learning-based approaches  prototype grasp configurations  robust contacts  fingertip contacts  physical parameters  force-closure analysis  object surface patches  zero-moment shift features  learning-free grasping  Grasping  Robots  Measurement  Grippers  Shape  Three-dimensional displays  Training data 
Abstract: This paper addresses the problem of grasping arbitrarily shaped objects, observed as partial point-clouds, without requiring: models of the objects, physics parameters, training data, or other a-priori knowledge. A grasp metric is proposed based on Local Contact Moment (LoCoMo). LoCoMo combines zero-moment shift features, of both hand and object surface patches, to determine local similarity. This metric is then used to search for a set of feasible grasp poses with associated grasp likelihoods. LoCoMo overcomes some limitations of both classical grasp planners and learning-based approaches. Unlike force-closure analysis, LoCoMo does not require knowledge of physical parameters such as friction coefficients, and avoids assumptions about fingertip contacts, instead enabling robust contacts of large areas of hand and object surface. Unlike more recent learning-based approaches, LoCoMo does not require training data, and does not need any prototype grasp configurations to be taught by kinesthetic demonstration. We present results of real-robot experiments grasping 21 different objects, observed by a wrist-mounted depth camera. All objects are grasped successfully when presented to the robot individually. The robot also successfully clears cluttered heaps of objects by sequentially grasping and lifting objects until none remain.


Title: Octree map based on sparse point cloud and heuristic probability distribution for labeled images
Key Words: calibration  cameras  convolutional neural nets  image recognition  object recognition  octrees  probability  stereo image processing  semantic octree maps  probabilistic octree framework  single lidar scans  octree map building algorithm  labeled lidar scan  camera-lidar calibration parameters  convolutional neural network  accurate driving maneuvers  automated vehicle  urban roads  labeled images  heuristic probability distribution  sparse point cloud  Three-dimensional displays  Semantics  Laser radar  Uncertainty  Octrees  Cameras  Buildings 
Abstract: To navigate through urban roads, an automated vehicle must be able to perceive and recognize objects in a three-dimensional environment. A high level contextual understanding of the surroundings is necessary to execute accurate driving maneuvers. This paper presents a novel approach to build three dimensional semantic octree maps from lidar scans and the output of a convolutional neural network (CNN) to obtain the labels of the environment. We present a heuristic method to associate uncertainties to the labels from the images based on a combination of the labels themselves, score maps retrieved by the CNN and the raw images. These uncertainties and the camera-lidar calibration parameters for multiple cameras are considered in the projection of the labels and their uncertainties into the point cloud. Every labeled lidar scan works as an input to an octree map building algorithm that calculates and updates the label probabilities of the voxels in the map. This paper also presents a qualitative and quantitative evaluation of accuracy, analyzing projection in single lidar scans and complete maps built with our probabilistic octree framework.


Title: LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain
Key Words: embedded systems  feature extraction  image segmentation  optical radar  optimisation  pose estimation  robot vision  SLAM (robots)  SLAM framework  edge features  feature extraction  point cloud segmentation  lightweight and ground-optimized lidar odometry  real-time six degree-of-freedom pose estimation  low-power embedded system  ground plane  two-step Levenberg-Marquardt optimization method  optimization steps  ground vehicles  LeGO-LOAM  Feature extraction  Three-dimensional displays  Laser radar  Image segmentation  Pose estimation  Real-time systems  Iterative closest point algorithm 
Abstract: We propose a lightweight and ground-optimized lidar odometry and mapping method, LeGO-LOAM, for realtime six degree-of-freedom pose estimation with ground vehicles. LeGO-LOAM is lightweight, as it can achieve realtime pose estimation on a low-power embedded system. LeGO-LOAM is ground-optimized, as it leverages the presence of a ground plane in its segmentation and optimization steps. We first apply point cloud segmentation to filter out noise, and feature extraction to obtain distinctive planar and edge features. A two-step Levenberg-Marquardt optimization method then uses the planar and edge features to solve different components of the six degree-of-freedom transformation across consecutive scans. We compare the performance of LeGO-LOAM with a state-of-the-art method, LOAM, using datasets gathered from variable-terrain environments with ground vehicles, and show that LeGO-LOAM achieves similar or better accuracy with reduced computational expense. We also integrate LeGO-LOAM into a SLAM framework to eliminate the pose estimation error caused by drift, which is tested using the KITTI dataset.


Title: Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map
Key Words: feature extraction  optical radar  robot vision  SLAM (robots)  stereo image processing  simultaneous localization and mapping  scan context performance  Light Detection and Ranging scans  visual scenes  two-phase search algorithm  3D LiDAR scans  loop-detection invariant  nonhistogram-based global descriptor  global localization  diverse sensors  dense 3D maps  structural information  diverse feature detectors  3D point cloud map  place recognition  Three-dimensional displays  Sensors  Laser radar  Histograms  Shape  Visualization  Encoding 
Abstract: Compared to diverse feature detectors and descriptors used for visual scenes, describing a place using structural information is relatively less reported. Recent advances in simultaneous localization and mapping (SLAM) provides dense 3D maps of the environment and the localization is proposed by diverse sensors. Toward the global localization based on the structural information, we propose Scan Context, a non-histogram-based global descriptor from 3D Light Detection and Ranging (LiDAR) scans. Unlike previously reported methods, the proposed approach directly records a 3D structure of a visible space from a sensor and does not rely on a histogram or on prior training. In addition, this approach proposes the use of a similarity score to calculate the distance between two scan contexts and also a two-phase search algorithm to efficiently detect a loop. Scan context and its search algorithm make loop-detection invariant to LiDAR viewpoint changes so that loops can be detected in places such as reverse revisit and corner. Scan context performance has been evaluated via various benchmark datasets of 3D LiDAR scans, and the proposed method shows a sufficiently improved performance.


Title: Classification of Hanging Garments Using Learned Features Extracted from 3D Point Clouds
Key Words: clothing  computer graphics  control engineering computing  convolutional neural nets  feature extraction  image classification  manipulators  neurocontrollers  robot vision  service robots  support vector machines  3D objects  feature vector extraction  t-shirts  hanging garments classification  3D point clouds  SVM  generalized convolution operation  single global feature vector  convolutional neural network  depth maps  robotic arm  hanging state  robotic manipulation  garment category  Clothing  Three-dimensional displays  Feature extraction  Robot sensing systems  Convolution  Image reconstruction 
Abstract: The presented work deals with classification of garment categories including pants, shorts, shirts, T-shirts and towels. The knowledge of the garment category is crucial for its robotic manipulation. Our work focuses particularly on garments being held in a hanging state by a robotic arm. The input of our method is a set of depth maps taken from different viewpoints around the garment. The depths are fused into a single 3D point cloud. The cloud is fed into a convolutional neural network that transforms it into a single global feature vector. The network utilizes a generalized convolution operation defined over the local neighborhood of a point. It can deal with permutations of the input points. It was trained on a large dataset of common 3D objects. The extracted feature vector is classified with SVM trained on smaller datasets of garments. The proposed method was evaluated on publicly available data and compared to the original methods, achieving competitive performance and better generalization capability.


Title: Joint 3D Proposal Generation and Object Detection from View Aggregation
Key Words: image classification  image colour analysis  image fusion  mobile robots  neural nets  object detection  optical radar  radar detection  regression analysis  road vehicle radar  robot vision  high resolution feature maps  reliable 3D object proposals  multiple object classes  category classification  second stage detection network  AVOD  KITTI 3D object detection  autonomous vehicles  3D bounding box regression  multimodal feature fusion  RPN  region proposal network  RGB images  LIDAR point clouds  neural network architecture  autonomous driving scenarios  Aggregate View Object Detection network  joint 3D proposal generation  Three-dimensional displays  Feature extraction  Proposals  Computer architecture  Agriculture  Object detection  Two dimensional displays 
Abstract: We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is available at: https://github.com/kujason/avod.


Title: Design and Implementation of Cloud-Like Soft Drone S-Cloud
Key Words: aerodynamics  airships  attitude control  autonomous aerial vehicles  flow control  helium  prototypes  rotors (mechanical)  soft blimp part  center-pierced torus-shaped part  flow control mechanism  co-axial rotors  2-axis crossed flaps  cloud-like soft drone S-cloud  translational motion  helium gas  collision damage  altitude control  attitude control  vehicle translational movements  Newton-Euler formulation  prototypes  He  Rotors  Force  Drones  Helium  Vehicle dynamics  Buoyancy 
Abstract: This study presents a new drone, called S-CLOUD, developed for safe and long flight time. It provides 3-axial (x, y, and z)translational motion and stable hovering for more than an hour after takeoff. S-CLOUD consists of two parts; soft blimp part and driving one. The soft blimp is a center-pierced torus-shaped part filled with Helium gas. Thus, it is safe to fly near people because it is light and soft, and all its rotating parts are at the center of the vehicle, which does not get damaged on collision. The driving part is plugged into the center of the soft blimp and includes the flow control mechanism, which consists of co-axial rotors and 2-axis crossed flaps. It controls the altitude, attitude, and translational movements of the vehicle. Its dynamic and reaction features against disturbances are derived using Newton-Euler formulation, and the simulation results are discussed. Finally, a prototype of S-CLOUD is fabricated and its feasibility is experimentally validated with practical applications.


Title: Towards a Real-Time Environment Reconstruction for VR-Based Teleoperation Through Model Segmentation
Key Words: control engineering computing  image reconstruction  image segmentation  industrial manipulators  man-machine systems  mobile robots  object recognition  real-time systems  robot vision  telerobotics  virtual reality  model segmentation  autonomous mobile robot systems  human-machine interfaces  virtual reality-technologies  mixed reality-technologies  multimodal teleoperation  real-time remote control  noise-reduced visualization  object recognition  operator-supporting teleoperation  real-time feedback  industrial articulated robotic arm  real-time environment reconstruction  VR-based teleoperation  known object segmentation  point-cloud visualization  long distance UDP/IP communication  Cameras  Calibration  Solid modeling  Robot vision systems  Task analysis 
Abstract: Over the next few years, more and more autonomous mobile robot systems will find their way into modern shop floors. However, it will be necessary to provide human-machine interfaces for interventions in unexpected situations like system-deadlocks, algorithm failures or inabilities. Using virtual or mixed reality-technologies, multi-modal teleoperation offers potential for being a suitable human-machine interface. Essential challenges in this field are, among others, a real-time remote control, a time-efficient and holistic environment detection using multiple sensors, a noise-reduced visualization of sensor-data, and capabilities of object recognition. This paper summarizes research results regarding an architecture capable of a near realtime, interoperable, and operator-supporting teleoperation. The focus of this paper is on a method to efficiently process and visualize point-clouds to meet high frame rate demands of virtual reality applications. To provide near real-time feedback of the robot and its environment over large distances, the presented method is capable to segment known objects from unknown objects to reduce bandwidth requirements. The results of this paper were evaluated using a industrial articulated robotic arm for teleoperation via a long distance UDP/IP communication.


Title: Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality
Key Words: augmented reality  calibration  cameras  human-robot interaction  image registration  image sensors  inspection  mobile robots  robot vision  SLAM (robots)  stereo image processing  mixed-reality headsets  headset orientation  structure inspection  marker-free self-localization  onboard depth sensor  simple point cloud registration  camera image  inspection information  joint point cloud and image-based localization  JPIL  human-robot interaction  time 20.0 min  Three-dimensional displays  Headphones  Inspection  Cameras  Virtual reality  Solid modeling  Robot sensing systems 
Abstract: This paper introduces a method of structure inspection using mixed-reality headsets to reduce the human effort in reporting accurate inspection information such as fault locations in 3D coordinates. Prior to every inspection, the headset needs to be localized. While external pose estimation and fiducial marker based localization would require setup, maintenance, and manual calibration; marker-free self-localization can be achieved using the onboard depth sensor and camera. However, due to limited depth sensor range of portable mixed-reality headsets like Microsoft HoloLens, localization based on simple point cloud registration (sPCR) would require extensive mapping of the environment. Also, localization based on camera image would face same issues as stereo ambiguities and hence depends on viewpoint. We thus introduce a novel approach to Joint Point Cloud and Image-based Localization (JPIL) for mixed-reality headsets that uses visual cues and headset orientation to register small, partially overlapped point clouds and save significant manual labor and time in environment mapping. Our empirical results compared to sPCR show average 10 fold reduction of required overlap surface area that could potentially save on average 20 minutes per inspection. JPIL is not only restricted to inspection tasks but also can be essential in enabling intuitive human-robot interaction for spatial mapping and scene understanding in conjunction with other agents like autonomous robotic systems that are increasingly being deployed in outdoor environments for applications like structural inspection.


Title: Deeply Informed Neural Sampling for Robot Motion Planning
Key Words: collision avoidance  computational complexity  feedforward neural nets  geometry  learning (artificial intelligence)  mobile robots  sampling methods  obstacle geometry  computational complexity  configuration space  optimal path solution  hand-crafted heuristics  high-dimensional spaces  neural network-based adaptive sampler  raw point cloud data  workspace encoding  collision-free optimal paths  point-mass robot  6-link robotic manipulator  dropout-based stochastic deep feedforward neural network  DeepSMPs neural architecture  deep sampling-based motion planner  robot motion planning  deeply informed neural sampling  contractive autoencoder  rigid-body  Planning  Robots  Encoding  Three-dimensional displays  Convergence  Switched mode power supplies  Transforms 
Abstract: Sampling-based Motion Planners (SMPs) have become increasingly popular as they provide collision-free path solutions regardless of obstacle geometry in a given environment. However, their computational complexity increases significantly with the dimensionality of the motion planning problem. Adaptive sampling is one of the ways to speed up SMPs by sampling a particular region of a configuration space that is more likely to contain an optimal path solution. Although there are a wide variety of algorithms for adaptive sampling, they rely on hand-crafted heuristics; furthermore, their performance decreases significantly in high-dimensional spaces. In this paper, we present a neural network-based adaptive sampler for motion planning called Deep Sampling-based Motion Planner (DeepSMP). DeepSMP generates samples for SMPs and enhances their overall speed significantly while exhibiting efficient scalability to higher-dimensional problems. DeepSMP's neural architecture comprises of a Contractive AutoEncoder which encodes given workspaces directly from a raw point cloud data, and a Dropout-based stochastic deep feedforward neural network which takes the workspace encoding, start and goal configuration, and iteratively generates feasible samples for SMPs to compute end-to-end collision-free optimal paths. DeepSMP is not only consistently computationally efficient in all tested environments but has also shown remarkable generalization to completely unseen environments. We evaluate DeepSMP on multiple planning problems including planning of a point-mass robot, rigid-body, 6-link robotic manipulator in various 2D and 3D environments. The results show that on average our method is at least 7 times faster in point-mass and rigid-body case and about 28 times faster in 6-link robot case than the existing state-of-the-art.


Title: Light-Weight Object Detection and Decision Making via Approximate Computing in Resource-Constrained Mobile Robots
Key Words: decision making  Markov processes  mobile robots  object detection  path planning  robot vision  light-weight object detection  approximate computing  resource-constrained mobile robots  autonomous flights  indoor environments  point clouds  computer vision algorithms  mobile autonomous platforms  video data  decision making  geometric maps  Markov decision process framework  Object detection  Proposals  Roads  Support vector machines  Computer vision  Cameras 
Abstract: Most of the current solutions for autonomous flights in indoor environments rely on purely geometric maps (e.g., point clouds). There has been, however, a growing interest in supplementing such maps with semantic information (e.g., object detections) using computer vision algorithms. Unfortunately, there is a disconnect between the relatively heavy computational requirements of these computer vision solutions, and the limited computation capacity available on mobile autonomous platforms. In this paper, we propose to bridge this gap with a novel Markov Decision Process framework that adapts the parameters of the vision algorithms to the incoming video data rather than fixing them a priori. As a concrete example, we test our framework on a object detection and tracking task, showing significant benefits in terms of energy consumption without considerable loss in accuracy, using a combination of publicly available and novel datasets.


Title: Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry
Key Words: cameras  distance measurement  feature extraction  image colour analysis  image segmentation  pose estimation  cylinder and plane extraction  pose optimization residuals  probabilistic RGB-D odometry framework  curve-aware deteriorates performance  plane extraction approach  single CPU core  organized point clouds  cylinder segments  CAPE  visual odometry  Histograms  Eigenvalues and eigenfunctions  Cameras  Image segmentation  Three-dimensional displays  Probabilistic logic  Principal component analysis 
Abstract: This paper presents CAPE, a method to extract planes and cylinder segments from organized point clouds, which processes 640 × 480 depth images on a single CPU core at an average of 300 Hz, by operating on a grid of planar cells. While, compared to state-of-the-art plane extraction, the latency of CAPE is more consistent and 4-10 times faster, depending on the scene, we also demonstrate empirically that applying CAPE to visual odometry can improve trajectory estimation on scenes made of cylindrical surfaces (e.g. tunnels), whereas using a plane extraction approach that is not curve-aware deteriorates performance on these scenes. To use these geometric primitives in visual odometry, we propose extending a probabilistic RGB-D odometry framework based on points, lines and planes to cylinder primitives. Following this framework, CAPE runs on fused depth maps and the parameters of cylinders are modelled probabilistically to account for uncertainty and weight accordingly the pose optimization residuals.


Title: A Model Predictive Control Approach for Vision-Based Object Grasping via Mobile Manipulator
Key Words: collision avoidance  dexterous manipulators  end effectors  grippers  image colour analysis  image sensors  mobile robots  motion control  nonlinear control systems  path planning  predictive control  robot vision  reach-to-grasp motion  optimal grasping regions  vision-based object grasping  motion control architecture  mobile manipulator system  partial point cloud  onboard RGB-D sensor system  KUKA Youbot  static obstacles  reach-to-grasp scenarios  model predictive control approach  nonlinear model predictive control scheme  Grasping  Three-dimensional displays  Manipulators  Grippers  Robot sensing systems  Predictive control 
Abstract: This paper presents the design of a vision-based object grasping and motion control architecture for a mobile manipulator system. The optimal grasping areas of the object are estimated using the partial point cloud acquired from an onboard RGB-D sensor system. The reach-to-grasp motion of the mobile manipulator is handled via a Nonlinear Model Predictive Control scheme. The controller is formulated accordingly in order to allow the system to operate in a constrained workspace with static obstacles. The goal of the proposed scheme is to guide the robot's end-effector towards the optimal grasping regions with guaranteed input and state constraints such as occlusion and obstacle avoidance, workspace boundaries and field of view constraints. The performance of the proposed strategy is experimentally verified using an 8 Degrees of Freedom KUKA Youbot in different reach-to-grasp scenarios.


Title: Seeing Behind the Scene: Using Symmetry to Reason About Objects in Cluttered Environments
Key Words: feature extraction  geometry  image reconstruction  image segmentation  natural scenes  object detection  cluttered scenes  scene extraction  3D reconstructions  objects segment  symmetry axes-planes  geometry  pointclouds  foreground segmentation problem  smooth surfaces  reflectional symmetries  natural scenes  symmetric objects  cluttered environments  Three-dimensional displays  Shape  Task analysis  Two dimensional displays  Pipelines  Surface treatment  Object segmentation 
Abstract: Symmetry is a common property shared by the majority of man-made objects. This paper presents a novel bottom-up approach for segmenting symmetric objects and recovering their symmetries from 3D pointclouds of natural scenes. Candidate rotational and reflectional symmetries are detected by fitting symmetry axes/planes to the geometry of the smooth surfaces extracted from the scene. Individual symmetries are used as constraints for the foreground segmentation problem that uses symmetry as a global grouping principle. Evaluation on a challenging dataset shows that our approach can reliably segment objects and extract their symmetries from incomplete 3D reconstructions of highly cluttered scenes, outperforming state-of-the-art methods by a wide margin.


Title: LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry
Key Words: calibration  cameras  distance measurement  image sensors  motion estimation  motion measurement  optical radar  sensor fusion  camera imaging  automatic targetless camera-LiDAR calibration method  2D-3D calibration  scaled camera motion calculation  three-dimensional point cloud  motion estimation  sensor-fusion odometry method  hand-eye calibration framework  LiDAR reflectance data  Cameras  Calibration  Three-dimensional displays  Laser radar  Two dimensional displays  Estimation  Sensor fusion 
Abstract: This paper proposes a targetless and automatic camera-LiDAR calibration method. Our approach extends the hand-eye calibration framework to 2D-3D calibration. The scaled camera motions are accurately calculated using a sensor-fusion odometry method. We also clarify the suitable motions for our calibration method. Whereas other calibrations require the LiDAR reflectance data and an initial extrinsic parameter, the proposed method requires only the three-dimensional point cloud and the camera image. The effectiveness of the method is demonstrated in experiments using several sensor configurations in indoor and outdoor scenes. Our method achieved higher accuracy than comparable state-of-the-art methods.


Title: Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding
Key Words: computer vision  edge detection  feature extraction  image recognition  image representation  image segmentation  stereo image processing  robotic welding  weld seams  point cloud  welding paths  Harris 3D  unorganized point clouds  edge detection method  local neighborhood  adaptive density  corner detector  clusters curvature vectors  RGB-D semantic segmentation  3D washer models  recall scores  automatic weld seam detection  Three-dimensional displays  Image edge detection  Welding  Feature extraction  Corner detection  Clustering algorithms  Detectors 
Abstract: In this paper, we propose novel edge and corner detection algorithms for unorganized point clouds. Our edge detection method evaluates symmetry in a local neighborhood and uses an adaptive density based threshold to differentiate 3D edge points. We extend this algorithm to propose a novel corner detector that clusters curvature vectors and uses their geometrical statistics to classify a point as corner. We perform rigorous evaluation of the algorithms on RGB-D semantic segmentation and 3D washer models from the ShapeNet dataset and report higher precision and recall scores. Finally, we also demonstrate how our edge and corner detectors can be used as a novel approach towards automatic weld seam detection for robotic welding. We propose to generate weld seams directly from a point cloud as opposed to using 3D models for offline planning of welding paths. For this application, we show a comparison between Harris 3D and our proposed approach on a panel workpiece.


Title: XBotCloud: A Scalable Cloud Computing Infrastructure for XBot Powered Robots
Key Words: cloud computing  control engineering computing  software agents  Web services  computational resources  robotic platforms  Amazon Web Services Cloud Security  XBotCloud  cross-robot flexibility  XBotCloud performances  robot local control unit  Real-Time modules  moderate execution time constraints  cloud services  cloud server  XBotCore Real-Time cross-robot software component  hard Real-Time execution/communication performance  soft Time execution/communication performance  XBot framework  cloud robotics concept  mobile robots  untethered robots  on-board computational resources  scalable cloud computing infrastructure  Cloud computing  Robot sensing systems  Software as a service  Task analysis  Servers 
Abstract: Limitations with the on-board computational resources installed on untethered robots such as humanoids and mobile robots in general affects significantly the performance and capabilities of these machines. An approach to address this issue is to make use of the cloud robotics concept and take advantage of the extensive computational resources of the cloud. XBotCloud is a recently developed component of the XBot framework. It tackles the above challenges by introducing the tools and mechanisms to enable users and robots to exploit the computational resources of the cloud allowing the execution of services with low, soft or hard Real-Time execution/communication performance. The latter is ensured thanks to the functionality provided by the XBotCore Real-Time cross-robot software component of the XBot framework. XBotCloud addresses also one of the main challenges related with cloud robotics: security. To avoid remote attacks it takes advantage of the Amazon Web Services (AWS)Cloud Security and it uses an internal VPN Network to handle the connectivity between the robot and the cloud server. The full implementation of the framework is presented and its functionality is demonstrated in realistic tasks involving pipelines that mix the execution of cloud services with moderate execution time constraints and Real-Time modules running on the robot local control unit. XBotCloud performances and cross-robot flexibility are experimentally validated on two different robotic platforms, the WALK-MAN humanoid and the CENTAURO upper body/full-body.


Title: NDVI Point Cloud Generator Tool Using Low-Cost RGB-D Sensor
Key Words: cameras  geophysical image processing  image colour analysis  image sensors  vegetation mapping  vegetation index estimation  Microsoft Kinect V2  vegetation monitoring purposes  ROS point cloud generation tools  active IR camera  active RGB-D sensor technology  RGB camera  NDVI point cloud generator tool  3D NDVI maps  Conferences  Intelligent robots 
Abstract: In this manuscript, a NDVI point cloud generator tool based on low-cost active RGB-D sensor is presented. Taking advantage of currently available ROS point cloud generation tools and RGB-D sensor technology (like Microsoft Kinect), that includes an inbuilt active IR camera and a RGB camera, 3D NDVI maps can be quickly and easily generated for vegetation monitoring purposes. When using low-cost sensors for vegetation index estimation, it is necessary to apply a rigorous methodology for extracting reliable information. In this paper, the methodology for NDVI generation using a low-cost sensor as well as experiments to evaluate its performance is presented. The experiments performed show that it is possible to obtain a reliable NDVI point cloud from a Kinect V2.


Title: PCAOT: A Manhattan Point Cloud Registration Method Towards Large Rotation and Small Overlap
Key Words: computational geometry  image registration  iterative methods  principal component analysis  Manhattan world assumption  transformation estimation  overlap estimation  ICP  rotation angle  PCAOT  Manhattan point cloud registration method  robot mapping  iterative closest point  robot localization  principal coordinate alignment with overlap tuning  3D cuboid  Three-dimensional displays  Estimation  Tuning  Iterative closest point algorithm  Mathematical model  Filtering  Task analysis 
Abstract: Point cloud registration is a popular research topic and has been widely used in many tasks, such as robot mapping and localization. It is a challenging problem when the overlap is small, or the rotation is large. The problem has not been well solved by existing methods such as the iterative closest point (ICP) and its variants. In this paper, a novel method named principal coordinate alignment with overlap tuning (PCAOT) is proposed based on the Manhattan world assumption. It solves two key problems together, the transformation estimation and the overlap estimation. The overlap is represented by a 3D cuboid and the transformation is computed only within the overlap region. Instead of finding point correspondence as in traditional methods, we estimate the rotation by principal coordinates alignment, which is faster and less sensitive than ICP and its variants to small overlaps and large rotations. Evaluations demonstrate that our method achieves much better results than the ICP and its variants when the overlap ratio is smaller than 50%, or the rotation angle is larger than 60°. Especially, it is effective when the overlap ratio is less than 30%, or the rotation angle is larger than 90°.


Title: Seeing the Wood for the Trees: Reliable Localization in Urban and Natural Environments
Key Words: feature extraction  geophysical image processing  image matching  image segmentation  robot vision  vegetation mapping  reliable localization  urban environments  natural environments  current state-of-the-art global approaches  structure-poor vegetated areas  orchards  environments clutter  repeatable extraction  distinctive landmarks  natural forests  tree trunks  foliage intertwines  planar structure  place recognition  feature extraction module segments  reliable object-sized segments  heavy clutter  foliage-heavy forest  urban scenarios  random forest  shape descriptor  Feature extraction  Vegetation  Three-dimensional displays  Forestry  Reliability  Clutter  Hidden Markov models 
Abstract: In this work we introduce Natural Segmentation and Matching (NSM), an algorithm for reliable localization, using laser, in both urban and natural environments. Current state-of-the-art global approaches do not generalize well to structure-poor vegetated areas such as forests or orchards. In these environments clutter and perceptual aliasing prevents repeatable extraction of distinctive landmarks between different test runs. In natural forests, tree trunks are not distinctive, foliage intertwines and there is a complete lack of planar structure. In this paper we propose a method for place recognition which uses a more involved feature extraction process which is better suited to this type of environment. First, a feature extraction module segments stable and reliable object-sized segments from a point cloud despite the presence of heavy clutter or tree foliage. Second, repeatable oriented key poses are extracted and matched with a reliable shape descriptor using a Random Forest to estimate the current sensor's position within the target map. We present qualitative and quantitative evaluation on three datasets from different environments - the KITTI benchmark, a parkland scene and a foliage-heavy forest. The experiments show how our approach can achieve place recognition in woodlands while also outperforming current state-of-the-art approaches in urban scenarios without specific tuning.


Title: Extracting Phenotypic Characteristics of Corn Crops Through the Use of Reconstructed 3D Models
Key Words: agriculture  crops  financial management  image reconstruction  image segmentation  solid modelling  social elements  cultivation process  financial losses  crop yield  hybrid plants  motion technology  segmentation process  corn crops  phenotypic characteristics  3D point cloud  reconstructed 3D models  financial elements  Three-dimensional displays  Agriculture  Solid modeling  Estimation  Computational modeling  Image segmentation  Vegetation 
Abstract: Financial and social elements of modern societies are closely connected to the cultivation of corn. Due to its massive production, deficiencies during the cultivation process directly translate to major financial losses. Since proper surveillance in a large scale is still very challenging, the companies that specialize in optimizing crop yield are trying to address the problem at its root by developing hybrid plants able to resist the harsh conditions of the field. The selection of the best hybrid is not easy and every year hundreds of test plants with different phenotypic characteristics are planted while their performance is quantified by inconsistent and rough measurements gathered by humans. We propose a pipeline that takes advantage of the structure from motion technology to create a detailed 3D point cloud of a few plants and segment it into the basic elements of the scene; the ground, the plants, the plant stems, and the plant leaves. The focus is on the segmentation process through which several phenotypic characteristics of individual plants can be extracted. As an example, we show the results for the plant counting and plant height estimation processes where we achieve an accuracy of 88.1% and 89.2%.


Title: A 3D Convolutional Neural Network Towards Real-Time Amodal 3D Object Detection
Key Words: convolutional neural nets  image colour analysis  object detection  object recognition  regression analysis  3D Convolutional Neural Network  3D detectors  object categories  object locations  real-time amodal 3D object detection  Three-dimensional displays  Two dimensional displays  Object detection  Proposals  Solid modeling  Detectors  Shape 
Abstract: We focus on the task of amodal 3D object detection, which is to predict object locations, dimensions, poses and categories in the real world. We introduce a 3D Convolutional Neural Network that takes a volumetric representation of an indoor scene as input and predicts 3D object bounding boxes, object categories, and orientations. Unlike prior state-of-the-arts, our approach does not depend on region proposal techniques to hypothesize object locations. We treat detection and recognition as one regression problem in a single network. Our elegant model is extremely fast and all predictions are reasoned from the global context of a point cloud in a continuous pipeline. We evaluate our approach on two standard datasets: the NYUv2 RGBD dataset and the SUN RGBD dataset. Experiments show that our approach is faster than start-of-the-art 3D detectors by several orders of magnitude towards real-time amodal 3D object detection.


