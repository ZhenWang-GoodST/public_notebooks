total paper: 42
Title: Automatic Labeled LiDAR Data Generation based on Precise Human Model
Key Words: data analysis  image motion analysis  image recognition  learning (artificial intelligence)  neural nets  optical radar  human recognition  point clouds  ground truth label  automatic labeled data generation pipeline  data generation environments  realistic artificial data  automatic labeled LiDAR data generation  precise human model  deep neural networks  Laser radar  Data models  Three-dimensional displays  Pipelines  Labeling  Legged locomotion  Solid modeling 
Abstract: Following improvements in deep neural networks, state-of-the-art networks have been proposed for human recognition using point clouds captured by LiDAR. However, the performance of these networks strongly depends on the training data. An issue with collecting training data is labeling. Labeling by humans is necessary to obtain the ground truth label; however, labeling requires huge costs. Therefore, we propose an automatic labeled data generation pipeline, for which we can change any parameters or data generation environments. Our approach uses a human model named Dhaiba and a background of Miraikan and consequently generated realistic artificial data. We present 500k + data generated by the proposed pipeline. This paper also describes the specification of the pipeline and data details with evaluations of various approaches.


Title: A Maximum Likelihood Approach to Extract Finite Planes from 3-D Laser Scans
Key Words: feature extraction  image registration  image segmentation  laser ranging  maximum likelihood estimation  pattern clustering  ray tracing  stereo image processing  measurement likelihood  point-to-plane distance  ray path information  maximum likelihood approach  object detection  model reconstruction  laser odometry  point cloud registration  robotic systems  strictly probabilistic method  agglomerative hierarchical clustering  3-D laser range scans  finite plane extraction  image segmentation  Laser modes  Three-dimensional displays  Measurement by laser beam  Clustering algorithms  Probabilistic logic  Computational modeling  Maximum likelihood estimation 
Abstract: Whether it is object detection, model reconstruction, laser odometry, or point cloud registration: Plane extraction is a vital component of many robotic systems. In this paper, we propose a strictly probabilistic method to detect finite planes in organized 3-D laser range scans. An agglomerative hierarchical clustering technique, our algorithm builds planes from bottom up, always extending a plane by the point that decreases the measurement likelihood of the scan the least. In contrast to most related methods, which rely on heuristics like orthogonal point-to-plane distance, we leverage the ray path information to compute the measurement likelihood. We evaluate our approach not only on the popular SegComp benchmark, but also provide a challenging synthetic dataset that overcomes SegComp's deficiencies. Both our implementation and the suggested dataset are available at [1].


Title: Rorg: Service Robot Software Management with Linux Containers
Key Words: cloud computing  control engineering computing  Linux  mobile robots  object-oriented programming  scheduling  service robots  Linux container-based scheme  monitor software components  service robots  Linux containers  service robot systems  resource constraints  programmable container management interface  resource time-sharing mechanism  Rorg  resource contention  long-term autonomous tour guide robot  robot software system  software processes  software components  computer resources  service robot software management  robot operating system  Containers  Software  Linux  Service robots  Data centers  Monitoring 
Abstract: Scaling up the software system on service robots increases the maintenance burden of developers and the risk of resource contention of the computer embedded on robots. As a result, developers spend much time on configuring, deploying, and monitoring the robot software system; robots may utilize significant computer resources when all software processes are running. We present Rorg, a Linux container-based scheme to manage, schedule, and monitor software components on service robots. Although Linux containers are already widely-used in cloud environments, this technique is challenging to efficiently adopt in service robot systems due to multi-tasking, resource constraints and performance requirements. To pave the way of Linux containers on service robots in an efficient manner, we present a programmable container management interface and a resource time-sharing mechanism incorporated with the Robot Operating System (ROS). Rorg allows developers to pack software into self-contained images and runs them in isolated environments using Linux containers; it also allows the robot to turn on and off software components on demand to avoid resource contention. We evaluate Rorg with a long-term autonomous tour guide robot: It manages 41 software components on the robot and relieved our maintenance burden, and it also reduces CPU load by 45.5% and memory usage by 16.5% on average.


Title: Distortion-free Robotic Surface-drawing using Conformal Mapping
Key Words: computational geometry  computer graphics  conformal mapping  distance measurement  image reconstruction  least squares approximations  manipulators  mobile robots  solid modelling  distortion-free robotic surface-drawing  robotic pen-drawing system  unknown surface  robotic system  seven-degree-of freedom manipulator  continuous surface  physical canvas surface  point-cloud estimation  drawing surface  2D vector pen art  surface parameterization  squares conformal mapping  complicated pen drawings  general surfaces  impedance-control  digital drawing  2D drawing  Surface impedance  Three-dimensional displays  Robot kinematics  Two dimensional displays  Service robots  Surface reconstruction 
Abstract: We present a robotic pen-drawing system that is capable of faithfully reproducing pen art on an unknown surface. Our robotic system relies on an industrial, seven-degree-of freedom manipulator that can be both position- and impedance-controlled. In order to estimate a rough geometry of the target, continuous surface, we first generate a point cloud of the surface using an RGB-D camera, which is filtered to remove outliers and calibrated to the physical canvas surface. Then, our control algorithm physically reproduces digital drawing on the surface by impedance-controlling the manipulator. Our impedance-controlled drawing algorithm compensates for the uncertainty and incompleteness inherent to a point-cloud estimation of the drawing surface. Moreover, since drawing 2D vector pen art on a 3D surface requires surface parameterization that does not destroy the original 2D drawing, we rely on the least squares conformal mapping. Specifically, the conformal map reduces angle distortion during surface parameterization. As a result, our system can create distortion-free and complicated pen drawings on general surfaces with many unpredictable bumps robustly and faithfully.


Title: Quantum Computation in Robotic Science and Applications
Key Words: cloud computing  intelligent robots  learning (artificial intelligence)  quantum computing  robot programming  quantum computing  cloud services  artificial intelligence  machine learning  robotic scientists  quantum mechanics  quantum computation  intelligent robots  powerful robots  Robot sensing systems  Computers  Optimization  Qubit  Acceleration 
Abstract: Using the effects of quantum mechanics for computing challenges has been an often discussed topic for decades. The frequent successes and early products in this area, which we have seen in recent years, indicate that we are currently entering a new era of computing. This paradigm shift will also impact the work of robotic scientists and the applications of robotics. New possibilities as well as new approaches to known problems will enable the creation of even more powerful and intelligent robots that make use of quantum computing cloud services or co-processors. In this position paper, we discuss potential application areas and also point out open research topics in quantum computing for robotics. We go into detail on the impact of quantum computing in artificial intelligence and machine learning, sensing and perception, kinematics as well as system diagnosis. For each topic we point out where quantum computing could be applied based on results from current research.


Title: Accurate Direct Visual-Laser Odometry with Explicit Occlusion Handling and Plane Detection
Key Words: distance measurement  motion estimation  camera information  mobile platform  direct laser-visual odometry approach building  photometric image alignment  information usage  laser scan  frame-to-frame motion estimate  planar point  individual point clouds  corresponding pixel patches  camera image  extracted planar image patches  nonplanar pixels  pixel alignments  high estimation accuracy  Clearpath Husky platform  competitive estimation accuracy  colored point clouds  accurate direct visual-laser odometry  explicit occlusion handling  plane detection  Three-dimensional displays  Cameras  Measurement by laser beam  Lasers  Estimation  Visual odometry  Motion estimation 
Abstract: In this paper, we address the problem of combining 3D laser scanner and camera information to estimate the motion of a mobile platform. We propose a direct laser-visual odometry approach building upon photometric image alignment. Our approach is designed to maximize the information usage of both, the image and the laser scan, to compute an accurate frame-to-frame motion estimate. To deal with the sparsity of the range measurements, our approach identifies planar point sets within individual point clouds and subsequently extract their corresponding pixel patches from the camera image. The extracted planar image patches are used together with the non-planar pixels to estimate the frame-to-frame motion using a homography formulation capable of incorporating both types of pixel alignments. To achieve high estimation accuracy, we explicitly predict possible occlusions caused by observations taken from different locations. We evaluate our proposed approach using the KITTI dataset as well as data recorded with a Clearpath Husky platform. The experiments suggest that our approach can achieve competitive estimation accuracy and produce consistently registered, colored point clouds.


Title: Adapting Everyday Manipulation Skills to Varied Scenarios
Key Words: control engineering computing  manipulators  mobile robots  motion control  robot vision  service robots  point clouds  target object  tool-using manipulation skills  motion trajectories  scraping material  robot perception module  PR2 robot  Tools  Task analysis  Robots  Three-dimensional displays  Trajectory  Containers  Dairy products 
Abstract: We address the problem of executing tool-using manipulation skills in scenarios where the objects to be used may vary. We assume that point clouds of the tool and target object can be obtained, but no interpretation or further knowledge about these objects is provided. The system must interpret the point clouds and decide how to use the tool to complete a manipulation task with a target object; this means it must adjust motion trajectories appropriately to complete the task. We tackle three everyday manipulations: scraping material from a tool into a container, cutting, and scooping from a container. Our solution encodes these manipulation skills in a generic way, with parameters that can be filled in at run-time via queries to a robot perception module; the perception module abstracts the functional parts of the tool and extracts key parameters that are needed for the task. The approach is evaluated in simulation and with selected examples on a PR2 robot.


Title: Transferring Grasp Configurations using Active Learning and Local Replanning
Key Words: dexterous manipulators  image segmentation  learning (artificial intelligence)  path planning  robot vision  grasp configurations  active learning  prior example objects  similar shapes  geometric shape characteristics  semantic shape characteristics  grasp space  model parts  corresponding grasps  local replanning  point cloud  robotic hands  Shape  Grasping  Semantics  Three-dimensional displays  Robots  Stability analysis  Particle swarm optimization 
Abstract: We present a new approach to transfer grasp configurations from prior example objects to novel objects. We assume the novel and example objects have the same topology and similar shapes. We perform 3D segmentation on these objects using geometric and semantic shape characteristics. We compute a grasp space for each part of the example object using active learning. We build bijective contact mapping between these model parts and compute the corresponding grasps for novel objects. Finally, we assemble the individual parts and use local replanning to adjust grasp configurations while maintaining its stability and physical constraints. Our approach is general, can handle all kind of objects represented using mesh or point cloud and a variety of robotic hands.


Title: A Fog Robotic System for Dynamic Visual Servoing
Key Words: cloud computing  control engineering computing  Internet  mobile robots  motion control  multi-robot systems  object recognition  position control  robot vision  service robots  telerobotics  visual servoing  dynamic visual  cloud robotics  multiple robots  cloud services  unlimited computation power  network communication  dynamic compliant service robots  human compliant service robots  dynamic self-balancing robot  cloud teleoperation  cloud-based image based visual servoing module  cloud teleoperator  real-time automation system  cloud-edge hybrid design  dynamic robotic control  deep-learning recognition systems  self-balancing service robot  fog robotic object recognition system  Cloud computing  Service robots  Robot sensing systems  Legged locomotion  Visualization 
Abstract: Cloud Robotics is a paradigm where multiple robots are connected to cloud services via Internet to access “unlimited” computation power, at the cost of network communication. However, due to limitations such as network latency and variability, it is difficult to control dynamic, human compliant service robots directly from the cloud. In this work, we combine cloud robotics with an agile edge device to build a Fog Robotic system by leveraging an asynchronous protocol with a “heartbeat” signal. We use the system to enable robust teleoperation of a dynamic self-balancing robot from the cloud. We use the system to pick up boxes from static locations, a task commonly performed in warehouse logistics. To make cloud teleoperation more intuitive and efficient, we program a cloud-based image based visual servoing (IBVS) module to automatically assist the cloud teleoperator during the object pickups. Visual feedbacks, including apriltag recognition and tracking, are performed in the cloud to emulate a Fog Robotic object recognition system for IBVS. We demonstrate the feasibility of a dynamic real-time automation system using this cloud-edge hybrid design, which opens up possibilities of deploying dynamic robotic control with deep-learning recognition systems in Fog Robotics. Finally, we show that Fog Robotics enables the self-balancing service robot to pick up a box automatically from a person under unstructured environments.


Title: Motion Planning Networks
Key Words: collision avoidance  computational complexity  manipulators  mobile robots  motion control  neurocontrollers  computational complexity  neural network  collision-free paths  motion planning networks  robotics applications  self-driving cars  7 DOF Baxter robot manipulator  MPNet  Planning  Three-dimensional displays  Neural networks  Training  Manipulators  Encoding 
Abstract: Fast and efficient motion planning algorithms are crucial for many state-of-the-art robotics applications such as self-driving cars. Existing motion planning methods become ineffective as their computational complexity increases exponentially with the dimensionality of the motion planning problem. To address this issue, we present Motion Planning Networks (MPNet), a neural network-based novel planning algorithm. The proposed method encodes the given workspaces directly from a point cloud measurement and generates the end-to-end collision-free paths for the given start and goal configurations. We evaluate MPNet on various 2D and 3D environments including the planning of a 7 DOF Baxter robot manipulator. The results show that MPNet is not only consistently computationally efficient in all environments but also generalizes to completely unseen environments. The results also show that the computation time of MPNet consistently remains less than 1 second in all presented experiments, which is significantly lower than existing state-of-the-art motion planning algorithms.


Title: Visual SLAM: Why Bundle Adjust?
Key Words: cameras  feature extraction  image sequences  motion estimation  optimisation  pose estimation  robot vision  SLAM (robots)  video signal processing  bundle adjustment  feature-based monocular SLAM  camera orientation optimisation  camera position estimation  quasiconvex formulation  keyframe rate  SLAM optimisation  rotational motion  slow motion  SLAM algorithm  3D structure estimation  input feature tracks  3D point cloud  3D map estimation  6DOF camera trajectory estimation  visual SLAM  Simultaneous localization and mapping  Cameras  Estimation  Bundle adjustment  Optimization  Visualization 
Abstract: Bundle adjustment plays a vital role in feature-based monocular SLAM. In many modern SLAM pipelines, bundle adjustment is performed to estimate the 6DOF camera trajectory and 3D map (3D point cloud) from the input feature tracks. However, two fundamental weaknesses plague SLAM systems based on bundle adjustment. First, the need to carefully initialise bundle adjustment means that all variables, in particular the map, must be estimated as accurately as possible and maintained over time, which makes the overall algorithm cumbersome. Second, since estimating the 3D structure (which requires sufficient baseline) is inherent in bundle adjustment, the SLAM algorithm will encounter difficulties during periods of slow motion or pure rotational motion. We propose a different SLAM optimisation core: instead of bundle adjustment, we conduct rotation averaging to incrementally optimise only camera orientations. Given the orientations, we estimate the camera positions and 3D points via a quasi-convex formulation that can be solved efficiently and globally optimally. Our approach not only obviates the need to estimate and maintain the positions and 3D map at keyframe rate (which enables simpler SLAM systems), it is also more capable of handling slow motions or pure rotational motions.


Title: Oriented Point Sampling for Plane Detection in Unorganized Point Clouds
Key Words: image reconstruction  image segmentation  image sensors  object detection  octrees  robot vision  SLAM (robots)  solid modelling  unorganized point clouds  crucial pre-processing step  point cloud segmentation  organized point clouds  plane hypotheses  unoriented points  efficient plane detection method  semantic mapping  SLAM  plane detection methods  OPS  oriented point sampling  Three-dimensional displays  Sun  Surface treatment  Clustering algorithms  Octrees  Estimation  Image segmentation 
Abstract: Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efficiently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efficient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset.


Title: Spatial change detection using voxel classification by normal distributions transform
Key Words: image classification  image colour analysis  image sensors  mobile robots  normal distribution  object detection  optical scanners  robot vision  SLAM (robots)  stereo image processing  transforms  voxel classification  robotic applications  mobile robot  3D laser scanner  grid data  ND voxels  normal distributions transform  spatial change detection  onboard RGB-D camera  stereo camera  real-time range sensors  real-time localization  Three-dimensional displays  Mobile robots  Cameras  Real-time systems  Measurement by laser beam  Sensors 
Abstract: Detection of spatial change around a robot is indispensable in several robotic applications, such as search and rescue, security, and surveillance. The present paper proposes a fast spatial change detection technique for a mobile robot using an on-board RGB-D/stereo camera and a highly precise 3D map created by a 3D laser scanner. This technique first converts point clouds in a map and measured data to grid data (ND voxels) using normal distributions transform and classifies the ND voxels into three categories. The voxels in the map and the measured data are then compared according to the category and features of the ND voxels. Overlapping and voting techniques are also introduced in order to detect the spatial changes more robustly. We conducted experiments using a mobile robot equipped with real-time range sensors to confirm the performance of the proposed real-time localization and spatial change detection techniques in indoor and outdoor environments.


Title: Detection and Tracking of Small Objects in Sparse 3D Laser Range Data
Key Words: autonomous aerial vehicles  data structures  image segmentation  image sensors  laser ranging  median filters  mobile robots  object detection  object tracking  robot vision  solid modelling  autonomous behavior  microaerial vehicles  multiobject tracking  lightweight sensors  sparse point clouds  Velodyne VLP-16 sensor  MAV hardware  unlabeled data  sparse 3d laser range data  objects detection  median filters  data structure  Three-dimensional displays  Sensors  Target tracking  Object tracking  Real-time systems  Vehicle dynamics  Heuristic algorithms 
Abstract: Detection and tracking of dynamic objects is a key feature for autonomous behavior in a continuously changing environment. With the increasing popularity and capability of micro aerial vehicles (MAVs) efficient algorithms have to be utilized to enable multi object tracking on limited hardware and data provided by lightweight sensors. We present a novel segmentation approach based on a combination of median filters and an efficient pipeline for detection and tracking of small objects within sparse point clouds generated by a Velodyne VLP-16 sensor. We achieve real-time performance on a single core of our MAV hardware by exploiting the inherent structure of the data. Our approach is evaluated on simulated and real scans of in- and outdoor environments, obtaining results comparable to the state of the art. Additionally, we provide an application for filtering the dynamic and mapping the static part of the data, generating further insights into the performance of the pipeline on unlabeled data.


Title: Point Cloud Compression for 3D LiDAR Sensor using Recurrent Neural Network with Residual Blocks
Key Words: computational geometry  data compression  image coding  iterative methods  mobile robots  octrees  optical radar  recurrent neural nets  SLAM (robots)  recurrent neural network  residual blocks  generic octree point cloud compression method  potential application scenarios  decompressed point cloud data  3D LiDAR sensor  autonomous driving systems  3D LiDAR data  raw D formatted LiDAR data  2D formatted LiDAR data  Three-dimensional displays  Image coding  Laser radar  Two dimensional displays  Robot sensing systems  Decoding  Recurrent neural networks 
Abstract: The use of 3D LiDAR, which has proven its capabilities in autonomous driving systems, is now expanding into many other fields. The sharing and transmission of point cloud data from 3D LiDAR sensors has broad application prospects in robotics. However, due to the sparseness and disorderly nature of this data, it is difficult to compress it directly into a very low volume. A potential solution is utilizing raw LiDAR data. We can rearrange the raw data from each frame losslessly in a 2D matrix, making the data compact and orderly. Due to the special structure of 3D LiDAR data, the texture of the 2D matrix is irregular, in contrast to 2D matrices of camera images. In order to compress this raw, 2D formatted LiDAR data efficiently, in this paper we propose a method which uses a recurrent neural network and residual blocks to progressively compress one frame's information from 3D LiDAR. Compared to our previous image compression based method and generic octree point cloud compression method, the proposed approach needs much less volume while giving the same decompression accuracy. Potential application scenarios for point cloud compression are also considered in this paper. We describe how decompressed point cloud data can be used with SLAM (simultaneous localization and mapping) as well as for localization using a given map, illustrating potential uses of the proposed method in real robotics applications.


Title: PointNetGPD: Detecting Grasp Configurations from Point Sets
Key Words: computational geometry  feature extraction  grippers  image colour analysis  learning (artificial intelligence)  PointNetGPD  end-to-end grasp evaluation  object grasping  3D point cloud  grasp configuration detection  point sets  robot grasp configurations  complex geometric structure  gripper  3D geometry information  deep neural network  RGB-D camera  Three-dimensional displays  Robot sensing systems  Measurement  Grippers  Grasping  Solid modeling  Geometry 
Abstract: In this paper, we propose an end-to-end grasp evaluation model to address the challenging problem of localizing robot grasp configurations directly from the point cloud. Compared to recent grasp evaluation metrics that are based on handcrafted depth features and a convolutional neural network (CNN), our proposed PointNetGPD is lightweight and can directly process the 3D point cloud that locates within the gripper for grasp evaluation. Taking the raw point cloud as input, our proposed grasp evaluation network can capture the complex geometric structure of the contact area between the gripper and the object even if the point cloud is very sparse. To further improve our proposed model, we generate a large-scale grasp dataset with 350k real point cloud and grasps with the YCB object set for training. The performance of the proposed model is quantitatively measured both in simulation and on robotic hardware. Experiments on object grasping and clutter removal show that our proposed model generalizes well to novel objects and outperforms state-of-the-art methods. Code and video are available at https://lianghongzhuo.github.io/PointNetGPD.


Title: Automated Seedling Height Assessment for Tree Nurseries Using Point Cloud Processing
Key Words: computer vision  forestry  height measurement  image sampling  solid modelling  stereo image processing  seedling measurement process  scanning laser profilometer  application specific point-cloud  point-cloud generation methods  3D structured light sensing  light intensity detection  height measurement  measurement accuracy  measurement system  tree nurseries  point cloud processing  automated seedling height assessment system  offline identification  report generation  seedling development process  production optimization purposes  data samples  industrial scale operations  measurement sample size  measurement resolution  Centre for Agriculture and Forestry Development  Canada  Newfoundland and Labrador  Wooddale  Three-dimensional displays  Robot sensing systems  Measurement by laser beam  Control systems  Vegetation  Laser radar 
Abstract: This paper presents a prototype of an automated seedling height assessment system for tree nurseries. The proposed system can acquire and store real-time 3D point-cloud data of seedlings; and perform offline identification, measurement, and report generation of seedling heights with an overall system accuracy that meets a 5mm accuracy specification. Periodic growth information of seedlings allows quantifying effects of different factors on the overall seedling development process for research and production optimization purposes. However, current manual sampling approaches used at these facilities produce quite limited data samples, and the process is rather time-consuming and labor intensive for industrial scale operations. In contrast, the proposed system is capable of significantly increasing the measurement sample size, measurement resolution, and frequency of measurement by automating the seedling measurement process using a scanning laser profilometer and an application specific point-cloud processing algorithm. The performance of the proposed profilometry solution for point-cloud generation is compared with several other point-cloud generation methods such as a 3D structured light sensing, light intensity detection and ranging (LiDAR), stereovision, and photogrammetry. This comparison results demonstrate a superior performance of the laser-profilometer over other sensing solutions available for seedling height measurement. The proposed system is experimentally validated for its measurement accuracy and repeatability. The field-test of the measurement system was conducted at Centre for Agriculture and Forestry Development, Wooddale, Newfoundland and Labrador (NL), Canada, and the results demonstrate the practical applicability and technological readiness of the proposed system for field deployment.


Title: SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data
Key Words: image colour analysis  image segmentation  object detection  optical radar  SEG-VoxelNet  LiDAR data  RGB images  LiDAR point clouds  autonomous driving scenarios  semantic segmentation technique  3D LiDAR point cloud based detection  image semantic segmentation network  SEG-Net  improved-VoxelNet  semantic segmentation map  point cloud data  image semantic feature  KITTI 3D vehicle detection benchmark  Three-dimensional displays  Laser radar  Semantics  Vehicle detection  Feature extraction  Image segmentation  Two dimensional displays 
Abstract: This paper proposes a SEG-VoxelNet that takes RGB images and LiDAR point clouds as inputs for accurately detecting 3D vehicles in autonomous driving scenarios, which for the first time introduces semantic segmentation technique to assist the 3D LiDAR point cloud based detection. Specifically, SEG-VoxelNet is composed of two sub-networks: an image semantic segmentation network (SEG-Net) and an improved-VoxelNet. The SEG-Net generates the semantic segmentation map which represents the probability of the category for each pixel. The improved-VoxelNet is capable of effectively fusing point cloud data with image semantic feature and generating accurate 3D bounding boxes of vehicles. Experiments on the KITTI 3D vehicle detection benchmark show that our approach outperforms the methods of state-of-the-art.


Title: Object Classification Based on Unsupervised Learned Multi-Modal Features For Overcoming Sensor Failures
Key Words: driver information systems  feature extraction  image classification  image fusion  unsupervised learning  unsupervised learned multimodal features  autonomous driving applications  road users  road side infrastructure  autonomous cars  classification modules  unseen sensor noise  object classification module  total sensor failure  unsupervised feature training  uni-modal classifiers training  multimodal classifiers training  feature space  sensor modalities  decision module  unsupervised learned multi-modal features  Three-dimensional displays  Feature extraction  Robot sensing systems  Computer architecture  Training  Decoding  Convolutional codes 
Abstract: For autonomous driving applications it is critical to know which type of road users and road side infrastructure are present to plan driving manoeuvres accordingly. Therefore autonomous cars are equipped with different sensor modalities to robustly perceive its environment. However, for classification modules based on machine learning techniques it is challenging to overcome unseen sensor noise. This work presents an object classification module operating on unsupervised learned multi-modal features with the ability to overcome gradual or total sensor failure. A two stage approach composed of an unsupervised feature training and a uni-modal and multimodal classifiers training is presented. We propose a simple but effective decision module switching between uni-modal and multi-modal classifiers based on the closeness in the feature space to the training data. Evaluations on the ModelNet 40 data set show that the proposed approach has a 14% accuracy gain compared to a late fusion approach operating on a noisy point cloud data and a 6% accuracy gain when operating on noisy image data.


Title: SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud
Key Words: image segmentation  object detection  optical radar  rendering (computer graphics)  unsupervised learning  LiDAR point cloud  deep-learning-based approaches  point cloud segmentation  SqueezeSetV2  data collection  domain-adaptation training pipeline  domain adaptation pipeline  unsupervised domain adaptation  road-object segmentation  domain-adaptation methods  Three-dimensional displays  Laser radar  Training  Adaptation models  Data models  Pipelines  Sensors 
Abstract: Earlier work demonstrates the promise of deep-learning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2. With an improved model structure, SqueezeSetV2 is more robust against dropout noises in LiDAR point cloud and therefore achieves significant accuracy improvement. Training models for point cloud segmentation requires large amounts of labeled data, which is expensive to obtain. To sidestep the cost of data collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. Existing domain-adaptation methods mainly focus on images and most of them cannot be directly applied to point clouds. We address this problem with a domain-adaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6% over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0% to 57.4%. Our source code and synthetic dataset are open sourced. https://github.com/xuanyuzhou98/SqueezeSegV2.


Title: A Fog Robotics Approach to Deep Robot Learning: Application to Object Recognition and Grasp Planning in Surface Decluttering
Key Words: cloud computing  control engineering computing  learning (artificial intelligence)  mobile robots  object recognition  path planning  robot programming  robot vision  storage management  fog robotics  mobile robot  grasp planning model  nonpublic synthetic images  deep object recognition  nonprivate synthetic images  deep models  centralized Cloud Robotics model  surface decluttering  deep robot learning  Cloud computing  Robot sensing systems  Computational modeling  Adaptation models  Security  Data models 
Abstract: The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4× to successfully declutter 86% of objects over 213 attempts.


Title: 2D3D-Matchnet: Learning To Match Keypoints Across 2D Image And 3D Point Cloud
Key Words: cameras  feature extraction  image classification  image matching  image representation  image retrieval  image sensors  learning (artificial intelligence)  object recognition  pose estimation  solid modelling  visual databases  image-based counterpart  visual pose estimation  2D-3D image  cloud correspondences  end-to-end deep network architecture  query image  3D point cloud reference map  Oxford 2D-3D Patches dataset  Oxford Robotcar dataset  ground truth camera pose  Three-dimensional displays  Two dimensional displays  Pose estimation  Visualization  Cameras  Training  Detectors 
Abstract: Large-scale point cloud generated from 3D sensors is more accurate than its image-based counterpart. However, it is seldom used in visual pose estimation due to the difficulty in obtaining 2D-3D image to point cloud correspondences. In this paper, we propose the 2D3D-MatchNet - an end-to-end deep network architecture to jointly learn the descriptors for 2D and 3D keypoint from image and point cloud, respectively. As a result, we are able to directly match and establish 2D-3D correspondences from the query image and 3D point cloud reference map for visual pose estimation. We create our Oxford 2D-3D Patches dataset from the Oxford Robotcar dataset with the ground truth camera poses and 2D-3D image to point cloud correspondences for training and testing the deep network. Experimental results verify the feasibility of our approach.


Title: Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications
Key Words: computer vision  learning (artificial intelligence)  mobile robots  object detection  pose estimation  high posture variance  real-time detection rates  3D object detection domain  mobile application  3D point clouds  robust 3D person detector  posture estimator  mobile robotic applications  computer vision domain  mobile robotics  standing postures  socially aware navigation  deep learning techniques  Kinect2 depth sensor  Three-dimensional displays  Detectors  Feature extraction  Robots  Histograms  Deep learning  Task analysis 
Abstract: Due to recent deep learning techniques, person detection seems to be solved in the computer vision domain, however, it is still an issue in mobile robotics. On a robot only limited computing capacities are available. The challenge gets even more difficult when operating in an environment, with people in poses different from the standard upright ones. In this work the environment of a supermarket is considered. Unlike most scenarios targeted by the community, persons not only occur in standing postures, but also grasping into the shelves or squatting in front of them. Furthermore, people are heavily occluded, e.g. by shopping carts. In such a challenging environment, it is important to perceive people early enough and in real-time in order to enable a socially aware navigation. Classical person detectors often suffer from a high posture variance or do not achieve acceptable real-time detection rates. For this reason, different components from the 3D object detection domain have been used to create a new robust person detector for mobile application. Operating on 3D point clouds allows fast detections in real-time up to our goal distance of ten meters and above using the Kinect2 depth sensor. The detector can even differentiate between typical postures of customers who stand or squat in front of shelves.


Title: Part Segmentation for Highly Accurate Deformable Tracking in Occlusions via Fully Convolutional Neural Networks
Key Words: convolutional neural nets  image filtering  image segmentation  learning (artificial intelligence)  object tracking  pose estimation  robot vision  stereo image processing  visual perception  direct pose estimation  machine learning  direct estimation techniques  geometric tracking methods  robotic applications  observed point clouds  segmentation maps  Fast-FCN network architecture  convolutional neural networks  Three-dimensional displays  Semantics  Computational modeling  Robots  Image segmentation  Computer architecture  Pose estimation 
Abstract: Successfully tracking the human body is an important perceptual challenge for robots that must work around people. Existing methods fall into two broad categories: geometric tracking and direct pose estimation using machine learning. While recent work has shown direct estimation techniques can be quite powerful, geometric tracking methods using point clouds can provide a very high level of 3D accuracy which is necessary for many robotic applications. However these approaches can have difficulty in clutter when large portions of the subject are occluded. To overcome this limitation, we propose a solution based on fully convolutional neural networks (FCN). We develop an optimized Fast-FCN network architecture for our application which allows us to filter observed point clouds and improve tracking accuracy while maintaining interactive frame rates. We also show that this model can be trained with a limited number of examples and almost no manual labelling by using an existing geometric tracker and data augmentation to automatically generate segmentation maps. We demonstrate the accuracy of our full system by comparing it against an existing geometric tracker, and show significant improvement in these challenging scenarios.


Title: Beyond Point Clouds: Fisher Information Field for Active Visual Localization
Key Words: mobile robots  path planning  robot vision  point clouds  Fisher information field  active visual localization  mobile robots  perception requirement  planning stage  localization information  perception-aware planning  3D landmarks  sensor visibility  Three-dimensional displays  Planning  Visualization  Cameras  Simultaneous localization and mapping 
Abstract: For mobile robots to localize robustly, actively considering the perception requirement at the planning stage is essential. In this paper, we propose a novel representation for active visual localization. By formulating the Fisher information and sensor visibility carefully, we are able to summarize the localization information into a discrete grid, namely the Fisher information field. The information for arbitrary poses can then be computed from the field in constant time, without the need of costly iterating all the 3D landmarks. Experimental results on simulated and real-world data show the great potential of our method in efficient active localization and perception-aware planning. To benefit related research, we release our implementation of the information field to the public.


Title: 3D Keypoint Repeatability for Heterogeneous Multi-Robot SLAM
Key Words: feature extraction  mobile robots  multi-robot systems  robot vision  SLAM (robots)  point cloud registration  loop closure  heterogenous multirobot SLAM applications  NARF detector  3D keypoint repeatability  heterogeneous multirobot SLAM  multirobot SLAM scenario  sensor measurement point clouds  point density  3D keypoint detectors  multirobot SLAM system  KPQ-SI  relative repeatability  Three-dimensional displays  Detectors  Simultaneous localization and mapping  Cameras  Laser radar 
Abstract: For robots with different types of sensors, loop closure in a multi-robot SLAM scenario requires keypoints that can be matched between sensor measurement point clouds with different properties such as point density and noise. In this paper, we evaluate the performance of several 3D keypoint detectors (Harris3D, ISS, KPQ, KPQ-SI, and NARF) for repeatability between scans from different sensors towards building a heterogeneous multi-robot SLAM system. We find that KPQ-SI and NARF have the best relative repeatability, with KPQ-SI finding more keypoints overall and a higher number of repeatable keypoints, at the cost of significantly worse computational performance. In scans of the same area from different poses, both detectors find enough keypoints for point cloud registration and loop closure. For heterogenous multirobot SLAM applications with computational or bandwidth restrictions, the NARF detector consistently finds repeatable keypoints while also allowing for real-time performance.


Title: IN2LAMA: INertial Lidar Localisation And MApping
Key Words: mobile robots  motion estimation  optical radar  optimisation  probability  robot vision  IN2LAMA  spinning mechanisms  resulting point clouds  lidar mapping literature  constant velocity motion model  upsampled inertial data  motion distortion  explicit motion-model  temporally precise upsampled preintegrated measurement  frame-to-frame planar  edge features association  probabilistic framework  inertial lidar localisation and mapping  batch on-manifold optimisation formulation  state change estimation  front-end interaction  back-end interaction  Laser radar  Distortion measurement  Three-dimensional displays  Distortion  Optimization  Trajectory  Gyroscopes 
Abstract: In this paper, we introduce a probabilistic framework for INertial Lidar Localisation And MApping (IN2LAMA). Most of today's lidars are based on spinning mechanisms that do not capture snapshots of the environment. As a result, movement of the sensor can occur while scanning. Without a good estimation of this motion, the resulting point clouds might be distorted. In the lidar mapping literature, a constant velocity motion model is commonly assumed. This is an approximation that does not necessarily always hold. The key idea of the proposed framework is to exploit preintegrated measurements over upsampled inertial data to handle motion distortion without the need for any explicit motion-model. It tightly integrates inertial and lidar data in a batch on-manifold optimisation formulation. Using temporally precise upsampled preintegrated measurement allows frame-to-frame planar and edge features association. Moreover, features are re-computed when the estimate of the state changes, consolidating front-end and back-end interaction. We validate the effectiveness of the approach through simulated and real data.


Title: Speeding Up Iterative Closest Point Using Stochastic Gradient Descent
Key Words: gradient methods  image colour analysis  image registration  iterative methods  optimisation  pose estimation  SLAM (robots)  stochastic gradient descent  3D laser scanners  RGB-D cameras  model registration  iterative closest point  SGD  convergence speed  3D point clouds  pose estimation  SLAM  optimisation problem  Three-dimensional displays  Standards  Stochastic processes  Euclidean distance  Cost function  Sensors 
Abstract: Sensors producing 3D point clouds such as 3D laser scanners and RGB-D cameras are widely used in robotics, be it for autonomous driving or manipulation. Aligning point clouds produced by these sensors is a vital component in such applications to perform tasks such as model registration, pose estimation, and SLAM. Iterative closest point (ICP) is the most widely used method for this task, due to its simplicity and efficiency. In this paper we propose a novel method which solves the optimisation problem posed by ICP using stochastic gradient descent (SGD). Using SGD allows us to improve the convergence speed of ICP without sacrificing solution quality. Experiments using Kinect as well as Velodyne data show that, our proposed method is faster than existing methods, while obtaining solutions comparable to standard ICP. An additional benefit is robustness to parameters when processing data from different sensors.


Title: Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds
Key Words: convolutional neural nets  feature extraction  graph theory  image segmentation  learning (artificial intelligence)  stereo image processing  hierarchical depthwise graph convolutional neural network  3D semantic segmentation  point clouds  point cloud semantic segmentation  depthwise convolution  pointwise convolution  local feature extraction  local features  global features  graph convolution  depthwise graph convolution  Three-dimensional displays  Convolution  Feature extraction  Semantics  Memory management  Shape  Convolutional neural networks 
Abstract: This paper proposes a hierarchical depthwise graph convolutional neural network (HDGCN) for point cloud semantic segmentation. The main chanllenge for learning on point clouds is to capture local structures or relationships. Graph convolution has the strong ability to extract local shape information from neighbors. Inspired by depthwise convolution, we propose a depthwise graph convolution which requires less memory consumption compared with the previous graph convolution. While depthwise graph convolution aggregates features channel-wisely, pointwise convolution is used to learn features across different channels. A customized block called DGConv is specially designed for local feature extraction based on depthwise graph convolution and pointwise convolution. The DGConv block can extract features from points and transfer features to neighbors while being invariant to different point orders. HDGCN is constructed by a series of DGConv blocks using a hierarchical structure which can extract both local and global features of point clouds. Experiments show that HDGCN achieves the state-of-the-art performance in the indoor dataset S3DIS and the outdoor dataset Paris-Lille-3D.


Title: CELLO-3D: Estimating the Covariance of ICP in the Real World
Key Words: covariance analysis  covariance matrices  data analysis  image registration  iterative methods  state estimation  CELLO-3D  state estimation frameworks  closed-form covariance estimation algorithms  data-driven approach  uncertainty estimation  closed-form solutions  covariance estimation and learning through likelihood optimization framework  iterative closest point registrations  3D datasets  ICP registrations  real 3D point clouds  Three-dimensional displays  Estimation  Linear programming  Uncertainty  Prediction algorithms  Measurement  Computational modeling 
Abstract: The fusion of Iterative Closest Point (ICP) registrations in existing state estimation frameworks relies on an accurate estimation of their uncertainty. In this paper, we study the estimation of this uncertainty in the form of a covariance. First, we scrutinize the limitations of existing closed-form covariance estimation algorithms over 3D datasets. Then, we set out to estimate the covariance of ICP registrations through a data-driven approach, with over 5100000 registrations on 1020 pairs from real 3D point clouds. We assess our solution upon a wide spectrum of environments, ranging from structured to unstructured and indoor to outdoor. The capacity of our algorithm to predict covariances is accurately assessed, as well as the usefulness of these estimations for uncertainty estimation over trajectories. The proposed method estimates covariances better than existing closed-form solutions, and makes predictions that are consistent with observed trajectories.


Title: Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities
Key Words: computational complexity  computational geometry  graph theory  mesh generation  mobile robots  optimisation  robot vision  SLAM (robots)  state estimation  structural regularities  point cloud representation  tightly couple mesh regularization  VIO optimization  per-frame approach  visual-inertial odometry algorithms  visual-inertial 3D mesh generation  decouple state estimation  factor-graph formulation  computational complexity  memory usage  localization accuracy  Three-dimensional displays  Optimization  Two dimensional displays  State estimation  Cameras  Histograms  Mesh generation  SLAM  Vision-Based Navigation  Sensor Fusion 
Abstract: Visual-Inertial Odometry (VIO) algorithms typically rely on a point cloud representation of the scene that does not model the topology of the environment. A 3D mesh instead offers a richer, yet lightweight, model. Nevertheless, building a 3D mesh out of the sparse and noisy 3D landmarks triangulated by a VIO algorithm often results in a mesh that does not fit the real scene. In order to regularize the mesh, previous approaches decouple state estimation from the 3D mesh regularization step, and either limit the 3D mesh to the current frame [1], [2] or let the mesh grow indefinitely [3], [4]. We propose instead to tightly couple mesh regularization and state estimation by detecting and enforcing structural regularities in a novel factor-graph formulation. We also propose to incrementally build the mesh by restricting its extent to the time-horizon of the VIO optimization; the resulting 3D mesh covers a larger portion of the scene than a per-frame approach while its memory usage and computational complexity remain bounded. We show that our approach successfully regularizes the mesh, while improving localization accuracy, when structural regularities are present, and remains operational in scenes without regularities.


Title: Guaranteed Active Constraints Enforcement on Point Cloud-approximated Regions for Surgical Applications
Key Words: end effectors  force control  haptic interfaces  human-robot interaction  manipulator dynamics  medical robotics  mobile robots  path planning  surgery  point cloud-approximated regions  surgical applications  human-robot interaction controller  pHRI  sensitive tissues  restricted region  constraint enforcement  interaction force  constraint satisfaction  KUKA LWR4+ robot  3D point-cloud  artificial potential fields  active constraint enforcement  kinesthetic guidance  KUKA LWR4+ robot end-effector  KUKA virtual slave  Tools  Force  Three-dimensional displays  Surgery  End effectors  Stability analysis 
Abstract: In this work, a passive physical human-robot interaction (pHRI) controller is proposed to intraoperatively ensure that sensitive tissues will not be damaged by the robot's tool. The proposed scheme uses the point cloud of the restricted region's surface as constraint definition and Artificial Potential fields for constraint enforcement. The controller is proven to be passive with respect to the interaction force and to guarantee constraint satisfaction in all cases. The proposed methodology is experimentally validated by the kinesthetic guidance of a KUKA LWR4+ robot's end-effector driving a virtual slave KUKA in the vicinity of a 3D point-cloud of a kidney and its adjacent vessels.


Title: OVPC Mesh: 3D Free-space Representation for Local Ground Vehicle Navigation
Key Words: computational geometry  mesh generation  mobile robots  navigation  path planning  remotely operated vehicles  robot vision  stereo image processing  OVPC Mesh  3D free-space representation  local ground vehicle navigation  autonomous unmanned ground vehicle  Visible Point Clouds Mesh  local point cloud data  UGV navigation  on visible point clouds mesh  watertight 3D mesh generation  trajectory planning  robot  Three-dimensional displays  Navigation  Robot sensing systems  Planning  Laser radar  Real-time systems 
Abstract: This paper presents a novel approach for local 3D environment representation for autonomous unmanned ground vehicle (UGV) navigation called On Visible Point Clouds Mesh (OVPC Mesh). Our approach represents the surrounding of the robot as a watertight 3D mesh generated from local point cloud data in order to represent the free space surrounding the robot. It is a conservative estimation of the free space and provides a desirable trade-off between representation precision and computational efficiency, without having to discretize the environment into a fixed grid size. Our experiments analyze the usability of the approach for UGV navigation in rough terrain, both in simulation and in a fully integrated real-world system. Additionally, we compare our approach to well-known state-of the-art solutions, such as Octomap and Elevation Mapping and show that OVPC Mesh can provide reliable 3D information for trajectory planning while fulfilling real-time constraints.


Title: Improving Underwater Obstacle Detection using Semantic Image Segmentation
Key Words: feature extraction  image classification  image enhancement  image matching  image segmentation  learning (artificial intelligence)  mobile robots  path planning  robot vision  stereo image processing  image-based underwater obstacle detection  sparse stereo point clouds  monocular semantic image segmentation  cluttered underwater environments  robust robotic path planning  feature-based stereo matching  learning-based segmentation  robust obstacle map  direct binary learning  underwater obstacles  multiclass learning approach  binary map  sparse stereo matching  3D obstacle maps  coral reef environments  image-wide obstacle detection  dynamic objects  image-based obstacle maps  Image segmentation  Semantics  Cameras  Three-dimensional displays  Real-time systems  Training  Robots 
Abstract: This paper presents two novel approaches for improving image-based underwater obstacle detection by combining sparse stereo point clouds with monocular semantic image segmentation. Generating accurate image-based obstacle maps in cluttered underwater environments, such as coral reefs, are essential for robust robotic path planning and navigation. However, these maps can be challenged by factors including visibility, lighting and dynamic objects (e.g. fish) that may lead to falsely identified free space or dynamic objects which trajectory planners may react to undesirably. We propose combining feature-based stereo matching with learning-based segmentation to produce a more robust obstacle map. This approach considers direct binary learning of the presence or absence of underwater obstacles, as well as a multiclass learning approach to classify their distance (near, mid and far) in the scene. An enhancement to the binary map is also shown by including depth information from sparse stereo matching to produce 3D obstacle maps of the scene. The performance is evaluated using field data collected in cluttered, and at times, visually degraded coral reef environments. The results show improved image-wide obstacle detection, rejection of transient objects (such as fish), and range estimation compared to feature-based sparse and dense stereo point clouds alone.


Title: Control from the Cloud: Edge Computing, Services and Digital Shadow for Automation Technologies*
Key Words: agile manufacturing  cloud computing  embedded systems  product development  production engineering computing  edge computing  digital shadow  agile product development  production systems  automation pyramid  interconnected cyber physical systems  adaptive process control  life cycle data management  Cloud computing  Automation  Computer architecture  Edge computing  Production  Process control  Manufacturing 
Abstract: Due to agile product development, production systems have to be flexible and adaptable to meet high quality standards and a high productivity. As a result, set-up processes has to be shorter and more resilient because of the increasing number of variants. To meet future requirements, the processes need to be self-adaptive and reconfigurable at any time. Nowadays, a shift of the automation pyramid to interconnected cyber physical systems can be observed as well as emerging technologies as cloud and edge computing are introduced to production systems. These technologies in combination with the Digital Shadow, which provides information about all production assets, open up potential for an adaptive process control and an overall life cycle data management. For this, the Digital Shadow has to be used not only for the aggregation of data, but also for pushing data back into the system and to control the process. As a result, services in regard to an architecture based on edge computing as an enabling technology for an adaptive production together with the Digital Shadow are presented, implemented and discussed on the basis of an industrial use case.


Title: The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes
Key Words: object detection  object tracking  optical radar  optical scanners  road traffic  stereo image processing  large-scale 3D point cloud dataset  crowded urban scenes  Honda Research Institute 3D Dataset  tracking dataset  3D LiDAR scanner  highly interactive traffic scenes  H3D dataset  Three-dimensional displays  Automobiles  Laser radar  Labeling  Robot sensing systems  Global Positioning System  Object detection 
Abstract: 3D multi-object detection and tracking are crucial for traffic scene understanding. However, the community pays less attention to these areas due to the lack of a standardized benchmark dataset to advance the field. Moreover, existing datasets (e.g., KITTI [1]) do not provide sufficient data and labels to tackle challenging scenes where highly interactive and occluded traffic participants are present. To address the issues, we present the Honda Research Institute 3D Dataset (H3D), a large-scale full-surround 3D multi-object detection and tracking dataset collected using a 3D LiDAR scanner. H3D comprises of 160 crowded and highly interactive traffic scenes with a total of 1 million labeled instances in 27,721 frames. With unique dataset size, rich annotations, and complex scenes, H3D is gathered to stimulate research on full-surround 3D multi-object detection and tracking. To effectively and efficiently annotate a large-scale 3D point cloud dataset, we propose a labeling methodology to speed up the overall annotation cycle. A standardized benchmark is created to evaluate full-surround 3D multi-object detection and tracking algorithms. 3D object detection and tracking algorithms are trained and tested on H3D. Finally, sources of errors are discussed for the development of future algorithms.


Title: Dense 3D Visual Mapping via Semantic Simplification
Key Words: data visualisation  image reconstruction  image segmentation  mesh generation  robot vision  solid modelling  semantic image segmentation  perceived point cloud  global statistics  class boundaries  infra-class edges  3D dense model  3D Delaunay Triangulation  variable point cloud density  semantic simplification  dense 3D visual mapping estimates  dense point clouds  pixel depths  point cloud simplification methods  roughly planar surface  Three-dimensional displays  Semantics  Solid modeling  Image reconstruction  Image segmentation  Structure from motion  Surface reconstruction 
Abstract: Dense 3D visual mapping estimates as many as possible pixel depths, for each image. This results in very dense point clouds that often contain redundant and noisy information, especially for surfaces that are roughly planar, for instance, the ground or the walls in the scene. In this paper we leverage on semantic image segmentation to discriminate which regions of the scene require simplification and which should be kept at high level of details. We propose four different point cloud simplification methods which decimate the perceived point cloud by relying on class-specific local and global statistics still maintaining more points in the proximity of class boundaries to preserve the infra-class edges and discontinuities. 3D dense model is obtained by fusing the point clouds in a 3D Delaunay Triangulation to deal with variable point cloud density. In the experimental evaluation we have shown that, by leveraging on semantics, it is possible to simplify the model and diminish the noise affecting the point clouds.


Title: Dense Surface Reconstruction from Monocular Vision and LiDAR
Key Words: cameras  graph theory  image reconstruction  mobile robots  optical radar  pipelines  robot vision  stereo image processing  surface reconstruction  LiDAR measurements  multiview stereo pipeline  watertight surface mesh  state-of-the-art camera-only  LiDAR-only reconstruction methods  monocular vision  surface reconstruction pipeline  monocular camera images  moving sensor rig  indoor scenes  indoor environments  3D mesh models  state-of-the-art multiview stereo  graph cut algorithm  Laser radar  Cameras  Three-dimensional displays  Image reconstruction  Surface reconstruction  Pipelines  Robot sensing systems 
Abstract: In this work, we develop a new surface reconstruction pipeline that combines monocular camera images and LiDAR measurements from a moving sensor rig to reconstruct dense 3D mesh models of indoor scenes. For surface reconstruction, the 3D LiDAR and camera are widely deployed for gathering geometric information from environments. Current state-of-the-art multi-view stereo or LiDAR-only reconstruction methods cannot reconstruct indoor environments accurately due to shortcomings of each sensor type. In our approach, LiDAR measurements are integrated into a multi-view stereo pipeline for point cloud densification and tetrahedralization. In addition to that, a graph cut algorithm is utilized to generate a watertight surface mesh. Because our proposed method leverages the complementary nature of these two sensors, the accuracy and completeness of the output model are improved. The experimental results on real world data show that our method significantly outperforms both the state-of-the-art camera-only and LiDAR-only reconstruction methods in accuracy and completeness.


Title: Robust low-overlap 3-D point cloud registration for outlier rejection
Key Words: feature extraction  image registration  iterative methods  Markov processes  stereo image processing  hidden Markov random field model  iterative closest point algorithm  outlier rejection  3D point cloud registration  Hidden Markov models  Three-dimensional displays  Cloud computing  Probabilistic logic  Robot sensing systems  Solid modeling  Measurement 
Abstract: When registering 3-D point clouds it is expected that some points in one cloud do not have corresponding points in the other cloud. These non-correspondences are likely to occur near one another, as surface regions visible from one sensor pose are obscured or out of frame for another. In this work, a hidden Markov random field model is used to capture this prior within the framework of the iterative closest point algorithm. The EM algorithm is used to estimate the distribution parameters and learn the hidden component memberships. Experiments are presented demonstrating that this method outperforms several other outlier rejection methods when the point clouds have low or moderate overlap.


Title: Discrete Rotation Equivariance for Point Cloud Recognition
Key Words: feature extraction  image recognition  learning (artificial intelligence)  discrete rotation equivariance  point cloud recognition  point clouds  deep networks  deep learning architecture  rotation group  rotated inputs  point cloud based networks  Three-dimensional displays  Two dimensional displays  Task analysis  Feature extraction  Robots  Deep learning  Computer architecture 
Abstract: Despite the recent active research on processing point clouds with deep networks, few attention has been on the sensitivity of the networks to rotations. In this paper, we propose a deep learning architecture that achieves discrete SO(2)/SO(3) rotation equivariance for point cloud recognition. Specifically, the rotation of an input point cloud with elements of a rotation group is similar to shuffling the feature vectors generated by our approach. The equivariance is easily reduced to invariance by eliminating the permutation with operations such as maximum or average. Our method can be directly applied to any existing point cloud based networks, resulting in significant improvements in their performance for rotated inputs. We show state-of-the-art results in the classification tasks with various datasets under both SO(2) and SO(3) rotations. In addition, we further analyze the necessary conditions of applying our approach to PointNet [1] based networks.


Title: MVX-Net: Multimodal VoxelNet for 3D Object Detection
Key Words: cameras  image colour analysis  image fusion  learning (artificial intelligence)  neural nets  object detection  stereo image processing  MVX-net  multimodal VoxelNet  3D object detection  neural network architectures  point cloud data  point cloud modalities  state-of-the-art multimodal algorithms  3D detection categories  simple single stage network  early-fusion approach  VoxelNet architecture  PointFusion  VoxelFusion  RGB modalities  KITTI dataset  birds eye view  Three-dimensional displays  Two dimensional displays  Feature extraction  Laser radar  Object detection  Proposals  Fuses 
Abstract: Many recent works on 3D object detection have focused on designing neural network architectures that can consume point cloud data. While these approaches demonstrate encouraging performance, they are typically based on a single modality and are unable to leverage information from other modalities, such as a camera. Although a few approaches fuse data from different modalities, these methods either use a complicated pipeline to process the modalities sequentially, or perform late-fusion and are unable to learn interaction between different modalities at early stages. In this work, we present PointFusion and VoxelFusion: two simple yet effective early-fusion approaches to combine the RGB and point cloud modalities, by leveraging the recently introduced VoxelNet architecture. Evaluation on the KITTI dataset demonstrates significant improvements in performance over approaches which only use point cloud data. Furthermore, the proposed method provides results competitive with the state-of-the-art multimodal algorithms, achieving top-2 ranking in five of the six birds eye view and 3D detection categories on the KITTI benchmark, by using a simple single stage network.


Title: Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data
Key Words: CAD  convolutional neural nets  image coding  image colour analysis  image enhancement  image resolution  image segmentation  learning (artificial intelligence)  masks  object tracking  category-agnostic instance segmentation  hand-labeled data  object tracking  automated dataset generation  network training  computer vision research  RGB imaging  synthetic depth data sensors  unknown object segmentation  SD mask R-CNN  high-resolution synthetic depth imaging  synthetic depth mask R-CNN  unknown 3D object segmentation  3D CAD models  domain randomization  point cloud clustering baselines  COCO benchmarks  hand-labeled RGB datasets  instance-specific grasping pipeline  synthetic training dataset  Image segmentation  Training  Solid modeling  Three-dimensional displays  Robots  Cameras  Grasping 
Abstract: The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.


