total paper: 65
Title: FMD Stereo SLAM: Fusing MVG and Direct Formulation Towards Accurate and Fast Stereo SLAM
Key Words: feature extraction  motion estimation  pose estimation  SLAM (robots)  stereo image processing  key-feature-based multiple view geometry  global map  3D structure  bundle adjustment  fast stereo SLAM  direct formulation  local map  constant motion model  direct-based formulation  novel stereo visual SLAM framework  FMD stereo SLAM  back-end process  stereo constraint  reprojection error minimization  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Feature extraction  Visualization  Robot vision systems  Two dimensional displays 
Abstract: We propose a novel stereo visual SLAM framework considering both accuracy and speed at the same time. The framework makes full use of the advantages of key-feature-based multiple view geometry (MVG) and direct-based formulation. At the front-end, our system performs direct formulation and constant motion model to predict a robust initial pose, reprojects local map to find 3D-2D correspondence and finally refines pose by the reprojection error minimization. This frontend process makes our system faster. At the back-end, MVG is used to estimate 3D structure. When a new keyframe is inserted, new mappoints are generated by triangulating. In order to improve the accuracy of the proposed system, bad mappoints are removed and a global map is kept by bundle adjustment. Especially, the stereo constraint is performed to optimize the map. This back-end process makes our system more accurate. Experimental evaluation on EuRoC dataset shows that the proposed algorithm can run at more than 100 frames per second on a consumer computer while achieving highly competitive accuracy.


Title: ScalableFusion: High-resolution Mesh-based Real-time 3D Reconstruction
Key Words: cameras  image colour analysis  image reconstruction  image resolution  image sensors  SLAM (robots)  sensor resolution  colorization  multiscale memory management process  high resolution global shutter camera  memory management approach  high-resolution mesh-based real-time 3D reconstruction  dense 3D reconstructions  robot applications  color resolution  depth resolution  surface reconstruction  surface texture  geometrical fidelity  RGB-D based reconstructions  PrimeSense RGB-D camera  Surface reconstruction  Image color analysis  Robot sensing systems  Geometry  Three-dimensional displays  Surface texture  Image reconstruction 
Abstract: Dense 3D reconstructions generate globally consistent data of the environment suitable for many robot applications. Current RGB-D based reconstructions, however, only maintain the color resolution equal to the depth resolution of the used sensor. This firmly limits the precision and realism of the generated reconstructions. In this paper we present a real-time approach for creating and maintaining a surface reconstruction in as high as possible geometrical fidelity with full sensor resolution for its colorization (or surface texture). A multi-scale memory management process and a Level of Detail scheme enable equally detailed reconstructions to be generated at small scales, such as objects, as well as large scales, such as rooms or buildings. We showcase the benefit of this novel pipeline with a PrimeSense RGB-D camera as well as combining the depth channel of this camera with a high resolution global shutter camera. Further experiments show that our memory management approach allows us to scale up to larger domains that are not achievable with current state-of-the-art methods.


Title: GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization and Mapping
Key Words: cameras  collision avoidance  convolutional neural nets  learning (artificial intelligence)  mobile robots  pose estimation  robot vision  SLAM (robots)  depth estimation system  GEN-SLAM  generative modeling  Deep Learning based system  obstacle avoidance  mobile robot  conventional geometric SLAM  single camera  topological map  camera image  topological location estimation  monocular localization  monocular simultaneous localization and mapping  Cameras  Image reconstruction  Simultaneous localization and mapping  Decoding  Training 
Abstract: We present a Deep Learning based system for the twin tasks of localization and obstacle avoidance essential to any mobile robot. Our system learns from conventional geometric SLAM, and outputs, using a single camera, the topological pose of the camera in an environment, and the depth map of obstacles around it. We use a CNN to localize in a topological map, and a conditional VAE to output depth for a camera image, conditional on this topological location estimation. We demonstrate the effectiveness of our monocular localization and depth estimation system on simulated and real datasets.


Title: RESLAM: A real-time robust edge-based SLAM system
Key Words: cameras  edge detection  image colour analysis  image representation  image sensors  motion estimation  optimisation  pose estimation  robot vision  SLAM (robots)  camera intrinsics  sliding window  edge-based verification  RESLAM  camera motions  RGBD sensors  sparse representation  SLAM pipeline  robust edge-based SLAM system  simultaneous localization and mapping  Image edge detection  Cameras  Simultaneous localization and mapping  Optimization  Real-time systems  Microsoft Windows 
Abstract: Simultaneous Localization and Mapping is a key requirement for many practical applications in robotics. In this work, we present RESLAM, a novel edge-based SLAM system for RGBD sensors. Due to their sparse representation, larger convergence basin and stability under illumination changes, edges are a promising alternative to feature-based or other direct approaches. We build a complete SLAM pipeline with camera pose estimation, sliding window optimization, loop closure and relocalisation capabilities that utilizes edges throughout all steps. In our system, we additionally refine the initial depth from the sensor, the camera poses and the camera intrinsics in a sliding window to increase accuracy. Further, we introduce an edge-based verification for loop closures that can also be applied for relocalisation. We evaluate RESLAM on wide variety of benchmark datasets that include difficult scenes and camera motions and also present qualitative results. We show that this novel edge-based SLAM system performs comparable to state-of-the-art methods, while running in real-time on a CPU. RESLAM is available as open-source software1.


Title: On-line 3D active pose-graph SLAM based on key poses using graph topology and sub-maps
Key Words: autonomous aerial vehicles  computational complexity  graph theory  mobile robots  optimisation  path planning  remotely operated vehicles  robot vision  SLAM (robots)  graph topology  pose-graph simultaneous localization  three-dimensional environments  D-optimality metrics  weighted node degree  T-optimality metric  sampling-based path  continuous-time trajectory optimization method  large-scale active SLAM problems  submap joining method  online 3D active pose-graph SLAM  Simultaneous localization and mapping  Measurement  Trajectory  Planning  Three-dimensional displays  Uncertainty 
Abstract: In this paper, we present an on-line active pose-graph simultaneous localization and mapping (SLAM) frame-work for robots in three-dimensional (3D) environments using graph topology and sub-maps. This framework aims to find the best trajectory for loop-closure by re-visiting old poses based on the T-optimality and D-optimality metrics of the Fisher information matrix (FIM) in pose-graph SLAM. In order to reduce computational complexity, graph topologies are introduced, including weighted node degree (T-optimality metric) and weighted tree-connectivity (D-optimality metric), to choose a candidate trajectory and several key poses. With the help of the key poses, a sampling-based path planning method and a continuous-time trajectory optimization method are combined hierarchically and applied in the whole framework. So as to further improve the real-time capability of the method, the sub-map joining method is used in the estimation and planning process for large-scale active SLAM problems. In simulations and experiments, we validate our approach by comparing against existing methods, and we demonstrate the on-line planning part using a quad-rotor unmanned aerial vehicle (UAV).


Title: Robust Object-based SLAM for High-speed Autonomous Navigation
Key Words: cameras  helicopters  image sequences  image texture  mobile robots  object detection  path planning  robot vision  SLAM (robots)  ROSHAN  object-level mapping  ellipsoid-based SLAM  object surface  autonomous quadrotor  bounding box detections  median shape error  forward-moving camera sequence  planar constraint  vehicle motions  semantic knowledge  robust object-based SLAM for high-speed autonomous navigation  Ellipsoids  Image edge detection  Semantics  Simultaneous localization and mapping  Cameras  Shape  Shape measurement 
Abstract: We present Robust Object-based SLAM for High-speed Autonomous Navigation (ROSHAN), a novel approach to object-level mapping suitable for autonomous navigation. In ROSHAN, we represent objects as ellipsoids and infer their parameters using three sources of information - bounding box detections, image texture, and semantic knowledge - to overcome the observability problem in ellipsoid-based SLAM under common forward-translating vehicle motions. Each bounding box provides four planar constraints on an object surface and we add a fifth planar constraint using the texture on the objects along with a semantic prior on the shape of ellipsoids. We demonstrate ROSHAN in simulation where we outperform the baseline, reducing the median shape error by 83% and the median position error by 72% in a forward-moving camera sequence. We demonstrate similar qualitative result on data collected on a fast-moving autonomous quadrotor.


Title: MH-iSAM2: Multi-hypothesis iSAM using Bayes Tree and Hypo-tree
Key Words: Bayes methods  data structures  mobile robots  optimisation  SLAM (robots)  original Bayes tree  hypothesis pruning strategy  multihypothesis iSAM  nonlinear incremental optimization algorithm  MH-iSAM2  simultaneous localization and mapping problems  SLAM problems  hypo-tree  multihypothesis inference  data structures  Simultaneous localization and mapping  Zirconium  Maximum likelihood estimation  Optimization  Inference algorithms  Robustness 
Abstract: A novel nonlinear incremental optimization algorithm MH-iSAM2 is developed to handle ambiguity in simultaneous localization and mapping (SLAM) problems in a multi-hypothesis fashion. It can output multiple possible solutions for each variable according to the ambiguous inputs, which is expected to greatly enhance the robustness of autonomous systems as a whole. The algorithm consists of two data structures: an extension of the original Bayes tree that allows efficient multi-hypothesis inference, and a Hypo-tree that is designed to explicitly track and associate the hypotheses of each variable as well as all the inference processes for optimization. With our proposed hypothesis pruning strategy, MH-iSAM2 enables fast optimization and avoids the exponential growth of hypotheses. We evaluate MH-iSAM2 using both simulated datasets and real-world experiments, demonstrating its improvements on the robustness and accuracy of SLAM systems.


Title: Fast and Robust Initialization for Visual-Inertial SLAM
Key Words: accelerometers  gyroscopes  inertial navigation  mobile robots  robot vision  SLAM (robots)  visual-inertial SLAM  VI-SLAM  gyroscope  initialization method  visual-inertial bundle adjustment  Cameras  Gravity  Simultaneous localization and mapping  Observability  Feature extraction  Jacobian matrices  Accelerometers 
Abstract: Visual-inertial SLAM (VI-SLAM) requires a good initial estimation of the initial velocity, orientation with respect to gravity and gyroscope and accelerometer biases. In this paper we build on the initialization method proposed by Martinelli [1] and extended by Kaiser et al. [2], modifying it to be more general and efficient. We improve accuracy with several rounds of visual-inertial bundle adjustment, and robustify the method with novel observability and consensus tests, that discard erroneous solutions. Our results on the EuRoC dataset show that, while the original method produces scale errors up to 156%, our method is able to consistently initialize in less than two seconds with scale errors around 5%, which can be further reduced to less than 1% performing visual-inertial bundle adjustment after ten seconds.


Title: Efficient Constellation-Based Map-Merging for Semantic SLAM
Key Words: covariance matrices  SLAM (robots)  tree searching  local information  SLAM graph  expensive recovery  system covariance matrix  joint compatibility  search space  clique-based pairwise compatibility  robust object-based loop-closure  SLAM problems  semantic SLAM  data association  high-confidence loop-closure mechanism  object-level SLAM  landmark uncertainty  constellation-based map-merging  branch-and-bound max-cardinality search  Simultaneous localization and mapping  Semantics  Covariance matrices  Uncertainty  Detectors  Search problems  Current measurement 
Abstract: Data association in SLAM is fundamentally challenging, and handling ambiguity well is crucial to achieve robust operation in real-world environments. When ambiguous measurements arise, conservatism often mandates that the measurement is discarded or a new landmark is initialized rather than risking an incorrect association. To address the inevitable “duplicate” landmarks that arise, we present an efficient map-merging framework to detect duplicate constellations of landmarks, providing a high-confidence loop-closure mechanism well-suited for object-level SLAM. This approach uses an incrementally-computable approximation of landmark uncertainty that only depends on local information in the SLAM graph, avoiding expensive recovery of the full system covariance matrix. This enables a search based on geometric consistency (GC) (rather than full joint compatibility (JC)) that inexpensively reduces the search space to a handful of “best” hypotheses. Furthermore, we reformulate the commonly-used interpretation tree to allow for more efficient integration of clique-based pairwise compatibility, accelerating the branch-and-bound max-cardinality search. Our method is demonstrated to match the performance of full JC methods at significantly-reduced computational cost, facilitating robust object-based loop-closure over large SLAM problems.


Title: Closed-loop MPC with Dense Visual SLAM - Stability through Reactive Stepping
Key Words: approximation theory  closed loop systems  humanoid robots  legged locomotion  mobile robots  motion control  nonlinear control systems  path planning  pendulums  position control  predictive control  SLAM (robots)  stability  fundamental capacity  external inputs  reference velocity  footstep plans  reference motion  external disturbances  closed-loop MPC scheme  proprioceptive sensors  imperfect open-loop control execution  HRP-4 humanoid robot  reactive stepping  walking gaits  humanoid locomotion  model predictive control  pendulum  dense visual SLAM stability  Simultaneous localization and mapping  Foot  Legged locomotion  Humanoid robots  Lips 
Abstract: Walking gaits generated using Model Predictive Control (MPC) is widely used due to its capability to handle several constraints that characterize humanoid locomotion. The use of simplified models such as the Linear Inverted Pendulum allows to perform computations in real-time, giving the robot the fundamental capacity to replan its motion to follow external inputs (e.g. reference velocity, footstep plans). However, usually the MPC does not take into account the current state of the robot when computing the reference motion, losing the ability to react to external disturbances. In this paper a closed-loop MPC scheme is proposed to estimate the robot's real state through Simultaneous Localization and Mapping (SLAM) and proprioceptive sensors (force/torque). With the proposed control scheme it is shown that the robot is able to react to external disturbances (push), by stepping to recover from the loss of balance. Moreover the localization allows the robot to navigate to target positions in the environment without being affected by the drift generated by imperfect open-loop control execution. We validate the proposed scheme through two different experiments with a HRP-4 humanoid robot.


Title: Robot Localization Based on Aerial Images for Precision Agriculture Tasks in Crop Fields
Key Words: autonomous aerial vehicles  crops  data visualisation  feature extraction  mobile robots  path planning  robot vision  SLAM (robots)  robot localization  aerial images  precision agriculture tasks  crop field environment  visual aliasing  localization system  aerial map  visual ambiguity problem  autonomous robots  Agriculture  Feature extraction  Cameras  Semantics  Visualization  Robot vision systems 
Abstract: Localization is a pre-requisite for most autonomous robots. For example, to carry out precision agriculture tasks effectively, a robot must be able to localize itself accurately in crop fields. The crop field environment presents unique challenges such as the highly repetitive structure of the crops leading to visual aliasing as well as the continuously changing appearance of the field, which makes it difficult to localize over time. In this paper, we present a localization system, which uses an aerial map of the field and exploits the semantic information of the crops, weeds, and their stem positions to resolve the visual ambiguity problem and to enable robot localization over extended periods of time. We evaluate our approach on a real field over multiple sessions spanning several weeks. Experiments suggest that our approach provides the necessary accuracy required by precision agriculture applications and works in cases where current techniques using typical visual features tend to fail.


Title: Visual Appearance Analysis of Forest Scenes for Monocular SLAM
Key Words: autonomous aerial vehicles  mobile robots  path planning  remotely operated vehicles  robot vision  SLAM (robots)  managed forests  tree health  SLAM research  structured human environments  unstructured forests  forest data  photorealistic simulated forest  straightforward forest terrain  forest scenes  natural scenes  visual appearance analysis  cheap energy efficient way  unmanned aerial vehicles  monocular SLAM systems  SLAM systems  visual appearance statistics  Forestry  Simultaneous localization and mapping  Cameras  Visualization  Vegetation  Feature extraction  Lighting 
Abstract: Monocular simultaneous localisation and mapping (SLAM) is a cheap and energy efficient way to enable Unmanned Aerial Vehicles (UAVs) to safely navigate managed forests and gather data crucial for monitoring tree health. SLAM research, however, has mostly been conducted in structured human environments, and as such is poorly adapted to unstructured forests. In this paper, we compare the performance of state of the art monocular SLAM systems on forest data and use visual appearance statistics to characterise the differences between forests and other environments, including a photorealistic simulated forest. We find that SLAM systems struggle with all but the most straightforward forest terrain and identify key attributes (lighting changes and in-scene motion) which distinguish forest scenes from “classic” urban datasets. These differences offer an insight into what makes forests harder to map and open the way for targeted improvements. We also demonstrate that even simulations that look impressive to the human eye can fail to properly reflect the difficult attributes of the environment they simulate, and provide suggestions for more closely mimicking natural scenes.


Title: Visual SLAM: Why Bundle Adjust?
Key Words: cameras  feature extraction  image sequences  motion estimation  optimisation  pose estimation  robot vision  SLAM (robots)  video signal processing  bundle adjustment  feature-based monocular SLAM  camera orientation optimisation  camera position estimation  quasiconvex formulation  keyframe rate  SLAM optimisation  rotational motion  slow motion  SLAM algorithm  3D structure estimation  input feature tracks  3D point cloud  3D map estimation  6DOF camera trajectory estimation  visual SLAM  Simultaneous localization and mapping  Cameras  Estimation  Bundle adjustment  Optimization  Visualization 
Abstract: Bundle adjustment plays a vital role in feature-based monocular SLAM. In many modern SLAM pipelines, bundle adjustment is performed to estimate the 6DOF camera trajectory and 3D map (3D point cloud) from the input feature tracks. However, two fundamental weaknesses plague SLAM systems based on bundle adjustment. First, the need to carefully initialise bundle adjustment means that all variables, in particular the map, must be estimated as accurately as possible and maintained over time, which makes the overall algorithm cumbersome. Second, since estimating the 3D structure (which requires sufficient baseline) is inherent in bundle adjustment, the SLAM algorithm will encounter difficulties during periods of slow motion or pure rotational motion. We propose a different SLAM optimisation core: instead of bundle adjustment, we conduct rotation averaging to incrementally optimise only camera orientations. Given the orientations, we estimate the camera positions and 3D points via a quasi-convex formulation that can be solved efficiently and globally optimally. Our approach not only obviates the need to estimate and maintain the positions and 3D map at keyframe rate (which enables simpler SLAM systems), it is also more capable of handling slow motions or pure rotational motions.


Title: Illumination Robust Monocular Direct Visual Odometry for Outdoor Environment Mapping
Key Words: distance measurement  image colour analysis  lighting  mobile robots  motion estimation  robot vision  SLAM (robots)  stereo image processing  illumination-robust direct monocular SLAM system  global lighting changes  local lighting changes  stereo SLAM systems  camera motion  scene structure  high-precision motion estimation  illumination robust monocular direct visual odometry  outdoor environment  illumination invariant photometric costs  vision-based localization and mapping  RGB-D  DSO system  ORBSLAM2 system  Lighting  Optimization  Robustness  Simultaneous localization and mapping  Cameras  Motion estimation  Three-dimensional displays 
Abstract: Vision-based localization and mapping in outdoor environments is still a challenging issue, which requests significant robustness against various unpredictable illumination changes. In this paper, an illumination-robust direct monocular SLAM system that focuses on modeling outdoor scenery is presented. To deal with global and local lighting changes, such as solar flares, the state-of-art illumination invariant photometric costs for RGB-D and stereo SLAM systems are revisited in the context of their monocular counterpart, where the camera motion and scene structure are jointly optimized with a reasonably poor initialization. Based on our analysis, a combined cost is proposed to achieve a high-precision motion estimation with an improved convergence radius. The proposed system is extensively evaluated on the synthetic and real-world datasets regarding accuracy, robustness, and processing time, where our approach outperforms systems with other costs and state-of-art DSO and ORBSLAM2 systems.


Title: A Comparison of CNN-Based and Hand-Crafted Keypoint Descriptors
Key Words: convolutional neural nets  image matching  neurocontrollers  robot vision  SLAM (robots)  pre-trained CNN descriptors  viewpoint changes  illumination changes  hand-crafted keypoint descriptors  keypoint matching  computer vision  keypoint description  trained convolutional neural networks  pre-trained CNNs  hand-crafted descriptors  visual simultaneous localization and mapping  CNN-based descriptors  SLAM  Lighting  Computational modeling  Detectors  Dogs  Simultaneous localization and mapping  Measurement 
Abstract: Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo condition changes such as those in illumination and viewpoint. Previous research in keypoint description has pursued three classes of descriptors: hand-crafted, those from trained convolutional neural networks (CNN), and those from pre-trained CNNs. This paper provides a comparative study of the three classes of keypoint descriptors, in terms of their ability to handle conditional changes. The study is conducted on the latest benchmark datasets in computer vision with challenging conditional changes. Our study finds that (a) in general CNN-based descriptors outperform hand-crafted descriptors, (b) the trained CNN descriptors perform better than pre-trained CNN descriptors with respect to viewpoint changes, and (c) pre-trained CNN descriptors perform better than trained CNN descriptors with respect to illumination changes. These findings can serve as a basis for selecting appropriate keypoint descriptors for various applications.


Title: Environment Driven Underwater Camera-IMU Calibration for Monocular Visual-Inertial SLAM
Key Words: autonomous underwater vehicles  calibration  cameras  inertial navigation  mobile robots  SLAM (robots)  visual-inertial SLAM  shallow water  calibration errors  environmental indexes  underwater camera-inertial measurement unit  simultaneous localization and mapping  intrinsic parameters  extrinsic parameters  underwater monocular vision systems  environment driven underwater camera-IMU calibration  Cameras  Calibration  Atmospheric modeling  Simultaneous localization and mapping  Geometry  Mathematical model 
Abstract: Most state-of-the-art underwater vision systems are calibrated manually in shallow water and used in open seas without changing. However, the refractivity of the water is adaptively changed depending on the salinity, temperature, depth or other underwater environmental indexes, which inevitably generate the calibration errors and induces incorrectness e.g., for underwater Simultaneously Localization and Mapping (SLAM). To address this issue, in this paper, we propose a new underwater Camera-Inertial Measurement Unit (IMU) calibration model, which just needs to be calibrated once in the air, and then both the intrinsic parameters and extrinsic parameters between the camera and IMU could be automatically calculated depending on the environment indexes. To our best knowledge, this is the first work to consider the underwater Camera-IMU calibration via environmental indexes. We also build a verification platform to validate the effectiveness of our proposed method on real experiments, and use it for underwater monocular Visual-Inertial SLAM.


Title: Leveraging Structural Regularity of Atlanta World for Monocular SLAM
Key Words: Kalman filters  maximum likelihood estimation  polynomials  SLAM (robots)  multiple local Atlanta frames  global Atlanta frames  world frame  structural regularity  monocular SLAM  common vertical axis  multiple horizontal axes  camera frame  Simultaneous localization and mapping  Three-dimensional displays  Cameras  Estimation  Optimization  Reliability 
Abstract: A wide range of man-made environments can be abstracted as the Atlanta world. It consists of a set of Atlanta frames with a common vertical (gravitational) axis and multiple horizontal axes orthogonal to this vertical axis. This paper focuses on leveraging the regularity of Atlanta world for monocular SLAM. First, we robustly cluster image lines. Based on these clusters, we compute the local Atlanta frames in the camera frame by solving polynomial equations. Our method provides the global optimum and satisfies inherent geometric constraints. Second, we define the posterior probabilities to refine the initial clusters and Atlanta frames alternately by the maximum a posteriori estimation. Third, based on multiple local Atlanta frames, we compute the global Atlanta frames in the world frame using Kalman filtering. We optimize rotations by the global alignment and then refine translations and 3D line-based map under the directional constraints. Experiments on both synthesized and real data have demonstrated that our approach outperforms state-of-the-art methods.


Title: Multimodal Semantic SLAM with Probabilistic Data Association
Key Words: image fusion  image representation  inference mechanisms  mobile robots  object detection  path planning  probability  robot vision  SLAM (robots)  nonGaussian sensor model  multimodal semantic SLAM  probabilistic data association  robot navigation  semantic SLAM problem  discrete inference problem  object class labels  measurement-landmark correspondences  continuous inference problem  robot poses  object detection systems  simultaneous localization and mapping  object-based representations  object locations  nonGaussian inference problem  Simultaneous localization and mapping  Semantics  Belief propagation  Maximum likelihood estimation  Maximum likelihood detection 
Abstract: The recent success of object detection systems motivates object-based representations for robot navigation; i.e. semantic simultaneous localization and mapping (SLAM). The semantic SLAM problem can be decomposed into a discrete inference problem: determining object class labels and measurement-landmark correspondences (the data association problem), and a continuous inference problem: obtaining the set of robot poses and object locations in the environment. A solution to the semantic SLAM problem necessarily addresses this joint inference, but under ambiguous data associations this is in general a non-Gaussian inference problem, while the majority of previous work focuses on Gaussian inference. Previous solutions to data association either produce solutions between potential hypotheses or maintain multiple explicit hypotheses for each association. We propose a solution that represents hypotheses as multiple modes of an equivalent non-Gaussian sensor model. We then solve the resulting non-Gaussian inference problem using nonparametric belief propagation. We validate our approach in a simulated hallway environment under a variety of sensor noise characteristics, as well as using real data from the KITTI dataset, demonstrating improved robustness to perceptual aliasing and odometry uncertainty.


Title: Oriented Point Sampling for Plane Detection in Unorganized Point Clouds
Key Words: image reconstruction  image segmentation  image sensors  object detection  octrees  robot vision  SLAM (robots)  solid modelling  unorganized point clouds  crucial pre-processing step  point cloud segmentation  organized point clouds  plane hypotheses  unoriented points  efficient plane detection method  semantic mapping  SLAM  plane detection methods  OPS  oriented point sampling  Three-dimensional displays  Sun  Surface treatment  Clustering algorithms  Octrees  Estimation  Image segmentation 
Abstract: Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efficiently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efficient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset.


Title: Spatial change detection using voxel classification by normal distributions transform
Key Words: image classification  image colour analysis  image sensors  mobile robots  normal distribution  object detection  optical scanners  robot vision  SLAM (robots)  stereo image processing  transforms  voxel classification  robotic applications  mobile robot  3D laser scanner  grid data  ND voxels  normal distributions transform  spatial change detection  onboard RGB-D camera  stereo camera  real-time range sensors  real-time localization  Three-dimensional displays  Mobile robots  Cameras  Real-time systems  Measurement by laser beam  Sensors 
Abstract: Detection of spatial change around a robot is indispensable in several robotic applications, such as search and rescue, security, and surveillance. The present paper proposes a fast spatial change detection technique for a mobile robot using an on-board RGB-D/stereo camera and a highly precise 3D map created by a 3D laser scanner. This technique first converts point clouds in a map and measured data to grid data (ND voxels) using normal distributions transform and classifies the ND voxels into three categories. The voxels in the map and the measured data are then compared according to the category and features of the ND voxels. Overlapping and voting techniques are also introduced in order to detect the spatial changes more robustly. We conducted experiments using a mobile robot equipped with real-time range sensors to confirm the performance of the proposed real-time localization and spatial change detection techniques in indoor and outdoor environments.


Title: Tightly Coupled 3D Lidar Inertial Odometry and Mapping
Key Words: distance measurement  image fusion  mobile robots  motion estimation  optical radar  pose estimation  robot vision  SLAM (robots)  fast motion conditions  ego-motion estimation  mobile robotic applications  sensor fusion  stand-alone sensors  tightly coupled lidar-IMU fusion method  IMU measurements  lidarIMU odometry  lidar measurement  rotation-constrained refinement algorithm  LIO-mapping  sensor pair  IMU update rate  lidar pose estimation  Laser radar  Three-dimensional displays  Estimation  Robot sensing systems  Feature extraction  Optimization 
Abstract: Ego-motion estimation is a fundamental requirement for most mobile robotic applications. By sensor fusion, we can compensate the deficiencies of stand-alone sensors and provide more reliable estimations. We introduce a tightly coupled lidar-IMU fusion method in this paper. By jointly minimizing the cost derived from lidar and IMU measurements, the lidarIMU odometry (LIO) can perform well with considerable drifts after long-term experiment, even in challenging cases where the lidar measurement can be degraded. Besides, to obtain more reliable estimations of the lidar poses, a rotation-constrained refinement algorithm (LIO-mapping) is proposed to further align the lidar poses with the global map. The experiment results demonstrate that the proposed method can estimate the poses of the sensor pair at the IMU update rate with high precision, even under fast motion conditions or with insufficient features.


Title: Point Cloud Compression for 3D LiDAR Sensor using Recurrent Neural Network with Residual Blocks
Key Words: computational geometry  data compression  image coding  iterative methods  mobile robots  octrees  optical radar  recurrent neural nets  SLAM (robots)  recurrent neural network  residual blocks  generic octree point cloud compression method  potential application scenarios  decompressed point cloud data  3D LiDAR sensor  autonomous driving systems  3D LiDAR data  raw D formatted LiDAR data  2D formatted LiDAR data  Three-dimensional displays  Image coding  Laser radar  Two dimensional displays  Robot sensing systems  Decoding  Recurrent neural networks 
Abstract: The use of 3D LiDAR, which has proven its capabilities in autonomous driving systems, is now expanding into many other fields. The sharing and transmission of point cloud data from 3D LiDAR sensors has broad application prospects in robotics. However, due to the sparseness and disorderly nature of this data, it is difficult to compress it directly into a very low volume. A potential solution is utilizing raw LiDAR data. We can rearrange the raw data from each frame losslessly in a 2D matrix, making the data compact and orderly. Due to the special structure of 3D LiDAR data, the texture of the 2D matrix is irregular, in contrast to 2D matrices of camera images. In order to compress this raw, 2D formatted LiDAR data efficiently, in this paper we propose a method which uses a recurrent neural network and residual blocks to progressively compress one frame's information from 3D LiDAR. Compared to our previous image compression based method and generic octree point cloud compression method, the proposed approach needs much less volume while giving the same decompression accuracy. Potential application scenarios for point cloud compression are also considered in this paper. We describe how decompressed point cloud data can be used with SLAM (simultaneous localization and mapping) as well as for localization using a given map, illustrating potential uses of the proposed method in real robotics applications.


Title: Non-Gaussian SLAM utilizing Synthetic Aperture Sonar
Key Words: acoustic signal processing  array signal processing  graph theory  marine navigation  pose estimation  probability  sensor fusion  SLAM (robots)  synthetic aperture sonar  SLAM framework  beacon position  acoustic measurements  factor graph formulation  nonGaussian SLAM  spatial resolution  navigational measurements  underwater missions  synthetic aperture sonar  SAS  simultaneous localization and mapping  accurate pose estimation  hydrophones acoustic data  empirical probability distribution  conventional beamformer  autonomous surface vehicle  Acoustics  Synthetic aperture sonar  Array signal processing  Simultaneous localization and mapping  Apertures  Receivers  Sonar navigation 
Abstract: Synthetic Aperture Sonar (SAS) is a technique to improve the spatial resolution from a moving set of receivers by extending the array in time, increasing the effective array length and aperture. This technique is limited by the accuracy of the receiver position estimates, necessitating highly accurate, typically expensive aided-inertial navigation systems for submerged platforms. We leverage simultaneous localization and mapping to fuse acoustic and navigational measurements and obtain accurate pose estimates even without the benefit of absolute positioning for lengthy underwater missions. We demonstrate a method of formulating the well-known SAS problem in a SLAM framework, using acoustic data from hydrophones to simultaneously estimate platform and beacon position. An empirical probability distribution is computed from a conventional beamformer to correctly account for uncertainty in the acoustic measurements. The non-parametric method relieves the familiar Gaussian-only assumption currently used in the localization and mapping discipline and fits effectively into a factor graph formulation with conventional factors such as ground-truth priors and odometry. We present results from field experiments performed on the Charles River with an autonomous surface vehicle which demonstrate simultaneous localization of an unknown acoustic beacon and vehicle positioning, and provide comparison to GPS ground truths.


Title: Underwater Terrain Reconstruction from Forward-Looking Sonar Imagery
Key Words: feature extraction  Gaussian processes  graph theory  image reconstruction  mobile robots  remotely operated vehicles  SLAM (robots)  sonar imaging  terrain mapping  underwater vehicles  terrain reconstruction  forward-looking sonar imagery  underwater simultaneous localization  multibeam imaging sonar  3D terrain mapping tasks  elevation angle information  data association  accurate 3D mapping  Euclidean space  optical flow  bearing-range images  subsea terrain  Gaussian Process random field  terrain factors  factor graph  terrain elevation estimate  variable-elevation tank environment  smooth height estimate  sonar images  Chow-Liu tree  extracted feature tracking  Feature extraction  Sonar measurements  Simultaneous localization and mapping  Three-dimensional displays  Gaussian processes  Imaging 
Abstract: In this paper, we propose a novel approach for underwater simultaneous localization and mapping using a multibeam imaging sonar for 3D terrain mapping tasks. The high levels of noise and the absence of elevation angle information in sonar images present major challenges for data association and accurate 3D mapping. Instead of repeatedly projecting extracted features into Euclidean space, we apply optical flow within bearing-range images for tracking extracted features. To deal with degenerate cases, such as when tracking is interrupted by noise, we model the subsea terrain as a Gaussian Process random field on a Chow-Liu tree. Terrain factors are incorporated into the factor graph, aimed at smoothing the terrain elevation estimate. We demonstrate the performance of our proposed algorithm in a simulated environment, which shows that terrain factors effectively reduce estimation error. We also show ROV experiments performed in a variable-elevation tank environment, where we are able to construct a descriptive and smooth height estimate of the tank bottom.


Title: A Linear-Complexity EKF for Visual-Inertial Navigation with Loop Closures
Key Words: computational complexity  correlation methods  inertial navigation  Kalman filters  mobile robots  nonlinear filters  robot vision  SLAM (robots)  Schmidt-MSCKF  real-time visual-inertial navigation  bounded-error performance  robotic applications  visual-inertial localization  loop closure constraints  long-term persistent navigation  Schmidt-Kalman formulation  multistate constraint Kalman filter framework  state vector  linear-complexity EKF  cross-correlations  navigation states  computational complexity  performance improvement  Navigation  Microsoft Windows  Standards  Current measurement  Real-time systems  Trajectory  Three-dimensional displays 
Abstract: Enabling real-time visual-inertial navigation in unknown environments while achieving bounded-error performance holds great potentials in robotic applications. To this end, in this paper, we propose a novel linear-complexity EKF for visual-inertial localization, which can efficiently utilize loop closure constraints, thus allowing for long-term persistent navigation. The key idea is to adapt the Schmidt-Kalman formulation within the multi-state constraint Kalman filter (MSCKF) framework, in which we selectively include keyframes as nuisance parameters in the state vector for loop closures but do not update their estimates and covariance in order to save computations while still tracking their cross-correlations with the current navigation states. As a result, the proposed Schmidt-MSCKF has only O(n) computational complexity while still incorporating loop closures into the system. The proposed approach is validated extensively on large-scale real-world experiments, showing significant performance improvements when compared to the standard MSCKF, while only incurring marginal computational overhead.


Title: KO-Fusion: Dense Visual SLAM with Tightly-Coupled Kinematic and Odometric Tracking
Key Words: cameras  distance measurement  image colour analysis  image fusion  image texture  manipulator kinematics  mobile robots  motion estimation  robot vision  SLAM (robots)  stereo image processing  KO-fusion  dense visual SLAM methods  observer  visual information  SLAM systems  inertial measurements  dense RGB-D SLAM system  wheeled robot  kinematic data  odometric data  kinematic measurements  tightly-coupled kinematic  odometric tracking  manipulator  odometry measurements  motion estimation  Cameras  Simultaneous localization and mapping  Robot vision systems  Kinematics  Manipulators 
Abstract: Dense visual SLAM methods are able to estimate the 3D structure of an environment and locate the observer within them. They estimate the motion of a camera by matching visual information between consecutive frames, and are thus prone to failure under extreme motion conditions or when observing texture-poor regions. The integration of additional sensor modalities has shown great promise in improving the robustness and accuracy of such SLAM systems. In contrast to the popular use of inertial measurements we propose to tightly-couple a dense RGB-D SLAM system with kinematic and odometry measurements from a wheeled robot equipped with a manipulator. The system has real-time capability while running on GPU. It optimizes the camera pose by considering the geometric alignment of the map as well as kinematic and odometric data from the robot. Through experimentation in the real-world, we show that the system is more robust to challenging trajectories featuring fast and loopy motion than the equivalent system without the additional kinematic and odometric knowledge, whilst retaining comparable performance to the equivalent RGB-D only system on easy trajectories.


Title: DeepFusion: Real-Time Dense 3D Reconstruction for Monocular SLAM using Single-View Depth and Gradient Predictions
Key Words: cameras  computerised instrumentation  graphics processing units  image fusion  image reconstruction  learning (artificial intelligence)  minimisation  neurocontrollers  photometry  probability  SLAM (robots)  stereo image processing  target tracking  depth cameras  CNN  DeepFusion  semidense multiview stereo algorithm  gradient predictions  monocular SLAM  single-view depth  keypoint-based maps  camera tracking  dense 3D reconstructions  convolutional neural network  sparse monocular simultaneous localisation and mapping systems  real-time dense 3D reconstruction system  photometric error minimization  GPU  probability  Image reconstruction  Uncertainty  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Real-time systems  Robot vision systems 
Abstract: While the keypoint-based maps created by sparse monocular Simultaneous Localisation and Mapping (SLAM) systems are useful for camera tracking, dense 3D reconstructions may be desired for many robotic tasks. Solutions involving depth cameras are limited in range and to indoor spaces, and dense reconstruction systems based on minimising the photometric error between frames are typically poorly constrained and suffer from scale ambiguity. To address these issues, we propose a 3D reconstruction system that leverages the output of a Convolutional Neural Network (CNN) to produce fully dense depth maps for keyframes that include metric scale. Our system, DeepFusion, is capable of producing real-time dense reconstructions on a GPU. It fuses the output of a semi-dense multiview stereo algorithm with the depth and gradient predictions of a CNN in a probabilistic fashion, using learned uncertainties produced by the network. While the network only needs to be run once per keyframe, we are able to optimise for the depth map with each new frame so as to constantly make use of new geometric constraints. Based on its performance on synthetic and real world datasets, we demonstrate that DeepFusion is capable of performing at least as well as other comparable systems.


Title: Evaluating the Effectiveness of Perspective Aware Planning with Panoramas
Key Words: image colour analysis  image sensors  object detection  path planning  robot vision  SLAM (robots)  binary coverage  goal selection strategy  image morphology  search space  CSQMI  perspective aware planning  information based exploration strategy  high resolution 3D maps  RGBD panoramas  angle enhanced occupancy grid  exploration strategy  maximal Cauchy-Schwarz quadratic mutual information  logging image control  Skeleton  Mutual information  Task analysis  Cameras  Robot vision systems 
Abstract: In this work, we present an information based exploration strategy tailored for the generation of high resolution 3D maps. We employ RGBD panoramas because they have been shown to provide memory efficient high quality representations of space. Robots explore the environment by selecting locations with maximal Cauchy-Schwarz Quadratic Mutual Information (CSQMI) computed on an angle enhanced occupancy grid to collect these RGBD panoramas. By employing the angle enhanced occupancy grid, the resulting exploration strategy emphasizes perspective in addition to binary coverage. Furthermore, the goal selection strategy is improved by using image morphology to reduce the search space over which CSQMI is computed. We present experimental results demonstrating the improved performance in perception related tasks by capturing panoramas using this approach, near frontier exploration, and a control of logging images at regular intervals while teleoperating the robot through the workspace. Collect imagery was passed through an object detection library with our perspective aware approach yielding a greater number of successful detections compared to near frontier exploration.


Title: Continuous Occupancy Map Fusion with Fast Bayesian Hilbert Maps
Key Words: Bayes methods  image fusion  mobile robots  multi-robot systems  SLAM (robots)  multirobot Hilbert Map systems  individual Fast-BHMs  decentralised manner  continuous representation  robot autonomy  traditional occupancy grid maps  continuous nature  continuous occupancy map fusion  fused fast-BHMs  global fast-BHM models  Bayesian Hilbert map models  fast Bayesian Hilbert maps  Bayes methods  Robots  Covariance matrices  Merging  Time complexity  Real-time systems  Training 
Abstract: Mapping the occupancy of an environment is central for robot autonomy. Traditional occupancy grid maps discretise the environment into independent cells, neglecting important spatial correlations, and are unable to capture the continuous nature of the real world. With these drawbacks of grid maps in mind, Hilbert Maps (HM) and more recently Bayesian Hilbert Maps (BHMs), were introduced as a continuous representation of the environment. In this paper we propose a method to merge Bayesian Hilbert Maps built by a team of robots in a decentralised manner. The training of BHMs requires the inversion of a large covariance matrix, incurring cubic complexity. We introduce an approximation, Fast Bayesian Hilbert Maps (Fast-BHM), which reduces the time complexity to below quadratic. Such speed-ups allow the building and merging of Bayesian Hilbert Map models to be practical, opening the door for multi-robot Hilbert Map systems which can be much faster and more robust than an individual robot. By merging several individual Fast-BHMs in a decentralised manner we obtain a unified model of the environment which is itself a Fast-BHM. We conduct experiments to show that global Fast-BHM models do not deteriorate after repeated merging and training. We then empirically demonstrate, due to its the compact representation, fused Fast-BHMs outperform fusion methods involving discretising continuous representations, when the amount of information communicated is limited.


Title: Enhancing V-SLAM Keyframe Selection with an Efficient ConvNet for Semantic Analysis
Key Words: convolutional neural nets  microprocessor chips  mobile robots  neural net architecture  object detection  robot vision  SLAM (robots)  video signal processing  image quality  semantic information  robotic systems  V-SLAM keyframe selection  semantic analysis  semantic image analysis  visual information  ConvNet  video  Visual-SLAM  CNN architecture  onboard CPU  Robots  Semantics  Task analysis  Computer architecture  Visualization  Image segmentation  Standards 
Abstract: Selecting relevant visual information from a video is a challenging task on its own and even more in robotics, due to strong computational restrictions. This work proposes a novel keyframe selection strategy based on image quality and semantic information, which boosts strategies currently used in Visual-SLAM (V-SLAM). Commonly used V-SLAM methods select keyframes based only on relative displacements and amount of tracked feature points. Our strategy to select more carefully these keyframes allows the robotic systems to make better use of them. With minimal computational cost, we show that our selection includes more relevant keyframes, which are useful for additional posterior recognition tasks, without penalizing the existing ones, mainly place recognition. A key ingredient is our novel CNN architecture to run a quick semantic image analysis at the onboard CPU of the robot. It provides sufficient accuracy significantly faster than related works. We demonstrate our hypothesis with several public datasets with challenging robotic data.


Title: Global Localization with Object-Level Semantics and Topology
Key Words: feature extraction  graph theory  image matching  image representation  pose estimation  stereo image processing  object-level representation  semantic object association  semantic-level point alignment  object-level semantics  appearance-based approach  3D dense semantics  semantic graph  vision-based global localization  topology  autonomous navigation  simultaneous localization and mapping  place recognition  6-DoF pose estimation  visual feature matching  Semantics  Three-dimensional displays  Topology  Simultaneous localization and mapping  Visualization  Cameras  Pose estimation 
Abstract: Global localization lies at the heart of autonomous navigation and Simultaneous Localization and Mapping (SLAM). The appearance-based approach has been successful, but still faces many open challenges in environments where visual conditions vary significantly over time. In this paper, we propose an integrated solution to leverage object-level dense semantics and spatial understanding of the environment for global localization. Our approach models an environment with 3D dense semantics, semantic graph and their topology. This object-level representation is then used for place recognition via semantic object association, followed by 6-DoF pose estimation by the semantic-level point alignment. Extensive experiments show that our approach can achieve robust global localization under extreme appearance changes. It is also capable of coping with other challenging scenarios, such as dynamic environments and incomplete query observations.


Title: CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction
Key Words: feature extraction  motion estimation  probability  SLAM (robots)  SVO mapping results  frame rate camera motion estimation  map points  initialized map point  depth uncertainty  single-image depth prediction network  feature location  probabilistic mapping method  direct pixel correspondence  semidirect visual odometry  V-SLAM algorithms  visual simultaneous localization  Uncertainty  Cameras  Visual odometry  Feature extraction  Reliability  Estimation  Motion estimation 
Abstract: Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that improved SVO mapping results in increased robustness and camera tracking accuracy. The implementation of this work is available at https: //github.com/yan99033/CNN-SVO.


Title: A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation
Key Words: image motion analysis  image segmentation  mobile robots  pose estimation  robot vision  SLAM (robots)  segmentation algorithms  semantic segmentation  robotics  refined 3D pose information  vision-based tasks  mutual improvement  unified framework  instantaneous motion change handling  long-term changes  simultaneous localization and segmentation  Image segmentation  Task analysis  Robot sensing systems  Motion segmentation  Feature extraction  Three-dimensional displays 
Abstract: This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.


Title: MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM
Key Words: cameras  image colour analysis  image motion analysis  image segmentation  image sequences  object detection  object tracking  octrees  pose estimation  probability  robot vision  SLAM (robots)  video signal processing  robustly track  foreground object probabilities  object model  object-level dynamic volumetric map  instance segmentation part  octree-based object-level multiinstance dynamic SLAM  multiinstance dynamic RGB-D SLAM system  robust camera tracking  geometric motion properties  geometric motion information  object-oriented tracking method  camera pose estimation  semantic motion properties  frequency 2.0 Hz to 3.0 Hz  Cameras  Simultaneous localization and mapping  Tracking  Motion segmentation  Semantics  Dynamics  Measurement uncertainty 
Abstract: We propose a new multi-instance dynamic RGB-D SLAM system using an object-level octree-based volumetric representation. It can provide robust camera tracking in dynamic environments and at the same time, continuously estimate geometric, semantic, and motion properties for arbitrary objects in the scene. For each incoming frame, we perform instance segmentation to detect objects and refine mask boundaries using geometric and motion information. Meanwhile, we estimate the pose of each existing moving object using an object-oriented tracking method and robustly track the camera pose against the static scene. Based on the estimated camera pose and object poses, we associate segmented masks with existing models and incrementally fuse corresponding colour, depth, semantic, and foreground object probabilities into each object model. In contrast to existing approaches, our system is the first system to generate an object-level dynamic volumetric map from a single RGB-D camera, which can be used directly for robotic tasks. Our method can run at 2-3 Hz on a CPU, excluding the instance segmentation part. We demonstrate its effectiveness by quantitatively and qualitatively testing it on both synthetic and real-world sequences.


Title: Surfel-Based Dense RGB-D Reconstruction With Global And Local Consistency
Key Words: computer vision  image colour analysis  image reconstruction  optimisation  pose estimation  surfel-based dense RGB-D reconstruction  local consistency  high surface reconstruction accuracy  dense mapping  vision communities  robotics literature  RGB-D cameras  dense map  depth input  accurate local pose estimation  locally consistent model  pose tracking  offline computer vision methods  structure-from-motion  multiview stereo  batch optimization  global consistency  heavy computation loads  consistent reconstruction  offline SfM pipeline  strong global constraints  off-the-shelf SLAM systems  high local accuracy  factor graph optimization  accurate camera  dense reconstruction  dense SLAM systems  SfM-MVS pipelines  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Pose estimation  Image reconstruction  Geometry 
Abstract: Achieving high surface reconstruction accuracy in dense mapping has been a desirable target for both robotics and vision communities. In the robotics literature, simultaneous localization and mapping (SLAM) systems use RGB-D cameras to reconstruct a dense map of the environment. They leverage the depth input to provide accurate local pose estimation and a locally consistent model. However, drift in the pose tracking over time leads to misalignments and artifacts. On the other hand, offline computer vision methods, such as the pipeline that combines structure-from-motion (SfM) and multi-view stereo (MVS), estimate the camera poses by performing batch optimization. These methods achieve global consistency, but suffer from heavy computation loads. We propose a novel approach that integrates both methods to achieve locally and globally consistent reconstruction. First, we estimate poses of keyframes in the offline SfM pipeline to provide strong global constraints at relatively low cost. Afterwards, we compute odometry between frames driven by off-the-shelf SLAM systems with high local accuracy. We fuse the two pose estimations using factor graph optimization to generate accurate camera poses for dense reconstruction. Experiments on real-world and synthetic datasets demonstrate that our approach produces more accurate models comparing to existing dense SLAM systems, while achieving significant speedup with respect to state-of-the-art SfM-MVS pipelines.


Title: A-SLAM: Human in-the-loop Augmented SLAM
Key Words: augmented reality  human-robot interaction  mobile robots  navigation  path planning  real-time systems  SLAM (robots)  telerobotics  A-SLAM  map editing  navigation-forbidden areas  navigation goals  SLAM algorithm  occupancy grid maps  human in-the-loop augmented SLAM  real environment representation  Microsoft HoloLens  robot teleoperation  pose correction  map correction  AR interface  Simultaneous localization and mapping  Navigation  Collaboration  Three-dimensional displays  Task analysis 
Abstract: In this work, we are proposing an intuitive Augmented SLAM method (A-SLAM) that allows the user to interact, in real-time, with a robot running SLAM to correct for pose and map errors. We built an AR application that works on HoloLens and allows the operator to view the robot's map superposed on the physical environment and edit it. Through map editing, the operator can account for errors affecting real environment's representation by adding navigation-forbidden areas to the map in addition to the ability to correct errors affecting the localization. The proposed system allows the operator to edit the robot's pose (based on SLAM request) and can be extended to sending navigation goals to the robot, viewing the planned path to evaluate it before execution, and teleoperating the robot. The proposed solution could be applied on any 2D-based SLAM algorithm and can easily be extended to 3D SLAM techniques. We validated our system through experimentation on pose correction and map editing. Experiments demonstrated that through A-SLAM, SLAM runtime is cut to half, post-processing of maps is totally eliminated, and high quality occupancy grid maps could be achieved with minimal added computational and hardware costs.


Title: Pose Graph optimization for Unsupervised Monocular Visual Odometry
Key Words: graph theory  neural nets  optimisation  pose estimation  unsupervised learning  pose graph optimization  unsupervised monocular visual odometry  unsupervised learning  label-free leaning ability  drift correction technique  large-scale odometry estimation  loop closure detection  hybrid VO system  NeuralBundler  temporal loss  spatial photometric loss  multiview 6DoF constraints  cycle consistency loss  global pose graph  local loop 6DoF constraints  KITTI odometry dataset  unsupervised monocular VO estimation  monocular SLAM systems  Optimization  Visual odometry  Simultaneous localization and mapping  Cameras  Training  Neural networks  Estimation 
Abstract: Unsupervised Learning based monocular visual odometry (VO) has lately drawn significant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for large-scale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler, and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efficient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.


Title: Learning ad-hoc Compact Representations from Salient Landmarks for Visual Place Recognition in Underwater Environments
Key Words: convolutional neural nets  feature extraction  image coding  image representation  mobile robots  object recognition  robot vision  SLAM (robots)  unsupervised learning  salient landmarks  visual place recognition  underwater environments  visual attention algorithm  hand-crafted local descriptors  ad hoc descriptor generator  convolutional autoencoder  ad-hoc compact representations  SeqSLAM  FAB-MAP  SURF method  Visualization  Feature extraction  Image color analysis  Training  Robots  Task analysis  Neural networks 
Abstract: In this paper, we propose an approach to learn compact representations from salient landmarks detected by a visual attention algorithm to recognize previously visited places in underwater environments. Instead of using hand-crafted local descriptors as it has been typically done in visual place recognition, we use a convolutional autoencoder to obtain an ad hoc descriptor generator from salient landmarks. The main advantage of using an autoencoder is that it can learn in an unsupervised manner directly from the salient landmarks. In addition, we show that it is possible to do the training with less than 100,000 examples instead of several hundreds of thousands or even millions of labeled examples as in other convolutional architectures. The trained convolutional autoencoder is used to obtain descriptors for salient landmarks that are later utilized in a voting scheme to calculate similarity between images with the objective of finding if a place has already been visited. The proposed method has obtained good results compared to SeqSLAM and FAB-MAP in different datasets obtained from robotic explorations of coral reefs in real life conditions. Moreover, when the visual attention algorithm is used, fewer features are required to get a good performance in terms of precision and recall compared when using the SURF method to extract visual features.


Title: A Variational Observation Model of 3D Object for Probabilistic Semantic SLAM
Key Words: Bayes methods  cameras  feature extraction  image colour analysis  inference mechanisms  object detection  pose estimation  probability  SLAM (robots)  variational techniques  probabilistic observation model  Bayesian inference  viewpoint-independent loop closure  variational observation model  Bayesian object observation model  3D object detection  probabilistic semantic SLAM  single view projection  RGB monocamera  object-oriented feature extraction  3D mapping  volumetric 3D object shape information  variational likelihood estimation  pose estimation  feature estimation  loop detector  Shape  Simultaneous localization and mapping  Three-dimensional displays  Object oriented modeling  Solid modeling  Semantics 
Abstract: We present a Bayesian object observation model for complete probabilistic semantic SLAM. Recent studies on object detection and feature extraction have become important for scene understanding and 3D mapping. However, 3D shape of the object is too complex to formulate the probabilistic observation model; therefore, performing the Bayesian inference of the object-oriented features as well as their pose is less considered. Besides, when the robot equipped with an RGB mono camera only observes the projected single view of an object, a significant amount of the 3D shape information is abandoned. Due to these limitations, semantic SLAM and viewpoint-independent loop closure using volumetric 3D object shape is challenging. In order to enable the complete formulation of probabilistic semantic SLAM, we approximate the observation model of a 3D object with a tractable distribution. We also estimate the variational likelihood from the 2D image of the object to exploit its observed single view. In order to evaluate the proposed method, we perform pose and feature estimation, and demonstrate that the automatic loop closure works seamlessly without additional loop detector in various environments.


Title: Night-to-Day Image Translation for Retrieval-based Localization
Key Words: approximation theory  image retrieval  learning (artificial intelligence)  neural nets  object detection  pose estimation  robot vision  SLAM (robots)  visual databases  geo-tagged images  night-to-day image translation  retrieval-based localization  visual localization  robotics pipelines  image retrieval techniques  database image retrieval  neural models  pose estimation  visual query photo  ToDayGAN  Task analysis  Visualization  Cameras  Feature extraction  Training  Generators  Data models 
Abstract: Visual localization is a key step in many robotics pipelines, allowing the robot to (approximately) determine its position and orientation in the world. An efficient and scalable approach to visual localization is to use image retrieval techniques. These approaches identify the image most similar to a query photo in a database of geo-tagged images and approximate the query's pose via the pose of the retrieved database image. However, image retrieval across drastically different illumination conditions, e.g. day and night, is still a problem with unsatisfactory results, even in this age of powerful neural models. This is due to a lack of a suitably diverse dataset with true correspondences to perform end-to-end learning. A recent class of neural models allows for realistic translation of images among visual domains with relatively little training data and, most importantly, without ground-truth pairings.In this paper, we explore the task of accurately localizing images captured from two traversals of the same area in both day and night. We propose ToDayGAN - a modified image-translation model to alter nighttime driving images to a more useful daytime representation. We then compare the daytime and translated night images to obtain a pose estimate for the night image using the known 6-DOF position of the closest day image. Our approach improves localization performance by over 250% compared the current state-of-the-art, in the context of standard metrics in multiple categories.


Title: Tightly-Coupled Aided Inertial Navigation with Point and Plane Features
Key Words: feature extraction  image fusion  image sensors  inertial navigation  mobile robots  Monte Carlo methods  object tracking  SLAM (robots)  planar point features  nonplanar point features  point-on-plane constraints  effective plane feature initialization algorithm  depth sensor  general sensor fusion framework  point feature tracking  plane extraction  geometrical structures  closest point  plane parameterization  Monte-Carlo simulations  visual sensor  tightly-coupled aided inertial navigation system  feature-based simultaneous localization and mapping  Feature extraction  Cameras  Calibration  Laser radar  Jacobian matrices  Simultaneous localization and mapping  Estimation 
Abstract: This paper presents a tightly-coupled aided inertial navigation system (INS) with point and plane features, a general sensor fusion framework applicable to any visual and depth sensor (e.g., RGBD, LiDAR) configuration, in which the camera is used for point feature tracking and depth sensor for plane extraction. The proposed system exploits geometrical structures (planes) of the environments and adopts the closest point (CP) for plane parameterization. Moreover, we distinguish planar point features from non-planar point features in order to enforce point-on-plane constraints which are used in our state estimator, thus further exploiting structural information from the environment. We also introduce a simple but effective plane feature initialization algorithm for feature-based simultaneous localization and mapping (SLAM). In addition, we perform online spatial calibration between the IMU and the depth sensor as it is difficult to obtain this critical calibration parameter in high precision. Both Monte-Carlo simulations and real-world experiments are performed to validate the proposed approach.


Title: Robotic Forceps without Position Sensors using Visual SLAM
Key Words: cameras  mobile robots  position control  robot vision  servomechanisms  SLAM (robots)  visual servoing  robotic forceps  position sensors  visual SLAM  wrist joint  joint angle sensing  rear end  rear joint  parallel linkage  monocular camera  position sensing  joint angles  visual servo system  static experiments  dynamic positioning experiments  visual servoing system  forceps tip  Cameras  Robot vision systems  Visualization  Gravity 
Abstract: In this study, a robotic forceps with a wrist joint using visual SLAM for joint angle sensing was developed. The forceps has a flexible joint connected to the wrist joint at its rear end and the motion of the rear joint is driven by a parallel linkage. A monocular camera attached on the rear of the parallel linkage is in charge of position sensing, and the joint angles are estimated from the pose of the camera. The pose of the camera is obtained by a visual SLAM. The visual servo system realizes a simple attaching mechanism. The static and dynamic positioning experiments are conducted. We confirmed that the visual servoing system controls the forceps tip within the error of 3 deg in the motion range of 50 deg.


Title: 3D Keypoint Repeatability for Heterogeneous Multi-Robot SLAM
Key Words: feature extraction  mobile robots  multi-robot systems  robot vision  SLAM (robots)  point cloud registration  loop closure  heterogenous multirobot SLAM applications  NARF detector  3D keypoint repeatability  heterogeneous multirobot SLAM  multirobot SLAM scenario  sensor measurement point clouds  point density  3D keypoint detectors  multirobot SLAM system  KPQ-SI  relative repeatability  Three-dimensional displays  Detectors  Simultaneous localization and mapping  Cameras  Laser radar 
Abstract: For robots with different types of sensors, loop closure in a multi-robot SLAM scenario requires keypoints that can be matched between sensor measurement point clouds with different properties such as point density and noise. In this paper, we evaluate the performance of several 3D keypoint detectors (Harris3D, ISS, KPQ, KPQ-SI, and NARF) for repeatability between scans from different sensors towards building a heterogeneous multi-robot SLAM system. We find that KPQ-SI and NARF have the best relative repeatability, with KPQ-SI finding more keypoints overall and a higher number of repeatable keypoints, at the cost of significantly worse computational performance. In scans of the same area from different poses, both detectors find enough keypoints for point cloud registration and loop closure. For heterogenous multirobot SLAM applications with computational or bandwidth restrictions, the NARF detector consistently finds repeatable keypoints while also allowing for real-time performance.


Title: SLAMBench 3.0: Systematic Automated Reproducible Evaluation of SLAM Systems for Robot Vision Challenges and Scene Understanding
Key Words: control engineering computing  convolutional neural nets  image reconstruction  natural scenes  robot vision  SLAM (robots)  scene understanding  nonrigid environments  dynamic SLAM  SLAMBench 3  evaluation infrastructure  systematic automated reproducible evaluation  robot vision  visual SLAM  SLAM research area  visulation aids  visulation metrics  convolutional neural networks  dynamicfusion  Simultaneous localization and mapping  Semantics  Three-dimensional displays  Measurement  Benchmark testing  Heuristic algorithms  C++ languages 
Abstract: As the SLAM research area matures and the number of SLAM systems available increases, the need for frameworks that can objectively evaluate them against prior work grows. This new version of SLAMBench moves beyond traditional visual SLAM, and provides new support for scene understanding and non-rigid environments (dynamic SLAM). More concretely for dynamic SLAM, SLAMBench 3.0 includes the first publicly available implementation of DynamicFusion, along with an evaluation infrastructure. In addition, we include two SLAM systems (one dense, one sparse) augmented with convolutional neural networks for scene understanding, together with datasets and appropriate metrics. Through a series of use-cases, we demonstrate the newly incorporated algorithms, visulation aids and metrics (6 new metrics, 4 new datasets and 5 new algorithms).


Title: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation
Key Words: motion estimation  pose estimation  SLAM (robots)  photometric loss  self-supervised ego-motion estimation  accurate relative pose  SLAM  self-supervised learning framework  image depth  photometric error  systematic error  realistic scenes  geometric loss  matching loss  self-supervised framework  unsupervised egomotion estimation methods  Simultaneous localization and mapping  Estimation  Geometry  Cameras  Motion estimation  Visualization  Visual odometry 
Abstract: Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised egomotion estimation methods by a large margin. The code and data are available at https://github.com/hlzz/DeepMatchVO.


Title: Speeding Up Iterative Closest Point Using Stochastic Gradient Descent
Key Words: gradient methods  image colour analysis  image registration  iterative methods  optimisation  pose estimation  SLAM (robots)  stochastic gradient descent  3D laser scanners  RGB-D cameras  model registration  iterative closest point  SGD  convergence speed  3D point clouds  pose estimation  SLAM  optimisation problem  Three-dimensional displays  Standards  Stochastic processes  Euclidean distance  Cost function  Sensors 
Abstract: Sensors producing 3D point clouds such as 3D laser scanners and RGB-D cameras are widely used in robotics, be it for autonomous driving or manipulation. Aligning point clouds produced by these sensors is a vital component in such applications to perform tasks such as model registration, pose estimation, and SLAM. Iterative closest point (ICP) is the most widely used method for this task, due to its simplicity and efficiency. In this paper we propose a novel method which solves the optimisation problem posed by ICP using stochastic gradient descent (SGD). Using SGD allows us to improve the convergence speed of ICP without sacrificing solution quality. Experiments using Kinect as well as Velodyne data show that, our proposed method is faster than existing methods, while obtaining solutions comparable to standard ICP. An additional benefit is robustness to parameters when processing data from different sensors.


Title: Integral Backstepping Position Control for Quadrotors in Tunnel-Like Confined Environments
Key Words: aerodynamics  aerospace robotics  helicopters  Kalman filters  mechanical stability  mobile robots  pose estimation  position control  robot dynamics  robot vision  SLAM (robots)  tunnels  vision-based localisation  cross-sectional localisation system  integral backstepping controller  quadrotors  tunnel-like confined environments  integral backstepping position control  kinematic Kalman filter  semiautonomous system  flying robots  aerodynamic disturbances  Backstepping  Aerodynamics  Kinematics  Kalman filters  Navigation  Rail transportation  Sensors 
Abstract: There are many potential applications that require flying robots to navigate through tunnel-like environments, such as inspections of small railway culverts and mineral mappings of mining tunnels. Nevertheless, those environments present many challenges for quadrotors to navigate through. The aerodynamic disturbances created from the fluid interaction between the propellers' downwash and the surrounding surfaces of the environment, as well as longitudinal wind gusts, add hardship in stabilising the vehicle while the restricted narrow space increases the risk of collision. Furthermore, poor visibility and dust blown by the downwash make vision-based localisation extremely difficult. This paper presents a cross-sectional localisation system using Hough Scan Matching and a simple kinematic Kalman filter. Using the estimated state information, an integral backstepping controller is implemented which enables quadrotors to robustly fly in tunnel-like confined environments. A semi-autonomous system is proposed with self-stabilisation in the vertical and lateral axes while a pilot provides commands in the longitudinal direction. The results of a series of experiments in a simulated tunnel show that the proposed system successfully hovered itself and tracked various trajectories in a cross-sectional area without the aid of any external sensing or computing system.


Title: Multirotor dynamics based online scale estimation for monocular SLAM
Key Words: autonomous aerial vehicles  cameras  helicopters  image sensors  Kalman filters  mobile robots  nonlinear filters  observability  robot vision  SLAM (robots)  observability  online scale estimation  extended Kalman filter framework  multirotor dynamics model  monocular camera  metric sensor  conventional scale estimation methods  monocular SLAM  monocular vision  Cameras  Observability  Drag  Estimation  Force  Mathematical model  Vehicle dynamics 
Abstract: This paper proposes a novel method to estimate the scale online for monocular SLAM. Unlike conventional scale estimation methods that require a metric sensor such as an IMU and apriori knowledge of its biases, this approach estimates the scale online by solely using the monocular camera and multirotor dynamics model in an extended Kalman Filter framework. Further, we discuss the observability of scale and theoretically show that the scale becomes observable when multirotor dynamics model and monocular vision are used in conjunction. We validate our proposition with extensive experimentation on the local as well as on the standard datasets and compare the same with other state of the art methods.


Title: Redundant Perception and State Estimation for Reliable Autonomous Racing
Key Words: automobiles  driver information systems  image colour analysis  learning (artificial intelligence)  particle filtering (numerical methods)  pose estimation  SLAM (robots)  state estimation  vehicle dynamics  reliable autonomous racing  sensor failure  critical consequences  state estimation approaches  autonomous race car  track delimiting objects  sensor modalities  learning-based approaches  camera data  redundant perception inputs  probabilistic failure detection algorithm  real-world racing conditions  slip dynamics  particle filter based SLAM algorithm  pose estimates  velocity 90.0 km/h  Robot sensing systems  Cameras  Three-dimensional displays  Automobiles  Image color analysis  Laser radar  Reliability 
Abstract: In autonomous racing, vehicles operate close to the limits of handling and a sensor failure can have critical consequences. To limit the impact of such failures, this paper presents the redundant perception and state estimation approaches developed for an autonomous race car. Redundancy in perception is achieved by estimating the color and position of the track delimiting objects using two sensor modalities independently. Specifically, learning-based approaches are used to generate color and pose estimates, from LiDAR and camera data respectively. The redundant perception inputs are fused by a particle filter based SLAM algorithm that operates in real-time. Velocity is estimated using slip dynamics, with reliability being ensured through a probabilistic failure detection algorithm. The sub-modules are extensively evaluated in real-world racing conditions using the autonomous race car gotthard driverless, achieving lateral accelerations up to 1. 7G and a top speed of 90km/h.


Title: UWB/LiDAR Fusion For Cooperative Range-Only SLAM
Key Words: distance measurement  laser ranging  mobile robots  optical radar  SLAM (robots)  ultra wideband radar  wireless sensor networks  cooperative sensor network  2D LiDAR sensor  UWB-LiDAR fusion  UWB beacon nodes  peer-to-peer ranges  nearby objects-obstacles  surrounding environment  drift-free SLAM  mobile robot  2D laser rangefinder  ultra-wideband node  cooperative range-only SLAM  LiDAR-based SLAM algorithm  UWB ranging measurements  UWB-only localization accuracy  LiDAR mapping  laser scanning information  Laser radar  Peer-to-peer computing  Distance measurement  Simultaneous localization and mapping  Two dimensional displays 
Abstract: We equip an ultra-wideband (UWB) node and a 2D LiDAR sensor a.k.a. 2D laser rangefinder on a mobile robot, and place UWB beacon nodes at unknown locations in an unknown environment. All UWB nodes can do ranging with each other thus forming a cooperative sensor network. We propose to fuse the peer-to-peer ranges measured between UWB nodes and laser scanning information, i.e., range measured between robot and nearby objects/obstacles, for simultaneous localization of the robot, all UWB beacons and LiDAR mapping. The fusion is inspired by two facts: 1) LiDAR may improve UWB-only localization accuracy as it gives a more precise and comprehensive picture of the surrounding environment; 2) on the other hand, UWB ranging measurements may remove the error accumulated in the LiDAR-based SLAM algorithm. Our experiments demonstrate that UWB/LiDAR fusion enables drift-free SLAM in real-time based on ranging measurements only.


Title: Characterizing Visual Localization and Mapping Datasets
Key Words: motion estimation  rendering (computer graphics)  SLAM (robots)  Wasserstein distance  motion estimation algorithm  robotics SLAM benchmarking  visual localization  mapping algorithms  real-world trajectories  high-quality scenes  synthetic datasets  dense map  key SLAM applications  ground robotics  mapping datasets  motion estimation algorithms  computer vision  Simultaneous localization and mapping  Trajectory  Time measurement  Visualization  Benchmark testing 
Abstract: Benchmarking mapping and motion estimation algorithms is established practice in robotics and computer vision. As the diversity of datasets increases, in terms of the trajectories, models, and scenes, it becomes a challenge to select datasets for a given benchmarking purpose. Inspired by the Wasserstein distance, this paper addresses this concern by developing novel metrics to evaluate trajectories and the environments without relying on any SLAM or motion estimation algorithm. The metrics, which so far have been missing in the research community, can be applied to the plethora of datasets that exist. Additionally, to improve the robotics SLAM benchmarking, the paper presents a new dataset for visual localization and mapping algorithms. A broad range of real-world trajectories is used in very high-quality scenes and a rendering framework to create a set of synthetic datasets with ground-truth trajectory and dense map which are representative of key SLAM applications such as virtual reality (VR), micro aerial vehicle (MAV) flight, and ground robotics.


Title: Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments
Key Words: autonomous underwater vehicles  geophysical image processing  image representation  object detection  oceanographic techniques  probability  robot vision  SLAM (robots)  unsupervised learning  co-robotic exploration  bandwidth tunable technique  real-time probabilistic scene modeling  communication constrained environments  deep sea  scene complexity  bandwidth requirements  underwater robot  high-level semantic scene constructs  artificially constructed tank environment  science interests  unsupervised scene model impact  resulting scene model  coral reef  bandwidth constraints  scene maps streaming  Robot sensing systems  Bandwidth  Oceans  Data models  Visualization  Bayes methods 
Abstract: This paper proposes a bandwidth tunable technique for real-time probabilistic scene modeling and mapping to enable co-robotic exploration in communication constrained environments such as the deep sea. The parameters of the system enable the user to characterize the scene complexity represented by the map, which in turn determines the bandwidth requirements. The approach is demonstrated using an underwater robot that learns an unsupervised scene model of the environment and then uses this scene model to communicate the spatial distribution of various high-level semantic scene constructs to a human operator. Preliminary experiments in an artificially constructed tank environment as well as simulated missions over a 10m×10m coral reef using real data show the tunability of the maps to different bandwidth constraints and science interests. To our knowledge this is the first paper to quantity how the free parameters of the unsupervised scene model impact both the scientific utility of and bandwidth required to communicate the resulting scene model.


Title: WISDOM: WIreless Sensing-assisted Distributed Online Mapping
Key Words: least mean squares methods  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  WISDOM  spatial sensing  robotics  augmented reality  urban spaces  wireless access points  coarse orientation  average Root Mean Square mapping error  wireless sensing-assisted distributed online mapping  robot swarm  custom ICP algorithm  absolute trajectory error  average root mean square mapping error  size 0.2 m  size 1.3 m  Three-dimensional displays  Simultaneous localization and mapping  Robot kinematics  Visualization  Merging 
Abstract: Spatial sensing is a fundamental requirement for applications in robotics and augmented reality. In urban spaces such as malls, airports, apartments, and others, it is quite challenging for a single robot to map the whole environment. So, we employ a swarm of robots to perform the mapping. One challenge with this approach is merging sub-maps built by each robot. In this work, we use wireless access points, which are ubiquitous in most urban spaces, to provide us with coarse orientation between sub-maps, and use a custom ICP algorithm to refine this orientation to merge them. We demonstrate our approach with maps from a building on campus and evaluate it using two metrics. Our results show that, in the building we studied, we can achieve an average Absolute Trajectory error of 0.2m in comparison to a map created by a single robot and average Root Mean Square mapping error of 1.3m from ground truth landmark locations.


Title: UAV/UGV Autonomous Cooperation: UAV assists UGV to climb a cliff by attaching a tether
Key Words: autonomous aerial vehicles  collision avoidance  inertial navigation  mobile robots  off-road vehicles  robot vision  SLAM (robots)  Unmanned Aerial Vehicle  Unmanned Ground Vehicle  tether attachment device  steep terrain  tether anchoring  UGV autonomous cooperation  UAV autonomous cooperation  visual inertial navigation  collaborative navigation  3D voxel mapping  obstacle avoidance planning  traversability analysis  Robot sensing systems  Navigation  Three-dimensional displays  Unmanned aerial vehicles  Trajectory  Attitude control 
Abstract: This paper proposes a novel cooperative system for an Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV) which utilizes the UAV not only as a flying sensor but also as a tether attachment device. Two robots are connected with a tether, allowing the UAV to anchor the tether to a structure located at the top of a steep terrain, impossible to reach for UGVs. Thus, enhancing the poor traversability of the UGV by not only providing a wider range of scanning and mapping from the air, but also by allowing the UGV to climb steep terrains with the winding of the tether. In addition, we present an autonomous framework for the collaborative navigation and tether attachment in an unknown environment. The UAV employs visual inertial navigation with 3D voxel mapping and obstacle avoidance planning. The UGV makes use of the voxel map and generates an elevation map to execute path planning based on a traversability analysis. Furthermore, we compared the pros and cons of possible methods for the tether anchoring from multiple points of view. To increase the probability of successful anchoring, we evaluated the anchoring strategy with an experiment. Finally, the feasibility and capability of our proposed system were demonstrated by an autonomous mission experiment in the field with an obstacle and a cliff.


Title: Low-latency Visual SLAM with Appearance-Enhanced Local Map Building
Key Words: file organisation  image enhancement  image fusion  mobile robots  pose estimation  robot vision  SLAM (robots)  appearance-enhanced local map building  local map module  local map contents  co-visibility local map building  compact local map  downstream data association  mapped features  local map size  appearance-based local map building method  low-latency visual SLAM  pose estimation  multi-index hashing  online hash table selection algorithm  MIH  VO-VSLAM mean performance  Three-dimensional displays  Buildings  Optimization  Feature extraction  Indexing  Simultaneous localization and mapping 
Abstract: A local map module is often implemented in modern VO/VSLAM systems to improve data association and pose estimation. Conventionally, the local map contents are determined by co-visibility. While co-visibility is cheap to establish, it utilizes the relatively-weak temporal prior (i.e. seen before, likely to be seen now), therefore admitting more features into the local map than necessary. This paper describes an enhancement to co-visibility local map building by incorporating a strong appearance prior, which leads to a more compact local map and latency reduction in downstream data association. The appearance prior collected from the current image influences the local map contents: only the map features visually similar to the current measurements are potentially useful for data association. To that end, mapped features are indexed and queried with Multi-index Hashing (MIH). An online hash table selection algorithm is developed to further reduce the query overhead of MIH and the local map size. The proposed appearance-based local map building method is integrated into a state-of-the-art VO/VSLAM system. When evaluated on two public benchmarks, the size of the local map, as well as the latency of real-time pose tracking in VO/VSLAM are significantly reduced. Meanwhile, the VO/VSLAM mean performance is preserved or improves.


Title: Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities
Key Words: computational complexity  computational geometry  graph theory  mesh generation  mobile robots  optimisation  robot vision  SLAM (robots)  state estimation  structural regularities  point cloud representation  tightly couple mesh regularization  VIO optimization  per-frame approach  visual-inertial odometry algorithms  visual-inertial 3D mesh generation  decouple state estimation  factor-graph formulation  computational complexity  memory usage  localization accuracy  Three-dimensional displays  Optimization  Two dimensional displays  State estimation  Cameras  Histograms  Mesh generation  SLAM  Vision-Based Navigation  Sensor Fusion 
Abstract: Visual-Inertial Odometry (VIO) algorithms typically rely on a point cloud representation of the scene that does not model the topology of the environment. A 3D mesh instead offers a richer, yet lightweight, model. Nevertheless, building a 3D mesh out of the sparse and noisy 3D landmarks triangulated by a VIO algorithm often results in a mesh that does not fit the real scene. In order to regularize the mesh, previous approaches decouple state estimation from the 3D mesh regularization step, and either limit the 3D mesh to the current frame [1], [2] or let the mesh grow indefinitely [3], [4]. We propose instead to tightly couple mesh regularization and state estimation by detecting and enforcing structural regularities in a novel factor-graph formulation. We also propose to incrementally build the mesh by restricting its extent to the time-horizon of the VIO optimization; the resulting 3D mesh covers a larger portion of the scene than a per-frame approach while its memory usage and computational complexity remain bounded. We show that our approach successfully regularizes the mesh, while improving localization accuracy, when structural regularities are present, and remains operational in scenes without regularities.


Title: RCM-SLAM: Visual localisation and mapping under remote centre of motion constraints
Key Words: cameras  image motion analysis  image reconstruction  medical robotics  mobile robots  pose estimation  robot vision  SLAM (robots)  surgery  laparoscopic camera motion  RCM constraints  minimal solver  absolute camera  2D-3D point correspondences  bundle adjustment optimiser  RCM-constrained parameterisation  relative pose estimation  SLAM pipeline suitable  robotic surgery  RCM position  robotic prostatectomy show  RCM-SLAM  visual localisation  remote centre  motion constraints  insertion ports  Simultaneous Localisation and Mapping  mapping approach  RCM-PnP  Cameras  Robot vision systems  Simultaneous localization and mapping  Laparoscopes  Three-dimensional displays  Robot kinematics 
Abstract: In robotic surgery the motion of instruments and the laparoscopic camera is constrained by their insertion ports, i. e. a remote centre of motion (RCM). We propose a Simultaneous Localisation and Mapping (SLAM) approach that estimates laparoscopic camera motion under RCM constraints. To achieve this we derive a minimal solver for the absolute camera pose given two 2D-3D point correspondences (RCM-PnP) and also a bundle adjustment optimiser that refines camera poses within an RCM-constrained parameterisation. These two methods are used together with previous work on relative pose estimation under RCM [1] to assemble a SLAM pipeline suitable for robotic surgery. Our simulations show that RCM-PnP outperforms conventional PnP for a wide noise range in the RCM position. Results with video footage from a robotic prostatectomy show that RCM constraints significantly improve camera pose estimation.


Title: Guaranteed Globally Optimal Planar Pose Graph and Landmark SLAM via Sparse-Bounded Sums-of-Squares Programming
Key Words: graph theory  maximum likelihood estimation  mobile robots  navigation  nonlinear programming  path planning  polynomials  pose estimation  robot vision  SLAM (robots)  autonomous navigation  nonlinear optimization techniques  maximum likelihood estimate  robot trajectory  polynomial optimization programs  planar pose graph  landmark SLAM  sparse-bounded sums-of-squares programming  simultaneous localization and mapping  pose-graph SLAM problem  sum-of-squares convex  SOS convex  sparse bounded degree sum-of-squares optimization method  sparse-BSOS optimization method  Simultaneous localization and mapping  Optimization  Maximum likelihood estimation  Position measurement  Noise measurement  Transmission line matrix methods 
Abstract: Autonomous navigation requires an accurate model or map of the environment. While dramatic progress in the prior two decades has enabled large-scale simultaneous localization and mapping (SLAM), the majority of existing methods rely on non-linear optimization techniques to find the maximum likelihood estimate (MLE) of the robot trajectory and surrounding environment. These methods are prone to local minima and are thus sensitive to initialization. Several recent papers have developed optimization algorithms for the Pose-Graph SLAM problem that can certify the optimality of a computed solution. Though this does not guarantee a priori that this approach generates an optimal solution, a recent extension has shown that when the noise lies within a critical threshold that the solution to the optimization algorithm is guaranteed to be optimal. To address the limitations of existing approaches, this paper illustrates that the Pose-Graph SLAM and Landmark SLAM can be formulated as polynomial optimization programs that are sum-of-squares (SOS) convex. This paper then describes how the Pose-Graph and Landmark SLAM problems can be solved to a global minimum without initialization regardless of noise level using the sparse bounded degree sum-of-squares (Sparse-BSOS) optimization method. Finally, the superior performance of the proposed approach when compared to existing SLAM methods is illustrated on graphs with several hundred nodes.


Title: Visual-Inertial Navigation: A Concise Review
Key Words: augmented reality  image sensors  inertial navigation  mobile computing  mobile robots  robot vision  SLAM (robots)  inertial sensors  visual sensors  visual-inertial navigation systems  mobile augmented reality  aerial navigation  autonomous driving  VINS  SLAM  Cameras  Visualization  Navigation  Sensors  Acceleration  Noise measurement  Fuses 
Abstract: As inertial and visual sensors are becoming ubiquitous, visual-inertial navigation systems (VINS) have prevailed in a wide range of applications from mobile augmented reality to aerial navigation to autonomous driving, in part because of the complementary sensing capabilities and the decreasing costs and size of the sensors. In this paper, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work - which is unfortunately missing in the literature while being greatly demanded by researchers and engineers - in the hope to accelerate the VINS research and beyond in our society as a whole.


Title: Predicting the Layout of Partially Observed Rooms from Grid Maps
Key Words: control engineering computing  mobile robots  path planning  robot vision  SLAM (robots)  SLAM algorithms  2D metric grid map  global structure  partially observed rooms  autonomous mobile robots  indoor environments  robot sensors  geometrical primitives  Layout  Measurement  Robots  Indoor environment  Two dimensional displays  Three-dimensional displays  Feature extraction 
Abstract: In several applications, autonomous mobile robots benefit from knowing the structure of the indoor environments where they operate. This knowledge can be extracted from the metric maps built (e.g., using SLAM algorithms) from the data perceived by the robots' sensors. The layout is a way to represent the structure of an indoor environment with geometrical primitives. Most of the current methods for reconstructing the layout from a metric map represent the parts of the environment that have been fully observed. In this paper, we propose an approach that predicts the layout of rooms which are only partially known in a 2D metric grid map. The prediction is made according to the global structure of the environment, as identified from its known parts. Experiments show that our approach is able to effectively predict the layout of several indoor environments that have been observed to different degrees.


Title: Real-time Scalable Dense Surfel Mapping
Key Words: image fusion  image reconstruction  pose estimation  SLAM (robots)  intensity images  depth images  globally consistent model  room-scale environments  urban-scale environments  RGB-D cameras  stereo cameras  monocular camera  superpixel-based surfels  reconstructed models  fast map deformation  global consistency  room-scale reconstruction  time scalable dense surfel  CPU computation  sparse SLAM system  camera poses  dense surfel mapping system  Cameras  Image reconstruction  Strain  Fuses  Robot vision systems  Simultaneous localization and mapping  Three-dimensional displays 
Abstract: In this paper, we propose a novel dense surfel mapping system that scales well in different environments with only CPU computation. Using a sparse SLAM system to estimate camera poses, the proposed mapping system can fuse intensity images and depth images into a globally consistent model. The system is carefully designed so that it can build from room-scale environments to urban-scale environments using depth images from RGB-D cameras, stereo cameras or even a monocular camera. First, superpixels extracted from both intensity and depth images are used to model surfels in the system. superpixel-based surfels make our method both runtime efficient and memory efficient. Second, surfels are further organized according to the pose graph of the SLAM system to achieve O(1) fusion time regardless of the scale of reconstructed models. Third, a fast map deformation using the optimized pose graph enables the map to achieve global consistency in real-time. The proposed surfel mapping system is compared with other state-of-the-art methods on synthetic datasets. The performances of urban-scale and room-scale reconstruction are demonstrated using the KITTI dataset [1] and autonomous aggressive flights, respectively. The code is available for the benefit of the community.


Title: Semantic Mapping for View-Invariant Relocalization
Key Words: cameras  feature extraction  image representation  object detection  pose estimation  robot vision  SLAM (robots)  semantic mapping  view-invariant relocalization  accurate local tracking  view-invariant object-driven relocalization  sampling-based approach  2D bounding box object detections  view-invariant representation  camera relocalization  view-invariance  relocalization rate  visual simultaneous localization and mapping  object landmarks  local appearance-based features  SLAM  3D pose  SIFT  mean rotational error  Three-dimensional displays  Cameras  Simultaneous localization and mapping  Semantics  Two dimensional displays  Visualization  Task analysis 
Abstract: We propose a system for visual simultaneous localization and mapping (SLAM) that combines traditional local appearance-based features with semantically meaningful object landmarks to achieve both accurate local tracking and highly view-invariant object-driven relocalization. Our mapping process uses a sampling-based approach to efficiently infer the 3D pose of object landmarks from 2D bounding box object detections. These 3D landmarks then serve as a view-invariant representation which we leverage to achieve camera relocalization even when the viewing angle changes by more than 125 degrees. This level of view-invariance cannot be attained by local appearance-based features (e.g. SIFT) since the same set of surfaces are not even visible when the viewpoint changes significantly. Our experiments show that even when existing methods fail completely for viewpoint changes of more than 70 degrees, our method continues to achieve a relocalization rate of around 90%, with a mean rotational error of around 8 degrees.


Title: Real-Time Monocular Object-Model Aware Sparse SLAM
Key Words: cameras  convolutional neural nets  feature extraction  learning (artificial intelligence)  mobile robots  object detection  robot vision  SLAM (robots)  mobile robotics  sparse point-based SLAM methods  CNN-based plane detector  semantic SLAM  simultaneous localization and mapping  camera localization  deep-learned object detector  CNN network  semantic objects representation  monocular object-model aware sparse SLAM framework  Simultaneous localization and mapping  Semantics  Image reconstruction  Real-time systems  Cameras  Three-dimensional displays 
Abstract: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in mobile robotics. While sparse point-based SLAM methods provide accurate camera localization, the generated maps lack semantic information. On the other hand, state of the art object detection methods provide rich information about entities present in the scene from a single image. This work incorporates a real-time deep-learned object detector to the monocular SLAM framework for representing generic objects as quadrics that permit detections to be seamlessly integrated while allowing the real-time performance. Finer reconstruction of an object, learned by a CNN network, is also incorporated and provides a shape prior for the quadric leading further refinement. To capture the structure of the scene, additional planar landmarks are detected by a CNN-based plane detector and modelled as independent landmarks in the map. Extensive experiments support our proposed inclusion of semantic objects and planar structures directly in the bundle-adjustment of SLAM - Semantic SLAM- that enriches the reconstructed map semantically, while significantly improving the camera localization.


Title: Probabilistic Projective Association and Semantic Guided Relocalization for Dense Reconstruction
Key Words: convolutional neural nets  image coding  image recognition  image reconstruction  image representation  image retrieval  image segmentation  object detection  probability  robot vision  SLAM (robots)  convolutional neural networks  geometric quality  probabilistic projective association  loop frames  2D labeling  CNN  semantic prediction  simultaneous localization and mapping system  SLAM system  3D scenes  randomized ferns  semantic recognition  geometric reconstruction  loop detection  reconstruction pipeline  semantic information  camera trajectory estimation  semantic labels  real-time dense mapping system  dense reconstruction  semantic guided relocalization  Semantics  Probabilistic logic  Labeling  Two dimensional displays  Cameras  Tracking loops  Trajectory 
Abstract: We present a real-time dense mapping system which uses the predicted 2D semantic labels for optimizing the geometric quality of reconstruction. With a combination of Convolutional Neural Networks (CNNs) for 2D labeling and a Simultaneous Localization and Mapping (SLAM) system for camera trajectory estimation, recent approaches have succeeded in incrementally fusing and labeling 3D scenes. However, the geometric quality of the reconstruction can be further improved by incorporating such semantic prediction results, which is not sufficiently exploited by existing methods. In this paper, we propose to use semantic information to improve two crucial modules in the reconstruction pipeline, namely tracking and loop detection, for obtaining mutual benefits in geometric reconstruction and semantic recognition. Specifically for tracking, we use a novel probabilistic projective association approach to efficiently pick out candidate correspondences, where the confidence of these correspondences is quantified concerning similarities on all available short-term invariant features. For the loop detection, we incorporate these semantic labels into the original encoding through Randomized Ferns to generate a more comprehensive representation for retrieving candidate loop frames. Evaluations on a publicly available synthetic dataset have shown the effectiveness of our approach that considers such semantic hints as a reliable feature for achieving higher geometric quality.


Title: MRS-VPR: a multi-resolution sampling based global visual place recognition method
Key Words: image filtering  image matching  image recognition  image resolution  image sampling  image sequences  mobile robots  navigation  object recognition  particle filtering (numerical methods)  path planning  robot vision  SLAM (robots)  global visual place recognition method  multiresolution sampling  particle filter-based global sampling scheme  matching efficiency  brute-force sequential matching method  long-term localization  long-term visual navigation tasks  loop closure detection  MRS-VPR  SeqSLAM  Testing  Feature extraction  Indexes  Task analysis  Trajectory  Visualization  Robots 
Abstract: Place recognition and loop closure detection are challenging for long-term visual navigation tasks. SeqSLAM is considered to be one of the most successful approaches to achieve long-term localization under varying environmental conditions and changing viewpoints. SeqSLAM uses a brute-force sequential matching method, which is computationally intensive. In this work, we introduce a multi-resolution sampling-based global visual place recognition method (MRS-VPR), which can significantly improve the matching efficiency and accuracy in sequential matching. The novelty of this method lies in the coarse-to-fine searching pipeline and a particle filter-based global sampling scheme, that can balance the matching efficiency and accuracy in the long-term navigation task. Moreover, our model works much better than SeqSLAM when the testing sequence is over a much smaller time scale than the reference sequence. Our experiments demonstrate that MRSVPR is efficient in locating short temporary trajectories within long-term reference ones without compromising on the accuracy compared to SeqSLAM.


