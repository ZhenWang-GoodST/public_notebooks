TY  - CONF
TI  - A Variational Observation Model of 3D Object for Probabilistic Semantic SLAM
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5866
EP  - 5872
AU  - H. W. Yu
AU  - J. Y. Moon
AU  - B. H. Lee
PY  - 2019
KW  - Bayes methods
KW  - cameras
KW  - feature extraction
KW  - image colour analysis
KW  - inference mechanisms
KW  - object detection
KW  - pose estimation
KW  - probability
KW  - SLAM (robots)
KW  - variational techniques
KW  - probabilistic observation model
KW  - Bayesian inference
KW  - viewpoint-independent loop closure
KW  - variational observation model
KW  - Bayesian object observation model
KW  - 3D object detection
KW  - probabilistic semantic SLAM
KW  - single view projection
KW  - RGB monocamera
KW  - object-oriented feature extraction
KW  - 3D mapping
KW  - volumetric 3D object shape information
KW  - variational likelihood estimation
KW  - pose estimation
KW  - feature estimation
KW  - loop detector
KW  - Shape
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Object oriented modeling
KW  - Solid modeling
KW  - Semantics
DO  - 10.1109/ICRA.2019.8794111
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We present a Bayesian object observation model for complete probabilistic semantic SLAM. Recent studies on object detection and feature extraction have become important for scene understanding and 3D mapping. However, 3D shape of the object is too complex to formulate the probabilistic observation model; therefore, performing the Bayesian inference of the object-oriented features as well as their pose is less considered. Besides, when the robot equipped with an RGB mono camera only observes the projected single view of an object, a significant amount of the 3D shape information is abandoned. Due to these limitations, semantic SLAM and viewpoint-independent loop closure using volumetric 3D object shape is challenging. In order to enable the complete formulation of probabilistic semantic SLAM, we approximate the observation model of a 3D object with a tractable distribution. We also estimate the variational likelihood from the 2D image of the object to exploit its observed single view. In order to evaluate the proposed method, we perform pose and feature estimation, and demonstrate that the automatic loop closure works seamlessly without additional loop detector in various environments.
ER  - 

TY  - CONF
TI  - Learning Action Representations for Self-supervised Visual Exploration
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5873
EP  - 5879
AU  - C. Oh
AU  - A. Cavallaro
PY  - 2019
KW  - cameras
KW  - image representation
KW  - learning (artificial intelligence)
KW  - learning action
KW  - self-supervised visual exploration
KW  - on-board camera
KW  - initial state
KW  - self-supervised prediction network
KW  - intrinsic rewards
KW  - current state-action pair
KW  - higher dimensional representations
KW  - representational power
KW  - transition network
KW  - sparse extrinsic rewards
KW  - camera view
KW  - input actions
KW  - action representation module
KW  - Training
KW  - Navigation
KW  - Task analysis
KW  - Visualization
KW  - Robots
KW  - Predictive models
KW  - Cameras
DO  - 10.1109/ICRA.2019.8794401
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Learning to efficiently navigate an environment using only an on-board camera is a difficult task for an agent when the final goal is far from the initial state and extrinsic rewards are sparse. To address this problem, we present a self-supervised prediction network to train the agent with intrinsic rewards that relate to achieving the desired final goal. The network learns to predict its future camera view (the future state) from a current state-action pair through an Action Representation Module that decodes input actions as higher dimensional representations. To increase the representational power of the network during exploration we fuse the responses from the Action Representation Module in the transition network, which predicts the future state. Moreover, to enhance the discrimination capability between predictions from different input actions we introduce joint regression and triplet ranking loss functions. We show that, despite the sparse extrinsic rewards, by learning action representations we achieve a faster training convergence than state-of-the-art methods with only a small increase in the number of the model parameters.
ER  - 

TY  - CONF
TI  - Plug-and-Play: Improve Depth Prediction via Sparse Data Propagation
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5880
EP  - 5886
AU  - T. Wang
AU  - F. Wang
AU  - J. Lin
AU  - Y. Tsai
AU  - W. Chiu
AU  - M. Sun
PY  - 2019
KW  - image colour analysis
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - optical radar
KW  - PnP module updates
KW  - intermediate feature map
KW  - sparse data propagation
KW  - RGB image
KW  - sparse LiDAR points
KW  - plug-and-play module
KW  - sparse depths
KW  - pretrained depth prediction model
KW  - dense depth map
KW  - Estimation
KW  - Training
KW  - Laser radar
KW  - Image reconstruction
KW  - Predictive models
KW  - Robot sensing systems
DO  - 10.1109/ICRA.2019.8794404
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We propose a novel plug-and-play (PnP) module for improving depth prediction with taking arbitrary patterns of sparse depths as input. Given any pre-trained depth prediction model, our PnP module updates the intermediate feature map such that the model outputs new depths consistent with the given sparse depths. Our method requires no additional training and can be applied to practical applications such as leveraging both RGB and sparse LiDAR points to robustly estimate dense depth map. Our approach achieves consistent improvements on various state-of-the-art methods on indoor (i.e., NYU-v2) and outdoor (i.e., KITTI) datasets. Various types of LiDARs are also synthesized in our experiments to verify the general applicability of our PnP module in practice.
ER  - 

TY  - CONF
TI  - DFNet: Semantic Segmentation on Panoramic Images with Dynamic Loss Weights and Residual Fusion Block
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5887
EP  - 5892
AU  - W. Jiang
AU  - Y. Wu
AU  - L. Guan
AU  - J. Zhao
PY  - 2019
KW  - feature extraction
KW  - image fusion
KW  - image segmentation
KW  - neural nets
KW  - object detection
KW  - roads
KW  - traffic engineering computing
KW  - visual perception
KW  - sight images
KW  - DFNet
KW  - dynamic loss weights
KW  - fusion layer
KW  - boundary information loss
KW  - semantic segmentation
KW  - automatic parking
KW  - lane markings
KW  - parking slots
KW  - pavement information
KW  - pixel multiplication
KW  - PSV dataset
KW  - residual fusion block
KW  - RFB
KW  - Image segmentation
KW  - Semantics
KW  - Feature extraction
KW  - Deep learning
KW  - Convolution
KW  - Training
KW  - Vehicle dynamics
DO  - 10.1109/ICRA.2019.8794476
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - For the domain of self-driving and automatic parking, perception is a basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. Compared with front sight images, panoramic images(PI) can capture more comprehensive pavement information. However, the imbalance of different classes in PI is even more serious. Additionally, the judgment of boundary information between areas is a hard problem in deep models. Therefore, we propose a new model named DFNet to solve these problems. The proposed model has two main contributions, one is dynamic loss weights, and the other is residual fusion block(RFB). DFNet use dynamic loss weights to overcome the negative effect of imbalance dataset, which are calculated according to the pixel number of each class in a batch. RFB is composed of several convolutional layers, a pooling layer, and a fusion layer to combine the feature maps by pixel multiplication, which can reduce boundary information loss. We evaluate our method on PSV dataset, and the achieved advanced results demonstrate the effectiveness of the proposed model.
ER  - 

TY  - CONF
TI  - Anytime Stereo Image Depth Estimation on Mobile Devices
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5893
EP  - 5900
AU  - Y. Wang
AU  - Z. Lai
AU  - G. Huang
AU  - B. H. Wang
AU  - L. van der Maaten
AU  - M. Campbell
AU  - K. Q. Weinberger
PY  - 2019
KW  - learning (artificial intelligence)
KW  - mobile computing
KW  - stereo image processing
KW  - end-to-end learned approach
KW  - inference time
KW  - mobile devices
KW  - stereo depth estimation
KW  - memory-constrained devices
KW  - disparity prediction
KW  - disparity maps
KW  - computational constraints
KW  - stereo image depth estimation
KW  - NVIDIA Jetson TX2 module
KW  - AnyNet
KW  - Estimation
KW  - Image resolution
KW  - Feature extraction
KW  - Computational modeling
KW  - Three-dimensional displays
KW  - Cameras
KW  - Predictive models
DO  - 10.1109/ICRA.2019.8794003
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process 1242×375 resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error - using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet.
ER  - 

TY  - CONF
TI  - Improved Generalization of Heading Direction Estimation for Aerial Filming Using Semi-Supervised Regression
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5901
EP  - 5907
AU  - W. Wang
AU  - A. Ahuja
AU  - Y. Zhang
AU  - R. Bonatti
AU  - S. Scherer
PY  - 2019
KW  - autonomous aerial vehicles
KW  - cinematography
KW  - image sequences
KW  - regression analysis
KW  - robot vision
KW  - unsupervised learning
KW  - video signal processing
KW  - improved generalization
KW  - semisupervised regression
KW  - visual input
KW  - data distributions
KW  - semisupervised algorithm
KW  - generalization ability
KW  - autonomous aerial filming
KW  - heading direction estimation problem
KW  - temporal continuity
KW  - unsupervised signal
KW  - testing performance
KW  - unlabeled sequences
KW  - performance improvement
KW  - labeled loss
KW  - unlabeled loss
KW  - moving actor filming
KW  - Task analysis
KW  - Drones
KW  - Training
KW  - Estimation
KW  - Data models
KW  - Cameras
KW  - Feature extraction
DO  - 10.1109/ICRA.2019.8793994
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In the task of Autonomous aerial filming of a moving actor (e.g. a person or a vehicle), it is crucial to have a good heading direction estimation for the actor from the visual input. However, the models obtained in other similar tasks, such as pedestrian collision risk analysis and human-robot interaction, are very difficult to generalize to the aerial filming task, because of the difference in data distributions. Towards improving generalization with less amount of labeled data, this paper presents a semi-supervised algorithm for heading direction estimation problem. We utilize temporal continuity as the unsupervised signal to regularize the model and achieve better generalization ability. This semi-supervised algorithm is applied to both training and testing phases, which increases the testing performance by a large margin. We show that by leveraging unlabeled sequences, the amount of labeled data required can be significantly reduced. We also discuss several important details on improving the performance by balancing labeled and unlabeled loss, and making good combinations. Experimental results show that our approach robustly outputs the heading direction for different types of actor. The aesthetic value of the video is also improved in the aerial filming task.
ER  - 

TY  - CONF
TI  - Graduated Fidelity Lattices for Motion Planning under Uncertainty
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5908
EP  - 5914
AU  - A. González-Sieira
AU  - M. Mucientes
AU  - A. Bugarín
PY  - 2019
KW  - mobile robots
KW  - path planning
KW  - probability
KW  - robot shape
KW  - motion models
KW  - graduated fidelity lattices
KW  - motion planning
KW  - state lattice based approach
KW  - mobile robotics
KW  - motion uncertainty
KW  - multiresolution heuristic
KW  - collision probability
KW  - Uncertainty
KW  - Planning
KW  - Robots
KW  - Lattices
KW  - Collision avoidance
KW  - Shape
KW  - Probability density function
DO  - 10.1109/ICRA.2019.8794400
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this work we present a state lattice based approach for motion planning in mobile robotics. Sensing and motion uncertainty are managed at planning time to obtain safe and optimal paths. To do this reliably, our approach estimates the probability of collision taking into account the robot shape and the uncertainty in heading. We also introduce a novel graduated fidelity approach and a multi-resolution heuristic which adapt to the obstacles in the map, improving the planning efficiency while maintaining its performance. Results for different environments, shapes and motion models are reported, including experiments with real robots.
ER  - 

TY  - CONF
TI  - Non-Parametric Informed Exploration for Sampling-Based Motion Planning
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5915
EP  - 5921
AU  - S. S. Joshi
AU  - T. Panagiotis
PY  - 2019
KW  - collision avoidance
KW  - path planning
KW  - sampling methods
KW  - search problems
KW  - effective sampling method
KW  - good initial solution
KW  - nonparametric exploration technique
KW  - nonparametric informed exploration
KW  - sampling-based motion planning
KW  - search space
KW  - Kernel
KW  - Planning
KW  - Heuristic algorithms
KW  - Sampling methods
KW  - Convergence
KW  - Search problems
KW  - Benchmark testing
DO  - 10.1109/ICRA.2019.8793933
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Efficient exploration of the search space is crucial for faster convergence in sampling-based motion planning. An effective sampling method must first concentrate on quickly finding a good initial solution and then focus the search on regions that can potentially improve the current best solution. In this paper, we propose a non-parametric exploration technique that addresses these challenges. The proposed algorithm prioritizes search by utilizing heuristics. After an initial solution is found, the method generates samples in the “$L_{2} -$informed set”, while leveraging collision data to reduce the number of samples in the obstacle space. We demonstrate the efficiency of the proposed approach with several benchmarking experiments.
ER  - 

TY  - CONF
TI  - Simulated Annealing-optimized Trajectory Planning within Non-Collision Nominal Intervals for Highway Autonomous Driving
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5922
EP  - 5928
AU  - L. Claussmann
AU  - M. Revilloud
AU  - S. Glaser
PY  - 2019
KW  - acceleration control
KW  - collision avoidance
KW  - mobile robots
KW  - predictive control
KW  - road traffic control
KW  - simulated annealing
KW  - time-varying systems
KW  - trajectory control
KW  - sigmoid trajectory
KW  - collision-free intervals
KW  - nominal conditions
KW  - velocity-space representation
KW  - highway autonomous driving
KW  - near-optimal trajectory generation
KW  - autonomous vehicles
KW  - highways
KW  - predictive reference trajectory
KW  - free evolution space
KW  - pre-calculated set
KW  - candidate trajectories
KW  - decoupling path
KW  - velocity optimizations
KW  - multicriteria functions
KW  - decision evaluation function
KW  - trajectory generator
KW  - simulated annealing approach
KW  - noncollision nominal intervals
KW  - simulated annealing-optimized trajectory planning
KW  - Trajectory
KW  - Roads
KW  - Acceleration
KW  - Vehicle dynamics
KW  - Autonomous vehicles
KW  - Decision making
DO  - 10.1109/ICRA.2019.8793838
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This article considers the problem of near-optimal trajectory generation for autonomous vehicles on highways. The goal is to select a predictive reference trajectory in the free evolution space, while avoiding both generating a pre-calculated set of candidate trajectories and decoupling path and velocity optimizations. Moreover, this trajectory aims at optimizing a decision process based on multi-criteria functions, which are not straightforward to design and can have a blackbox formulation. The main idea of this article is to use the decision evaluation function in the trajectory generator with a Simulated Annealing (SA) approach. The parameters of a sigmoid trajectory are optimized within Non-Collision Nominal Intervals (NCNI), which are defined as collision-free intervals under nominal conditions using a velocity-space representation.
ER  - 

TY  - CONF
TI  - On the Impact of Uncertainty for Path Planning
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5929
EP  - 5935
AU  - J. Guzzi
AU  - R. O. Chavez-Garcia
AU  - L. M. Gambardella
AU  - A. Giusti
PY  - 2019
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - probability
KW  - travelling salesman problems
KW  - path planning
KW  - planning paths
KW  - uncertain edge
KW  - learned classifier
KW  - mobile robots
KW  - partially-known environments
KW  - simulation campaign
KW  - real-world maps
KW  - planning strategy
KW  - traversability estimates
KW  - Canadian traveller problem
KW  - Robot sensing systems
KW  - Navigation
KW  - Uncertainty
KW  - Path planning
KW  - Estimation
KW  - Optimized production technology
DO  - 10.1109/ICRA.2019.8793782
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We consider the problem of planning paths on graphs with some edges whose traversability is uncertain; for each uncertain edge, we are given a probability of being traversable (e.g., by a learned classifier). We categorize different interpretations of the problem that are meaningful for mobile robots navigating partially-known environments, each of which yields a different formalization; we then focus on the case in which the true traversability of an edge is revealed only when the agent visits one of its endpoints (Canadian Traveller Problem). In this context, we design a large simulation campaign on synthetic and real-world maps to study the impact of two different factors: the planning strategy, and the amount of uncertainty (which could depend on the quality of the classifier producing traversability estimates).
ER  - 

TY  - CONF
TI  - Localization with Sliding Window Factor Graphs on Third-Party Maps for Automated Driving
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5951
EP  - 5957
AU  - D. Wilbers
AU  - C. Merfels
AU  - C. Stachniss
PY  - 2019
KW  - optimisation
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - position measurement
KW  - window factor graphs
KW  - third-party maps
KW  - robotic applications
KW  - window optimization
KW  - odometry measurements
KW  - fast vehicle localization
KW  - accurate vehicle localization
KW  - estimation problem
KW  - sliding window formulation
KW  - factor graph
KW  - landmark detections
KW  - automated car
KW  - automated driving applications
KW  - Microsoft Windows
KW  - Optimization
KW  - Simultaneous localization and mapping
KW  - Global navigation satellite system
KW  - Laser radar
KW  - Measurement uncertainty
DO  - 10.1109/ICRA.2019.8793971
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Localizing a vehicle in a map is essential for automated driving and various other robotic applications. This paper addresses the problem of vehicle localization in urban environments. Our approach performs a graph-based sliding window optimization over a set of recent landmark and odometry measurements for fast and accurate vehicle localization on third-party maps. Our work incorporates landmark priors from third-party maps into the estimation problem and shows how to exploit the sliding window formulation for revising data associations. We describe how to construct our factor graph and derive its necessary factors to model the information from the map as a prior over the landmark detections. We implemented our approach on an automated car and thoroughly tested it on real-world data. The experiments suggest that the approach provides highly accurate pose estimates, is fast enough for automated driving applications, and outperforms localization using particle filters.
ER  - 

TY  - CONF
TI  - Night-to-Day Image Translation for Retrieval-based Localization
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5958
EP  - 5964
AU  - A. Anoosheh
AU  - T. Sattler
AU  - R. Timofte
AU  - M. Pollefeys
AU  - L. V. Gool
PY  - 2019
KW  - approximation theory
KW  - image retrieval
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - visual databases
KW  - geo-tagged images
KW  - night-to-day image translation
KW  - retrieval-based localization
KW  - visual localization
KW  - robotics pipelines
KW  - image retrieval techniques
KW  - database image retrieval
KW  - neural models
KW  - pose estimation
KW  - visual query photo
KW  - ToDayGAN
KW  - Task analysis
KW  - Visualization
KW  - Cameras
KW  - Feature extraction
KW  - Training
KW  - Generators
KW  - Data models
DO  - 10.1109/ICRA.2019.8794387
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Visual localization is a key step in many robotics pipelines, allowing the robot to (approximately) determine its position and orientation in the world. An efficient and scalable approach to visual localization is to use image retrieval techniques. These approaches identify the image most similar to a query photo in a database of geo-tagged images and approximate the query's pose via the pose of the retrieved database image. However, image retrieval across drastically different illumination conditions, e.g. day and night, is still a problem with unsatisfactory results, even in this age of powerful neural models. This is due to a lack of a suitably diverse dataset with true correspondences to perform end-to-end learning. A recent class of neural models allows for realistic translation of images among visual domains with relatively little training data and, most importantly, without ground-truth pairings.In this paper, we explore the task of accurately localizing images captured from two traversals of the same area in both day and night. We propose ToDayGAN - a modified image-translation model to alter nighttime driving images to a more useful daytime representation. We then compare the daytime and translated night images to obtain a pose estimate for the night image using the known 6-DOF position of the closest day image. Our approach improves localization performance by over 250% compared the current state-of-the-art, in the context of standard metrics in multiple categories.
ER  - 

TY  - CONF
TI  - Accurate and Efficient Self-Localization on Roads using Basic Geometric Primitives
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5965
EP  - 5971
AU  - J. Kümmerle
AU  - M. Sons
AU  - F. Poggenhans
AU  - T. Kühner
AU  - M. Lauer
AU  - C. Stiller
PY  - 2019
KW  - automobiles
KW  - distance measurement
KW  - graph theory
KW  - path planning
KW  - pose estimation
KW  - position control
KW  - road traffic control
KW  - localization update rate
KW  - roads
KW  - basic geometric primitives
KW  - behavior generation
KW  - distinctive signature
KW  - map elements
KW  - localization framework
KW  - next generation series cars
KW  - association measurement
KW  - odometry measurement
KW  - robust pose graph optimization
KW  - time 30.0 min
KW  - size 10.0 cm
KW  - frequency 50.0 Hz
KW  - Roads
KW  - Automobiles
KW  - Frequency modulation
KW  - Detectors
KW  - Global navigation satellite system
KW  - Feature extraction
KW  - Optimization
DO  - 10.1109/ICRA.2019.8793497
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Highly accurate localization with very limited amount of memory and computational power is one of the big challenges for next generation series cars. We propose localization based on geometric primitives which are compact in representation and further valuable for other tasks like planning and behavior generation. The primitives lack distinctive signature which makes association between detections and map elements highly ambiguous. We resolve ambiguities early in the pipeline by online building up a local map which is key to runtime efficiency. Further, we introduce a new framework to fuse association and odometry measurements based on robust pose graph optimization.We evaluate our localization framework on over 30 min of data recorded in urban scenarios. Our map is memory efficient with less than 8 kB/km and we achieve high localization accuracy with a mean position error of less than 10 cm and a mean yaw angle error of less than 0. 25° at a localization update rate of 50Hz.
ER  - 

TY  - CONF
TI  - Efficient 2D-3D Matching for Multi-Camera Visual Localization
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5972
EP  - 5978
AU  - M. Geppert
AU  - P. Liu
AU  - Z. Cui
AU  - M. Pollefeys
AU  - T. Sattler
PY  - 2019
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image matching
KW  - motion estimation
KW  - pose estimation
KW  - autonomous driving
KW  - multicamera visual inertial localization algorithm
KW  - prioritized feature matching scheme
KW  - multicamera systems
KW  - monocular cameras
KW  - prioritization function
KW  - multicamera setup
KW  - matching efforts
KW  - pose priors
KW  - localization system
KW  - motion estimates
KW  - multicamera visual inertial odometry pipeline
KW  - large scale environments
KW  - pose estimation stages
KW  - pre-built global 3D map
KW  - Cameras
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Visualization
KW  - Feature extraction
KW  - Pipelines
KW  - Reliability
DO  - 10.1109/ICRA.2019.8794280
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Visual localization, i.e., determining the position and orientation of a vehicle with respect to a map, is a key problem in autonomous driving. We present a multi-camera visual inertial localization algorithm for large scale environments. To efficiently and effectively match features against a pre-built global 3D map, we propose a prioritized feature matching scheme for multi-camera systems. In contrast to existing works, designed for monocular cameras, we (1) tailor the prioritization function to the multi-camera setup and (2) run feature matching and pose estimation in parallel. This significantly accelerates the matching and pose estimation stages and allows us to dynamically adapt the matching efforts based on the surrounding environment. In addition, we show how pose priors can be integrated into the localization system to increase efficiency and robustness. Finally, we extend our algorithm by fusing the absolute pose estimates with motion estimates from a multi-camera visual inertial odometry pipeline (VIO). This results in a system that provides reliable and drift-less pose estimation. Extensive experiments show that our localization runs fast and robust under varying conditions, and that our extended algorithm enables reliable real-time pose estimation.
ER  - 

TY  - CONF
TI  - Localizing Discriminative Visual Landmarks for Place Recognition
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5979
EP  - 5985
AU  - Z. Xin
AU  - Y. Cai
AU  - T. Lu
AU  - X. Xing
AU  - S. Cai
AU  - J. Zhang
AU  - Y. Yang
AU  - Y. Wang
PY  - 2019
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - image representation
KW  - object recognition
KW  - visual place recognition
KW  - perceptual changes
KW  - robust image representations
KW  - environmental changes
KW  - viewpoint changes
KW  - convolutional neural networks
KW  - landmark localization network
KW  - appearance changes
KW  - vegetations
KW  - buildings
KW  - similarity measurement
KW  - CNN
KW  - feature extraction
KW  - discriminative visual landmark localization
KW  - Visualization
KW  - Feature extraction
KW  - Training
KW  - Image recognition
KW  - Buildings
KW  - Task analysis
KW  - Measurement
DO  - 10.1109/ICRA.2019.8794383
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We address the problem of visual place recognition with perceptual changes. The fundamental problem of visual place recognition is generating robust image representations which are not only insensitive to environmental changes but also distinguishable to different places. Taking advantage of the feature extraction ability of Convolutional Neural Networks (CNNs), we further investigate how to localize discriminative visual landmarks that positively contribute to the similarity measurement, such as buildings and vegetations. In particular, a Landmark Localization Network (LLN) is designed to indicate which regions of an image are used for discrimination. Detailed experiments are conducted on open source datasets with varied appearance and viewpoint changes. The proposed approach achieves superior performance against state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Beyond Point Clouds: Fisher Information Field for Active Visual Localization
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5986
EP  - 5992
AU  - Z. Zhang
AU  - D. Scaramuzza
PY  - 2019
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - point clouds
KW  - Fisher information field
KW  - active visual localization
KW  - mobile robots
KW  - perception requirement
KW  - planning stage
KW  - localization information
KW  - perception-aware planning
KW  - 3D landmarks
KW  - sensor visibility
KW  - Three-dimensional displays
KW  - Planning
KW  - Visualization
KW  - Cameras
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA.2019.8793680
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - For mobile robots to localize robustly, actively considering the perception requirement at the planning stage is essential. In this paper, we propose a novel representation for active visual localization. By formulating the Fisher information and sensor visibility carefully, we are able to summarize the localization information into a discrete grid, namely the Fisher information field. The information for arbitrary poses can then be computed from the field in constant time, without the need of costly iterating all the 3D landmarks. Experimental results on simulated and real-world data show the great potential of our method in efficient active localization and perception-aware planning. To benefit related research, we release our implementation of the information field to the public.
ER  - 

TY  - CONF
TI  - Deep Reinforcement Learning of Navigation in a Complex and Crowded Environment with a Limited Field of View
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 5993
EP  - 6000
AU  - J. Choi
AU  - K. Park
AU  - M. Kim
AU  - S. Seok
PY  - 2019
KW  - cameras
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - neural nets
KW  - optical radar
KW  - path planning
KW  - robot vision
KW  - deep reinforcement learning-based methods
KW  - DRL agents
KW  - LSTM agent
KW  - Local-Map Critic
KW  - LSTM-LMC
KW  - wide FOV
KW  - single depth camera
KW  - mobile robots
KW  - lidar devices
KW  - DRL method
KW  - depth cameras
KW  - dynamics randomization technique
KW  - Navigation
KW  - Mobile robots
KW  - Reinforcement learning
KW  - Laser radar
KW  - Robot sensing systems
KW  - Cameras
DO  - 10.1109/ICRA.2019.8793979
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Mobile robots are required to navigate freely in a complex and crowded environment in order to provide services to humans. For this navigation ability, deep reinforcement learning (DRL)-based methods are gaining increasing attentions. However, existing DRL methods require a wide field of view (FOV), which imposes the usage of high-cost lidar devices. In this paper, we explore the possibility of replacing expensive lidar devices with affordable depth cameras which have a limited FOV. First, we analyze the effect of a limited field of view in the DRL agents. Second, we propose a LSTM agent with Local-Map Critic (LSTM-LMC), which is a novel DRL method to learn efficient navigation in a complex environment with a limited FOV. Lastly, we introduce the dynamics randomization technique to improve the robustness of the DRL agents in the real world. We found that our method with a limited FOV can outperform the methods having a wide FOV but limited memory. We provide the empirical evidence that our method learns to implicitly model the surrounding environment and dynamics of other agents. We also show that a robot with a single depth camera can navigate through a complex real-world environment using our method.
ER  - 

TY  - CONF
TI  - Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6001
EP  - 6007
AU  - J. v. Baar
AU  - A. Sullivan
AU  - R. Cordorel
AU  - D. Jha
AU  - D. Romeres
AU  - D. Nikovski
PY  - 2019
KW  - learning (artificial intelligence)
KW  - robot dynamics
KW  - robust control
KW  - robustified controller
KW  - robotic tasks
KW  - complex dynamics
KW  - robot tasks
KW  - deep reinforcement learning
KW  - simulated environment
KW  - learned task
KW  - fine-tuning
KW  - simulation parameters
KW  - nontrivial task
KW  - nonrobustified controller
KW  - sim-to-real transfer learning
KW  - robustified controllers
KW  - Task analysis
KW  - Robots
KW  - Physics
KW  - Games
KW  - Reinforcement learning
KW  - Training
KW  - Computational modeling
DO  - 10.1109/ICRA.2019.8793561
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Learning robot tasks or controllers using deep reinforcement learning has been proven effective in simulations. Learning in simulation has several advantages. For example, one can fully control the simulated environment, including halting motions while performing computations. Another advantage when robots are involved, is that the amount of time a robot is occupied learning a task-rather than being productive-can be reduced by transferring the learned task to the real robot. Transfer learning requires some amount of fine-tuning on the real robot. For tasks which involve complex (non-linear) dynamics, the fine-tuning itself may take a substantial amount of time. In order to reduce the amount of fine-tuning we propose to learn robustified controllers in simulation. Robustified controllers are learned by exploiting the ability to change simulation parameters (both appearance and dynamics) for successive training episodes. An additional benefit for this approach is that it alleviates the precise determination of physics parameters for the simulator, which is a non-trivial task. We demonstrate our proposed approach on a real setup in which a robot aims to solve a maze game, which involves complex dynamics due to static friction and potentially large accelerations. We show that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.
ER  - 

TY  - CONF
TI  - Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6008
EP  - 6014
AU  - K. Kang
AU  - S. Belkhale
AU  - G. Kahn
AU  - P. Abbeel
AU  - S. Levine
PY  - 2019
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - data analysis
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - vision-based autonomous flight
KW  - fragile scale quadrotors
KW  - small-scale quadrotors
KW  - complex physics
KW  - air currents
KW  - hybrid deep reinforcement learning algorithm
KW  - generalizable perception system
KW  - nanoaerial vehicle collision avoidance task
KW  - real data
KW  - simulated data
KW  - Data models
KW  - Robots
KW  - Task analysis
KW  - Predictive models
KW  - Neural networks
KW  - Reinforcement learning
KW  - Collision avoidance
DO  - 10.1109/ICRA.2019.8793735
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS.
ER  - 

TY  - CONF
TI  - Crowd-Robot Interaction: Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6015
EP  - 6022
AU  - C. Chen
AU  - Y. Liu
AU  - S. Kreiss
AU  - A. Alahi
PY  - 2019
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - pedestrians
KW  - crowded spaces
KW  - Human-Human interactions
KW  - deep reinforcement learning framework
KW  - dense crowds
KW  - human dynamics
KW  - crowd-aware robot navigation
KW  - attention-based deep reinforcement learning
KW  - robot operations
KW  - crowd-robot interaction
KW  - Robots
KW  - Navigation
KW  - Reinforcement learning
KW  - Planning
KW  - Task analysis
KW  - Human-robot interaction
KW  - Biological system modeling
DO  - 10.1109/ICRA.2019.8794134
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a self-attention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot's anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Residual Reinforcement Learning for Robot Control
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6023
EP  - 6029
AU  - T. Johannink
AU  - S. Bahl
AU  - A. Nair
AU  - J. Luo
AU  - A. Kumar
AU  - M. Loskyll
AU  - J. A. Ojea
AU  - E. Solowjow
AU  - S. Levine
PY  - 2019
KW  - continuous systems
KW  - control system synthesis
KW  - feedback
KW  - friction
KW  - industrial robots
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mechanical contact
KW  - motion control
KW  - robot dynamics
KW  - first-order physical modeling
KW  - brittle controllers
KW  - inaccurate controllers
KW  - reinforcement learning methods
KW  - continuous robot controllers
KW  - control signals
KW  - robot control problems
KW  - modern manufacturing
KW  - control design
KW  - feedback control methods
KW  - control policy
KW  - residual reinforcement learning
KW  - rigid body equations of motion
KW  - contacts
KW  - friction
KW  - robot learning
KW  - unstable objects
KW  - block assembly task
KW  - Robots
KW  - Task analysis
KW  - Feedback control
KW  - Reinforcement learning
KW  - Mathematical model
KW  - Manufacturing
KW  - Adaptive control
DO  - 10.1109/ICRA.2019.8794127
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.
ER  - 

TY  - CONF
TI  - A Reinforcement Learning Approach for Control of a Nature-Inspired Aerial Vehicle
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6030
EP  - 6036
AU  - D. Sufiyan
AU  - L. T. S. Win
AU  - S. K. H. Win
AU  - G. S. Soh
AU  - S. Foong
PY  - 2019
KW  - autonomous aerial vehicles
KW  - function approximation
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - position control
KW  - three-term control
KW  - nature-inspired aerial vehicle
KW  - position controller
KW  - UAV
KW  - fixed-wing aircraft
KW  - neural network function approximators
KW  - reinforcement learning agent
KW  - learned controller
KW  - deep deterministic policy gradients
KW  - PID controller
KW  - Ape-X distributed prioritized experience replay
KW  - multi-rotors
KW  - underactuated nature-inspired unmanned aerial vehicle
KW  - body contrary
KW  - Training
KW  - Aerodynamics
KW  - Neural networks
KW  - Mathematical model
KW  - Reinforcement learning
KW  - Drag
KW  - Prototypes
DO  - 10.1109/ICRA.2019.8794446
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this work, reinforcement learning is used to develop a position controller for an underactuated nature-inspired Unmanned Aerial Vehicle (UAV). This particular configuration of UAVs achieves lift by spinning its entire body contrary to standard multi-rotors or fixed-wing aircraft. Deep Deterministic Policy Gradients (DDPG) with Ape-X Distributed Prioritized Experience Replay was used to train neural network function approximators that were implemented as the final control policy. The reinforcement learning agent was trained in simulations and directly ported over to real-life hardware. Position control tests were performed on the learned control policy and compared to a baseline PID controller. The learned controller was found to exhibit better control over the inherent oscillations that arise from the non-linear dynamics of the platform.
ER  - 

TY  - CONF
TI  - Formal Policy Learning from Demonstrations for Reachability Properties
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6037
EP  - 6043
AU  - H. Ravanbakhsh
AU  - S. Sankaranarayanan
AU  - S. A. Seshia
PY  - 2019
KW  - closed loop systems
KW  - feedback
KW  - learning (artificial intelligence)
KW  - predictive control
KW  - reachability analysis
KW  - formal behavioral specifications
KW  - counterexample-guided iterative loop
KW  - receding horizon model-predictive controllers
KW  - demonstrator actions
KW  - formally-verified policies
KW  - formal policy learning from demonstrations
KW  - reachability properties
KW  - structured closed-loop policies
KW  - MPC
KW  - cost-to-go function
KW  - Robots
KW  - Trajectory
KW  - Training data
KW  - Predictive models
KW  - Real-time systems
KW  - Measurement
DO  - 10.1109/ICRA.2019.8793828
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We consider the problem of learning structured, closed-loop policies (feedback laws) from demonstrations in order to control under-actuated robotic systems, so that formal behavioral specifications such as reaching a target set of states are satisfied. Our approach uses a “counterexample-guided” iterative loop that involves the interaction between a policy learner, a demonstrator and a verifier. The learner is responsible for querying the demonstrator in order to obtain the training data to guide the construction of a policy candidate. This candidate is analyzed by the verifier and either accepted as correct, or rejected with a counterexample. In the latter case, the counterexample is used to update the training data and further refine the policy.The approach is instantiated using receding horizon model-predictive controllers (MPCs) as demonstrators. Rather than using regression to fit a policy to the demonstrator actions, we extend the MPC formulation with the gradient of the cost-to-go function evaluated at sample states in order to constrain the set of policies compatible with the behavior of the demonstrator. We demonstrate the successful application of the resulting policy learning schemes on two case studies and we show how simple, formally-verified policies can be inferred starting from a complex and unverified nonlinear MPC implementations. As a further benefit, the policies are many orders of magnitude faster to implement when compared to the original MPCs.
ER  - 

TY  - CONF
TI  - Formalized Task Characterization for Human-Robot Autonomy Allocation
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6044
EP  - 6050
AU  - M. Young
AU  - C. Miller
AU  - Y. Bi
AU  - W. Chen
AU  - B. D. Argall
PY  - 2019
KW  - design of experiments
KW  - human-robot interaction
KW  - multi-robot systems
KW  - Taguchi methods
KW  - task characterization
KW  - human-robot autonomy allocation
KW  - human-robot teams
KW  - conjoint analysis
KW  - rotational features
KW  - translational features
KW  - autonomy assistance
KW  - robotic arm
KW  - kinematic features
KW  - Task analysis
KW  - Kinematics
KW  - Complexity theory
KW  - Manipulators
KW  - Feature extraction
KW  - Service robots
DO  - 10.1109/ICRA.2019.8793475
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Humans and robots team together to perform tasks in various domains. Some tasks are easier to perform than others, but little work focuses on discovering the underlying mechanisms that affect perceived difficulty and task performance. To fill this gap, we propose a formalized approach to task characterization for human-robot teams using Taguchi design of experiments and conjoint analysis. With this, we conduct a 20 person study where participants operate a 6 degree of freedom robotic arm to perform manipulations defined by 6 kinematic features. We find that rotational features of a task contribute significantly more to decreased performance and increased difficulty than translational features. The participants also perform the activities with autonomy assistance. The data shows a reduction in the effect of these features on performance and difficulty when assistance is active. Furthermore, we examine when to trigger assistance based on thresholds set from outlier detection. The analysis indicates that rotational features and features leading to kinematic singularities are useful for triggering assistance.
ER  - 

TY  - CONF
TI  - DoS-Resilient Multi-Robot Temporal Logic Motion Planning
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6051
EP  - 6057
AU  - X. Sun
AU  - R. Nambiar
AU  - M. Melhorn
AU  - Y. Shoukry
AU  - P. Nuzzo
PY  - 2019
KW  - convex programming
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - robot dynamics
KW  - robust control
KW  - temporal logic
KW  - DoS-resilient multirobot temporal logic motion planning
KW  - multirobot motion
KW  - linear temporal logic specifications
KW  - denial-of-service attacks
KW  - robot trajectories
KW  - DoS attacks
KW  - DoS-free workspace
KW  - DoS-resilient mission constraints
KW  - robot dynamics
KW  - DoS-resilient plans
KW  - satisfiability modulo convex programming
KW  - Base stations
KW  - Planning
KW  - Trajectory
KW  - Robot kinematics
KW  - Jamming
KW  - Radar
DO  - 10.1109/ICRA.2019.8794477
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We propose an efficient multi-robot motion planning algorithm for missions captured by linear temporal logic (LTL) specifications, in the presence of bounded disturbances and denial-of-service (DoS) attacks against the communication between robots and base stations. Given an LTL formula Ψ, our goal is to construct robot trajectories, and associated control strategies, to satisfy Ψ and continuously establish communication paths between robots and base stations despite the DoS attacks and the disturbances on the robot states. Our approach combines and extends results from robust control and efficient motion planning via satisfiability modulo convex programming (SMC). We first compute a feedback controller that rejects the disturbance together with a perturbation of the DoS-free workspace that accounts for the worst-case disturbance scenario. On the perturbed workspace, we formulate the planning problem as a feasibility problem over Boolean and convex constraints, respectively capturing the DoS-resilient mission constraints and the constraints on the nominal, disturbance-free, robot dynamics. Numerical results show the effectiveness of our algorithm in providing DoS-resilient plans that are robust to disturbances and support the execution of complex missions.
ER  - 

TY  - CONF
TI  - Task-Based Design of Ad-hoc Modular Manipulators
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6058
EP  - 6064
AU  - T. Campos
AU  - J. P. Inala
AU  - A. Solar-Lezama
AU  - H. Kress-Gazit
PY  - 2019
KW  - control system synthesis
KW  - genetic algorithms
KW  - manipulators
KW  - demand robots
KW  - modular robots
KW  - task description
KW  - degree-of-freedom modules
KW  - task-based design
KW  - ad-hoc modular manipulators
KW  - genetic algorithm
KW  - Task analysis
KW  - Manipulators
KW  - Trajectory
KW  - Actuators
KW  - Genetic algorithms
KW  - Kinematics
DO  - 10.1109/ICRA.2019.8794171
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - The great promise of modular robots is the ability to create on demand robots; however, choosing the “right” design based on a task is still a challenging problem. In this paper, we present an approach to automatically synthesize both the design and control for modular robots from a task description. In particular, we focus on manipulators composed of one degree-of-freedom (DoF) modules. Our approach is able to handle partially infeasible tasks by either identifying the infeasible part and finding a design that satisfies the feasible part or searching for multiple designs that together satisfy the entire task. We compare our approach to a baseline genetic algorithm in a series of increasingly complex environments.
ER  - 

TY  - CONF
TI  - SweepNet: Wide-baseline Omnidirectional Depth Estimation
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6073
EP  - 6079
AU  - C. Won
AU  - J. Ryu
AU  - J. Lim
PY  - 2019
KW  - calibration
KW  - cameras
KW  - image reconstruction
KW  - image sensors
KW  - lenses
KW  - neural nets
KW  - object recognition
KW  - robot vision
KW  - stereo image processing
KW  - multiple sets
KW  - rectified images
KW  - dense omnidirectional depth map
KW  - rig global coordinate system
KW  - warped images
KW  - final depth map
KW  - aggregated cost volume
KW  - deep neural network
KW  - conventional depth estimation methods
KW  - highly accurate depth maps
KW  - wide-baseline omnidirectional depth estimation
KW  - omnidirectional depth sensing
KW  - conventional stereo systems
KW  - blind regions
KW  - novel wide-baseline omnidirectional stereo algorithm
KW  - dense depth estimate
KW  - fisheye images
KW  - deep convolutional neural network
KW  - capture system
KW  - multiple cameras
KW  - wide-baseline rig
KW  - ultra-wide field
KW  - view lenses
KW  - calibration algorithm
KW  - Cameras
KW  - Lenses
KW  - Calibration
KW  - Neural networks
KW  - Three-dimensional displays
KW  - Robot vision systems
DO  - 10.1109/ICRA.2019.8793823
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Omnidirectional depth sensing has its advantage over the conventional stereo systems since it enables us to recognize the objects of interest in all directions without any blind regions. In this paper, we propose a novel wide-baseline omnidirectional stereo algorithm which computes the dense depth estimate from the fisheye images using a deep convolutional neural network. The capture system consists of multiple cameras mounted on a wide-baseline rig with ultra-wide field of view (FOV) lenses, and we present the calibration algorithm for the extrinsic parameters based on the bundle adjustment. Instead of estimating depth maps from multiple sets of rectified images and stitching them, our approach directly generates one dense omnidirectional depth map with full 360° coverage at the rig global coordinate system. To this end, the proposed neural network is designed to output the cost volume from the warped images in the sphere sweeping method, and the final depth map is estimated by taking the minimum cost indices of the aggregated cost volume by SGM. For training the deep neural network and testing the entire system, realistic synthetic urban datasets are rendered using Blender. The experiments using the synthetic and real-world datasets show that our algorithm outperforms the conventional depth estimation methods and generate highly accurate depth maps.
ER  - 

TY  - CONF
TI  - 3D Surface Reconstruction Using A Two-Step Stereo Matching Method Assisted with Five Projected Patterns
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6080
EP  - 6086
AU  - C. Sui
AU  - K. He
AU  - C. Lyu
AU  - Z. Wang
AU  - Y. Liu
PY  - 2019
KW  - correlation methods
KW  - image matching
KW  - image reconstruction
KW  - image resolution
KW  - robot vision
KW  - solid modelling
KW  - stereo image processing
KW  - surface reconstruction
KW  - three-dimensional vision
KW  - 3D surface reconstruction scheme
KW  - stereo matching pattern projection
KW  - stereo images
KW  - phase maps
KW  - phase-shifting patterns
KW  - image acquisition time
KW  - correspondence refinement algorithm
KW  - high-resolution reconstruction
KW  - object reconstruction
KW  - two-step stereo matching method
KW  - Three-dimensional displays
KW  - Image reconstruction
KW  - Surface reconstruction
KW  - Cameras
KW  - Robots
KW  - Pattern matching
KW  - Robustness
DO  - 10.1109/ICRA.2019.8794063
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Three-dimensional vision plays an important role in robotics. In this paper, we present a 3D surface reconstruction scheme based on combination of stereo matching and pattern projection. A two-step matching scheme is proposed to establish reliable correspondence between stereo images with high computation efficiency and accuracy. The first step (coarse matching) can quickly find the correlation candidates, and the second step (precise matching) is responsible for determining the most precise correspondence within the candidates. Two phase maps serve as codewords and are utilized in the two-step stereo matching, respectively. The phase maps are derived from phase-shifting patterns to provide robustness to the background noises. Only five patterns are required, which reduces the image acquisition time. Moreover, the precision is further enhanced by applying a correspondence refinement algorithm. The precision and accuracy are validated by experiments on standard objects. Furthermore, various experiments are conducted to verify the capability of the proposed method, which includes the complex object reconstruction, the high-resolution reconstruction, and the occlusion avoidance. The real-time experimental results are also provided.
ER  - 

TY  - CONF
TI  - Real-Time Dense Mapping for Self-Driving Vehicles using Fisheye Cameras
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6087
EP  - 6093
AU  - Z. Cui
AU  - L. Heng
AU  - Y. C. Yeo
AU  - A. Geiger
AU  - M. Pollefeys
AU  - T. Sattler
PY  - 2019
KW  - cameras
KW  - image capture
KW  - image fusion
KW  - image resolution
KW  - image sensors
KW  - mobile robots
KW  - object detection
KW  - robot vision
KW  - stereo image processing
KW  - visual perception
KW  - fisheye cameras
KW  - real-time dense geometric mapping algorithm
KW  - pinhole cameras
KW  - visual-inertial odometry
KW  - visual localization
KW  - vision-only 3D scene perception
KW  - depth map
KW  - reference camera
KW  - plane-sweeping stereo
KW  - fast object detection framework
KW  - YOLOv3
KW  - fisheye depth images
KW  - computer vision applications
KW  - angular resolution
KW  - image resolutions
KW  - in-vehicle PC
KW  - truncated signed distance function
KW  - TSDF volume
KW  - 3D map
KW  - self-driving vehicles
KW  - Cameras
KW  - Three-dimensional displays
KW  - Real-time systems
KW  - Image resolution
KW  - Vehicle dynamics
KW  - Estimation
KW  - Object detection
DO  - 10.1109/ICRA.2019.8793884
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We present a real-time dense geometric mapping algorithm for large-scale environments. Unlike existing methods which use pinhole cameras, our implementation is based on fisheye cameras whose large field of view benefits various computer vision applications for self-driving vehicles such as visual-inertial odometry, visual localization, and object detection. Our algorithm runs on in-vehicle PCs at approximately 15 Hz, enabling vision-only 3D scene perception for self-driving vehicles. For each synchronized set of images captured by multiple cameras, we first compute a depth map for a reference camera using plane-sweeping stereo. To maintain both accuracy and efficiency, while accounting for the fact that fisheye images have a lower angular resolution, we recover the depths using multiple image resolutions. We adopt the fast object detection framework, YOLOv3, to remove potentially dynamic objects. At the end of the pipeline, we fuse the fisheye depth images into the truncated signed distance function (TSDF) volume to obtain a 3D map. We evaluate our method on large-scale urban datasets, and results show that our method works well in complex dynamic environments.
ER  - 

TY  - CONF
TI  - Tightly-Coupled Aided Inertial Navigation with Point and Plane Features
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6094
EP  - 6100
AU  - Y. Yang
AU  - P. Geneva
AU  - X. Zuo
AU  - K. Eckenhoff
AU  - Y. Liu
AU  - G. Huang
PY  - 2019
KW  - feature extraction
KW  - image fusion
KW  - image sensors
KW  - inertial navigation
KW  - mobile robots
KW  - Monte Carlo methods
KW  - object tracking
KW  - SLAM (robots)
KW  - planar point features
KW  - nonplanar point features
KW  - point-on-plane constraints
KW  - effective plane feature initialization algorithm
KW  - depth sensor
KW  - general sensor fusion framework
KW  - point feature tracking
KW  - plane extraction
KW  - geometrical structures
KW  - closest point
KW  - plane parameterization
KW  - Monte-Carlo simulations
KW  - visual sensor
KW  - tightly-coupled aided inertial navigation system
KW  - feature-based simultaneous localization and mapping
KW  - Feature extraction
KW  - Cameras
KW  - Calibration
KW  - Laser radar
KW  - Jacobian matrices
KW  - Simultaneous localization and mapping
KW  - Estimation
DO  - 10.1109/ICRA.2019.8794078
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper presents a tightly-coupled aided inertial navigation system (INS) with point and plane features, a general sensor fusion framework applicable to any visual and depth sensor (e.g., RGBD, LiDAR) configuration, in which the camera is used for point feature tracking and depth sensor for plane extraction. The proposed system exploits geometrical structures (planes) of the environments and adopts the closest point (CP) for plane parameterization. Moreover, we distinguish planar point features from non-planar point features in order to enforce point-on-plane constraints which are used in our state estimator, thus further exploiting structural information from the environment. We also introduce a simple but effective plane feature initialization algorithm for feature-based simultaneous localization and mapping (SLAM). In addition, we perform online spatial calibration between the IMU and the depth sensor as it is difficult to obtain this critical calibration parameter in high precision. Both Monte-Carlo simulations and real-world experiments are performed to validate the proposed approach.
ER  - 

TY  - CONF
TI  - FastDepth: Fast Monocular Depth Estimation on Embedded Systems
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6101
EP  - 6108
AU  - D. Wofk
AU  - F. Ma
AU  - T. Yang
AU  - S. Karaman
AU  - V. Sze
PY  - 2019
KW  - autonomous aerial vehicles
KW  - cameras
KW  - computational complexity
KW  - embedded systems
KW  - estimation theory
KW  - image colour analysis
KW  - image segmentation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - microrobots
KW  - mobile robots
KW  - neural nets
KW  - object detection
KW  - robot vision
KW  - low-latency decoder
KW  - NYU Depth v2 dataset
KW  - real-time monocular depth estimation
KW  - deep neural network
KW  - embedded platform
KW  - microaerial vehicle
KW  - embedded systems
KW  - robotic tasks
KW  - obstacle detection
KW  - single RGB image
KW  - monocular cameras
KW  - lightweight encoder-decoder network architecture
KW  - computational complexity
KW  - FastDepth
KW  - fast monocular depth estimation
KW  - depth sensing
KW  - deep neural networks
KW  - Estimation
KW  - Decoding
KW  - Neural networks
KW  - Runtime
KW  - Convolution
KW  - Complexity theory
KW  - Task analysis
DO  - 10.1109/ICRA.2019.8794182
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.
ER  - 

TY  - CONF
TI  - Sliding Mode Momentum Observers for Estimation of External Torques and Joint Acceleration
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6117
EP  - 6123
AU  - G. Garofalo
AU  - N. Mansfeld
AU  - J. Jankowski
AU  - C. Ott
PY  - 2019
KW  - collision avoidance
KW  - end effectors
KW  - human-robot interaction
KW  - observers
KW  - torque control
KW  - variable structure systems
KW  - external torques
KW  - joint acceleration
KW  - external wrenches
KW  - robot structure
KW  - human-robot interaction
KW  - momentum dynamics
KW  - classic momentum observer
KW  - reaction strategies
KW  - sliding mode momentum observers
KW  - control loop
KW  - proprioceptive sensors
KW  - first-order filtered version
KW  - finite-time convergence
KW  - Observers
KW  - Robots
KW  - Collision avoidance
KW  - Torque
KW  - Convergence
KW  - Noise measurement
DO  - 10.1109/ICRA.2019.8793529
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Interactions between robots and their environment give rise to external wrenches acting on the robot structure. The estimation of the resulting torques in the joints is fundamental in human-robot interaction to detect/identify collisions and perform suitable reaction strategies. Other applications may require to use the estimation for compensating the effects of the external torques within the control loop. The well-established momentum observer, which relies on proprioceptive sensors only, is usually used for these purposes. In this work, the momentum dynamics is used to derive new observers. While the classic momentum observer provides a first-order filtered version of the external torques, here a (theoretically) finite-time convergence is achieved. Simulations and experiments are used to validate the performance of the proposed methods.
ER  - 

TY  - CONF
TI  - Discrete Layer Jamming for Safe Co-Robots
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6124
EP  - 6129
AU  - Y. Zhou
AU  - L. M. Headings
AU  - M. J. Dapino
PY  - 2019
KW  - beams (structures)
KW  - bending
KW  - clamps
KW  - elasticity
KW  - human-robot interaction
KW  - manipulator dynamics
KW  - mechanical testing
KW  - multi-robot systems
KW  - pneumatic actuators
KW  - position control
KW  - safe co-robots
KW  - stiff robot arm
KW  - robot systems
KW  - safe interaction
KW  - tunable stiffness mechanism
KW  - discrete layer jamming mechanism
KW  - robot link
KW  - multiple clamps
KW  - safer human-robot interaction
KW  - discrete layer jamming beam
KW  - injury severity
KW  - clamping pressure
KW  - pneumatic layer jamming
KW  - positioning performance
KW  - payload capacity
KW  - dynamic actuators
KW  - ABS laminates
KW  - aluminum clamps
KW  - stiffness tests
KW  - bending stiffness
KW  - pressure 0.0 MPa to 1.0 MPa
KW  - Clamps
KW  - Robots
KW  - Jamming
KW  - Laminates
KW  - Force
KW  - Friction
KW  - Safety
DO  - 10.1109/ICRA.2019.8793908
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - High injury severity occurs when a stiff robot arm hits an operator. Introducing compliance into robot systems reduces the impact and enables safe interaction, but at the expense of positioning performance and payload capacity. This paper presents a tunable stiffness mechanism for safe human-robot interaction based on discrete layer jamming. The proposed design of a discrete layer jamming mechanism is a robot link made of multiple thin layers of ABS and multiple clamps. By applying high clamping pressure to the laminates, the link behaves like a rigid link; reducing the clamping pressure softens the link which yields safer human-robot interaction. Compared to conventional pneumatic layer jamming, discrete layer jamming allows for simplicity of installation with dynamic actuators, faster control, greater portability since no air supply is needed, and no sealing issues. To validate the concept, this paper investigates a discrete layer jamming beam made of ten ABS laminates and two aluminum clamps that cover 10% of the surface of the beam. Stiffness tests have been performed, showing that around 17 times bending stiffness change is achieved by increasing the clamping pressure of two clamps from 0 to 1 MPa.
ER  - 

TY  - CONF
TI  - Admittance Control for Human-Robot Interaction Using an Industrial Robot Equipped with a F/T Sensor
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6130
EP  - 6136
AU  - E. Mariotti
AU  - E. Magrini
AU  - A. D. Luca
PY  - 2019
KW  - end effectors
KW  - force sensors
KW  - human-robot interaction
KW  - industrial manipulators
KW  - manipulator kinematics
KW  - closed control architecture
KW  - robot dynamics
KW  - low-level joint controllers
KW  - kinematic information
KW  - admittance control law
KW  - whole-body collision detection
KW  - KUKA KR5 Sixx R650 robot
KW  - industrial robot
KW  - physical human-robot interaction
KW  - torque sensors
KW  - pHRI strategy
KW  - end-effector
KW  - force/torque sensor
KW  - ATI F/T sensor
KW  - Robot sensing systems
KW  - Collision avoidance
KW  - Service robots
KW  - Collaboration
KW  - End effectors
KW  - Current measurement
DO  - 10.1109/ICRA.2019.8793657
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We present an approach to safe physical Human-Robot Interaction (pHRI) for industrial robots, including collision detection, distinguishing accidental from intentional contacts, and achieving collaborative tasks. Typical industrial robots have a closed control architecture that accepts only velocity/position reference inputs, there are no joint torque sensors, and little or no information is available to the user on robot dynamics and on low-level joint controllers. Nonetheless, taking also advantage of the presence of a Force/Torque (F/T) sensor at the end-effector, a safe pHRI strategy based on kinematic information, on measurements from joint encoders and motor currents, and on end-effector forces/torques can be realized. An admittance control law has been implemented for collaboration in manual guidance mode, with whole-body collision detection in place both when the robot is in autonomous operation and when is simultaneously collaborating with a human. Several pHRI experiments validate the approach on a KUKA KR5 Sixx R650 robot equipped with an ATI F/T sensor.
ER  - 

TY  - CONF
TI  - Effect of Mechanical Resistance on Cognitive Conflict in Physical Human-Robot Collaboration
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6137
EP  - 6143
AU  - S. Aldini
AU  - A. Akella
AU  - A. K. Singh
AU  - Y. Wang
AU  - M. Carmichael
AU  - D. Liu
AU  - C. Lin
PY  - 2019
KW  - bioelectric potentials
KW  - cognition
KW  - electroencephalography
KW  - human-robot interaction
KW  - medical signal processing
KW  - cognitive conflict
KW  - pHRC
KW  - PEN
KW  - mechanical resistance
KW  - conflict level
KW  - human operator
KW  - direct contact
KW  - intuitiveness
KW  - prediction error negativity
KW  - negative deflection
KW  - event related potential
KW  - physical human-robot collaboration
KW  - Robots
KW  - Task analysis
KW  - Immune system
KW  - Electroencephalography
KW  - Force
KW  - Collaboration
DO  - 10.1109/ICRA.2019.8793748
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Physical Human-Robot Collaboration (pHRC) is about the interaction between one or more human operator(s) and one or more robot(s) in direct contact and voluntarily exchanging forces to accomplish a common task. In any pHRC, the intuitiveness of the interaction has always been a priority, so that the operator can comfortably and safely interact with the robot. So far, the intuitiveness has always been described in a qualitative way. In this paper, we suggest an objective way to evaluate intuitiveness, known as prediction error negativity (PEN) using electroencephalogram (EEG). PEN is defined as a negative deflection in event related potential (ERP) due to cognitive conflict, as a consequence of a mismatch between perception and reality. Experimental results showed that the forces exchanged between robot and human during pHRC modulate the amplitude of PEN, representing different levels of cognitive conflict. We also found that PEN amplitude significantly decreases (p <; 0.05) when a mechanical resistance is being applied smoothly and more time in advance before an invisible obstacle, when compared to a scenario in which the resistance is applied abruptly before the obstacle. These results indicate that an earlier and smoother resistance reduces the conflict level. Consequently, this suggests that smoother changes in resistance make the interaction more intuitive.
ER  - 

TY  - CONF
TI  - Lifelong Learning for Heterogeneous Multi-Modal Tasks
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6158
EP  - 6164
AU  - H. Liu
AU  - F. Sun
AU  - B. Fang
PY  - 2019
KW  - learning (artificial intelligence)
KW  - pattern classification
KW  - heterogeneous modalities
KW  - learned classifier
KW  - multimodal task
KW  - multimodal lifelong learning framework
KW  - consecutive multimodal learning tasks
KW  - multimodal lifelong learning problem
KW  - heterogeneous multimodal tasks
KW  - heterogeneous multimodal fusion
KW  - material recognition task
KW  - online dictionary learning algorithm
KW  - Task analysis
KW  - Dictionaries
KW  - Knowledge based systems
KW  - Machine learning
KW  - Encoding
KW  - Optimization
KW  - Robots
DO  - 10.1109/ICRA.2019.8793517
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this work, we investigate the lifelong learning problem from the viewpoint of heterogeneous multi-modal fusion. The main challenges come from the fact that the common representation between heterogeneous modalities should be persistently learned and the learned classifier for each multi-modal task should be persistently updated. To address this problem, we construct a multi-modal lifelong learning framework which deals with the consecutive multi-modal learning tasks and develop an efficient online dictionary learning algorithm to solve the multi-modal lifelong learning problem. Finally, we perform experimental validation on a complicated material recognition task and show the promising results.
ER  - 

TY  - CONF
TI  - Magnetic-Field-Inspired Navigation for Quadcopter Robot in Unknown Environments
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6165
EP  - 6171
AU  - A. Ataka
AU  - H. K. Lam
AU  - K. Althoefer
PY  - 2019
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - control engineering computing
KW  - helicopters
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - flying robots
KW  - local sensory information
KW  - quadcopter robot
KW  - magnetic-field-inspired robot navigation
KW  - under-actuated quad-copter
KW  - arbitrary-shaped convex obstacles
KW  - magnetic field interaction
KW  - reactive navigation algorithms
KW  - unknown environments
KW  - motion commands
KW  - local minima configurations
KW  - dynamic model
KW  - commercial AscTec Pelican microaerial vehicle
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Navigation
KW  - Wires
KW  - Force
DO  - 10.1109/ICRA.2019.8793682
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper, a magnetic-field-inspired robot navigation is used to navigate an under-actuated quad-copter towards the desired position amidst previously-unknown arbitrary-shaped convex obstacles. Taking inspiration from the phenomena of magnetic field interaction with charged particles observed in nature, the algorithm outperforms previous reactive navigation algorithms for flying robots found in the literature as it is able to reactively generate motion commands relying only on a local sensory information without prior knowledge of the obstacles' shape or location and without getting trapped in local minima configurations. The application of the algorithm in a dynamic model of quadcopter system and in the realistic model of the commercial AscTec Pelican micro-aerial vehicle confirm the superior performance of the algorithm.
ER  - 

TY  - CONF
TI  - Human-Care Rounds Robot with Contactless Breathing Measurement
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6172
EP  - 6177
AU  - R. Saegusa
AU  - H. Ito
AU  - D. M. Duong
PY  - 2019
KW  - biomedical measurement
KW  - handicapped aids
KW  - health care
KW  - human-robot interaction
KW  - medical robotics
KW  - mobile robots
KW  - patient care
KW  - pneumodynamics
KW  - robot vision
KW  - physical support
KW  - care staffs
KW  - medical facilities
KW  - vision-based contactless breathing measurement system
KW  - human detection
KW  - nursing facility
KW  - human measurement system
KW  - autonomous rounds robot
KW  - breathing measurement system
KW  - human-care rounds robot
KW  - three-dimensional shape
KW  - thermal information
KW  - Lucia robot
KW  - body postures
KW  - Robot kinematics
KW  - Machine vision
KW  - Monitoring
KW  - Medical services
KW  - Cameras
KW  - Frequency-domain analysis
DO  - 10.1109/ICRA.2019.8794037
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper describes a human measurement system for autonomous rounds robot aiming in physical support of care staffs working in nursing and medical facilities. We propose a novel scheme of vision-based contactless breathing measurement system that integrates three-dimensional shape and thermal information of the target person. We developed the human-care rounds robot Lucia and implemented the measurement system on the robot. We then evaluated the human detection and breathing measurement based on the conditions of real incident cases that occurred in the nursing facility for the physically handicapped. In the experiments, we examined that the breathing measurement system successfully measured volume variation of human subjects with different configurations of body postures. The robot is also capable of discrimination between the breathing and non-breathing states of targets based on the difference in the power spectrum patterns in the frequency domain. The experimental results showed that the proposed system detected the presence of breathing within the accuracy of about 90% or more, and moreover, the ability of anomaly detection in breathing was suggested.
ER  - 

TY  - CONF
TI  - An Improved Control-Oriented Modeling of the Magnetic Field
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6178
EP  - 6184
AU  - M. Etiévant
AU  - A. Bolopion
AU  - S. Régnier
AU  - N. Andreff
PY  - 2019
KW  - closed loop systems
KW  - coils
KW  - interpolation
KW  - magnetic fields
KW  - microrobots
KW  - mobile robots
KW  - motion control
KW  - control-oriented model
KW  - coil
KW  - untethered microscale mobile robotics
KW  - elliptic integral functions
KW  - magnetically actuated microrobots
KW  - map-based interpolation
KW  - computation time
KW  - closed-loop control
KW  - dipole approximation
KW  - Magnetic domains
KW  - Computational modeling
KW  - Magnetic hysteresis
KW  - Soft magnetic materials
KW  - Numerical models
KW  - Robots
KW  - Magnetic cores
DO  - 10.1109/ICRA.2019.8793679
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper proposes a new control-oriented model to compute the magnetic field created by a coil. A major challenge for untethered microscale mobile robotics is the control of objects for precise and fast displacements. In this work, we propose to use an alternative implementation of a model based on elliptic integral functions to control magnetically actuated micro-robots. It allows to compute the magnetic field even in the area close to the coil quickly and accurately. This model is evaluated numerically and compared to classical approaches - dipole approximation, map-based interpolation and classical elliptic integral models - in terms of accuracy, computation time and memory requirement. Simulation results show that this works allows to have an accurate model in the whole workspace by avoiding numerical issues encountered in previous works. It can be computed in a few milliseconds, making it the right candidate for closed-loop control of magnetically actuated micro-robots.
ER  - 

TY  - CONF
TI  - Efficient Micro Waveguide Coupling based on Microrobotic Positioning
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6185
EP  - 6190
AU  - P. Wang
AU  - D. Li
AU  - H. Lu
AU  - Y. Yang
AU  - S. Shen
AU  - Y. Shen
PY  - 2019
KW  - fuzzy control
KW  - integrated optics
KW  - micro-optics
KW  - microrobots
KW  - optical fibre couplers
KW  - optical microscopy
KW  - path planning
KW  - fuzzy controller
KW  - degrees of freedoms
KW  - microwaveguide coupling
KW  - path planning strategy
KW  - integrated optical component
KW  - optical fiber
KW  - micromanufacture field
KW  - traditional manual method
KW  - optical microscopy
KW  - microrobotic positioning system
KW  - light intensity feedback
KW  - commercial optoelectronic devices
KW  - optical devices
KW  - time 40.0 s
KW  - Couplings
KW  - Optical fibers
KW  - Optical fiber sensors
KW  - Optical distortion
DO  - 10.1109/ICRA.2019.8794444
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Coupling the endface of an optical fiber to an integrated optical component is currently a low-throughput and costly manual process in the fabrication of the optical devices. In order to meet the high-volume demand for commercial optoelectronic devices, coupling must be automated. This paper presents a robotic positioning system and corresponding path planning strategy based on both the position and light intensity feedback. In this work, a micro-robotic positioning system with 3 degrees of freedoms (DOFs) is developed and integrated with an optical microscopy. Then the fuzzy controller is developed to design the trajectory. Lastly, simulation and experimental results demonstrate the accuracy and efficiency of the proposed system. Compared with the traditional manual method, the robotic positioning system can realize the coupling within 40 seconds. This method will have a significant impact on the automatic process of the micro manufacture field.
ER  - 

TY  - CONF
TI  - Assembly of Multilayered Hepatic Lobule-like Vascular Network by using Heptapole Magnetic Tweezer
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6200
EP  - 6205
AU  - E. Kim
AU  - M. Takeuchi
AU  - T. Kozuka
AU  - T. Nomura
AU  - A. Hasegawa
AU  - A. Ichikawa
AU  - Q. Huang
AU  - T. Fukuda
PY  - 2019
KW  - biomedical materials
KW  - blood
KW  - blood vessels
KW  - cellular biophysics
KW  - hydrogels
KW  - liver
KW  - multilayers
KW  - steel
KW  - rat liver cells
KW  - fibrin gel
KW  - cell viability
KW  - cellular structure
KW  - 3D channel network
KW  - magnetic tweezer
KW  - magnetic fields
KW  - central veins
KW  - portal veins
KW  - hepatic lobule tissue
KW  - magnetic hydrogel fibers
KW  - multilayered channel system
KW  - cell-laden hydrogels
KW  - heptapole magnetic tweezer
KW  - multilayered hepatic lobule-like vascular network
KW  - steel rods
KW  - Steel
KW  - Three-dimensional displays
KW  - Optical fiber networks
KW  - Veins
KW  - Magnetic flux
KW  - Magnetic fields
KW  - Magnetic multilayers
DO  - 10.1109/ICRA.2019.8794210
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper, we have fabricated a multilayered hepatic lobule-like vascular network in a 3D tissue using a heptapole magnetic tweezer. The tissue consists of cell-laden hydrogels with 3D channel networks. To fabricate multilayered channel system, magnetic hydrogel fibers were manipulated by a magnetic tweezer. The hepatic lobule tissue shows a hexagonal structure with different sizes of veins. Six portal veins transfer the blood including nutrients and oxygen to a central vein by sinusoids. The portal and central veins are made by steel rods, whereas the magnetic hydrogel fibers has a role of sinusoids. An important point of this research is to connect two veins - portal and central vein - by magnetic fibers. For this, we used magnetic tweezer with seven poles to magnetize the steel rods. In order to generate high magnetic fields, we design magnetic tweezer with a flat tip and additional lower tweezer based on simulation data. The manipulation was performed in fibrin gel inside rat liver cells. By applying high magnetic fields, we attracted magnetic fibers to the steel rods and constructed 3D channel network in cellular structure. To verify the efficiency of the channel, we supply culture medium to the channel and then analyze the cell viability according to the distance from the channel. As a result, the cells located at close to the channel show higher cell viability than others.
ER  - 

TY  - CONF
TI  - Sizing the aortic annulus with a robotised, commercially available soft balloon catheter: in vitro study on idealised phantoms
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6230
EP  - 6236
AU  - A. Palombi
AU  - G. M. Bosi
AU  - S. D. Giuseppe
AU  - E. De Momi
AU  - S. Homer-Vanniasinkam
AU  - G. Burriesci
AU  - H. A. Wurdemann
PY  - 2019
KW  - blood vessels
KW  - cardiology
KW  - catheters
KW  - diseases
KW  - iterative methods
KW  - medical robotics
KW  - phantoms
KW  - prosthetics
KW  - surgery
KW  - intraballoon pressure
KW  - soft balloon catheter
KW  - heart valve diseases
KW  - linear regression
KW  - iterative method
KW  - pressure-volume data
KW  - idealised aortic phantoms
KW  - balloon free inflation
KW  - inflation device
KW  - robotised valvuloplasty balloon catheter
KW  - cardiac electrical signal
KW  - prosthetic valve leakage
KW  - implanted prosthetic valve
KW  - aortic heart valve diseases
KW  - minimally invasive surgical technique
KW  - transcatheter aortic valve implantation
KW  - aortic annulus
KW  - Valves
KW  - Catheters
KW  - Prosthetics
KW  - Robots
KW  - Three-dimensional displays
KW  - Geometry
KW  - Mathematical model
DO  - 10.1109/ICRA.2019.8794041
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Transcatheter aortic valve implantation (TAVI) is a minimally invasive surgical technique to treat aortic heart valve diseases. According to current clinical guidelines, the implanted prosthetic valve replacing the native one is selected based on pre-operative size assessment of the aortic annulus through different imaging techniques. That very often leads to suboptimal device selection resulting in major complications, such as prosthetic valve leakage or interruption of the cardiac electrical signal. In this paper, we propose a new, intra-operative approach to determine the diameter of theaortic annulus exploiting intra-balloon pressure and volume data, acquired from a robotised valvuloplasty balloon catheter. An inflation device, capable of collecting real-time intra-balloon pressure and volume data, was designed and interfaced with a commercially available valvuloplasty balloon catheter. A sizing algorithm allowing to precisely estimate the annular diameter was integrated. The algorithm relies on a characterised analytical model of the balloon free inflation and an iterative method based on linear regression. In vitro tests were performed on idealised aortic phantoms. Experimental results show that pressure-volume data can be used to determine annular diameters bigger than the unstretched diameter of the balloon catheter. For these cases, the proposed approach exhibited good precision (maximum average error 0.93%) and good repeatability (maximum standard deviation ±0.11 mm).
ER  - 

TY  - CONF
TI  - Miniature Robotic Tubes with Rotational Tip-Joints as a Medical Delivery Platform
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6237
EP  - 6243
AU  - R. Karthikeyan
AU  - S. Pattanshetti
AU  - S. C. Ryu
PY  - 2019
KW  - controllability
KW  - hinges
KW  - medical robotics
KW  - motion control
KW  - needles
KW  - position control
KW  - robot kinematics
KW  - surgery
KW  - miniature robotic tubes
KW  - rotational tip-joints
KW  - medical delivery platform
KW  - medical-needle-sized robotic tube
KW  - instrument tip
KW  - two-axis laser micromachining
KW  - direct tip controllability
KW  - human body
KW  - hinged instrument
KW  - flexure joints
KW  - shorter joint length
KW  - compact articulation
KW  - joint strength
KW  - lateral directions
KW  - robust delivery platform
KW  - hinge rotation
KW  - instrumented prototype
KW  - laser beam
KW  - fine angle control
KW  - kinematic model
KW  - tip motion control
KW  - Fasteners
KW  - Joints
KW  - Robots
KW  - Instruments
KW  - Electron tubes
KW  - Kinematics
KW  - Tendons
DO  - 10.1109/ICRA.2019.8794391
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We present a medical-needle-sized robotic tube with multi-degrees of freedom (M-DOF) rotational hinge joints at the instrument tip, fabricated by two-axis laser micro-machining. Due to the presence of an ample working channel and direct tip controllability, this tube is a potential candidate for the precise delivery of radioactive seeds, probes and micro-forceps to regions of interest within the human body. In this paper, the advantages of the proposed hinged instrument are studied in contrast with that of flexure joints, which include-fine angle control (posability), and a shorter joint length that enables compact articulation. In addition, the joint strength both in the axial and lateral directions was experimentally investigated to demonstrate its feasibility as a robust delivery platform. Further, the intuitive nature of hinge rotation permits the use of a simple kinematic model for accurate tip motion control, under fewer simplifying assumptions than flexure joints which are impeded by material non-linearity and geometric discontinuities. An instrumented prototype was used to test this model by delivering a laser beam along a prescribed path (synonymous to simple ablation tasks). The observed RMS position error for the projected beam was ~0.364 mm.
ER  - 

TY  - CONF
TI  - Nonlinear System Identification of Soft Robot Dynamics Using Koopman Operator Theory
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6244
EP  - 6250
AU  - D. Bruder
AU  - C. D. Remy
AU  - R. Vasudevan
PY  - 2019
KW  - autoregressive processes
KW  - identification
KW  - learning (artificial intelligence)
KW  - mean square error methods
KW  - mobile robots
KW  - neural nets
KW  - nonlinear control systems
KW  - nonlinear dynamical systems
KW  - pneumatic actuators
KW  - regression analysis
KW  - robot dynamics
KW  - state-space methods
KW  - soft robot dynamics
KW  - Koopman operator theory
KW  - large-scale data collection
KW  - system identification method
KW  - nonlinear dynamical systems
KW  - linear regression
KW  - linear representation
KW  - infinite-dimensional space
KW  - nonlinear system identification methods
KW  - dynamic model
KW  - pneumatic soft robot arm
KW  - linear state space model
KW  - nonlinear Hammerstein-Wiener model
KW  - neural network
KW  - total normalized-root-mean-square error
KW  - NRMSE
KW  - Soft robotics
KW  - Nonlinear dynamical systems
KW  - Predictive models
KW  - Linear regression
KW  - Data models
KW  - Tuning
DO  - 10.1109/ICRA.2019.8793766
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Soft robots are challenging to model due in large part to the nonlinear properties of soft materials. Fortunately, this softness makes it possible to safely observe their behavior under random control inputs, making them amenable to large-scale data collection and system identification. This paper implements and evaluates a system identification method based on Koopman operator theory in which models of nonlinear dynamical systems are constructed via linear regression of observed data by exploiting the fact that every nonlinear system has a linear representation in the infinite-dimensional space of real-valued functions called observables. The approach does not suffer from some of the shortcomings of other nonlinear system identification methods, which typically require the manual tuning of training parameters and have limited convergence guarantees. A dynamic model of a pneumatic soft robot arm is constructed via this method, and used to predict the behavior of the real system. The total normalized-root-mean-square error (NRMSE) of its predictions is lower than that of several other identified models including a neural network, NLARX, nonlinear Hammerstein-Wiener, and linear state space model.
ER  - 

TY  - CONF
TI  - Data Driven Inverse Kinematics of Soft Robots using Local Models
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6251
EP  - 6257
AU  - F. Holsten
AU  - M. P. Engell-Nørregård
AU  - S. Darkner
AU  - K. Erleben
PY  - 2019
KW  - computational complexity
KW  - data visualisation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot kinematics
KW  - data driven inverse kinematics
KW  - soft robot
KW  - computational approaches
KW  - data quantity
KW  - memory complexity
KW  - time complexity
KW  - actuation system
KW  - visual markers
KW  - motion planning
KW  - motion control
KW  - learning cube environment
KW  - Shape
KW  - Soft robotics
KW  - Sensors
KW  - Kinematics
KW  - Calibration
KW  - Computational modeling
DO  - 10.1109/ICRA.2019.8794191
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Soft robots are advantageous in terms of flexibility, safety and adaptability. It is challenging to find efficient computational approaches for planning and controlling their motion. This work takes a direct data-driven approach to learn the kinematics of the three-dimensional shape of a soft robot, by using visual markers. No prior information about the robot at hand is required. The model is oblivious to the design of the robot and type of actuation system. This allows adaptation to erroneous manufacturing. We present a highly versatile and inexpensive learning cube environment for collecting and analysing data. We prove that using multiple, lower order models of data opposed to one global, higher order model, will reduce the required data quantity, time complexity and memory complexity significantly without compromising accuracy. Further, our approach allows for embarrassingly parallelism. Yielding an overall much more simple and efficient approach.
ER  - 

TY  - CONF
TI  - Modal Dynamics and Analysis of a Vertical Stretch-Retractable Continuum Manipulator with Large Deflection
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6258
EP  - 6264
AU  - H. Wang
AU  - G. Gao
AU  - Q. Xia
AU  - X. Zhang
PY  - 2019
KW  - beams (structures)
KW  - bending
KW  - dynamic response
KW  - elasticity
KW  - integral equations
KW  - manipulator dynamics
KW  - vibrations
KW  - vertical stretch-retractable continuum manipulator
KW  - backbone continuum manipulator
KW  - deformation modal properties
KW  - tip horizontal load
KW  - elastic bending potential energy
KW  - free dynamic response
KW  - modal dynamic models
KW  - static model
KW  - vibration modal characteristics
KW  - equivalent-guided beam
KW  - frequency properties
KW  - elliptic integral approach
KW  - Manipulator dynamics
KW  - Analytical models
KW  - Potential energy
KW  - Shape
KW  - Dynamics
KW  - Force
DO  - 10.1109/ICRA.2019.8794172
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Efficient and reliable dynamic modelling and analysis is critical to the shape control and estimate of a backbone continuum manipulator. This paper presents a novel dynamic modelling method to investigate the deformation modal properties of a vertical stretch-retractable continuum manipulator (VSRCM) under the tip horizontal load. Based on the equivalence of the elastic bending potential energy, this method simplifies the backbone continuum manipulator to a single beam, an equivalent-guided beam (EGB). One end of the EGB is fixed, and the other end is guided. The modal dynamics model of the EGB is then established by employing the vibration theory of a continuous beam, and the modal properties of the continuum manipulator are analyzed. Static and Dynamic experiments of the VSRCM are performed to validate the dynamic modeling method through the comparison and analysis of the free dynamic response and frequency properties of the continuum manipulator. To analyze the free dynamic response, the large deflection static model with tip horizontal load is also established based on the elliptic integral approach. Experimental results and comparison demonstrate that the static model is efficient and precise to predict the large deformation, and the modal dynamic models well reflect the actual vibration modal characteristics of the VSRCM.
ER  - 

TY  - CONF
TI  - ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6265
EP  - 6271
AU  - Y. Hu
AU  - J. Liu
AU  - A. Spielberg
AU  - J. B. Tenenbaum
AU  - W. T. Freeman
AU  - J. Wu
AU  - D. Rus
AU  - W. Matusik
PY  - 2019
KW  - deformation
KW  - elasticity
KW  - gradient methods
KW  - inverse problems
KW  - least squares approximations
KW  - manipulator dynamics
KW  - mobile robots
KW  - multi-robot systems
KW  - optimal control
KW  - optimisation
KW  - path planning
KW  - ChainQueen
KW  - real-time differentiable physical simulator
KW  - robot planning
KW  - gradient-based optimization algorithms
KW  - inverse problems
KW  - optimal control
KW  - motion planning
KW  - rigid body simulators
KW  - deformable objects
KW  - rigid body dynamics
KW  - Lagrangian-Eulerian physical simulator
KW  - MLS-MPM
KW  - soft robotic systems
KW  - forward simulation
KW  - backward gradient computation
KW  - moving least squares material point method
KW  - Graphics processing units
KW  - Computational modeling
KW  - Soft robotics
KW  - Three-dimensional displays
KW  - Planning
KW  - Inverse problems
DO  - 10.1109/ICRA.2019.8794333
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Therefore, rigid body simulators and recently their differentiable variants are studied extensively. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and there-fore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects with collisions and can be seamlessly incorporated into soft robotic systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of inference, control and co-design tasks for soft robotics.
ER  - 

TY  - CONF
TI  - A Validated Physical Model For Real-Time Simulation of Soft Robotic Snakes
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6272
EP  - 6279
AU  - R. Gasoto
AU  - M. Macklin
AU  - X. Liu
AU  - Y. Sun
AU  - K. Erleben
AU  - C. Onal
AU  - J. Fu
PY  - 2019
KW  - actuators
KW  - biomimetics
KW  - closed loop systems
KW  - deformation
KW  - legged locomotion
KW  - pneumatic actuators
KW  - robot dynamics
KW  - constraint-based dynamics model
KW  - multiphysics environment
KW  - soft robotic actuators
KW  - real-time simulation
KW  - validated physical model
KW  - real-time performance
KW  - dynamic locomotion open-loop control experiments
KW  - multiple 1D actuators
KW  - soft robotic snake
KW  - internal pressure forces
KW  - 1-dimensional pneumatic soft actuator
KW  - Actuators
KW  - Deformable models
KW  - Soft robotics
KW  - Strain
KW  - Finite element analysis
KW  - Mathematical model
DO  - 10.1109/ICRA.2019.8794375
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this work we present a framework that is capable of accurately representing soft robotic actuators in a multiphysics environment in real-time. We propose a constraint-based dynamics model of a 1-dimensional pneumatic soft actuator that accounts for internal pressure forces, as well as the effect of actuator latency and damping under inflation and deflation and demonstrate its accuracy a full soft robotic snake with the composition of multiple 1D actuators. We verify our model's accuracy in static deformation and dynamic locomotion open-loop control experiments. To achieve real-time performance we leverage the parallel computation power of GPUs to allow interactive control and feedback.
ER  - 

TY  - CONF
TI  - SpaceBok: A Dynamic Legged Robot for Space Exploration
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6288
EP  - 6294
AU  - P. Arm
AU  - R. Zenkl
AU  - P. Barton
AU  - L. Beglinger
AU  - A. Dietsche
AU  - L. Ferrazzini
AU  - E. Hampp
AU  - J. Hinder
AU  - C. Huber
AU  - D. Schaufelberger
AU  - F. Schmitt
AU  - B. Sun
AU  - B. Stolz
AU  - H. Kolvenbach
AU  - M. Hutter
PY  - 2019
KW  - aerospace robotics
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - SpaceBok
KW  - dynamic legged robot
KW  - space exploration
KW  - quadrupedal robot
KW  - dynamic legged locomotion
KW  - parallel elastic elements
KW  - high-torque brushless motors
KW  - force control
KW  - walking velocity
KW  - planetary gear transmissions
KW  - optimized parallel motion
KW  - jumping maneuvers
KW  - Legged locomotion
KW  - Hip
KW  - Actuators
KW  - Gravity
KW  - Torque
KW  - Foot
DO  - 10.1109/ICRA.2019.8794136
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper introduces SpaceBok, a quadrupedal robot created to investigate dynamic legged locomotion for the exploration of low-gravity celestial bodies. With a hip height of 500 mm and a mass of 20 kg, its dimensions are comparable to a medium-sized dog. The robot's leg configuration is based on an optimized parallel motion mechanism that allows the integration of parallel elastic elements to store and release energy for powerful jumping maneuvers. High-torque brushless motors in combination with customized single-stage planetary gear transmissions enable force control at the foot contact points based on motor currents. We present successful walking, trotting, and pronking experiments. Thereby, Spacebok achieved maximal jump heights in single jump experiments of up to 1.05 m (more than twice the hip height) and a walking velocity of 1m/s. Moreover, simulation results for low gravity on the moon suggest that our robot can move with up to 1.1m/s at an approximate cost of transport of 1 in moon gravity when using the pronking gait.
ER  - 

TY  - CONF
TI  - Mini Cheetah: A Platform for Pushing the Limits of Dynamic Quadruped Control
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6295
EP  - 6301
AU  - B. Katz
AU  - J. D. Carlo
AU  - S. Kim
PY  - 2019
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - nonlinear control systems
KW  - optimisation
KW  - predictive control
KW  - robot dynamics
KW  - Mini Cheetah
KW  - powerful robot
KW  - mechanically robust quadruped robot
KW  - control systems
KW  - legged robots
KW  - custom backdriveable modular actuators
KW  - high-bandwidth force control
KW  - high force density
KW  - dynamic trot
KW  - dynamic quadruped control
KW  - convex model-predictive control
KW  - cMPC
KW  - offline nonlinear optimization
KW  - size 0.3 m
KW  - mass 9.0 kg
KW  - Actuators
KW  - Legged locomotion
KW  - Torque
KW  - Bandwidth
KW  - Knee
KW  - Torque control
DO  - 10.1109/ICRA.2019.8793865
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Mini Cheetah is a small and inexpensive, yet powerful and mechanically robust quadruped robot, intended to enable rapid development of control systems for legged robots. The robot uses custom backdriveable modular actuators, which enable high-bandwidth force control, high force density, and robustness to impacts. Standing around 0.3 m tall and weighing 9 kg, Mini Cheetah can easily be handled by a single operator. We have demonstrated dynamic trot, trot-run, bounding, and pronking gaits on the robot to speeds of up to 2.45 meters per second using Convex Model-Predictive Control (cMPC). In addition to locomotion, we have used the robot to execute 360° backflips, with trajectories generated using offline nonlinear optimization.
ER  - 

TY  - CONF
TI  - Optimal Leg Sequencing for a Hexapod Subject to External Forces and Slopes
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6302
EP  - 6308
AU  - G. Rekleitis
AU  - M. Vidakis
AU  - E. Papadopoulos
PY  - 2019
KW  - force control
KW  - gait analysis
KW  - legged locomotion
KW  - stability
KW  - hexapod subject
KW  - optimal leg sequence selection method
KW  - hexapod robot stability
KW  - terrain slope
KW  - stable leg sequence
KW  - search method
KW  - force-angle stability margin criterion
KW  - Legged locomotion
KW  - Stability criteria
KW  - Robot sensing systems
KW  - Foot
DO  - 10.1109/ICRA.2019.8793826
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - An optimal leg sequence selection method is developed, which maximizes hexapod robot stability, considering feasible gaits, motion modes, and terrain slope. A novel and fast search method is employed to find the most stable leg sequence for a given gait; if no such sequence exists, the next fastest stable gait is chosen and the most stable leg sequence for this gait is selected. The method can be based on any stability measure; here the Force-Angle Stability Margin criterion is employed that is sensitive to top-heaviness, and inertial and external forces. Results show that the developed method senses instabilities accurately and selects the best leg sequence for maximum stability far faster than exhaustive searches, offering distinct advantages when varied external forces are applied.
ER  - 

TY  - CONF
TI  - Stanford Doggo: An Open-Source, Quasi-Direct-Drive Quadruped
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6309
EP  - 6315
AU  - N. Kau
AU  - A. Schultz
AU  - N. Ferrante
AU  - P. Slade
PY  - 2019
KW  - legged locomotion
KW  - robot dynamics
KW  - robot kinematics
KW  - legged robots
KW  - quasidirect-drive quadruped
KW  - open-source
KW  - quasidirect-drive design methodology
KW  - performing animal
KW  - average vertical speed
KW  - vertical jumping agility
KW  - common performance metrics
KW  - dynamic locomotion
KW  - Stanford Doggo
KW  - Legged locomotion
KW  - Torque
KW  - Force
KW  - Measurement
KW  - Robot sensing systems
KW  - Foot
DO  - 10.1109/ICRA.2019.8794436
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper presents Stanford Doggo, a quasi-direct-drive quadruped capable of dynamic locomotion. This robot matches or exceeds common performance metrics of state-of-the-art legged robots. In terms of vertical jumping agility, a measure of average vertical speed, Stanford Doggo matches the best performing animal and surpasses the previous best robot by 22%. An overall design architecture is presented with focus on our quasi-direct-drive design methodology. The hardware and software to replicate this robot is open-source, requires only hand tools for manufacturing and assembly, and costs less than $3000.
ER  - 

TY  - CONF
TI  - Workspace CPG with Body Pose Control for Stable, Directed Vision during Omnidirectional Locomotion
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6316
EP  - 6322
AU  - S. Shaw
AU  - G. Sartoretti
AU  - J. Olkin
AU  - W. Paivine
AU  - H. Choset
PY  - 2019
KW  - legged locomotion
KW  - motion control
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - vision system
KW  - DOF
KW  - central pattern generator
KW  - degree-of-freedom
KW  - gaze orientation
KW  - body velocity
KW  - path planning
KW  - legged robot
KW  - omnidirectional locomotion
KW  - hexapod robot
KW  - CPG framework
KW  - body pose control
KW  - robot body
KW  - Foot
KW  - Machine vision
KW  - Legged locomotion
KW  - Trajectory
KW  - Transforms
KW  - Phase change materials
DO  - 10.1109/ICRA.2019.8794313
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper, we focus on the problem of directing the gaze of a vision system mounted to the body of a high-degree-of-freedom (DOF) legged robot for active perception deployments. In particular, we consider the case where the vision system is rigidly attached to the robot's body (i.e., without any additional DOF between the vision system and robot body) and show how the supernumerary DOFs of the robot can be leveraged to allow independent locomotion and gaze control. Specifically, we augment a workspace central pattern generator (CPG) with omnidirectional capabilities by coupling it with a body pose control mechanism. We leverage the smoothing nature of the CPG framework to allow online adaptation of relevant locomotion parameters, and obtain a stable mid-level controller that translates desired gaze orientation and body velocity directly into joint angles. We validate our approach on an 18-DOF hexapod robot, in a series of indoor and outdoor trials, where the robot inspects an environmental feature or follows a pre-planned path relative to a visually-tracked landmark, demonstrating simultaneous locomotion and directed vision.
ER  - 

TY  - CONF
TI  - Robotic Forceps without Position Sensors using Visual SLAM
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6331
EP  - 6336
AU  - T. Iwai
AU  - T. Kanno
AU  - T. Miyazaki
AU  - T. Kawase
AU  - K. Kawashima
PY  - 2019
KW  - cameras
KW  - mobile robots
KW  - position control
KW  - robot vision
KW  - servomechanisms
KW  - SLAM (robots)
KW  - visual servoing
KW  - robotic forceps
KW  - position sensors
KW  - visual SLAM
KW  - wrist joint
KW  - joint angle sensing
KW  - rear end
KW  - rear joint
KW  - parallel linkage
KW  - monocular camera
KW  - position sensing
KW  - joint angles
KW  - visual servo system
KW  - static experiments
KW  - dynamic positioning experiments
KW  - visual servoing system
KW  - forceps tip
KW  - Cameras
KW  - Robot vision systems
KW  - Visualization
KW  - Gravity
DO  - 10.1109/ICRA.2019.8794321
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this study, a robotic forceps with a wrist joint using visual SLAM for joint angle sensing was developed. The forceps has a flexible joint connected to the wrist joint at its rear end and the motion of the rear joint is driven by a parallel linkage. A monocular camera attached on the rear of the parallel linkage is in charge of position sensing, and the joint angles are estimated from the pose of the camera. The pose of the camera is obtained by a visual SLAM. The visual servo system realizes a simple attaching mechanism. The static and dynamic positioning experiments are conducted. We confirmed that the visual servoing system controls the forceps tip within the error of 3 deg in the motion range of 50 deg.
ER  - 

TY  - CONF
TI  - 3D Keypoint Repeatability for Heterogeneous Multi-Robot SLAM
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6337
EP  - 6343
AU  - E. R. Boroson
AU  - N. Ayanian
PY  - 2019
KW  - feature extraction
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - SLAM (robots)
KW  - point cloud registration
KW  - loop closure
KW  - heterogenous multirobot SLAM applications
KW  - NARF detector
KW  - 3D keypoint repeatability
KW  - heterogeneous multirobot SLAM
KW  - multirobot SLAM scenario
KW  - sensor measurement point clouds
KW  - point density
KW  - 3D keypoint detectors
KW  - multirobot SLAM system
KW  - KPQ-SI
KW  - relative repeatability
KW  - Three-dimensional displays
KW  - Detectors
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Laser radar
DO  - 10.1109/ICRA.2019.8793609
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - For robots with different types of sensors, loop closure in a multi-robot SLAM scenario requires keypoints that can be matched between sensor measurement point clouds with different properties such as point density and noise. In this paper, we evaluate the performance of several 3D keypoint detectors (Harris3D, ISS, KPQ, KPQ-SI, and NARF) for repeatability between scans from different sensors towards building a heterogeneous multi-robot SLAM system. We find that KPQ-SI and NARF have the best relative repeatability, with KPQ-SI finding more keypoints overall and a higher number of repeatable keypoints, at the cost of significantly worse computational performance. In scans of the same area from different poses, both detectors find enough keypoints for point cloud registration and loop closure. For heterogenous multirobot SLAM applications with computational or bandwidth restrictions, the NARF detector consistently finds repeatable keypoints while also allowing for real-time performance.
ER  - 

TY  - CONF
TI  - ROVO: Robust Omnidirectional Visual Odometry for Wide-baseline Wide-FOV Camera Systems
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6344
EP  - 6350
AU  - H. Seok
AU  - J. Lim
PY  - 2019
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image matching
KW  - image sequences
KW  - lenses
KW  - motion estimation
KW  - optimisation
KW  - pose estimation
KW  - stereo image processing
KW  - hybrid projection model
KW  - multiview P3P RANSAC algorithm
KW  - multiview images
KW  - wide-baseline wide-FOV camera systems
KW  - robust visual odometry system
KW  - wide-baseline camera rig
KW  - field-of-view fisheye lenses
KW  - omnidirectional stereo observations
KW  - ego-motion estimation
KW  - VO pipeline
KW  - feature matching
KW  - pose estimation
KW  - omnidirectional visual odometry
KW  - Cameras
KW  - Robot vision systems
KW  - Visual odometry
KW  - Calibration
KW  - Pose estimation
DO  - 10.1109/ICRA.2019.8793758
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper we propose a robust visual odometry system for a wide-baseline camera rig with wide field-of-view (FOV) fisheye lenses, which provides full omnidirectional stereo observations of the environment. For more robust and accurate ego-motion estimation we adds three components to the standard VO pipeline, 1) the hybrid projection model for improved feature matching, 2) multi-view P3P RANSAC algorithm for pose estimation, and 3) online update of rig extrinsic parameters. The hybrid projection model combines the perspective and cylindrical projection to maximize the overlap between views and minimize the image distortion that degrades feature matching performance. The multi-view P3P RANSAC algorithm extends the conventional P3P RANSAC to multi-view images so that all feature matches in all views are considered in the inlier counting for robust pose estimation. Finally the online extrinsic calibration is seamlessly integrated in the backend optimization framework so that the changes in camera poses due to shocks or vibrations can be corrected automatically. The proposed system is extensively evaluated with synthetic datasets with ground-truth and real sequences of highly dynamic environment, and its superior performance is demonstrated.
ER  - 

TY  - CONF
TI  - SLAMBench 3.0: Systematic Automated Reproducible Evaluation of SLAM Systems for Robot Vision Challenges and Scene Understanding
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6351
EP  - 6358
AU  - M. Bujanca
AU  - P. Gafton
AU  - S. Saeedi
AU  - A. Nisbet
AU  - B. Bodin
AU  - M. F. P. O'Boyle
AU  - A. J. Davison
AU  - P. H. J. Kelly
AU  - G. Riley
AU  - B. Lennox
AU  - M. Luján
AU  - S. Furber
PY  - 2019
KW  - control engineering computing
KW  - convolutional neural nets
KW  - image reconstruction
KW  - natural scenes
KW  - robot vision
KW  - SLAM (robots)
KW  - scene understanding
KW  - nonrigid environments
KW  - dynamic SLAM
KW  - SLAMBench 3
KW  - evaluation infrastructure
KW  - systematic automated reproducible evaluation
KW  - robot vision
KW  - visual SLAM
KW  - SLAM research area
KW  - visulation aids
KW  - visulation metrics
KW  - convolutional neural networks
KW  - dynamicfusion
KW  - Simultaneous localization and mapping
KW  - Semantics
KW  - Three-dimensional displays
KW  - Measurement
KW  - Benchmark testing
KW  - Heuristic algorithms
KW  - C++ languages
DO  - 10.1109/ICRA.2019.8794369
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - As the SLAM research area matures and the number of SLAM systems available increases, the need for frameworks that can objectively evaluate them against prior work grows. This new version of SLAMBench moves beyond traditional visual SLAM, and provides new support for scene understanding and non-rigid environments (dynamic SLAM). More concretely for dynamic SLAM, SLAMBench 3.0 includes the first publicly available implementation of DynamicFusion, along with an evaluation infrastructure. In addition, we include two SLAM systems (one dense, one sparse) augmented with convolutional neural networks for scene understanding, together with datasets and appropriate metrics. Through a series of use-cases, we demonstrate the newly incorporated algorithms, visulation aids and metrics (6 new metrics, 4 new datasets and 5 new algorithms).
ER  - 

TY  - CONF
TI  - Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6359
EP  - 6365
AU  - T. Shen
AU  - Z. Luo
AU  - L. Zhou
AU  - H. Deng
AU  - R. Zhang
AU  - T. Fang
AU  - L. Quan
PY  - 2019
KW  - motion estimation
KW  - pose estimation
KW  - SLAM (robots)
KW  - photometric loss
KW  - self-supervised ego-motion estimation
KW  - accurate relative pose
KW  - SLAM
KW  - self-supervised learning framework
KW  - image depth
KW  - photometric error
KW  - systematic error
KW  - realistic scenes
KW  - geometric loss
KW  - matching loss
KW  - self-supervised framework
KW  - unsupervised egomotion estimation methods
KW  - Simultaneous localization and mapping
KW  - Estimation
KW  - Geometry
KW  - Cameras
KW  - Motion estimation
KW  - Visualization
KW  - Visual odometry
DO  - 10.1109/ICRA.2019.8793479
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised egomotion estimation methods by a large margin. The code and data are available at https://github.com/hlzz/DeepMatchVO.
ER  - 

TY  - CONF
TI  - Efficient Integrity Monitoring for KF-based Localization
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6374
EP  - 6380
AU  - G. D. Arana
AU  - M. Joerger
AU  - M. Spenko
PY  - 2019
KW  - Kalman filters
KW  - location based services
KW  - mobile radio
KW  - mobile robots
KW  - KF-based localization
KW  - localization safety
KW  - mobile robots
KW  - aviation performance metric
KW  - aviation integrity monitoring solutions
KW  - robot navigation
KW  - integrity monitoring
KW  - global navigation satellite system
KW  - positioning sensors
KW  - Kalman filter-based localization
KW  - autonomous navigation
KW  - Sensors
KW  - Monitoring
KW  - Current measurement
KW  - Mathematical model
KW  - Fault detection
KW  - Safety
KW  - Time measurement
DO  - 10.1109/ICRA.2019.8794362
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper presents a new method to efficiently monitor localization safety in mobile robots. Localization safety is quantified by measuring the system's integrity risk, which is a well-known aviation performance metric. However, aviation integrity monitoring solutions almost exclusively rely on the Global Navigation Satellite System (GNSS) while robot navigation usually needs the additional information provided by a state evolution model and/or relative positioning sensors, which makes previously established approaches impractical. In response, this paper develops an efficient integrity monitoring methodology applicable to Kalman Filter-based localization. The work is intended for life-or mission-critical operations such as co-robot applications where ignoring the impact of faults can jeopardize human safety.
ER  - 

TY  - CONF
TI  - High-Precision Localization Using Ground Texture
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6381
EP  - 6387
AU  - L. Zhang
AU  - A. Finkelstein
AU  - S. Rusinkiewicz
PY  - 2019
KW  - cameras
KW  - Global Positioning System
KW  - image capture
KW  - image texture
KW  - location based services
KW  - location-aware applications
KW  - satellite-based localization
KW  - image-based global localization system
KW  - index distinctive local keypoints
KW  - fine texture
KW  - image processing pipeline
KW  - captured texture patch
KW  - test images
KW  - outdoor ground surfaces
KW  - indoor ground surfaces
KW  - ground textures
KW  - Cameras
KW  - Global Positioning System
KW  - Databases
KW  - Robots
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Asphalt
DO  - 10.1109/ICRA.2019.8794106
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Location-aware applications play an increasingly critical role in everyday life. However, satellite-based localization (e.g., GPS) has limited accuracy and can be unusable in dense urban areas and indoors. We introduce an image-based global localization system that is accurate to a few millimeters and performs reliable localization both indoors and outside. The key idea is to capture and index distinctive local keypoints in ground textures. This is based on the observation that ground textures including wood, carpet, tile, concrete, and asphalt may look random and homogeneous, but all contain cracks, scratches, or unique arrangements of fibers. These imperfections are persistent, and can serve as local features. Our system incorporates a downward-facing camera to capture the fine texture of the ground, together with an image processing pipeline that locates the captured texture patch in a compact database constructed offline. We demonstrate the capability of our system to robustly, accurately, and quickly locate test images on various types of outdoor and indoor ground surfaces. This paper contains a supplementary video. All datasets and code are available online at microgps.cs.princeton.edu.
ER  - 

TY  - CONF
TI  - IN2LAMA: INertial Lidar Localisation And MApping
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6388
EP  - 6394
AU  - C. L. Gentil
AU  - T. Vidal-Calleja
AU  - S. Huang
PY  - 2019
KW  - mobile robots
KW  - motion estimation
KW  - optical radar
KW  - optimisation
KW  - probability
KW  - robot vision
KW  - IN2LAMA
KW  - spinning mechanisms
KW  - resulting point clouds
KW  - lidar mapping literature
KW  - constant velocity motion model
KW  - upsampled inertial data
KW  - motion distortion
KW  - explicit motion-model
KW  - temporally precise upsampled preintegrated measurement
KW  - frame-to-frame planar
KW  - edge features association
KW  - probabilistic framework
KW  - inertial lidar localisation and mapping
KW  - batch on-manifold optimisation formulation
KW  - state change estimation
KW  - front-end interaction
KW  - back-end interaction
KW  - Laser radar
KW  - Distortion measurement
KW  - Three-dimensional displays
KW  - Distortion
KW  - Optimization
KW  - Trajectory
KW  - Gyroscopes
DO  - 10.1109/ICRA.2019.8794429
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper, we introduce a probabilistic framework for INertial Lidar Localisation And MApping (IN2LAMA). Most of today's lidars are based on spinning mechanisms that do not capture snapshots of the environment. As a result, movement of the sensor can occur while scanning. Without a good estimation of this motion, the resulting point clouds might be distorted. In the lidar mapping literature, a constant velocity motion model is commonly assumed. This is an approximation that does not necessarily always hold. The key idea of the proposed framework is to exploit preintegrated measurements over upsampled inertial data to handle motion distortion without the need for any explicit motion-model. It tightly integrates inertial and lidar data in a batch on-manifold optimisation formulation. Using temporally precise upsampled preintegrated measurement allows frame-to-frame planar and edge features association. Moreover, features are re-computed when the estimate of the state changes, consolidating front-end and back-end interaction. We validate the effectiveness of the approach through simulated and real data.
ER  - 

TY  - CONF
TI  - Speeding Up Iterative Closest Point Using Stochastic Gradient Descent
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6395
EP  - 6401
AU  - F. A. Maken
AU  - F. Ramos
AU  - L. Ott
PY  - 2019
KW  - gradient methods
KW  - image colour analysis
KW  - image registration
KW  - iterative methods
KW  - optimisation
KW  - pose estimation
KW  - SLAM (robots)
KW  - stochastic gradient descent
KW  - 3D laser scanners
KW  - RGB-D cameras
KW  - model registration
KW  - iterative closest point
KW  - SGD
KW  - convergence speed
KW  - 3D point clouds
KW  - pose estimation
KW  - SLAM
KW  - optimisation problem
KW  - Three-dimensional displays
KW  - Standards
KW  - Stochastic processes
KW  - Euclidean distance
KW  - Cost function
KW  - Sensors
DO  - 10.1109/ICRA.2019.8794011
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Sensors producing 3D point clouds such as 3D laser scanners and RGB-D cameras are widely used in robotics, be it for autonomous driving or manipulation. Aligning point clouds produced by these sensors is a vital component in such applications to perform tasks such as model registration, pose estimation, and SLAM. Iterative closest point (ICP) is the most widely used method for this task, due to its simplicity and efficiency. In this paper we propose a novel method which solves the optimisation problem posed by ICP using stochastic gradient descent (SGD). Using SGD allows us to improve the convergence speed of ICP without sacrificing solution quality. Experiments using Kinect as well as Velodyne data show that, our proposed method is faster than existing methods, while obtaining solutions comparable to standard ICP. An additional benefit is robustness to parameters when processing data from different sensors.
ER  - 

TY  - CONF
TI  - Energy Tank-Based Wrench/Impedance Control of a Fully-Actuated Hexarotor: A Geometric Port-Hamiltonian Approach
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6418
EP  - 6424
AU  - R. Rashad
AU  - J. B. C. Engelen
AU  - S. Stramigioli
PY  - 2019
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - controllability
KW  - end effectors
KW  - feedback
KW  - force control
KW  - helicopters
KW  - mobile robots
KW  - nonlinear control systems
KW  - observers
KW  - stability
KW  - trajectory control
KW  - fully-actuated hexarotor
KW  - geometrically consistent manner
KW  - wrench observer
KW  - geometric port-Hamiltonian approach
KW  - aerial robot
KW  - port-Hamiltonian framework
KW  - special Euclidean group
KW  - UAV nonlinear geometric structure
KW  - energy tanks concept
KW  - contact stability
KW  - Unmanned aerial vehicles
KW  - Impedance
KW  - Robots
KW  - Springs
KW  - Propellers
KW  - Mathematical model
KW  - Observers
DO  - 10.1109/ICRA.2019.8793939
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this work, we show how the interactive behavior of an aerial robot can be modeled and controlled effectively and elegantly in the port-Hamiltonian framework. We present an observer-based wrench/impedance controller for a fully-actuated hexarotor. The analysis and control are performed in a geometrically consistent manner on the configuration manifold of the special Euclidean group SE (3) such that the UAV's nonlinear geometric structure is exploited. The controller uses a wrench observer to estimate the interaction wrench without the use of a force/torque sensor. Moreover, the concept of energy tanks is used to guarantee the system's overall contact stability to arbitrary passive environments. The reliability and robustness of the proposed approach is validated through simulation and experiment.
ER  - 

TY  - CONF
TI  - Integral Backstepping Position Control for Quadrotors in Tunnel-Like Confined Environments
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6425
EP  - 6431
AU  - C. H. Vong
AU  - K. Ryan
AU  - H. Chung
PY  - 2019
KW  - aerodynamics
KW  - aerospace robotics
KW  - helicopters
KW  - Kalman filters
KW  - mechanical stability
KW  - mobile robots
KW  - pose estimation
KW  - position control
KW  - robot dynamics
KW  - robot vision
KW  - SLAM (robots)
KW  - tunnels
KW  - vision-based localisation
KW  - cross-sectional localisation system
KW  - integral backstepping controller
KW  - quadrotors
KW  - tunnel-like confined environments
KW  - integral backstepping position control
KW  - kinematic Kalman filter
KW  - semiautonomous system
KW  - flying robots
KW  - aerodynamic disturbances
KW  - Backstepping
KW  - Aerodynamics
KW  - Kinematics
KW  - Kalman filters
KW  - Navigation
KW  - Rail transportation
KW  - Sensors
DO  - 10.1109/ICRA.2019.8793893
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - There are many potential applications that require flying robots to navigate through tunnel-like environments, such as inspections of small railway culverts and mineral mappings of mining tunnels. Nevertheless, those environments present many challenges for quadrotors to navigate through. The aerodynamic disturbances created from the fluid interaction between the propellers' downwash and the surrounding surfaces of the environment, as well as longitudinal wind gusts, add hardship in stabilising the vehicle while the restricted narrow space increases the risk of collision. Furthermore, poor visibility and dust blown by the downwash make vision-based localisation extremely difficult. This paper presents a cross-sectional localisation system using Hough Scan Matching and a simple kinematic Kalman filter. Using the estimated state information, an integral backstepping controller is implemented which enables quadrotors to robustly fly in tunnel-like confined environments. A semi-autonomous system is proposed with self-stabilisation in the vertical and lateral axes while a pilot provides commands in the longitudinal direction. The results of a series of experiments in a simulated tunnel show that the proposed system successfully hovered itself and tracked various trajectories in a cross-sectional area without the aid of any external sensing or computing system.
ER  - 

TY  - CONF
TI  - Control and Configuration Planning of an Aerial Cable Towed System
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6440
EP  - 6446
AU  - J. Erskine
AU  - A. Chriette
AU  - S. Caro
PY  - 2019
KW  - autonomous aerial vehicles
KW  - cables (mechanical)
KW  - feedback
KW  - helicopters
KW  - linearisation techniques
KW  - manipulator dynamics
KW  - manipulator kinematics
KW  - manipulators
KW  - mobile robots
KW  - path planning
KW  - robust control
KW  - trajectory control
KW  - aerial cable towed system
KW  - robot configuration
KW  - kinematic models
KW  - centralized feedback linearization controller
KW  - optimal configurations
KW  - dynamic trajectories
KW  - ACTS
KW  - quadrotors manipulating
KW  - robustness
KW  - Payloads
KW  - Kinematics
KW  - Vehicle dynamics
KW  - Dynamics
KW  - Mathematical model
KW  - Trajectory
KW  - Propulsion
KW  - Aerial Systems
KW  - Multi-Robot Systems
KW  - Quadrotors
KW  - Control
KW  - Reconfiguration
KW  - Cable-Driven Parallel Robots
DO  - 10.1109/ICRA.2019.8794396
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper investigates the effect of the robot configuration on the performance of an aerial cable towed system (ACTS) composed of three quadrotors manipulating a point mass payload. The kinematic and dynamic models of the ACTS are derived in a minimal set of geometric coordinates, and a centralized feedback linearization controller is developed. Independent to the payload trajectory, the configuration of the ACTS is controlled and is evaluated using a robustness index named the capacity margin. Experiments are performed with optimal, suboptimal, and wrench infeasible configurations. It is shown that configurations near the point of zero capacity margin allow the ACTS to hover but not to follow dynamic trajectories, and that the ACTS cannot fly with a negative capacity margin. Dynamic tests of the ACTS show the effects of the configuration on the achievable accelerations.
ER  - 

TY  - CONF
TI  - Adaptive Control of Aerobatic Quadrotor Maneuvers in the Presence of Propeller-Aerodynamic-Coefficient and Torque-Latency Time-Variations
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6447
EP  - 6453
AU  - Y. Chen
AU  - N. O. Pérez-Arancibia
PY  - 2019
KW  - adaptive control
KW  - aerodynamics
KW  - aerospace propulsion
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control nonlinearities
KW  - control system synthesis
KW  - helicopters
KW  - least squares approximations
KW  - linear systems
KW  - momentum
KW  - time-varying systems
KW  - torque control
KW  - aerobatic flight
KW  - time-varying torque generation
KW  - propeller-aerodynamic-coefficient
KW  - torque-latency time-variations
KW  - momentum-theory-based analysis
KW  - dynamic linear time-varying description
KW  - flyer
KW  - backstepping controller
KW  - time-varying dynamics
KW  - aerobatic quadrotor
KW  - adaptive control
KW  - robot
KW  - LTV
KW  - recursive least-squares estimation
KW  - Pugachev's Cobras
KW  - triple flips
KW  - aerial vehicle
KW  - Rotors
KW  - Aerodynamics
KW  - Vehicle dynamics
KW  - Torque
KW  - Propellers
KW  - Analytical models
KW  - Robots
DO  - 10.1109/ICRA.2019.8793614
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We present a study of the dynamics and control of a 28-gram quadrotor during the execution of aerobatic maneuvers in the presence of propeller-aerodynamic-coefficient and torque-latency time-variations. First, through a momentum-theory-based analysis of the flow field surrounding the robot during aerobatic flight, we develop a dynamic linear time-varying (LTV) description of the torque acting on the flyer in which both considered effects explicitly appear as distinct mathematical terms. Then, an adaptive control scheme, composed of a backstepping controller and a modified recursive least-squares (RLS) estimator, is designed to counteract the negative effects produced by the time-varying dynamics of the torque that drives the flyer. The suitability and efficacy of the proposed methods are demonstrated through real-time flight experiments in which the quadrotor autonomously performs three different types of aerobatic maneuvers: triple flips, Pugachev's Cobras and mixed flips. Furthermore, analyses of the experimental data compellingly show that the proposed control scheme consistently improves the performance of the aerial vehicle during aerobatic flight, compared to those achieved by using a high-performance linear time-invariant (LTI) controller that does not account for time-varying torque generation.
ER  - 

TY  - CONF
TI  - Fast Terminal Sliding Mode Super Twisting Controller For Position And Altitude Tracking of the Quadrotor
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6468
EP  - 6474
AU  - V. K. Tripathi
AU  - A. K. Kamath
AU  - N. K. Verma
AU  - L. Behera
PY  - 2019
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - closed loop systems
KW  - control system synthesis
KW  - helicopters
KW  - Lyapunov methods
KW  - nonlinear control systems
KW  - position control
KW  - stability
KW  - variable structure systems
KW  - fast terminal sliding mode super twisting controller
KW  - altitude tracking
KW  - nonlinear fast terminal sliding manifold
KW  - super twisting reaching law
KW  - quadrotor position
KW  - FTSMSTC design
KW  - chattering phenomena
KW  - Lyapunov stability theory
KW  - MATLAB simulation
KW  - DJI Matrice M100
KW  - complete closed loop system stability
KW  - Convergence
KW  - Manifolds
KW  - Attitude control
KW  - Stability analysis
KW  - Sliding mode control
KW  - Trajectory
KW  - Backstepping
DO  - 10.1109/ICRA.2019.8794296
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper proposes a fast terminal sliding mode super twisting controller (FTSMSTC) design for quadrotor position and altitude tracking in the presence of bounded disturbances. A nonlinear fast terminal sliding manifold has been proposed for fast convergence of the tracking error to zero in finite time unlike the conventional sliding mode control (CSMC) that guarantee only asymptotic convergence of the tracking error. The super twisting reaching law has been proposed to deal with the chattering phenomena, which is inherent in the CSMC. The finite time stability of the complete closed loop system is investigated using Lyapunov stability theory and an analytical expression for the convergence time has also been derived. The effectiveness of the designed controller is checked against the CSMC using MATLAB simulation. The controller has been experimentally validated using the DJI Matrice M100 as a proof of utility in real time applications.
ER  - 

TY  - CONF
TI  - Multirotor dynamics based online scale estimation for monocular SLAM
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6475
EP  - 6481
AU  - M. Ludhiyani
AU  - V. Rustagi
AU  - R. Dasgupta
AU  - A. Sinha
PY  - 2019
KW  - autonomous aerial vehicles
KW  - cameras
KW  - helicopters
KW  - image sensors
KW  - Kalman filters
KW  - mobile robots
KW  - nonlinear filters
KW  - observability
KW  - robot vision
KW  - SLAM (robots)
KW  - observability
KW  - online scale estimation
KW  - extended Kalman filter framework
KW  - multirotor dynamics model
KW  - monocular camera
KW  - metric sensor
KW  - conventional scale estimation methods
KW  - monocular SLAM
KW  - monocular vision
KW  - Cameras
KW  - Observability
KW  - Drag
KW  - Estimation
KW  - Force
KW  - Mathematical model
KW  - Vehicle dynamics
DO  - 10.1109/ICRA.2019.8794372
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper proposes a novel method to estimate the scale online for monocular SLAM. Unlike conventional scale estimation methods that require a metric sensor such as an IMU and apriori knowledge of its biases, this approach estimates the scale online by solely using the monocular camera and multirotor dynamics model in an extended Kalman Filter framework. Further, we discuss the observability of scale and theoretically show that the scale becomes observable when multirotor dynamics model and monocular vision are used in conjunction. We validate our proposition with extensive experimentation on the local as well as on the standard datasets and compare the same with other state of the art methods.
ER  - 

TY  - CONF
TI  - Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth Measurements
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6482
EP  - 6488
AU  - S. S. Shivakumar
AU  - K. Mohta
AU  - B. Pfrommer
AU  - V. Kumar
AU  - C. J. Taylor
PY  - 2019
KW  - image fusion
KW  - image matching
KW  - stereo image processing
KW  - sparse depth measurements
KW  - stereo pair
KW  - sparse range measurements
KW  - LIDAR sensor
KW  - range camera
KW  - sensor modalities
KW  - randomly sampled ground truth range measurements
KW  - sparse depth input
KW  - PMDTec Monstar sensor
KW  - stereo information
KW  - stereo images
KW  - real time dense depth estimation
KW  - anisotropic diffusion
KW  - semi-global matching
KW  - frequency 5.0 Hz
KW  - Robot sensing systems
KW  - Estimation
KW  - Laser radar
KW  - Cameras
KW  - Pipelines
KW  - Three-dimensional displays
KW  - Gray-scale
DO  - 10.1109/ICRA.2019.8794023
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We present an approach to depth estimation that fuses information from a stereo pair with sparse range measurements derived from a LIDAR sensor or a range camera. The goal of this work is to exploit the complementary strengths of the two sensor modalities, the accurate but sparse range measurements and the ambiguous but dense stereo information. These two sources are effectively and efficiently fused by combining ideas from anisotropic diffusion and semi-global matching.We evaluate our approach on the KITTI 2015 and Middlebury 2014 datasets, using randomly sampled ground truth range measurements as our sparse depth input. We achieve significant performance improvements with a small fraction of range measurements on both datasets. We also provide qualitative results from our platform using the PMDTec Monstar sensor. Our entire pipeline runs on an NVIDIA TX-2 platform at 5Hz on 1280×1024 stereo images with 128 disparity levels.
ER  - 

TY  - CONF
TI  - Vision-based Control of a Quadrotor in User Proximity: Mediated vs End-to-End Learning Approaches
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6489
EP  - 6495
AU  - D. Mantegazza
AU  - J. Guzzi
AU  - L. M. Gambardella
AU  - A. Giusti
PY  - 2019
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - state estimation
KW  - vision-based control
KW  - quadrotor
KW  - onboard camera
KW  - control signals
KW  - high-level state estimation
KW  - learning approaches
KW  - Task analysis
KW  - Drones
KW  - Robot sensing systems
KW  - Training
KW  - Cameras
KW  - Computer architecture
DO  - 10.1109/ICRA.2019.8794377
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We consider the task of controlling a quadrotor to hover in front of a freely moving user, using input data from an onboard camera. On this specific task we compare two widespread learning paradigms: a mediated approach, which learns a high-level state from the input and then uses it for deriving control signals; and an end-to-end approach, which skips high-level state estimation altogether. We show that despite their fundamental difference, both approaches yield equivalent performance on this task. We finally qualitatively analyze the behavior of a quadrotor implementing such approaches.
ER  - 

TY  - CONF
TI  - Parity-Based Diagnosis in UAVs: Detectability and Robustness Analyses
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6496
EP  - 6502
AU  - G. Zogopoulos-Papaliakos
AU  - K. J. Kyriakopoulos
PY  - 2019
KW  - autonomous aerial vehicles
KW  - fault diagnosis
KW  - particle swarm optimisation
KW  - robust control
KW  - real flight data
KW  - parity-based methodologies
KW  - parity-based diagnosis
KW  - particle swarm optimization
KW  - static residuals
KW  - robustness metrics
KW  - robustness analyses
KW  - detectability
KW  - nonlinear residual generators
KW  - fault diagnosis
KW  - UAV model
KW  - fault detection system
KW  - dynamic residuals
KW  - Mathematical model
KW  - Robustness
KW  - Generators
KW  - Measurement
KW  - Numerical models
KW  - Atmospheric modeling
KW  - Unmanned aerial vehicles
DO  - 10.1109/ICRA.2019.8794125
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Parity-Based methodologies for fault diagnosis in UAVs often result in nonlinear residual generators. Still, a systematic framework to perform detectability and robustness analyses of residual generators does not exist. In this work, detectability and robustness metrics for static and dynamic residuals are presented, while numerical methods, specifically Particle Swarm Optimization, are employed to calculate them. The results are used to characterize the performance of a fault detection system. An application on a UAV model is shown, based on real flight data.
ER  - 

TY  - CONF
TI  - Exploiting a Human-Aware World Model for Dynamic Task Allocation in Flexible Human-Robot Teams
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6511
EP  - 6517
AU  - D. Riedelbauch
AU  - D. Henrich
PY  - 2019
KW  - cameras
KW  - human-robot interaction
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - robot vision
KW  - human participation
KW  - human models
KW  - human-robot teaming framework
KW  - human-aware world model
KW  - dynamic task allocation
KW  - human-robot teams
KW  - human-robot cooperation
KW  - eye-in-hand camera images
KW  - task operations
KW  - trust measure
KW  - action selection algorithm
KW  - Task analysis
KW  - Resource management
KW  - Cameras
KW  - Robot kinematics
KW  - Robot sensing systems
KW  - Dynamic scheduling
DO  - 10.1109/ICRA.2019.8794288
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We propose a highly flexible approach to human-robot cooperation, where a robot dynamically selects operations contributing to a shared goal from a given task model. Therefore, knowledge on the task progress is extracted from a world model constructed from eye-in-hand camera images. Data generated from such partial workspace observations is not reliable over time, as humans may interact with resources. We therefore use a human-aware world model maintaining a measure for trust in stored objects regarding recent human presence and previous task progress. Our contribution is an action selection algorithm that uses this trust measure to interleave task operations with active vision to refresh the world model. Large-scale experiments cover various sorts of human participation in different benchmark tasks through simulation of simplified, partially randomized human models. Results illuminate system behaviour and performance for different parametrizations of our human-robot teaming framework.
ER  - 

TY  - CONF
TI  - Group Surfing: A Pedestrian-Based Approach to Sidewalk Robot Navigation
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6518
EP  - 6524
AU  - Y. Du
AU  - N. J. Hetherington
AU  - C. L. Oon
AU  - W. P. Chan
AU  - C. P. Quintero
AU  - E. Croft
AU  - H. F. Machiel Van der Loos
PY  - 2019
KW  - collision avoidance
KW  - edge detection
KW  - human-robot interaction
KW  - mobile robots
KW  - navigation
KW  - pedestrians
KW  - traffic engineering computing
KW  - pedestrian-based approach
KW  - sidewalk robot navigation
KW  - mobile robots
KW  - pedestrian-rich sidewalk environments
KW  - pedestrian-shared space
KW  - indoor spaces
KW  - pedestrian movement
KW  - linear flows
KW  - opposing directions
KW  - pedestrians
KW  - random movements
KW  - safe navigation
KW  - sidewalk space
KW  - natural human motion
KW  - socially-compliant manner
KW  - group surfing method
KW  - optimal pedestrian group
KW  - pedestrian-sparse environments
KW  - sidewalk edge detection
KW  - following method
KW  - integrated navigation stack
KW  - Navigation
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Roads
KW  - Legged locomotion
DO  - 10.1109/ICRA.2019.8793608
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper, we propose a novel navigation system for mobile robots in pedestrian-rich sidewalk environments. Sidewalks are unique in that the pedestrian-shared space has characteristics of both roads and indoor spaces. Like vehicles on roads, pedestrian movement often manifests as linear flows in opposing directions. On the other hand, pedestrians also form crowds and can exhibit much more random movements than vehicles. Classical algorithms are insufficient for safe navigation around pedestrians and remaining on the sidewalk space. Thus, our approach takes advantage of natural human motion to allow a robot to adapt to sidewalk navigation in a safe and socially-compliant manner. We developed a group surfing method which aims to imitate the optimal pedestrian group for bringing the robot closer to its goal. For pedestrian-sparse environments, we propose a sidewalk edge detection and following method. Underlying these two navigation methods, the collision avoidance scheme is human-aware. The integrated navigation stack is evaluated and demonstrated in simulation. A hardware demonstration is also presented.
ER  - 

TY  - CONF
TI  - Dentronics: Review, First Concepts and Pilot Study of a New Application Domain for Collaborative Robots in Dental Assistance
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6525
EP  - 6532
AU  - J. Grischke
AU  - L. Johannsmeier
AU  - L. Eich
AU  - S. Haddadin
PY  - 2019
KW  - dentistry
KW  - human-robot interaction
KW  - medical robotics
KW  - dentronics
KW  - new application domain
KW  - collaborative robots
KW  - dental assistance
KW  - multimodal interaction framework
KW  - Dentistry
KW  - Robots
KW  - Task analysis
KW  - Reliability
KW  - Prototypes
KW  - Standards
KW  - Collision avoidance
DO  - 10.1109/ICRA.2019.8794139
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper we introduce dentronics as a new emerging application domain for collaborative lightweight robots in the dental context backed up by a user survey supporting the clear need. Specifically, we developed a multi-modal interaction framework, applied this framework to a specific dental use-case, and conducted a preliminary user-study for evaluation. Our results demonstrate usability and feasibility beyond a controlled experimental setup. We conclude that dentronics is indeed within reach given today's technology and deserves further investigation towards clinical use.
ER  - 

TY  - CONF
TI  - Activity recognition in manufacturing: The roles of motion capture and sEMG+inertial wearables in detecting fine vs. gross motion
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6533
EP  - 6539
AU  - A. Kubota
AU  - T. Iqbal
AU  - J. A. Shah
AU  - L. D. Riek
PY  - 2019
KW  - assembling
KW  - feature extraction
KW  - human-robot interaction
KW  - image classification
KW  - image motion analysis
KW  - industrial robots
KW  - manufacturing systems
KW  - production engineering computing
KW  - sensor fusion
KW  - wearable computers
KW  - unimodal sensor data
KW  - UCSD-MIT Human Motion dataset
KW  - Vicon motion capture system
KW  - gross motion recognition
KW  - sensor modalities
KW  - sEMG
KW  - human activity recognition
KW  - inertial wearables
KW  - fine motion detection
KW  - manufacturing
KW  - safety-critical environments
KW  - assembly tasks
KW  - HAR algorithms
KW  - Robot sensing systems
KW  - Task analysis
KW  - Wearable sensors
KW  - Grasping
KW  - Automotive engineering
KW  - Tracking
DO  - 10.1109/ICRA.2019.8793954
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In safety-critical environments, robots need to reliably recognize human activity to be effective and trust-worthy partners. Since most human activity recognition (HAR) approaches rely on unimodal sensor data (e.g. motion capture or wearable sensors), it is unclear how the relationship between the sensor modality and motion granularity (e.g. gross or fine) of the activities impacts classification accuracy. To our knowledge, we are the first to investigate the efficacy of using motion capture as compared to wearable sensor data for recognizing human motion in manufacturing settings. We introduce the UCSD-MIT Human Motion dataset, composed of two assembly tasks that entail either gross or fine-grained motion. For both tasks, we compared the accuracy of a Vicon motion capture system to a Myo armband using three widely used HAR algorithms. We found that motion capture yielded higher accuracy than the wearable sensor for gross motion recognition (up to 36.95%), while the wearable sensor yielded higher accuracy for fine-grained motion (up to 28.06%). These results suggest that these sensor modalities are complementary, and that robots may benefit from systems that utilize multiple modalities to simultaneously, but independently, detect gross and fine-grained motion. Our findings will help guide researchers in numerous fields of robotics including learning from demonstration and grasping to effectively choose sensor modalities that are most suitable for their applications.
ER  - 

TY  - CONF
TI  - Optimal Proactive Path Planning for Collaborative Robots in Industrial Contexts
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6540
EP  - 6546
AU  - A. Casalino
AU  - D. Bazzi
AU  - A. M. Zanchettin
AU  - P. Rocco
PY  - 2019
KW  - collision avoidance
KW  - industrial manipulators
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - motion controllers
KW  - production plants
KW  - optimal proactive path planning
KW  - Industry 4.0
KW  - ABB YuMi
KW  - industrial contexts
KW  - collaborative robots
KW  - 7 degrees robotic arm
KW  - human operator
KW  - robotic paths
KW  - proactive approach
KW  - local corrective actions
KW  - safe motion planning strategies
KW  - Trajectory
KW  - Service robots
KW  - Probabilistic logic
KW  - Computational modeling
KW  - Collaboration
KW  - Task analysis
DO  - 10.1109/ICRA.2019.8793847
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - The coexistence of humans and robots in the future production plants is one of the pillars of Industry 4.0. Humans and robots will collaborate to accomplish common tasks in order to mutually compensate their deficiencies. In recent years, many efforts have been spent to develop safe motion planning strategies, designed to prevent robots from injuring humans. Most of the previous techniques are classifiable as reactive, since the considered motion controllers impose some local corrective actions in order to dodge the space occupied by the human. In this paper, a proactive approach is adopted, optimizing robotic paths according to a prediction of the volume occupied by the human when collaborating with the robot. The validity of the approach is shown in a realistic use-case involving the collaboration of a human operator with a 7 degrees robotic arm, the ABB YuMi.
ER  - 

TY  - CONF
TI  - Build your own hybrid thermal/EO camera for autonomous vehicle
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6555
EP  - 6560
AU  - Y. Zhang
AU  - Y. Gao
AU  - S. Gu
AU  - Y. Guo
AU  - M. Liu
AU  - Z. Sun
AU  - Z. Hou
AU  - H. Yang
AU  - Y. Wang
AU  - J. Yang
AU  - J. Ponce
AU  - H. Kong
PY  - 2019
KW  - cameras
KW  - image colour analysis
KW  - image motion analysis
KW  - image registration
KW  - image resolution
KW  - image sensors
KW  - image sequences
KW  - object detection
KW  - remotely operated vehicles
KW  - single hybrid camera
KW  - hybrid camera array
KW  - disparity images
KW  - spatial-alignment
KW  - autonomous vehicle
KW  - thermal RGB frames
KW  - thermal EO cameras
KW  - pixel-wise spatial registration
KW  - visible-light camera
KW  - hybrid thermal/EO camera
KW  - RGB frames
KW  - constant homography warping
KW  - image modalities
KW  - Cameras
KW  - Layout
KW  - Technological innovation
KW  - Tuners
KW  - Optical sensors
KW  - Autonomous vehicles
DO  - 10.1109/ICRA.2019.8794320
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this work, we propose a novel paradigm to design a hybrid thermal/EO (Electro-Optical or visible-light) camera, whose thermal and RGB frames are pixel-wisely aligned and temporally synchronized. Compared with the existing schemes, we innovate in three ways in order to make it more compact in dimension, and thus more practical and extendable for real-world applications. The first is a redesign of the structure layout of the thermal and EO cameras. The second is on obtaining a pixel-wise spatial registration of the thermal and RGB frames by a coarse mechanical adjustment and a fine alignment through a constant homography warping. The third innovation is on extending one single hybrid camera to a hybrid camera array, through which we can obtain wide-view spatially aligned thermal, RGB and disparity images simultaneously. The experimental results show that the average error of spatial-alignment of two image modalities can be less than one pixel.
ER  - 

TY  - CONF
TI  - Redundant Perception and State Estimation for Reliable Autonomous Racing
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6561
EP  - 6567
AU  - N. Gosala
AU  - A. Bühler
AU  - M. Prajapat
AU  - C. Ehmke
AU  - M. Gupta
AU  - R. Sivanesan
AU  - A. Gawel
AU  - M. Pfeiffer
AU  - M. Bürki
AU  - I. Sa
AU  - R. Dubé
AU  - R. Siegwart
PY  - 2019
KW  - automobiles
KW  - driver information systems
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - SLAM (robots)
KW  - state estimation
KW  - vehicle dynamics
KW  - reliable autonomous racing
KW  - sensor failure
KW  - critical consequences
KW  - state estimation approaches
KW  - autonomous race car
KW  - track delimiting objects
KW  - sensor modalities
KW  - learning-based approaches
KW  - camera data
KW  - redundant perception inputs
KW  - probabilistic failure detection algorithm
KW  - real-world racing conditions
KW  - slip dynamics
KW  - particle filter based SLAM algorithm
KW  - pose estimates
KW  - velocity 90.0 km/h
KW  - Robot sensing systems
KW  - Cameras
KW  - Three-dimensional displays
KW  - Automobiles
KW  - Image color analysis
KW  - Laser radar
KW  - Reliability
DO  - 10.1109/ICRA.2019.8794155
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In autonomous racing, vehicles operate close to the limits of handling and a sensor failure can have critical consequences. To limit the impact of such failures, this paper presents the redundant perception and state estimation approaches developed for an autonomous race car. Redundancy in perception is achieved by estimating the color and position of the track delimiting objects using two sensor modalities independently. Specifically, learning-based approaches are used to generate color and pose estimates, from LiDAR and camera data respectively. The redundant perception inputs are fused by a particle filter based SLAM algorithm that operates in real-time. Velocity is estimated using slip dynamics, with reliability being ensured through a probabilistic failure detection algorithm. The sub-modules are extensively evaluated in real-world racing conditions using the autonomous race car gotthard driverless, achieving lateral accelerations up to 1. 7G and a top speed of 90km/h.
ER  - 

TY  - CONF
TI  - UWB/LiDAR Fusion For Cooperative Range-Only SLAM
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6568
EP  - 6574
AU  - Y. Song
AU  - M. Guan
AU  - W. P. Tay
AU  - C. L. Law
AU  - C. Wen
PY  - 2019
KW  - distance measurement
KW  - laser ranging
KW  - mobile robots
KW  - optical radar
KW  - SLAM (robots)
KW  - ultra wideband radar
KW  - wireless sensor networks
KW  - cooperative sensor network
KW  - 2D LiDAR sensor
KW  - UWB-LiDAR fusion
KW  - UWB beacon nodes
KW  - peer-to-peer ranges
KW  - nearby objects-obstacles
KW  - surrounding environment
KW  - drift-free SLAM
KW  - mobile robot
KW  - 2D laser rangefinder
KW  - ultra-wideband node
KW  - cooperative range-only SLAM
KW  - LiDAR-based SLAM algorithm
KW  - UWB ranging measurements
KW  - UWB-only localization accuracy
KW  - LiDAR mapping
KW  - laser scanning information
KW  - Laser radar
KW  - Peer-to-peer computing
KW  - Distance measurement
KW  - Simultaneous localization and mapping
KW  - Two dimensional displays
DO  - 10.1109/ICRA.2019.8794222
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We equip an ultra-wideband (UWB) node and a 2D LiDAR sensor a.k.a. 2D laser rangefinder on a mobile robot, and place UWB beacon nodes at unknown locations in an unknown environment. All UWB nodes can do ranging with each other thus forming a cooperative sensor network. We propose to fuse the peer-to-peer ranges measured between UWB nodes and laser scanning information, i.e., range measured between robot and nearby objects/obstacles, for simultaneous localization of the robot, all UWB beacons and LiDAR mapping. The fusion is inspired by two facts: 1) LiDAR may improve UWB-only localization accuracy as it gives a more precise and comprehensive picture of the surrounding environment; 2) on the other hand, UWB ranging measurements may remove the error accumulated in the LiDAR-based SLAM algorithm. Our experiments demonstrate that UWB/LiDAR fusion enables drift-free SLAM in real-time based on ranging measurements only.
ER  - 

TY  - CONF
TI  - Localization and Tracking of Uncontrollable Underwater Agents: Particle Filter Based Fusion of On-Body IMUs and Stationary Cameras
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6575
EP  - 6581
AU  - D. Zhang
AU  - J. Gabaldon
AU  - L. Lauderdale
AU  - M. Johnson-Roberson
AU  - L. J. Miller
AU  - K. Barton
AU  - K. A. Shorter
PY  - 2019
KW  - cameras
KW  - fuzzy set theory
KW  - marine control
KW  - multi-agent systems
KW  - object tracking
KW  - particle filtering (numerical methods)
KW  - nonlinear agent dynamics
KW  - sparse camera observations
KW  - robust agent localization
KW  - uncontrollable underwater agents
KW  - stationary cameras
KW  - multiagent control
KW  - multiagent tracking problem
KW  - wearable sensors
KW  - uncontrollable biological agents
KW  - camera detections
KW  - on-body IMUs
KW  - fuzzy observation concept
KW  - nonGaussian noise
KW  - particle filter based fusion
KW  - Cameras
KW  - Two dimensional displays
KW  - Tracking
KW  - Wearable sensors
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Particle filters
DO  - 10.1109/ICRA.2019.8794141
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Tracking of uncontrollable agents in a controlled environment is an important research question for the coordination of controllable and uncontrollable agents and bio-inspired multi-agent control. This paper presents a framework that approaches the multiagent tracking problem from a localization perspective, utilizing a combination of wearable sensors and stationary cameras. Specifically, this framework was applied to localize uncontrollable biological agents (dolphins) in a well defined environment. The biological agents were outfitted with wearable sensors (IMU, speed, depth) and were free to move in their three dimensional habitat. The dynamic data collected by the wearable sensors was supplemented with image data collected using a pair of cameras mounted above the habitat. The framework presented in this paper combines data from these sensor streams to calculate an accurate estimate of the animal's location during extended periods of free movement. The associations between camera detections and tagged agents are handled using a particle filter embedded with a fuzzy observation concept. The platform is readily implementable in similar water / land environments, and is able to handle nonlinear agent dynamics, non-Gaussian noise, and sparse camera observations while maintaining robust agent localization and tracking.
ER  - 

TY  - CONF
TI  - Steering Co-centered and Co-directional Optical and Acoustic Beams with a Water-immersible MEMS Scanning Mirror for Underwater Ranging and Communication*
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6582
EP  - 6587
AU  - X. Duan
AU  - D. Song
AU  - J. Zou
PY  - 2019
KW  - beam steering
KW  - integrated optics
KW  - laser beams
KW  - laser ranging
KW  - micromechanical devices
KW  - micromirrors
KW  - microsensors
KW  - optical communication equipment
KW  - photodetectors
KW  - remotely operated vehicles
KW  - underwater optics
KW  - underwater vehicles
KW  - Hall effect
KW  - reception modes
KW  - transmission modes
KW  - underwater ranging and communication
KW  - steering co-directional optical beams
KW  - steering co-directional acoustic beams
KW  - steering co-centered laser beams
KW  - steering co-centered ultrasonic beams
KW  - steering co-directional ultrasonic beams
KW  - steering co-directional laser beams
KW  - steering co-centered acoustic beams
KW  - steering co-centered optical beams
KW  - bi-modal communication
KW  - scan position sensors
KW  - ultrasound beams
KW  - water-immersible MEMS scanning mirror
KW  - Laser beams
KW  - Ultrasonic imaging
KW  - Mirrors
KW  - Acoustic beams
KW  - Optical beams
KW  - Optical sensors
DO  - 10.1109/ICRA.2019.8793747
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper reports the development of a compact optical-acoustic frontend module for underwater communication and ranging. The module is enabled by a new water-immersible MEMS scanning mirror (WIMSM). It is capable of transmitting, receiving and steering co-centered and co-directional laser and ultrasound beams under water. To monitor its rotating angle in real time, scan position sensors based on Hall effect have been integrated into the WIMSM. The angular alignment of the laser and ultrasound beams in both transmission and reception modes has been examined. The experimental results show that the laser and ultrasound beams can remain aligned with less than 2.1 degrees under envelope of pan and tilt rotations. This capability is critical for the continuing development of the new bi-modal communication and ranging underwater Vehicles (AUVs).
ER  - 

TY  - CONF
TI  - A Simple Adaptive Tracker with Reminiscences
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6596
EP  - 6603
AU  - C. Xie
AU  - E. Fox
AU  - Z. Harchaoui
PY  - 2019
KW  - convex programming
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - VOT benchmark dataset
KW  - OTB benchmark dataset
KW  - gradient-based convex optimization
KW  - MTCF
KW  - appearance changes
KW  - adaptive tracker
KW  - temporal windows
KW  - base trackers
KW  - ensemble method
KW  - visual object tracking
KW  - correlation filters
KW  - reminiscences
KW  - visual appearance
KW  - video history
KW  - Correlation
KW  - Visualization
KW  - Object tracking
KW  - History
KW  - Standards
KW  - Convolution
KW  - Training
DO  - 10.1109/ICRA.2019.8794234
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Correlation filters have provided exceptional results in the field of visual object tracking in the past few years. However, these methods typically learn a single filter to be robust to many different appearance changes, which can be challenging. We propose a simple solution to this problem by utilizing an ensemble method of base trackers trained on different temporal windows of the video history. The proposed tracker, called MTCF, exhibits the following features: i) it can be trained using gradient-based convex optimization; ii) it is robust to short-term and long-term changes in visual appearance. MTCF performs on par with or outperforms state-of-the-art trackers on the OTB and the VOT benchmark datasets. We present an extensive analysis of the performance of MTCF on these benchmark datasets.
ER  - 

TY  - CONF
TI  - Learning-driven Coarse-to-Fine Articulated Robot Tracking
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6604
EP  - 6610
AU  - C. Rauch
AU  - V. Ivan
AU  - T. Hospedales
AU  - J. Shotton
AU  - M. Fallon
PY  - 2019
KW  - computer vision
KW  - edge detection
KW  - image colour analysis
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object tracking
KW  - coarse-to-fine articulated robot tracking
KW  - articulated tracking approach
KW  - robotic manipulators
KW  - visual cues
KW  - subpixel-level accurate correspondences
KW  - discriminative depth information
KW  - coarse-to-fine articulated state estimator
KW  - robot state distribution
KW  - depth image
KW  - Schunk SDH2 hand interacting
KW  - edge tracking
KW  - depth keypoints
KW  - articulated model fitting
KW  - colour edge correspondences
KW  - RGB-D sequences
KW  - KUICA LWR arm
KW  - palm position estimation
KW  - Visualization
KW  - Optimization
KW  - Manipulators
KW  - Image edge detection
KW  - Two dimensional displays
KW  - Robot sensing systems
DO  - 10.1109/ICRA.2019.8794359
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this work we present an articulated tracking approach for robotic manipulators, which relies only on visual cues from colour and depth images to estimate the robot's state when interacting with or being occluded by its environment. We hypothesise that articulated model fitting approaches can only achieve accurate tracking if subpixel-level accurate correspondences between observed and estimated state can be established. Previous work in this area has exclusively relied on either discriminative depth information or colour edge correspondences as tracking objective and required initialisation from joint encoders. In this paper we propose a coarse-to-fine articulated state estimator, which relies only on visual cues from colour edges and learned depth keypoints, and which is initialised from a robot state distribution predicted from a depth image. We evaluate our approach on four RGB-D sequences showing a KUICA LWR arm with a Schunk SDH2 hand interacting with its environment and demonstrate that this combined keypoint and edge tracking objective can estimate the palm position with an average error of 2. 5cm without using any joint encoder sensing.
ER  - 

TY  - CONF
TI  - Diagonally-Decoupled Direct Visual Servoing
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6611
EP  - 6616
AU  - G. Silveira
AU  - L. Mirisola
PY  - 2019
KW  - observers
KW  - robot vision
KW  - visual servoing
KW  - diagonally-decoupled direct visual servoing
KW  - vision-based robot control
KW  - reference image
KW  - intensity-based nonmetric solutions
KW  - fully coupled control error dynamics
KW  - translational part
KW  - lower triangular system
KW  - system dynamics
KW  - analysis complexity
KW  - system performance
KW  - nonlinear observer
KW  - rotational part
KW  - decoupling properties
KW  - robotic arm
KW  - control error dynamics
KW  - Visual servoing
KW  - Cameras
KW  - Convergence
KW  - Observers
KW  - Transmission line matrix methods
KW  - Voltage control
DO  - 10.1109/ICRA.2019.8793717
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper addresses the problem of vision-based robot control where a reference image defines the equilibrium. Specifically, we consider the class of intensity-based nonmetric solutions, which provide for high accuracy, versatility, and robustness. Existing techniques within that class present either a fully coupled control error dynamics or at best only achieve decoupling of the translational part, i.e., they can only obtain a lower triangular system. These couplings in the system dynamics increase analysis complexity and may degrade system performance. This work proposes a new nonlinear observer for also decoupling the rotational part, i.e., for diagonally decoupling the entire control error dynamics. Theoretical proofs of stability and of those decoupling properties are provided. Improved performances are also experimentally confirmed using synthetic and real data, planar and nonplanar objects, simulating and applying a camera-mounted 6-DoF robotic arm.
ER  - 

TY  - CONF
TI  - 2D LiDAR Map Prediction via Estimating Motion Flow with GRU
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6617
EP  - 6623
AU  - Y. Song
AU  - Y. Tian
AU  - G. Wang
AU  - M. Li
PY  - 2019
KW  - feature extraction
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - motion estimation
KW  - optical radar
KW  - radar imaging
KW  - recurrent neural nets
KW  - LiDAR-FlowNet model
KW  - motion flow based method
KW  - 2D LiDAR map prediction
KW  - optical flow
KW  - recurrent neural network
KW  - motion flow estimation
KW  - gated recurrent unit
KW  - robotics navigation
KW  - path planning
KW  - Laser radar
KW  - Two dimensional displays
KW  - Dynamics
KW  - Logic gates
KW  - Recurrent neural networks
KW  - Training
KW  - Robot sensing systems
DO  - 10.1109/ICRA.2019.8793490
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - It is a significant problem to predict the 2D LiDAR map at next moment for robotics navigation and path-planning. To tackle this problem, we resort to the motion flow between adjacent maps, as motion flow is a powerful tool to process and analyze the dynamic data, which is named optical flow in video processing. However, unlike video, which contains abundant visual features in each frame, a 2D LiDAR map lacks distinctive local features. To alleviate this challenge, we propose to estimate the motion flow based on deep neural networks inspired by its powerful representation learning ability in estimating the optical flow of the video. To this end, we design a recurrent neural network based on gated recurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can encode the temporal dynamic information, our LiDAR-FlowNet can estimate motion flow between the current map and the unknown next map only from the current frame and previous frames. A self-supervised strategy is further designed to train the LiDAR-FlowNet model effectively, while no training data need to be manually annotated. With the estimated motion flow, it is straightforward to predict the 2D LiDAR map at the next moment. Experimental results verify the effectiveness of our LiDAR-FlowNet as well as the proposed training strategy. The results of the predicted LiDAR map also show the advantages of our motion flow based method.
ER  - 

TY  - CONF
TI  - Robot eye-hand coordination learning by watching human demonstrations: a task function approximation approach
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6624
EP  - 6630
AU  - J. Jin
AU  - L. Petrich
AU  - M. Dehghan
AU  - Z. Zhang
AU  - M. Jagersand
PY  - 2019
KW  - feedback
KW  - function approximation
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - robot vision
KW  - visual servoing
KW  - human demonstrations
KW  - task function approximation approach
KW  - robot eye-hand coordination learning method
KW  - visual task specification
KW  - inverse reinforcement learning
KW  - learned reward model
KW  - uncalibrated visual servoing
KW  - hand-engineered task specification
KW  - traditional UVS controller
KW  - learned policy
KW  - robot platforms
KW  - Task analysis
KW  - Robot kinematics
KW  - Videos
KW  - Training
KW  - Visualization
KW  - Visual servoing
DO  - 10.1109/ICRA.2019.8793649
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We present a robot eye-hand coordination learning method that can directly learn visual task specification by watching human demonstrations. Task specification is represented as a task function, which is learned using inverse reinforcement learning(IRL [1]) by inferring a reward model from state transitions. The learned reward model is then used as continuous feedbacks in an uncalibrated visual servoing(UVS [2]) controller designed for the execution phase. Our proposed method can directly learn from raw videos, which removes the need for hand-engineered task specification. Benefiting from the use of a traditional UVS controller, the training on real robot only happens at initial Jacobian estimation which takes an average of 4-7 seconds for a new task. Besides, the learned policy is independent from a particular robot, thus has the potential of fast adapting to other robot platforms. Various experiments were designed to show that, for a task with certain DOFs, our method can adapt to task/environment changes in target positions, backgrounds, illuminations, and occlusions.
ER  - 

TY  - CONF
TI  - Vision-Based Dynamic Control of Car-Like Mobile Robots
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6631
EP  - 6636
AU  - S. Zhou
AU  - Z. Liu
AU  - C. Suo
AU  - H. Wang
AU  - H. Zhao
AU  - Y. Liu
PY  - 2019
KW  - automobiles
KW  - Lyapunov methods
KW  - mobile robots
KW  - robot vision
KW  - stability
KW  - steering systems
KW  - velocity control
KW  - car-like mobile robots
KW  - CLMR
KW  - steering control system
KW  - slipping effects
KW  - velocity estimation error
KW  - speed tracking error
KW  - vision-based dynamic control
KW  - visual algorithm
KW  - skidding effects
KW  - speed control system
KW  - Lyapunov method
KW  - stability
KW  - electric autonomous tractor
KW  - Aerodynamics
KW  - Mobile robots
KW  - Velocity measurement
KW  - Estimation
KW  - Heuristic algorithms
DO  - 10.1109/ICRA.2019.8793980
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Most existing controllers for Car-Like Mobile Robots (CLMR) are designed to handle dynamic effects by decoupling speed and steering controls, also assume that full states are accessible, which are unrealistic for real-world applications. This paper presents a combined speed and steering control system for CLMR. To provide the essential state for the controller, a newly developed visual algorithm is adopted for estimating the high-update rate longitudinal and lateral velocities of the robot which cannot be accurately measured by wheel encoders due to the skidding and slipping effects. The stability of the proposed system can be guaranteed by Lyapunov method since the velocity estimation error, the speed tracking error and the lateral deviation converging to zero simultaneously. Real-world experiments are conducted on an electric autonomous tractor with online estimation to demonstrate the feasibility of the approach.
ER  - 

TY  - CONF
TI  - Uncertainty Estimation for Projecting Lidar Points onto Camera Images for Moving Platforms
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6637
EP  - 6643
AU  - C. De Alvis
AU  - M. Shan
AU  - S. Worrall
AU  - E. Nebot
PY  - 2019
KW  - calibration
KW  - cameras
KW  - distance measurement
KW  - optical radar
KW  - radar imaging
KW  - heterogeneous sensors
KW  - lidar sensors
KW  - precise range information
KW  - visual image data
KW  - context based algorithms
KW  - intrinsic calibration
KW  - extrinsic calibration
KW  - lidar measurements
KW  - consistent odometry frame
KW  - image frame
KW  - moving platforms
KW  - projection error
KW  - motion correction algorithm
KW  - extended uncertainty model
KW  - real-world data
KW  - wide-angle cameras
KW  - 16-beam scanning lidar
KW  - uncertainty estimation
KW  - lidar points
KW  - camera images
KW  - advanced perception
KW  - crucial requirement
KW  - autonomous vehicle navigation
KW  - sensor frames
KW  - Laser radar
KW  - Cameras
KW  - Calibration
KW  - Uncertainty
KW  - Sensors
KW  - Distortion
KW  - Three-dimensional displays
DO  - 10.1109/ICRA.2019.8794424
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Combining multiple sensors for advanced perception is a crucial requirement for autonomous vehicle navigation. Heterogeneous sensors are used to obtain rich information about the surrounding environment. The combination of the camera and lidar sensors enables precise range information that can be projected onto the visual image data. This gives a high level understanding of the scene which can be used to enable context based algorithms such as collision avoidance and navigation. The main challenge when combining these sensors is aligning the data into a common domain. This can be difficult due to the errors in the intrinsic calibration of the camera, extrinsic calibration between the camera and the lidar and errors resulting from the motion of the platform. In this paper, we examine the algorithms required to provide motion correction for scanning lidar sensors. The error resulting from the projection of the lidar measurements into a consistent odometry frame is not possible to remove entirely, and as such it is essential to incorporate the uncertainty of this projection when combining the two different sensor frames. This work proposes a novel framework for the prediction of the uncertainty of lidar measurements (in 3D) projected in to the image frame (in 2D) for moving platforms. The proposed approach fuses the uncertainty of the motion correction with uncertainty resulting from errors in the extrinsic and intrinsic calibration. By incorporating the main components of the projection error, the uncertainty of the estimation process is better represented. Experimental results for our motion correction algorithm and the proposed extended uncertainty model are demonstrated using real-world data collected on an electric vehicle equipped with wide-angle cameras covering a 180-degree field of view and a 16-beam scanning lidar.
ER  - 

TY  - CONF
TI  - Modeling and Analysis of Motion Data from Dynamically Positioned Vessels for Sea State Estimation
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6644
EP  - 6650
AU  - X. Cheng
AU  - G. Li
AU  - R. Skulstad
AU  - S. Chen
AU  - H. P. Hildre
AU  - H. Zhang
PY  - 2019
KW  - convolutional neural nets
KW  - data analysis
KW  - fast Fourier transforms
KW  - feature extraction
KW  - learning (artificial intelligence)
KW  - marine engineering
KW  - position control
KW  - recurrent neural nets
KW  - sensitivity analysis
KW  - sensor fusion
KW  - ships
KW  - state estimation
KW  - time series
KW  - long dependency
KW  - ship motion data
KW  - convolutional neural network
KW  - frequency features
KW  - feature fusion layer
KW  - raw time series data
KW  - hand-engineered features
KW  - sensitivity analysis method
KW  - data preprocessing
KW  - ship motion dataset
KW  - SeaStateNet
KW  - sea state estimation
KW  - dynamically positioned vessels
KW  - autonomous ship
KW  - deep neural network model
KW  - time-invariant feature extraction
KW  - long-short-term memory recurrent neural network
KW  - fast Fourier transform block
KW  - Sea state
KW  - Marine vehicles
KW  - Time series analysis
KW  - Estimation
KW  - Sensors
KW  - Feature extraction
KW  - Data models
DO  - 10.1109/ICRA.2019.8794069
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Developing a reliable model to identify the sea state is significant for the autonomous ship. This paper introduces a novel deep neural network model (SeaStateNet) to estimate the sea state based on the ship motion data from dynamically positioned vessels. The SeaStateNet mainly consists of three components: an Long-Short-Term Memory (LSTM) recurrent neural network to capture the long dependency in the ship motion data; a convolutional neural network (CNN) to extract time-invariant features; and a Fast Fourier Transform (FFT) block to extract frequency features. A feature fusion layer is designed to learn the degree affected by each component. The proposed model is applied directly to the raw time series data, without needing of any hand-engineered features. A sensitivity analysis (SA) method is applied to assess the influence of data preprocessing. Through benchmark test and experiment on ship motion dataset, SeaStateNet is verified effective for sea state estimation. The investigation on real-time test further shows the practicality of the proposed model.
ER  - 

TY  - CONF
TI  - Visual Localization at Intersections with Digital Maps
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6651
EP  - 6657
AU  - A. L. Ballardini
AU  - D. Cattaneo
AU  - D. G. Sorrenti
PY  - 2019
KW  - computer vision
KW  - feature extraction
KW  - image reconstruction
KW  - image segmentation
KW  - neural nets
KW  - object detection
KW  - pose estimation
KW  - road vehicles
KW  - stereo image processing
KW  - traffic engineering computing
KW  - ego-vehicle localization
KW  - autonomous road driving
KW  - online vision-based method
KW  - digital map service
KW  - pixel-level semantic segmentation
KW  - intersection approaches
KW  - visual localization
KW  - deep neural networks
KW  - coarse street-level pose estimation
KW  - Roads
KW  - Three-dimensional displays
KW  - Semantics
KW  - Image segmentation
KW  - Pipelines
KW  - Geometry
KW  - Task analysis
DO  - 10.1109/ICRA.2019.8794413
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper deals with the task of ego-vehicle localization at intersections, a significant task in autonomous road driving. We propose an online vision-based method that can hence be applied if the intersection is visible. It relies on stereo images and on a coarse street-level pose estimate, used to retrieve intersection data from a digital map service. Pixel-level semantic segmentation, and 3D reconstruction from state-of-the art Deep Neural Networks are coupled with an intersection model; this allows good positioning accuracy compared to the state-of-the-art in this task. To demonstrate the effectiveness of the method and make it possible to compare it with other methods, an extensive activity has been conducted in order to set up a dataset of approaches to an intersection, which has then been used to benchmark the proposed method. The dataset is made available to the community, and it currently includes more than forty intersection approaches, from KITTI. Another important contribution of the paper is the definition of criteria for the comparison of different methods, on recorded datasets. The proposed method achieves nearly sub-meter accuracy in difficult real conditions.
ER  - 

TY  - CONF
TI  - Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6658
EP  - 6664
AU  - J. Li
AU  - H. Ma
AU  - M. Tomizuka
PY  - 2019
KW  - decision making
KW  - interactive systems
KW  - learning (artificial intelligence)
KW  - multi-agent systems
KW  - neural nets
KW  - probability
KW  - hyperparameter values
KW  - adversarial learning
KW  - interaction-aware multiagent tracking
KW  - probabilistic behavior prediction
KW  - motion planning
KW  - intelligent systems
KW  - multiple interactive agents
KW  - distribution learning
KW  - decision making
KW  - generative adversarial network
KW  - Generators
KW  - Gallium nitride
KW  - Predictive models
KW  - Training
KW  - Generative adversarial networks
KW  - State estimation
KW  - Optimization
DO  - 10.1109/ICRA.2019.8793661
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In order to enable high-quality decision making and motion planning of intelligent systems such as robotics and autonomous vehicles, accurate probabilistic predictions for surrounding interactive objects is a crucial prerequisite. Although many research studies have been devoted to making predictions on a single entity, it remains an open challenge to forecast future behaviors for multiple interactive agents simultaneously. In this work, we take advantage of the Generative Adversarial Network (GAN) due to its capability of distribution learning and propose a generic multi-agent probabilistic prediction and tracking framework which takes the interactions among multiple entities into account, in which all the entities are treated as a whole. However, since GAN is very hard to train, we make an empirical research and present the relationship between training performance and hyperparameter values with a numerical case study. The results imply that the proposed model can capture both the mean, variance and multi-modalities of the groundtruth distribution. Moreover, we apply the proposed approach to a real-world task of vehicle behavior prediction to demonstrate its effectiveness and accuracy. The results illustrate that the proposed model trained by adversarial learning can achieve a better prediction performance than other state-of-the-art models trained by traditional supervised learning which maximizes the data likelihood. The well-trained model can also be utilized as an implicit proposal distribution for particle filtered based Bayesian state estimation.
ER  - 

TY  - CONF
TI  - Model Predictive Control of Ride-sharing Autonomous Mobility-on-Demand Systems
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6665
EP  - 6671
AU  - M. Tsao
AU  - D. Milojevic
AU  - C. Ruch
AU  - M. Salazar
AU  - E. Frazzoli
AU  - M. Pavone
PY  - 2019
KW  - predictive control
KW  - road traffic control
KW  - model predictive control approach
KW  - self-driving vehicles
KW  - on-demand mobility
KW  - time-expanded network flow model
KW  - real-time MPC algorithm
KW  - customer-carrying vehicles
KW  - social welfare
KW  - RAMoD system
KW  - ride-sharing autonomous mobility-on-demand systems
KW  - empty vehicle
KW  - customer-carrying vehicle
KW  - San Francisco
KW  - CA
KW  - Roads
KW  - Automobiles
KW  - Prediction algorithms
KW  - Artificial neural networks
KW  - Analytical models
KW  - Optimization
KW  - Predictive models
DO  - 10.1109/ICRA.2019.8794194
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper presents a model predictive control (MPC) approach to optimize routes for Ride-sharing Autonomous Mobility-on-Demand (RAMoD) systems, whereby self-driving vehicles provide coordinated on-demand mobility, possibly allowing multiple customers to share a ride. Specifically, we first devise a time-expanded network flow model for RAMoD. Second, leveraging this model, we design a real-time MPC algorithm to optimize the routes of both empty and customer-carrying vehicles, with the goal of optimizing social welfare, namely, a weighted combination of customers' travel time and vehicles' mileage. Finally, we present a real-world case study for the city of San Francisco, CA, by using the micro-scopic traffic simulator MATSim. The simulation results show that a RAMoD system can significantly improve social welfare with respect to a single-occupancy Autonomous Mobility-on-Demand (AMoD) system, and that the predictive structure of the proposed MPC controller allows it to outperform existing reactive ride-sharing coordination algorithms for RAMoD.
ER  - 

TY  - CONF
TI  - A Hierarchical Framework for Coordinating Large-Scale Robot Networks
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6672
EP  - 6677
AU  - Z. Liu
AU  - S. Zhou
AU  - H. Wang
AU  - Y. Shen
AU  - H. Li
AU  - Y. Liu
PY  - 2019
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - road traffic control
KW  - hierarchical framework
KW  - large-scale robot networks
KW  - motion coordination problems
KW  - multirobot system
KW  - robotic warehouses
KW  - automated transportation systems
KW  - life-long planning problem
KW  - coordination performance
KW  - robot motion uncertainties
KW  - hierarchical path planning
KW  - motion coordination structure
KW  - traffic heat-map
KW  - path planning level
KW  - sector-level path
KW  - path distance
KW  - motion coordination level
KW  - collision-free local path
KW  - rolling planning manner
KW  - traffic condition
KW  - robot uncertainty
KW  - Robot kinematics
KW  - Path planning
KW  - Task analysis
KW  - Collision avoidance
KW  - Planning
KW  - Topology
DO  - 10.1109/ICRA.2019.8793719
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In this paper, we study the cooperative path planning and motion coordination problems of the multi-robot system with large number of robots, aiming for practical applications in robotic warehouses and automated transportation systems. Particularly, we solve the life-long planning problem and guarantee the coordination performance in the presence of robot motion uncertainties. A hierarchical path planning and motion coordination structure is presented. The environment is divided into several sectors and a traffic heat-map is presented to describe the current sector-level traffic condition. In path planning level, the sector-level path is calculated by considering the path distance, the current traffic condition and the current robot uncertainty. In motion coordination level, local cooperative A* algorithm and conflict-based searching strategy are utilized within each sector to generate the collision-free local path of each robot in a rolling planning manner. The effectiveness and practical applicability of the proposed approach are validated by simulations with more than one thousand robots and real experiments.
ER  - 

TY  - CONF
TI  - EasyLabel: A Semi-Automatic Pixel-wise Object Annotation Tool for Creating Robotic RGB-D Datasets
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6678
EP  - 6684
AU  - M. Suchi
AU  - T. Patten
AU  - D. Fischinger
AU  - M. Vincze
PY  - 2019
KW  - feature extraction
KW  - image colour analysis
KW  - image segmentation
KW  - mobile robots
KW  - object detection
KW  - object recognition
KW  - robot vision
KW  - video signal processing
KW  - object segmentation methods
KW  - object-wise annotation
KW  - robot vision
KW  - OCID
KW  - robots face
KW  - robot perception systems
KW  - computer vision algorithms
KW  - expected operating domain
KW  - ground truth data
KW  - EasyLabel tool
KW  - high-quality ground truth annotation
KW  - pixel-level
KW  - densely cluttered scenes
KW  - semiautomatic process
KW  - complex scenes
KW  - sensor
KW  - scene distance
KW  - robotic RGB-d datasets
KW  - object masks
KW  - object cluttered indoor dataset
KW  - Robots
KW  - Three-dimensional displays
KW  - Tools
KW  - Object segmentation
KW  - Clutter
KW  - Image segmentation
KW  - Task analysis
DO  - 10.1109/ICRA.2019.8793917
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Developing robot perception systems for recognizing objects in the real world requires computer vision algorithms to be carefully scrutinized with respect to the expected operating domain. This demands large quantities of ground truth data to rigorously evaluate the performance of algorithms. This paper presents the EasyLabel tool for easily acquiring high-quality ground truth annotation of objects at pixel-level in densely cluttered scenes. In a semi-automatic process, complex scenes are incrementally built and EasyLabel exploits depth changes to extract precise object masks at each step. We use this tool to generate the Object Cluttered Indoor Dataset (OCID) that captures diverse settings of objects, background, context, sensor to scene distance, viewpoint angle and lighting conditions. OCID is used to perform a systematic comparison of existing object segmentation methods. The baseline comparison supports the need for pixel- and object-wise annotation to progress robot vision towards realistic applications. This insight reveals the usefulness of EasyLabel and OCID to better understand the challenges that robots face in the real world.
ER  - 

TY  - CONF
TI  - BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6685
EP  - 6691
AU  - J. Xue
AU  - J. Fang
AU  - T. Li
AU  - B. Zhang
AU  - P. Zhang
AU  - Z. Ye
AU  - J. Dou
PY  - 2019
KW  - image segmentation
KW  - object detection
KW  - robot vision
KW  - stereo image processing
KW  - autonomous driving community
KW  - large-scale dataset platform
KW  - large-scale 5D semantics benchmark
KW  - 3D+temporal
KW  - 4D+interactive
KW  - 5D interactive event recognition
KW  - 5D intention prediction
KW  - dynamic 4D tracking
KW  - Three-dimensional displays
KW  - Benchmark testing
KW  - Trajectory
KW  - Roads
KW  - Task analysis
KW  - Semantics
KW  - Legged locomotion
DO  - 10.1109/ICRA.2019.8793523
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249, 129 3D annotations, 4, 902 independent individuals for tracking with the length of overall 214, 922 points, 6, 004 valid fragments for 5D interactive event recognition, and 4, 900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/.
ER  - 

TY  - CONF
TI  - A Benchmarking Framework for Systematic Evaluation of Robotic Pick-and-Place Systems in an Industrial Grocery Setting
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6692
EP  - 6698
AU  - P. Triantafyllou
AU  - H. Mnyusiwalla
AU  - P. Sotiropoulos
AU  - M. A. Roa
AU  - D. Russell
AU  - G. Deacon
PY  - 2019
KW  - end effectors
KW  - industrial manipulators
KW  - materials handling equipment
KW  - path planning
KW  - robot vision
KW  - benchmarking framework
KW  - industrial grocery setting
KW  - robotic manipulation
KW  - industrial robotic applications
KW  - robotic solution
KW  - industrial setting
KW  - motion planning
KW  - pick-and-place operations
KW  - pick-and-place task
KW  - object placement
KW  - end-effectors
KW  - robotic pick-and-place systems
KW  - Benchmark testing
KW  - Task analysis
KW  - Service robots
KW  - Grasping
KW  - Protocols
KW  - System performance
DO  - 10.1109/ICRA.2019.8793993
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Robotic manipulation is a very active field of research nowadays; however, pick-and-place operations constitute the majority of today's industrial robotic applications. In order to adopt a robotic solution for an industrial setting, proper evaluation processes should be defined to assess the system's performance. A number of benchmarks have been proposed in the literature focusing mainly on individual components needed to perform the task, like grasping, perception and motion planning; thus, they do not provide enough information on the performance of the entire robotic system. To address this, we propose a benchmarking framework for a pick-and-place task inspired by a use case for picking fruits and vegetables in an industrial setting. To foster reproducible research and comparison of different robotic systems, the benchmarking framework uses surrogate objects with instructions on how to build them, an easy-to-reproduce environment, and guidelines for object placement. The proposed benchmark is applied to evaluate the performance of two variants of a robotic system with different end-effectors.
ER  - 

TY  - CONF
TI  - Characterizing Visual Localization and Mapping Datasets
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6699
EP  - 6705
AU  - S. Saeedi
AU  - E. D. C. Carvalho
AU  - W. Li
AU  - D. Tzoumanikas
AU  - S. Leutenegger
AU  - P. H. J. Kelly
AU  - A. J. Davison
PY  - 2019
KW  - motion estimation
KW  - rendering (computer graphics)
KW  - SLAM (robots)
KW  - Wasserstein distance
KW  - motion estimation algorithm
KW  - robotics SLAM benchmarking
KW  - visual localization
KW  - mapping algorithms
KW  - real-world trajectories
KW  - high-quality scenes
KW  - synthetic datasets
KW  - dense map
KW  - key SLAM applications
KW  - ground robotics
KW  - mapping datasets
KW  - motion estimation algorithms
KW  - computer vision
KW  - Simultaneous localization and mapping
KW  - Trajectory
KW  - Time measurement
KW  - Visualization
KW  - Benchmark testing
DO  - 10.1109/ICRA.2019.8793528
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Benchmarking mapping and motion estimation algorithms is established practice in robotics and computer vision. As the diversity of datasets increases, in terms of the trajectories, models, and scenes, it becomes a challenge to select datasets for a given benchmarking purpose. Inspired by the Wasserstein distance, this paper addresses this concern by developing novel metrics to evaluate trajectories and the environments without relying on any SLAM or motion estimation algorithm. The metrics, which so far have been missing in the research community, can be applied to the plethora of datasets that exist. Additionally, to improve the robotics SLAM benchmarking, the paper presents a new dataset for visual localization and mapping algorithms. A broad range of real-world trajectories is used in very high-quality scenes and a rendering framework to create a set of synthetic datasets with ground-truth trajectory and dense map which are representative of key SLAM applications such as virtual reality (VR), micro aerial vehicle (MAV) flight, and ground robotics.
ER  - 

TY  - CONF
TI  - Quantifying the Reality Gap in Robotic Manipulation Tasks
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6706
EP  - 6712
AU  - J. Collins
AU  - D. Howard
AU  - J. Leitner
PY  - 2019
KW  - image motion analysis
KW  - manipulators
KW  - mobile robots
KW  - robot vision
KW  - robotic manipulation tasks
KW  - Kinova robotic manipulator
KW  - motion capture system
KW  - manipulation-oriented robotic tasks
KW  - robotic reaching task
KW  - robotic interaction task
KW  - quantitative data
KW  - Physics
KW  - Engines
KW  - Task analysis
KW  - Hardware
KW  - Manipulators
KW  - Computational modeling
DO  - 10.1109/ICRA.2019.8793591
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - We quantify the accuracy of various simulators compared to a real world robotic reaching and interaction task. Simulators are used in robotics to design solutions for real world hardware without the need for physical access. The `reality gap' prevents solutions developed or learnt in simulation from performing well, or at all, when transferred to real-world hardware. Making use of a Kinova robotic manipulator and a motion capture system, we record a ground truth enabling comparisons with various simulators, and present quantitative data for various manipulation-oriented robotic tasks. We show the relative strengths and weaknesses of numerous contemporary simulators, highlighting areas of significant discrepancy, and assisting researchers in the field in their selection of appropriate simulators for their use cases.
ER  - 

TY  - CONF
TI  - Are We Ready for Autonomous Drone Racing? The UZH-FPV Drone Racing Dataset
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6713
EP  - 6719
AU  - J. Delmerico
AU  - T. Cieslewski
AU  - H. Rebecq
AU  - M. Faessler
AU  - D. Scaramuzza
PY  - 2019
KW  - helicopters
KW  - image capture
KW  - image sensors
KW  - image sequences
KW  - motion estimation
KW  - remotely operated vehicles
KW  - video cameras
KW  - UZH-FPV Drone Racing dataset
KW  - first-person-view racing quadrotor
KW  - state estimation algorithms
KW  - autonomous Drone Racing
KW  - visual-inertial state estimation
KW  - motion estimation
KW  - Drones
KW  - Optical imaging
KW  - Cameras
KW  - Trajectory
KW  - Optical sensors
KW  - Measurement
KW  - High-speed optical techniques
DO  - 10.1109/ICRA.2019.8793887
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - Despite impressive results in visual-inertial state estimation in recent years, high speed trajectories with six degree of freedom motion remain challenging for existing estimation algorithms. Aggressive trajectories feature large accelerations and rapid rotational motions, and when they pass close to objects in the environment, this induces large apparent motions in the vision sensors, all of which increase the difficulty in estimation. Existing benchmark datasets do not address these types of trajectories, instead focusing on slow speed or constrained trajectories, targeting other tasks such as inspection or driving. We introduce the UZH-FPV Drone Racing dataset, consisting of over 27 sequences, with more than 10 km of flight distance, captured on a first-person-view (FPV) racing quadrotor flown by an expert pilot. The dataset features camera images, inertial measurements, event-camera data, and precise ground truth poses. These sequences are faster and more challenging, in terms of apparent scene motion, than any existing dataset. Our goal is to enable advancement of the state of the art in aggressive motion estimation by providing a dataset that is beyond the capabilities of existing state estimation algorithms.
ER  - 

TY  - CONF
TI  - Practical guide to solve the minimum-effort problem with geometric algorithms and B-Splines
T2  - 2019 International Conference on Robotics and Automation (ICRA)
SP  - 6720
EP  - 6726
AU  - A. Paz
AU  - G. Arechavaleta
PY  - 2019
KW  - geometry
KW  - Jacobian matrices
KW  - legged locomotion
KW  - Lie algebras
KW  - Lie groups
KW  - motion control
KW  - optimal control
KW  - splines (mathematics)
KW  - discrete representation
KW  - direct collocation method
KW  - B-splines
KW  - biped robot
KW  - robot motions
KW  - articulated robots
KW  - Jacobian function
KW  - Lie groups
KW  - Lie algebra
KW  - transcription methods
KW  - numerical optimal control
KW  - geometric algorithms
KW  - minimum-effort problem
KW  - Splines (mathematics)
KW  - Robot motion
KW  - Optimal control
KW  - Mathematical model
KW  - Jacobian matrices
DO  - 10.1109/ICRA.2019.8794398
JO  - 2019 International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2019 International Conference on Robotics and Automation (ICRA)
Y1  - 20-24 May 2019
AB  - This paper focuses on important implementation issues of numerical optimal control that are often overlooked. In particular, transcription methods should be carefully implemented for obtaining a discrete representation of the problem. For this purpose, we explain the algorithms to solve the minimum-effort problem by applying a direct collocation method based on B-Splines. In addition, we describe how to compute the gradient of the objective function as well as the Jacobian of the constraints without the use of finite differences and automatic differentiation. Geometric algorithms based on Lie groups and Lie algebra are examined to efficiently compute the analytical derivatives of the equations of motion of articulated robots. These ingredients allow the fast computation of dynamically feasible robot motions. We provide numerical comparisons with a biped robot to validate our recipe against classical direct collocation methods.
ER  - 


