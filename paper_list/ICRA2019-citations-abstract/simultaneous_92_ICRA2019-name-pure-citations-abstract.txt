total paper: 92
Title: FMD Stereo SLAM: Fusing MVG and Direct Formulation Towards Accurate and Fast Stereo SLAM
Key Words: feature extraction  motion estimation  pose estimation  SLAM (robots)  stereo image processing  key-feature-based multiple view geometry  global map  3D structure  bundle adjustment  fast stereo SLAM  direct formulation  local map  constant motion model  direct-based formulation  novel stereo visual SLAM framework  FMD stereo SLAM  back-end process  stereo constraint  reprojection error minimization  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Feature extraction  Visualization  Robot vision systems  Two dimensional displays 
Abstract: We propose a novel stereo visual SLAM framework considering both accuracy and speed at the same time. The framework makes full use of the advantages of key-feature-based multiple view geometry (MVG) and direct-based formulation. At the front-end, our system performs direct formulation and constant motion model to predict a robust initial pose, reprojects local map to find 3D-2D correspondence and finally refines pose by the reprojection error minimization. This frontend process makes our system faster. At the back-end, MVG is used to estimate 3D structure. When a new keyframe is inserted, new mappoints are generated by triangulating. In order to improve the accuracy of the proposed system, bad mappoints are removed and a global map is kept by bundle adjustment. Especially, the stereo constraint is performed to optimize the map. This back-end process makes our system more accurate. Experimental evaluation on EuRoC dataset shows that the proposed algorithm can run at more than 100 frames per second on a consumer computer while achieving highly competitive accuracy.


Title: GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization and Mapping
Key Words: cameras  collision avoidance  convolutional neural nets  learning (artificial intelligence)  mobile robots  pose estimation  robot vision  SLAM (robots)  depth estimation system  GEN-SLAM  generative modeling  Deep Learning based system  obstacle avoidance  mobile robot  conventional geometric SLAM  single camera  topological map  camera image  topological location estimation  monocular localization  monocular simultaneous localization and mapping  Cameras  Image reconstruction  Simultaneous localization and mapping  Decoding  Training 
Abstract: We present a Deep Learning based system for the twin tasks of localization and obstacle avoidance essential to any mobile robot. Our system learns from conventional geometric SLAM, and outputs, using a single camera, the topological pose of the camera in an environment, and the depth map of obstacles around it. We use a CNN to localize in a topological map, and a conditional VAE to output depth for a camera image, conditional on this topological location estimation. We demonstrate the effectiveness of our monocular localization and depth estimation system on simulated and real datasets.


Title: RESLAM: A real-time robust edge-based SLAM system
Key Words: cameras  edge detection  image colour analysis  image representation  image sensors  motion estimation  optimisation  pose estimation  robot vision  SLAM (robots)  camera intrinsics  sliding window  edge-based verification  RESLAM  camera motions  RGBD sensors  sparse representation  SLAM pipeline  robust edge-based SLAM system  simultaneous localization and mapping  Image edge detection  Cameras  Simultaneous localization and mapping  Optimization  Real-time systems  Microsoft Windows 
Abstract: Simultaneous Localization and Mapping is a key requirement for many practical applications in robotics. In this work, we present RESLAM, a novel edge-based SLAM system for RGBD sensors. Due to their sparse representation, larger convergence basin and stability under illumination changes, edges are a promising alternative to feature-based or other direct approaches. We build a complete SLAM pipeline with camera pose estimation, sliding window optimization, loop closure and relocalisation capabilities that utilizes edges throughout all steps. In our system, we additionally refine the initial depth from the sensor, the camera poses and the camera intrinsics in a sliding window to increase accuracy. Further, we introduce an edge-based verification for loop closures that can also be applied for relocalisation. We evaluate RESLAM on wide variety of benchmark datasets that include difficult scenes and camera motions and also present qualitative results. We show that this novel edge-based SLAM system performs comparable to state-of-the-art methods, while running in real-time on a CPU. RESLAM is available as open-source software1.


Title: On-line 3D active pose-graph SLAM based on key poses using graph topology and sub-maps
Key Words: autonomous aerial vehicles  computational complexity  graph theory  mobile robots  optimisation  path planning  remotely operated vehicles  robot vision  SLAM (robots)  graph topology  pose-graph simultaneous localization  three-dimensional environments  D-optimality metrics  weighted node degree  T-optimality metric  sampling-based path  continuous-time trajectory optimization method  large-scale active SLAM problems  submap joining method  online 3D active pose-graph SLAM  Simultaneous localization and mapping  Measurement  Trajectory  Planning  Three-dimensional displays  Uncertainty 
Abstract: In this paper, we present an on-line active pose-graph simultaneous localization and mapping (SLAM) frame-work for robots in three-dimensional (3D) environments using graph topology and sub-maps. This framework aims to find the best trajectory for loop-closure by re-visiting old poses based on the T-optimality and D-optimality metrics of the Fisher information matrix (FIM) in pose-graph SLAM. In order to reduce computational complexity, graph topologies are introduced, including weighted node degree (T-optimality metric) and weighted tree-connectivity (D-optimality metric), to choose a candidate trajectory and several key poses. With the help of the key poses, a sampling-based path planning method and a continuous-time trajectory optimization method are combined hierarchically and applied in the whole framework. So as to further improve the real-time capability of the method, the sub-map joining method is used in the estimation and planning process for large-scale active SLAM problems. In simulations and experiments, we validate our approach by comparing against existing methods, and we demonstrate the on-line planning part using a quad-rotor unmanned aerial vehicle (UAV).


Title: Linear Heterogeneous Reconfiguration of Cubic Modular Robots via Simultaneous Tunneling and Permutation
Key Words: computational complexity  control engineering computing  distributed control  evolutionary computation  mobile robots  multi-robot systems  reconfigurable architectures  heterogeneous lattice modular robots  linear operation time cost  2×2×2 cubic meta-module-based connected robot structure  heterogeneous modular robots  linear heterogeneous reconfiguration  cubic modular robots  ordinary heterogeneous reconfiguration  linear homogeneous transformation  linear heterogeneous permutation  simultaneous tunneling and permutation  Tunneling  Computer aided software engineering  Cameras  Robot vision systems  Robot kinematics  Lattices 
Abstract: Reconfiguring heterogeneous modular robots in which all modules are not identical is much more time consuming than reconfiguring homogeneous ones, because ordinary heterogeneous reconfiguration is a combination of homogeneous transformation and heterogeneous permutation. While linear homogeneous transformation has been accomplished in previous research, linear heterogeneous permutation has not. This paper studies a reconfiguration algorithm for heterogeneous lattice modular robots with linear operation time cost. The algorithm is based on simultaneous tunneling and permutation, where a robot transforms its configuration via tunneling motion while permutation of each module's position is performed simultaneously during the tunneling transformation. To achieve this, we introduce the idea of a transparent meta-module that allows modules belonging to a meta-module to pass through the spaces occupied by other meta-modules. We prove the correctness and completeness of the proposed algorithm for a 2×2×2 cubic meta-module-based connected robot structure. We also show examples of the reconfiguration simulations of heterogeneous modular robots by the proposed algorithm.


Title: Design and Fabrication of Transformable Head Structures for Endoscopic Catheters*
Key Words: biomedical optical imaging  cameras  catheters  endoscopes  medical image processing  medical robotics  polymers  camera module  transformable catheter head structure  endoscopic catheter  laser micromachining  polymer catheter  Catheters  Head  Magnetic heads  Tools  Electron tubes  Polymers  Cameras 
Abstract: We present a transformable catheter head structure for endoscopic catheter allowing the simultaneous use of a camera module and a large tool channel introduced through a small incision. At the site of interest, the head with a camera can be expanded from the initial straight configuration, which opens a window for advancing a tool that is located behind the camera. Two different designs were proposed and prototyped. One option has flexure joints directly fabricated at the distal end of a polymer catheter by laser micro-machining, while another design employs a hinged metal head assembled at the tip of the same type of catheter. The kinematic behavior of each head was evaluated during the head-up and tip steering motions, and compared with each other to draw a selection guideline between them. Experimental results prove the feasibility of the proposed head structure for smarter endoscopic catheters.


Title: Robust Object-based SLAM for High-speed Autonomous Navigation
Key Words: cameras  helicopters  image sequences  image texture  mobile robots  object detection  path planning  robot vision  SLAM (robots)  ROSHAN  object-level mapping  ellipsoid-based SLAM  object surface  autonomous quadrotor  bounding box detections  median shape error  forward-moving camera sequence  planar constraint  vehicle motions  semantic knowledge  robust object-based SLAM for high-speed autonomous navigation  Ellipsoids  Image edge detection  Semantics  Simultaneous localization and mapping  Cameras  Shape  Shape measurement 
Abstract: We present Robust Object-based SLAM for High-speed Autonomous Navigation (ROSHAN), a novel approach to object-level mapping suitable for autonomous navigation. In ROSHAN, we represent objects as ellipsoids and infer their parameters using three sources of information - bounding box detections, image texture, and semantic knowledge - to overcome the observability problem in ellipsoid-based SLAM under common forward-translating vehicle motions. Each bounding box provides four planar constraints on an object surface and we add a fifth planar constraint using the texture on the objects along with a semantic prior on the shape of ellipsoids. We demonstrate ROSHAN in simulation where we outperform the baseline, reducing the median shape error by 83% and the median position error by 72% in a forward-moving camera sequence. We demonstrate similar qualitative result on data collected on a fast-moving autonomous quadrotor.


Title: Dynamically-consistent Generalized Hierarchical Control
Key Words: C++ language  control engineering computing  manipulator dynamics  manipulator kinematics  Matlab  redundant manipulators  robot programming  source code (software)  dynamically-consistent generalized hierarchical control  redundant robots  strict prioritization schemes  GHC  nullspace projection operator  dynamically-consistent stack-of-tasks hierarchies  DynGHC  soft prioritization schemes  Matlab  C++ source code  Task analysis  Jacobian matrices  Robots  Couplings  Matrix decomposition  Transmission line matrix methods  Matlab 
Abstract: Tracking multiple prioritized tasks simultaneously with redundant robots have been investigated extensively over the last decades. Recent research focuses on combining advantages from both classical soft and strict prioritization schemes which is non-trivial. Among the proposed methods to tackle this issue, Generalized Hierarchical Control (GHC) seems to have a reasonable performance, however, it does not include a weighting matrix in the computation of the nullspace projection operator and hence cannot construct dynamically-consistent stack-of-tasks hierarchies as a special case. We extend GHC by adding dynamic-consistency to the control scheme and refer to it as DynGHC. The extension is also advantageous when choosing non-strict priorities because inertia coupling between tasks is reduced. DynGHC allows to smoothly rearrange priorities which is important for robots acting in dynamically changing contexts. Comparative simulations with a 4 DOF planar manipulator and a KUKA LWR validate our approach. Matlab and C++ source code is made available.


Title: MH-iSAM2: Multi-hypothesis iSAM using Bayes Tree and Hypo-tree
Key Words: Bayes methods  data structures  mobile robots  optimisation  SLAM (robots)  original Bayes tree  hypothesis pruning strategy  multihypothesis iSAM  nonlinear incremental optimization algorithm  MH-iSAM2  simultaneous localization and mapping problems  SLAM problems  hypo-tree  multihypothesis inference  data structures  Simultaneous localization and mapping  Zirconium  Maximum likelihood estimation  Optimization  Inference algorithms  Robustness 
Abstract: A novel nonlinear incremental optimization algorithm MH-iSAM2 is developed to handle ambiguity in simultaneous localization and mapping (SLAM) problems in a multi-hypothesis fashion. It can output multiple possible solutions for each variable according to the ambiguous inputs, which is expected to greatly enhance the robustness of autonomous systems as a whole. The algorithm consists of two data structures: an extension of the original Bayes tree that allows efficient multi-hypothesis inference, and a Hypo-tree that is designed to explicitly track and associate the hypotheses of each variable as well as all the inference processes for optimization. With our proposed hypothesis pruning strategy, MH-iSAM2 enables fast optimization and avoids the exponential growth of hypotheses. We evaluate MH-iSAM2 using both simulated datasets and real-world experiments, demonstrating its improvements on the robustness and accuracy of SLAM systems.


Title: Improving Keypoint Matching Using a Landmark-Based Image Representation
Key Words: convolutional neural nets  image matching  image representation  learning (artificial intelligence)  landmark-based image representation  visual loop closure verification  multiview geometry  MVG  deep learning  convolutional neural network features  matched landmark pairs  image representation  verification method  keypoint matching method  ConvNet features  Lighting  Proposals  Visualization  Standards  Feature extraction  Simultaneous localization and mapping  Image representation 
Abstract: Motivated by the need to improve the performance of visual loop closure verification via multi-view geometry (MVG) under significant illumination and viewpoint changes, we propose a keypoint matching method that uses landmarks as an intermediate image representation in order to leverage the power of deep learning. In environments with various changes, the traditional verification method via MVG may encounter difficulty because of their inability to generate a sufficient number of correctly matched keypoints. Our method exploits the excellent invariance properties of convolutional neural network (ConvNet) features, which have shown outstanding performance for matching landmarks between images. By generating and matching landmarks first in the images and then matching the keypoints within the matched landmark pairs, we can significantly improve the quality of matched keypoints in terms of precision and recall measures. The proposed method is validated on challenging datasets that involve significant illumination and viewpoint changes, to establish its superior performance to the standard keypoint matching method.


Title: Fast and Robust Initialization for Visual-Inertial SLAM
Key Words: accelerometers  gyroscopes  inertial navigation  mobile robots  robot vision  SLAM (robots)  visual-inertial SLAM  VI-SLAM  gyroscope  initialization method  visual-inertial bundle adjustment  Cameras  Gravity  Simultaneous localization and mapping  Observability  Feature extraction  Jacobian matrices  Accelerometers 
Abstract: Visual-inertial SLAM (VI-SLAM) requires a good initial estimation of the initial velocity, orientation with respect to gravity and gyroscope and accelerometer biases. In this paper we build on the initialization method proposed by Martinelli [1] and extended by Kaiser et al. [2], modifying it to be more general and efficient. We improve accuracy with several rounds of visual-inertial bundle adjustment, and robustify the method with novel observability and consensus tests, that discard erroneous solutions. Our results on the EuRoC dataset show that, while the original method produces scale errors up to 156%, our method is able to consistently initialize in less than two seconds with scale errors around 5%, which can be further reduced to less than 1% performing visual-inertial bundle adjustment after ten seconds.


Title: Efficient Constellation-Based Map-Merging for Semantic SLAM
Key Words: covariance matrices  SLAM (robots)  tree searching  local information  SLAM graph  expensive recovery  system covariance matrix  joint compatibility  search space  clique-based pairwise compatibility  robust object-based loop-closure  SLAM problems  semantic SLAM  data association  high-confidence loop-closure mechanism  object-level SLAM  landmark uncertainty  constellation-based map-merging  branch-and-bound max-cardinality search  Simultaneous localization and mapping  Semantics  Covariance matrices  Uncertainty  Detectors  Search problems  Current measurement 
Abstract: Data association in SLAM is fundamentally challenging, and handling ambiguity well is crucial to achieve robust operation in real-world environments. When ambiguous measurements arise, conservatism often mandates that the measurement is discarded or a new landmark is initialized rather than risking an incorrect association. To address the inevitable “duplicate” landmarks that arise, we present an efficient map-merging framework to detect duplicate constellations of landmarks, providing a high-confidence loop-closure mechanism well-suited for object-level SLAM. This approach uses an incrementally-computable approximation of landmark uncertainty that only depends on local information in the SLAM graph, avoiding expensive recovery of the full system covariance matrix. This enables a search based on geometric consistency (GC) (rather than full joint compatibility (JC)) that inexpensively reduces the search space to a handful of “best” hypotheses. Furthermore, we reformulate the commonly-used interpretation tree to allow for more efficient integration of clique-based pairwise compatibility, accelerating the branch-and-bound max-cardinality search. Our method is demonstrated to match the performance of full JC methods at significantly-reduced computational cost, facilitating robust object-based loop-closure over large SLAM problems.


Title: Closed-loop MPC with Dense Visual SLAM - Stability through Reactive Stepping
Key Words: approximation theory  closed loop systems  humanoid robots  legged locomotion  mobile robots  motion control  nonlinear control systems  path planning  pendulums  position control  predictive control  SLAM (robots)  stability  fundamental capacity  external inputs  reference velocity  footstep plans  reference motion  external disturbances  closed-loop MPC scheme  proprioceptive sensors  imperfect open-loop control execution  HRP-4 humanoid robot  reactive stepping  walking gaits  humanoid locomotion  model predictive control  pendulum  dense visual SLAM stability  Simultaneous localization and mapping  Foot  Legged locomotion  Humanoid robots  Lips 
Abstract: Walking gaits generated using Model Predictive Control (MPC) is widely used due to its capability to handle several constraints that characterize humanoid locomotion. The use of simplified models such as the Linear Inverted Pendulum allows to perform computations in real-time, giving the robot the fundamental capacity to replan its motion to follow external inputs (e.g. reference velocity, footstep plans). However, usually the MPC does not take into account the current state of the robot when computing the reference motion, losing the ability to react to external disturbances. In this paper a closed-loop MPC scheme is proposed to estimate the robot's real state through Simultaneous Localization and Mapping (SLAM) and proprioceptive sensors (force/torque). With the proposed control scheme it is shown that the robot is able to react to external disturbances (push), by stepping to recover from the loss of balance. Moreover the localization allows the robot to navigate to target positions in the environment without being affected by the drift generated by imperfect open-loop control execution. We validate the proposed scheme through two different experiments with a HRP-4 humanoid robot.


Title: A Kalman Filter-Based Algorithm for Simultaneous Time Synchronization and Localization in UWB Networks
Key Words: indoor radio  Kalman filters  mobile robots  path planning  position control  radionavigation  synchronisation  time-of-arrival estimation  transceivers  ultra wideband communication  EKF-based navigation system design  mobile robot positioning  Kalman filter-based algorithm  UWB transceivers  ultra-wideband wireless communication transceivers  time difference of arrival  TDOA  passive UWB receivers  UWB networks  ranging-based positioning systems  simultaneous time synchronization-localization  signal time-of-flight measurement  time of arrival  TOA  multiple UWB base station synchronization  Clocks  Synchronization  Protocols  Distance measurement  Thermodynamics  Transceivers 
Abstract: The ability to accurately measure signal time-of-flight between ultra-wideband (UWB) wireless communication transceivers, even in multipath environments, makes this technology ideally suited to develop ranging-based positioning systems, especially for indoor applications where GPS signals are not available. In recent years, low-cost commercial UWB transceivers have become more easily available and increasingly used to develop custom robot positioning systems. In this paper, we focus in particular on positioning techniques requiring the synchronization of base stations such as Time of Arrival (TOA) and Time Difference of Arrival (TDOA). We present a protocol based on Kalman filtering for simultaneous synchronization of multiple UWB base stations and positioning of an arbitrary number of passive UWB receivers. We illustrate experimentally using our protocol and an EKF-based navigation system design the level of accuracy achievable with small low-power UWB modules for mobile robot positioning. We discuss in details measurement errors and system tuning issues applicable to popular commercial UWB transceivers.


Title: Active Contraints for Tool-Shaft Collision Avoidance in Minimally Invasive Surgery
Key Words: force control  medical robotics  surgery  telerobotics  tool-shaft collision avoidance  clinical adoption  master-slave surgical systems  physical separation  Active Constraints  sensory information  surgical robot operators  haptic cues  visual cues  audible cues  surgical tool-clashing  elasto-plastic frictional force control  ACs methods  teleoperated da Vinci Surgical System  ACs assistance  teleoperation-based robotic-assisted minimally invasive surgery  minimally invasive partial nephrectomy  Force  Tools  Surgery  Image segmentation  Arteries  Shafts  Robots 
Abstract: Recent advances in teleoperation-based robotic-assisted Minimally Invasive Surgery (MIS) have made significant inroads in clinical adoption. However, such master-slave surgical systems create a physical separation between the surgeon and the patient. The concept of Active Constraints (ACs) provides guidance and sensory information to surgical robot operators in a form of haptic, visual or audible cues. This work proposes a novel ACs approach to avoid surgical tool-clashing and collision of the tool-shaft with delicate anatomy using elasto-plastic frictional force control. The presented framework is designed to reduce the occurence of direct coupling during electrocautery and to protect high-risk regions in Minimally Invasive Partial Nephrectomy (MIPN). Moreover, we combine aforementioned ACs methods and propose a solution when simultaneous penetration of both constraints occurs. The proposed methodology is implemented on the teleoperated da Vinci Surgical System using the da Vinci Research Kit (dVRK) and its performance is demonstrated through three types of user experiments. The experimental results show that the developed algorithms are of significant benefit in performing the tasks with ACs assistance.


Title: Goal-Driven Navigation for Non-holonomic Multi-Robot System by Learning Collision
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  multi-robot systems  goal-driven navigation  nonholonomic multirobot system  learning collision  reinforcement learning  multirobot collision avoidance approach  training agent robots  agent robot  trained policy  multiple obstacle robots  robot simulation  robot experiment  Robots  Collision avoidance  Training  Planning  Task analysis  Servers  Heuristic algorithms 
Abstract: In this paper, we propose the reinforcement learning based multi-robot collision avoidance approach by learning collision. Dynamical path re-planning, which is massively used in classical collision avoidance methods, needs overall information of the environment. Also, training agent robots to avoid the collision and pursue a goal point simultaneously is inefficient since the agent should learn two tasks. As the number of tasks that the agent should learn increases, it is difficult to make the performance of an algorithm consistent, which is known as reproducibility issue. To overcome these limitations, Collision Avoidance by Learning Collision (CALC), which learns collision instead of avoiding an obstacle robot is suggested. To solve the collision avoidance problem efficiently, the proposed method divides the problem into training and planning. In the training algorithm, an agent robot learns how to collide with a single obstacle robot and then generates a trained policy. With the trained policy, the agent can pursue a goal point since the policy leads the agent to `collide' with the goal. Furthermore, by taking action in a reverse way from the trained policy, the agent can avoid multiple obstacle robots in the planning algorithm at once. The proposed method is validated both in the robot simulation and real robot experiment, and compared with the existing collision avoidance method.


Title: Visual Appearance Analysis of Forest Scenes for Monocular SLAM
Key Words: autonomous aerial vehicles  mobile robots  path planning  remotely operated vehicles  robot vision  SLAM (robots)  managed forests  tree health  SLAM research  structured human environments  unstructured forests  forest data  photorealistic simulated forest  straightforward forest terrain  forest scenes  natural scenes  visual appearance analysis  cheap energy efficient way  unmanned aerial vehicles  monocular SLAM systems  SLAM systems  visual appearance statistics  Forestry  Simultaneous localization and mapping  Cameras  Visualization  Vegetation  Feature extraction  Lighting 
Abstract: Monocular simultaneous localisation and mapping (SLAM) is a cheap and energy efficient way to enable Unmanned Aerial Vehicles (UAVs) to safely navigate managed forests and gather data crucial for monitoring tree health. SLAM research, however, has mostly been conducted in structured human environments, and as such is poorly adapted to unstructured forests. In this paper, we compare the performance of state of the art monocular SLAM systems on forest data and use visual appearance statistics to characterise the differences between forests and other environments, including a photorealistic simulated forest. We find that SLAM systems struggle with all but the most straightforward forest terrain and identify key attributes (lighting changes and in-scene motion) which distinguish forest scenes from “classic” urban datasets. These differences offer an insight into what makes forests harder to map and open the way for targeted improvements. We also demonstrate that even simulations that look impressive to the human eye can fail to properly reflect the difficult attributes of the environment they simulate, and provide suggestions for more closely mimicking natural scenes.


Title: Lightweight Contrast Modeling for Attention-Aware Visual Localization
Key Words: intelligent robots  object detection  salient object detection task  object saliency details  lightweight refinement module  high-level visual contrast  superior lightweight network architecture  computational resource consumption  detection performance  attention-aware visual objects  attention-aware visual localization  lightweight contrast modeling  Convolution  Visualization  Object detection  Feature extraction  Robots  Training  Task analysis 
Abstract: Salient object detection, which aims at localizing the attention-aware visual objects, is the indispensable technology for intelligent robots to understand and interact with the complicated environments. Existing salient object detection approaches mainly focus on the optimization of detection performance, while ignoring the considerations for computational resource consumption and algorithm efficiency. Contrarily, we build a superior lightweight network architecture to simultaneously improve performance on both accuracy and efficiency for salient object detection. Specifically, our proposed approach adopts the lightweight bottleneck as its primary building block to significantly reduce the number of parameters and to speed up the process of training and inference. In practice, the visual contrast is insufficiently discovered with the limitation of the small empirical receptive field of CNN. To alleviate this issue, we design a multi-scale convolution module to rapidly discover high-level visual contrast. Moreover, a lightweight refinement module is utilized to restore object saliency details with negligible extra cost. Extensive experiments on efficiency and accuracy trade-offs show that our model is more competitive than the state-of-the-art works on salient object detection task and has prominent potentials for robots applications in real time.


Title: Distributed Radiation Field Estimation and Informative Path Planning for Nuclear Environment Characterization
Key Words: Global Positioning System  mobile robots  optical radar  path planning  photomultipliers  scintillation counters  solid scintillation detectors  thallium  distributed radiation field estimation  informative path planning  nuclear environment characterization  autonomous estimation  distributed nuclear radiation fields  GPS-denied environments  sensing apparatus  radially placed Thallium-doped Cesium Iodide  Silicon Photomultipliers  pulse counting circuitry  provided readings  LiDAR-based localization  radiation intensity readings  immediate field gradient  believed field intensity  local measurement  field gradient co-estimation  informative data gathering  path planning strategy  uncertainty  admissible paths  autonomous exploration  SiPm  Estimation  Detectors  Robot sensing systems  Path planning  Uncertainty  Area measurement 
Abstract: This paper details the system and methods designed to enable the autonomous estimation of distributed nuclear radiation fields within complex and possibly GPS-denied environments. A sensing apparatus consisting of three radially placed Thallium-doped Cesium Iodide (CsI(Tl)) scintillators and Silicon Photomultipliers (SiPm) combined with custom- built pulse counting circuitry is designed and the provided readings are pose-annotated using LiDAR-based localization. Given this capacity, a method that utilizes the radiation intensity readings to first calculate the immediate field gradient and then combine this information to update and co-estimate the believed field intensity and gradient across the whole environment is developed. The strategy propagates the effect of each local measurement through field gradient co-estimation and simultaneously derives a model of the underlying uncertainty. To further support the need for informative data gathering, especially in the framework of emergency and rapid reconnaissance missions, a path planning strategy is also developed that first utilizes the field intensity and uncertainty estimates to select its new waypoint and then performs terrain traversability analysis to derive admissible paths. The complete system is evaluated both in simulation and experimentally. The experimental results refer to the autonomous exploration and field estimation inside an indoor facility within which actual radioactive uranium and thorium ore sources have been distributed.


Title: Visual SLAM: Why Bundle Adjust?
Key Words: cameras  feature extraction  image sequences  motion estimation  optimisation  pose estimation  robot vision  SLAM (robots)  video signal processing  bundle adjustment  feature-based monocular SLAM  camera orientation optimisation  camera position estimation  quasiconvex formulation  keyframe rate  SLAM optimisation  rotational motion  slow motion  SLAM algorithm  3D structure estimation  input feature tracks  3D point cloud  3D map estimation  6DOF camera trajectory estimation  visual SLAM  Simultaneous localization and mapping  Cameras  Estimation  Bundle adjustment  Optimization  Visualization 
Abstract: Bundle adjustment plays a vital role in feature-based monocular SLAM. In many modern SLAM pipelines, bundle adjustment is performed to estimate the 6DOF camera trajectory and 3D map (3D point cloud) from the input feature tracks. However, two fundamental weaknesses plague SLAM systems based on bundle adjustment. First, the need to carefully initialise bundle adjustment means that all variables, in particular the map, must be estimated as accurately as possible and maintained over time, which makes the overall algorithm cumbersome. Second, since estimating the 3D structure (which requires sufficient baseline) is inherent in bundle adjustment, the SLAM algorithm will encounter difficulties during periods of slow motion or pure rotational motion. We propose a different SLAM optimisation core: instead of bundle adjustment, we conduct rotation averaging to incrementally optimise only camera orientations. Given the orientations, we estimate the camera positions and 3D points via a quasi-convex formulation that can be solved efficiently and globally optimally. Our approach not only obviates the need to estimate and maintain the positions and 3D map at keyframe rate (which enables simpler SLAM systems), it is also more capable of handling slow motions or pure rotational motions.


Title: Illumination Robust Monocular Direct Visual Odometry for Outdoor Environment Mapping
Key Words: distance measurement  image colour analysis  lighting  mobile robots  motion estimation  robot vision  SLAM (robots)  stereo image processing  illumination-robust direct monocular SLAM system  global lighting changes  local lighting changes  stereo SLAM systems  camera motion  scene structure  high-precision motion estimation  illumination robust monocular direct visual odometry  outdoor environment  illumination invariant photometric costs  vision-based localization and mapping  RGB-D  DSO system  ORBSLAM2 system  Lighting  Optimization  Robustness  Simultaneous localization and mapping  Cameras  Motion estimation  Three-dimensional displays 
Abstract: Vision-based localization and mapping in outdoor environments is still a challenging issue, which requests significant robustness against various unpredictable illumination changes. In this paper, an illumination-robust direct monocular SLAM system that focuses on modeling outdoor scenery is presented. To deal with global and local lighting changes, such as solar flares, the state-of-art illumination invariant photometric costs for RGB-D and stereo SLAM systems are revisited in the context of their monocular counterpart, where the camera motion and scene structure are jointly optimized with a reasonably poor initialization. Based on our analysis, a combined cost is proposed to achieve a high-precision motion estimation with an improved convergence radius. The proposed system is extensively evaluated on the synthetic and real-world datasets regarding accuracy, robustness, and processing time, where our approach outperforms systems with other costs and state-of-art DSO and ORBSLAM2 systems.


Title: A Comparison of CNN-Based and Hand-Crafted Keypoint Descriptors
Key Words: convolutional neural nets  image matching  neurocontrollers  robot vision  SLAM (robots)  pre-trained CNN descriptors  viewpoint changes  illumination changes  hand-crafted keypoint descriptors  keypoint matching  computer vision  keypoint description  trained convolutional neural networks  pre-trained CNNs  hand-crafted descriptors  visual simultaneous localization and mapping  CNN-based descriptors  SLAM  Lighting  Computational modeling  Detectors  Dogs  Simultaneous localization and mapping  Measurement 
Abstract: Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo condition changes such as those in illumination and viewpoint. Previous research in keypoint description has pursued three classes of descriptors: hand-crafted, those from trained convolutional neural networks (CNN), and those from pre-trained CNNs. This paper provides a comparative study of the three classes of keypoint descriptors, in terms of their ability to handle conditional changes. The study is conducted on the latest benchmark datasets in computer vision with challenging conditional changes. Our study finds that (a) in general CNN-based descriptors outperform hand-crafted descriptors, (b) the trained CNN descriptors perform better than pre-trained CNN descriptors with respect to viewpoint changes, and (c) pre-trained CNN descriptors perform better than trained CNN descriptors with respect to illumination changes. These findings can serve as a basis for selecting appropriate keypoint descriptors for various applications.


Title: Environment Driven Underwater Camera-IMU Calibration for Monocular Visual-Inertial SLAM
Key Words: autonomous underwater vehicles  calibration  cameras  inertial navigation  mobile robots  SLAM (robots)  visual-inertial SLAM  shallow water  calibration errors  environmental indexes  underwater camera-inertial measurement unit  simultaneous localization and mapping  intrinsic parameters  extrinsic parameters  underwater monocular vision systems  environment driven underwater camera-IMU calibration  Cameras  Calibration  Atmospheric modeling  Simultaneous localization and mapping  Geometry  Mathematical model 
Abstract: Most state-of-the-art underwater vision systems are calibrated manually in shallow water and used in open seas without changing. However, the refractivity of the water is adaptively changed depending on the salinity, temperature, depth or other underwater environmental indexes, which inevitably generate the calibration errors and induces incorrectness e.g., for underwater Simultaneously Localization and Mapping (SLAM). To address this issue, in this paper, we propose a new underwater Camera-Inertial Measurement Unit (IMU) calibration model, which just needs to be calibrated once in the air, and then both the intrinsic parameters and extrinsic parameters between the camera and IMU could be automatically calculated depending on the environment indexes. To our best knowledge, this is the first work to consider the underwater Camera-IMU calibration via environmental indexes. We also build a verification platform to validate the effectiveness of our proposed method on real experiments, and use it for underwater monocular Visual-Inertial SLAM.


Title: Leveraging Structural Regularity of Atlanta World for Monocular SLAM
Key Words: Kalman filters  maximum likelihood estimation  polynomials  SLAM (robots)  multiple local Atlanta frames  global Atlanta frames  world frame  structural regularity  monocular SLAM  common vertical axis  multiple horizontal axes  camera frame  Simultaneous localization and mapping  Three-dimensional displays  Cameras  Estimation  Optimization  Reliability 
Abstract: A wide range of man-made environments can be abstracted as the Atlanta world. It consists of a set of Atlanta frames with a common vertical (gravitational) axis and multiple horizontal axes orthogonal to this vertical axis. This paper focuses on leveraging the regularity of Atlanta world for monocular SLAM. First, we robustly cluster image lines. Based on these clusters, we compute the local Atlanta frames in the camera frame by solving polynomial equations. Our method provides the global optimum and satisfies inherent geometric constraints. Second, we define the posterior probabilities to refine the initial clusters and Atlanta frames alternately by the maximum a posteriori estimation. Third, based on multiple local Atlanta frames, we compute the global Atlanta frames in the world frame using Kalman filtering. We optimize rotations by the global alignment and then refine translations and 3D line-based map under the directional constraints. Experiments on both synthesized and real data have demonstrated that our approach outperforms state-of-the-art methods.


Title: Multimodal Semantic SLAM with Probabilistic Data Association
Key Words: image fusion  image representation  inference mechanisms  mobile robots  object detection  path planning  probability  robot vision  SLAM (robots)  nonGaussian sensor model  multimodal semantic SLAM  probabilistic data association  robot navigation  semantic SLAM problem  discrete inference problem  object class labels  measurement-landmark correspondences  continuous inference problem  robot poses  object detection systems  simultaneous localization and mapping  object-based representations  object locations  nonGaussian inference problem  Simultaneous localization and mapping  Semantics  Belief propagation  Maximum likelihood estimation  Maximum likelihood detection 
Abstract: The recent success of object detection systems motivates object-based representations for robot navigation; i.e. semantic simultaneous localization and mapping (SLAM). The semantic SLAM problem can be decomposed into a discrete inference problem: determining object class labels and measurement-landmark correspondences (the data association problem), and a continuous inference problem: obtaining the set of robot poses and object locations in the environment. A solution to the semantic SLAM problem necessarily addresses this joint inference, but under ambiguous data associations this is in general a non-Gaussian inference problem, while the majority of previous work focuses on Gaussian inference. Previous solutions to data association either produce solutions between potential hypotheses or maintain multiple explicit hypotheses for each association. We propose a solution that represents hypotheses as multiple modes of an equivalent non-Gaussian sensor model. We then solve the resulting non-Gaussian inference problem using nonparametric belief propagation. We validate our approach in a simulated hallway environment under a variety of sensor noise characteristics, as well as using real data from the KITTI dataset, demonstrating improved robustness to perceptual aliasing and odometry uncertainty.


Title: ATLAS FaST: Fast and Simple Scheduled TDOA for Reliable Ultra-Wideband Localization
Key Words: direction-of-arrival estimation  mobile robots  operating systems (computers)  public domain software  time-of-arrival estimation  ATLAS FaST  fast scheduled TDOA  simple scheduled TDOA  ultra-wideband localization  wireless localization  aerial robot control  high precision personal safety tracking  required localization systems  multiuser scalability  energy efficiency  real-time capabilities  real-time localization  robot operating system  open source access  precise robotic location estimation  scheduled time-difference of arrival channel access  Synchronization  Clocks  Mobile nodes  Reliability  Wireless communication  Batteries 
Abstract: The ever increasing need for precise location estimation in robotics is challenging a significant amount of research. Hence, new applications such as wireless localization based aerial robot control or high precision personal safety tracking are developed. However, most of the current developments and research solely focus on the accuracy of the required localization systems. Multi-user scalability, energy efficiency and real-time capabilities are often neglected. This work aims to overcome the technology barrier by providing scalable, high accuracy, real-time localization through energy-efficient, scheduled time-difference of arrival channel access. We could show that simultaneous processing and provisioning of more than a thousand localization results per second with high reliability is possible using the proposed approach. To enable wide-spread adoption, we provide an open source implementation of our system for the robot operating system (ROS). Furthermore, we provide open source access to the raw data created during our evaluation.


Title: DSNet: Joint Learning for Scene Segmentation and Disparity Estimation
Key Words: feature extraction  image coding  image matching  image segmentation  image sequences  learning (artificial intelligence)  semantic features  deep disparity features  semantic labels  scene segmentation  disparity estimation  scene semantics  optical flow estimation  depth information  dense depth maps  image frames  deep semantic information  disparity feature maps  independent encoding modules  semantic disparity information  multitasking architecture DSNet  ResNet encoding module  Semantics  Estimation  Task analysis  Feature extraction  Optical imaging  Training  Three-dimensional displays 
Abstract: Recently, research works have attempted the joint prediction of scene semantics and optical flow estimation, which demonstrate the mutual improvement between both tasks. Besides, the depth information is also indispensable for the scene understanding, and disparity estimation is necessary for outputting dense depth maps. Such task shares a great similarity with the optical flow estimation since they can all be cast into a problem of capturing the difference at a location of two image frames. However, as far as we know, currently there are few networks for the joint learning of semantic and disparity. Moreover, since deep semantic information and disparity feature maps can learn from each other, we find it unnecessary with two independent encoding modules to separately extract semantic and disparity features. Therefore, we propose a unified multi-tasking architecture DSNet, for the simultaneous estimation of semantic and disparity information. In our model, semantic features, extracted by the encoding module ResNet from the left and right images, are used to obtain the deep disparity features via a novel matching module which performs pixel-to-pixel matching. In addition, we also use the disparity map to perform warp operation on deep features of the right image to deal with the problem of lacking of semantic labels. The effectiveness of our method is demonstrated by extensive experiments.


Title: An Autonomous Loop-Closure Approach for Simultaneous Exploration and Coverage of Unknown Infrastructure Using MAVs
Key Words: autonomous aerial vehicles  inspection  microrobots  mobile robots  spatial measurements  MAV motions  complete exploration  autonomous loop-closure approach  simultaneous exploration  unknown infrastructure  attractive means  critical infrastructure  autonomous tasks  precise spatial model  operational area  sensor measurements  autonomous inspection capabilities  autonomous MAV exploration  unknown structure  spatial information  high-fidelity 3D model  low-cost microaerial vehicles  Planning  Three-dimensional displays  Task analysis  Inspection  Simultaneous localization and mapping 
Abstract: The recent proliferation of low-cost Micro Aerial Vehicles (MAV) offers an attractive means for inspecting critical infrastructure autonomously. However, to enable such autonomous tasks requires a precise spatial model of the structure and operational area, typically constructed using sensor measurements obtained from the environment. To facilitate autonomous inspection capabilities, we address the problem of autonomous MAV exploration and coverage of an unknown structure to acquire the spatial information necessary for the development of a high-fidelity 3D model of the structure. Key to this problem is to not only cover the entire structure to acquire a complete set of spatial measurements, but also to minimize accumulative data errors during the exploration through direct planning of loop closures. We introduce a real-time waypoint planning approach to guide MAV motions to achieve complete exploration, coverage, and loop closure while respecting limited onboard resources.


Title: Point Cloud Compression for 3D LiDAR Sensor using Recurrent Neural Network with Residual Blocks
Key Words: computational geometry  data compression  image coding  iterative methods  mobile robots  octrees  optical radar  recurrent neural nets  SLAM (robots)  recurrent neural network  residual blocks  generic octree point cloud compression method  potential application scenarios  decompressed point cloud data  3D LiDAR sensor  autonomous driving systems  3D LiDAR data  raw D formatted LiDAR data  2D formatted LiDAR data  Three-dimensional displays  Image coding  Laser radar  Two dimensional displays  Robot sensing systems  Decoding  Recurrent neural networks 
Abstract: The use of 3D LiDAR, which has proven its capabilities in autonomous driving systems, is now expanding into many other fields. The sharing and transmission of point cloud data from 3D LiDAR sensors has broad application prospects in robotics. However, due to the sparseness and disorderly nature of this data, it is difficult to compress it directly into a very low volume. A potential solution is utilizing raw LiDAR data. We can rearrange the raw data from each frame losslessly in a 2D matrix, making the data compact and orderly. Due to the special structure of 3D LiDAR data, the texture of the 2D matrix is irregular, in contrast to 2D matrices of camera images. In order to compress this raw, 2D formatted LiDAR data efficiently, in this paper we propose a method which uses a recurrent neural network and residual blocks to progressively compress one frame's information from 3D LiDAR. Compared to our previous image compression based method and generic octree point cloud compression method, the proposed approach needs much less volume while giving the same decompression accuracy. Potential application scenarios for point cloud compression are also considered in this paper. We describe how decompressed point cloud data can be used with SLAM (simultaneous localization and mapping) as well as for localization using a given map, illustrating potential uses of the proposed method in real robotics applications.


Title: Integrated Mapping and Path Planning for Very Large-Scale Robotic (VLSR) Systems
Key Words: collision avoidance  control engineering computing  mobile robots  multi-robot systems  regression analysis  sensor fusion  decentralized approach  obstacle mapping  continuous probabilistic representation  mapping problem  binary classification task  kernel logistic regression  discriminative classifier online  individual robot maps  path planning algorithm  maximum information value  obstacle avoidance  VLSR system  prior obstacle information  robot paths  Hilbert map fusion method  large-scale robotic systems  onboard range sensors  Robot sensing systems  Entropy  Robot kinematics  Collision avoidance  Path planning 
Abstract: This paper develops a decentralized approach for mapping and information-driven path planning for Very Large Scale Robotic (VLSR) systems. In this approach, obstacle mapping is performed using a continuous probabilistic representation known as a Hilbert map, which formulates the mapping problem as a binary classification task and uses kernel logistic regression to train a discriminative classifier online. A novel Hilbert map fusion method is presented that quickly and efficiently combines the information from individual robot maps. An integrated mapping and path planning algorithm is presented to determine paths of maximum information value, while simultaneously performing obstacle avoidance. Furthermore, the effect of how percentage communication failure effects the overall performance of the system is investigated. The approach is demonstrated on a VLSR system with hundreds of robots that must map obstacles collaboratively over a large region of interest using onboard range sensors and no prior obstacle information. The results show that, through fusion and decentralized processing, the entropy of the map decreases over time and robot paths remain collision-free.


Title: Non-Gaussian SLAM utilizing Synthetic Aperture Sonar
Key Words: acoustic signal processing  array signal processing  graph theory  marine navigation  pose estimation  probability  sensor fusion  SLAM (robots)  synthetic aperture sonar  SLAM framework  beacon position  acoustic measurements  factor graph formulation  nonGaussian SLAM  spatial resolution  navigational measurements  underwater missions  synthetic aperture sonar  SAS  simultaneous localization and mapping  accurate pose estimation  hydrophones acoustic data  empirical probability distribution  conventional beamformer  autonomous surface vehicle  Acoustics  Synthetic aperture sonar  Array signal processing  Simultaneous localization and mapping  Apertures  Receivers  Sonar navigation 
Abstract: Synthetic Aperture Sonar (SAS) is a technique to improve the spatial resolution from a moving set of receivers by extending the array in time, increasing the effective array length and aperture. This technique is limited by the accuracy of the receiver position estimates, necessitating highly accurate, typically expensive aided-inertial navigation systems for submerged platforms. We leverage simultaneous localization and mapping to fuse acoustic and navigational measurements and obtain accurate pose estimates even without the benefit of absolute positioning for lengthy underwater missions. We demonstrate a method of formulating the well-known SAS problem in a SLAM framework, using acoustic data from hydrophones to simultaneously estimate platform and beacon position. An empirical probability distribution is computed from a conventional beamformer to correctly account for uncertainty in the acoustic measurements. The non-parametric method relieves the familiar Gaussian-only assumption currently used in the localization and mapping discipline and fits effectively into a factor graph formulation with conventional factors such as ground-truth priors and odometry. We present results from field experiments performed on the Charles River with an autonomous surface vehicle which demonstrate simultaneous localization of an unknown acoustic beacon and vehicle positioning, and provide comparison to GPS ground truths.


Title: Underwater Terrain Reconstruction from Forward-Looking Sonar Imagery
Key Words: feature extraction  Gaussian processes  graph theory  image reconstruction  mobile robots  remotely operated vehicles  SLAM (robots)  sonar imaging  terrain mapping  underwater vehicles  terrain reconstruction  forward-looking sonar imagery  underwater simultaneous localization  multibeam imaging sonar  3D terrain mapping tasks  elevation angle information  data association  accurate 3D mapping  Euclidean space  optical flow  bearing-range images  subsea terrain  Gaussian Process random field  terrain factors  factor graph  terrain elevation estimate  variable-elevation tank environment  smooth height estimate  sonar images  Chow-Liu tree  extracted feature tracking  Feature extraction  Sonar measurements  Simultaneous localization and mapping  Three-dimensional displays  Gaussian processes  Imaging 
Abstract: In this paper, we propose a novel approach for underwater simultaneous localization and mapping using a multibeam imaging sonar for 3D terrain mapping tasks. The high levels of noise and the absence of elevation angle information in sonar images present major challenges for data association and accurate 3D mapping. Instead of repeatedly projecting extracted features into Euclidean space, we apply optical flow within bearing-range images for tracking extracted features. To deal with degenerate cases, such as when tracking is interrupted by noise, we model the subsea terrain as a Gaussian Process random field on a Chow-Liu tree. Terrain factors are incorporated into the factor graph, aimed at smoothing the terrain elevation estimate. We demonstrate the performance of our proposed algorithm in a simulated environment, which shows that terrain factors effectively reduce estimation error. We also show ROV experiments performed in a variable-elevation tank environment, where we are able to construct a descriptive and smooth height estimate of the tank bottom.


Title: Using comanipulation with active force feedback to undistort stiffness perception in laparoscopy
Key Words: force feedback  force measurement  medical computing  medical robotics  surgery  laparoscopic surgery  undistort stiffness perception  comanipulation paradigm  fulcrum  lever effect  laparoscopy  stiffness perception  active force feedback  preliminary assessment experiment  lever ratio  tool tip  surgeon  robotic device  Tools  Force  Robot sensing systems  Surgery  Instruments  Laparoscopes 
Abstract: Surgeons performing laparoscopic surgery experience distortion when perceiving the stiffness of a patient's tissues. This is due to the lever effect induced by the introduction of instruments in their patient's body through a fulcrum. To address this problem, we propose to use the comanipulation paradigm. A robotic device is connected to the handle of the instrument while simultaneously being held by the surgeon. This device applies a force on the handle that reflects the force measured at the tool tip, with a gain that depends on the lever ratio. The implementation of this method is presented on an experimental setup and a preliminary assessment experiment is presented.


Title: KO-Fusion: Dense Visual SLAM with Tightly-Coupled Kinematic and Odometric Tracking
Key Words: cameras  distance measurement  image colour analysis  image fusion  image texture  manipulator kinematics  mobile robots  motion estimation  robot vision  SLAM (robots)  stereo image processing  KO-fusion  dense visual SLAM methods  observer  visual information  SLAM systems  inertial measurements  dense RGB-D SLAM system  wheeled robot  kinematic data  odometric data  kinematic measurements  tightly-coupled kinematic  odometric tracking  manipulator  odometry measurements  motion estimation  Cameras  Simultaneous localization and mapping  Robot vision systems  Kinematics  Manipulators 
Abstract: Dense visual SLAM methods are able to estimate the 3D structure of an environment and locate the observer within them. They estimate the motion of a camera by matching visual information between consecutive frames, and are thus prone to failure under extreme motion conditions or when observing texture-poor regions. The integration of additional sensor modalities has shown great promise in improving the robustness and accuracy of such SLAM systems. In contrast to the popular use of inertial measurements we propose to tightly-couple a dense RGB-D SLAM system with kinematic and odometry measurements from a wheeled robot equipped with a manipulator. The system has real-time capability while running on GPU. It optimizes the camera pose by considering the geometric alignment of the map as well as kinematic and odometric data from the robot. Through experimentation in the real-world, we show that the system is more robust to challenging trajectories featuring fast and loopy motion than the equivalent system without the additional kinematic and odometric knowledge, whilst retaining comparable performance to the equivalent RGB-D only system on easy trajectories.


Title: DeepFusion: Real-Time Dense 3D Reconstruction for Monocular SLAM using Single-View Depth and Gradient Predictions
Key Words: cameras  computerised instrumentation  graphics processing units  image fusion  image reconstruction  learning (artificial intelligence)  minimisation  neurocontrollers  photometry  probability  SLAM (robots)  stereo image processing  target tracking  depth cameras  CNN  DeepFusion  semidense multiview stereo algorithm  gradient predictions  monocular SLAM  single-view depth  keypoint-based maps  camera tracking  dense 3D reconstructions  convolutional neural network  sparse monocular simultaneous localisation and mapping systems  real-time dense 3D reconstruction system  photometric error minimization  GPU  probability  Image reconstruction  Uncertainty  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Real-time systems  Robot vision systems 
Abstract: While the keypoint-based maps created by sparse monocular Simultaneous Localisation and Mapping (SLAM) systems are useful for camera tracking, dense 3D reconstructions may be desired for many robotic tasks. Solutions involving depth cameras are limited in range and to indoor spaces, and dense reconstruction systems based on minimising the photometric error between frames are typically poorly constrained and suffer from scale ambiguity. To address these issues, we propose a 3D reconstruction system that leverages the output of a Convolutional Neural Network (CNN) to produce fully dense depth maps for keyframes that include metric scale. Our system, DeepFusion, is capable of producing real-time dense reconstructions on a GPU. It fuses the output of a semi-dense multiview stereo algorithm with the depth and gradient predictions of a CNN in a probabilistic fashion, using learned uncertainties produced by the network. While the network only needs to be run once per keyframe, we are able to optimise for the depth map with each new frame so as to constantly make use of new geometric constraints. Based on its performance on synthetic and real world datasets, we demonstrate that DeepFusion is capable of performing at least as well as other comparable systems.


Title: How Shall I Drive? Interaction Modeling and Motion Planning towards Empathetic and Socially-Graceful Driving
Key Words: game theory  intelligent robots  learning (artificial intelligence)  mobile robots  path planning  predictive control  remotely operated vehicles  AV  human driver  social awareness  passive-aggressive motions  interaction modeling  socially-graceful driving  autonomous vehicles  two-player game  model predictive control  social gracefulness  intent inference  motion planning  Planning  Games  Vehicles  Adaptation models  Inference algorithms  Estimation  Loss measurement 
Abstract: While intelligence of autonomous vehicles (AVs) has significantly advanced in recent years, accidents involving AVs suggest that these autonomous systems lack gracefulness in driving when interacting with human drivers. In the setting of a two-player game, we propose model predictive control based on social gracefulness, which is measured by the discrepancy between the actions taken by the AV and those that could have been taken in favor of the human driver. We define social awareness as the ability of an agent to infer such favorable actions based on knowledge about the other agent's intent, and further show that empathy, i.e., the ability to understand others' intent by simultaneously inferring others' understanding of the agent's self intent, is critical to successful intent inference. Lastly, through an intersection case, we show that the proposed gracefulness objective allows an AV to learn more sophisticated behavior, such as passive-aggressive motions that gently force the other agent to yield.


Title: Vibration Control for Manipulators on a Translationally Flexible Base
Key Words: damping  flexible manipulators  Lyapunov methods  manipulator dynamics  numerical analysis  springs (mechanical)  stability  vibration control  vibration control  manipulators  translationally flexible base  fundamental oscillatory system  mass spring system  control strategy couples  n-link manipulator  linear translational stiffness  conditional stability argument  base vibrations  input transformation  coordinate transformation  semidefinite Lyapunov functions  Vibrations  Manipulator dynamics  Task analysis  Robot kinematics  Damping 
Abstract: In this contribution the problem of vibration control is studied on the basis of a fundamental oscillatory system consisting of a mass spring system and an additional mass. The proposed control strategy couples the orbits of the two masses such that both masses stop, while simultaneously stabilizing the second mass to a desired equilibrium. Using a coordinate and input transformation, the control strategy is directly transferred to an n-link manipulator mounted on a base with linear translational stiffness. Using semidefinite Lyapunov functions and a conditional stability argument, it is shown that the proposed control strategy damps out base vibrations, while additionally achieving a desired configuration in the task-space. Finally, the proposed method is compared to a state-of-the-art approach using numerical simulations.


Title: Improving the Robustness of Visual-Inertial Extended Kalman Filtering
Key Words: drag  image filtering  inertial navigation  Kalman filters  Monte Carlo methods  nonlinear filters  observability  state estimation  observability  consistency problems  three-fold improvement  linear drag term  velocity dynamics  estimation accuracy  partial-update formulation  linearization errors  partially-observable states  sensor biases  normally unobservable position  heading states  visual-inertial state estimation problem  Monte Carlo simulation experiment  visual-inertial Kalman filters  visual-inertial extended Kalman filtering  visual-inertial navigation methods  global measurements  Cameras  Quaternions  Estimation  Kalman filters  Mathematical model  Robustness  Navigation 
Abstract: Visual-inertial navigation methods have been shown to be an effective, low-cost way to operate autonomously without GPS or other global measurements, however most filtering approaches to VI suffer from observability and consistency problems. To increase robustness of the state-of-the-art methods, we propose a three-fold improvement. First, we propose the addition of a linear drag term in the velocity dynamics which improves estimation accuracy. Second, we propose the use of a partial-update formulation which limits the effect of linearization errors in partially-observable states, such as sensor biases. Finally, we propose the use of a keyframe reset step to enforce observability and consistency of the normally unobservable position and heading states. While all of these concepts have been used independently in the past, our experiments demonstrate additional strength when they are used simultaneously in a visual-inertial state estimation problem. In this paper, we derive the proposed filter and use a Monte Carlo simulation experiment to analyze the response of visual-inertial Kalman filters with the above described additions. The results of this study show that the combination of all of these features significantly improves estimation accuracy and consistency.


Title: Towards Fully Dense Direct Filter-Based Monocular Visual-Inertial Odometry
Key Words: covariance matrices  distance measurement  gradient methods  image filtering  image texture  matrix inversion  mobile robots  robot vision  smoothing methods  low-textured areas  smooth gradients  complexity reduction methods  direct filter-based monocular visual-inertial odometry  direct filter-based visual-inertial odometry method  Cameras  Integrated circuits  Uncertainty  Complexity theory  Estimation  Covariance matrices  Jacobian matrices 
Abstract: We propose a fully dense direct filter-based visual-inertial odometry method estimating both pixel depth for all pixels and robot state simultaneously, having all uncertainties in the same state vector. Due to the fully dense method, our approach works even in low-textured areas with very low, smooth gradients (i.e. scenes where feature based or semi-dense approaches fail). Our algorithm performs in real-time on a CPU with a time complexity linearly dependent on the amount of pixels in the provided image. To achieve this, we propose complexity reduction methods for fast matrix inversion, exploiting specific structures of the covariance matrix. We provide both simulated and real-world results in low-textured areas with a smooth gradient.


Title: Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks
Key Words: cameras  image matching  image sequences  motion estimation  object detection  unsupervised learning  video signal processing  monocular depth  multiple masks  unsupervised learning method  depth estimation network  ego-motion estimation network  projection target imaging plane  fine masks  image pixel mismatch  repeated masking  KITTI dataset  low-quality uncalibrated bike video dataset  Image reconstruction  Estimation  Cameras  Unsupervised learning  Training  Simultaneous localization and mapping 
Abstract: A new unsupervised learning method of depth and ego-motion using multiple masks from monocular video is proposed in this paper. The depth estimation network and the ego-motion estimation network are trained according to the constraints of depth and ego-motion without truth values. The main contribution of our method is to carefully consider the occlusion of the pixels generated when the adjacent frames are projected to each other, and the blank problem generated in the projection target imaging plane. Two fine masks are designed to solve most of the image pixel mismatch caused by the movement of the camera. In addition, some relatively rare circumstances are considered, and repeated masking is proposed. To some extent, the method is to use a geometric relationship to filter the mismatched pixels for training, making unsupervised learning more efficient and accurate. The experiments on KITTI dataset show our method achieves good performance in terms of depth and ego-motion. The generalization capability of our method is demonstrated by training on the low-quality uncalibrated bike video dataset and evaluating on KITTI dataset, and the results are still good.


Title: Adaptive H∞ Controller for Precise Manoeuvring of a Space Robot
Key Words: adaptive control  aerospace robotics  assembling  control system synthesis  H∞ control  manipulators  mirrors  motion control  nonlinear control systems  position control  robust control  space vehicles  space robot  precise manoeuvring  controlled-floating mode  in-orbit telescope assembly  robotic arm  slow manoeuvres  precise manoeuvres  orbital assembly missions  robustness  optical mirrors  nonlinear H∞ controller  adaptive H∞ controller  Space vehicles  Aerospace electronics  Robot kinematics  Manipulators  Uncertainty  Orbits 
Abstract: A space robot working in a controlled-floating mode can be used for performing in-orbit telescope assembly through simultaneously controlling the motion of the spacecraft base and its robotic arm. Handling and assembling optical mirrors requires the space robot to achieve slow and precise manoeuvres regardless of the disturbances and errors in the trajectory. The robustness offered by the nonlinear H∞ controller, in the presence of environmental disturbances and parametric uncertainties, makes it a viable solution. However, using fixed tuning parameters for this controller does not always result in the desired performance as the arm's trajectory is not known a priori for orbital assembly missions. In this paper, a complete study on the impact of the different tuning parameters is performed and a new adaptive H∞ controller is developed based on bounded functions. The simulation results presented show that the proposed adaptive H∞ controller guarantees robustness and precise tracking using a minimal amount of forces and torques for assembly operations using a small space robot.


Title: Self-supervised Learning for Single View Depth and Surface Normal Estimation
Key Words: convolutional neural nets  natural scenes  stereo image processing  supervised learning  single view depth  surface normal estimation  self-supervised learning framework  surface normals  outdoor scenes  fronto-parallel planes  piece-wise smooth depth  surface orientation  natural scenes  piece-wise smooth normals  trained normal network  depth network  realistic smooth normal assumption  self-supervised depth prediction network  convolutional neural networks  depth-normal consistency  Training  Estimation  Geometry  Cameras  Sensors  Visual odometry  Neural networks 
Abstract: In this work we present a self-supervised learning framework to simultaneously train two Convolutional Neural Networks (CNNs) to predict depth and surface normals from a single image. In contrast to most existing frameworks which represent outdoor scenes as fronto-parallel planes at piece-wise smooth depth, we propose to predict depth with surface orientation while assuming that natural scenes have piece-wise smooth normals. We show that a simple depth-normal consistency as a soft-constraint on the predictions is sufficient and effective for training both these networks simultaneously. The trained normal network provides state-of-the-art predictions while the depth network, relying on much realistic smooth normal assumption, outperforms the traditional self-supervised depth prediction network by a large margin on the KITTI benchmark.


Title: Global Localization with Object-Level Semantics and Topology
Key Words: feature extraction  graph theory  image matching  image representation  pose estimation  stereo image processing  object-level representation  semantic object association  semantic-level point alignment  object-level semantics  appearance-based approach  3D dense semantics  semantic graph  vision-based global localization  topology  autonomous navigation  simultaneous localization and mapping  place recognition  6-DoF pose estimation  visual feature matching  Semantics  Three-dimensional displays  Topology  Simultaneous localization and mapping  Visualization  Cameras  Pose estimation 
Abstract: Global localization lies at the heart of autonomous navigation and Simultaneous Localization and Mapping (SLAM). The appearance-based approach has been successful, but still faces many open challenges in environments where visual conditions vary significantly over time. In this paper, we propose an integrated solution to leverage object-level dense semantics and spatial understanding of the environment for global localization. Our approach models an environment with 3D dense semantics, semantic graph and their topology. This object-level representation is then used for place recognition via semantic object association, followed by 6-DoF pose estimation by the semantic-level point alignment. Extensive experiments show that our approach can achieve robust global localization under extreme appearance changes. It is also capable of coping with other challenging scenarios, such as dynamic environments and incomplete query observations.


Title: Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation
Key Words: cameras  computer vision  feature extraction  image filtering  image matching  image representation  image sensors  image sequences  object detection  keypoint sequence  single query image  depth-filtered keypoint sequences  camera motion  varying scene appearance  single-view depth estimation  familiar visual place  extreme environmental appearance change  field-of-view vision  temporal-aware visual place recognition system  extreme appearance-change visual place recognition problem  sequence-to-single frame matching  depth-filtered keypoints  depth estimation pipeline  Visualization  Estimation  Feature extraction  Cameras  Indexes  Robots  Measurement 
Abstract: Visual place recognition (VPR) - the act of recognizing a familiar visual place - becomes difficult when there is extreme environmental appearance change or viewpoint change. Particularly challenging is the scenario where both phenomena occur simultaneously, such as when returning for the first time along a road at night that was previously traversed during the day in the opposite direction. While such problems can be solved with panoramic sensors, humans solve this problem regularly with limited field-of-view vision and without needing to constantly turn around. In this paper, we present a new depth- and temporal-aware visual place recognition system that solves the opposing viewpoint, extreme appearance-change visual place recognition problem. Our system performs sequence-to-single frame matching by extracting depth-filtered keypoints using a state-of-the-art depth estimation pipeline, constructing a keypoint sequence over multiple frames from the reference dataset, and comparing these keypoints to the keypoints extracted from a single query image. We evaluate the system on a challenging benchmark dataset and show that it consistently outperforms state-of-the-art techniques. We also develop a range of diagnostic simulation experiments that characterize the contribution of depth-filtered keypoint sequences with respect to key domain parameters including the degree of appearance change and camera motion.


Title: CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction
Key Words: feature extraction  motion estimation  probability  SLAM (robots)  SVO mapping results  frame rate camera motion estimation  map points  initialized map point  depth uncertainty  single-image depth prediction network  feature location  probabilistic mapping method  direct pixel correspondence  semidirect visual odometry  V-SLAM algorithms  visual simultaneous localization  Uncertainty  Cameras  Visual odometry  Feature extraction  Reliability  Estimation  Motion estimation 
Abstract: Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that improved SVO mapping results in increased robustness and camera tracking accuracy. The implementation of this work is available at https: //github.com/yan99033/CNN-SVO.


Title: A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation
Key Words: image motion analysis  image segmentation  mobile robots  pose estimation  robot vision  SLAM (robots)  segmentation algorithms  semantic segmentation  robotics  refined 3D pose information  vision-based tasks  mutual improvement  unified framework  instantaneous motion change handling  long-term changes  simultaneous localization and segmentation  Image segmentation  Task analysis  Robot sensing systems  Motion segmentation  Feature extraction  Three-dimensional displays 
Abstract: This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.


Title: MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM
Key Words: cameras  image colour analysis  image motion analysis  image segmentation  image sequences  object detection  object tracking  octrees  pose estimation  probability  robot vision  SLAM (robots)  video signal processing  robustly track  foreground object probabilities  object model  object-level dynamic volumetric map  instance segmentation part  octree-based object-level multiinstance dynamic SLAM  multiinstance dynamic RGB-D SLAM system  robust camera tracking  geometric motion properties  geometric motion information  object-oriented tracking method  camera pose estimation  semantic motion properties  frequency 2.0 Hz to 3.0 Hz  Cameras  Simultaneous localization and mapping  Tracking  Motion segmentation  Semantics  Dynamics  Measurement uncertainty 
Abstract: We propose a new multi-instance dynamic RGB-D SLAM system using an object-level octree-based volumetric representation. It can provide robust camera tracking in dynamic environments and at the same time, continuously estimate geometric, semantic, and motion properties for arbitrary objects in the scene. For each incoming frame, we perform instance segmentation to detect objects and refine mask boundaries using geometric and motion information. Meanwhile, we estimate the pose of each existing moving object using an object-oriented tracking method and robustly track the camera pose against the static scene. Based on the estimated camera pose and object poses, we associate segmented masks with existing models and incrementally fuse corresponding colour, depth, semantic, and foreground object probabilities into each object model. In contrast to existing approaches, our system is the first system to generate an object-level dynamic volumetric map from a single RGB-D camera, which can be used directly for robotic tasks. Our method can run at 2-3 Hz on a CPU, excluding the instance segmentation part. We demonstrate its effectiveness by quantitatively and qualitatively testing it on both synthetic and real-world sequences.


Title: Surfel-Based Dense RGB-D Reconstruction With Global And Local Consistency
Key Words: computer vision  image colour analysis  image reconstruction  optimisation  pose estimation  surfel-based dense RGB-D reconstruction  local consistency  high surface reconstruction accuracy  dense mapping  vision communities  robotics literature  RGB-D cameras  dense map  depth input  accurate local pose estimation  locally consistent model  pose tracking  offline computer vision methods  structure-from-motion  multiview stereo  batch optimization  global consistency  heavy computation loads  consistent reconstruction  offline SfM pipeline  strong global constraints  off-the-shelf SLAM systems  high local accuracy  factor graph optimization  accurate camera  dense reconstruction  dense SLAM systems  SfM-MVS pipelines  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Pose estimation  Image reconstruction  Geometry 
Abstract: Achieving high surface reconstruction accuracy in dense mapping has been a desirable target for both robotics and vision communities. In the robotics literature, simultaneous localization and mapping (SLAM) systems use RGB-D cameras to reconstruct a dense map of the environment. They leverage the depth input to provide accurate local pose estimation and a locally consistent model. However, drift in the pose tracking over time leads to misalignments and artifacts. On the other hand, offline computer vision methods, such as the pipeline that combines structure-from-motion (SfM) and multi-view stereo (MVS), estimate the camera poses by performing batch optimization. These methods achieve global consistency, but suffer from heavy computation loads. We propose a novel approach that integrates both methods to achieve locally and globally consistent reconstruction. First, we estimate poses of keyframes in the offline SfM pipeline to provide strong global constraints at relatively low cost. Afterwards, we compute odometry between frames driven by off-the-shelf SLAM systems with high local accuracy. We fuse the two pose estimations using factor graph optimization to generate accurate camera poses for dense reconstruction. Experiments on real-world and synthetic datasets demonstrate that our approach produces more accurate models comparing to existing dense SLAM systems, while achieving significant speedup with respect to state-of-the-art SfM-MVS pipelines.


Title: A-SLAM: Human in-the-loop Augmented SLAM
Key Words: augmented reality  human-robot interaction  mobile robots  navigation  path planning  real-time systems  SLAM (robots)  telerobotics  A-SLAM  map editing  navigation-forbidden areas  navigation goals  SLAM algorithm  occupancy grid maps  human in-the-loop augmented SLAM  real environment representation  Microsoft HoloLens  robot teleoperation  pose correction  map correction  AR interface  Simultaneous localization and mapping  Navigation  Collaboration  Three-dimensional displays  Task analysis 
Abstract: In this work, we are proposing an intuitive Augmented SLAM method (A-SLAM) that allows the user to interact, in real-time, with a robot running SLAM to correct for pose and map errors. We built an AR application that works on HoloLens and allows the operator to view the robot's map superposed on the physical environment and edit it. Through map editing, the operator can account for errors affecting real environment's representation by adding navigation-forbidden areas to the map in addition to the ability to correct errors affecting the localization. The proposed system allows the operator to edit the robot's pose (based on SLAM request) and can be extended to sending navigation goals to the robot, viewing the planned path to evaluate it before execution, and teleoperating the robot. The proposed solution could be applied on any 2D-based SLAM algorithm and can easily be extended to 3D SLAM techniques. We validated our system through experimentation on pose correction and map editing. Experiments demonstrated that through A-SLAM, SLAM runtime is cut to half, post-processing of maps is totally eliminated, and high quality occupancy grid maps could be achieved with minimal added computational and hardware costs.


Title: Pose Graph optimization for Unsupervised Monocular Visual Odometry
Key Words: graph theory  neural nets  optimisation  pose estimation  unsupervised learning  pose graph optimization  unsupervised monocular visual odometry  unsupervised learning  label-free leaning ability  drift correction technique  large-scale odometry estimation  loop closure detection  hybrid VO system  NeuralBundler  temporal loss  spatial photometric loss  multiview 6DoF constraints  cycle consistency loss  global pose graph  local loop 6DoF constraints  KITTI odometry dataset  unsupervised monocular VO estimation  monocular SLAM systems  Optimization  Visual odometry  Simultaneous localization and mapping  Cameras  Training  Neural networks  Estimation 
Abstract: Unsupervised Learning based monocular visual odometry (VO) has lately drawn significant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for large-scale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler, and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efficient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.


Title: Adding Cues to Binary Feature Descriptors for Visual Place Recognition
Key Words: feature extraction  image retrieval  binary feature descriptors  visual place recognition  multidimensional continuous cues  feature descriptor  binary string  continuous cue  binary descriptor types  Hamming distance  Search problems  Quantization (signal)  Visualization  Measurement  Simultaneous localization and mapping  Image retrieval 
Abstract: In this paper we propose an approach to embed multi-dimensional continuous cues in binary feature descriptors used for visual place recognition. The embedding is achieved by extending each feature descriptor with a binary string that encodes a cue and supports the Hamming distance metric. Augmenting the descriptors in such a way has the advantage of being transparent to the procedure used to compare them. We present a concrete application of our methodology, demonstrating the considered type of continuous cue. Additionally, we conducted a broad quantitative and comparative evaluation on that application, covering five benchmark datasets and several state-of-the-art image retrieval approaches in combination with various binary descriptor types.


Title: Recursive Bayesian Classification for Perception of Evolving Targets using a Gaussian Toroid Prediction Model
Key Words: Bayes methods  belief networks  feature extraction  Gaussian processes  image classification  real-time systems  recursive estimation  uncertainty handling  Gaussian toroid prediction model  probabilistic framework  recursive Bayesian estimation  perception-oriented context  recursive Bayesian classification scheme  high-dimensional belief spaces  evolving target classification  perception target evolution  RBC scheme  feature extraction  multiGaussian belief representation  real-time analysis  observational uncertainty  Predictive models  Bayes methods  Probabilistic logic  Probability density function  Estimation  Decision making  Mathematical model 
Abstract: This paper proposes a probabilistic framework for classification of evolving targets, leveraging the principles of recursive Bayesian estimation in a perception-oriented context. By implementing a Gaussian toroid prediction model of the perception target's evolution, the proposed recursive Bayesian classification (RBC) scheme provides probabilistically robust classification. Appropriate features are extracted from the target, which is then probabilistically represented in a belief space. This approach is capable of handling high-dimensional belief spaces, while simultaneously allowing for multi-Gaussian representation of belief without computational complexity that hinders real-time analysis. The proposed technique is validated over several parameter values by thousands of simulated experiments, where it is shown to outperform naıve classification when high observational uncertainty is present.


Title: A Variational Observation Model of 3D Object for Probabilistic Semantic SLAM
Key Words: Bayes methods  cameras  feature extraction  image colour analysis  inference mechanisms  object detection  pose estimation  probability  SLAM (robots)  variational techniques  probabilistic observation model  Bayesian inference  viewpoint-independent loop closure  variational observation model  Bayesian object observation model  3D object detection  probabilistic semantic SLAM  single view projection  RGB monocamera  object-oriented feature extraction  3D mapping  volumetric 3D object shape information  variational likelihood estimation  pose estimation  feature estimation  loop detector  Shape  Simultaneous localization and mapping  Three-dimensional displays  Object oriented modeling  Solid modeling  Semantics 
Abstract: We present a Bayesian object observation model for complete probabilistic semantic SLAM. Recent studies on object detection and feature extraction have become important for scene understanding and 3D mapping. However, 3D shape of the object is too complex to formulate the probabilistic observation model; therefore, performing the Bayesian inference of the object-oriented features as well as their pose is less considered. Besides, when the robot equipped with an RGB mono camera only observes the projected single view of an object, a significant amount of the 3D shape information is abandoned. Due to these limitations, semantic SLAM and viewpoint-independent loop closure using volumetric 3D object shape is challenging. In order to enable the complete formulation of probabilistic semantic SLAM, we approximate the observation model of a 3D object with a tractable distribution. We also estimate the variational likelihood from the 2D image of the object to exploit its observed single view. In order to evaluate the proposed method, we perform pose and feature estimation, and demonstrate that the automatic loop closure works seamlessly without additional loop detector in various environments.


Title: Localization with Sliding Window Factor Graphs on Third-Party Maps for Automated Driving
Key Words: optimisation  particle filtering (numerical methods)  pose estimation  position measurement  window factor graphs  third-party maps  robotic applications  window optimization  odometry measurements  fast vehicle localization  accurate vehicle localization  estimation problem  sliding window formulation  factor graph  landmark detections  automated car  automated driving applications  Microsoft Windows  Optimization  Simultaneous localization and mapping  Global navigation satellite system  Laser radar  Measurement uncertainty 
Abstract: Localizing a vehicle in a map is essential for automated driving and various other robotic applications. This paper addresses the problem of vehicle localization in urban environments. Our approach performs a graph-based sliding window optimization over a set of recent landmark and odometry measurements for fast and accurate vehicle localization on third-party maps. Our work incorporates landmark priors from third-party maps into the estimation problem and shows how to exploit the sliding window formulation for revising data associations. We describe how to construct our factor graph and derive its necessary factors to model the information from the map as a prior over the landmark detections. We implemented our approach on an automated car and thoroughly tested it on real-world data. The experiments suggest that the approach provides highly accurate pose estimates, is fast enough for automated driving applications, and outperforms localization using particle filters.


Title: Beyond Point Clouds: Fisher Information Field for Active Visual Localization
Key Words: mobile robots  path planning  robot vision  point clouds  Fisher information field  active visual localization  mobile robots  perception requirement  planning stage  localization information  perception-aware planning  3D landmarks  sensor visibility  Three-dimensional displays  Planning  Visualization  Cameras  Simultaneous localization and mapping 
Abstract: For mobile robots to localize robustly, actively considering the perception requirement at the planning stage is essential. In this paper, we propose a novel representation for active visual localization. By formulating the Fisher information and sensor visibility carefully, we are able to summarize the localization information into a discrete grid, namely the Fisher information field. The information for arbitrary poses can then be computed from the field in constant time, without the need of costly iterating all the 3D landmarks. Experimental results on simulated and real-world data show the great potential of our method in efficient active localization and perception-aware planning. To benefit related research, we release our implementation of the information field to the public.


Title: Tightly-Coupled Aided Inertial Navigation with Point and Plane Features
Key Words: feature extraction  image fusion  image sensors  inertial navigation  mobile robots  Monte Carlo methods  object tracking  SLAM (robots)  planar point features  nonplanar point features  point-on-plane constraints  effective plane feature initialization algorithm  depth sensor  general sensor fusion framework  point feature tracking  plane extraction  geometrical structures  closest point  plane parameterization  Monte-Carlo simulations  visual sensor  tightly-coupled aided inertial navigation system  feature-based simultaneous localization and mapping  Feature extraction  Cameras  Calibration  Laser radar  Jacobian matrices  Simultaneous localization and mapping  Estimation 
Abstract: This paper presents a tightly-coupled aided inertial navigation system (INS) with point and plane features, a general sensor fusion framework applicable to any visual and depth sensor (e.g., RGBD, LiDAR) configuration, in which the camera is used for point feature tracking and depth sensor for plane extraction. The proposed system exploits geometrical structures (planes) of the environments and adopts the closest point (CP) for plane parameterization. Moreover, we distinguish planar point features from non-planar point features in order to enforce point-on-plane constraints which are used in our state estimator, thus further exploiting structural information from the environment. We also introduce a simple but effective plane feature initialization algorithm for feature-based simultaneous localization and mapping (SLAM). In addition, we perform online spatial calibration between the IMU and the depth sensor as it is difficult to obtain this critical calibration parameter in high precision. Both Monte-Carlo simulations and real-world experiments are performed to validate the proposed approach.


Title: Admittance Control for Human-Robot Interaction Using an Industrial Robot Equipped with a F/T Sensor
Key Words: end effectors  force sensors  human-robot interaction  industrial manipulators  manipulator kinematics  closed control architecture  robot dynamics  low-level joint controllers  kinematic information  admittance control law  whole-body collision detection  KUKA KR5 Sixx R650 robot  industrial robot  physical human-robot interaction  torque sensors  pHRI strategy  end-effector  force/torque sensor  ATI F/T sensor  Robot sensing systems  Collision avoidance  Service robots  Collaboration  End effectors  Current measurement 
Abstract: We present an approach to safe physical Human-Robot Interaction (pHRI) for industrial robots, including collision detection, distinguishing accidental from intentional contacts, and achieving collaborative tasks. Typical industrial robots have a closed control architecture that accepts only velocity/position reference inputs, there are no joint torque sensors, and little or no information is available to the user on robot dynamics and on low-level joint controllers. Nonetheless, taking also advantage of the presence of a Force/Torque (F/T) sensor at the end-effector, a safe pHRI strategy based on kinematic information, on measurements from joint encoders and motor currents, and on end-effector forces/torques can be realized. An admittance control law has been implemented for collaboration in manual guidance mode, with whole-body collision detection in place both when the robot is in autonomous operation and when is simultaneously collaborating with a human. Several pHRI experiments validate the approach on a KUKA KR5 Sixx R650 robot equipped with an ATI F/T sensor.


Title: Workspace CPG with Body Pose Control for Stable, Directed Vision during Omnidirectional Locomotion
Key Words: legged locomotion  motion control  path planning  pose estimation  robot vision  vision system  DOF  central pattern generator  degree-of-freedom  gaze orientation  body velocity  path planning  legged robot  omnidirectional locomotion  hexapod robot  CPG framework  body pose control  robot body  Foot  Machine vision  Legged locomotion  Trajectory  Transforms  Phase change materials 
Abstract: In this paper, we focus on the problem of directing the gaze of a vision system mounted to the body of a high-degree-of-freedom (DOF) legged robot for active perception deployments. In particular, we consider the case where the vision system is rigidly attached to the robot's body (i.e., without any additional DOF between the vision system and robot body) and show how the supernumerary DOFs of the robot can be leveraged to allow independent locomotion and gaze control. Specifically, we augment a workspace central pattern generator (CPG) with omnidirectional capabilities by coupling it with a body pose control mechanism. We leverage the smoothing nature of the CPG framework to allow online adaptation of relevant locomotion parameters, and obtain a stable mid-level controller that translates desired gaze orientation and body velocity directly into joint angles. We validate our approach on an 18-DOF hexapod robot, in a series of indoor and outdoor trials, where the robot inspects an environmental feature or follows a pre-planned path relative to a visually-tracked landmark, demonstrating simultaneous locomotion and directed vision.


Title: 3D Keypoint Repeatability for Heterogeneous Multi-Robot SLAM
Key Words: feature extraction  mobile robots  multi-robot systems  robot vision  SLAM (robots)  point cloud registration  loop closure  heterogenous multirobot SLAM applications  NARF detector  3D keypoint repeatability  heterogeneous multirobot SLAM  multirobot SLAM scenario  sensor measurement point clouds  point density  3D keypoint detectors  multirobot SLAM system  KPQ-SI  relative repeatability  Three-dimensional displays  Detectors  Simultaneous localization and mapping  Cameras  Laser radar 
Abstract: For robots with different types of sensors, loop closure in a multi-robot SLAM scenario requires keypoints that can be matched between sensor measurement point clouds with different properties such as point density and noise. In this paper, we evaluate the performance of several 3D keypoint detectors (Harris3D, ISS, KPQ, KPQ-SI, and NARF) for repeatability between scans from different sensors towards building a heterogeneous multi-robot SLAM system. We find that KPQ-SI and NARF have the best relative repeatability, with KPQ-SI finding more keypoints overall and a higher number of repeatable keypoints, at the cost of significantly worse computational performance. In scans of the same area from different poses, both detectors find enough keypoints for point cloud registration and loop closure. For heterogenous multirobot SLAM applications with computational or bandwidth restrictions, the NARF detector consistently finds repeatable keypoints while also allowing for real-time performance.


Title: SLAMBench 3.0: Systematic Automated Reproducible Evaluation of SLAM Systems for Robot Vision Challenges and Scene Understanding
Key Words: control engineering computing  convolutional neural nets  image reconstruction  natural scenes  robot vision  SLAM (robots)  scene understanding  nonrigid environments  dynamic SLAM  SLAMBench 3  evaluation infrastructure  systematic automated reproducible evaluation  robot vision  visual SLAM  SLAM research area  visulation aids  visulation metrics  convolutional neural networks  dynamicfusion  Simultaneous localization and mapping  Semantics  Three-dimensional displays  Measurement  Benchmark testing  Heuristic algorithms  C++ languages 
Abstract: As the SLAM research area matures and the number of SLAM systems available increases, the need for frameworks that can objectively evaluate them against prior work grows. This new version of SLAMBench moves beyond traditional visual SLAM, and provides new support for scene understanding and non-rigid environments (dynamic SLAM). More concretely for dynamic SLAM, SLAMBench 3.0 includes the first publicly available implementation of DynamicFusion, along with an evaluation infrastructure. In addition, we include two SLAM systems (one dense, one sparse) augmented with convolutional neural networks for scene understanding, together with datasets and appropriate metrics. Through a series of use-cases, we demonstrate the newly incorporated algorithms, visulation aids and metrics (6 new metrics, 4 new datasets and 5 new algorithms).


Title: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation
Key Words: motion estimation  pose estimation  SLAM (robots)  photometric loss  self-supervised ego-motion estimation  accurate relative pose  SLAM  self-supervised learning framework  image depth  photometric error  systematic error  realistic scenes  geometric loss  matching loss  self-supervised framework  unsupervised egomotion estimation methods  Simultaneous localization and mapping  Estimation  Geometry  Cameras  Motion estimation  Visualization  Visual odometry 
Abstract: Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised egomotion estimation methods by a large margin. The code and data are available at https://github.com/hlzz/DeepMatchVO.


Title: Activity recognition in manufacturing: The roles of motion capture and sEMG+inertial wearables in detecting fine vs. gross motion
Key Words: assembling  feature extraction  human-robot interaction  image classification  image motion analysis  industrial robots  manufacturing systems  production engineering computing  sensor fusion  wearable computers  unimodal sensor data  UCSD-MIT Human Motion dataset  Vicon motion capture system  gross motion recognition  sensor modalities  sEMG  human activity recognition  inertial wearables  fine motion detection  manufacturing  safety-critical environments  assembly tasks  HAR algorithms  Robot sensing systems  Task analysis  Wearable sensors  Grasping  Automotive engineering  Tracking 
Abstract: In safety-critical environments, robots need to reliably recognize human activity to be effective and trust-worthy partners. Since most human activity recognition (HAR) approaches rely on unimodal sensor data (e.g. motion capture or wearable sensors), it is unclear how the relationship between the sensor modality and motion granularity (e.g. gross or fine) of the activities impacts classification accuracy. To our knowledge, we are the first to investigate the efficacy of using motion capture as compared to wearable sensor data for recognizing human motion in manufacturing settings. We introduce the UCSD-MIT Human Motion dataset, composed of two assembly tasks that entail either gross or fine-grained motion. For both tasks, we compared the accuracy of a Vicon motion capture system to a Myo armband using three widely used HAR algorithms. We found that motion capture yielded higher accuracy than the wearable sensor for gross motion recognition (up to 36.95%), while the wearable sensor yielded higher accuracy for fine-grained motion (up to 28.06%). These results suggest that these sensor modalities are complementary, and that robots may benefit from systems that utilize multiple modalities to simultaneously, but independently, detect gross and fine-grained motion. Our findings will help guide researchers in numerous fields of robotics including learning from demonstration and grasping to effectively choose sensor modalities that are most suitable for their applications.


Title: Build your own hybrid thermal/EO camera for autonomous vehicle
Key Words: cameras  image colour analysis  image motion analysis  image registration  image resolution  image sensors  image sequences  object detection  remotely operated vehicles  single hybrid camera  hybrid camera array  disparity images  spatial-alignment  autonomous vehicle  thermal RGB frames  thermal EO cameras  pixel-wise spatial registration  visible-light camera  hybrid thermal/EO camera  RGB frames  constant homography warping  image modalities  Cameras  Layout  Technological innovation  Tuners  Optical sensors  Autonomous vehicles 
Abstract: In this work, we propose a novel paradigm to design a hybrid thermal/EO (Electro-Optical or visible-light) camera, whose thermal and RGB frames are pixel-wisely aligned and temporally synchronized. Compared with the existing schemes, we innovate in three ways in order to make it more compact in dimension, and thus more practical and extendable for real-world applications. The first is a redesign of the structure layout of the thermal and EO cameras. The second is on obtaining a pixel-wise spatial registration of the thermal and RGB frames by a coarse mechanical adjustment and a fine alignment through a constant homography warping. The third innovation is on extending one single hybrid camera to a hybrid camera array, through which we can obtain wide-view spatially aligned thermal, RGB and disparity images simultaneously. The experimental results show that the average error of spatial-alignment of two image modalities can be less than one pixel.


Title: UWB/LiDAR Fusion For Cooperative Range-Only SLAM
Key Words: distance measurement  laser ranging  mobile robots  optical radar  SLAM (robots)  ultra wideband radar  wireless sensor networks  cooperative sensor network  2D LiDAR sensor  UWB-LiDAR fusion  UWB beacon nodes  peer-to-peer ranges  nearby objects-obstacles  surrounding environment  drift-free SLAM  mobile robot  2D laser rangefinder  ultra-wideband node  cooperative range-only SLAM  LiDAR-based SLAM algorithm  UWB ranging measurements  UWB-only localization accuracy  LiDAR mapping  laser scanning information  Laser radar  Peer-to-peer computing  Distance measurement  Simultaneous localization and mapping  Two dimensional displays 
Abstract: We equip an ultra-wideband (UWB) node and a 2D LiDAR sensor a.k.a. 2D laser rangefinder on a mobile robot, and place UWB beacon nodes at unknown locations in an unknown environment. All UWB nodes can do ranging with each other thus forming a cooperative sensor network. We propose to fuse the peer-to-peer ranges measured between UWB nodes and laser scanning information, i.e., range measured between robot and nearby objects/obstacles, for simultaneous localization of the robot, all UWB beacons and LiDAR mapping. The fusion is inspired by two facts: 1) LiDAR may improve UWB-only localization accuracy as it gives a more precise and comprehensive picture of the surrounding environment; 2) on the other hand, UWB ranging measurements may remove the error accumulated in the LiDAR-based SLAM algorithm. Our experiments demonstrate that UWB/LiDAR fusion enables drift-free SLAM in real-time based on ranging measurements only.


Title: Vision-Based Dynamic Control of Car-Like Mobile Robots
Key Words: automobiles  Lyapunov methods  mobile robots  robot vision  stability  steering systems  velocity control  car-like mobile robots  CLMR  steering control system  slipping effects  velocity estimation error  speed tracking error  vision-based dynamic control  visual algorithm  skidding effects  speed control system  Lyapunov method  stability  electric autonomous tractor  Aerodynamics  Mobile robots  Velocity measurement  Estimation  Heuristic algorithms 
Abstract: Most existing controllers for Car-Like Mobile Robots (CLMR) are designed to handle dynamic effects by decoupling speed and steering controls, also assume that full states are accessible, which are unrealistic for real-world applications. This paper presents a combined speed and steering control system for CLMR. To provide the essential state for the controller, a newly developed visual algorithm is adopted for estimating the high-update rate longitudinal and lateral velocities of the robot which cannot be accurately measured by wheel encoders due to the skidding and slipping effects. The stability of the proposed system can be guaranteed by Lyapunov method since the velocity estimation error, the speed tracking error and the lateral deviation converging to zero simultaneously. Real-world experiments are conducted on an electric autonomous tractor with online estimation to demonstrate the feasibility of the approach.


Title: Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning
Key Words: decision making  interactive systems  learning (artificial intelligence)  multi-agent systems  neural nets  probability  hyperparameter values  adversarial learning  interaction-aware multiagent tracking  probabilistic behavior prediction  motion planning  intelligent systems  multiple interactive agents  distribution learning  decision making  generative adversarial network  Generators  Gallium nitride  Predictive models  Training  Generative adversarial networks  State estimation  Optimization 
Abstract: In order to enable high-quality decision making and motion planning of intelligent systems such as robotics and autonomous vehicles, accurate probabilistic predictions for surrounding interactive objects is a crucial prerequisite. Although many research studies have been devoted to making predictions on a single entity, it remains an open challenge to forecast future behaviors for multiple interactive agents simultaneously. In this work, we take advantage of the Generative Adversarial Network (GAN) due to its capability of distribution learning and propose a generic multi-agent probabilistic prediction and tracking framework which takes the interactions among multiple entities into account, in which all the entities are treated as a whole. However, since GAN is very hard to train, we make an empirical research and present the relationship between training performance and hyperparameter values with a numerical case study. The results imply that the proposed model can capture both the mean, variance and multi-modalities of the groundtruth distribution. Moreover, we apply the proposed approach to a real-world task of vehicle behavior prediction to demonstrate its effectiveness and accuracy. The results illustrate that the proposed model trained by adversarial learning can achieve a better prediction performance than other state-of-the-art models trained by traditional supervised learning which maximizes the data likelihood. The well-trained model can also be utilized as an implicit proposal distribution for particle filtered based Bayesian state estimation.


Title: Characterizing Visual Localization and Mapping Datasets
Key Words: motion estimation  rendering (computer graphics)  SLAM (robots)  Wasserstein distance  motion estimation algorithm  robotics SLAM benchmarking  visual localization  mapping algorithms  real-world trajectories  high-quality scenes  synthetic datasets  dense map  key SLAM applications  ground robotics  mapping datasets  motion estimation algorithms  computer vision  Simultaneous localization and mapping  Trajectory  Time measurement  Visualization  Benchmark testing 
Abstract: Benchmarking mapping and motion estimation algorithms is established practice in robotics and computer vision. As the diversity of datasets increases, in terms of the trajectories, models, and scenes, it becomes a challenge to select datasets for a given benchmarking purpose. Inspired by the Wasserstein distance, this paper addresses this concern by developing novel metrics to evaluate trajectories and the environments without relying on any SLAM or motion estimation algorithm. The metrics, which so far have been missing in the research community, can be applied to the plethora of datasets that exist. Additionally, to improve the robotics SLAM benchmarking, the paper presents a new dataset for visual localization and mapping algorithms. A broad range of real-world trajectories is used in very high-quality scenes and a rendering framework to create a set of synthetic datasets with ground-truth trajectory and dense map which are representative of key SLAM applications such as virtual reality (VR), micro aerial vehicle (MAV) flight, and ground robotics.


Title: Skill Acquisition via Automated Multi-Coordinate Cost Balancing
Key Words: convex programming  intelligent robots  learning (artificial intelligence)  learning framework  MCCB  point-to-point movement skills  multiple differential coordinates  local geometric properties  convex optimization problem  multicoordinate cost function  complex skill datasets  skill acquisition  automated multicoordinate cost balancing  Trajectory  Laplace equations  Cost function  Task analysis  Encoding  Robots 
Abstract: We propose a learning framework, named Multi-Coordinate Cost Balancing (MCCB), to address the problem of acquiring point-to-point movement skills from demonstrations. MCCB encodes demonstrations simultaneously in multiple differential coordinates that specify local geometric properties. MCCB generates reproductions by solving a convex optimization problem with a multi-coordinate cost function and linear constraints on the reproductions, such as initial, target, and via points. Further, since the relative importance of each coordinate system in the cost function might be unknown for a given skill, MCCB learns optimal weighting factors that balance the cost function. We demonstrate the effectiveness of MCCB via detailed experiments conducted on one handwriting dataset and three complex skill datasets.


Title: Active Multi-Contact Continuous Tactile Exploration with Gaussian Process Differential Entropy
Key Words: end effectors  entropy  Gaussian processes  mobile robots  path planning  tactile sensors  touch (physiological)  active multicontact continuous tactile exploration  Gaussian process differential entropy  active tactile exploration framework  exploration strategy  information theoretic context  nonmyopic multistep planning  end-effectors  tactile stimuli  compliant controller framework  tactile exploration approach  nonconvex objects  Gaussian process implicit surface model  sliding based tactile exploration  End effectors  Surface treatment  Robot sensing systems  Shape  Gaussian processes  Entropy 
Abstract: In the present work, we propose an active tactile exploration framework to obtain a surface model of an unknown object utilizing multiple contacts simultaneously. To incorporate these multiple contacts, the exploration strategy is based on the differential entropy of the underlying Gaussian process implicit surface model, which formalizes the exploration with multiple contacts within an information theoretic context and additionally allows for nonmyopic multi-step planning. In contrast to many previous approaches, the robot continuously slides along the surface with its end-effectors to gather the tactile stimuli, instead of touching it at discrete locations. This is realized by closely integrating the surface model into the compliant controller framework. Furthermore, we extend our recently proposed sliding based tactile exploration approach to handle non-convex objects. In the experiments, it is shown that multiple contacts simultaneously leads to a more efficient exploration of complex, non-convex objects, not only in terms of time, but also with respect to the total moved distance of all end-effectors. Finally, we demonstrate our methodology with a real PR2 robot that explores an object with both of its arms.


Title: UWStereoNet: Unsupervised Learning for Depth Estimation and Color Correction of Underwater Stereo Imagery
Key Words: cameras  computerised instrumentation  feature extraction  geophysical image processing  image colour analysis  image matching  image resolution  image restoration  light propagation  neural nets  oceanographic techniques  spatial variables measurement  stereo image processing  unsupervised learning  visual perception  unsupervised learning  stereo cameras  navigation  underwater robotic systems  constrained camera geometry  feature detection  underwater light propagation lead  deep learning  underwater image restoration  unsupervised deep neural network  input raw color underwater stereo imagery  color corrected imagery  underwater image formation  image processing techniques  depth estimation  stereo vision algorithms  disparity estimation  DNN  Image color analysis  Estimation  Image restoration  Cameras  Attenuation  Stereo vision  Deep learning 
Abstract: Stereo cameras are widely used for sensing and navigation of underwater robotic systems. They can provide high resolution color views of a scene; the constrained camera geometry enables metrically accurate depth estimation; they are also relatively cost-effective. Traditional stereo vision algorithms rely on feature detection and matching to enable triangulation of points for estimating disparity. However, for underwater applications, the effects of underwater light propagation lead to image degradation, reducing image quality and contrast. This makes it especially challenging to detect and match features, especially from varying viewpoints. Recently, deep learning has shown success in end-to-end learning of dense disparity maps from stereo images. Still, many state-of-the-art methods are supervised and require ground truth depth or disparity, which is challenging to gather in subsea environments. Simultaneously, deep learning has also been applied to the problem of underwater image restoration. Again, it is difficult or impossible to gather real ground truth data for this problem. In this work, we present an unsupervised deep neural network (DNN) that takes input raw color underwater stereo imagery and outputs dense depth maps and color corrected imagery of underwater scenes. We leverage a model of the process of underwater image formation, image processing techniques, as well as the geometric constraints inherent to the stereo vision problem to develop a modular network that outperforms existing methods.


Title: WISDOM: WIreless Sensing-assisted Distributed Online Mapping
Key Words: least mean squares methods  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  WISDOM  spatial sensing  robotics  augmented reality  urban spaces  wireless access points  coarse orientation  average Root Mean Square mapping error  wireless sensing-assisted distributed online mapping  robot swarm  custom ICP algorithm  absolute trajectory error  average root mean square mapping error  size 0.2 m  size 1.3 m  Three-dimensional displays  Simultaneous localization and mapping  Robot kinematics  Visualization  Merging 
Abstract: Spatial sensing is a fundamental requirement for applications in robotics and augmented reality. In urban spaces such as malls, airports, apartments, and others, it is quite challenging for a single robot to map the whole environment. So, we employ a swarm of robots to perform the mapping. One challenge with this approach is merging sub-maps built by each robot. In this work, we use wireless access points, which are ubiquitous in most urban spaces, to provide us with coarse orientation between sub-maps, and use a custom ICP algorithm to refine this orientation to merge them. We demonstrate our approach with maps from a building on campus and evaluate it using two metrics. Our results show that, in the building we studied, we can achieve an average Absolute Trajectory error of 0.2m in comparison to a map created by a single robot and average Root Mean Square mapping error of 1.3m from ground truth landmark locations.


Title: Proximity Human-Robot Interaction Using Pointing Gestures and a Wrist-mounted IMU
Key Words: gesture recognition  human-robot interaction  mobile robots  pointed locations  slow ground robots  proximity human-robot interaction  pointing gestures  wrist-mounted IMU  co-located humans  mobile robots  moving robot  Robot kinematics  Robot sensing systems  Real-time systems  Drones  Trajectory  Mobile robots 
Abstract: We present a system for interaction between co-located humans and mobile robots, which uses pointing gestures sensed by a wrist-mounted IMU. The operator begins by pointing, for a short time, at a moving robot. The system thus simultaneously determines: that the operator wants to interact; the robot they want to interact with; and the relative pose among the two. Then, the system can reconstruct pointed locations in the robot's own reference frame, and provide real-time feedback about them so that the user can adapt to misalignments. We discuss the challenges to be solved to implement such a system and propose practical solutions, including variants for fast flying robots and slow ground robots. We report different experiments with real robots and untrained users, validating the individual components and the system as a whole.


Title: An Extrinsic Calibration Tool for Radar, Camera and Lidar
Key Words: calibration  cameras  Gaussian noise  optical radar  pose estimation  probability  radar imaging  lidar  joint extrinsic calibration  sensing modalities  calibration target design  calibration procedure  multimodal measurements  optimization criterion  error terms  sensor pairs  calibration boards  calibration performance  camera errors  radar errors  extrinsic calibration tool  novel open-source tool  loop closure constraints  pose estimation  zero mean Gaussian noise  probabilistic model  Calibration  Cameras  Laser radar  Tools  Robot vision systems 
Abstract: We present a novel open-source tool for extrinsic calibration of radar, camera and lidar. Unlike currently available offerings, our tool facilitates joint extrinsic calibration of all three sensing modalities on multiple measurements. Furthermore, our calibration target design extends existing work to obtain simultaneous measurements for all these modalities. We study how various factors of the calibration procedure affect the outcome on real multi-modal measurements of the target. Three different configurations of the optimization criterion are considered, namely using error terms for a minimal amount of sensor pairs, or using terms for all sensor pairs with additional loop closure constraints, or by adding terms for structure estimation in a probabilistic model. The experiments further evaluate how the number of calibration boards affect calibration performance, and robustness against different levels of zero mean Gaussian noise. Our results show that all configurations achieve good results for lidar to camera errors and that fully connected pose estimation shows the best performance for lidar to radar errors when more than five board locations are used.


Title: Low-latency Visual SLAM with Appearance-Enhanced Local Map Building
Key Words: file organisation  image enhancement  image fusion  mobile robots  pose estimation  robot vision  SLAM (robots)  appearance-enhanced local map building  local map module  local map contents  co-visibility local map building  compact local map  downstream data association  mapped features  local map size  appearance-based local map building method  low-latency visual SLAM  pose estimation  multi-index hashing  online hash table selection algorithm  MIH  VO-VSLAM mean performance  Three-dimensional displays  Buildings  Optimization  Feature extraction  Indexing  Simultaneous localization and mapping 
Abstract: A local map module is often implemented in modern VO/VSLAM systems to improve data association and pose estimation. Conventionally, the local map contents are determined by co-visibility. While co-visibility is cheap to establish, it utilizes the relatively-weak temporal prior (i.e. seen before, likely to be seen now), therefore admitting more features into the local map than necessary. This paper describes an enhancement to co-visibility local map building by incorporating a strong appearance prior, which leads to a more compact local map and latency reduction in downstream data association. The appearance prior collected from the current image influences the local map contents: only the map features visually similar to the current measurements are potentially useful for data association. To that end, mapped features are indexed and queried with Multi-index Hashing (MIH). An online hash table selection algorithm is developed to further reduce the query overhead of MIH and the local map size. The proposed appearance-based local map building method is integrated into a state-of-the-art VO/VSLAM system. When evaluated on two public benchmarks, the size of the local map, as well as the latency of real-time pose tracking in VO/VSLAM are significantly reduced. Meanwhile, the VO/VSLAM mean performance is preserved or improves.


Title: A Depth Camera-Based Soft Fingertip Device for Contact Region Estimation and Perception-Action Coupling
Key Words: cameras  deformation  estimation theory  robots  tactile sensors  depth camera-based soft fingertip device  contact region estimation  perception-action coupling  robotic applications  unconstrained environments  dynamic environments  soft robotic technologies  soft tactile sensor design  human fingertip  perception mechanism  compliance-modulating capabilities  estimation sensitivity  internal fluid states  force-deformation characteristics  Robot sensing systems  Cameras  Three-dimensional displays  Force  Estimation  Image reconstruction 
Abstract: As the demand for robotic applications in unconstrained and dynamic environments rises, so does the benefit of advancing the state of the art in soft robotic technologies. However, the complex capabilities of soft robots elicited by their high-dimensional, non-linear characteristics simultaneously yield difficult challenges in control and sensing. Moreover, embedding tactile sensing capabilities in soft materials is often expensive and difficult to fabricate. In recent years, however, the invention of small-scale depth-sensing cameras introduced a promising channel for soft tactile sensor design. In this work, we propose a novel soft device inspired by the human fingertip that not only utilizes a small depth camera as the perception mechanism, but also possesses compliance-modulating capabilities. We demonstrate its ability to accurately estimate contact regions upon interaction with an external obstacle, and show that the estimation sensitivity can be modulated via internal fluid states. In addition, we determine an empirical model of the device's force-deformation characteristics under simplifying assumptions, and validate its performance with real-time force matching control experiments.


Title: Early Failure Detection of Deep End-to-End Control Policy by Reinforcement Learning
Key Words: belief networks  control engineering computing  convolutional neural nets  learning (artificial intelligence)  learning systems  observability  predictive control  learned control policies  reinforcement learning  end-to-end imitation  predictive uncertainty  model predictive controller  fully-observable vision-based partially-observable systems  deep convolutional Bayesian neural networks  deep end-to-end control policy  Bayesian networks  mean value  corrective action  partial state observability  Uncertainty  Bayes methods  Task analysis  Neural networks  Safety  Training  Autonomous vehicles 
Abstract: We propose the use of Bayesian networks, which provide both a mean value and an uncertainty estimate as output, to enhance the safety of learned control policies under circumstances in which a test-time input differs significantly from the training set. Our algorithm combines reinforcement learning and end-to-end imitation learning to simultaneously learn a control policy as well as a threshold over the predictive uncertainty of the learned model, with no hand-tuning required. Corrective action, such as a return of control to the model predictive controller or human expert, is taken before the failure of tasks, when the uncertainty threshold is exceeded. We validate our method on fully-observable and vision-based partially-observable systems using cart-pole and autonomous driving simulations using deep convolutional Bayesian neural networks. We demonstrate that our method is robust to uncertainty resulting from varying system dynamics as well as from partial state observability.


Title: A Constraint Programming Approach to Simultaneous Task Allocation and Motion Scheduling for Industrial Dual-Arm Manipulation Tasks
Key Words: constraint handling  industrial manipulators  motion control  optimisation  robot programming  robotic assembly  scheduling  robotic platforms  constraint programming approach  simultaneous task allocation  motion scheduling  industrial dual-arm manipulation tasks  dual-arm robots  industrial manipulation  assembly tasks  robot motion models  constraint optimization problems  makespan-optimized robot programs  industrial workplaces  robot-independent task model  lightweight dual-arm robots  ordered visiting constraint  ordering constraints  Task analysis  Planning  Manipulators  Robot kinematics  Job shop scheduling  Service robots 
Abstract: Modern lightweight dual-arm robots bring the physical capabilities to quickly take over tasks at typical industrial workplaces designed for workers. Low setup times - including the instructing/specifying of new tasks - are crucial to stay competitive. We propose a constraint programming approach to simultaneous task allocation and motion scheduling for such industrial manipulation and assembly tasks. Our approach covers the robot as well as connected machines. The key concept are Ordered Visiting Constraints, a descriptive and extensible model to specify such tasks with their spatiotemporal requirements and combinatorial or ordering constraints. Our solver integrates such task models and robot motion models into constraint optimization problems and solves them efficiently using various heuristics to produce makespan-optimized robot programs. For large manipulation tasks with 200 objects, our solver implemented using Google's Operations Research tools requires less than a minute to compute usable plans. The proposed task model is robot-independent and can easily be deployed to other robotic platforms. This portability is validated through several simulation-based experiments.


Title: Complete and Near-Optimal Path Planning for Simultaneous Sensor-Based Inspection and Footprint Coverage in Robotic Crack Filling
Key Words: filling  inspection  mobile robots  optimal control  path planning  sensors  surface cracks  near-optimal path planning  robotic crack  simultaneous robotic footprint  range sensors  complete sensor coverage  planning strategy  crack-filling robotic prototype  online planning algorithm  sensor-based inspection  near-optimal footprint coverage  online sensor-based complete coverage planning  online SCC planning  Robot sensing systems  Planning  Inspection  Space exploration  Surface cracks 
Abstract: A simultaneous robotic footprint and sensor coverage planning scheme is proposed to efficiently detect all the unknown targets with range sensors and cover the targets with the robot's footprint in a structured environment. The proposed online Sensor-based Complete Coverage (online SCC) planning minimizes the total traveling distance of the robot, guarantees the complete sensor coverage of the whole free space, and achieves near-optimal footprint coverage of all the targets. The planning strategy is applied to a crack-filling robotic prototype to detect and fill all the unknown cracks on ground surfaces. Simulation and experimental results are presented that confirm the efficiency and effectiveness of the proposed online planning algorithm.


Title: Towards Semi-Autonomous and Soft-Robotics Enabled Upper-Limb Exoprosthetics: First Concepts and Robot-Based Emulation Prototype
Key Words: artificial limbs  biomechanics  medical robotics  mobile robots  motion control  orthopaedics  patient rehabilitation  3D visual perception  semiautonomous coordinated motion strategies  app-based programming framework  established standard sequential strategies all joints  human embodied dynamics model  robot-based exoskeleton substitute  soft-robotics design  intelligent coordinated control concepts  upper body  gravity effects  residual limb  unnecessary interaction forces  central goal  prostheses  exoskeletons  robot-based emulation prototype  soft-robotics enabled upper-limb exoprosthetics  strategy goals  Prosthetics  Exoskeletons  Robot kinematics  Prototypes  Task analysis  End effectors 
Abstract: In this paper the first robot-based prototype of a semi-autonomous upper-limb exoprosthesis is introduced, unifying exoskeletons and prostheses [1]. A central goal of this work is to minimize unnecessary interaction forces on the residual limb by compensating gravity effects via a upper body grounded exoskeleton. Furthermore, the exoskeleton provides the residual limb's kinematic data that allows to design more intelligent coordinated control concepts. The soft-robotics design of a prototype consisting of a transhumeral prosthesis and a robot-based exoskeleton substitute is outlined. For this class of hybrid systems a human embodied dynamics model and semi-autonomous coordinated motion strategies are derived. Here, in contrast to established standard sequential strategies all joints are moved simultaneously according to a desired task. In combination with an app-based programming framework the strategy goals are set either user-based via kinesthetic teaching or autonomously via 3D visual perception. This enables the user to execute tasks faster and more intuitive. First experimental evaluations show promising performance with a healthy subject.


Title: Flappy Hummingbird: An Open Source Dynamic Simulation of Flapping Wing Robots and Animals
Key Words: aerodynamics  aerospace components  aerospace robotics  aircraft control  autonomous aerial vehicles  closed loop systems  control system synthesis  learning (artificial intelligence)  microrobots  mobile robots  motion control  nonlinear control systems  robot dynamics  robot kinematics  stability  vehicle dynamics  flappy hummingbird  open source dynamic simulation  flapping Wing robots  hummingbirds  extraordinary flight performance  stable hovering maneuvering  aggressive maneuvering  conventional small scale man-made vehicles  FWMAVs  performance gap  open source high fidelity dynamic simulation  optimization  flight control  at-scale hummingbird robot  system identification  dynamic response  open-loop  loop systems  simulated flights  experimental flights  highly nonlinear flight dynamics  control problems  control algorithms  linear controller  control policy  simulation-to-real transfer  physical robot  flapping wing microair vehicles  Aerodynamics  Robots  Vehicle dynamics  Force  Torque  Animals 
Abstract: Insects and hummingbirds exhibit extraordinary flight performance and can simultaneously master seemingly conflicting goals: stable hovering and aggressive maneuvering, which are unmatched by conventional small scale man-made vehicles. Flapping Wing Micro Air Vehicles (FWMAVs) hold great promise for closing this performance gap. However, design and control of such systems remain challenging. Here, we present an open source high fidelity dynamic simulation for FWMAVs. The simulator serves as a testbed for the design, optimization and flight control of FWMAVs. To validate the simulation, we recreated the at-scale hummingbird robot developed in our lab in the simulation. System identification was performed to obtain the model parameters. Force generation and dynamic response of open-loop and closed loop systems between simulated and experimental flights were compared. The unsteady aerodynamics and the highly nonlinear flight dynamics present challenging control problems for conventional and learning control algorithms such as Reinforcement Learning. The interface of the simulation is fully compatible with OpenAI Gym environment. As a benchmark study, we present a linear controller for hovering stabilization and a Deep Reinforcement Learning control policy for goal-directed maneuvering. Finally, we demonstrate direct simulation-to-real transfer of both control policies onto the physical robot, further demonstrating the fidelity of the simulation.


Title: RCM-SLAM: Visual localisation and mapping under remote centre of motion constraints
Key Words: cameras  image motion analysis  image reconstruction  medical robotics  mobile robots  pose estimation  robot vision  SLAM (robots)  surgery  laparoscopic camera motion  RCM constraints  minimal solver  absolute camera  2D-3D point correspondences  bundle adjustment optimiser  RCM-constrained parameterisation  relative pose estimation  SLAM pipeline suitable  robotic surgery  RCM position  robotic prostatectomy show  RCM-SLAM  visual localisation  remote centre  motion constraints  insertion ports  Simultaneous Localisation and Mapping  mapping approach  RCM-PnP  Cameras  Robot vision systems  Simultaneous localization and mapping  Laparoscopes  Three-dimensional displays  Robot kinematics 
Abstract: In robotic surgery the motion of instruments and the laparoscopic camera is constrained by their insertion ports, i. e. a remote centre of motion (RCM). We propose a Simultaneous Localisation and Mapping (SLAM) approach that estimates laparoscopic camera motion under RCM constraints. To achieve this we derive a minimal solver for the absolute camera pose given two 2D-3D point correspondences (RCM-PnP) and also a bundle adjustment optimiser that refines camera poses within an RCM-constrained parameterisation. These two methods are used together with previous work on relative pose estimation under RCM [1] to assemble a SLAM pipeline suitable for robotic surgery. Our simulations show that RCM-PnP outperforms conventional PnP for a wide noise range in the RCM position. Results with video footage from a robotic prostatectomy show that RCM constraints significantly improve camera pose estimation.


Title: Guaranteed Globally Optimal Planar Pose Graph and Landmark SLAM via Sparse-Bounded Sums-of-Squares Programming
Key Words: graph theory  maximum likelihood estimation  mobile robots  navigation  nonlinear programming  path planning  polynomials  pose estimation  robot vision  SLAM (robots)  autonomous navigation  nonlinear optimization techniques  maximum likelihood estimate  robot trajectory  polynomial optimization programs  planar pose graph  landmark SLAM  sparse-bounded sums-of-squares programming  simultaneous localization and mapping  pose-graph SLAM problem  sum-of-squares convex  SOS convex  sparse bounded degree sum-of-squares optimization method  sparse-BSOS optimization method  Simultaneous localization and mapping  Optimization  Maximum likelihood estimation  Position measurement  Noise measurement  Transmission line matrix methods 
Abstract: Autonomous navigation requires an accurate model or map of the environment. While dramatic progress in the prior two decades has enabled large-scale simultaneous localization and mapping (SLAM), the majority of existing methods rely on non-linear optimization techniques to find the maximum likelihood estimate (MLE) of the robot trajectory and surrounding environment. These methods are prone to local minima and are thus sensitive to initialization. Several recent papers have developed optimization algorithms for the Pose-Graph SLAM problem that can certify the optimality of a computed solution. Though this does not guarantee a priori that this approach generates an optimal solution, a recent extension has shown that when the noise lies within a critical threshold that the solution to the optimization algorithm is guaranteed to be optimal. To address the limitations of existing approaches, this paper illustrates that the Pose-Graph SLAM and Landmark SLAM can be formulated as polynomial optimization programs that are sum-of-squares (SOS) convex. This paper then describes how the Pose-Graph and Landmark SLAM problems can be solved to a global minimum without initialization regardless of noise level using the sparse bounded degree sum-of-squares (Sparse-BSOS) optimization method. Finally, the superior performance of the proposed approach when compared to existing SLAM methods is illustrated on graphs with several hundred nodes.


Title: Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems
Key Words: driver information systems  image coding  image sequences  recurrent neural nets  road vehicles  egocentric vision-based future vehicle localization  intelligent driving assistance systems  safety-critical applications  autonomous driving  target vehicles  first-person view  ego-vehicle  multistream recurrent neural network encoder-decoder model  object location  pixel-level observations  future motion  prediction accuracy  intelligent vehicles  automated vehicles  motion planning capability  vehicle trajectories  dense optical flow  Trajectory  Cameras  Optical imaging  Decoding  Videos  Predictive models  Advanced driver assistance systems 
Abstract: Predicting the future location of vehicles is essential for safety-critical applications such as advanced driver assistance systems (ADAS) and autonomous driving. This paper introduces a novel approach to simultaneously predict both the location and scale of target vehicles in the first-person (egocentric) view of an ego-vehicle. We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixel-level observations for future vehicle localization. We show that incorporating dense optical flow improves prediction results significantly since it captures information about motion as well as appearance change. We also find that explicitly modeling future motion of the ego-vehicle improves the prediction accuracy, which could be especially beneficial in intelligent and automated vehicles that have motion planning capability. To evaluate the performance of our approach, we present a new dataset of first-person videos collected from a variety of scenarios at road intersections, which are particularly challenging moments for prediction because vehicle trajectories are diverse and dynamic. Code and dataset have been made available at: https://usa.honda-ri.com/hevi.


Title: Real-time Scalable Dense Surfel Mapping
Key Words: image fusion  image reconstruction  pose estimation  SLAM (robots)  intensity images  depth images  globally consistent model  room-scale environments  urban-scale environments  RGB-D cameras  stereo cameras  monocular camera  superpixel-based surfels  reconstructed models  fast map deformation  global consistency  room-scale reconstruction  time scalable dense surfel  CPU computation  sparse SLAM system  camera poses  dense surfel mapping system  Cameras  Image reconstruction  Strain  Fuses  Robot vision systems  Simultaneous localization and mapping  Three-dimensional displays 
Abstract: In this paper, we propose a novel dense surfel mapping system that scales well in different environments with only CPU computation. Using a sparse SLAM system to estimate camera poses, the proposed mapping system can fuse intensity images and depth images into a globally consistent model. The system is carefully designed so that it can build from room-scale environments to urban-scale environments using depth images from RGB-D cameras, stereo cameras or even a monocular camera. First, superpixels extracted from both intensity and depth images are used to model surfels in the system. superpixel-based surfels make our method both runtime efficient and memory efficient. Second, surfels are further organized according to the pose graph of the SLAM system to achieve O(1) fusion time regardless of the scale of reconstructed models. Third, a fast map deformation using the optimized pose graph enables the map to achieve global consistency in real-time. The proposed surfel mapping system is compared with other state-of-the-art methods on synthetic datasets. The performances of urban-scale and room-scale reconstruction are demonstrated using the KITTI dataset [1] and autonomous aggressive flights, respectively. The code is available for the benefit of the community.


Title: Semantic Mapping for View-Invariant Relocalization
Key Words: cameras  feature extraction  image representation  object detection  pose estimation  robot vision  SLAM (robots)  semantic mapping  view-invariant relocalization  accurate local tracking  view-invariant object-driven relocalization  sampling-based approach  2D bounding box object detections  view-invariant representation  camera relocalization  view-invariance  relocalization rate  visual simultaneous localization and mapping  object landmarks  local appearance-based features  SLAM  3D pose  SIFT  mean rotational error  Three-dimensional displays  Cameras  Simultaneous localization and mapping  Semantics  Two dimensional displays  Visualization  Task analysis 
Abstract: We propose a system for visual simultaneous localization and mapping (SLAM) that combines traditional local appearance-based features with semantically meaningful object landmarks to achieve both accurate local tracking and highly view-invariant object-driven relocalization. Our mapping process uses a sampling-based approach to efficiently infer the 3D pose of object landmarks from 2D bounding box object detections. These 3D landmarks then serve as a view-invariant representation which we leverage to achieve camera relocalization even when the viewing angle changes by more than 125 degrees. This level of view-invariance cannot be attained by local appearance-based features (e.g. SIFT) since the same set of surfaces are not even visible when the viewpoint changes significantly. Our experiments show that even when existing methods fail completely for viewpoint changes of more than 70 degrees, our method continues to achieve a relocalization rate of around 90%, with a mean rotational error of around 8 degrees.


Title: Real-Time Monocular Object-Model Aware Sparse SLAM
Key Words: cameras  convolutional neural nets  feature extraction  learning (artificial intelligence)  mobile robots  object detection  robot vision  SLAM (robots)  mobile robotics  sparse point-based SLAM methods  CNN-based plane detector  semantic SLAM  simultaneous localization and mapping  camera localization  deep-learned object detector  CNN network  semantic objects representation  monocular object-model aware sparse SLAM framework  Simultaneous localization and mapping  Semantics  Image reconstruction  Real-time systems  Cameras  Three-dimensional displays 
Abstract: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in mobile robotics. While sparse point-based SLAM methods provide accurate camera localization, the generated maps lack semantic information. On the other hand, state of the art object detection methods provide rich information about entities present in the scene from a single image. This work incorporates a real-time deep-learned object detector to the monocular SLAM framework for representing generic objects as quadrics that permit detections to be seamlessly integrated while allowing the real-time performance. Finer reconstruction of an object, learned by a CNN network, is also incorporated and provides a shape prior for the quadric leading further refinement. To capture the structure of the scene, additional planar landmarks are detected by a CNN-based plane detector and modelled as independent landmarks in the map. Extensive experiments support our proposed inclusion of semantic objects and planar structures directly in the bundle-adjustment of SLAM - Semantic SLAM- that enriches the reconstructed map semantically, while significantly improving the camera localization.


Title: Probabilistic Projective Association and Semantic Guided Relocalization for Dense Reconstruction
Key Words: convolutional neural nets  image coding  image recognition  image reconstruction  image representation  image retrieval  image segmentation  object detection  probability  robot vision  SLAM (robots)  convolutional neural networks  geometric quality  probabilistic projective association  loop frames  2D labeling  CNN  semantic prediction  simultaneous localization and mapping system  SLAM system  3D scenes  randomized ferns  semantic recognition  geometric reconstruction  loop detection  reconstruction pipeline  semantic information  camera trajectory estimation  semantic labels  real-time dense mapping system  dense reconstruction  semantic guided relocalization  Semantics  Probabilistic logic  Labeling  Two dimensional displays  Cameras  Tracking loops  Trajectory 
Abstract: We present a real-time dense mapping system which uses the predicted 2D semantic labels for optimizing the geometric quality of reconstruction. With a combination of Convolutional Neural Networks (CNNs) for 2D labeling and a Simultaneous Localization and Mapping (SLAM) system for camera trajectory estimation, recent approaches have succeeded in incrementally fusing and labeling 3D scenes. However, the geometric quality of the reconstruction can be further improved by incorporating such semantic prediction results, which is not sufficiently exploited by existing methods. In this paper, we propose to use semantic information to improve two crucial modules in the reconstruction pipeline, namely tracking and loop detection, for obtaining mutual benefits in geometric reconstruction and semantic recognition. Specifically for tracking, we use a novel probabilistic projective association approach to efficiently pick out candidate correspondences, where the confidence of these correspondences is quantified concerning similarities on all available short-term invariant features. For the loop detection, we incorporate these semantic labels into the original encoding through Randomized Ferns to generate a more comprehensive representation for retrieving candidate loop frames. Evaluations on a publicly available synthetic dataset have shown the effectiveness of our approach that considers such semantic hints as a reliable feature for achieving higher geometric quality.


Title: GraspFusion: Realizing Complex Motion by Learning and Fusing Grasp Modalities with Instance Segmentation
Key Words: grippers  image fusion  image matching  image segmentation  learning (artificial intelligence)  object detection  deep learning  multimodal grippers  simultaneous pinch  multimodal grasp fusion  object-class-agnostic grasp  modality detection  object-class-agnostic instance segmentation  grasp template matching  object manipulation  object geometry  grasp modalities  instance segmentation  integrated system  Image segmentation  Grippers  Grasping  Motion segmentation  Image color analysis  Task analysis 
Abstract: Recent progress of deep learning improved the capability of a robot to find a proper grasp of a novel object for different grasp modalities (e.g., pinch and suction). While these previous studies consider multiple modalities separately, several studies develop multi-modal grippers that can achieve simultaneous pinch and suction grasp (multi-modal grasp fusion) for more capable and stable object manipulation. However, the previous studies with these grippers restrict the situations: simple object geometry and uncluttered environments. To overcome these difficulties, we propose a system that consists of: 1) object-class-agnostic grasp modality detection; 2) object-class-agnostic instance segmentation; and 3) grasp template matching for different modalities. The key idea of our work is the introduction of instance segmentation to fuse multiple modalities regarding each instance eluding a grasp of multiple objects at once. In the experiments, we evaluated the proposed system on the real-world picking task in clutter. The experimental results show that the effectiveness of modality detection, instance segmentation, and the integrated system as a whole.


Title: Handling robot constraints within a Set-Based Multi-Task Priority Inverse Kinematics Framework
Key Words: manipulator kinematics  optimisation  redundant manipulators  safety related tasks  set-based task  equality tasks  set-bases tasks  optimization tasks  set-based multitask priority framework  set-based multitask priority inverse kinematics framework  robot constraint handling  redundant structures  7DOF Jaco2 arm  Task analysis  Optimization  Safety  Kinematics  Robots  Jacobian matrices  Redundancy 
Abstract: Set-Based Multi-Task Priority is a recent framework to handle inverse kinematics for redundant structures. Both equality tasks, i.e., control objectives to be driven to a desired value, and set-bases tasks, i.e., control objectives to be satisfied with a set/range of values can be addressed in a rigorous manner within a priority framework. In addition, optimization tasks, driven by the gradient of a proper function, may be considered as well, usually as lower priority tasks. In this paper the proper design of the tasks, their priority and the use of a Set-Based Multi-Task Priority framework is proposed in order to handle several constraints simultaneously in real-time. It is shown that safety related tasks such as, e.g., joint limits or kinematic singularity, may be properly handled by consider them both at an higher priority as set-based task and at a lower within a proper optimization functional. Experimental results on a 7DOF Jaco2 arm with and without the proposed approach show the effectiveness of the proposed method.


Title: Hierarchical optimization for Whole-Body Control of Wheeled Inverted Pendulum Humanoids
Key Words: control system synthesis  end effectors  humanoid robots  manipulator dynamics  manipulator kinematics  mobile robots  motion control  nonlinear control systems  optimisation  pendulums  redundant manipulators  high-level controller plans  hierarchical optimization  whole-body control framework  redundant manipulators  wheels  optimal participation  low level controller  control zero dynamics  low-level controller plans  body joint manipulation  WIP humanoids  wheeled inverted pendulum humanoids  Wheels  Manipulator dynamics  Humanoid robots  Task analysis  Mobile robots  Robot kinematics 
Abstract: In this paper, we present a whole-body control framework for Wheeled Inverted Pendulum (WIP) Humanoids. WIP Humanoids are redundant manipulators dynamically balancing themselves on wheels. Characterized by several degrees of freedom, they have the ability to perform several tasks simultaneously, such as balancing, maintaining a body pose, controlling the gaze, lifting a load or maintaining end-effector configuration in operation space. The problem of whole-body control is to enable simultaneous performance of these tasks with optimal participation of all degrees of freedom at specified priorities for each objective. The control also has to obey constraint of angle and torque limits on each joint. The proposed approach is hierarchical with a low level controller for body joints manipulation and a high-level controller that defines center of mass (CoM) targets for the low-level controller to control zero dynamics of the system driving the wheels. The low-level controller plans for shorter horizons while considering more complete dynamics of the system, while the high-level controller plans for longer horizon based on an approximate model of the robot for computational efficiency.


Title: Algorithmic Resolution of Multiple Impacts in Nonsmooth Mechanical Systems with Switching Constraints
Key Words: complementarity  differential algebraic equations  impact (mechanical)  iterative methods  legged locomotion  plasticity  robot dynamics  algorithmic resolution  multiple impacts  nonsmooth mechanical systems  switching constraints  differential-algebraic formulation  nonsmooth dynamics  robotic systems  changing constraints  kinematic constraints  algorithmic impact resolution method  classical plastic impact law  multiple simultaneous impacts  prior linear-complementarity-based formulations  implicit impact resolution  Switches  Mathematical model  Mechanical systems  Robots  Dynamics  Heuristic algorithms  Plastics 
Abstract: We present a differential-algebraic formulation with switching constraints to model the nonsmooth dynamics of robotic systems subject to changing constraints and multiple impacts. The formulation combines a single structurally simple governing equation, a set of switching kinematic constraints, and the plastic impact law, to represent the dynamics of robots that interact with their environment. The main contribution of this formulation is a novel algorithmic impact resolution method which provides an explicit solution to the classical plastic impact law in the case of multiple simultaneous impacts. This method serves as an alternative to prior linear-complementarity-based formulations which offer an implicit impact resolution through iterative calculation. We demonstrate the utility of the proposed method by simulating the locomotion of a planar anthropometric biped.


Title: Rigid Body Motion Prediction with Planar Non-convex Contact Patch
Key Words: computational geometry  manipulator dynamics  mechanical contact  shear modulus  planar nonconvex contact patch  intermittent contact  rigid body dynamic simulation  convex contact patches  contact detection  contacting rigid bodies  multiple point contact  single point contact  rigid body motion prediction  convex hull  Mathematical model  Dynamics  Numerical models  Transmission line matrix methods  Robots  Bars  Symmetric matrices 
Abstract: We present a principled method for motion prediction via dynamic simulation for rigid bodies in intermittent contact with each other where the contact is assumed to be a planar non-convex contact patch. The planar non-convex contact patch can either be a topologically connected set or disconnected set. Such algorithms are useful in planning and control for robotic manipulation. Most work in rigid body dynamic simulation assume that the contact between objects is a point contact, which may not be valid in many applications. In this paper, by using the convex hull of the contact patch, we build on our recent work on simulating rigid bodies with convex contact patches, for simulating motion of objects with planar non-convex contact patches. We formulate a discrete-time mixed complementarity problem where we solve the contact detection and integration of the equations of motion simultaneously. Thus, our method is a geometrically-implicit method and we prove that in our formulation, there is no artificial penetration between the contacting rigid bodies. We solve for the equivalent contact point (ECP) and contact impulse of each contact patch simultaneously along with the state, i.e., configuration and velocity of the objects. We provide empirical evidence to show that our method can seamlessly capture transition between different contact modes like patch contact to multiple or single point contact during simulation.


