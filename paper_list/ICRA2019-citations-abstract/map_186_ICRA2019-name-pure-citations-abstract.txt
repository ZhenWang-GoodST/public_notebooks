total paper: 186
Title: Navigating Dynamically Unknown Environments Leveraging Past Experience
Key Words: adaptive control  collision avoidance  mobile robots  navigation  autonomous robot navigation  unknown dynamic obstacles  real-time adaptive motion planner  robot motion online  sensed environmental data  limited sensing range  RAMP framework  probabilistic model  unknown dynamic environment  sensing information  RAMP robot  dynamic environment changes  unknown ways  learned probabilistic data  Hilbert maps framework  dynamically unknown environment navigation  Robot sensing systems  Trajectory  Sociology  Statistics  Planning 
Abstract: To enable autonomous robot navigation among unknown dynamic obstacles, a real-time adaptive motion planner (RAMP) plans the robot motion online based on sensing the environment as the robot moves with sensors mounted on the robot. However, the sensed environmental data from the robot's local view is usually incomplete due to occlusions from obstacles and limited sensing range.This paper incorporates learning about the environment into the RAMP framework by leveraging the Hilbert Maps framework to generate a probabilistic model of occupancy of the unknown dynamic environment based on past observations. Utilizing this probabilistic model enables RAMP to reason about trajectory fitness when sensing information is partial and incomplete. This allows the RAMP robot to take advantage of what it has experienced from being in the dynamic environment before to inform its subsequent executions even though the dynamic environment changes in unknown ways. The effectiveness of incorporating such learned probabilistic data into RAMP is shown in both simulation and real experiments.


Title: VPE: Variational Policy Embedding for Transfer Reinforcement Learning
Key Words: learning (artificial intelligence)  Markov processes  pendulums  variational techniques  variational policy embedding  transfer reinforcement Learning  complex problems  deployment conditions  data collection  simulation training  Q-function  master policy  latent variables  latent space  low-dimensional space  simulation-to-real transfer  reinforcement learning methods  Markov decision processes  Optimization  Training  Task analysis  Robots  Adaptation models  Reinforcement learning  Supervised learning 
Abstract: Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.


Title: Morphology-Specific Convolutional Neural Networks for Tactile Object Recognition with a Multi-Fingered Hand
Key Words: convolutional neural nets  dexterous manipulators  force measurement  force sensors  object recognition  tactile sensors  morphology-specific convolutional neural network  distributed tactile sensors  multifingered hands  high-dimensional information  grasping objects  abundant tactile information  MS-CNN  Allegro Hand  uSkin modules  consecutive layers  finger segment  tactile map  force measurements  joint angle measurements  object recognition rate  triaxial force sensors  Tactile sensors  Convolution  Task analysis  Object recognition 
Abstract: Distributed tactile sensors on multi-fingered hands can provide high-dimensional information for grasping objects, but it is not clear how to optimally process such abundant tactile information. The current paper explores the possibility of using a morphology-specific convolutional neural network (MS-CNN). uSkin tactile sensors are mounted on an Allegro Hand, which provides 720 force measurements (15 patches of uSkin modules with 16 triaxial force sensors each) in addition to 16 joint angle measurements. Consecutive layers in the CNN get input from parts of one finger segment, one finger, and the whole hand. Since the sensors give 3D (x, y, z) vector tactile information, inputs with 3 channels (x, y and z) are used in the first layer, based on the idea of such inputs for RGB images from cameras. Overall, the layers are combined, resulting in the building of a tactile map based on the relative position of the tactile sensors on the hand. Seven different combination variations were evaluated, and an over-95% object recognition rate with 20 objects was achieved, even though only one random time instance from a repeated squeezing motion of an object in an unknown pose within the hand was used as input.


Title: Designing Worm-inspired Neural Networks for Interpretable Robotic Control
Key Words: brain  manipulator dynamics  mobile robots  neurocontrollers  neurophysiology  nonlinear control systems  recurrent neural nets  search problems  supervised learning  time-varying systems  interpretable robotic control  nonlinear time-varying synaptic links  liquid time-constants dynamics  neuron-pair communication motifs  compact neuronal network structures  sequential robotic tasks  sensory neurons  recurrently-wired interneurons  motor neurons  interpretable dynamics  mobile arm robots  artificial neural network-based control agents  wiring structure  worm-inspired neural networks  liquid time-constant recurrent neural networks  nematode  C. elegans  supervised-learning scheme  search-based algorithm  Neurons  Biological neural networks  Robot sensing systems  Synapses  Correlation  Couplings 
Abstract: In this paper, we design novel liquid time-constant recurrent neural networks for robotic control, inspired by the brain of the nematode, C. elegans. In the worm's nervous system, neurons communicate through nonlinear time-varying synaptic links established amongst them by their particular wiring structure. This property enables neurons to express liquid time-constants dynamics and therefore allows the network to originate complex behaviors with a small number of neurons. We identify neuron-pair communication motifs as design operators and use them to configure compact neuronal network structures to govern sequential robotic tasks. The networks are systematically designed to map the environmental observations to motor actions, by their hierarchical topology from sensory neurons, through recurrently-wired interneurons, to motor neurons. The networks are then parametrized in a supervised-learning scheme by a search-based algorithm. We demonstrate that obtained networks realize interpretable dynamics. We evaluate their performance in controlling mobile and arm robots, and compare their attributes to other artificial neural network-based control agents. Finally, we experimentally show their superior resilience to environmental noise, compared to the existing machine learning-based methods.


Title: FMD Stereo SLAM: Fusing MVG and Direct Formulation Towards Accurate and Fast Stereo SLAM
Key Words: feature extraction  motion estimation  pose estimation  SLAM (robots)  stereo image processing  key-feature-based multiple view geometry  global map  3D structure  bundle adjustment  fast stereo SLAM  direct formulation  local map  constant motion model  direct-based formulation  novel stereo visual SLAM framework  FMD stereo SLAM  back-end process  stereo constraint  reprojection error minimization  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Feature extraction  Visualization  Robot vision systems  Two dimensional displays 
Abstract: We propose a novel stereo visual SLAM framework considering both accuracy and speed at the same time. The framework makes full use of the advantages of key-feature-based multiple view geometry (MVG) and direct-based formulation. At the front-end, our system performs direct formulation and constant motion model to predict a robust initial pose, reprojects local map to find 3D-2D correspondence and finally refines pose by the reprojection error minimization. This frontend process makes our system faster. At the back-end, MVG is used to estimate 3D structure. When a new keyframe is inserted, new mappoints are generated by triangulating. In order to improve the accuracy of the proposed system, bad mappoints are removed and a global map is kept by bundle adjustment. Especially, the stereo constraint is performed to optimize the map. This back-end process makes our system more accurate. Experimental evaluation on EuRoC dataset shows that the proposed algorithm can run at more than 100 frames per second on a consumer computer while achieving highly competitive accuracy.


Title: GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization and Mapping
Key Words: cameras  collision avoidance  convolutional neural nets  learning (artificial intelligence)  mobile robots  pose estimation  robot vision  SLAM (robots)  depth estimation system  GEN-SLAM  generative modeling  Deep Learning based system  obstacle avoidance  mobile robot  conventional geometric SLAM  single camera  topological map  camera image  topological location estimation  monocular localization  monocular simultaneous localization and mapping  Cameras  Image reconstruction  Simultaneous localization and mapping  Decoding  Training 
Abstract: We present a Deep Learning based system for the twin tasks of localization and obstacle avoidance essential to any mobile robot. Our system learns from conventional geometric SLAM, and outputs, using a single camera, the topological pose of the camera in an environment, and the depth map of obstacles around it. We use a CNN to localize in a topological map, and a conditional VAE to output depth for a camera image, conditional on this topological location estimation. We demonstrate the effectiveness of our monocular localization and depth estimation system on simulated and real datasets.


Title: RESLAM: A real-time robust edge-based SLAM system
Key Words: cameras  edge detection  image colour analysis  image representation  image sensors  motion estimation  optimisation  pose estimation  robot vision  SLAM (robots)  camera intrinsics  sliding window  edge-based verification  RESLAM  camera motions  RGBD sensors  sparse representation  SLAM pipeline  robust edge-based SLAM system  simultaneous localization and mapping  Image edge detection  Cameras  Simultaneous localization and mapping  Optimization  Real-time systems  Microsoft Windows 
Abstract: Simultaneous Localization and Mapping is a key requirement for many practical applications in robotics. In this work, we present RESLAM, a novel edge-based SLAM system for RGBD sensors. Due to their sparse representation, larger convergence basin and stability under illumination changes, edges are a promising alternative to feature-based or other direct approaches. We build a complete SLAM pipeline with camera pose estimation, sliding window optimization, loop closure and relocalisation capabilities that utilizes edges throughout all steps. In our system, we additionally refine the initial depth from the sensor, the camera poses and the camera intrinsics in a sliding window to increase accuracy. Further, we introduce an edge-based verification for loop closures that can also be applied for relocalisation. We evaluate RESLAM on wide variety of benchmark datasets that include difficult scenes and camera motions and also present qualitative results. We show that this novel edge-based SLAM system performs comparable to state-of-the-art methods, while running in real-time on a CPU. RESLAM is available as open-source software1.


Title: On-line 3D active pose-graph SLAM based on key poses using graph topology and sub-maps
Key Words: autonomous aerial vehicles  computational complexity  graph theory  mobile robots  optimisation  path planning  remotely operated vehicles  robot vision  SLAM (robots)  graph topology  pose-graph simultaneous localization  three-dimensional environments  D-optimality metrics  weighted node degree  T-optimality metric  sampling-based path  continuous-time trajectory optimization method  large-scale active SLAM problems  submap joining method  online 3D active pose-graph SLAM  Simultaneous localization and mapping  Measurement  Trajectory  Planning  Three-dimensional displays  Uncertainty 
Abstract: In this paper, we present an on-line active pose-graph simultaneous localization and mapping (SLAM) frame-work for robots in three-dimensional (3D) environments using graph topology and sub-maps. This framework aims to find the best trajectory for loop-closure by re-visiting old poses based on the T-optimality and D-optimality metrics of the Fisher information matrix (FIM) in pose-graph SLAM. In order to reduce computational complexity, graph topologies are introduced, including weighted node degree (T-optimality metric) and weighted tree-connectivity (D-optimality metric), to choose a candidate trajectory and several key poses. With the help of the key poses, a sampling-based path planning method and a continuous-time trajectory optimization method are combined hierarchically and applied in the whole framework. So as to further improve the real-time capability of the method, the sub-map joining method is used in the estimation and planning process for large-scale active SLAM problems. In simulations and experiments, we validate our approach by comparing against existing methods, and we demonstrate the on-line planning part using a quad-rotor unmanned aerial vehicle (UAV).


Title: Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT
Key Words: adaptive control  control engineering computing  control system synthesis  feedback  learning (artificial intelligence)  legged locomotion  mobile robots  PD control  robust control  stability  feedback controllers  bipedal robots  high-dimensional bipedal models  bipedal walking  bipedal control  walking limit cycles  HZD framework  policy learning  adaptive PD controller  stable control policy  robust control policy  RL framework  RABBIT robot model  reinforcement learning  local stability  hybrid zero dynamics  OpenAI gym  MuJoCo physics engine  Legged locomotion  Hip  Rabbits  Robot kinematics  Training  Computational modeling 
Abstract: The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot's reduced order states to a set of parameters that define the desired trajectories for the robot's joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.


Title: Recursive Integrity Monitoring for Mobile Robot Localization Safety
Key Words: fault diagnosis  Kalman filters  mobile robots  nonlinear filters  constant computation requirements  sequential chi-squared integrity monitoring methodology  fault detection  Extended Kalman Filter  mobile ground robots  open-sky aviation applications  integrity risk  mobile robot localization safety  recursive integrity monitoring  preceding time window  Monitoring  Feature extraction  Safety  Fault detection  Robot sensing systems  Kalman filters 
Abstract: This paper presents a new methodology to quantify robot localization safety by evaluating integrity risk, a performance metric widely used in open-sky aviation applications that has been recently extended to mobile ground robots. Here, a robot is localized by feeding relative measurements to mapped landmarks into an Extended Kalman Filter while a sequence of innovations is evaluated for fault detection. The main contribution is the derivation of a sequential chi-squared integrity monitoring methodology that maintains constant computation requirements by employing a preceding time window and, at the same time, is robust against faults occurring prior to the window. Additionally, no assumptions are made on either the nature or shape of the faults because safety is evaluated under the worst possible combination of sensor faults.


Title: Event-based, Direct Camera Tracking from a Photometric 3D Map using Nonlinear Optimization
Key Words: cameras  image reconstruction  image sensors  maximum likelihood estimation  motion estimation  motion measurement  nonlinear programming  photometry  pose estimation  asynchronous sensors  low power consumption  photometric 3D map  classic dense 3D reconstruction algorithms  bioinspired vision sensors  video imaging  output pixel-level intensity  event-based direct camera tracking  nonlinear optimization  robot localization  AR-VR  6-DOF pose tracking  maximum-likelihood framework  event camera motion estimation  Cameras  Robot vision systems  Three-dimensional displays  Optimization 
Abstract: Event cameras are novel bio-inspired vision sensors that output pixel-level intensity changes, called “events”, instead of traditional video images. These asynchronous sensors naturally respond to motion in the scene with very low latency (microseconds) and have a very high dynamic range. These features, along with a very low power consumption, make event cameras an ideal sensor for fast robot localization and wearable applications, such as AR/VR and gaming. Considering these applications, we present a method to track the 6-DOF pose of an event camera in a known environment, which we contemplate to be described by a photometric 3D map (i.e., intensity plus depth information) built via classic dense 3D reconstruction algorithms. Our approach uses the raw events, directly, without intermediate features, within a maximum-likelihood framework to estimate the camera motion that best explains the events via a generative model. We successfully evaluate the method using both simulated and real data, and show improved results over the state of the art. We release the datasets to the public to foster reproducibility and research in this topic.


Title: Distortion-free Robotic Surface-drawing using Conformal Mapping
Key Words: computational geometry  computer graphics  conformal mapping  distance measurement  image reconstruction  least squares approximations  manipulators  mobile robots  solid modelling  distortion-free robotic surface-drawing  robotic pen-drawing system  unknown surface  robotic system  seven-degree-of freedom manipulator  continuous surface  physical canvas surface  point-cloud estimation  drawing surface  2D vector pen art  surface parameterization  squares conformal mapping  complicated pen drawings  general surfaces  impedance-control  digital drawing  2D drawing  Surface impedance  Three-dimensional displays  Robot kinematics  Two dimensional displays  Service robots  Surface reconstruction 
Abstract: We present a robotic pen-drawing system that is capable of faithfully reproducing pen art on an unknown surface. Our robotic system relies on an industrial, seven-degree-of freedom manipulator that can be both position- and impedance-controlled. In order to estimate a rough geometry of the target, continuous surface, we first generate a point cloud of the surface using an RGB-D camera, which is filtered to remove outliers and calibrated to the physical canvas surface. Then, our control algorithm physically reproduces digital drawing on the surface by impedance-controlling the manipulator. Our impedance-controlled drawing algorithm compensates for the uncertainty and incompleteness inherent to a point-cloud estimation of the drawing surface. Moreover, since drawing 2D vector pen art on a 3D surface requires surface parameterization that does not destroy the original 2D drawing, we rely on the least squares conformal mapping. Specifically, the conformal map reduces angle distortion during surface parameterization. As a result, our system can create distortion-free and complicated pen drawings on general surfaces with many unpredictable bumps robustly and faithfully.


Title: Mobile Robotic Painting of Texture
Key Words: image colour analysis  image reconstruction  image texture  learning (artificial intelligence)  mobile robots  neural nets  painting  spraying  mobile robotic painting  mobile robots  robotic paint delivery systems  spray painting  painting tasks  image texture  robotic paint commands  deep learning approach  appearance reconstruction  Painting  Robots  Paints  Ink  Atmospheric modeling  Spraying 
Abstract: Robotic painting is well-established in controlled factory environments, but there is now potential for mobile robots to do functional painting tasks around the everyday world. An obvious first target for such robots is painting a uniform single color. A step further is the painting of textured images. Texture involves a varying appearance, and requires that paint is delivered accurately onto the physical surface to produce the desired effect. Robotic painting of texture is relevant for architecture and in themed environments. A key challenge for robotic painting of texture is to take a desired image as input, and to generate the paint commands to as closely as possible create the desired appearance, according to the robotic capabilities. This paper describes a deep learning approach to take an input ink map of a desired texture, and infer robotic paint commands to produce that texture. We analyze the trade-offs between quality of reconstructed appearance and ease of execution. Our method is general for different kinds of robotic paint delivery systems, but the emphasis here is on spray painting. More generally, the framework can be viewed as an approach for solving a specific class of inverse imaging problems.


Title: Robust Object-based SLAM for High-speed Autonomous Navigation
Key Words: cameras  helicopters  image sequences  image texture  mobile robots  object detection  path planning  robot vision  SLAM (robots)  ROSHAN  object-level mapping  ellipsoid-based SLAM  object surface  autonomous quadrotor  bounding box detections  median shape error  forward-moving camera sequence  planar constraint  vehicle motions  semantic knowledge  robust object-based SLAM for high-speed autonomous navigation  Ellipsoids  Image edge detection  Semantics  Simultaneous localization and mapping  Cameras  Shape  Shape measurement 
Abstract: We present Robust Object-based SLAM for High-speed Autonomous Navigation (ROSHAN), a novel approach to object-level mapping suitable for autonomous navigation. In ROSHAN, we represent objects as ellipsoids and infer their parameters using three sources of information - bounding box detections, image texture, and semantic knowledge - to overcome the observability problem in ellipsoid-based SLAM under common forward-translating vehicle motions. Each bounding box provides four planar constraints on an object surface and we add a fifth planar constraint using the texture on the objects along with a semantic prior on the shape of ellipsoids. We demonstrate ROSHAN in simulation where we outperform the baseline, reducing the median shape error by 83% and the median position error by 72% in a forward-moving camera sequence. We demonstrate similar qualitative result on data collected on a fast-moving autonomous quadrotor.


Title: Beauty and the Beast: Optimal Methods Meet Learning for Drone Racing
Key Words: autonomous aerial vehicles  collision avoidance  Kalman filters  learning (artificial intelligence)  mobile robots  navigation  optimisation  predictive control  state estimation  robust flight  previously-unseen race tracks  optimal methods  fast maneuvers  agile maneuvers  dynamic environments  imperfect sensing  state estimation drift  human pilots  unseen track  practice runs  state-of-the-art autonomous navigation algorithms  precise metric map  training data  unseen environment  precise map  expensive data collection  global track layout  coarse gate locations  single demonstration flight  convolutional network  closest gates  extended Kalman filter  maximum-a-posteriori estimates  high-variance estimates  poor observability  visible gates  estimated gate poses  model predictive control  agile flight  autonomous microaerial vehicles  autonomous drone racing  IROS 2018 autonomous drone race competition  Logic gates  Drones  Navigation  Training data  Current measurement  Layout  Uncertainty 
Abstract: Autonomous micro aerial vehicles still struggle with fast and agile maneuvers, dynamic environments, imperfect sensing, and state estimation drift. Autonomous drone racing brings these challenges to the fore. Human pilots can fly a previously unseen track after a handful of practice runs. In contrast, state-of-the-art autonomous navigation algorithms require either a precise metric map of the environment or a large amount of training data collected in the track of interest. To bridge this gap, we propose an approach that can fly a new track in a previously unseen environment without a precise map or expensive data collection. Our approach represents the global track layout with coarse gate locations, which can be easily estimated from a single demonstration flight. At test time, a convolutional network predicts the poses of the closest gates along with their uncertainty. These predictions are incorporated by an extended Kalman filter to maintain optimal maximum-a-posteriori estimates of gate locations. This allows the framework to cope with misleading high-variance estimates that could stem from poor observability or lack of visible gates. Given the estimated gate poses, we use model predictive control to quickly and accurately navigate through the track. We conduct extensive experiments in the physical world, demonstrating agile and robust flight through complex and diverse previously-unseen race tracks. The presented approach was used to win the IROS 2018 Autonomous Drone Race Competition, outracing the second-placing team by a factor of two.


Title: Real-Time Planning with Multi-Fidelity Models for Agile Flights in Unknown Environments
Key Words: autonomous aerial vehicles  collision avoidance  mobile robots  sensors  low-fidelity models  fast planner  planning framework  agile flights  replanning times  cluttered environments  multifidelity models  autonomous navigation  real-time localization  lightweight sensing  planning methodologies  hierarchical planning architecture  low-fidelity global planner  high-fidelity local planner  erratic behavior  unstable behavior  global plan  higher-order dynamics  real-time planning  sensor data  collision check  UAV  time 5.0 ms to 40.0 ms  Planning  Trajectory  Computational modeling  Vehicle dynamics  Robot sensing systems  Optimization 
Abstract: Autonomous navigation through unknown environments is a challenging task that entails real-time localization, perception, planning, and control. UAVs with this capability have begun to emerge in the literature with advances in lightweight sensing and computing. Although the planning methodologies vary from platform to platform, many algorithms adopt a hierarchical planning architecture where a slow, low-fidelity global planner guides a fast, high-fidelity local planner. However, in unknown environments, this approach can lead to erratic or unstable behavior due to the interaction between the global planner, whose solution is changing constantly, and the local planner; a consequence of not capturing higher-order dynamics in the global plan. This work proposes a planning framework in which multi-fidelity models are used to reduce the discrepancy between the local and global planner. Our approach uses high-, medium-, and low-fidelity models to compose a path that captures higher-order dynamics while remaining computationally tractable. In addition, we address the interaction between a fast planner and a slower mapper by considering the sensor data not yet fused into the map during the collision check. This novel mapping and planning framework for agile flights is validated in simulation and hardware experiments, showing replanning times of 5-40 ms in cluttered environments.


Title: Priority Maps for Surveillance and Intervention of Wildfires and other Spreading Processes
Key Words: autonomous aerial vehicles  optimisation  path planning  wildfires  priority maps  wildfires  knowledge reward function  dynamic spreading processes  surveillance  bushfire spreading dynamics  wildfire intervention  unmanned aerial vehicle  optimization framework  UAV path planning  Mathematical model  Surveillance  Stochastic processes  Unmanned aerial vehicles  Path planning  Vehicle dynamics  Computational modeling 
Abstract: Unmanned Aerial Vehicle (UAV) path planning algorithms often assume a knowledge reward function or priority map, indicating the most important areas to visit. In this paper we propose a method to create priority maps for monitoring or intervention of dynamic spreading processes such as wildfires. The presented optimization framework utilizes the properties of positive systems, in particular the separable structure of value (cost-to-go) functions, to provide scalable algorithms for surveillance and intervention. We present results obtained for a 16 and 1000 node example and convey how the priority map responds to changes in the dynamics of the system. The larger example of 1000 nodes, representing a fictional landscape, shows how the method can integrate bushfire spreading dynamics, landscape and wind conditions. Finally, we give an example of combining the proposed method with a travelling salesman problem for UAV path planning for wildfire intervention.


Title: A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies
Key Words: gradient methods  image coding  learning (artificial intelligence)  mobile robots  path planning  data-efficient framework  sim-to-real transfer  navigation policies  effective visuomotor policies  learning-based system  manual tuning  robot operating  training process  leverage simulation  off-policy data  initial image  lower dimensional latent state  planner modules  meta-learning strategy  adversarial domain transfer  simulated environments  similarly distributed latent representation  fine tuning  encoder + planner  planning performances  navigation tasks  unlabelled random images  Robots  Data models  Planning  Task analysis  Trajectory  Training  Navigation 
Abstract: Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage simulation and off-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is first trained through a meta-learning strategy in simulation. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with only a few real world expert demonstrations, we show successful planning performances in different navigation tasks.


Title: Fast Stochastic Functional Path Planning in Occupancy Maps
Key Words: computational complexity  Gaussian processes  optimisation  path planning  robots  sampling methods  trajectory control  occupancy map  stochastic trajectory optimiser  Gaussian process path representation  trajectory optimisation  fast stochastic functional path planning  path planners  highly expressive path representation  sampling-based planners  fully defined artificial potential field  partially observed model  cubic complexity  kernel approximation  computational complexity  stochastic sampling  sampling-based methods  Planning  Optimization  Trajectory  Kernel  Robots  Stochastic processes 
Abstract: Path planners are generally categorised as either trajectory optimisers or sampling-based planners. The latter is the predominant planning paradigm for occupancy maps. Most trajectory optimisers require a fully defined artificial potential field for planning and cannot incorporate updates from a partially observed model such as an occupancy map. A stochastic trajectory optimiser capable of planning over occupancy map was presented in [1]. However, its scalability is limited by the cubic complexity of the Gaussian process path representation. In this work, we introduce a novel highly expressive path representation based on kernel approximation to perform trajectory optimisation over occupancy maps. This approach reduces the computational complexity to a fixed cost that only depends on the number of features. We show that stochastic sampling is crucial for planning in occupancy maps and present comparisons to other state-of-the-art planning methods, using simulated and real occupancy data. These experiments demonstrate the significant reduction in runtime, resulting in performance comparable to or better than sampling-based methods.


Title: Door opening and traversal with an industrial cartesian impedance controlled mobile robot
Key Words: convolutional neural nets  image colour analysis  image segmentation  manipulator dynamics  mobile robots  robot vision  handle frame  manipulator  inner controller loops  door opening  industrial cartesian impedance  mobile robot  holistic approach  door model  door handle detection  convolutional neural network-based architecture  handle shapes  detection rate  door plane  control structure  Robot kinematics  Impedance  Task analysis  Three-dimensional displays  Robot sensing systems  End effectors 
Abstract: This paper presents a holistic approach for door opening with a cartesian impedance controlled mobile robot, a KUICA KMR iiwa. Based on a given map of the environment, the robot autonomously detects the door handle, opens doors and traverses doorways without knowledge of a door model or the door's geometry. The door handle detection uses a convolutional neural network (CNN)-based architecture to obtain the handle's bounding box in a RGB image that works robustly for various handle shapes and colors. We achieve a detection rate of 100% for an evaluation set of 38 different door handles, by always selecting for highest confidence score. Registered depth data segmentation defines the door plane to construct a handle coordinate frame. We introduce a control structure based on the task frame formalism that uses the handle frame for reference in an outer loop for the manipulator's impedance controller. It runs in soft real-time on an external computer with approximately 20 Hz since access to inner controller loops is not available for the KMR iiwa. With the approach proposed in this paper, the robot successfully opened and traversed for 22 out of 25 trials at five different doors.


Title: Turn-minimizing multirobot coverage
Key Words: minimisation  mobile robots  multi-robot systems  path planning  travelling salesman problems  turn-minimizing multirobot coverage  identical robots  mission time  robot  multiple travelling salesperson problem  coverage plans  robotic vacuum  turn minimization  coverage time  path planning  partitioning heuristic  coverage plan  multirobot coverage  Robots  Tools  Turning  Minimization  Shape  Merging  Planning 
Abstract: Multirobot coverage is the problem of planning paths for several identical robots such that the combined regions traced out by the robots completely cover their environment. We consider the problem of multirobot coverage with the objective of minimizing the mission time, which depends on the number of turns taken by the robots. To solve this problem, we first partition the environment into ranks which are long thin rectangles the width of the robot's coverage tool. Our novel partitioning heuristic produces a set of ranks which minimizes the number of turns. Next, we solve a variant of the multiple travelling salesperson problem (m-TSP) on the set of ranks to minimize the robots' mission time. The resulting coverage plan is guaranteed to cover the entire environment. We present coverage plans for a robotic vacuum using real maps of 25 indoor environments and compare the solutions to paths planned without the objective of minimizing turns. Turn minimization reduced the number of turns by 6.7% and coverage time by 3.8% on average for teams of 1-5 robots.


Title: Learned Map Prediction for Enhanced Mobile Robot Exploration
Key Words: information theory  learning (artificial intelligence)  mobile robots  neural nets  autonomous ground robot  geometric heuristics  information theory  deep learning  mobile robot exploration  generative neural network  2D maps  reinforcement learning  robots behavior  Deep learning  Robot sensing systems  Decoding  Gain measurement  Navigation  Training 
Abstract: We demonstrate an autonomous ground robot capable of exploring unknown indoor environments for reconstructing their 2D maps. This problem has been traditionally tackled by geometric heuristics and information theory. More recently, deep learning and reinforcement learning based approaches have been proposed to learn exploration behavior in an end-to-end manner. We present a method that combines the strengths of these different approaches. Specifically, we employ a state-of-the-art generative neural network to predict unknown regions of a partially explored map, and use the prediction to enhance the exploration in an information-theoretic manner. We evaluate our system in simulation using floor plans of real buildings. We also present comparisons with traditional methods which demonstrate the advantage of our method in terms of exploration efficiency. We retain an advantage over end-to-end learned exploration methods in that the robot's behavior is easily explicable in terms of the predicted map.


Title: MH-iSAM2: Multi-hypothesis iSAM using Bayes Tree and Hypo-tree
Key Words: Bayes methods  data structures  mobile robots  optimisation  SLAM (robots)  original Bayes tree  hypothesis pruning strategy  multihypothesis iSAM  nonlinear incremental optimization algorithm  MH-iSAM2  simultaneous localization and mapping problems  SLAM problems  hypo-tree  multihypothesis inference  data structures  Simultaneous localization and mapping  Zirconium  Maximum likelihood estimation  Optimization  Inference algorithms  Robustness 
Abstract: A novel nonlinear incremental optimization algorithm MH-iSAM2 is developed to handle ambiguity in simultaneous localization and mapping (SLAM) problems in a multi-hypothesis fashion. It can output multiple possible solutions for each variable according to the ambiguous inputs, which is expected to greatly enhance the robustness of autonomous systems as a whole. The algorithm consists of two data structures: an extension of the original Bayes tree that allows efficient multi-hypothesis inference, and a Hypo-tree that is designed to explicitly track and associate the hypotheses of each variable as well as all the inference processes for optimization. With our proposed hypothesis pruning strategy, MH-iSAM2 enables fast optimization and avoids the exponential growth of hypotheses. We evaluate MH-iSAM2 using both simulated datasets and real-world experiments, demonstrating its improvements on the robustness and accuracy of SLAM systems.


Title: Improving Keypoint Matching Using a Landmark-Based Image Representation
Key Words: convolutional neural nets  image matching  image representation  learning (artificial intelligence)  landmark-based image representation  visual loop closure verification  multiview geometry  MVG  deep learning  convolutional neural network features  matched landmark pairs  image representation  verification method  keypoint matching method  ConvNet features  Lighting  Proposals  Visualization  Standards  Feature extraction  Simultaneous localization and mapping  Image representation 
Abstract: Motivated by the need to improve the performance of visual loop closure verification via multi-view geometry (MVG) under significant illumination and viewpoint changes, we propose a keypoint matching method that uses landmarks as an intermediate image representation in order to leverage the power of deep learning. In environments with various changes, the traditional verification method via MVG may encounter difficulty because of their inability to generate a sufficient number of correctly matched keypoints. Our method exploits the excellent invariance properties of convolutional neural network (ConvNet) features, which have shown outstanding performance for matching landmarks between images. By generating and matching landmarks first in the images and then matching the keypoints within the matched landmark pairs, we can significantly improve the quality of matched keypoints in terms of precision and recall measures. The proposed method is validated on challenging datasets that involve significant illumination and viewpoint changes, to establish its superior performance to the standard keypoint matching method.


Title: Fast and Robust Initialization for Visual-Inertial SLAM
Key Words: accelerometers  gyroscopes  inertial navigation  mobile robots  robot vision  SLAM (robots)  visual-inertial SLAM  VI-SLAM  gyroscope  initialization method  visual-inertial bundle adjustment  Cameras  Gravity  Simultaneous localization and mapping  Observability  Feature extraction  Jacobian matrices  Accelerometers 
Abstract: Visual-inertial SLAM (VI-SLAM) requires a good initial estimation of the initial velocity, orientation with respect to gravity and gyroscope and accelerometer biases. In this paper we build on the initialization method proposed by Martinelli [1] and extended by Kaiser et al. [2], modifying it to be more general and efficient. We improve accuracy with several rounds of visual-inertial bundle adjustment, and robustify the method with novel observability and consensus tests, that discard erroneous solutions. Our results on the EuRoC dataset show that, while the original method produces scale errors up to 156%, our method is able to consistently initialize in less than two seconds with scale errors around 5%, which can be further reduced to less than 1% performing visual-inertial bundle adjustment after ten seconds.


Title: Efficient Constellation-Based Map-Merging for Semantic SLAM
Key Words: covariance matrices  SLAM (robots)  tree searching  local information  SLAM graph  expensive recovery  system covariance matrix  joint compatibility  search space  clique-based pairwise compatibility  robust object-based loop-closure  SLAM problems  semantic SLAM  data association  high-confidence loop-closure mechanism  object-level SLAM  landmark uncertainty  constellation-based map-merging  branch-and-bound max-cardinality search  Simultaneous localization and mapping  Semantics  Covariance matrices  Uncertainty  Detectors  Search problems  Current measurement 
Abstract: Data association in SLAM is fundamentally challenging, and handling ambiguity well is crucial to achieve robust operation in real-world environments. When ambiguous measurements arise, conservatism often mandates that the measurement is discarded or a new landmark is initialized rather than risking an incorrect association. To address the inevitable “duplicate” landmarks that arise, we present an efficient map-merging framework to detect duplicate constellations of landmarks, providing a high-confidence loop-closure mechanism well-suited for object-level SLAM. This approach uses an incrementally-computable approximation of landmark uncertainty that only depends on local information in the SLAM graph, avoiding expensive recovery of the full system covariance matrix. This enables a search based on geometric consistency (GC) (rather than full joint compatibility (JC)) that inexpensively reduces the search space to a handful of “best” hypotheses. Furthermore, we reformulate the commonly-used interpretation tree to allow for more efficient integration of clique-based pairwise compatibility, accelerating the branch-and-bound max-cardinality search. Our method is demonstrated to match the performance of full JC methods at significantly-reduced computational cost, facilitating robust object-based loop-closure over large SLAM problems.


Title: Closed-loop MPC with Dense Visual SLAM - Stability through Reactive Stepping
Key Words: approximation theory  closed loop systems  humanoid robots  legged locomotion  mobile robots  motion control  nonlinear control systems  path planning  pendulums  position control  predictive control  SLAM (robots)  stability  fundamental capacity  external inputs  reference velocity  footstep plans  reference motion  external disturbances  closed-loop MPC scheme  proprioceptive sensors  imperfect open-loop control execution  HRP-4 humanoid robot  reactive stepping  walking gaits  humanoid locomotion  model predictive control  pendulum  dense visual SLAM stability  Simultaneous localization and mapping  Foot  Legged locomotion  Humanoid robots  Lips 
Abstract: Walking gaits generated using Model Predictive Control (MPC) is widely used due to its capability to handle several constraints that characterize humanoid locomotion. The use of simplified models such as the Linear Inverted Pendulum allows to perform computations in real-time, giving the robot the fundamental capacity to replan its motion to follow external inputs (e.g. reference velocity, footstep plans). However, usually the MPC does not take into account the current state of the robot when computing the reference motion, losing the ability to react to external disturbances. In this paper a closed-loop MPC scheme is proposed to estimate the robot's real state through Simultaneous Localization and Mapping (SLAM) and proprioceptive sensors (force/torque). With the proposed control scheme it is shown that the robot is able to react to external disturbances (push), by stepping to recover from the loss of balance. Moreover the localization allows the robot to navigate to target positions in the environment without being affected by the drift generated by imperfect open-loop control execution. We validate the proposed scheme through two different experiments with a HRP-4 humanoid robot.


Title: LookUP: Vision-Only Real-Time Precise Underground Localisation for Autonomous Mining Vehicles
Key Words: cameras  image sensors  mining  mining industry  mobile robots  neural nets  robot vision  autonomous underground mining vehicles  neural-network-based pixel sampling strategy  visual based technique  real-time accurate localisation system  range sensor-based system  ceiling-facing cameras  Optical imaging  Cameras  Optical sensors  Adaptive optics  Heating systems  Optical network units  Real-time systems 
Abstract: A key capability for autonomous underground mining vehicles is real-time accurate localisation. While significant progress has been made, currently deployed systems have several limitations ranging from dependence on costly additional infrastructure to failure of both visual and range-sensor-based techniques in highly aliased or visually challenging environments. In our previous work, we presented a lightweight coarse vision-based localisation system that could map and then localise to within a few metres in an underground mining environment. However, this level of precision is insufficient for providing a cheaper, more reliable vision-based automation alternative to current range sensor-based systems. Here we present a new precision localisation system dubbed “LookUP”, which learns a neural-network-based pixel sampling strategy for estimating homographies based on ceiling-facing cameras without requiring any manual labelling. This new system runs in real time on limited computation resource and is demonstrated on two different underground mine sites, achieving real time performance at ~5 frames per second and a much improved average localisation error of ~1.2 metre.


Title: EMG-Controlled Non-Anthropomorphic Hand Teleoperation Using a Continuous Teleoperation Subspace
Key Words: dexterous manipulators  electromyography  human-robot interaction  sensors  signal processing  telerobotics  EMG-controlled nonanthropomorphic hand teleoperation  continuous teleoperation subspace  EMG-driven teleoperation  EMG sensors  EMG signals  pose space  forearm EMG  robot hand  EMG teleoperation methods  nonanthropomorphic multiDOF robot hands  Electromyography  Aerospace electronics  Grippers  Wrist  Muscles  Teleoperators 
Abstract: We present a method for EMG-driven teleoperation of non-anthropomorphic robot hands. EMG sensors are appealing as a wearable, inexpensive, and unobtrusive way to gather information about the teleoperator's hand pose. However, mapping from EMG signals to the pose space of a non-anthropomorphic hand presents multiple challenges. We present a method that first projects from forearm EMG into a subspace relevant to teleoperation. To increase robustness, we use a model which combines continuous and discrete predictors along different dimensions of this subspace. We then project from the teleoperation subspace into the pose space of the robot hand. Our method is effective and intuitive, as it enables novice users to teleoperate pick and place tasks faster and more robustly than state-of-the-art EMG teleoperation methods when applied to a non-anthropomorphic, multi-DOF robot hand.


Title: Transferring Grasp Configurations using Active Learning and Local Replanning
Key Words: dexterous manipulators  image segmentation  learning (artificial intelligence)  path planning  robot vision  grasp configurations  active learning  prior example objects  similar shapes  geometric shape characteristics  semantic shape characteristics  grasp space  model parts  corresponding grasps  local replanning  point cloud  robotic hands  Shape  Grasping  Semantics  Three-dimensional displays  Robots  Stability analysis  Particle swarm optimization 
Abstract: We present a new approach to transfer grasp configurations from prior example objects to novel objects. We assume the novel and example objects have the same topology and similar shapes. We perform 3D segmentation on these objects using geometric and semantic shape characteristics. We compute a grasp space for each part of the example object using active learning. We build bijective contact mapping between these model parts and compute the corresponding grasps for novel objects. Finally, we assemble the individual parts and use local replanning to adjust grasp configurations while maintaining its stability and physical constraints. Our approach is general, can handle all kind of objects represented using mesh or point cloud and a variety of robotic hands.


Title: Robot Localization Based on Aerial Images for Precision Agriculture Tasks in Crop Fields
Key Words: autonomous aerial vehicles  crops  data visualisation  feature extraction  mobile robots  path planning  robot vision  SLAM (robots)  robot localization  aerial images  precision agriculture tasks  crop field environment  visual aliasing  localization system  aerial map  visual ambiguity problem  autonomous robots  Agriculture  Feature extraction  Cameras  Semantics  Visualization  Robot vision systems 
Abstract: Localization is a pre-requisite for most autonomous robots. For example, to carry out precision agriculture tasks effectively, a robot must be able to localize itself accurately in crop fields. The crop field environment presents unique challenges such as the highly repetitive structure of the crops leading to visual aliasing as well as the continuously changing appearance of the field, which makes it difficult to localize over time. In this paper, we present a localization system, which uses an aerial map of the field and exploits the semantic information of the crops, weeds, and their stem positions to resolve the visual ambiguity problem and to enable robot localization over extended periods of time. We evaluate our approach on a real field over multiple sessions spanning several weeks. Experiments suggest that our approach provides the necessary accuracy required by precision agriculture applications and works in cases where current techniques using typical visual features tend to fail.


Title: Visual Appearance Analysis of Forest Scenes for Monocular SLAM
Key Words: autonomous aerial vehicles  mobile robots  path planning  remotely operated vehicles  robot vision  SLAM (robots)  managed forests  tree health  SLAM research  structured human environments  unstructured forests  forest data  photorealistic simulated forest  straightforward forest terrain  forest scenes  natural scenes  visual appearance analysis  cheap energy efficient way  unmanned aerial vehicles  monocular SLAM systems  SLAM systems  visual appearance statistics  Forestry  Simultaneous localization and mapping  Cameras  Visualization  Vegetation  Feature extraction  Lighting 
Abstract: Monocular simultaneous localisation and mapping (SLAM) is a cheap and energy efficient way to enable Unmanned Aerial Vehicles (UAVs) to safely navigate managed forests and gather data crucial for monitoring tree health. SLAM research, however, has mostly been conducted in structured human environments, and as such is poorly adapted to unstructured forests. In this paper, we compare the performance of state of the art monocular SLAM systems on forest data and use visual appearance statistics to characterise the differences between forests and other environments, including a photorealistic simulated forest. We find that SLAM systems struggle with all but the most straightforward forest terrain and identify key attributes (lighting changes and in-scene motion) which distinguish forest scenes from “classic” urban datasets. These differences offer an insight into what makes forests harder to map and open the way for targeted improvements. We also demonstrate that even simulations that look impressive to the human eye can fail to properly reflect the difficult attributes of the environment they simulate, and provide suggestions for more closely mimicking natural scenes.


Title: An Approach for Semantic Segmentation of Tree-like Vegetation
Key Words: convolutional neural nets  forestry  image classification  image colour analysis  image fusion  image segmentation  learning (artificial intelligence)  vegetation  semantic segmentation  tree-like vegetation  pipeline  single RGB-D image  deep network  multiple convolutional neural network architectures  colour data  asynchronous training approach  3-channel HHA image  late fusion architecture  synthetic dataset  broadleaf trees  tree species  Vegetation  Image segmentation  Image color analysis  Semantics  Vegetation mapping  Training  Robots 
Abstract: This paper presents a pipeline for semantic segmentation of trees into their components. Given a single RGB-D image of a tree, we employ a deep network to predict labels to classify each pixel of the tree into trunk, branches, twigs and leaves. Multiple convolutional neural network architectures to combine the complementary modalities of depth and colour data are investigated. An asynchronous training approach where two networks trained separately on RGB and depth encoded as a 3-channel HHA image are combined using a late fusion architecture with different learning rates performs the best. Training and evaluation are performed on a synthetic dataset of 6 species of broadleaf trees. We further demonstrate the network's generalization capabilities, across various tree species on the synthetic dataset, achieving an accuracy of upto 92.5%. Furthermore, we present a qualitative evaluation of our approach on real-world data.


Title: Thermal Image Based Navigation System for Skid-Steering Mobile Robots in Sugarcane Crops*
Key Words: agricultural machinery  agriculture  crops  Global Positioning System  infrared imaging  mobile robots  path planning  robot vision  thermal image  navigation system  skid-steering mobile robots  sugarcane crops  autonomous navigation  sugarcane plantations  ordinary agricultural fields  sugarcane farms  row crop tunnels  low-cost skid-steering mobile robot  bioenergy farm  infrared thermal imaging  laser-based sensors  image analysis  navigation methodology  robot swarm  tankette for intelligent bioenergy agriculture  Agriculture  Robot kinematics  Navigation  Mobile robots  Computed tomography  Image color analysis 
Abstract: This work proposes a new strategy for autonomous navigation of mobile robots in sugarcane plantations based on thermal imaging. Unlike ordinary agricultural fields, sugarcane farms are generally vast and accommodates numerous arrangements of row crop tunnels, which are very tall, dense and hard-to-access. Moreover, sugarcane crops lie in harsh regions, which hinder the logistics for employing staff and heavy machinery for mapping, monitoring, and sampling. One solution for this problem is TIBA (Tankette for Intelligent BioEnergy Agriculture), a low-cost skid-steering mobile robot capable of infiltrating the crop tunnels with several sensing/sampling systems. The project concept is to reduce the product cost for making the deployment of a robot swarm feasible over a larger area. A prototype was built and tested in a bioenergy farm in order to improve the understanding of the environment and bring about the challenges for the next development steps. The major problem is the navigation through the crop tunnels, since most of the developed systems are suitable for open field operations and employ laser scanners and/or GPS/IMU, which in general are expensive technologies. In this context, we propose a low-cost solution based on infrared (IR) thermal imaging. IR cameras are simple and inexpensive devices, which do not pose risks to the user health, unlike laser-based sensors. This idea was highly motivated by the data collected in the field, which have shown a significant temperature difference between the ground and the crop. From the image analysis, it is possible to clearly visualize a distinguishable corridor and, consequently, generate a straight path for the robot to follow by using computationally efficient approaches. A rigorous analysis of the collected thermal data, numerical simulations and preliminary experiments in the real environment were included to illustrate the efficiency and feasibility of the proposed navigation methodology.


Title: Dynamic Obstacles Detection for Robotic Soil Explorations*
Key Words: collision avoidance  feedback  force sensors  mobile robots  complex environment  dynamic obstacles detection  6-axis force torque sensor  plant-inspired robot  soil exploration  complex environments  dynamic environments  robotic soil explorations  Soil  Robot sensing systems  Navigation  Force measurement  Shafts  Force 
Abstract: Nowadays, robots can navigate complex and dynamic environments such as air, water, and different terrain. However, moving into the underground, and especially into the soil, is still a challenge. Soil is a complex environment, and its exploration and monitoring is a crucial aspect in different engineering fields. Although some robotic solutions for mapping the soil are available, none of them can navigate into it. In this work, we propose a new solution for dynamic obstacles detection by embedding a 6-axis force torque sensor into a plant-inspired robot for soil exploration. We measured the forces acting on the apical part of the robot while it penetrates the soil by growing. We tested the system in different configurations, and at different depths. Results show that it is possible to identify the relative position of the obstacle before touching it with the robot. By using the proposed method as control feedback it is possible to move toward the development of novel robotic systems for navigating in complex and dynamic environments, such as the soil.


Title: Learning to Write Anywhere with Spatial Transformer Image-to-Motion Encoder-Decoder Networks
Key Words: affine transforms  handwritten character recognition  humanoid robots  image coding  image motion analysis  learning (artificial intelligence)  robot vision  DMP  digit drawings  image-to-motion encoder-decoder networks  convolutional layers  humanoid robot  affine transformed digits  fully differentiable overall network  spatial transformer  motion trajectories  digit images  robot vision  handwritten characters  Trajectory  Robots  Transforms  Task analysis  Writing  Neural networks  Decoding 
Abstract: Learning to recognize and reproduce handwritten characters is already a challenging task both for humans and robots alike, but learning to do the same thing for characters that can be transformed arbitrarily in space, as humans do when writing on a blackboard for instance, significantly ups the ante from a robot vision and control perspective. In previous work we proposed various different forms of encoder-decoder networks that were capable of mapping raw images of digits to dynamic movement primitives (DMPs) such that a robot could learn to translate the digit images into motion trajectories in order to reproduce them in written form. However, even with the addition of convolutional layers in the image encoder, the extent to which these networks are spatially invariant or equivariant is rather limited. In this paper, we propose a new architecture that incorporates both an image-to-motion encoder-decoder and a spatial transformer in a fully differentiable overall network that learns to rectify affine transformed digits in input images into canonical forms, before converting them into DMPs with accompanying motion trajectories that are finally transformed back to match up with the original digit drawings such that a robot can write them in their original forms. We present experiments with various challenging datasets that demonstrate the superiority of the new architecture compared to our previous work and demonstrate its use with a humanoid robot in a real writing task.


Title: Color-Coded Fiber-Optic Tactile Sensor for an Elastomeric Robot Skin
Key Words: cameras  robots  tactile sensors  contact localization  robotic perception system  force sensing range  color-coded tactile sensor  tactile exploration  robust tactile sensing  color-coded fiber-optic tactile sensor  elastomeric robot skin  artificial tactile skin  transparent silicone rubber  off-the-shelf color camera  camera POFs  Optical sensors  Tactile sensors  Image color analysis  Cameras 
Abstract: The sense of touch is essential for reliable mapping between the environment and a robot which interacts physically with objects. Presumably, an artificial tactile skin would facilitate safe interaction of the robots with the environment. In this work, we present our color-coded tactile sensor, incorporating plastic optical fibers (POF), transparent silicone rubber and an off-the-shelf color camera. Processing electronics are placed away from the sensing surface to make the sensor robust to harsh environments. Contact localization is possible thanks to the lower number of light sources compared to the number of camera POFs. Classical machine learning techniques and a hierarchical classification scheme were used for contact localization. Specifically, we generated the mapping from stimulation to sensation of a robotic perception system using our sensor. We achieved a force sensing range up to 18 N with the force resolution of around 3.6 N and the spatial resolution of 8 mm. The color-coded tactile sensor is suitable for tactile exploration and might enable further innovations in robust tactile sensing.


Title: Visual SLAM: Why Bundle Adjust?
Key Words: cameras  feature extraction  image sequences  motion estimation  optimisation  pose estimation  robot vision  SLAM (robots)  video signal processing  bundle adjustment  feature-based monocular SLAM  camera orientation optimisation  camera position estimation  quasiconvex formulation  keyframe rate  SLAM optimisation  rotational motion  slow motion  SLAM algorithm  3D structure estimation  input feature tracks  3D point cloud  3D map estimation  6DOF camera trajectory estimation  visual SLAM  Simultaneous localization and mapping  Cameras  Estimation  Bundle adjustment  Optimization  Visualization 
Abstract: Bundle adjustment plays a vital role in feature-based monocular SLAM. In many modern SLAM pipelines, bundle adjustment is performed to estimate the 6DOF camera trajectory and 3D map (3D point cloud) from the input feature tracks. However, two fundamental weaknesses plague SLAM systems based on bundle adjustment. First, the need to carefully initialise bundle adjustment means that all variables, in particular the map, must be estimated as accurately as possible and maintained over time, which makes the overall algorithm cumbersome. Second, since estimating the 3D structure (which requires sufficient baseline) is inherent in bundle adjustment, the SLAM algorithm will encounter difficulties during periods of slow motion or pure rotational motion. We propose a different SLAM optimisation core: instead of bundle adjustment, we conduct rotation averaging to incrementally optimise only camera orientations. Given the orientations, we estimate the camera positions and 3D points via a quasi-convex formulation that can be solved efficiently and globally optimally. Our approach not only obviates the need to estimate and maintain the positions and 3D map at keyframe rate (which enables simpler SLAM systems), it is also more capable of handling slow motions or pure rotational motions.


Title: Illumination Robust Monocular Direct Visual Odometry for Outdoor Environment Mapping
Key Words: distance measurement  image colour analysis  lighting  mobile robots  motion estimation  robot vision  SLAM (robots)  stereo image processing  illumination-robust direct monocular SLAM system  global lighting changes  local lighting changes  stereo SLAM systems  camera motion  scene structure  high-precision motion estimation  illumination robust monocular direct visual odometry  outdoor environment  illumination invariant photometric costs  vision-based localization and mapping  RGB-D  DSO system  ORBSLAM2 system  Lighting  Optimization  Robustness  Simultaneous localization and mapping  Cameras  Motion estimation  Three-dimensional displays 
Abstract: Vision-based localization and mapping in outdoor environments is still a challenging issue, which requests significant robustness against various unpredictable illumination changes. In this paper, an illumination-robust direct monocular SLAM system that focuses on modeling outdoor scenery is presented. To deal with global and local lighting changes, such as solar flares, the state-of-art illumination invariant photometric costs for RGB-D and stereo SLAM systems are revisited in the context of their monocular counterpart, where the camera motion and scene structure are jointly optimized with a reasonably poor initialization. Based on our analysis, a combined cost is proposed to achieve a high-precision motion estimation with an improved convergence radius. The proposed system is extensively evaluated on the synthetic and real-world datasets regarding accuracy, robustness, and processing time, where our approach outperforms systems with other costs and state-of-art DSO and ORBSLAM2 systems.


Title: A Comparison of CNN-Based and Hand-Crafted Keypoint Descriptors
Key Words: convolutional neural nets  image matching  neurocontrollers  robot vision  SLAM (robots)  pre-trained CNN descriptors  viewpoint changes  illumination changes  hand-crafted keypoint descriptors  keypoint matching  computer vision  keypoint description  trained convolutional neural networks  pre-trained CNNs  hand-crafted descriptors  visual simultaneous localization and mapping  CNN-based descriptors  SLAM  Lighting  Computational modeling  Detectors  Dogs  Simultaneous localization and mapping  Measurement 
Abstract: Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo condition changes such as those in illumination and viewpoint. Previous research in keypoint description has pursued three classes of descriptors: hand-crafted, those from trained convolutional neural networks (CNN), and those from pre-trained CNNs. This paper provides a comparative study of the three classes of keypoint descriptors, in terms of their ability to handle conditional changes. The study is conducted on the latest benchmark datasets in computer vision with challenging conditional changes. Our study finds that (a) in general CNN-based descriptors outperform hand-crafted descriptors, (b) the trained CNN descriptors perform better than pre-trained CNN descriptors with respect to viewpoint changes, and (c) pre-trained CNN descriptors perform better than trained CNN descriptors with respect to illumination changes. These findings can serve as a basis for selecting appropriate keypoint descriptors for various applications.


Title: Environment Driven Underwater Camera-IMU Calibration for Monocular Visual-Inertial SLAM
Key Words: autonomous underwater vehicles  calibration  cameras  inertial navigation  mobile robots  SLAM (robots)  visual-inertial SLAM  shallow water  calibration errors  environmental indexes  underwater camera-inertial measurement unit  simultaneous localization and mapping  intrinsic parameters  extrinsic parameters  underwater monocular vision systems  environment driven underwater camera-IMU calibration  Cameras  Calibration  Atmospheric modeling  Simultaneous localization and mapping  Geometry  Mathematical model 
Abstract: Most state-of-the-art underwater vision systems are calibrated manually in shallow water and used in open seas without changing. However, the refractivity of the water is adaptively changed depending on the salinity, temperature, depth or other underwater environmental indexes, which inevitably generate the calibration errors and induces incorrectness e.g., for underwater Simultaneously Localization and Mapping (SLAM). To address this issue, in this paper, we propose a new underwater Camera-Inertial Measurement Unit (IMU) calibration model, which just needs to be calibrated once in the air, and then both the intrinsic parameters and extrinsic parameters between the camera and IMU could be automatically calculated depending on the environment indexes. To our best knowledge, this is the first work to consider the underwater Camera-IMU calibration via environmental indexes. We also build a verification platform to validate the effectiveness of our proposed method on real experiments, and use it for underwater monocular Visual-Inertial SLAM.


Title: Leveraging Structural Regularity of Atlanta World for Monocular SLAM
Key Words: Kalman filters  maximum likelihood estimation  polynomials  SLAM (robots)  multiple local Atlanta frames  global Atlanta frames  world frame  structural regularity  monocular SLAM  common vertical axis  multiple horizontal axes  camera frame  Simultaneous localization and mapping  Three-dimensional displays  Cameras  Estimation  Optimization  Reliability 
Abstract: A wide range of man-made environments can be abstracted as the Atlanta world. It consists of a set of Atlanta frames with a common vertical (gravitational) axis and multiple horizontal axes orthogonal to this vertical axis. This paper focuses on leveraging the regularity of Atlanta world for monocular SLAM. First, we robustly cluster image lines. Based on these clusters, we compute the local Atlanta frames in the camera frame by solving polynomial equations. Our method provides the global optimum and satisfies inherent geometric constraints. Second, we define the posterior probabilities to refine the initial clusters and Atlanta frames alternately by the maximum a posteriori estimation. Third, based on multiple local Atlanta frames, we compute the global Atlanta frames in the world frame using Kalman filtering. We optimize rotations by the global alignment and then refine translations and 3D line-based map under the directional constraints. Experiments on both synthesized and real data have demonstrated that our approach outperforms state-of-the-art methods.


Title: Multimodal Semantic SLAM with Probabilistic Data Association
Key Words: image fusion  image representation  inference mechanisms  mobile robots  object detection  path planning  probability  robot vision  SLAM (robots)  nonGaussian sensor model  multimodal semantic SLAM  probabilistic data association  robot navigation  semantic SLAM problem  discrete inference problem  object class labels  measurement-landmark correspondences  continuous inference problem  robot poses  object detection systems  simultaneous localization and mapping  object-based representations  object locations  nonGaussian inference problem  Simultaneous localization and mapping  Semantics  Belief propagation  Maximum likelihood estimation  Maximum likelihood detection 
Abstract: The recent success of object detection systems motivates object-based representations for robot navigation; i.e. semantic simultaneous localization and mapping (SLAM). The semantic SLAM problem can be decomposed into a discrete inference problem: determining object class labels and measurement-landmark correspondences (the data association problem), and a continuous inference problem: obtaining the set of robot poses and object locations in the environment. A solution to the semantic SLAM problem necessarily addresses this joint inference, but under ambiguous data associations this is in general a non-Gaussian inference problem, while the majority of previous work focuses on Gaussian inference. Previous solutions to data association either produce solutions between potential hypotheses or maintain multiple explicit hypotheses for each association. We propose a solution that represents hypotheses as multiple modes of an equivalent non-Gaussian sensor model. We then solve the resulting non-Gaussian inference problem using nonparametric belief propagation. We validate our approach in a simulated hallway environment under a variety of sensor noise characteristics, as well as using real data from the KITTI dataset, demonstrating improved robustness to perceptual aliasing and odometry uncertainty.


Title: Improving Incremental Planning Performance through Overlapping Replanning and Execution
Key Words: path planning  robots  trajectory control  integrated motion planning  Chekov  trajectory optimization problems  roadmap seed trajectories  motion planning algorithms  control information  incremental planning  Planning  Dynamics  Mobile robots  Trajectory optimization  Collision avoidance 
Abstract: Deployment of motion planning algorithms in practical applications has lagged due to their slow speed in reacting to disturbances. We believe that the best way to address this is to reuse learned planning and control information across queries. In previous work, we introduced Chekov, a reactive, integrated motion planning and execution system that reuses learned information in the form of an enhanced roadmap. We have previously shown how we can use Chekov to formulate trajectory optimization problems that result in superior performance in static environments. In this work, we show how incremental planning can be incorporated into the formulation of optimized trajectories from roadmap seed trajectories. Further, we show how an incremental planner can be adapted to reduce the overhead incurred for replanning when trajectories become invalid during execution.


Title: Energy Gradient-Based Graphs for Planning Within-Hand Caging Manipulation
Key Words: dexterous manipulators  gradient methods  graph theory  grippers  mobile robots  path planning  hand-object configurations  Yale T42 hand  stored energy profile  energy gradient-based graph  energy map  low energy states  actuation input  underactuated hands  caging grasps  within-hand caging manipulation  Actuators  Computational modeling  Grippers  Force  Planning  Energy states  Trajectory 
Abstract: In this work, we present a within-hand manipulation approach that leverages a simple energy model based on caging grasps made by underactuated hands. Instead of explicitly modeling the contacts and dynamics in manipulation, we can calculate a map to describe the energy states of different hand-object configurations under an actuation input. Since the system intrinsically steers towards low energy states, the object's movement is uniquely described by the gradient of the energy map if the corresponding actuation is applied. Such maps are pre-calculated for a range of actuation inputs to represent the system's energy profile. We discretize the workspace into a grid and construct an energy gradient-based graph by locally exploring the gradients of the stored energy profile. Given a goal configuration of a simple cylindrical object, a sequence of actuation inputs can be calculated to manipulate it towards the goal by exploiting the connectivity in the graph. The proposed approach is experimentally implemented on a Yale T42 hand. Our evaluation results show that parts of the graph are well connected, explaining our ability to successfully plan and execute trajectories within the gripper's workspace.


Title: Prediction Maps for Real-Time 3D Footstep Planning in Dynamic Environments
Key Words: collision avoidance  humanoid robots  mobile robots  robot vision  humanoids  smaller wheeled robots  planar regions  simple 2D occupancy map  environment representation  height information  prediction maps  real-time 3D footstep planning  mobile robots  dynamic obstacle detection  time 10.0 ms  Three-dimensional displays  Tracking  Real-time systems  Mobile robots  Task analysis  Humanoid robots 
Abstract: Perception of the local environment is a precondition for mobile robots to navigate safely in dynamic environments. Most robots, i.e., humanoids and smaller wheeled robots rely on planar regions. For humanoids, a simple 2D occupancy map as environment representation on which a path is planned is hereby not sufficient since they can step over and onto objects and therefore need height information. Considering dynamic obstacles introduces another level of complexity, since they can lead to necessary replanning or collisions at later stages. In this paper, we present a framework that first extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be efficiently planned in real time at low CPU cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms for one computation cycle and allows for foresighted real-time 3D footstep planning.


Title: HD Map Change Detection with a Boosted Particle Filter
Key Words: image classification  learning (artificial intelligence)  object detection  particle filtering (numerical methods)  probability  road vehicles  traffic engineering computing  automated driving  landmark readings  probability distribution  HD map change detection  boosted particle filter  change detection algorithm  backend-based stream processing pipeline  floating car data  series-production vehicles  automotive high definition digital map  crowd-based approach  Roads  Global navigation satellite system  Measurement  Visualization  Automobiles  Robot sensing systems  Feature extraction 
Abstract: In this paper, we present a change detection algorithm that can run in real time as part of a backend-based stream processing pipeline. It can process the floating car data collected by series-production vehicles to detect changes in an automotive high definition digital (HD) map used for automated driving. The algorithm uses a particle filter approach with odometry, GNSS and landmark readings to localize the vehicle within the digital map. While all particles together represent the probability distribution for the vehicle's position at a given time, each individual particle also serves as a hypothesis about the vehicle's position. This is used to compute various metrics for how well the current sensor readings match the world model encoded in the HD map. The different metrics are evaluated by a number of weak classifiers that are used as input for a trained Adaboost classifier. The achievable detection rate of a single vehicle is then compared to that of a simple crowd-based approach, where each vehicle votes on whether or not the current section of the road has changed.


Title: Self-Supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment
Key Words: direction-of-arrival estimation  geometry  indoor environment  learning (artificial intelligence)  microphone arrays  mobile robots  geometry features  self-supervision process  ground truth label  pre-collected data  human supervisions  explicit GCC-PHAT features  supervised incremental learning  sound source localization  complex indoor environment  incremental learning framework  mobile robots  human sound source  microphone array  multiple rooms  training data  prediction model  incremental learning scheme  implicit acoustic features  training samples  direction-of-arrival estimation  Feature extraction  Robots  Acoustics  Predictive models  Indoor environment  Microphones  Data models 
Abstract: This paper presents an incremental learning framework for mobile robots localizing the human sound source using a microphone array in a complex indoor environment consisting of multiple rooms. In contrast to conventional approaches that leverage direction-of-arrival (DOA) estimation, the framework allows a robot to accumulate training data and improve the performance of the prediction model over time using an incremental learning scheme. Specifically, we use implicit acoustic features obtained from an auto-encoder together with the geometry features from the map for training. A self-supervision process is developed such that the model ranks the priority of rooms to explore and assigns the ground truth label to the collected data, updating the learned model on-the-fly. The framework does not require pre-collected data and can be directly applied to real-world scenarios without any human supervisions or interventions. In experiments, we demonstrate that the prediction accuracy reaches 67% using about 20 training samples and eventually achieves 90% accuracy within 120 samples, surpassing prior classification-based methods with explicit GCC-PHAT features.


Title: Oriented Point Sampling for Plane Detection in Unorganized Point Clouds
Key Words: image reconstruction  image segmentation  image sensors  object detection  octrees  robot vision  SLAM (robots)  solid modelling  unorganized point clouds  crucial pre-processing step  point cloud segmentation  organized point clouds  plane hypotheses  unoriented points  efficient plane detection method  semantic mapping  SLAM  plane detection methods  OPS  oriented point sampling  Three-dimensional displays  Sun  Surface treatment  Clustering algorithms  Octrees  Estimation  Image segmentation 
Abstract: Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efficiently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efficient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset.


Title: A Novel Multi-layer Framework for Tiny Obstacle Discovery
Key Words: collision avoidance  edge detection  feature extraction  mobile robots  probability  regression analysis  robot vision  tiny obstacle discovery  monocular image  obstacle-aware discovery method  multilayer regions  tiny obstacle proposals  multilayer framework  edge detection  missing contour recovery  visual cues  obstacle-aware occlusion edge maps  proposals extraction  obstacle occupied probability map  obstacle-aware regressor  Lost and Found dataset  Image edge detection  Proposals  Visualization  Roads  Cameras  Three-dimensional displays  Two dimensional displays 
Abstract: For tiny obstacle discovery in a monocular image, edge is a fundamental visual element. Nevertheless, because of various reasons, e.g., noise and similar color distribution with background, it is still difficult to detect the edges of tiny (b) obstacles at long distance. In this paper, we propose an obstacle-aware discovery method to recover the missing contours of these obstacles, which helps to obtain obstacle proposals as much as possible. First, by using visual cues in monocular images, several multi-layer regions are elaborately inferred to reveal the distances from the camera. Second, several novel obstacle-aware occlusion edge maps are constructed to well capture the contours of tiny obstacles, which combines cues from each layer. Third, to ensure the existence of the tiny obstacle proposals, the maps from all layers are used for proposals extraction. Finally, based on these proposals containing tiny obstacles, a novel obstacle-aware regressor is proposed to generate an obstacle occupied probability map with high confidence. The convincing experimental results with comparisons on the Lost and Found dataset demonstrate the effectiveness of our approach, achieving around 9.5% improvement on the accuracy than FPHT and PHT, it even gets comparable performance to MergeNet. Moreover, our method outperforms the state-of-the-art algorithms and significantly improves the discovery ability for tiny obstacles at long distance.


Title: DSNet: Joint Learning for Scene Segmentation and Disparity Estimation
Key Words: feature extraction  image coding  image matching  image segmentation  image sequences  learning (artificial intelligence)  semantic features  deep disparity features  semantic labels  scene segmentation  disparity estimation  scene semantics  optical flow estimation  depth information  dense depth maps  image frames  deep semantic information  disparity feature maps  independent encoding modules  semantic disparity information  multitasking architecture DSNet  ResNet encoding module  Semantics  Estimation  Task analysis  Feature extraction  Optical imaging  Training  Three-dimensional displays 
Abstract: Recently, research works have attempted the joint prediction of scene semantics and optical flow estimation, which demonstrate the mutual improvement between both tasks. Besides, the depth information is also indispensable for the scene understanding, and disparity estimation is necessary for outputting dense depth maps. Such task shares a great similarity with the optical flow estimation since they can all be cast into a problem of capturing the difference at a location of two image frames. However, as far as we know, currently there are few networks for the joint learning of semantic and disparity. Moreover, since deep semantic information and disparity feature maps can learn from each other, we find it unnecessary with two independent encoding modules to separately extract semantic and disparity features. Therefore, we propose a unified multi-tasking architecture DSNet, for the simultaneous estimation of semantic and disparity information. In our model, semantic features, extracted by the encoding module ResNet from the left and right images, are used to obtain the deep disparity features via a novel matching module which performs pixel-to-pixel matching. In addition, we also use the disparity map to perform warp operation on deep features of the right image to deal with the problem of lacking of semantic labels. The effectiveness of our method is demonstrated by extensive experiments.


Title: Spatial change detection using voxel classification by normal distributions transform
Key Words: image classification  image colour analysis  image sensors  mobile robots  normal distribution  object detection  optical scanners  robot vision  SLAM (robots)  stereo image processing  transforms  voxel classification  robotic applications  mobile robot  3D laser scanner  grid data  ND voxels  normal distributions transform  spatial change detection  onboard RGB-D camera  stereo camera  real-time range sensors  real-time localization  Three-dimensional displays  Mobile robots  Cameras  Real-time systems  Measurement by laser beam  Sensors 
Abstract: Detection of spatial change around a robot is indispensable in several robotic applications, such as search and rescue, security, and surveillance. The present paper proposes a fast spatial change detection technique for a mobile robot using an on-board RGB-D/stereo camera and a highly precise 3D map created by a 3D laser scanner. This technique first converts point clouds in a map and measured data to grid data (ND voxels) using normal distributions transform and classifies the ND voxels into three categories. The voxels in the map and the measured data are then compared according to the category and features of the ND voxels. Overlapping and voting techniques are also introduced in order to detect the spatial changes more robustly. We conducted experiments using a mobile robot equipped with real-time range sensors to confirm the performance of the proposed real-time localization and spatial change detection techniques in indoor and outdoor environments.


Title: Detection and Tracking of Small Objects in Sparse 3D Laser Range Data
Key Words: autonomous aerial vehicles  data structures  image segmentation  image sensors  laser ranging  median filters  mobile robots  object detection  object tracking  robot vision  solid modelling  autonomous behavior  microaerial vehicles  multiobject tracking  lightweight sensors  sparse point clouds  Velodyne VLP-16 sensor  MAV hardware  unlabeled data  sparse 3d laser range data  objects detection  median filters  data structure  Three-dimensional displays  Sensors  Target tracking  Object tracking  Real-time systems  Vehicle dynamics  Heuristic algorithms 
Abstract: Detection and tracking of dynamic objects is a key feature for autonomous behavior in a continuously changing environment. With the increasing popularity and capability of micro aerial vehicles (MAVs) efficient algorithms have to be utilized to enable multi object tracking on limited hardware and data provided by lightweight sensors. We present a novel segmentation approach based on a combination of median filters and an efficient pipeline for detection and tracking of small objects within sparse point clouds generated by a Velodyne VLP-16 sensor. We achieve real-time performance on a single core of our MAV hardware by exploiting the inherent structure of the data. Our approach is evaluated on simulated and real scans of in- and outdoor environments, obtaining results comparable to the state of the art. Additionally, we provide an application for filtering the dynamic and mapping the static part of the data, generating further insights into the performance of the pipeline on unlabeled data.


Title: GPS-Denied UAV Localization using Pre-existing Satellite Imagery
Key Words: artificial satellites  autonomous aerial vehicles  cameras  convolutional neural nets  distance measurement  Global Positioning System  mobile robots  object detection  robot vision  flight location  UAV imagery  image capturing conditions  localization accuracy  adjacent UAV frames  satellite map  GPS-denied flight  average localization error  GPS-denied UAV localization  onboard GPS system  noisy GPS signal  unreliable GPS signal  monocular RGB camera  convolutional neural network representations  satellite data  satellite imagery  unmanned aerial vehicles  distance 0.85 km  distance 0.2 km  Satellites  Global Positioning System  Unmanned aerial vehicles  Training  Meters  Imaging  Buildings 
Abstract: We present a method for localization of Unmanned Aerial Vehicles (UAVs) which is meant to replace an onboard GPS system in the event of a noisy or unreliable GPS signal. Our method requires only a downward-facing monocular RGB camera on the UAV, and pre-existing satellite imagery of the flight location to which the UAV imagery is compared and aligned. To overcome differences in the image capturing conditions between the satellite and UAV, such as seasonal and perspective changes, we propose the use of Convolutional Neural Network (CNN) representations trained on readily available satellite data. To increase localization accuracy, we also develop an optimization which jointly minimizes the error between adjacent UAV frames as well as the satellite map. We demonstrate how our method improves on recent systems from literature by achieving greater performance in flight environments with very few landmarks. For a GPS-denied flight at 0.2km altitude, over a flight distance of 0.85km, we achieve average localization error of less than 8 meters. We make our source code and datasets available to encourage further work on this emerging topic.


Title: An Autonomous Loop-Closure Approach for Simultaneous Exploration and Coverage of Unknown Infrastructure Using MAVs
Key Words: autonomous aerial vehicles  inspection  microrobots  mobile robots  spatial measurements  MAV motions  complete exploration  autonomous loop-closure approach  simultaneous exploration  unknown infrastructure  attractive means  critical infrastructure  autonomous tasks  precise spatial model  operational area  sensor measurements  autonomous inspection capabilities  autonomous MAV exploration  unknown structure  spatial information  high-fidelity 3D model  low-cost microaerial vehicles  Planning  Three-dimensional displays  Task analysis  Inspection  Simultaneous localization and mapping 
Abstract: The recent proliferation of low-cost Micro Aerial Vehicles (MAV) offers an attractive means for inspecting critical infrastructure autonomously. However, to enable such autonomous tasks requires a precise spatial model of the structure and operational area, typically constructed using sensor measurements obtained from the environment. To facilitate autonomous inspection capabilities, we address the problem of autonomous MAV exploration and coverage of an unknown structure to acquire the spatial information necessary for the development of a high-fidelity 3D model of the structure. Key to this problem is to not only cover the entire structure to acquire a complete set of spatial measurements, but also to minimize accumulative data errors during the exploration through direct planning of loop closures. We introduce a real-time waypoint planning approach to guide MAV motions to achieve complete exploration, coverage, and loop closure while respecting limited onboard resources.


Title: Reinforcement Learning on Variable Impedance Controller for High-Precision Robotic Assembly
Key Words: assembling  control engineering computing  force control  industrial robots  learning (artificial intelligence)  neural net architecture  position control  robotic assembly  wheels  high-precision robotic assembly  precise robotic manipulation skills  industrial settings  reinforcement learning methods  RL  perceived forces  high-precision tasks  proper operational space force controller  open-source Siemens Robot Learning Challenge  precise force-controlled behavior  delicate force-controlled behavior  variable impedance controller  Task analysis  Gears  Aerospace electronics  Robots  Reinforcement learning  Trajectory  Neural networks 
Abstract: Precise robotic manipulation skills are desirable in many industrial settings, reinforcement learning (RL) methods hold the promise of acquiring these skills autonomously. In this paper, we explicitly consider incorporating operational space force/torque information into reinforcement learning; this is motivated by humans heuristically mapping perceived forces to control actions, which results in completing high-precision tasks in a fairly easy manner. Our approach combines RL with force/torque information by incorporating a proper operational space force controller; where we also exploit different ablations on processing this information. Moreover, we propose a neural network architecture that generalizes to reasonable variations of the environment. We evaluate our method on the open-source Siemens Robot Learning Challenge, which requires precise and delicate force-controlled behavior to assemble a tight-fit gear wheel set.


Title: Tightly Coupled 3D Lidar Inertial Odometry and Mapping
Key Words: distance measurement  image fusion  mobile robots  motion estimation  optical radar  pose estimation  robot vision  SLAM (robots)  fast motion conditions  ego-motion estimation  mobile robotic applications  sensor fusion  stand-alone sensors  tightly coupled lidar-IMU fusion method  IMU measurements  lidarIMU odometry  lidar measurement  rotation-constrained refinement algorithm  LIO-mapping  sensor pair  IMU update rate  lidar pose estimation  Laser radar  Three-dimensional displays  Estimation  Robot sensing systems  Feature extraction  Optimization 
Abstract: Ego-motion estimation is a fundamental requirement for most mobile robotic applications. By sensor fusion, we can compensate the deficiencies of stand-alone sensors and provide more reliable estimations. We introduce a tightly coupled lidar-IMU fusion method in this paper. By jointly minimizing the cost derived from lidar and IMU measurements, the lidarIMU odometry (LIO) can perform well with considerable drifts after long-term experiment, even in challenging cases where the lidar measurement can be degraded. Besides, to obtain more reliable estimations of the lidar poses, a rotation-constrained refinement algorithm (LIO-mapping) is proposed to further align the lidar poses with the global map. The experiment results demonstrate that the proposed method can estimate the poses of the sensor pair at the IMU update rate with high precision, even under fast motion conditions or with insufficient features.


Title: Semantic Predictive Control for Explainable and Efficient Policy Learning
Key Words: image motion analysis  image representation  learning (artificial intelligence)  optimisation  predictive control  visual explanation  policy decisions  SPC  future semantic segmentation  multiscale feature maps  guidance model  multiple simulation environments  model-based reinforcement  data efficiency  short time horizons  human-level performance  complex environments  driving policy learning framework  feature representations  sampling-based optimization  semantic predictive control framework  Semantics  Predictive models  Feature extraction  Visualization  Task analysis  Predictive control  Optimization 
Abstract: Visual anticipation of ego and object motion over a short time horizons is a key feature of human-level performance in complex environments. We propose a driving policy learning framework that predicts feature representations of future visual inputs; our predictive model infers not only future events but also semantics, which provide a visual explanation of policy decisions. Our Semantic Predictive Control (SPC) framework predicts future semantic segmentation and events by aggregating multi-scale feature maps. A guidance model assists action selection and enables efficient sampling-based optimization. Experiments on multiple simulation environments show that networks which implement SPC can outperform existing model-based reinforcement learning algorithms in terms of data efficiency and total rewards while providing clear explanations for the policy's behavior.


Title: Point Cloud Compression for 3D LiDAR Sensor using Recurrent Neural Network with Residual Blocks
Key Words: computational geometry  data compression  image coding  iterative methods  mobile robots  octrees  optical radar  recurrent neural nets  SLAM (robots)  recurrent neural network  residual blocks  generic octree point cloud compression method  potential application scenarios  decompressed point cloud data  3D LiDAR sensor  autonomous driving systems  3D LiDAR data  raw D formatted LiDAR data  2D formatted LiDAR data  Three-dimensional displays  Image coding  Laser radar  Two dimensional displays  Robot sensing systems  Decoding  Recurrent neural networks 
Abstract: The use of 3D LiDAR, which has proven its capabilities in autonomous driving systems, is now expanding into many other fields. The sharing and transmission of point cloud data from 3D LiDAR sensors has broad application prospects in robotics. However, due to the sparseness and disorderly nature of this data, it is difficult to compress it directly into a very low volume. A potential solution is utilizing raw LiDAR data. We can rearrange the raw data from each frame losslessly in a 2D matrix, making the data compact and orderly. Due to the special structure of 3D LiDAR data, the texture of the 2D matrix is irregular, in contrast to 2D matrices of camera images. In order to compress this raw, 2D formatted LiDAR data efficiently, in this paper we propose a method which uses a recurrent neural network and residual blocks to progressively compress one frame's information from 3D LiDAR. Compared to our previous image compression based method and generic octree point cloud compression method, the proposed approach needs much less volume while giving the same decompression accuracy. Potential application scenarios for point cloud compression are also considered in this paper. We describe how decompressed point cloud data can be used with SLAM (simultaneous localization and mapping) as well as for localization using a given map, illustrating potential uses of the proposed method in real robotics applications.


Title: Depth Completion with Deep Geometry and Context Guidance
Key Words: computer vision  convolutional neural nets  feature extraction  learning (artificial intelligence)  object detection  bilateral weight  deep geometry  context guidance  geometry network  context network  single encoder-decoder network  initial propagated depth map  slanted surfaces  convolutional neural network  CNN-based depth completions  local feature extraction  global feature extraction  Three-dimensional displays  Geometry  Feature extraction  Image edge detection  Reliability  Laser radar  Convolution 
Abstract: In this paper, we present an end-to-end convolutional neural network (CNN) for depth completion. Our network consists of a geometry network and a context network. The geometry network, a single encoder-decoder network, learns to optimize a multi-task loss to generate an initial propagated depth map and a surface normal. The complementary outputs allow it to correctly propagate initial sparse depth points in slanted surfaces. The context network extracts a local and a global feature of an image to compute a bilateral weight, which enables it to preserve edges and fine details in the depth maps. At the end, a final output is produced by multiplying the initially propagated depth map with the bilateral weight. In order to validate the effectiveness and the robustness of our network, we performed extensive ablation studies and compared the results against state-of-the-art CNN-based depth completions, where we showed promising results on various scenes.


Title: Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera
Key Words: cameras  image colour analysis  image fusion  image sequences  optical radar  regression analysis  robot vision  semidense annotations  KITTI depth completion benchmark  self-supervised sparse-to-dense  self-supervised depth completion  dense depth image  sparse depth measurements  irregularly spaced pattern  multiple sensor modalities  dense level ground truth depth  pixel-level ground truth depth  self-supervised training framework  sparse depth images  color images  Laser radar  Training  Color  Robot sensing systems  Three-dimensional displays  Convolution  Extraterrestrial measurements 
Abstract: Depth completion, the technique of estimating a dense depth image from sparse depth measurements, has a variety of applications in robotics and autonomous driving. However, depth completion faces 3 main challenges: the irregularly spaced pattern in the sparse depth input, the difficulty in handling multiple sensor modalities (when color images are available), as well as the lack of dense, pixel-level ground truth depth labels for training. In this work, we address all these challenges. Specifically, we develop a deep regression model to learn a direct mapping from sparse depth (and color images) input to dense depth prediction. We also propose a self-supervised training framework that requires only sequences of color and sparse depth images, without the need for dense depth labels. Our experiments demonstrate that the self-supervised framework outperforms a number of existing solutions trained with semi-dense annotations. Furthermore, when trained with semi-dense annotations, our network attains state-of-the-art accuracy and is the winning approach on the KITTI depth completion benchmark at the time of submission.


Title: Integrated Mapping and Path Planning for Very Large-Scale Robotic (VLSR) Systems
Key Words: collision avoidance  control engineering computing  mobile robots  multi-robot systems  regression analysis  sensor fusion  decentralized approach  obstacle mapping  continuous probabilistic representation  mapping problem  binary classification task  kernel logistic regression  discriminative classifier online  individual robot maps  path planning algorithm  maximum information value  obstacle avoidance  VLSR system  prior obstacle information  robot paths  Hilbert map fusion method  large-scale robotic systems  onboard range sensors  Robot sensing systems  Entropy  Robot kinematics  Collision avoidance  Path planning 
Abstract: This paper develops a decentralized approach for mapping and information-driven path planning for Very Large Scale Robotic (VLSR) systems. In this approach, obstacle mapping is performed using a continuous probabilistic representation known as a Hilbert map, which formulates the mapping problem as a binary classification task and uses kernel logistic regression to train a discriminative classifier online. A novel Hilbert map fusion method is presented that quickly and efficiently combines the information from individual robot maps. An integrated mapping and path planning algorithm is presented to determine paths of maximum information value, while simultaneously performing obstacle avoidance. Furthermore, the effect of how percentage communication failure effects the overall performance of the system is investigated. The approach is demonstrated on a VLSR system with hundreds of robots that must map obstacles collaboratively over a large region of interest using onboard range sensors and no prior obstacle information. The results show that, through fusion and decentralized processing, the entropy of the map decreases over time and robot paths remain collision-free.


Title: Non-Gaussian SLAM utilizing Synthetic Aperture Sonar
Key Words: acoustic signal processing  array signal processing  graph theory  marine navigation  pose estimation  probability  sensor fusion  SLAM (robots)  synthetic aperture sonar  SLAM framework  beacon position  acoustic measurements  factor graph formulation  nonGaussian SLAM  spatial resolution  navigational measurements  underwater missions  synthetic aperture sonar  SAS  simultaneous localization and mapping  accurate pose estimation  hydrophones acoustic data  empirical probability distribution  conventional beamformer  autonomous surface vehicle  Acoustics  Synthetic aperture sonar  Array signal processing  Simultaneous localization and mapping  Apertures  Receivers  Sonar navigation 
Abstract: Synthetic Aperture Sonar (SAS) is a technique to improve the spatial resolution from a moving set of receivers by extending the array in time, increasing the effective array length and aperture. This technique is limited by the accuracy of the receiver position estimates, necessitating highly accurate, typically expensive aided-inertial navigation systems for submerged platforms. We leverage simultaneous localization and mapping to fuse acoustic and navigational measurements and obtain accurate pose estimates even without the benefit of absolute positioning for lengthy underwater missions. We demonstrate a method of formulating the well-known SAS problem in a SLAM framework, using acoustic data from hydrophones to simultaneously estimate platform and beacon position. An empirical probability distribution is computed from a conventional beamformer to correctly account for uncertainty in the acoustic measurements. The non-parametric method relieves the familiar Gaussian-only assumption currently used in the localization and mapping discipline and fits effectively into a factor graph formulation with conventional factors such as ground-truth priors and odometry. We present results from field experiments performed on the Charles River with an autonomous surface vehicle which demonstrate simultaneous localization of an unknown acoustic beacon and vehicle positioning, and provide comparison to GPS ground truths.


Title: Underwater Terrain Reconstruction from Forward-Looking Sonar Imagery
Key Words: feature extraction  Gaussian processes  graph theory  image reconstruction  mobile robots  remotely operated vehicles  SLAM (robots)  sonar imaging  terrain mapping  underwater vehicles  terrain reconstruction  forward-looking sonar imagery  underwater simultaneous localization  multibeam imaging sonar  3D terrain mapping tasks  elevation angle information  data association  accurate 3D mapping  Euclidean space  optical flow  bearing-range images  subsea terrain  Gaussian Process random field  terrain factors  factor graph  terrain elevation estimate  variable-elevation tank environment  smooth height estimate  sonar images  Chow-Liu tree  extracted feature tracking  Feature extraction  Sonar measurements  Simultaneous localization and mapping  Three-dimensional displays  Gaussian processes  Imaging 
Abstract: In this paper, we propose a novel approach for underwater simultaneous localization and mapping using a multibeam imaging sonar for 3D terrain mapping tasks. The high levels of noise and the absence of elevation angle information in sonar images present major challenges for data association and accurate 3D mapping. Instead of repeatedly projecting extracted features into Euclidean space, we apply optical flow within bearing-range images for tracking extracted features. To deal with degenerate cases, such as when tracking is interrupted by noise, we model the subsea terrain as a Gaussian Process random field on a Chow-Liu tree. Terrain factors are incorporated into the factor graph, aimed at smoothing the terrain elevation estimate. We demonstrate the performance of our proposed algorithm in a simulated environment, which shows that terrain factors effectively reduce estimation error. We also show ROV experiments performed in a variable-elevation tank environment, where we are able to construct a descriptive and smooth height estimate of the tank bottom.


Title: Learning Object Localization and 6D Pose Estimation from Simulation and Weakly Labeled Real Images
Key Words: image annotation  image classification  image segmentation  object detection  pose estimation  robot vision  unsupervised learning  6D pose estimation  computer vision models  object localization  multiple domain classifiers  synthetic images  object poses  time-consuming annotations  occluded scenes  cluttered scenes  weak object detector  deep learning approaches  cluttered environments  robust robotic grasping  Feature extraction  Pose estimation  Training  Robots  Heating systems  Solid modeling  Task analysis 
Abstract: Accurate pose estimation is often a requirement for robust robotic grasping and manipulation of objects placed in cluttered, tight environments, such as a shelf with multiple objects. When deep learning approaches are employed to perform this task, they typically require a large amount of training data. However, obtaining precise 6 degrees of freedom for ground-truth can be prohibitively expensive. This work therefore proposes an architecture and a training process to solve this issue. More precisely, we present a weak object detector that enables localizing objects and estimating their 6D poses in cluttered and occluded scenes. To minimize the human labor required for annotations, the proposed detector is trained with a combination of synthetic and a few weakly annotated real images (as little as 10 images per object), for which a human provides only a list of objects present in each image (no time-consuming annotations, such as bounding boxes, segmentation masks and object poses). To close the gap between real and synthetic images, we use multiple domain classifiers trained adversarially. During the inference phase, the resulting class-specific heatmaps of the weak detector are used to guide the search of 6D poses of objects. Our proposed approach is evaluated on several publicly available datasets for pose estimation. We also evaluated our model on classification and localization in unsupervised and semi-supervised settings. The results clearly indicate that this approach could provide an efficient way toward fully automating the training process of computer vision models used in robotics.


Title: Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints
Key Words: feature extraction  graph theory  mobile robots  motion control  object detection  path planning  road vehicles  robot vision  ground vehicle  odometric sensors  monocular visual sensors  stochastic constraint  odometric measurement processing  visual-odometric localization  mapping system  out-of SE(2) motion perturbation  SE(2)-XYZ constraint  image feature measurement  preintegration algorithm  graph optimization structure  Visualization  Optimization  Land vehicles  Perturbation methods  Robots  Jacobian matrices  Sensors 
Abstract: This paper focuses on the localization and mapping problem on ground vehicles using odometric and monocular visual sensors. To improve the accuracy of vision based estimation on ground vehicles, researchers have exploited the constraint of approximately planar motion, and usually implemented it as a stochastic constraint on an SE(3) pose. In this paper, we propose a simpler algorithm that directly parameterizes the ground vehicle poses on SE(2). The out-of SE(2) motion perturbations are not neglected, but incorporated into an integrated noise term of a novel SE(2)-XYZ constraint, which associates an SE(2) pose and a 3D landmark via the image feature measurement. For odometric measurement processing, we also propose an efficient preintegration algorithm on SE(2). Utilizing these constraints, a complete visual-odometric localization and mapping system is developed, in a commonly used graph optimization structure. Its superior performance in accuracy and robustness is validated by real-world experiments in industrial indoor environments.


Title: Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost
Key Words: control engineering computing  dexterous manipulators  learning (artificial intelligence)  neural nets  robot programming  deep reinforcement learning  multifingered hands  contact-rich manipulation behavior  model-free deep RL algorithms  complex multifingered manipulation skills  direct deep RL training  model-based control  dexterous manipulation  dexterous multifingered robotic hands  general-purpose robotic manipulators  autonomous control  complex intermittent contact interactions  Task analysis  Valves  Acceleration  Reinforcement learning  Robot sensing systems  Hardware 
Abstract: Dexterous multi-fingered robotic hands can perform a wide range of manipulation skills, making them an appealing component for general-purpose robotic manipulators. However, such hands pose a major challenge for autonomous control, due to the high dimensionality of their configuration space and complex intermittent contact interactions. In this work, we propose deep reinforcement learning (deep RL) as a scalable solution for learning complex, contact rich behaviors with multi-fingered hands. Deep RL provides an end-to-end approach to directly map sensor readings to actions, without the need for task specific models or policy classes. We show that contact-rich manipulation behavior with multi-fingered hands can be learned by directly training with model-free deep RL algorithms in the real world, with minimal additional assumption and without the aid of simulation. We learn to perform a variety of tasks on two different low-cost hardware platforms entirely from scratch, and further study how the learning can be accelerated by using a small number of human demonstrations. Our experiments demonstrate that complex multi-fingered manipulation skills can be learned in the real world in about 4-7 hours for most tasks, and that demonstrations can decrease this to 2-3 hours, indicating that direct deep RL training in the real world is a viable and practical alternative to simulation and model-based control. https:// sites.google.com/view/deeprl-handmanipulation.


Title: A Multi-modal Sensor Array for Safe Human-Robot Interaction and Mapping
Key Words: electric sensing devices  end effectors  force sensors  human-robot interaction  mobile robots  motion control  sensor arrays  multimodal sensor array  safe human-robot interaction  time-of-flight sensors  accidental contact detection  contact localization  force sensing  proximity sensing  proximity mapping  Hall effect  collaborative continuum robot  bracing constraint  admissible rolling motion  end effector  I2C communication network  Robot sensing systems  Force  Sensor arrays  Hall effect  Multiplexing  Robot perception  Collaborative robots  Continuum robots  Bracing  Mapping 
Abstract: In the future, human-robot interaction will include collaboration in close-quarters where the environment geometry is partially unknown. As a means for enabling such interaction, this paper presents a multi-modal sensor array capable of contact detection and localization, force sensing, proximity sensing, and mapping. The sensor array integrates Hall effect and time-of-flight (ToF) sensors in an I2C communication network. The design, fabrication, and characterization of the sensor array for a future in-situ collaborative continuum robot are presented. Possible perception benefits of the sensor array are demonstrated for accidental contact detection, mapping of the environment, selection of admissible zones for bracing, and constrained motion control of the end effector while maintaining a bracing constraint with an admissible rolling motion.


Title: Tactile Mapping and Localization from High-Resolution Tactile Imprints
Key Words: haptic interfaces  manipulators  tactile sensors  high-resolution tactile imprints  shape reconstruction  object localization  vision-based tactile sensor  local shapes  reconstructed objects  tactile sensing  tactile feedback  online object identification  Shape  Image reconstruction  Tactile sensors  Three-dimensional displays  Estimation 
Abstract: This work studies the problem of shape reconstruction and object localization using a vision-based tactile sensor, GelSlim. The main contributions are the recovery of local shapes from contact, an approach to reconstruct the tactile shape of objects from tactile imprints, and an accurate method for object localization of previously reconstructed objects. The algorithms can be applied to a large variety of 3D objects and provide accurate tactile feedback for in-hand manipulation. Results show that by exploiting the dense tactile information we can reconstruct the shape of objects with high accuracy and do on-line object identification and localization, opening the door to reactive manipulation guided by tactile sensing. We provide videos and supplemental information in the project's website web.mit.edu/mcube/research/tactile localization.html.


Title: Semantic mapping extension for OpenStreetMap applied to indoor robot navigation
Key Words: graph theory  mobile robots  path planning  OpenStreetMap  geometrical information  basic indoor structures  architectural principles  application-specific knowledge  graph-based map representation  hierarchical structure  semantic mapping extension  indoor robot navigation  grid-based motion planning algorithms  Robots  Semantics  Data models  Geometry  Navigation  Indoor environment  Tagging 
Abstract: In this work a graph-based, semantic mapping approach for indoor robotics applications is presented, which is extending OpenStreetMap (OSM) with robotic-specific, semantic, topological, and geometrical information. Models are introduced for basic indoor structures such as walls, doors, corridors, elevators, etc. The architectural principles support composition with additional domain and application-specific knowledge. As an example, a model for an area is introduced, and it is explained how this can be used in navigation. A key advantage of the proposed graph-based map representation is that it allows exploiting the hierarchical structure of the graphs. Finally, the compatibility of the approach with existing, grid-based motion planning algorithms is shown.


Title: KO-Fusion: Dense Visual SLAM with Tightly-Coupled Kinematic and Odometric Tracking
Key Words: cameras  distance measurement  image colour analysis  image fusion  image texture  manipulator kinematics  mobile robots  motion estimation  robot vision  SLAM (robots)  stereo image processing  KO-fusion  dense visual SLAM methods  observer  visual information  SLAM systems  inertial measurements  dense RGB-D SLAM system  wheeled robot  kinematic data  odometric data  kinematic measurements  tightly-coupled kinematic  odometric tracking  manipulator  odometry measurements  motion estimation  Cameras  Simultaneous localization and mapping  Robot vision systems  Kinematics  Manipulators 
Abstract: Dense visual SLAM methods are able to estimate the 3D structure of an environment and locate the observer within them. They estimate the motion of a camera by matching visual information between consecutive frames, and are thus prone to failure under extreme motion conditions or when observing texture-poor regions. The integration of additional sensor modalities has shown great promise in improving the robustness and accuracy of such SLAM systems. In contrast to the popular use of inertial measurements we propose to tightly-couple a dense RGB-D SLAM system with kinematic and odometry measurements from a wheeled robot equipped with a manipulator. The system has real-time capability while running on GPU. It optimizes the camera pose by considering the geometric alignment of the map as well as kinematic and odometric data from the robot. Through experimentation in the real-world, we show that the system is more robust to challenging trajectories featuring fast and loopy motion than the equivalent system without the additional kinematic and odometric knowledge, whilst retaining comparable performance to the equivalent RGB-D only system on easy trajectories.


Title: DeepFusion: Real-Time Dense 3D Reconstruction for Monocular SLAM using Single-View Depth and Gradient Predictions
Key Words: cameras  computerised instrumentation  graphics processing units  image fusion  image reconstruction  learning (artificial intelligence)  minimisation  neurocontrollers  photometry  probability  SLAM (robots)  stereo image processing  target tracking  depth cameras  CNN  DeepFusion  semidense multiview stereo algorithm  gradient predictions  monocular SLAM  single-view depth  keypoint-based maps  camera tracking  dense 3D reconstructions  convolutional neural network  sparse monocular simultaneous localisation and mapping systems  real-time dense 3D reconstruction system  photometric error minimization  GPU  probability  Image reconstruction  Uncertainty  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Real-time systems  Robot vision systems 
Abstract: While the keypoint-based maps created by sparse monocular Simultaneous Localisation and Mapping (SLAM) systems are useful for camera tracking, dense 3D reconstructions may be desired for many robotic tasks. Solutions involving depth cameras are limited in range and to indoor spaces, and dense reconstruction systems based on minimising the photometric error between frames are typically poorly constrained and suffer from scale ambiguity. To address these issues, we propose a 3D reconstruction system that leverages the output of a Convolutional Neural Network (CNN) to produce fully dense depth maps for keyframes that include metric scale. Our system, DeepFusion, is capable of producing real-time dense reconstructions on a GPU. It fuses the output of a semi-dense multiview stereo algorithm with the depth and gradient predictions of a CNN in a probabilistic fashion, using learned uncertainties produced by the network. While the network only needs to be run once per keyframe, we are able to optimise for the depth map with each new frame so as to constantly make use of new geometric constraints. Based on its performance on synthetic and real world datasets, we demonstrate that DeepFusion is capable of performing at least as well as other comparable systems.


Title: Dynamic Hilbert Maps: Real-Time Occupancy Predictions in Changing Environments
Key Words: Hilbert spaces  mobile robots  remotely operated vehicles  real-time occupancy predictions  static occupancy models  continuous occupancy map  high-dimensional feature space  data-efficient model  crowded unstructured outdoor environments  dynamic Hilbert maps  temporal dependencies  3D laser data  2D laser data  Vehicle dynamics  Predictive models  Uncertainty  Dynamics  Indexes  Real-time systems  Clustering algorithms 
Abstract: This paper addresses the problem of learning instantaneous occupancy levels of dynamic environments and predicting future occupancy levels. Due to the complexity of most real environments, such as urban streets or crowded areas, the efficient and robust incorporation of temporal dependencies into otherwise static occupancy models remains a challenge. We propose a method to capture the uncertainty of moving objects and incorporate this uncertainty information into a continuous occupancy map represented in a rich high-dimensional feature space. This data-efficient model not only allows us to learn the occupancy states incrementally, but also makes predictions about what the future occupancy states will be. Experiments performed using 2D and 3D laser data collected from crowded unstructured outdoor environments show that the proposed methodology can accurately predict occupancy states for areas of around 1000 m2 at 10 Hz, making the proposed framework ideal for online applications under real-time constraints.


Title: Evaluating the Effectiveness of Perspective Aware Planning with Panoramas
Key Words: image colour analysis  image sensors  object detection  path planning  robot vision  SLAM (robots)  binary coverage  goal selection strategy  image morphology  search space  CSQMI  perspective aware planning  information based exploration strategy  high resolution 3D maps  RGBD panoramas  angle enhanced occupancy grid  exploration strategy  maximal Cauchy-Schwarz quadratic mutual information  logging image control  Skeleton  Mutual information  Task analysis  Cameras  Robot vision systems 
Abstract: In this work, we present an information based exploration strategy tailored for the generation of high resolution 3D maps. We employ RGBD panoramas because they have been shown to provide memory efficient high quality representations of space. Robots explore the environment by selecting locations with maximal Cauchy-Schwarz Quadratic Mutual Information (CSQMI) computed on an angle enhanced occupancy grid to collect these RGBD panoramas. By employing the angle enhanced occupancy grid, the resulting exploration strategy emphasizes perspective in addition to binary coverage. Furthermore, the goal selection strategy is improved by using image morphology to reduce the search space over which CSQMI is computed. We present experimental results demonstrating the improved performance in perception related tasks by capturing panoramas using this approach, near frontier exploration, and a control of logging images at regular intervals while teleoperating the robot through the workspace. Collect imagery was passed through an object detection library with our perspective aware approach yielding a greater number of successful detections compared to near frontier exploration.


Title: Continuous Occupancy Map Fusion with Fast Bayesian Hilbert Maps
Key Words: Bayes methods  image fusion  mobile robots  multi-robot systems  SLAM (robots)  multirobot Hilbert Map systems  individual Fast-BHMs  decentralised manner  continuous representation  robot autonomy  traditional occupancy grid maps  continuous nature  continuous occupancy map fusion  fused fast-BHMs  global fast-BHM models  Bayesian Hilbert map models  fast Bayesian Hilbert maps  Bayes methods  Robots  Covariance matrices  Merging  Time complexity  Real-time systems  Training 
Abstract: Mapping the occupancy of an environment is central for robot autonomy. Traditional occupancy grid maps discretise the environment into independent cells, neglecting important spatial correlations, and are unable to capture the continuous nature of the real world. With these drawbacks of grid maps in mind, Hilbert Maps (HM) and more recently Bayesian Hilbert Maps (BHMs), were introduced as a continuous representation of the environment. In this paper we propose a method to merge Bayesian Hilbert Maps built by a team of robots in a decentralised manner. The training of BHMs requires the inversion of a large covariance matrix, incurring cubic complexity. We introduce an approximation, Fast Bayesian Hilbert Maps (Fast-BHM), which reduces the time complexity to below quadratic. Such speed-ups allow the building and merging of Bayesian Hilbert Map models to be practical, opening the door for multi-robot Hilbert Map systems which can be much faster and more robust than an individual robot. By merging several individual Fast-BHMs in a decentralised manner we obtain a unified model of the environment which is itself a Fast-BHM. We conduct experiments to show that global Fast-BHM models do not deteriorate after repeated merging and training. We then empirically demonstrate, due to its the compact representation, fused Fast-BHMs outperform fusion methods involving discretising continuous representations, when the amount of information communicated is limited.


Title: Detection-by-Localization: Maintenance-Free Change Object Detector
Key Words: feature extraction  image fusion  image retrieval  image segmentation  object detection  query processing  robot vision  object-level subimages  pixel-wise LoC maps  detection-by-localization scheme  ranking function  ranked list  ranking based self-localization model  unsupervised rank fusion  cross-season change detection  maintenance-free change object detector  likelihood-of-change  object-level change detection  generalized task  query image  subimagelevel pixel-wise LoC maps  publicly available North Campus Long-Term dataset  publicly available NCLT dataset  multimodal information retrieval  MMR  Image segmentation  Task analysis  Robots  Computational modeling  Visualization  Databases  Real-time systems 
Abstract: Recent researches demonstrate that selflocalization performance is a very useful measure of likelihood-of-change (LoC) for change detection. In this paper, this “detection-by-localization” scheme is studied in a novel generalized task of object-level change detection. In our framework, a given query image is segmented into object-level subimages (termed “scene parts”), which are then converted to subimagelevel pixel-wise LoC maps via the detection-by-localization scheme. Our approach models a self-localization system as a ranking function, outputting a ranked list of reference images, without requiring relevance score. Thanks to this new setting, we can generalize our approach to a broad class of selflocalization systems. We further propose an aggregation of different self-localization results from different queries so as to achieve higher precision. Our ranking based self-localization model allows to fuse self-localization results from different modalities via an unsupervised rank fusion derived from a field of multi-modal information retrieval (MMR). Our framework does not rely on the raw-score-merging hypothesis. Challenging experiments of cross-season change detection using the publicly available North Campus Long-Term (NCLT) dataset validates the efficacy of our proposed method.


Title: SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data
Key Words: image colour analysis  image segmentation  object detection  optical radar  SEG-VoxelNet  LiDAR data  RGB images  LiDAR point clouds  autonomous driving scenarios  semantic segmentation technique  3D LiDAR point cloud based detection  image semantic segmentation network  SEG-Net  improved-VoxelNet  semantic segmentation map  point cloud data  image semantic feature  KITTI 3D vehicle detection benchmark  Three-dimensional displays  Laser radar  Semantics  Vehicle detection  Feature extraction  Image segmentation  Two dimensional displays 
Abstract: This paper proposes a SEG-VoxelNet that takes RGB images and LiDAR point clouds as inputs for accurately detecting 3D vehicles in autonomous driving scenarios, which for the first time introduces semantic segmentation technique to assist the 3D LiDAR point cloud based detection. Specifically, SEG-VoxelNet is composed of two sub-networks: an image semantic segmentation network (SEG-Net) and an improved-VoxelNet. The SEG-Net generates the semantic segmentation map which represents the probability of the category for each pixel. The improved-VoxelNet is capable of effectively fusing point cloud data with image semantic feature and generating accurate 3D bounding boxes of vehicles. Experiments on the KITTI 3D vehicle detection benchmark show that our approach outperforms the methods of state-of-the-art.


Title: Analysis of 3D Position Control for a Multi-Agent System of Self-Propelled Agents Steered by a Shared, Global Control Input
Key Words: closed loop systems  controllability  matrix algebra  multi-agent systems  multi-robot systems  nonlinear control systems  position control  control laws  multiagent system  self-propelled agents  shared control input  global control input  3D multiagent position control  control inputs  rotation commands  rotation matrix  controllability results  2D case  Three-dimensional displays  Two dimensional displays  Orbits  Position control  Perturbation methods  Aerospace electronics  Robots 
Abstract: This paper investigates strategies for 3D multi-agent position control using a shared control input and self-propelled agents. The only control inputs allowed are rotation commands that rotate all agents by the same rotation matrix. In the 2D case, only two degrees-of-freedom (DOF) in position are controllable. We review controllability results in 2D, and then show that interesting things happen in 3D. We provide control laws for steering up to nine DOF in position, which can be mapped in various ways, including to control the x, y, z position of three agents, make four agents meet, or reduce the spread of n agents.


Title: Persistent Multi-Robot Mapping in an Uncertain Environment
Key Words: mobile robots  multi-agent systems  multi-robot systems  path planning  probability  multiagent spatio-temporal states  world model  persistent multirobot mapping  uncertain environment  constrained energy capacities  typical occupancy map approaches  static world  occupancy probability  grid cells  promotes revisitation  unchanging areas  naive planning  tractable subproblems  tractable computation time  Robots  Clustering algorithms  Indexes  Planning  Computational modeling  Sensors  Heuristic algorithms 
Abstract: This paper proposes a method to deploy teams of robots with constrained energy capacities to persistently maintain a map of an uncertain environment. Typical occupancy map approaches assume a static world; however, we introduce a decay in confidence that degrades the occupancy probability of grid cells and promotes revisitation. Further, sections of the map whose occupancy differs between observations are visited more frequently, while unchanging areas are scheduled less frequently. While naive planning is intractable through the entire space of multi-agent spatio-temporal states, the proposed algorithm decouples planning such that constraints are resolved separately by solving tracTable subproblems. We evaluate this approach in simulation and show how the uncertainty of our world model is maintained below an acceptable threshold while the algorithm retains a tractable computation time.


Title: Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks
Key Words: cameras  image matching  image sequences  motion estimation  object detection  unsupervised learning  video signal processing  monocular depth  multiple masks  unsupervised learning method  depth estimation network  ego-motion estimation network  projection target imaging plane  fine masks  image pixel mismatch  repeated masking  KITTI dataset  low-quality uncalibrated bike video dataset  Image reconstruction  Estimation  Cameras  Unsupervised learning  Training  Simultaneous localization and mapping 
Abstract: A new unsupervised learning method of depth and ego-motion using multiple masks from monocular video is proposed in this paper. The depth estimation network and the ego-motion estimation network are trained according to the constraints of depth and ego-motion without truth values. The main contribution of our method is to carefully consider the occlusion of the pixels generated when the adjacent frames are projected to each other, and the blank problem generated in the projection target imaging plane. Two fine masks are designed to solve most of the image pixel mismatch caused by the movement of the camera. In addition, some relatively rare circumstances are considered, and repeated masking is proposed. To some extent, the method is to use a geometric relationship to filter the mismatched pixels for training, making unsupervised learning more efficient and accurate. The experiments on KITTI dataset show our method achieves good performance in terms of depth and ego-motion. The generalization capability of our method is demonstrated by training on the low-quality uncalibrated bike video dataset and evaluating on KITTI dataset, and the results are still good.


Title: Belief Space Planning for Reducing Terrain Relative Localization Uncertainty in Noisy Elevation Maps
Key Words: astronomical image processing  digital elevation models  Global Positioning System  mobile robots  path planning  planetary rovers  robot vision  solid modelling  terrain mapping  belief space planning  noisy map data  elevation data  lunar orbital imagery  terrain relative localization uncertainty  noisy elevation  accurate global localization  operational risk  initial exploration missions  global position  terrain relative navigation  TRN  planetary rover-perspective images  digital elevation models  absolute positioning  orbital data  terrain features  GPS  satellite orbital imagery  Uncertainty  Planning  Noise measurement  Space vehicles  Navigation  Planetary orbits 
Abstract: Accurate global localization is essential for planetary rovers to reach mission goals and mitigate operational risk. For initial exploration missions, it is inappropriate to deploy GPS or build other infrastructure for navigating. One way of determining global position is to use terrain relative navigation (TRN). TRN compares planetary rover-perspective images and 3D models to existing satellite orbital imagery and digital elevation models (DEMs) for absolute positioning. However, TRN is limited by the quality of orbital data and the presence and uniqueness of terrain features. This work presents a novel combination of belief space planning with terrain relative navigation. Additionally, we introduce a new method for increasing the robustness of belief space planning to noisy map data. The new algorithm provides a statistically significant reduction in localization uncertainty when tested on elevation data produced from lunar orbital imagery.


Title: 2D3D-Matchnet: Learning To Match Keypoints Across 2D Image And 3D Point Cloud
Key Words: cameras  feature extraction  image classification  image matching  image representation  image retrieval  image sensors  learning (artificial intelligence)  object recognition  pose estimation  solid modelling  visual databases  image-based counterpart  visual pose estimation  2D-3D image  cloud correspondences  end-to-end deep network architecture  query image  3D point cloud reference map  Oxford 2D-3D Patches dataset  Oxford Robotcar dataset  ground truth camera pose  Three-dimensional displays  Two dimensional displays  Pose estimation  Visualization  Cameras  Training  Detectors 
Abstract: Large-scale point cloud generated from 3D sensors is more accurate than its image-based counterpart. However, it is seldom used in visual pose estimation due to the difficulty in obtaining 2D-3D image to point cloud correspondences. In this paper, we propose the 2D3D-MatchNet - an end-to-end deep network architecture to jointly learn the descriptors for 2D and 3D keypoint from image and point cloud, respectively. As a result, we are able to directly match and establish 2D-3D correspondences from the query image and 3D point cloud reference map for visual pose estimation. We create our Oxford 2D-3D Patches dataset from the Oxford Robotcar dataset with the ground truth camera poses and 2D-3D image to point cloud correspondences for training and testing the deep network. Experimental results verify the feasibility of our approach.


Title: Part Segmentation for Highly Accurate Deformable Tracking in Occlusions via Fully Convolutional Neural Networks
Key Words: convolutional neural nets  image filtering  image segmentation  learning (artificial intelligence)  object tracking  pose estimation  robot vision  stereo image processing  visual perception  direct pose estimation  machine learning  direct estimation techniques  geometric tracking methods  robotic applications  observed point clouds  segmentation maps  Fast-FCN network architecture  convolutional neural networks  Three-dimensional displays  Semantics  Computational modeling  Robots  Image segmentation  Computer architecture  Pose estimation 
Abstract: Successfully tracking the human body is an important perceptual challenge for robots that must work around people. Existing methods fall into two broad categories: geometric tracking and direct pose estimation using machine learning. While recent work has shown direct estimation techniques can be quite powerful, geometric tracking methods using point clouds can provide a very high level of 3D accuracy which is necessary for many robotic applications. However these approaches can have difficulty in clutter when large portions of the subject are occluded. To overcome this limitation, we propose a solution based on fully convolutional neural networks (FCN). We develop an optimized Fast-FCN network architecture for our application which allows us to filter observed point clouds and improve tracking accuracy while maintaining interactive frame rates. We also show that this model can be trained with a limited number of examples and almost no manual labelling by using an existing geometric tracker and data augmentation to automatically generate segmentation maps. We demonstrate the accuracy of our full system by comparing it against an existing geometric tracker, and show significant improvement in these challenging scenarios.


Title: Estimating the Localizability in Tunnel-like Environments using LiDAR and UWB
Key Words: Global Positioning System  inspection  mobile robots  optical radar  path planning  probability  robot vision  sensor fusion  tunnels  ultra wideband technology  UWB  inspection tasks  autonomous navigation technology  robot localization techniques  GPS-denied environments  onboard sensors  cameras  LiDAR  probabilistic sensor fusion method  tunnel-like environments  degeneration characterization model  ultra-wideband ranging radio  Robot sensing systems  Laser radar  Uncertainty  Probabilistic logic  Distance measurement 
Abstract: The application of robots in inspection tasks has been growing quickly thanks to the advancements in autonomous navigation technology, especially the robot localization techniques in GPS-denied environments. Although many methods have been proposed to localize a robot using onboard sensors such as cameras and LiDARs, achieving robust localization in geometrically degenerated environments, e.g. tunnels, remains a challenging problem. In this work, we focus on the robust localization problem in such situations. A novel degeneration characterization model is presented to estimate the localizability at a given location in the prior map. And the localizability of a LiDAR and an Ultra-Wideband (UWB) ranging radio is analyzed. Additionally, a probabilistic sensor fusion method is developed to combine IMU, LiDAR and the UWB. Experiment results show that this method allows for robust localization inside a long straight tunnel.


Title: Global Localization with Object-Level Semantics and Topology
Key Words: feature extraction  graph theory  image matching  image representation  pose estimation  stereo image processing  object-level representation  semantic object association  semantic-level point alignment  object-level semantics  appearance-based approach  3D dense semantics  semantic graph  vision-based global localization  topology  autonomous navigation  simultaneous localization and mapping  place recognition  6-DoF pose estimation  visual feature matching  Semantics  Three-dimensional displays  Topology  Simultaneous localization and mapping  Visualization  Cameras  Pose estimation 
Abstract: Global localization lies at the heart of autonomous navigation and Simultaneous Localization and Mapping (SLAM). The appearance-based approach has been successful, but still faces many open challenges in environments where visual conditions vary significantly over time. In this paper, we propose an integrated solution to leverage object-level dense semantics and spatial understanding of the environment for global localization. Our approach models an environment with 3D dense semantics, semantic graph and their topology. This object-level representation is then used for place recognition via semantic object association, followed by 6-DoF pose estimation by the semantic-level point alignment. Extensive experiments show that our approach can achieve robust global localization under extreme appearance changes. It is also capable of coping with other challenging scenarios, such as dynamic environments and incomplete query observations.


Title: MetaGrasp: Data Efficient Grasping by Affordance Interpreter Network
Key Words: dexterous manipulators  grippers  image colour analysis  inference mechanisms  learning (artificial intelligence)  robot vision  data efficient grasping  data-driven approach  training data  data collection  grasp training system  model inference  antipodal grasp rule  affordance map  ungraspability  grasp affordances  pixel-level affordance interpreter network  quantitative experiments  real-world grasp experiments  qualitative experiments  Grasping  Training  Data collection  Data models  Grippers  Robots  Deep learning 
Abstract: Data-driven approach for grasping shows significant advance recently. But these approaches usually require much training data. To increase the efficiency of grasping data collection, this paper presents a novel grasp training system including the whole pipeline from data collection to model inference. The system can collect effective grasp sample with a corrective strategy assisted by antipodal grasp rule, and we design an affordance interpreter network to predict pixelwise grasp affordance map. We define graspability, ungraspability and background as grasp affordances. The key advantage of our system is that the pixel-level affordance interpreter network trained with only a small number of grasp samples under antipodal rule can achieve significant performance on totally unseen objects and backgrounds. The training sample is only collected in simulation. Extensive qualitative and quantitative experiments demonstrate the accuracy and robustness of our proposed approach. In the real-world grasp experiments, we achieve a grasp success rate of 93% on a set of household items and 91% on a set of adversarial items with only about 6,300 simulated samples. We also achieve 87% accuracy in clutter scenario. Although the model is trained using only RGB image, when changing the background textures, it also performs well and can achieve even 94% accuracy on the set of adversarial objects, which outperforms current state-of-the-art methods.


Title: Safe and Efficient High Dimensional Motion Planning in Space-Time with Time Parameterized Prediction
Key Words: collision avoidance  human-robot interaction  manipulators  mobile robots  probability  trajectory control  human-robot collaborative environments  pre-planned path  6-joint manipulator  human hand  robot trajectories  obstacle-avoidance strategies  motion planning  lazy safe interval probabilistic roadmap  Planning  Robots  Trajectory  Prediction algorithms  Heuristic algorithms  Collision avoidance  Real-time systems 
Abstract: In this work, we propose an algorithm that can plan safe and efficient robot trajectories in real time, given time-parameterized motion predictions, in order to avoid fast-moving obstacles in human-robot collaborative environments. Our algorithm is able to reduce the robot configuration space and the time domain significantly by constructing a Lazy Safe Interval Probabilistic Roadmap based on a pre-planned path. The algorithm then plans efficient obstacle-avoidance strategies within the space-time roadmap. We benchmarked our algorithm by evaluating the performance of a simulated 6-joint manipulator attempting to avoid a quickly moving human hand, using a dataset collected from human experiments. We compared our algorithm's performance with those of 8 variations of prior state-of-the-art planners. Results from this empirical evaluation indicate that our method generated safe plans in 97.5% of the evaluated situations, achieved a planning speed 30 times faster than the benchmarked methods that planned in the time domain without space reduction, and accomplished the minimal solution execution time among the benchmarked planners with a similar planning speed.


Title: CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction
Key Words: feature extraction  motion estimation  probability  SLAM (robots)  SVO mapping results  frame rate camera motion estimation  map points  initialized map point  depth uncertainty  single-image depth prediction network  feature location  probabilistic mapping method  direct pixel correspondence  semidirect visual odometry  V-SLAM algorithms  visual simultaneous localization  Uncertainty  Cameras  Visual odometry  Feature extraction  Reliability  Estimation  Motion estimation 
Abstract: Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that improved SVO mapping results in increased robustness and camera tracking accuracy. The implementation of this work is available at https: //github.com/yan99033/CNN-SVO.


Title: MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM
Key Words: cameras  image colour analysis  image motion analysis  image segmentation  image sequences  object detection  object tracking  octrees  pose estimation  probability  robot vision  SLAM (robots)  video signal processing  robustly track  foreground object probabilities  object model  object-level dynamic volumetric map  instance segmentation part  octree-based object-level multiinstance dynamic SLAM  multiinstance dynamic RGB-D SLAM system  robust camera tracking  geometric motion properties  geometric motion information  object-oriented tracking method  camera pose estimation  semantic motion properties  frequency 2.0 Hz to 3.0 Hz  Cameras  Simultaneous localization and mapping  Tracking  Motion segmentation  Semantics  Dynamics  Measurement uncertainty 
Abstract: We propose a new multi-instance dynamic RGB-D SLAM system using an object-level octree-based volumetric representation. It can provide robust camera tracking in dynamic environments and at the same time, continuously estimate geometric, semantic, and motion properties for arbitrary objects in the scene. For each incoming frame, we perform instance segmentation to detect objects and refine mask boundaries using geometric and motion information. Meanwhile, we estimate the pose of each existing moving object using an object-oriented tracking method and robustly track the camera pose against the static scene. Based on the estimated camera pose and object poses, we associate segmented masks with existing models and incrementally fuse corresponding colour, depth, semantic, and foreground object probabilities into each object model. In contrast to existing approaches, our system is the first system to generate an object-level dynamic volumetric map from a single RGB-D camera, which can be used directly for robotic tasks. Our method can run at 2-3 Hz on a CPU, excluding the instance segmentation part. We demonstrate its effectiveness by quantitatively and qualitatively testing it on both synthetic and real-world sequences.


Title: Surfel-Based Dense RGB-D Reconstruction With Global And Local Consistency
Key Words: computer vision  image colour analysis  image reconstruction  optimisation  pose estimation  surfel-based dense RGB-D reconstruction  local consistency  high surface reconstruction accuracy  dense mapping  vision communities  robotics literature  RGB-D cameras  dense map  depth input  accurate local pose estimation  locally consistent model  pose tracking  offline computer vision methods  structure-from-motion  multiview stereo  batch optimization  global consistency  heavy computation loads  consistent reconstruction  offline SfM pipeline  strong global constraints  off-the-shelf SLAM systems  high local accuracy  factor graph optimization  accurate camera  dense reconstruction  dense SLAM systems  SfM-MVS pipelines  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Pose estimation  Image reconstruction  Geometry 
Abstract: Achieving high surface reconstruction accuracy in dense mapping has been a desirable target for both robotics and vision communities. In the robotics literature, simultaneous localization and mapping (SLAM) systems use RGB-D cameras to reconstruct a dense map of the environment. They leverage the depth input to provide accurate local pose estimation and a locally consistent model. However, drift in the pose tracking over time leads to misalignments and artifacts. On the other hand, offline computer vision methods, such as the pipeline that combines structure-from-motion (SfM) and multi-view stereo (MVS), estimate the camera poses by performing batch optimization. These methods achieve global consistency, but suffer from heavy computation loads. We propose a novel approach that integrates both methods to achieve locally and globally consistent reconstruction. First, we estimate poses of keyframes in the offline SfM pipeline to provide strong global constraints at relatively low cost. Afterwards, we compute odometry between frames driven by off-the-shelf SLAM systems with high local accuracy. We fuse the two pose estimations using factor graph optimization to generate accurate camera poses for dense reconstruction. Experiments on real-world and synthetic datasets demonstrate that our approach produces more accurate models comparing to existing dense SLAM systems, while achieving significant speedup with respect to state-of-the-art SfM-MVS pipelines.


Title: A-SLAM: Human in-the-loop Augmented SLAM
Key Words: augmented reality  human-robot interaction  mobile robots  navigation  path planning  real-time systems  SLAM (robots)  telerobotics  A-SLAM  map editing  navigation-forbidden areas  navigation goals  SLAM algorithm  occupancy grid maps  human in-the-loop augmented SLAM  real environment representation  Microsoft HoloLens  robot teleoperation  pose correction  map correction  AR interface  Simultaneous localization and mapping  Navigation  Collaboration  Three-dimensional displays  Task analysis 
Abstract: In this work, we are proposing an intuitive Augmented SLAM method (A-SLAM) that allows the user to interact, in real-time, with a robot running SLAM to correct for pose and map errors. We built an AR application that works on HoloLens and allows the operator to view the robot's map superposed on the physical environment and edit it. Through map editing, the operator can account for errors affecting real environment's representation by adding navigation-forbidden areas to the map in addition to the ability to correct errors affecting the localization. The proposed system allows the operator to edit the robot's pose (based on SLAM request) and can be extended to sending navigation goals to the robot, viewing the planned path to evaluate it before execution, and teleoperating the robot. The proposed solution could be applied on any 2D-based SLAM algorithm and can easily be extended to 3D SLAM techniques. We validated our system through experimentation on pose correction and map editing. Experiments demonstrated that through A-SLAM, SLAM runtime is cut to half, post-processing of maps is totally eliminated, and high quality occupancy grid maps could be achieved with minimal added computational and hardware costs.


Title: Balance Map Analysis as a Measure of Walking Balance Based on Pendulum-Like Leg Movements
Key Words: legged locomotion  linearisation techniques  motion control  nonlinear control systems  pendulums  state-space methods  balance map analysis  pendulum-like leg movements  swing legs  inverted pendulum  simple pendulum  linearization  nondimensionalization  compass gait model  energy ratio  phase difference  stance leg  swing leg  orbital energy conservation  step transition  balance loss  state space  reachability  walking balance  Legged locomotion  Trajectory  Orbits  Mathematical model  Compass  Computational modeling  Computer simulation 
Abstract: This paper proposes an analysis of walking balance in terms of movements of stance and swing legs based on an inverted pendulum and a simple pendulum. Linearization, decoupling, and non-dimensionalization of a compass gait model enable to characterize the relationship of the trajectories between the stance and swing legs by only two parameters (energy ratio and phase difference). The energy ratio is defined by the ratio of the orbital energy between the pendulums. The phase difference represents the position of the stance leg in relation to the swing leg. This study considers an orbital energy conservation of a step transition and analyzes reachability of a desirable touchdown condition. If the time evolution from a current state is not reachable to the desired touchdown region, the state is labeled as a state in balance loss. By analyzing the reachability limits of the energy ratio and phase difference, we illustrate the balance loss and safe regions on the state space of the inverted pendulum, which is termed as balance map. We examined the effects of the simplification and linearization of the compass gait model by using computer simulations. Through the simulations of walking with perturbations, we confirmed that the balance map analysis could predict a future fall in an early phase for even trajectories derived by the nonlinear model.


Title: Pose Graph optimization for Unsupervised Monocular Visual Odometry
Key Words: graph theory  neural nets  optimisation  pose estimation  unsupervised learning  pose graph optimization  unsupervised monocular visual odometry  unsupervised learning  label-free leaning ability  drift correction technique  large-scale odometry estimation  loop closure detection  hybrid VO system  NeuralBundler  temporal loss  spatial photometric loss  multiview 6DoF constraints  cycle consistency loss  global pose graph  local loop 6DoF constraints  KITTI odometry dataset  unsupervised monocular VO estimation  monocular SLAM systems  Optimization  Visual odometry  Simultaneous localization and mapping  Cameras  Training  Neural networks  Estimation 
Abstract: Unsupervised Learning based monocular visual odometry (VO) has lately drawn significant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for large-scale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler, and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efficient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.


Title: Probably Unknown: Deep Inverse Sensor Modelling Radar
Key Words: image segmentation  learning (artificial intelligence)  neural nets  object detection  optical radar  probability  radar computing  radar imaging  autonomous vehicle applications  weather conditions  raw radar power returns  sensor noise  occlusion  Inverse Sensor Model  grid map  grid cell  heteroscedastic uncertainty  deep Inverse Sensor modelling radar  sensor observation  model formulation  standard CFAR filtering approaches  dynamic urban environment  world occupancy  lidar  partial occupancy labels  deep neural network  occupancy probabilities  Uncertainty  Robot sensing systems  Laser radar  Training  Spaceborne radar  Neural networks 
Abstract: Radar presents a promising alternative to lidar and vision in autonomous vehicle applications, able to detect objects at long range under a variety of weather conditions. However, distinguishing between occupied and free space from raw radar power returns is challenging due to complex interactions between sensor noise and occlusion. To counter this we propose to learn an Inverse Sensor Model (ISM) converting a raw radar scan to a grid map of occupancy probabilities using a deep neural network. Our network is selfsupervised using partial occupancy labels generated by lidar, allowing a robot to learn about world occupancy from past experience without human supervision. We evaluate our approach on five hours of data recorded in a dynamic urban environment. By accounting for the scene context of each grid cell our model is able to successfully segment the world into occupied and free space, outperforming standard CFAR filtering approaches. Additionally by incorporating heteroscedastic uncertainty into our model formulation, we are able to quantify the variance in the uncertainty throughout the sensor observation. Through this mechanism we are able to successfully identify regions of space that are likely to be occluded.


Title: Uncertainty-Aware Occupancy Map Prediction Using Generative Networks for Robot Navigation
Key Words: mobile robots  multi-robot systems  neural net architecture  path planning  sensor field of view  sensor FOV  generated hypotheses  information-theoretic exploration strategy  combined map prediction  neural network architecture  custom loss function  deep neural networks  future robot motions  sensor data  occupancy map representations  biological systems  sensor field  future motion  robotic systems  robot navigation  generative networks  uncertainty-aware occupancy map prediction  Neural networks  Robot sensing systems  Training  Measurement  Navigation  Uncertainty 
Abstract: Efficient exploration through unknown environments remains a challenging problem for robotic systems. In these situations, the robot's ability to reason about its future motion is often severely limited by sensor field of view (FOV). By contrast, biological systems routinely make decisions by taking into consideration what might exist beyond their FOV based on prior experience. We present an approach for predicting occupancy map representations of sensor data for future robot motions using deep neural networks. We develop a custom loss function used to make accurate prediction while emphasizing physical boundaries. We further study extensions to our neural network architecture to account for uncertainty and ambiguity inherent in mapping and exploration. Finally, we demonstrate a combined map prediction and information-theoretic exploration strategy using the variance of the generated hypotheses as the heuristic for efficient exploration of unknown environments.


Title: Autonomous Exploration, Reconstruction, and Surveillance of 3D Environments Aided by Deep Learning
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  multi-robot systems  neural nets  autonomous exploration  surveillance  greedy learning approach  supervised learning approach  level set representation  convolutional neural network  visibility  on-line computational cost  topologically accurate maps  complex 3D environments  frontier-based strategies  potential vantage points  deep learning approaches  obstacle avoidance  local navigation  global exploration problem  3D urban environments  Training  Level set  Surveillance  Three-dimensional displays  Two dimensional displays  Sensors  Convolution 
Abstract: We propose a greedy and supervised learning approach for visibility-based exploration, reconstruction and surveillance. Using a level set representation, we train a convolutional neural network to determine vantage points that maximize visibility. We show that this method drastically reduces the on-line computational cost and determines a small set of vantage points that solve the problem. This enables us to efficiently produce highly-resolved and topologically accurate maps of complex 3D environments. Unlike traditional next-best-view and frontier-based strategies, the proposed method accounts for geometric priors while evaluating potential vantage points. While existing deep learning approaches focus on obstacle avoidance and local navigation, our method aims at finding near-optimal solutions to the more global exploration problem. We present realistic simulations on 2D and 3D urban environments.


Title: GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks
Key Words: cameras  convolutional neural nets  distance measurement  image colour analysis  image motion analysis  image sequences  pose estimation  unsupervised learning  unlabelled RGB image sequences  deep convolutional Generative Adversarial Networks  single-view depth generation network  unsupervised deep VO methods  unsupervised deep monocular visual odometry  depth estimation  supervised deep learning approaches  visual odometry applications  unsupervised deep learning approaches  VO research  generative unsupervised learning framework  multiview pose estimation  6-DoF pose camera motion  Image reconstruction  Pose estimation  Cameras  Deep learning  Training  Feature extraction  Gallium nitride 
Abstract: In the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI [1] and Cityscapes [2] datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.


Title: Adding Cues to Binary Feature Descriptors for Visual Place Recognition
Key Words: feature extraction  image retrieval  binary feature descriptors  visual place recognition  multidimensional continuous cues  feature descriptor  binary string  continuous cue  binary descriptor types  Hamming distance  Search problems  Quantization (signal)  Visualization  Measurement  Simultaneous localization and mapping  Image retrieval 
Abstract: In this paper we propose an approach to embed multi-dimensional continuous cues in binary feature descriptors used for visual place recognition. The embedding is achieved by extending each feature descriptor with a binary string that encodes a cue and supports the Hamming distance metric. Augmenting the descriptors in such a way has the advantage of being transparent to the procedure used to compare them. We present a concrete application of our methodology, demonstrating the considered type of continuous cue. Additionally, we conducted a broad quantitative and comparative evaluation on that application, covering five benchmark datasets and several state-of-the-art image retrieval approaches in combination with various binary descriptor types.


Title: The Robust Canadian Traveler Problem Applied to Robot Routing
Key Words: graph theory  mobile robots  stochastic processes  telecommunication network routing  worst-case cost  Robust Canadian Traveler Problem applied  stochastic Canadian Traveler Problem  CTP  robot route selection  traversal policy  policy cost  evaluation criteria  approximate algorithm  traversal cost  robot field trials  RCTP framework  sub-optimal policy alternatives  minimum expected cost  distance 5.0 km  Approximation algorithms  Robot sensing systems  Visualization  Search problems  Heuristic algorithms  Uncertainty 
Abstract: The stochastic Canadian Traveler Problem (CTP), which finds application in robot route selection under uncertainty, aims to find the traversal policy with the minimum expected cost. This paper extends the CTP to what we call the Robust Canadian Traveler Problem (RCTP), in which the variability of the policy cost is also part of the evaluation criteria. An optimal (offline) algorithm and an approximate (online) algorithm are then proposed to compute the policy that has a good balance of both mean and variation of the traversal cost. The benefit of the proposed framework versus traditional approaches is shown by doing simulations in randomly generated worlds as well as on a map of 5 km of paths built from robot field trials. Specifically, the RCTP framework is able to search for sub-optimal policy alternatives with significantly lower worst-case cost and less computational time compared to the optimal policy, but with little sacrifice on the expected cost.


Title: Optimization-Based Terrain Analysis and Path Planning in Unstructured Environments
Key Words: data structures  graph theory  mobile robots  navigation  optimisation  path planning  position control  remotely operated vehicles  terrain mapping  trees (mathematics)  path planning  environment representation  terrain modeling  graph edge expansions  optimization-based terrain analysis  unmanned ground vehicle  hierarchical model  local terrain map  graph search algorithms  vertex positions  compact data structure  space-dividing tree  environment model  rough environments  real-time optimization-based approach  unstructured environments  autonomous ground vehicle navigation  Optimization  Path planning  Planning  Data structures  Collision avoidance  Navigation  Robots 
Abstract: Accurate environment representation is one of the key challenges in autonomous ground vehicle navigation in unstructured environments. We propose a real-time optimization-based approach to terrain modeling and path planning in off-road and rough environments. Our method uses an irregular, hierarchical, graph-like environment model. A space-dividing tree is used to define a compact data structure capturing vertex positions and establishing connectivity. The same unique underlying data structure is used for both terrain modeling and path planning without memory reallocation. Local plans are generated by graph search algorithms and are continuously regenerated for on-the-fly obstacle avoidance inside the scope of the local terrain map. We show that implementing a hierarchical model over a regular space division reduces graph edge expansions by up to 84%. We illustrate the applicability of the method through experiments with an unmanned ground vehicle in both structured and unstructured environments.


Title: Towards the Design of Robotic Drivers for Full-Scale Self-Driving Racing Cars
Key Words: automobiles  control system synthesis  mobile robots  nonlinear control systems  path planning  predictive control  full-scale self-driving racing cars  autonomous vehicles  planning  control methods  autonomous racing cars  electric full scale autonomous racing car  control system architecture  localization methods  nonlinear model predictive control  pre-planned racing line  robotic driver design  Automobiles  Planning  Real-time systems  Computer architecture  Optimization  Vehicle dynamics 
Abstract: Autonomous vehicles are undergoing a rapid development thanks to advances in perception, planning and control methods and technologies achieved in the last two decades. Moreover, the lowering costs of sensors and computing platforms are attracting industrial entities, empowering the integration and development of innovative solutions for civilian use. Still, the development of autonomous racing cars has been confined mainly to laboratory studies and small to middle scale vehicles. This paper tackles the development of a planning and control framework for an electric full scale autonomous racing car, which is an absolute novelty in the literature, upon which we report our preliminary experiments and perspectives on future work. Our system leverages real time Nonlinear Model Predictive Control to track a pre-planned racing line. We describe the whole control system architecture including the mapping and localization methods employed.


Title: Learning ad-hoc Compact Representations from Salient Landmarks for Visual Place Recognition in Underwater Environments
Key Words: convolutional neural nets  feature extraction  image coding  image representation  mobile robots  object recognition  robot vision  SLAM (robots)  unsupervised learning  salient landmarks  visual place recognition  underwater environments  visual attention algorithm  hand-crafted local descriptors  ad hoc descriptor generator  convolutional autoencoder  ad-hoc compact representations  SeqSLAM  FAB-MAP  SURF method  Visualization  Feature extraction  Image color analysis  Training  Robots  Task analysis  Neural networks 
Abstract: In this paper, we propose an approach to learn compact representations from salient landmarks detected by a visual attention algorithm to recognize previously visited places in underwater environments. Instead of using hand-crafted local descriptors as it has been typically done in visual place recognition, we use a convolutional autoencoder to obtain an ad hoc descriptor generator from salient landmarks. The main advantage of using an autoencoder is that it can learn in an unsupervised manner directly from the salient landmarks. In addition, we show that it is possible to do the training with less than 100,000 examples instead of several hundreds of thousands or even millions of labeled examples as in other convolutional architectures. The trained convolutional autoencoder is used to obtain descriptors for salient landmarks that are later utilized in a voting scheme to calculate similarity between images with the objective of finding if a place has already been visited. The proposed method has obtained good results compared to SeqSLAM and FAB-MAP in different datasets obtained from robotic explorations of coral reefs in real life conditions. Moreover, when the visual attention algorithm is used, fewer features are required to get a good performance in terms of precision and recall compared when using the SURF method to extract visual features.


Title: Robotic Detection of Marine Litter Using Deep Visual Detection Models
Key Words: autonomous underwater vehicles  convolutional neural nets  learning (artificial intelligence)  marine engineering  marine pollution  mobile robots  neural net architecture  object detection  robotic detection  marine litter  deep visual detection models  trash deposits  aquatic environments  marine ecosystems  autonomous underwater vehicles  AUV  deep-learning algorithms  convolutional neural network architectures  object detection  trained networks  underwater trash removal  Plastics  Training  Oceans  Data models  Object detection  Visualization  Biological system modeling 
Abstract: Trash deposits in aquatic environments have a destructive effect on marine ecosystems and pose a long-term economic and environmental threat. Autonomous underwater vehicles (AUVs) could very well contribute to the solution of this problem by finding and eventually removing trash. This paper evaluates a number of deep-learning algorithms performing the task of visually detecting trash in realistic underwater environments, with the eventual goal of exploration, mapping, and extraction of such debris by using AUVs. A large and publicly-available dataset of actual debris in open-water locations is annotated for training a number of convolutional neural network architectures for object detection. The trained networks are then evaluated on a set of images from other portions of that dataset, providing insight into approaches for developing the detection capabilities of an AUV for underwater trash removal. In addition, the evaluation is performed on three different platforms of varying processing power, which serves to assess these algorithms' fitness for real-time applications.


Title: Real-time Model Based Path Planning for Wheeled Vehicles
Key Words: electric vehicles  image colour analysis  image sensors  mobile robots  path planning  pose estimation  road safety  road vehicles  robot vision  search problems  wheels  model based traversability analysis method  complex environments  vehicles 3D pose  chassis collision  elevation map  reactive planning  safe paths  wheeled mobile robots  real world environment setups  real-time model  real-time path planning  simulated world environment setups  wheeled vehicles  vehicle model  scoring function  A*-like search strategy  RGB-D sensor  frequency 30.0 Hz  Wheels  Path planning  Robot sensing systems  Planning  Mobile robots  Real-time systems 
Abstract: This work presents a model based traversability analysis method which employs a detailed vehicle model to perform real-time path planning in complex environments. The vehicle model represents the vehicle's wheels and chassis, allowing it to accurately predict the vehicles 3D pose, detailed contact information for each wheel and the occurrence of a chassis collision given a 2D pose on an elevation map. These predictions are weighted, depending on the safety requirements of the vehicle, to provide a scoring function for an A*-like search strategy. The proposed method is designed to run at frame rates of 30Hz on data from a RGB-D sensor to provide reactive planning of safe paths. For evaluation, two wheeled mobile robots in different simulated and real world environment setups were tested to show the reliability and performance of the proposed method.


Title: Integrity Risk-Based Model Predictive Control for Mobile Robots
Key Words: mobile robots  path planning  predictive control  risk management  localization sensors  mission-critical situations  local nearest neighbor integrity risk evaluation methodology  data association faults  localization safety  control-input constraints  mobile robots  navigation integrity risk  integrity risk-based model predictive control  MPC  Feature extraction  Safety  Technological innovation  Predictive models  Mobile robots  Navigation 
Abstract: This paper presents a Model Predictive Controller (MPC) that uses navigation integrity risk as a constraint. Navigation integrity risk accounts for the presence of faults in localization sensors and algorithms, an increasingly important consideration as the number of robots operating in life and mission-critical situations is expected to increase dramatically in near future (e.g. a potential influx of self-driving cars). Specifically, the work uses a local nearest neighbor integrity risk evaluation methodology that accounts for data association faults as a constraint in order to guarantee localization safety over a receding horizon. Moreover, state and control-input constraints have also been enforced in this work. The proposed MPC design is tested using real-world mapped environments, showing that a robot is capable of maintaining a predefined minimum level of localization safety while operating in an urban environment.


Title: Dynamic Risk Density for Autonomous Navigation in Cluttered Environments without Object Detection
Key Words: collision avoidance  handicapped aids  object detection  path planning  wheelchairs  dynamic risk density  congestion density  cost function  occupancy risk  velocity fields  object-based congestion cost  cluttered environments  autonomous navigation  object detection  object tracking  autonomous wheelchair  Navigation  Cost function  Wheelchairs  Dynamics  Vehicle dynamics  Planning  Level set 
Abstract: In this paper, we examine the problem of navigating cluttered environments without explicit object detection and tracking. We introduce the dynamic risk density to map the congestion density and spatial flow of the environment to a cost function for the agent to determine risk when navigating that environment. We build upon our prior work, wherein the agent maps the density and motion of objects to an occupancy risk, then navigate the environment over a specified risk level set. Here, the agent does not need to identify objects to compute the occupancy risk, and instead computes this cost function using the occupancy density and velocity fields around them. Simulations show how this dynamic risk density encodes movement information for the ego agent and closely models the object-based congestion cost. We implement our dynamic risk density on an autonomous wheelchair and show how it can be used for navigating unstructured, crowded and cluttered environments.


Title: Deep Local Trajectory Replanning and Control for Robot Navigation
Key Words: collision avoidance  control engineering computing  learning (artificial intelligence)  mobile robots  motion control  navigation  velocity control  robot navigation  hierarchical planning  machine learning  optimal paths  deep local trajectory planner  velocity controller  motion commands  attention mechanisms  nearby pedestrians  map global plan information  sensor data  velocity commands  hand-designed traditional navigation system  deep local trajectory replanning  global planner  Navigation  Robot kinematics  Trajectory  Planning  Robot sensing systems  Laser radar 
Abstract: We present a navigation system that combines ideas from hierarchical planning and machine learning. The system uses a traditional global planner to compute optimal paths towards a goal, and a deep local trajectory planner and velocity controller to compute motion commands. The latter components of the system adjust the behavior of the robot through attention mechanisms such that it moves towards the goal, avoids obstacles, and respects the space of nearby pedestrians. Both the structure of the proposed deep models and the use of attention mechanisms make the system's execution interpretable. Our simulation experiments suggest that the proposed architecture outperforms baselines that try to map global plan information and sensor data directly to velocity commands. In comparison to a hand-designed traditional navigation system, the proposed approach showed more consistent performance.


Title: A Variational Observation Model of 3D Object for Probabilistic Semantic SLAM
Key Words: Bayes methods  cameras  feature extraction  image colour analysis  inference mechanisms  object detection  pose estimation  probability  SLAM (robots)  variational techniques  probabilistic observation model  Bayesian inference  viewpoint-independent loop closure  variational observation model  Bayesian object observation model  3D object detection  probabilistic semantic SLAM  single view projection  RGB monocamera  object-oriented feature extraction  3D mapping  volumetric 3D object shape information  variational likelihood estimation  pose estimation  feature estimation  loop detector  Shape  Simultaneous localization and mapping  Three-dimensional displays  Object oriented modeling  Solid modeling  Semantics 
Abstract: We present a Bayesian object observation model for complete probabilistic semantic SLAM. Recent studies on object detection and feature extraction have become important for scene understanding and 3D mapping. However, 3D shape of the object is too complex to formulate the probabilistic observation model; therefore, performing the Bayesian inference of the object-oriented features as well as their pose is less considered. Besides, when the robot equipped with an RGB mono camera only observes the projected single view of an object, a significant amount of the 3D shape information is abandoned. Due to these limitations, semantic SLAM and viewpoint-independent loop closure using volumetric 3D object shape is challenging. In order to enable the complete formulation of probabilistic semantic SLAM, we approximate the observation model of a 3D object with a tractable distribution. We also estimate the variational likelihood from the 2D image of the object to exploit its observed single view. In order to evaluate the proposed method, we perform pose and feature estimation, and demonstrate that the automatic loop closure works seamlessly without additional loop detector in various environments.


Title: Plug-and-Play: Improve Depth Prediction via Sparse Data Propagation
Key Words: image colour analysis  image segmentation  learning (artificial intelligence)  optical radar  PnP module updates  intermediate feature map  sparse data propagation  RGB image  sparse LiDAR points  plug-and-play module  sparse depths  pretrained depth prediction model  dense depth map  Estimation  Training  Laser radar  Image reconstruction  Predictive models  Robot sensing systems 
Abstract: We propose a novel plug-and-play (PnP) module for improving depth prediction with taking arbitrary patterns of sparse depths as input. Given any pre-trained depth prediction model, our PnP module updates the intermediate feature map such that the model outputs new depths consistent with the given sparse depths. Our method requires no additional training and can be applied to practical applications such as leveraging both RGB and sparse LiDAR points to robustly estimate dense depth map. Our approach achieves consistent improvements on various state-of-the-art methods on indoor (i.e., NYU-v2) and outdoor (i.e., KITTI) datasets. Various types of LiDARs are also synthesized in our experiments to verify the general applicability of our PnP module in practice.


Title: DFNet: Semantic Segmentation on Panoramic Images with Dynamic Loss Weights and Residual Fusion Block
Key Words: feature extraction  image fusion  image segmentation  neural nets  object detection  roads  traffic engineering computing  visual perception  sight images  DFNet  dynamic loss weights  fusion layer  boundary information loss  semantic segmentation  automatic parking  lane markings  parking slots  pavement information  pixel multiplication  PSV dataset  residual fusion block  RFB  Image segmentation  Semantics  Feature extraction  Deep learning  Convolution  Training  Vehicle dynamics 
Abstract: For the domain of self-driving and automatic parking, perception is a basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. Compared with front sight images, panoramic images(PI) can capture more comprehensive pavement information. However, the imbalance of different classes in PI is even more serious. Additionally, the judgment of boundary information between areas is a hard problem in deep models. Therefore, we propose a new model named DFNet to solve these problems. The proposed model has two main contributions, one is dynamic loss weights, and the other is residual fusion block(RFB). DFNet use dynamic loss weights to overcome the negative effect of imbalance dataset, which are calculated according to the pixel number of each class in a batch. RFB is composed of several convolutional layers, a pooling layer, and a fusion layer to combine the feature maps by pixel multiplication, which can reduce boundary information loss. We evaluate our method on PSV dataset, and the achieved advanced results demonstrate the effectiveness of the proposed model.


Title: Anytime Stereo Image Depth Estimation on Mobile Devices
Key Words: learning (artificial intelligence)  mobile computing  stereo image processing  end-to-end learned approach  inference time  mobile devices  stereo depth estimation  memory-constrained devices  disparity prediction  disparity maps  computational constraints  stereo image depth estimation  NVIDIA Jetson TX2 module  AnyNet  Estimation  Image resolution  Feature extraction  Computational modeling  Three-dimensional displays  Cameras  Predictive models 
Abstract: Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process 1242×375 resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error - using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet.


Title: Graduated Fidelity Lattices for Motion Planning under Uncertainty
Key Words: mobile robots  path planning  probability  robot shape  motion models  graduated fidelity lattices  motion planning  state lattice based approach  mobile robotics  motion uncertainty  multiresolution heuristic  collision probability  Uncertainty  Planning  Robots  Lattices  Collision avoidance  Shape  Probability density function 
Abstract: In this work we present a state lattice based approach for motion planning in mobile robotics. Sensing and motion uncertainty are managed at planning time to obtain safe and optimal paths. To do this reliably, our approach estimates the probability of collision taking into account the robot shape and the uncertainty in heading. We also introduce a novel graduated fidelity approach and a multi-resolution heuristic which adapt to the obstacles in the map, improving the planning efficiency while maintaining its performance. Results for different environments, shapes and motion models are reported, including experiments with real robots.


Title: On the Impact of Uncertainty for Path Planning
Key Words: graph theory  learning (artificial intelligence)  mobile robots  navigation  path planning  probability  travelling salesman problems  path planning  planning paths  uncertain edge  learned classifier  mobile robots  partially-known environments  simulation campaign  real-world maps  planning strategy  traversability estimates  Canadian traveller problem  Robot sensing systems  Navigation  Uncertainty  Path planning  Estimation  Optimized production technology 
Abstract: We consider the problem of planning paths on graphs with some edges whose traversability is uncertain; for each uncertain edge, we are given a probability of being traversable (e.g., by a learned classifier). We categorize different interpretations of the problem that are meaningful for mobile robots navigating partially-known environments, each of which yields a different formalization; we then focus on the case in which the true traversability of an edge is revealed only when the agent visits one of its endpoints (Canadian Traveller Problem). In this context, we design a large simulation campaign on synthetic and real-world maps to study the impact of two different factors: the planning strategy, and the amount of uncertainty (which could depend on the quality of the classifier producing traversability estimates).


Title: Localization with Sliding Window Factor Graphs on Third-Party Maps for Automated Driving
Key Words: optimisation  particle filtering (numerical methods)  pose estimation  position measurement  window factor graphs  third-party maps  robotic applications  window optimization  odometry measurements  fast vehicle localization  accurate vehicle localization  estimation problem  sliding window formulation  factor graph  landmark detections  automated car  automated driving applications  Microsoft Windows  Optimization  Simultaneous localization and mapping  Global navigation satellite system  Laser radar  Measurement uncertainty 
Abstract: Localizing a vehicle in a map is essential for automated driving and various other robotic applications. This paper addresses the problem of vehicle localization in urban environments. Our approach performs a graph-based sliding window optimization over a set of recent landmark and odometry measurements for fast and accurate vehicle localization on third-party maps. Our work incorporates landmark priors from third-party maps into the estimation problem and shows how to exploit the sliding window formulation for revising data associations. We describe how to construct our factor graph and derive its necessary factors to model the information from the map as a prior over the landmark detections. We implemented our approach on an automated car and thoroughly tested it on real-world data. The experiments suggest that the approach provides highly accurate pose estimates, is fast enough for automated driving applications, and outperforms localization using particle filters.


Title: Accurate and Efficient Self-Localization on Roads using Basic Geometric Primitives
Key Words: automobiles  distance measurement  graph theory  path planning  pose estimation  position control  road traffic control  localization update rate  roads  basic geometric primitives  behavior generation  distinctive signature  map elements  localization framework  next generation series cars  association measurement  odometry measurement  robust pose graph optimization  time 30.0 min  size 10.0 cm  frequency 50.0 Hz  Roads  Automobiles  Frequency modulation  Detectors  Global navigation satellite system  Feature extraction  Optimization 
Abstract: Highly accurate localization with very limited amount of memory and computational power is one of the big challenges for next generation series cars. We propose localization based on geometric primitives which are compact in representation and further valuable for other tasks like planning and behavior generation. The primitives lack distinctive signature which makes association between detections and map elements highly ambiguous. We resolve ambiguities early in the pipeline by online building up a local map which is key to runtime efficiency. Further, we introduce a new framework to fuse association and odometry measurements based on robust pose graph optimization.We evaluate our localization framework on over 30 min of data recorded in urban scenarios. Our map is memory efficient with less than 8 kB/km and we achieve high localization accuracy with a mean position error of less than 10 cm and a mean yaw angle error of less than 0. 25° at a localization update rate of 50Hz.


Title: Efficient 2D-3D Matching for Multi-Camera Visual Localization
Key Words: cameras  distance measurement  feature extraction  image matching  motion estimation  pose estimation  autonomous driving  multicamera visual inertial localization algorithm  prioritized feature matching scheme  multicamera systems  monocular cameras  prioritization function  multicamera setup  matching efforts  pose priors  localization system  motion estimates  multicamera visual inertial odometry pipeline  large scale environments  pose estimation stages  pre-built global 3D map  Cameras  Three-dimensional displays  Pose estimation  Visualization  Feature extraction  Pipelines  Reliability 
Abstract: Visual localization, i.e., determining the position and orientation of a vehicle with respect to a map, is a key problem in autonomous driving. We present a multi-camera visual inertial localization algorithm for large scale environments. To efficiently and effectively match features against a pre-built global 3D map, we propose a prioritized feature matching scheme for multi-camera systems. In contrast to existing works, designed for monocular cameras, we (1) tailor the prioritization function to the multi-camera setup and (2) run feature matching and pose estimation in parallel. This significantly accelerates the matching and pose estimation stages and allows us to dynamically adapt the matching efforts based on the surrounding environment. In addition, we show how pose priors can be integrated into the localization system to increase efficiency and robustness. Finally, we extend our algorithm by fusing the absolute pose estimates with motion estimates from a multi-camera visual inertial odometry pipeline (VIO). This results in a system that provides reliable and drift-less pose estimation. Extensive experiments show that our localization runs fast and robust under varying conditions, and that our extended algorithm enables reliable real-time pose estimation.


Title: Beyond Point Clouds: Fisher Information Field for Active Visual Localization
Key Words: mobile robots  path planning  robot vision  point clouds  Fisher information field  active visual localization  mobile robots  perception requirement  planning stage  localization information  perception-aware planning  3D landmarks  sensor visibility  Three-dimensional displays  Planning  Visualization  Cameras  Simultaneous localization and mapping 
Abstract: For mobile robots to localize robustly, actively considering the perception requirement at the planning stage is essential. In this paper, we propose a novel representation for active visual localization. By formulating the Fisher information and sensor visibility carefully, we are able to summarize the localization information into a discrete grid, namely the Fisher information field. The information for arbitrary poses can then be computed from the field in constant time, without the need of costly iterating all the 3D landmarks. Experimental results on simulated and real-world data show the great potential of our method in efficient active localization and perception-aware planning. To benefit related research, we release our implementation of the information field to the public.


Title: Deep Reinforcement Learning of Navigation in a Complex and Crowded Environment with a Limited Field of View
Key Words: cameras  learning (artificial intelligence)  mobile robots  navigation  neural nets  optical radar  path planning  robot vision  deep reinforcement learning-based methods  DRL agents  LSTM agent  Local-Map Critic  LSTM-LMC  wide FOV  single depth camera  mobile robots  lidar devices  DRL method  depth cameras  dynamics randomization technique  Navigation  Mobile robots  Reinforcement learning  Laser radar  Robot sensing systems  Cameras 
Abstract: Mobile robots are required to navigate freely in a complex and crowded environment in order to provide services to humans. For this navigation ability, deep reinforcement learning (DRL)-based methods are gaining increasing attentions. However, existing DRL methods require a wide field of view (FOV), which imposes the usage of high-cost lidar devices. In this paper, we explore the possibility of replacing expensive lidar devices with affordable depth cameras which have a limited FOV. First, we analyze the effect of a limited field of view in the DRL agents. Second, we propose a LSTM agent with Local-Map Critic (LSTM-LMC), which is a novel DRL method to learn efficient navigation in a complex environment with a limited FOV. Lastly, we introduce the dynamics randomization technique to improve the robustness of the DRL agents in the real world. We found that our method with a limited FOV can outperform the methods having a wide FOV but limited memory. We provide the empirical evidence that our method learns to implicitly model the surrounding environment and dynamics of other agents. We also show that a robot with a single depth camera can navigate through a complex real-world environment using our method.


Title: SweepNet: Wide-baseline Omnidirectional Depth Estimation
Key Words: calibration  cameras  image reconstruction  image sensors  lenses  neural nets  object recognition  robot vision  stereo image processing  multiple sets  rectified images  dense omnidirectional depth map  rig global coordinate system  warped images  final depth map  aggregated cost volume  deep neural network  conventional depth estimation methods  highly accurate depth maps  wide-baseline omnidirectional depth estimation  omnidirectional depth sensing  conventional stereo systems  blind regions  novel wide-baseline omnidirectional stereo algorithm  dense depth estimate  fisheye images  deep convolutional neural network  capture system  multiple cameras  wide-baseline rig  ultra-wide field  view lenses  calibration algorithm  Cameras  Lenses  Calibration  Neural networks  Three-dimensional displays  Robot vision systems 
Abstract: Omnidirectional depth sensing has its advantage over the conventional stereo systems since it enables us to recognize the objects of interest in all directions without any blind regions. In this paper, we propose a novel wide-baseline omnidirectional stereo algorithm which computes the dense depth estimate from the fisheye images using a deep convolutional neural network. The capture system consists of multiple cameras mounted on a wide-baseline rig with ultra-wide field of view (FOV) lenses, and we present the calibration algorithm for the extrinsic parameters based on the bundle adjustment. Instead of estimating depth maps from multiple sets of rectified images and stitching them, our approach directly generates one dense omnidirectional depth map with full 360° coverage at the rig global coordinate system. To this end, the proposed neural network is designed to output the cost volume from the warped images in the sphere sweeping method, and the final depth map is estimated by taking the minimum cost indices of the aggregated cost volume by SGM. For training the deep neural network and testing the entire system, realistic synthetic urban datasets are rendered using Blender. The experiments using the synthetic and real-world datasets show that our algorithm outperforms the conventional depth estimation methods and generate highly accurate depth maps.


Title: 3D Surface Reconstruction Using A Two-Step Stereo Matching Method Assisted with Five Projected Patterns
Key Words: correlation methods  image matching  image reconstruction  image resolution  robot vision  solid modelling  stereo image processing  surface reconstruction  three-dimensional vision  3D surface reconstruction scheme  stereo matching pattern projection  stereo images  phase maps  phase-shifting patterns  image acquisition time  correspondence refinement algorithm  high-resolution reconstruction  object reconstruction  two-step stereo matching method  Three-dimensional displays  Image reconstruction  Surface reconstruction  Cameras  Robots  Pattern matching  Robustness 
Abstract: Three-dimensional vision plays an important role in robotics. In this paper, we present a 3D surface reconstruction scheme based on combination of stereo matching and pattern projection. A two-step matching scheme is proposed to establish reliable correspondence between stereo images with high computation efficiency and accuracy. The first step (coarse matching) can quickly find the correlation candidates, and the second step (precise matching) is responsible for determining the most precise correspondence within the candidates. Two phase maps serve as codewords and are utilized in the two-step stereo matching, respectively. The phase maps are derived from phase-shifting patterns to provide robustness to the background noises. Only five patterns are required, which reduces the image acquisition time. Moreover, the precision is further enhanced by applying a correspondence refinement algorithm. The precision and accuracy are validated by experiments on standard objects. Furthermore, various experiments are conducted to verify the capability of the proposed method, which includes the complex object reconstruction, the high-resolution reconstruction, and the occlusion avoidance. The real-time experimental results are also provided.


Title: Real-Time Dense Mapping for Self-Driving Vehicles using Fisheye Cameras
Key Words: cameras  image capture  image fusion  image resolution  image sensors  mobile robots  object detection  robot vision  stereo image processing  visual perception  fisheye cameras  real-time dense geometric mapping algorithm  pinhole cameras  visual-inertial odometry  visual localization  vision-only 3D scene perception  depth map  reference camera  plane-sweeping stereo  fast object detection framework  YOLOv3  fisheye depth images  computer vision applications  angular resolution  image resolutions  in-vehicle PC  truncated signed distance function  TSDF volume  3D map  self-driving vehicles  Cameras  Three-dimensional displays  Real-time systems  Image resolution  Vehicle dynamics  Estimation  Object detection 
Abstract: We present a real-time dense geometric mapping algorithm for large-scale environments. Unlike existing methods which use pinhole cameras, our implementation is based on fisheye cameras whose large field of view benefits various computer vision applications for self-driving vehicles such as visual-inertial odometry, visual localization, and object detection. Our algorithm runs on in-vehicle PCs at approximately 15 Hz, enabling vision-only 3D scene perception for self-driving vehicles. For each synchronized set of images captured by multiple cameras, we first compute a depth map for a reference camera using plane-sweeping stereo. To maintain both accuracy and efficiency, while accounting for the fact that fisheye images have a lower angular resolution, we recover the depths using multiple image resolutions. We adopt the fast object detection framework, YOLOv3, to remove potentially dynamic objects. At the end of the pipeline, we fuse the fisheye depth images into the truncated signed distance function (TSDF) volume to obtain a 3D map. We evaluate our method on large-scale urban datasets, and results show that our method works well in complex dynamic environments.


Title: Tightly-Coupled Aided Inertial Navigation with Point and Plane Features
Key Words: feature extraction  image fusion  image sensors  inertial navigation  mobile robots  Monte Carlo methods  object tracking  SLAM (robots)  planar point features  nonplanar point features  point-on-plane constraints  effective plane feature initialization algorithm  depth sensor  general sensor fusion framework  point feature tracking  plane extraction  geometrical structures  closest point  plane parameterization  Monte-Carlo simulations  visual sensor  tightly-coupled aided inertial navigation system  feature-based simultaneous localization and mapping  Feature extraction  Cameras  Calibration  Laser radar  Jacobian matrices  Simultaneous localization and mapping  Estimation 
Abstract: This paper presents a tightly-coupled aided inertial navigation system (INS) with point and plane features, a general sensor fusion framework applicable to any visual and depth sensor (e.g., RGBD, LiDAR) configuration, in which the camera is used for point feature tracking and depth sensor for plane extraction. The proposed system exploits geometrical structures (planes) of the environments and adopts the closest point (CP) for plane parameterization. Moreover, we distinguish planar point features from non-planar point features in order to enforce point-on-plane constraints which are used in our state estimator, thus further exploiting structural information from the environment. We also introduce a simple but effective plane feature initialization algorithm for feature-based simultaneous localization and mapping (SLAM). In addition, we perform online spatial calibration between the IMU and the depth sensor as it is difficult to obtain this critical calibration parameter in high precision. Both Monte-Carlo simulations and real-world experiments are performed to validate the proposed approach.


Title: FastDepth: Fast Monocular Depth Estimation on Embedded Systems
Key Words: autonomous aerial vehicles  cameras  computational complexity  embedded systems  estimation theory  image colour analysis  image segmentation  image sensors  learning (artificial intelligence)  microrobots  mobile robots  neural nets  object detection  robot vision  low-latency decoder  NYU Depth v2 dataset  real-time monocular depth estimation  deep neural network  embedded platform  microaerial vehicle  embedded systems  robotic tasks  obstacle detection  single RGB image  monocular cameras  lightweight encoder-decoder network architecture  computational complexity  FastDepth  fast monocular depth estimation  depth sensing  deep neural networks  Estimation  Decoding  Neural networks  Runtime  Convolution  Complexity theory  Task analysis 
Abstract: Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.


Title: An Improved Control-Oriented Modeling of the Magnetic Field
Key Words: closed loop systems  coils  interpolation  magnetic fields  microrobots  mobile robots  motion control  control-oriented model  coil  untethered microscale mobile robotics  elliptic integral functions  magnetically actuated microrobots  map-based interpolation  computation time  closed-loop control  dipole approximation  Magnetic domains  Computational modeling  Magnetic hysteresis  Soft magnetic materials  Numerical models  Robots  Magnetic cores 
Abstract: This paper proposes a new control-oriented model to compute the magnetic field created by a coil. A major challenge for untethered microscale mobile robotics is the control of objects for precise and fast displacements. In this work, we propose to use an alternative implementation of a model based on elliptic integral functions to control magnetically actuated micro-robots. It allows to compute the magnetic field even in the area close to the coil quickly and accurately. This model is evaluated numerically and compared to classical approaches - dipole approximation, map-based interpolation and classical elliptic integral models - in terms of accuracy, computation time and memory requirement. Simulation results show that this works allows to have an accurate model in the whole workspace by avoiding numerical issues encountered in previous works. It can be computed in a few milliseconds, making it the right candidate for closed-loop control of magnetically actuated micro-robots.


Title: 3D Keypoint Repeatability for Heterogeneous Multi-Robot SLAM
Key Words: feature extraction  mobile robots  multi-robot systems  robot vision  SLAM (robots)  point cloud registration  loop closure  heterogenous multirobot SLAM applications  NARF detector  3D keypoint repeatability  heterogeneous multirobot SLAM  multirobot SLAM scenario  sensor measurement point clouds  point density  3D keypoint detectors  multirobot SLAM system  KPQ-SI  relative repeatability  Three-dimensional displays  Detectors  Simultaneous localization and mapping  Cameras  Laser radar 
Abstract: For robots with different types of sensors, loop closure in a multi-robot SLAM scenario requires keypoints that can be matched between sensor measurement point clouds with different properties such as point density and noise. In this paper, we evaluate the performance of several 3D keypoint detectors (Harris3D, ISS, KPQ, KPQ-SI, and NARF) for repeatability between scans from different sensors towards building a heterogeneous multi-robot SLAM system. We find that KPQ-SI and NARF have the best relative repeatability, with KPQ-SI finding more keypoints overall and a higher number of repeatable keypoints, at the cost of significantly worse computational performance. In scans of the same area from different poses, both detectors find enough keypoints for point cloud registration and loop closure. For heterogenous multirobot SLAM applications with computational or bandwidth restrictions, the NARF detector consistently finds repeatable keypoints while also allowing for real-time performance.


Title: SLAMBench 3.0: Systematic Automated Reproducible Evaluation of SLAM Systems for Robot Vision Challenges and Scene Understanding
Key Words: control engineering computing  convolutional neural nets  image reconstruction  natural scenes  robot vision  SLAM (robots)  scene understanding  nonrigid environments  dynamic SLAM  SLAMBench 3  evaluation infrastructure  systematic automated reproducible evaluation  robot vision  visual SLAM  SLAM research area  visulation aids  visulation metrics  convolutional neural networks  dynamicfusion  Simultaneous localization and mapping  Semantics  Three-dimensional displays  Measurement  Benchmark testing  Heuristic algorithms  C++ languages 
Abstract: As the SLAM research area matures and the number of SLAM systems available increases, the need for frameworks that can objectively evaluate them against prior work grows. This new version of SLAMBench moves beyond traditional visual SLAM, and provides new support for scene understanding and non-rigid environments (dynamic SLAM). More concretely for dynamic SLAM, SLAMBench 3.0 includes the first publicly available implementation of DynamicFusion, along with an evaluation infrastructure. In addition, we include two SLAM systems (one dense, one sparse) augmented with convolutional neural networks for scene understanding, together with datasets and appropriate metrics. Through a series of use-cases, we demonstrate the newly incorporated algorithms, visulation aids and metrics (6 new metrics, 4 new datasets and 5 new algorithms).


Title: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation
Key Words: motion estimation  pose estimation  SLAM (robots)  photometric loss  self-supervised ego-motion estimation  accurate relative pose  SLAM  self-supervised learning framework  image depth  photometric error  systematic error  realistic scenes  geometric loss  matching loss  self-supervised framework  unsupervised egomotion estimation methods  Simultaneous localization and mapping  Estimation  Geometry  Cameras  Motion estimation  Visualization  Visual odometry 
Abstract: Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised egomotion estimation methods by a large margin. The code and data are available at https://github.com/hlzz/DeepMatchVO.


Title: IN2LAMA: INertial Lidar Localisation And MApping
Key Words: mobile robots  motion estimation  optical radar  optimisation  probability  robot vision  IN2LAMA  spinning mechanisms  resulting point clouds  lidar mapping literature  constant velocity motion model  upsampled inertial data  motion distortion  explicit motion-model  temporally precise upsampled preintegrated measurement  frame-to-frame planar  edge features association  probabilistic framework  inertial lidar localisation and mapping  batch on-manifold optimisation formulation  state change estimation  front-end interaction  back-end interaction  Laser radar  Distortion measurement  Three-dimensional displays  Distortion  Optimization  Trajectory  Gyroscopes 
Abstract: In this paper, we introduce a probabilistic framework for INertial Lidar Localisation And MApping (IN2LAMA). Most of today's lidars are based on spinning mechanisms that do not capture snapshots of the environment. As a result, movement of the sensor can occur while scanning. Without a good estimation of this motion, the resulting point clouds might be distorted. In the lidar mapping literature, a constant velocity motion model is commonly assumed. This is an approximation that does not necessarily always hold. The key idea of the proposed framework is to exploit preintegrated measurements over upsampled inertial data to handle motion distortion without the need for any explicit motion-model. It tightly integrates inertial and lidar data in a batch on-manifold optimisation formulation. Using temporally precise upsampled preintegrated measurement allows frame-to-frame planar and edge features association. Moreover, features are re-computed when the estimate of the state changes, consolidating front-end and back-end interaction. We validate the effectiveness of the approach through simulated and real data.


Title: Integral Backstepping Position Control for Quadrotors in Tunnel-Like Confined Environments
Key Words: aerodynamics  aerospace robotics  helicopters  Kalman filters  mechanical stability  mobile robots  pose estimation  position control  robot dynamics  robot vision  SLAM (robots)  tunnels  vision-based localisation  cross-sectional localisation system  integral backstepping controller  quadrotors  tunnel-like confined environments  integral backstepping position control  kinematic Kalman filter  semiautonomous system  flying robots  aerodynamic disturbances  Backstepping  Aerodynamics  Kinematics  Kalman filters  Navigation  Rail transportation  Sensors 
Abstract: There are many potential applications that require flying robots to navigate through tunnel-like environments, such as inspections of small railway culverts and mineral mappings of mining tunnels. Nevertheless, those environments present many challenges for quadrotors to navigate through. The aerodynamic disturbances created from the fluid interaction between the propellers' downwash and the surrounding surfaces of the environment, as well as longitudinal wind gusts, add hardship in stabilising the vehicle while the restricted narrow space increases the risk of collision. Furthermore, poor visibility and dust blown by the downwash make vision-based localisation extremely difficult. This paper presents a cross-sectional localisation system using Hough Scan Matching and a simple kinematic Kalman filter. Using the estimated state information, an integral backstepping controller is implemented which enables quadrotors to robustly fly in tunnel-like confined environments. A semi-autonomous system is proposed with self-stabilisation in the vertical and lateral axes while a pilot provides commands in the longitudinal direction. The results of a series of experiments in a simulated tunnel show that the proposed system successfully hovered itself and tracked various trajectories in a cross-sectional area without the aid of any external sensing or computing system.


Title: UWB/LiDAR Fusion For Cooperative Range-Only SLAM
Key Words: distance measurement  laser ranging  mobile robots  optical radar  SLAM (robots)  ultra wideband radar  wireless sensor networks  cooperative sensor network  2D LiDAR sensor  UWB-LiDAR fusion  UWB beacon nodes  peer-to-peer ranges  nearby objects-obstacles  surrounding environment  drift-free SLAM  mobile robot  2D laser rangefinder  ultra-wideband node  cooperative range-only SLAM  LiDAR-based SLAM algorithm  UWB ranging measurements  UWB-only localization accuracy  LiDAR mapping  laser scanning information  Laser radar  Peer-to-peer computing  Distance measurement  Simultaneous localization and mapping  Two dimensional displays 
Abstract: We equip an ultra-wideband (UWB) node and a 2D LiDAR sensor a.k.a. 2D laser rangefinder on a mobile robot, and place UWB beacon nodes at unknown locations in an unknown environment. All UWB nodes can do ranging with each other thus forming a cooperative sensor network. We propose to fuse the peer-to-peer ranges measured between UWB nodes and laser scanning information, i.e., range measured between robot and nearby objects/obstacles, for simultaneous localization of the robot, all UWB beacons and LiDAR mapping. The fusion is inspired by two facts: 1) LiDAR may improve UWB-only localization accuracy as it gives a more precise and comprehensive picture of the surrounding environment; 2) on the other hand, UWB ranging measurements may remove the error accumulated in the LiDAR-based SLAM algorithm. Our experiments demonstrate that UWB/LiDAR fusion enables drift-free SLAM in real-time based on ranging measurements only.


Title: 2D LiDAR Map Prediction via Estimating Motion Flow with GRU
Key Words: feature extraction  image sequences  learning (artificial intelligence)  motion estimation  optical radar  radar imaging  recurrent neural nets  LiDAR-FlowNet model  motion flow based method  2D LiDAR map prediction  optical flow  recurrent neural network  motion flow estimation  gated recurrent unit  robotics navigation  path planning  Laser radar  Two dimensional displays  Dynamics  Logic gates  Recurrent neural networks  Training  Robot sensing systems 
Abstract: It is a significant problem to predict the 2D LiDAR map at next moment for robotics navigation and path-planning. To tackle this problem, we resort to the motion flow between adjacent maps, as motion flow is a powerful tool to process and analyze the dynamic data, which is named optical flow in video processing. However, unlike video, which contains abundant visual features in each frame, a 2D LiDAR map lacks distinctive local features. To alleviate this challenge, we propose to estimate the motion flow based on deep neural networks inspired by its powerful representation learning ability in estimating the optical flow of the video. To this end, we design a recurrent neural network based on gated recurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can encode the temporal dynamic information, our LiDAR-FlowNet can estimate motion flow between the current map and the unknown next map only from the current frame and previous frames. A self-supervised strategy is further designed to train the LiDAR-FlowNet model effectively, while no training data need to be manually annotated. With the estimated motion flow, it is straightforward to predict the 2D LiDAR map at the next moment. Experimental results verify the effectiveness of our LiDAR-FlowNet as well as the proposed training strategy. The results of the predicted LiDAR map also show the advantages of our motion flow based method.


Title: Visual Localization at Intersections with Digital Maps
Key Words: computer vision  feature extraction  image reconstruction  image segmentation  neural nets  object detection  pose estimation  road vehicles  stereo image processing  traffic engineering computing  ego-vehicle localization  autonomous road driving  online vision-based method  digital map service  pixel-level semantic segmentation  intersection approaches  visual localization  deep neural networks  coarse street-level pose estimation  Roads  Three-dimensional displays  Semantics  Image segmentation  Pipelines  Geometry  Task analysis 
Abstract: This paper deals with the task of ego-vehicle localization at intersections, a significant task in autonomous road driving. We propose an online vision-based method that can hence be applied if the intersection is visible. It relies on stereo images and on a coarse street-level pose estimate, used to retrieve intersection data from a digital map service. Pixel-level semantic segmentation, and 3D reconstruction from state-of-the art Deep Neural Networks are coupled with an intersection model; this allows good positioning accuracy compared to the state-of-the-art in this task. To demonstrate the effectiveness of the method and make it possible to compare it with other methods, an extensive activity has been conducted in order to set up a dataset of approaches to an intersection, which has then been used to benchmark the proposed method. The dataset is made available to the community, and it currently includes more than forty intersection approaches, from KITTI. Another important contribution of the paper is the definition of criteria for the comparison of different methods, on recorded datasets. The proposed method achieves nearly sub-meter accuracy in difficult real conditions.


Title: A Hierarchical Framework for Coordinating Large-Scale Robot Networks
Key Words: collision avoidance  mobile robots  motion control  multi-robot systems  road traffic control  hierarchical framework  large-scale robot networks  motion coordination problems  multirobot system  robotic warehouses  automated transportation systems  life-long planning problem  coordination performance  robot motion uncertainties  hierarchical path planning  motion coordination structure  traffic heat-map  path planning level  sector-level path  path distance  motion coordination level  collision-free local path  rolling planning manner  traffic condition  robot uncertainty  Robot kinematics  Path planning  Task analysis  Collision avoidance  Planning  Topology 
Abstract: In this paper, we study the cooperative path planning and motion coordination problems of the multi-robot system with large number of robots, aiming for practical applications in robotic warehouses and automated transportation systems. Particularly, we solve the life-long planning problem and guarantee the coordination performance in the presence of robot motion uncertainties. A hierarchical path planning and motion coordination structure is presented. The environment is divided into several sectors and a traffic heat-map is presented to describe the current sector-level traffic condition. In path planning level, the sector-level path is calculated by considering the path distance, the current traffic condition and the current robot uncertainty. In motion coordination level, local cooperative A* algorithm and conflict-based searching strategy are utilized within each sector to generate the collision-free local path of each robot in a rolling planning manner. The effectiveness and practical applicability of the proposed approach are validated by simulations with more than one thousand robots and real experiments.


Title: Characterizing Visual Localization and Mapping Datasets
Key Words: motion estimation  rendering (computer graphics)  SLAM (robots)  Wasserstein distance  motion estimation algorithm  robotics SLAM benchmarking  visual localization  mapping algorithms  real-world trajectories  high-quality scenes  synthetic datasets  dense map  key SLAM applications  ground robotics  mapping datasets  motion estimation algorithms  computer vision  Simultaneous localization and mapping  Trajectory  Time measurement  Visualization  Benchmark testing 
Abstract: Benchmarking mapping and motion estimation algorithms is established practice in robotics and computer vision. As the diversity of datasets increases, in terms of the trajectories, models, and scenes, it becomes a challenge to select datasets for a given benchmarking purpose. Inspired by the Wasserstein distance, this paper addresses this concern by developing novel metrics to evaluate trajectories and the environments without relying on any SLAM or motion estimation algorithm. The metrics, which so far have been missing in the research community, can be applied to the plethora of datasets that exist. Additionally, to improve the robotics SLAM benchmarking, the paper presents a new dataset for visual localization and mapping algorithms. A broad range of real-world trajectories is used in very high-quality scenes and a rendering framework to create a set of synthetic datasets with ground-truth trajectory and dense map which are representative of key SLAM applications such as virtual reality (VR), micro aerial vehicle (MAV) flight, and ground robotics.


Title: Learning Robust Manipulation Skills with Guided Policy Search via Generative Motor Reflexes
Key Words: learning (artificial intelligence)  manipulators  neural nets  search problems  trajectory control  robust manipulation skills  control policies  complex manipulation tasks  high-dimensional neural networks  robot actions  real-world trajectory samples  resulting neural networks  policy representation  robust actions  broader state space  state-dependent motor reflex  similar motor reflexes  real-world manipulation tasks  guided policy search  generative motor reflexes map states  state-action policies  Neural networks  Trajectory  Robots  Robustness  Reinforcement learning  Space exploration  Training 
Abstract: Guided Policy Search enables robots to learn control policies for complex manipulation tasks efficiently. Therein, the control policies are represented as high-dimensional neural networks which derive robot actions based on states. However, due to the small number of real-world trajectory samples in Guided Policy Search, the resulting neural networks are only robust in the neighbourhood of the trajectory distribution explored by real-world interactions. In this paper, we present a new policy representation called Generative Motor Reflexes, which is able to generate robust actions over a broader state space compared to previous methods. In contrast to prior state-action policies, Generative Motor Reflexes map states to parameters for a state-dependent motor reflex, which is then used to derive actions. Robustness is achieved by generating similar motor reflexes for many states. We evaluate the presented method in simulated and real-world manipulation tasks, including contact-rich peg-in-hole tasks. Using these evaluation tasks, we show that policies represented as Generative Motor Reflexes lead to robust manipulation skills also outside the explored trajectory distribution with less training needs compared to previous methods.


Title: Discontinuity-Sensitive Optimal Control Learning by Mixture of Experts
Key Words: approximation theory  function approximation  iterative methods  learning (artificial intelligence)  neural nets  nonlinear control systems  optimal control  optimisation  discontinuity-sensitive optimal control learning  machine learning method  parametric input  problem parameters  optimal solutions  problem-optimum map  discrete homotopy classes  control switching  MoE  standard neural networks  dynamic vehicle control problems  nonlinear optimal control problems  function approximators  mixture of experts model  trajectory prediction  Trajectory  Training  Optimal control  Neural networks  Optimization  Standards  Predictive models 
Abstract: This paper proposes a machine learning method to predict the solutions of related nonlinear optimal control problems given some parametric input, such as the initial state. The map between problem parameters to optimal solutions is called the problem-optimum map, and is often discontinuous due to nonconvexity, discrete homotopy classes, and control switching. This causes difficulties for traditional function approximators such as neural networks, which assume continuity of the underlying function. This paper proposes a mixture of experts (MoE) model composed of a classifier and several regressors, where each regressor is tuned to a particular continuous region. A novel training approach is proposed that trains classifier and regressors independently. MoE greatly outperforms standard neural networks, and achieves highly reliable trajectory prediction (over 99.5% accuracy) in several dynamic vehicle control problems.


Title: Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments
Key Words: autonomous underwater vehicles  geophysical image processing  image representation  object detection  oceanographic techniques  probability  robot vision  SLAM (robots)  unsupervised learning  co-robotic exploration  bandwidth tunable technique  real-time probabilistic scene modeling  communication constrained environments  deep sea  scene complexity  bandwidth requirements  underwater robot  high-level semantic scene constructs  artificially constructed tank environment  science interests  unsupervised scene model impact  resulting scene model  coral reef  bandwidth constraints  scene maps streaming  Robot sensing systems  Bandwidth  Oceans  Data models  Visualization  Bayes methods 
Abstract: This paper proposes a bandwidth tunable technique for real-time probabilistic scene modeling and mapping to enable co-robotic exploration in communication constrained environments such as the deep sea. The parameters of the system enable the user to characterize the scene complexity represented by the map, which in turn determines the bandwidth requirements. The approach is demonstrated using an underwater robot that learns an unsupervised scene model of the environment and then uses this scene model to communicate the spatial distribution of various high-level semantic scene constructs to a human operator. Preliminary experiments in an artificially constructed tank environment as well as simulated missions over a 10m×10m coral reef using real data show the tunability of the maps to different bandwidth constraints and science interests. To our knowledge this is the first paper to quantity how the free parameters of the unsupervised scene model impact both the scientific utility of and bandwidth required to communicate the resulting scene model.


Title: UWStereoNet: Unsupervised Learning for Depth Estimation and Color Correction of Underwater Stereo Imagery
Key Words: cameras  computerised instrumentation  feature extraction  geophysical image processing  image colour analysis  image matching  image resolution  image restoration  light propagation  neural nets  oceanographic techniques  spatial variables measurement  stereo image processing  unsupervised learning  visual perception  unsupervised learning  stereo cameras  navigation  underwater robotic systems  constrained camera geometry  feature detection  underwater light propagation lead  deep learning  underwater image restoration  unsupervised deep neural network  input raw color underwater stereo imagery  color corrected imagery  underwater image formation  image processing techniques  depth estimation  stereo vision algorithms  disparity estimation  DNN  Image color analysis  Estimation  Image restoration  Cameras  Attenuation  Stereo vision  Deep learning 
Abstract: Stereo cameras are widely used for sensing and navigation of underwater robotic systems. They can provide high resolution color views of a scene; the constrained camera geometry enables metrically accurate depth estimation; they are also relatively cost-effective. Traditional stereo vision algorithms rely on feature detection and matching to enable triangulation of points for estimating disparity. However, for underwater applications, the effects of underwater light propagation lead to image degradation, reducing image quality and contrast. This makes it especially challenging to detect and match features, especially from varying viewpoints. Recently, deep learning has shown success in end-to-end learning of dense disparity maps from stereo images. Still, many state-of-the-art methods are supervised and require ground truth depth or disparity, which is challenging to gather in subsea environments. Simultaneously, deep learning has also been applied to the problem of underwater image restoration. Again, it is difficult or impossible to gather real ground truth data for this problem. In this work, we present an unsupervised deep neural network (DNN) that takes input raw color underwater stereo imagery and outputs dense depth maps and color corrected imagery of underwater scenes. We leverage a model of the process of underwater image formation, image processing techniques, as well as the geometric constraints inherent to the stereo vision problem to develop a modular network that outperforms existing methods.


Title: WISDOM: WIreless Sensing-assisted Distributed Online Mapping
Key Words: least mean squares methods  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  WISDOM  spatial sensing  robotics  augmented reality  urban spaces  wireless access points  coarse orientation  average Root Mean Square mapping error  wireless sensing-assisted distributed online mapping  robot swarm  custom ICP algorithm  absolute trajectory error  average root mean square mapping error  size 0.2 m  size 1.3 m  Three-dimensional displays  Simultaneous localization and mapping  Robot kinematics  Visualization  Merging 
Abstract: Spatial sensing is a fundamental requirement for applications in robotics and augmented reality. In urban spaces such as malls, airports, apartments, and others, it is quite challenging for a single robot to map the whole environment. So, we employ a swarm of robots to perform the mapping. One challenge with this approach is merging sub-maps built by each robot. In this work, we use wireless access points, which are ubiquitous in most urban spaces, to provide us with coarse orientation between sub-maps, and use a custom ICP algorithm to refine this orientation to merge them. We demonstrate our approach with maps from a building on campus and evaluate it using two metrics. Our results show that, in the building we studied, we can achieve an average Absolute Trajectory error of 0.2m in comparison to a map created by a single robot and average Root Mean Square mapping error of 1.3m from ground truth landmark locations.


Title: UAV/UGV Autonomous Cooperation: UAV assists UGV to climb a cliff by attaching a tether
Key Words: autonomous aerial vehicles  collision avoidance  inertial navigation  mobile robots  off-road vehicles  robot vision  SLAM (robots)  Unmanned Aerial Vehicle  Unmanned Ground Vehicle  tether attachment device  steep terrain  tether anchoring  UGV autonomous cooperation  UAV autonomous cooperation  visual inertial navigation  collaborative navigation  3D voxel mapping  obstacle avoidance planning  traversability analysis  Robot sensing systems  Navigation  Three-dimensional displays  Unmanned aerial vehicles  Trajectory  Attitude control 
Abstract: This paper proposes a novel cooperative system for an Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV) which utilizes the UAV not only as a flying sensor but also as a tether attachment device. Two robots are connected with a tether, allowing the UAV to anchor the tether to a structure located at the top of a steep terrain, impossible to reach for UGVs. Thus, enhancing the poor traversability of the UGV by not only providing a wider range of scanning and mapping from the air, but also by allowing the UGV to climb steep terrains with the winding of the tether. In addition, we present an autonomous framework for the collaborative navigation and tether attachment in an unknown environment. The UAV employs visual inertial navigation with 3D voxel mapping and obstacle avoidance planning. The UGV makes use of the voxel map and generates an elevation map to execute path planning based on a traversability analysis. Furthermore, we compared the pros and cons of possible methods for the tether anchoring from multiple points of view. To increase the probability of successful anchoring, we evaluated the anchoring strategy with an experiment. Finally, the feasibility and capability of our proposed system were demonstrated by an autonomous mission experiment in the field with an obstacle and a cliff.


Title: Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping
Key Words: Gaussian distribution  optical radar  sensor fusion  multiple sensors  Robosense RS-LiDAR-16  accurate maps  lidar measurement bias estimation  return waveform modelling  zero-mean Gaussian distribution  localisation drifts  Laser radar  Laser beams  Two dimensional displays  Sensors  Laser modes  Robots  Computational modeling  Bias Estimation  Sensor Error Modelling  Waveform Modelling  LIDAR  3D Mapping 
Abstract: In a context of 3D mapping, it is very important to obtain accurate measurements from sensors. In particular, Light Detection And Ranging (LIDAR) measurements are typically treated as a zero-mean Gaussian distribution. We show that this assumption leads to predictable localisation drifts, especially when a bias related to measuring obstacles with high incidence angles is not taken into consideration. Moreover, we present a way to physically understand and model this bias, which generalizes to multiple sensors. Using an experimental setup, we measured the bias of the Sick LMS151, Velodyne HDL-32E, and Robosense RS-LiDAR-16 as a function of depth and incidence angle, and showed that the bias can reach 20 cm for high incidence angles. We then used our model to remove the bias from the measurements, leading to more accurate maps and a reduced localisation drift.


Title: Low-latency Visual SLAM with Appearance-Enhanced Local Map Building
Key Words: file organisation  image enhancement  image fusion  mobile robots  pose estimation  robot vision  SLAM (robots)  appearance-enhanced local map building  local map module  local map contents  co-visibility local map building  compact local map  downstream data association  mapped features  local map size  appearance-based local map building method  low-latency visual SLAM  pose estimation  multi-index hashing  online hash table selection algorithm  MIH  VO-VSLAM mean performance  Three-dimensional displays  Buildings  Optimization  Feature extraction  Indexing  Simultaneous localization and mapping 
Abstract: A local map module is often implemented in modern VO/VSLAM systems to improve data association and pose estimation. Conventionally, the local map contents are determined by co-visibility. While co-visibility is cheap to establish, it utilizes the relatively-weak temporal prior (i.e. seen before, likely to be seen now), therefore admitting more features into the local map than necessary. This paper describes an enhancement to co-visibility local map building by incorporating a strong appearance prior, which leads to a more compact local map and latency reduction in downstream data association. The appearance prior collected from the current image influences the local map contents: only the map features visually similar to the current measurements are potentially useful for data association. To that end, mapped features are indexed and queried with Multi-index Hashing (MIH). An online hash table selection algorithm is developed to further reduce the query overhead of MIH and the local map size. The proposed appearance-based local map building method is integrated into a state-of-the-art VO/VSLAM system. When evaluated on two public benchmarks, the size of the local map, as well as the latency of real-time pose tracking in VO/VSLAM are significantly reduced. Meanwhile, the VO/VSLAM mean performance is preserved or improves.


Title: Learning to Drive in a Day
Key Words: learning (artificial intelligence)  mobile robots  road safety  road traffic control  road vehicles  robot vision  autonomous driving tasks  single monocular image  safety driver  model-free deep reinforcement learning algorithm  lane following  on-vehicle  Reinforcement learning  Autonomous vehicles  Task analysis  Markov processes  Global Positioning System  Sensors  Training 
Abstract: We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.


Title: A Real-Time Interactive Augmented Reality Depth Estimation Technique for Surgical Robotics
Key Words: augmented reality  kinematics  medical computing  medical robotics  surgery  Stereo-No CDE  CDE technique  forward kinematics joint encoder data  surgical field  virtual surgical instrument method  AR technique  blue-red color spectrum  tissue surface  tumor  medical abnormality  surgical robotics  color depth encoding  real-time interactive augmented reality depth estimation  Tools  Robots  Tumors  Image color analysis  Cameras  Instruments  Biomedical imaging 
Abstract: Augmented reality (AR) is a promising technology where the surgeon can see the medical abnormality in the context of the patient. It makes the anatomy of interest visible to the surgeon which otherwise is not visible. It can result in better surgical precision and therefore, potentially better surgical outcomes and faster recovery times. Despite these benefits, the current AR systems suffer from two major challenges; first, incorrect depth perception and, second, the lack of suitable evaluation systems. Therefore, in the current paper we addressed both of these problems. We proposed a color depth encoding (CDE) technique to estimate the distance between the tumor and the tissue surface using a surgical instrument. We mapped the distance between the tumor and the tissue surface to the blue-red color spectrum. For evaluation and interaction with our AR technique, we propose to use a virtual surgical instrument method using the CAD model of the instrument. The users were asked to reach the judged distance in the surgical field using the virtual tool. Realistic tool movement was simulated by collecting the forward kinematics joint encoder data. The results showed significant improvement in depth estimation, time for task completion and confidence, using our CDE technique with and without stereo versus other two cases, that are, Stereo-No CDE and No Stereo-No CDE.


Title: Mixed Frame-/Event-Driven Fast Pedestrian Detection
Key Words: cameras  computer vision  convolutional neural nets  image fusion  image sensors  pedestrians  traffic engineering computing  conventional frame-based camera  bad light condition  high-speed motion  gray-scale frames  traffic monitoring scenario  YOLOv3 models  YOLO-tiny models  confidence map fusion method  CNN-based detection results  DAVIS channels  intelligent transportation system  mixed frame-event-driven fast pedestrian detection  ITS  frame-based camera  dynamic and active pixel sensor  asynchronous low-latency temporal contrast events  convolutional neural networks  TUM campus  Voltage control  Vision sensors  Feature extraction  Neuromorphics  Cameras  Object detection 
Abstract: Pedestrian detection has attracted enormous research attention in the field of Intelligent Transportation System (ITS) due to that pedestrians are the most vulnerable traffic participants. So far, almost all pedestrian detection solutions are based on the conventional frame-based camera. However, they cannot perform very well in scenarios with bad light condition and high-speed motion. In this work, a Dynamic and Active Pixel Sensor (DAVIS), whose two channels concurrently output conventional gray-scale frames and asynchronous low-latency temporal contrast events of light intensity, was first used to detect pedestrians in a traffic monitoring scenario. Data from two camera channels were fed into Convolutional Neural Networks (CNNs) including three YOLOv3 models and three YOLO-tiny models to gather bounding boxes of pedestrians with respective confidence map. Furthermore, a confidence map fusion method combining the CNN-based detection results from both DAVIS channels was proposed to obtain higher accuracy. The experiments were conducted on a custom dataset collected on TUM campus. Benefiting from the high speed, low latency and wide dynamic range of the event channel, our method achieved higher frame rate and lower latency than those only using a conventional camera. Additionally, it reached higher average precision by using the fusion approach.


Title: Real-Time Vehicle Detection from Short-range Aerial Image with Compressed MobileNet
Key Words: feature extraction  mobile computing  motorcycles  neural nets  object detection  road vehicles  traffic engineering computing  MobileNet family network engineering  compressed MobileNet  feature map downsampling stage  feature map plateau stage  reduced inference time  vehicle categories  crowded bicycles  high detection accuracy  real-time detection speed  real time vehicle detection  object interference  short-range aerial image  crowded motorcycles  truck  car  bus  Convolution  Neural networks  Proposals  Vehicle detection  Object detection  Computational modeling  Feature extraction 
Abstract: Vehicle detection from short-range aerial image faces challenges including vehicle blocking, irrelevant object interference, motion blurring, color variation etc., leading to the difficulty to achieve high detection accuracy and real-time detection speed. In this paper, benefiting from the recent development in MobileNet family network engineering, we propose a compressed MobileNet which is not only internally resistant to the above listed challenges but also gains the best detection accuracy/speed tradeoff when comparing with the original MobileNet. In a nutshell, we reduce the bottleneck architecture number during the feature map downsampling stage but add more bottlenecks during the feature map plateau stage, neither extra FLOPs nor parameters are thus involved but reduced inference time and better accuracy are expected. We conduct experiment on our collected 5-k short-range aerial images, containing six vehicle categories: truck, car, bus, bicycle, motorcycle, crowded bicycles and crowded motorcycles. Our proposed compressed MobileNet achieves 110 FPS (GPU), 31 FPS (CPU) and 15 FPS (mobile phone), 1.2 times faster and 2% more accurate (mAP) than the original MobileNet.


Title: CHiMP: A Contact based Hilbert Map Planner
Key Words: gradient methods  manipulators  mobile robots  path planning  robot vision  stochastic processes  tactile sensors  multimodal robot skin  contact-based robot system  skin compliant control  tactile-based explorative behavior  CHiMP  skin-based sparse contact data  contact-based 3D path planning approach  6 DOF robot arm  stochastic functional gradient path planner  contact based Hilbert map planner  manipulators  Trajectory  Skin  Planning  Kernel  Robot sensing systems  Cost function 
Abstract: This work presents a new contact-based 3D path planning approach for manipulators using robot skin. We make use of the Stochastic Functional Gradient Path Planner, extending it to the 3D case, and assess its usefulness in combination with multi-modal robot skin. Our proposed algorithm is verified on a 6 DOF robot arm that has been covered with multi-modal robot skin. The experimental platform is combined with a skin based compliant controller, making the robot inherently reactive. We implement different state-of-the-art planners within our contact-based robot system to compare their performance under the same conditions. In this way, all the planners use the same skin compliant control during evaluation. Furthermore, we extend the stochastic planner with tactile-based explorative behavior to improve its performance, especially for unknown environments. We show that CHiMP is able to outperform state of the art algorithms when working with skin-based sparse contact data.


Title: Support Surface Estimation for Legged Robots
Key Words: Gaussian processes  legged locomotion  path planning  regression analysis  robot dynamics  terrain mapping  legged robots  legged systems  rugged outdoor environments  terrain geometry  foothold planning  safe locomotion  penetrable terrain  depth sensors  haptic information  foot contact closure locations  exteroceptive sensing  dense support surface estimate  Gaussian process regression  support surface estimation procedure  penetrable surface layer  discrete penetration depth measurements  continuous support surface estimate  partial exteroceptive information  terrain topography  quadrupedal robot ANYmal  Vegetation mapping  Surface topography  Kernel  Estimation  Robot sensing systems 
Abstract: The high agility of legged systems allows them to operate in rugged outdoor environments. In these situations, knowledge about the terrain geometry is key for foothold planning to enable safe locomotion. However, on penetrable or highly compliant terrain (e.g. grass) the visibility of the supporting ground surface is obstructed, i.e. it cannot directly be perceived by depth sensors. We present a method to estimate the underlying terrain topography by fusing haptic information about foot contact closure locations with exteroceptive sensing. To obtain a dense support surface estimate from sparsely sampled footholds we apply Gaussian process regression. Exteroceptive information is integrated into the support surface estimation procedure by estimating the height of the penetrable surface layer from discrete penetration depth measurements at the footholds. The method is designed such that it provides a continuous support surface estimate even if there is only partial exteroceptive information available due to shadowing effects. Field experiments with the quadrupedal robot ANYmal show how the robot can smoothly and safely navigate in dense vegetation.


Title: Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning
Key Words: approximation theory  control engineering computing  dynamic programming  gradient methods  learning (artificial intelligence)  mobile robots  optimal control  partial differential equations  dynamic programming equation  contraction mapping  Hamilton-Jacobi safety analysis  control-theoretic safety analysis  optimal safety policy  quantitative safety analysis  reinforcement learning techniques  time-discounted modification  optimal control problems  robust optimal control theory  autonomous robotic systems  policy gradient techniques  value learning  Safety  Automation  Reinforcement learning  Robots  Optimal control  Jacobian matrices  Reachability analysis 
Abstract: Safety analysis is a necessary component in the design and deployment of autonomous robotic systems. Techniques from robust optimal control theory, such as Hamilton-Jacobi reachability analysis, allow a rigorous formalization of safety as guaranteed constraint satisfaction. Unfortunately, the computational complexity of these tools for general dynamical systems scales poorly with state dimension, making existing tools impractical beyond small problems. Modern reinforcement learning methods have shown promising ability to find approximate yet proficient solutions to optimal control problems in complex and high-dimensional systems, however their application has in practice been restricted to problems with an additive payoff over time, unsuitable for reasoning about safety. In recent work, we introduced a time-discounted modification of the problem of maximizing the minimum payoff over time, central to safety analysis, through a modified dynamic programming equation that induces a contraction mapping. Here, we show how a similar contraction mapping can render reinforcement learning techniques amenable to quantitative safety analysis as tools to approximate the safe set and optimal safety policy. This opens a new avenue of research connecting control-theoretic safety analysis and the reinforcement learning domain. We validate the correctness of our formulation by comparing safety results computed through Q-learning to analytic and numerical solutions, and demonstrate its scalability by learning safe sets and control policies for simulated systems of up to 18 state dimensions using value learning and policy gradient techniques.


Title: OVPC Mesh: 3D Free-space Representation for Local Ground Vehicle Navigation
Key Words: computational geometry  mesh generation  mobile robots  navigation  path planning  remotely operated vehicles  robot vision  stereo image processing  OVPC Mesh  3D free-space representation  local ground vehicle navigation  autonomous unmanned ground vehicle  Visible Point Clouds Mesh  local point cloud data  UGV navigation  on visible point clouds mesh  watertight 3D mesh generation  trajectory planning  robot  Three-dimensional displays  Navigation  Robot sensing systems  Planning  Laser radar  Real-time systems 
Abstract: This paper presents a novel approach for local 3D environment representation for autonomous unmanned ground vehicle (UGV) navigation called On Visible Point Clouds Mesh (OVPC Mesh). Our approach represents the surrounding of the robot as a watertight 3D mesh generated from local point cloud data in order to represent the free space surrounding the robot. It is a conservative estimation of the free space and provides a desirable trade-off between representation precision and computational efficiency, without having to discretize the environment into a fixed grid size. Our experiments analyze the usability of the approach for UGV navigation in rough terrain, both in simulation and in a fully integrated real-world system. Additionally, we compare our approach to well-known state-of the-art solutions, such as Octomap and Elevation Mapping and show that OVPC Mesh can provide reliable 3D information for trajectory planning while fulfilling real-time constraints.


Title: Stable Bin Packing of Non-convex 3D Objects with a Robot Manipulator
Key Words: computational geometry  control engineering computing  industrial manipulators  production engineering computing  warehouse automation  nonconvex objects  bin packing  heightmap-minimization heuristic  constructive packing pipeline  placement plans  robot motion  automated warehousing domain  packing problem  fully automatic object packing  robot manipulator  nonconvex 3D objects  high-quality packing  robot packability constraints  Robots  Three-dimensional displays  Containers  Stability analysis  Collision avoidance  Pipelines  Geometry 
Abstract: Recent progress in the field of robotic manipulation has generated interest in fully automatic object packing in warehouses. This paper proposes a formulation of the packing problem that is tailored to the automated warehousing domain. Besides minimizing waste space inside a container, the problem requires stability of the object pile during packing and the feasibility of the robot motion executing the placement plans. To address this problem, a set of constraints are formulated, and a constructive packing pipeline is proposed to solve these constraints. The pipeline is able to pack geometrically complex, non-convex objects while satisfying stability and robot packability constraints. In particular, a new 3D positioning heuristic called Heightmap-Minimization heuristic is proposed, and heightmaps are used to speed up the search. Experimental evaluation of the method is conducted with a realistic physical simulator on a dataset of scanned real-world items, demonstrating stable and high-quality packing plans compared with other 3D packing methods.


Title: Model-Free Optimal Estimation and Sensor Placement Framework for Elastic Kinematic Chain
Key Words: elasticity  manipulator kinematics  maximum likelihood estimation  motion control  optimal control  probability  sensor placement  sensors  inertial measurement unit sensors  high-DOF EKC  high-degree-of-freedom EKC  maximum a posteriori estimation  posterior probability  real-time output estimation  optimal placement  optimal IMU placement  MAP estimation  POD mode  nondominant modes  proper orthogonal decomposition  IMU sensors  elastic kinematic chain  sensor placement framework  model-free optimal estimation  Estimation  Robot sensing systems  Vibrations  Kinematics  Manipulators  Telerobotics  Covariance matrices 
Abstract: We propose a novel model-free optimal estimation and sensor placement framework for a high-DOF (degree-of-freedom) EKC (elastic kinematic chain) with only a limited number of IMU (inertial measurement unit) sensors based on POD (proper orthogonal decomposition) and MAP (maximum a posteriori) estimation. First, we (off-line) excite the system richly enough, collect the data and perform the POD to extract dominant and non-dominant modes. We then decide the minimum number of IMUs according to the dominant modes, and construct the prior distribution of the output (i.e., top-end position of EKC) based on the singular value of each POD mode. We also formulate the MAP estimation given the prior distribution and different placements of the IMUs and choose the optimal IMU placement to maximize the posterior probability. This optimal placement is then used for real-time output estimation of the EKC. Experiments are also performed to verify the theory.


Title: Chance Constrained Motion Planning for High-Dimensional Robots
Key Words: collision avoidance  mobile robots  optimal control  probability  chance constrained motion planning  high-dimensional robots  Probabilistic Chekov  chance-constrained motion planning system  degree-of-freedom robots  motion uncertainty  state information  observation noise models  deterministic motion planning  integrated trajectory optimization  sparse roadmap framework  planning speed  high-dimensional tasks  linear-quadratic Gaussian motion planning approach  robot state probability distribution  collision risk estimation  robotic planning tasks  p-Chekov system  user-specified chance constraints  real-world planning scenarios  Planning  Robots  Trajectory  Collision avoidance  Task analysis  Estimation  Uncertainty 
Abstract: This paper introduces Probabilistic Chekov (p-Chekov), a chance-constrained motion planning system that can be applied to high degree-of-freedom (DOF) robots under motion uncertainty and imperfect state information. Given process and observation noise models, it can find feasible trajectories which satisfy a user-specified bound over the probability of collision. Leveraging our previous work in deterministic motion planning which integrated trajectory optimization into a sparse roadmap framework, p-Chekov shows superiority in its planning speed for high-dimensional tasks. P-Chekov incorporates a linear-quadratic Gaussian motion planning approach into the estimation of the robot state probability distribution, applies quadrature theories to waypoint collision risk estimation, and adapts risk allocation approaches to assign allowable probabilities of failure among waypoints. Unlike other existing risk-aware planners, p-Chekov can be applied to high-DOF robotic planning tasks without the convexification of the environment. The experiment results in this paper show that this p-Chekov system can effectively reduce collision risk and satisfy user-specified chance constraints in typical real-world planning scenarios for high-DOF robots.


Title: Visual Representations for Semantic Target Driven Navigation
Key Words: image representation  image segmentation  learning (artificial intelligence)  mobile robots  navigation  path planning  robot vision  visual representations  semantic target driven navigation  semantic visual navigation  semantic segmentation  domain adaptation  computer vision algorithms  robot  deep network  navigation policy learning  Navigation  Visualization  Semantics  Training  Adaptation models  Robots  Task analysis 
Abstract: What is a good visual representation for navigation? We study this question in the context of semantic visual navigation, which is the problem of a robot finding its way through a previously unseen environment to a target object, e.g. go to the refrigerator. Instead of acquiring a metric semantic map of an environment and using planning for navigation, our approach learns navigation policies on top of representations that capture spatial layout and semantic contextual cues. We propose to use semantic segmentation and detection masks as observations obtained by state-of-the-art computer vision algorithms and use a deep network to learn the navigation policy. The availability of equitable representations in simulated environments enables joint training using real and simulated data and alleviates the need for domain adaptation or domain randomization commonly used to tackle the sim-to-real transfer of the learned policies. Both the representation and the navigation policy can be readily applied to real non-synthetic environments as demonstrated on the Active Vision Dataset [1]. Our approach successfully gets to the target in 54% of the cases in unexplored environments, compared to 46% for a non-learning based approach, and 28% for a learning-based baseline.


Title: Variational End-to-End Navigation and Localization
Key Words: cameras  Global Positioning System  learning (artificial intelligence)  mobile robots  navigation  path planning  probability  variational techniques  point-topoint navigation algorithms  full-scale autonomous vehicle  localization algorithm  variational end-to-end navigation  deep learning  autonomous vehicle control  raw sensory data  navigation instruction  end-to-end driving networks  point-to-point navigation  probabilistic localization  noisy GPS data  raw camera data  higher level roadmaps  probability distribution  deterministic control command  rough localization  real-world driving data  variational network  Navigation  Roads  Cameras  Robot sensing systems  Visualization  Partitioning algorithms 
Abstract: Deep learning has revolutionized the ability to learn “end-to-end” autonomous vehicle control directly from raw sensory data. While there have been recent extensions to handle forms of navigation instruction, these works are unable to capture the full distribution of possible actions that could be taken and to reason about localization of the robot within the environment. In this paper, we extend end-to-end driving networks with the ability to perform point-to-point navigation as well as probabilistic localization using only noisy GPS data. We define a novel variational network capable of learning from raw camera data of the environment as well as higher level roadmaps to predict (1) a full probability distribution over the possible control commands; and (2) a deterministic control command capable of navigating on the route specified within the map. Additionally, we formulate how our model can be used to localize the robot according to correspondences between the map and the observed visual road topology, inspired by the rough localization that human drivers can perform. We test our algorithms on real-world driving data that the vehicle has never driven through before, and integrate our point-topoint navigation algorithms onboard a full-scale autonomous vehicle for real-time performance. Our localization algorithm is also evaluated over a new set of roads and intersections to demonstrates rough pose localization even in situations without any GPS prior.


Title: Robust Learning of Tactile Force Estimation through Robot Interaction
Key Words: control engineering computing  force feedback  learning (artificial intelligence)  neural nets  robots  tactile sensors  learned force model  robust learning  tactile force estimation  robot interaction  analytic models  robust model  SynTouch BioTac sensor  neural networks  voxelized input feature layer  spatial signals  sensor surface  robust tactile force model  force torque sensor  FT sensor  force inference  planar pushing task  force direction  force estimation  tactile sensor signals  force feedback grasp controller  Force  Robot sensing systems  Biological system modeling  Task analysis  Biosensors  Neural networks 
Abstract: Current methods for estimating force from tactile sensor signals are either inaccurate analytic models or task-specific learned models. In this paper, we explore learning a robust model that maps tactile sensor signals to force. We specifically explore learning a mapping for the SynTouch BioTac sensor via neural networks. We propose a voxelized input feature layer for spatial signals and leverage information about the sensor surface to regularize the loss function. To learn a robust tactile force model that transfers across tasks, we generate ground truth data from three different sources: (1) the BioTac rigidly mounted to a force torque (FT) sensor, (2) a robot interacting with a ball rigidly attached to the same FT sensor, and (3) through force inference on a planar pushing task by formalizing the mechanics as a system of particles and optimizing over the object motion. A total of 140k samples were collected from the three sources. We achieve a median angular accuracy of 3.5 degrees in predicting force direction (66% improvement over the current state of the art) and a median magnitude accuracy of 0.06 N (93% improvement) on a test dataset. Additionally, we evaluate the learned force model in a force feedback grasp controller performing object lifting and gentle placement. Our results can be found on https: //sites.google.com/view/tactile-force.


Title: Learning Scene Geometry for Visual Localization in Challenging Conditions
Key Words: feature extraction  image colour analysis  image retrieval  learning (artificial intelligence)  object detection  robot vision  daytime images  learning scene geometry  visual localization  outdoor large scale image  cross-season  learned global image descriptor  scene geometry information  depth map  query image  localization accuracy  cross-weather  long-term localization scenario  night images  winter localization sequence  summer localization sequence  Training  Decoding  Feature extraction  Robots  Image reconstruction  Geometry  Visualization 
Abstract: We propose a new approach for outdoor large scale image based localization that can deal with challenging scenarios like cross-season, cross-weather, day/night and long-term localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We are able to increase recall@1 performances by 2.15% on cross-weather and long-term localization scenario and by 4.24% points on a challenging winter/summer localization sequence versus state-of-the-art methods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images.


Title: Search-based 3D Planning and Trajectory Optimization for Safe Micro Aerial Vehicle Flight Under Sensor Visibility Constraints
Key Words: aerospace safety  collision avoidance  graph theory  navigation  search problems  trajectory optimisation (aerospace)  trajectory optimization  sensor visibility constraints  obstacle-free flight paths  Velodyne Puck Lite 3D laser scanner  flight dynamics  navigation safety  allocentric complete planning  microaerial vehicle flight safety  search-based 3D planning  collision avoidance  graph search  Planning  Robot sensing systems  Three-dimensional displays  Trajectory optimization  Vehicle dynamics 
Abstract: Safe navigation of Micro Aerial Vehicles (MAVs) requires not only obstacle-free flight paths according to a static environment map, but also the perception of and reaction to previously unknown and dynamic objects. This implies that the onboard sensors cover the current flight direction. Due to the limited payload of MAVs, full sensor coverage of the environment has to be traded off with flight time. Thus, often only a part of the environment is covered. We present a combined allocentric complete planning and trajectory optimization approach taking these sensor visibility constraints into account. The optimized trajectories yield flight paths within the apex angle of a Velodyne Puck Lite 3D laser scanner enabling low-level collision avoidance to perceive obstacles in the flight direction. Furthermore, the optimized trajectories take the flight dynamics into account and contain the velocities and accelerations along the path. We evaluate our approach with a DJI Matrice 600 MAV and in simulation employing hardware-in-the-loop.


Title: SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation
Key Words: convolutional neural nets  image classification  image motion analysis  image resolution  image sampling  learning (artificial intelligence)  pose estimation  self-supervised monocular depth estimation  self-supervised monocular depth prediction  subpixel convolutional layer extension  depth super-resolution  high-resolution disparities  low-resolution convolutional features  flip-augmentation layer  single-image super-resolution  deep learning methods  super-resolved monocular depth estimation  public KITTI benchmark  pose estimation  Estimation  Convolutional codes  Cameras  Spatial resolution  Three-dimensional displays  Training 
Abstract: Recent techniques in self-supervised monocular depth estimation are approaching the performance of supervised methods, but operate in low resolution only. We show that high resolution is key towards high-fidelity self-supervised monocular depth prediction. Inspired by recent deep learning methods for Single-Image Super-Resolution, we propose a subpixel convolutional layer extension for depth super-resolution that accurately synthesizes high-resolution disparities from their corresponding low-resolution convolutional features. In addition, we introduce a differentiable flip-augmentation layer that accurately fuses predictions from the image and its horizontally flipped version, reducing the effect of left and right shadow regions generated in the disparity map due to occlusions. Both contributions provide significant performance gains over the state-of-the-art in self-supervised depth and pose estimation on the public KITTI benchmark. A video of our approach can be found at https://youtu.be/jKNgBeBMx0I.


Title: Improving Underwater Obstacle Detection using Semantic Image Segmentation
Key Words: feature extraction  image classification  image enhancement  image matching  image segmentation  learning (artificial intelligence)  mobile robots  path planning  robot vision  stereo image processing  image-based underwater obstacle detection  sparse stereo point clouds  monocular semantic image segmentation  cluttered underwater environments  robust robotic path planning  feature-based stereo matching  learning-based segmentation  robust obstacle map  direct binary learning  underwater obstacles  multiclass learning approach  binary map  sparse stereo matching  3D obstacle maps  coral reef environments  image-wide obstacle detection  dynamic objects  image-based obstacle maps  Image segmentation  Semantics  Cameras  Three-dimensional displays  Real-time systems  Training  Robots 
Abstract: This paper presents two novel approaches for improving image-based underwater obstacle detection by combining sparse stereo point clouds with monocular semantic image segmentation. Generating accurate image-based obstacle maps in cluttered underwater environments, such as coral reefs, are essential for robust robotic path planning and navigation. However, these maps can be challenged by factors including visibility, lighting and dynamic objects (e.g. fish) that may lead to falsely identified free space or dynamic objects which trajectory planners may react to undesirably. We propose combining feature-based stereo matching with learning-based segmentation to produce a more robust obstacle map. This approach considers direct binary learning of the presence or absence of underwater obstacles, as well as a multiclass learning approach to classify their distance (near, mid and far) in the scene. An enhancement to the binary map is also shown by including depth information from sparse stereo matching to produce 3D obstacle maps of the scene. The performance is evaluated using field data collected in cluttered, and at times, visually degraded coral reef environments. The results show improved image-wide obstacle detection, rejection of transient objects (such as fish), and range estimation compared to feature-based sparse and dense stereo point clouds alone.


Title: RCM-SLAM: Visual localisation and mapping under remote centre of motion constraints
Key Words: cameras  image motion analysis  image reconstruction  medical robotics  mobile robots  pose estimation  robot vision  SLAM (robots)  surgery  laparoscopic camera motion  RCM constraints  minimal solver  absolute camera  2D-3D point correspondences  bundle adjustment optimiser  RCM-constrained parameterisation  relative pose estimation  SLAM pipeline suitable  robotic surgery  RCM position  robotic prostatectomy show  RCM-SLAM  visual localisation  remote centre  motion constraints  insertion ports  Simultaneous Localisation and Mapping  mapping approach  RCM-PnP  Cameras  Robot vision systems  Simultaneous localization and mapping  Laparoscopes  Three-dimensional displays  Robot kinematics 
Abstract: In robotic surgery the motion of instruments and the laparoscopic camera is constrained by their insertion ports, i. e. a remote centre of motion (RCM). We propose a Simultaneous Localisation and Mapping (SLAM) approach that estimates laparoscopic camera motion under RCM constraints. To achieve this we derive a minimal solver for the absolute camera pose given two 2D-3D point correspondences (RCM-PnP) and also a bundle adjustment optimiser that refines camera poses within an RCM-constrained parameterisation. These two methods are used together with previous work on relative pose estimation under RCM [1] to assemble a SLAM pipeline suitable for robotic surgery. Our simulations show that RCM-PnP outperforms conventional PnP for a wide noise range in the RCM position. Results with video footage from a robotic prostatectomy show that RCM constraints significantly improve camera pose estimation.


Title: Guaranteed Globally Optimal Planar Pose Graph and Landmark SLAM via Sparse-Bounded Sums-of-Squares Programming
Key Words: graph theory  maximum likelihood estimation  mobile robots  navigation  nonlinear programming  path planning  polynomials  pose estimation  robot vision  SLAM (robots)  autonomous navigation  nonlinear optimization techniques  maximum likelihood estimate  robot trajectory  polynomial optimization programs  planar pose graph  landmark SLAM  sparse-bounded sums-of-squares programming  simultaneous localization and mapping  pose-graph SLAM problem  sum-of-squares convex  SOS convex  sparse bounded degree sum-of-squares optimization method  sparse-BSOS optimization method  Simultaneous localization and mapping  Optimization  Maximum likelihood estimation  Position measurement  Noise measurement  Transmission line matrix methods 
Abstract: Autonomous navigation requires an accurate model or map of the environment. While dramatic progress in the prior two decades has enabled large-scale simultaneous localization and mapping (SLAM), the majority of existing methods rely on non-linear optimization techniques to find the maximum likelihood estimate (MLE) of the robot trajectory and surrounding environment. These methods are prone to local minima and are thus sensitive to initialization. Several recent papers have developed optimization algorithms for the Pose-Graph SLAM problem that can certify the optimality of a computed solution. Though this does not guarantee a priori that this approach generates an optimal solution, a recent extension has shown that when the noise lies within a critical threshold that the solution to the optimization algorithm is guaranteed to be optimal. To address the limitations of existing approaches, this paper illustrates that the Pose-Graph SLAM and Landmark SLAM can be formulated as polynomial optimization programs that are sum-of-squares (SOS) convex. This paper then describes how the Pose-Graph and Landmark SLAM problems can be solved to a global minimum without initialization regardless of noise level using the sparse bounded degree sum-of-squares (Sparse-BSOS) optimization method. Finally, the superior performance of the proposed approach when compared to existing SLAM methods is illustrated on graphs with several hundred nodes.


Title: Building a Winning Self-Driving Car in Six Months
Key Words: automobiles  closed loop systems  mobile robots  multi-robot systems  robot vision  basic autonomy features  robust algorithms  multisensor visual localization solution  winning self-driving car  SAE AutoDrive Challenge  Level 4 autonomous vehicle  Yuma  Arizona  Zeus' complete system architecture  CPU  closed-loop performance  Cameras  Real-time systems  Navigation  Computer architecture  Laser radar  Visualization  Systems architecture 
Abstract: The SAE AutoDrive Challenge is a three-year competition to develop a Level 4 autonomous vehicle by 2020. The first set of challenges were held in April of 2018 in Yuma, Arizona. Our team (aUToronto/Zeus) placed first. In this paper, we describe Zeus' complete system architecture and specialized algorithms that enabled us to win. We show that it is possible to develop a vehicle with basic autonomy features in just six months relying on simple, robust algorithms. We do not make use of a prior map. Instead, we have developed a multi-sensor visual localization solution. All the algorithms in the paper run in real-time using CPUs only. We also highlight the closed-loop performance of the system in detail in several experiments.


Title: Adaptive motor control and learning in a spiking neural network realised on a mixed-signal neuromorphic processor
Key Words: control engineering computing  feedback  learning (artificial intelligence)  mobile robots  neural chips  neurocontrollers  adaptive motor control  mixed-signal neuromorphic processor  neuromorphic computing  biological neural networks  spiking neural network architecture  sensory feedback  control rotational velocity  robotic vehicle  correct motor command  miniature mobile vehicle  two-layer spiking neural network  neuromorphic chip  purely neuromorphic motor control  spiking neurons  neuromorphic device  on-chip plastic synaptic weights  Neuromorphics  Neurons  Biological neural networks  Computer architecture  Robot sensing systems  Sociology 
Abstract: Neuromorphic computing is a new paradigm for design of both the computing hardware and algorithms inspired by biological neural networks. The event-based nature and the inherent parallelism make neuromorphic computing a promising paradigm for building efficient neural network based architectures for control of fast and agile robots. In this paper, we present a spiking neural network architecture that uses sensory feedback to control rotational velocity of a robotic vehicle. When the velocity reaches the target value, the mapping from the target velocity of the vehicle to the correct motor command, both represented in the spiking neural network on the neuromorphic device, is autonomously stored on the device using on-chip plastic synaptic weights. We validate the controller using a wheel motor of a miniature mobile vehicle and inertia measurement unit as the sensory feedback and demonstrate online learning of a simple “inverse model” in a two-layer spiking neural network on the neuromorphic chip. The prototype neuromorphic device that features 256 spiking neurons allows us to realise a simple proof of concept architecture for the purely neuromorphic motor control and learning. The architecture can be easily scaled-up if a larger neuromorphic device is available.


Title: Adaptive Genomic Evolution of Neural Network Topologies (AGENT) for State-to-Action Mapping in Autonomous Agents
Key Words: autonomous aerial vehicles  collision avoidance  genetic algorithms  learning (artificial intelligence)  neurocontrollers  autonomous agents  neural networks  NN  evolutionary algorithm  state-to-action mapping model  neuroevolution process  population diversity  unmanned aerial vehicle collision avoidance problem  adaptive genomic evolution  neural network topologies  augmented topologies formalism  Open AI platform  UAV collision avoidance problem  Genomics  Topology  Sociology  Statistics  Artificial neural networks  Network topology 
Abstract: Neuroevolution is a process of training neural networks (NN) through an evolutionary algorithm, usually to serve as a state-to-action mapping model in control or reinforcement learning-type problems. This paper builds on the Neuro Evolution of Augmented Topologies (NEAT) formalism that allows designing topology and weight evolving NNs. Fundamental advancements are made to the neuroevolution process to address premature stagnation and convergence issues, central among which is the incorporation of automated mechanisms to control the population diversity and average fitness improvement within the neuroevolution process. Insights into the performance and efficiency of the new algorithm is obtained by evaluating it on three benchmark problems from the Open AI platform and an Unmanned Aerial Vehicle (UAV) collision avoidance problem.


Title: A Motion Planning Scheme for Cooperative Loading Using Heterogeneous Robotic Agents
Key Words: collision avoidance  mobile robots  motion control  probability  obstacle avoidance  robotic agents  probabilistic road maps technique  cooperative loading task  redundant static manipulator  mobile platform  static obstacles  cluttered workspace  control architecture  decentralized motion planning  motion planning scheme  convergence properties  motion control scheme  optimal loading configuration  Manipulators  Loading  Task analysis  Robot kinematics  Kinematics  Mobile robots 
Abstract: In this work, we present a decentralized motion planning and control architecture for the cooperative loading task using heterogeneous robotic agents operating in a cluttered workspace with static obstacles. Initially, we tackle the problem of calculating a set of feasible loading configurations via a Probabilistic Road Maps technique. Next, an optimal loading configuration is selected considering the connectivity of the space and the Euclidean distance between the robotic agents. A motion control scheme for each agent is designed and implemented in order to autonomously guide each robot to the desired loading configuration with guaranteed obstacle avoidance and convergence properties. The performance and the applicability of the proposed strategy is experimentally verified in a variety of loading scenarios using a redundant static manipulator and a mobile platform.


Title: Go with the Flow: Exploration and Mapping of Pedestrian Flow Patterns from Partial Observations
Key Words: mobile robots  navigation  path planning  pedestrians  stochastic processes  pedestrian flow patterns  partial observations  safe robot navigation  spatial constraints  temporal constraints  multiple Poisson processes  long-term pedestrian datasets  uninformed exploration strategies  human motion patterns  robot navigation  Robots  Uncertainty  Predictive models  Buildings  Probabilistic logic  Data models  Collision avoidance 
Abstract: Understanding how people are likely to behave in an environment is a key requirement for efficient and safe robot navigation. However, mobile platforms are subject to spatial and temporal constraints, meaning that only partial observations of human activities are typically available to a robot, while the activity patterns of people in a given environment may also change at different times. To address these issues we present as the main contribution an exploration strategy for acquiring models of pedestrian flows, which decides not only the locations to explore but also the times when to explore them. The approach is driven by the uncertainty from multiple Poisson processes built from past observations. The approach is evaluated using two long-term pedestrian datasets, comparing its performance against uninformed exploration strategies. The results show that when using the uncertainty in the exploration policy, model accuracy increases, enabling faster learning of human motion patterns.


Title: Steering a Multi-armed Robotic Sheath Using Eccentric Precurved Tubes
Key Words: dexterous manipulators  elasticity  medical robotics  neurophysiology  pipes  robot kinematics  surgery  kinematic model  multiarmed robotic sheath  eccentric precurved tubes  single-port minimally invasive procedures  multiple robotic arms  precurved superelastic tubes  Cosserat rod theory  two-arm sheath  concentric tube balanced pair  neuroendoscopy  elastic backbone  push-pull tendons  continuum robot sheath  Electron tubes  Force  Kinematics  Shape  Endoscopes  Manipulators  Steerable sheath  Multiple arms  Concentric tube robots 
Abstract: This paper presents a novel continuum robot sheath for use in single-port minimally invasive procedures such as neuroendoscopy in which the sheath is designed to deliver multiple robotic arms. Articulation of the sheath is achieved by using precurved superelastic tubes lining the working channels used for arm delivery. These tubes perform a similar role to push/pull tendons, but can accomplish shape change of the sheath via rotation as well as translation. A kinematic model using Cosserat rod theory is derived which is based on modeling the system as a set of eccentrically aligned precurved tubes constrained along their length by an elastic backbone. The specific case of a two-arm sheath is considered in detail and its relationship to a concentric tube balanced pair is described. Simulation and experiment are used to investigate the concept, map its workspace and to evaluate the kinematic model.


Title: Online Continuous Mapping using Gaussian Process Implicit Surfaces
Key Words: approximation theory  Gaussian processes  mobile robots  path planning  regression analysis  implicit surface  SDF  regressor  gaussian process implicit surface  robotic tasks  signed-distance function  sparse measurements  grid-based methods  online continuous mapping  Surface treatment  Planning  Noise measurement  Training  Robot kinematics  Robot sensing systems 
Abstract: The representation of the environment strongly affects how robots can move and interact with it. This paper presents an online approach for continuous mapping using Gaussian Process Implicit Surfaces (GPISs). Compared with grid-based methods, GPIS better utilizes sparse measurements to represent the world seamlessly. It provides direct access to the signed-distance function (SDF) and its derivatives which are invaluable for other robotic tasks and it incorporates uncertainty in the sensor measurements. Our approach incrementally and efficiently updates GPIS by employing a regressor on observations and a spatial tree structure. The effectiveness of the suggested approach is demonstrated using simulations and real world 2D/3D data.


Title: Dense 3D Visual Mapping via Semantic Simplification
Key Words: data visualisation  image reconstruction  image segmentation  mesh generation  robot vision  solid modelling  semantic image segmentation  perceived point cloud  global statistics  class boundaries  infra-class edges  3D dense model  3D Delaunay Triangulation  variable point cloud density  semantic simplification  dense 3D visual mapping estimates  dense point clouds  pixel depths  point cloud simplification methods  roughly planar surface  Three-dimensional displays  Semantics  Solid modeling  Image reconstruction  Image segmentation  Structure from motion  Surface reconstruction 
Abstract: Dense 3D visual mapping estimates as many as possible pixel depths, for each image. This results in very dense point clouds that often contain redundant and noisy information, especially for surfaces that are roughly planar, for instance, the ground or the walls in the scene. In this paper we leverage on semantic image segmentation to discriminate which regions of the scene require simplification and which should be kept at high level of details. We propose four different point cloud simplification methods which decimate the perceived point cloud by relying on class-specific local and global statistics still maintaining more points in the proximity of class boundaries to preserve the infra-class edges and discontinuities. 3D dense model is obtained by fusing the point clouds in a 3D Delaunay Triangulation to deal with variable point cloud density. In the experimental evaluation we have shown that, by leveraging on semantics, it is possible to simplify the model and diminish the noise affecting the point clouds.


Title: Predicting the Layout of Partially Observed Rooms from Grid Maps
Key Words: control engineering computing  mobile robots  path planning  robot vision  SLAM (robots)  SLAM algorithms  2D metric grid map  global structure  partially observed rooms  autonomous mobile robots  indoor environments  robot sensors  geometrical primitives  Layout  Measurement  Robots  Indoor environment  Two dimensional displays  Three-dimensional displays  Feature extraction 
Abstract: In several applications, autonomous mobile robots benefit from knowing the structure of the indoor environments where they operate. This knowledge can be extracted from the metric maps built (e.g., using SLAM algorithms) from the data perceived by the robots' sensors. The layout is a way to represent the structure of an indoor environment with geometrical primitives. Most of the current methods for reconstructing the layout from a metric map represent the parts of the environment that have been fully observed. In this paper, we propose an approach that predicts the layout of rooms which are only partially known in a 2D metric grid map. The prediction is made according to the global structure of the environment, as identified from its known parts. Experiments show that our approach is able to effectively predict the layout of several indoor environments that have been observed to different degrees.


Title: FSMI: Fast Computation of Shannon Mutual Information for Information-Theoretic Mapping
Key Words: approximation theory  gradient methods  information theory  mobile robots  multi-robot systems  FSMI algorithm  fast Shannon mutual information  Cauchy-Schwarz quadratic mutual information  information-based mapping algorithms  information-theoretic mapping  CSQMI  FSMI  Mutual information  Approximation algorithms  Robot sensing systems  Measurement  Standards  Random variables 
Abstract: Information-based mapping algorithms are critical to robot exploration tasks in several applications ranging from disaster response to space exploration. Unfortunately, most existing information-based mapping algorithms are plagued by the computational difficulty of evaluating the Shannon mutual information between potential future sensor measurements and the map. This has lead researchers to develop approximate methods, such as Cauchy-Schwarz Quadratic Mutual Information (CSQMI). In this paper, we propose a new algorithm, called Fast Shannon Mutual Information (FSMI), which is significantly faster than existing methods at computing the exact Shannon mutual information. The key insight behind FSMI is recognizing that the integral over the sensor beam can be evaluated analytically, removing an expensive numerical integration. In addition, we provide a number of approximation techniques for FSMI, which significantly improve computation time. Equipped with these approximation techniques, the FSMI algorithm is more than three orders of magnitude faster than the existing computation for Shannon mutual information; it also outperforms the CSQMI algorithm significantly, being roughly twice as fast, in our experiments.


Title: Real-time Scalable Dense Surfel Mapping
Key Words: image fusion  image reconstruction  pose estimation  SLAM (robots)  intensity images  depth images  globally consistent model  room-scale environments  urban-scale environments  RGB-D cameras  stereo cameras  monocular camera  superpixel-based surfels  reconstructed models  fast map deformation  global consistency  room-scale reconstruction  time scalable dense surfel  CPU computation  sparse SLAM system  camera poses  dense surfel mapping system  Cameras  Image reconstruction  Strain  Fuses  Robot vision systems  Simultaneous localization and mapping  Three-dimensional displays 
Abstract: In this paper, we propose a novel dense surfel mapping system that scales well in different environments with only CPU computation. Using a sparse SLAM system to estimate camera poses, the proposed mapping system can fuse intensity images and depth images into a globally consistent model. The system is carefully designed so that it can build from room-scale environments to urban-scale environments using depth images from RGB-D cameras, stereo cameras or even a monocular camera. First, superpixels extracted from both intensity and depth images are used to model surfels in the system. superpixel-based surfels make our method both runtime efficient and memory efficient. Second, surfels are further organized according to the pose graph of the SLAM system to achieve O(1) fusion time regardless of the scale of reconstructed models. Third, a fast map deformation using the optimized pose graph enables the map to achieve global consistency in real-time. The proposed surfel mapping system is compared with other state-of-the-art methods on synthetic datasets. The performances of urban-scale and room-scale reconstruction are demonstrated using the KITTI dataset [1] and autonomous aggressive flights, respectively. The code is available for the benefit of the community.


Title: Flight, Camera, Action! Using Natural Language and Mixed Reality to Control a Drone
Key Words: aerospace computing  autonomous aerial vehicles  control engineering computing  human-robot interaction  Markov processes  mobile robots  natural language processing  user interfaces  natural language commands  Markov decision process framework  web interface  MR interface  exploratory user study  fully autonomous language grounding  autonomous drone  natural language grounding  goal-oriented setting  mixed reality  radio-controlled controller  users control  Drones  Natural languages  Virtual reality  Robots  Task analysis  Planning  Grounding 
Abstract: With increasing autonomy, robots like drones are increasingly accessible to untrained users. Most users control drones using a low-level interface, such as a radio-controlled (RC) controller. For a wider adoption of these technologies by the public, a much higher-level interface, such as natural language or mixed reality (MR), allows the automation of the control of the agent in a goal-oriented setting. We present an interface that uses natural language grounding within an MR environment to solve high-level task and navigational instructions given to an autonomous drone. To the best of our knowledge, this is the first work to perform fully autonomous language grounding in an MR setting for a robot. Given a map, our interface first grounds natural language commands to reward specifications within a Markov Decision Process (MDP) framework. Then, it passes the reward specification to an MDP solver. Finally, the drone performs the desired operations in the real world while planning and localizing itself. Our approach uses MR to provide a set of known virtual landmarks, enabling the drone to understand commands referring to objects without being equipped with object detectors for multiple novel objects or a predefined environment model. We conducted an exploratory user study to assess users' experience of our MR interface with and without natural language, as compared to a web interface. We found that users were able to command the drone more quickly via both MR interfaces as compared to the web interface, with roughly equal system usability scores across all three interfaces.


Title: An Interactive Scene Generation Using Natural Language
Key Words: dexterous manipulators  discrete event systems  learning (artificial intelligence)  natural language processing  natural scenes  text analysis  interactive scene generation  robotic drawing  discrete event system  MSCOCO evaluation dataset  CIDEr metric  ROUGH-L metric  METEOR metric  Amazon Mechanical Turk  natural language descriptions  Layout  Dogs  Robots  Generators  Natural language processing  Semantics  Training 
Abstract: Scene generation is an important step of robotic drawing. Recent works have shown success in scene generation conditioned on text using a variety of approaches, with which the generated scenes cannot be revised after its generation. To allow modification on generated scenes, we model the scene generation process as a discrete event system. Instead of training text-to-pixel mappings using large datasets, the proposed approach uses object instances retrieved from the Internet to synthesize scenes. Evaluated on 128 experiments using MSCOCO evaluation dataset, the result shows the scene generation performance has been increased by 197%, 22.3%, and 55.7% compared with the state of the art approach on three standard metrics (CIDEr, ROUGH-L, METEOR), respectively. Human evaluation conducted on Amazon Mechanical Turk shows over 80% of generated scenes are considered to have higher recognizability and better alignment with natural language descriptions than baseline works.


Title: Efficient Generation of Motion Plans from Attribute-Based Natural Language Instructions Using Dynamic Constraint Mapping
Key Words: graph theory  human-robot interaction  learning (artificial intelligence)  mobile robots  motion control  natural language processing  optimisation  path planning  robot programming  dynamic constraint mapping  robot motion planning  dynamic grounding graph  7-DOF Fetch robot  factor graph  optimization-based motion planning  parametric constraints  attribute-based natural language instructions  motion plans  Robots  Grounding  Natural languages  Cost function  Planning  Dynamics  Heuristic algorithms 
Abstract: We present an algorithm for combining natural language processing (NLP) and fast robot motion planning to automatically generate robot movements. Our formulation uses a novel concept called Dynamic Constraint Mapping to transform complex, attribute-based natural language instructions into appropriate cost functions and parametric constraints for optimization-based motion planning. We generate a factor graph from natural language instructions called the Dynamic Grounding Graph (DGG), which takes latent parameters into account. The coefficients of this factor graph are learned based on conditional random fields (CRFs) and are used to dynamically generate the constraints for motion planning. We map the cost function directly to the motion parameters of the planner and compute smooth trajectories in dynamic scenes. We highlight the performance of our approach in a simulated environment and via a human interacting with a 7-DOF Fetch robot using intricate language commands including negation, orientation specification, and distance constraints.


Title: Semantic Mapping for View-Invariant Relocalization
Key Words: cameras  feature extraction  image representation  object detection  pose estimation  robot vision  SLAM (robots)  semantic mapping  view-invariant relocalization  accurate local tracking  view-invariant object-driven relocalization  sampling-based approach  2D bounding box object detections  view-invariant representation  camera relocalization  view-invariance  relocalization rate  visual simultaneous localization and mapping  object landmarks  local appearance-based features  SLAM  3D pose  SIFT  mean rotational error  Three-dimensional displays  Cameras  Simultaneous localization and mapping  Semantics  Two dimensional displays  Visualization  Task analysis 
Abstract: We propose a system for visual simultaneous localization and mapping (SLAM) that combines traditional local appearance-based features with semantically meaningful object landmarks to achieve both accurate local tracking and highly view-invariant object-driven relocalization. Our mapping process uses a sampling-based approach to efficiently infer the 3D pose of object landmarks from 2D bounding box object detections. These 3D landmarks then serve as a view-invariant representation which we leverage to achieve camera relocalization even when the viewing angle changes by more than 125 degrees. This level of view-invariance cannot be attained by local appearance-based features (e.g. SIFT) since the same set of surfaces are not even visible when the viewpoint changes significantly. Our experiments show that even when existing methods fail completely for viewpoint changes of more than 70 degrees, our method continues to achieve a relocalization rate of around 90%, with a mean rotational error of around 8 degrees.


Title: Real-Time Monocular Object-Model Aware Sparse SLAM
Key Words: cameras  convolutional neural nets  feature extraction  learning (artificial intelligence)  mobile robots  object detection  robot vision  SLAM (robots)  mobile robotics  sparse point-based SLAM methods  CNN-based plane detector  semantic SLAM  simultaneous localization and mapping  camera localization  deep-learned object detector  CNN network  semantic objects representation  monocular object-model aware sparse SLAM framework  Simultaneous localization and mapping  Semantics  Image reconstruction  Real-time systems  Cameras  Three-dimensional displays 
Abstract: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in mobile robotics. While sparse point-based SLAM methods provide accurate camera localization, the generated maps lack semantic information. On the other hand, state of the art object detection methods provide rich information about entities present in the scene from a single image. This work incorporates a real-time deep-learned object detector to the monocular SLAM framework for representing generic objects as quadrics that permit detections to be seamlessly integrated while allowing the real-time performance. Finer reconstruction of an object, learned by a CNN network, is also incorporated and provides a shape prior for the quadric leading further refinement. To capture the structure of the scene, additional planar landmarks are detected by a CNN-based plane detector and modelled as independent landmarks in the map. Extensive experiments support our proposed inclusion of semantic objects and planar structures directly in the bundle-adjustment of SLAM - Semantic SLAM- that enriches the reconstructed map semantically, while significantly improving the camera localization.


Title: Probabilistic Projective Association and Semantic Guided Relocalization for Dense Reconstruction
Key Words: convolutional neural nets  image coding  image recognition  image reconstruction  image representation  image retrieval  image segmentation  object detection  probability  robot vision  SLAM (robots)  convolutional neural networks  geometric quality  probabilistic projective association  loop frames  2D labeling  CNN  semantic prediction  simultaneous localization and mapping system  SLAM system  3D scenes  randomized ferns  semantic recognition  geometric reconstruction  loop detection  reconstruction pipeline  semantic information  camera trajectory estimation  semantic labels  real-time dense mapping system  dense reconstruction  semantic guided relocalization  Semantics  Probabilistic logic  Labeling  Two dimensional displays  Cameras  Tracking loops  Trajectory 
Abstract: We present a real-time dense mapping system which uses the predicted 2D semantic labels for optimizing the geometric quality of reconstruction. With a combination of Convolutional Neural Networks (CNNs) for 2D labeling and a Simultaneous Localization and Mapping (SLAM) system for camera trajectory estimation, recent approaches have succeeded in incrementally fusing and labeling 3D scenes. However, the geometric quality of the reconstruction can be further improved by incorporating such semantic prediction results, which is not sufficiently exploited by existing methods. In this paper, we propose to use semantic information to improve two crucial modules in the reconstruction pipeline, namely tracking and loop detection, for obtaining mutual benefits in geometric reconstruction and semantic recognition. Specifically for tracking, we use a novel probabilistic projective association approach to efficiently pick out candidate correspondences, where the confidence of these correspondences is quantified concerning similarities on all available short-term invariant features. For the loop detection, we incorporate these semantic labels into the original encoding through Randomized Ferns to generate a more comprehensive representation for retrieving candidate loop frames. Evaluations on a publicly available synthetic dataset have shown the effectiveness of our approach that considers such semantic hints as a reliable feature for achieving higher geometric quality.


Title: What am I touching? Learning to classify terrain via haptic sensing
Key Words: control engineering computing  haptic interfaces  image classification  learning (artificial intelligence)  legged locomotion  pattern clustering  robot vision  terrain mapping  haptic sensing  mobile robots  real-world outdoors applications  robot control  optimal terrain negotiation  terrain classification  terrain identification  legged robot foot  fixed-length step  controlled environment  clustering method  robot perception  machine learning  Legged locomotion  Robot sensing systems  Computer architecture  Convolution  Force  Foot 
Abstract: Mobile robots are becoming very popular in real-world outdoors applications, where there are many challenges in robot control and perception. One of the most critical problems is to characterise the terrain traversed by the robot. This knowledge is indispensable for optimal terrain negotiation. Currently, most approaches are performing terrain classification from vision, but there is not enough research on terrain identification from a direct interaction of the robot with the environment. In our work, we proposed new methods for classification of force/torque data from an interaction of the legged robot foot with the ground, gathered during the walking process. We provided machine learning methods for terrain classification from raw force/torque signals for which we achieved 93% accuracy on a challenging dataset with 160 minutes of recorded fixed-length steps. We also worked on a dataset where the assumption of a fixed-length step is not valid. In this case, the final result is around 80% of accuracy. The most important fact is that the data in both cases was recorded while the robot was walking, no particular movements or controlled environment were needed. Additionally, we also proposed a clustering method which allows us to learn about the class membership based on the recorded data only, without any human supervision.


Title: Depth Generation Network: Estimating Real World Depth from Stereo and Depth Images*
Key Words: image colour analysis  learning (artificial intelligence)  stereo image processing  Depth Generation Network  real world Depth  dense depth estimation  deep-learning technique  stereo RGB images  stereo pairs  depth ground-truth  stereo setting parameters  image pairs  supervision learning  synthetic depth maps  relative dense depth  stereo geometric settings  optic settings  epipolar geometric cues  DGN  falling things dataset  variational method  Training  Estimation  Three-dimensional displays  Data models  Fats  Cameras  Robots 
Abstract: In this work, we propose the Depth Generation Network (DGN) to address the problem of dense depth estimation by exploiting the variational method and the deep-learning technique. In particular, we focus on improving the feasibility of depth estimation under complex scenarios given stereo RGB images, where the stereo pairs and/or depth ground-truth captured by real sensors may be deteriorated; the stereo setting parameters may be unavailable or unreliable, hence hamper efforts to establish the correspondence between image pairs via supervision learning or epipolar geometric cues. Instead of relying on real data, we supervise the training of our model using synthetic depth maps generated by the simulator, which deliver complex scenes and reliable data with ease. Two non-trivial challenges, i.e., (i) attaining reasonable amount yet realistic samples for training, and (ii) developing a model that adapts to both synthetic and real scenes arise, whereas in this work we mainly deal with the later one yet leveraging state-of-the-art Falling Things (FAT) dataset to overcome the first. Experiments on FAT and KITTI datasets demonstrate that our model estimates relative dense depth in fine details, potentially generalizable to real scenes without knowing the stereo geometric and optic settings.


Title: Multi-Task Template Matching for Object Detection, Segmentation and Pose Estimation Using Depth Images
Key Words: image colour analysis  image matching  image sampling  image segmentation  object detection  pose estimation  MultiTask Template Matching  object detection  pose estimation  color images  target object  segmentation masks  object region  texture-less objects  Training  Pose estimation  Feature extraction  Task analysis  Image segmentation  Robots  Solid modeling 
Abstract: Template matching has been shown to accurately estimate the pose of a new object given a limited number of samples. However, pose estimation of occluded objects is still challenging. Furthermore, many robot application domains encounter texture-less objects for which depth images are more suitable than color images. In this paper, we propose a novel framework, Multi-Task Template Matching (MTTM), that finds the nearest template of a target object from a depth image while predicting segmentation masks and a pose transformation between the template and a detected object in the scene using the same feature map of the object region. The proposed feature comparison network computes segmentation masks and pose predictions by comparing feature maps of templates and cropped features of a scene. The segmentation result from this network improves the robustness of the pose estimation by excluding points that do not belong to the object. Experimental results show that MTTM outperforms baseline methods for segmentation and pose estimation of occluded objects despite using only depth images.


Title: Dynamic Period-two Gait Generation in a Hexapod Robot based on the Fixed-point Motion of a Reduced-order Model
Key Words: gait analysis  legged locomotion  pendulums  reduced order systems  robot dynamics  springs (mechanical)  dynamic period-two gait generation  hexapod robot  fixed-point motion  reduced-order model  period-two dynamic running motion  spring-loaded inverted pendulum model  stance phases  flight phases  period-two fixed points  motion cycle  period-two motion trajectories  landing angles  R-SLIP model  Legged locomotion  Trajectory  Bifurcation  Numerical models  Dynamics  Robot kinematics 
Abstract: This research explored the generation of period-two dynamic running motion in a robot, based on the passive dynamic period-two motion of the reduced order, rolling spring-loaded inverted pendulum (R-SLIP) model. Each cycle of period-two motion consists of two stance phases separated by two flight phases. The distribution of the period-two fixed points of the model was analyzed using a return map. Models with the same or different landing angles per motion cycle were studied, and two sets of period-two motion trajectories were implemented in a robot for experimental evaluation. Without sensory feedback or control, this evaluation relied on the open loop trajectory of the model. Based on the experiments, the robot was capable of performing dynamic period-two motion.


Title: Single-shot Foothold Selection and Constraint Evaluation for Quadruped Locomotion
Key Words: control engineering computing  convolutional neural nets  geometry  legged locomotion  motion control  optimal control  robot dynamics  robot kinematics  single-shot foothold selection  constraint evaluation  quadruped locomotion  optimal footholds  legged systems  swing leg  local elevation map  kinematic constraints  convolutional neural network  geometrical characteristics  Legged locomotion  Foot  Kinematics  Robot kinematics  Collision avoidance  Neural networks 
Abstract: In this paper, we propose a method for selecting the optimal footholds for legged systems. The goal of the proposed method is to find the best foothold for the swing leg on a local elevation map. First, we evaluate the geometrical characteristics of each cell on the elevation map, checks kinematic constraints and collisions. Then, we apply the Convolutional Neural Network to learn the relationship between the local elevation map and the quality of potential footholds. During execution time, the controller obtains the qualitative measurement of each potential foothold from the neural model. This method evaluates hundreds of potential footholds and checks multiple constraints in a single step which takes 10 ms on a standard computer without GPU. The experiments were carried out on a quadruped robot walking over rough terrain in both simulation and real robotic platforms.


Title: Coverage Path Planning in Belief Space
Key Words: C++ language  collision avoidance  control engineering computing  lawnmowers  mobile robots  navigation  operating systems (computers)  robot programming  coverage path planning  belief space  robotic lawn mowers  safety-critical tasks  robot safety  cheap range sensors  low range sensors  uncertainty-aware coverage path  lawn mower  safe navigation  collision avoidance  C++ language  ROS  Robot sensing systems  Planning  Path planning  Uncertainty  Collision avoidance 
Abstract: For safety reasons, robotic lawn mowers and similar devices are required to stay within a predefined working area. Keeping the robot within its workspace is typically achieved by special safeguards such as a wire installed in the ground. In the case of robotic lawn mowers, this causes a certain customer reluctance. It is more desirable to fulfill those safety-critical tasks by safe navigation and path planning. In this paper, we tackle the problem of planning a coverage path composed of parallel lanes that maximizes robot safety under the constraints of cheap, low range sensors and thus substantial uncertainty in the robot's belief and ability to execute actions. Our approach uses a map of the environment to estimate localizability at all locations, and it uses these estimates to search for an uncertainty-aware coverage path while avoiding collisions. We implemented our approach using C++ and ROS and thoroughly tested it on real garden data. The experiment shows that our approach leads to safer meander patterns for the lawn mower and takes expected localizability information into account.


Title: Experimental Assessment of Plume Mapping using Point Measurements from Unmanned Vehicles
Key Words: air pollution  air quality  autonomous aerial vehicles  environmental monitoring (geophysics)  Gaussian processes  interpolation  mobile robots  Monte Carlo methods  regression analysis  point measurements  autonomous robots  mapping algorithms  piecewise linear interpolation  steady state ground truth  unmanned aerial vehicle  Gaussian process regression  polynomial interpolation  plume mapping  neural networks  Robot sensing systems  Interpolation  Gaussian processes  Dispersion  Noise measurement  Neural networks 
Abstract: This paper presents experiments to assess the plume mapping performance of autonomous robots. The paper compares several mapping algorithms including Gaussian Process regression, Neural networks and polynomial and piecewise linear interpolation. The methods are compared in Monte Carlo simulations using a well known plume model and in indoor experiments using a ground robot. Unlike previous work on mapping using unmanned vehicles, the indoor experiments were performed in a controlled and repeatable manner where a steady state ground truth could be obtained in order to properly assess the various regression methods using data from a real dispersive source and sensor. The effect of sampling time during data collection was assessed with regards to the mapping accuracy, and the data collected during the experiments have been made available. Overall, the Gaussian Process method was found to perform the best among the regression algorithms, showing more robustness to the noisy measurements obtained from short sampling periods, enabling an accurate map to be produced in significantly less time. Finally, plume mapping results are presented in uncontrolled outdoor conditions, using an unmanned aerial vehicle, to demonstrate the system in a realistic uncontrolled environment.


