total paper: 315
Title: Trajectory-based Probabilistic Policy Gradient for Learning Locomotion Behaviors
Key Words: control engineering computing  gradient methods  learning (artificial intelligence)  legged locomotion  probability  robot programming  moderate sample complexity  trajectory-based probabilistic policy gradient  trajectory-based reinforcement learning method  deep latent policy gradient  DLPG  policy function  probability distribution  deep latent variable model  curriculum learning  locomotion skills  Snapbot  four-legged walking robot  Trajectory  Legged locomotion  Task analysis  Gradient methods  Stochastic processes  Training 
Abstract: In this paper, we propose a trajectory-based reinforcement learning method named deep latent policy gradient (DLPG) for learning locomotion skills. We define the policy function as a probability distribution over trajectories and train the policy using a deep latent variable model to achieve sample efficient skill learning. We first evaluate the sample efficiency of DLPG compared to the state-of-the-art reinforcement learning methods in simulated environments. Then, we apply the proposed method to a four-legged walking robot named Snapbot to learn three basic locomotion skills of turn left, go straight, and turn right. We demonstrate that, by properly designing two reward functions for curriculum learning, Snapbot successfully learns the desired locomotion skills with moderate sample complexity.


Title: Learning Motion Planning Policies in Uncertain Environments through Repeated Task Executions
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  motion control  learned reactive planning problem  motion planning policy learning  execution cost  motion policy  navigation task  online replanning  reactive algorithms  goal location  repeated task executions  uncertain environments  Task analysis  Robot sensing systems  Planning  Navigation  Reinforcement learning  Heuristic algorithms 
Abstract: The ability to navigate uncertain environments from a start to a goal location is a necessity in many applications. While there are many reactive algorithms for online replanning, there has not been much investigation in leveraging past executions of the same navigation task to improve future executions. In this work, we first formalize this problem by introducing the Learned Reactive Planning Problem (LRPP). Second, we propose a method to capture these past executions and from that determine a motion policy to handle obstacles that the robot has seen before. Third, we show from our experiments that using this policy can significantly reduce the execution cost over just using reactive algorithms.


Title: BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning
Key Words: learning (artificial intelligence)  Markov processes  optimisation  path planning  robots  BaRC  initial state distribution backwards  model-free RL algorithm  goal-directed continuous control MDPs  curriculum strategy  representative dynamic robotic learning problems  goal-directed tasks  learning signal  model-free policy optimization algorithm  backward reachability curriculum  curriculum generation techniques  robotic reinforcement learning  model-free reinforcement learning  model-free algorithms  reward function  exploration strategies  Robots  Task analysis  Training  Computational modeling  Heuristic algorithms  Complexity theory  Approximation algorithms 
Abstract: Model-free Reinforcement Learning (RL) offers an attractive approach to learn control policies for high dimensional systems, but its relatively poor sample complexity often necessitates training in simulated environments. Even in simulation, goal-directed tasks whose natural reward function is sparse remain intractable for state-of-the-art model-free algorithms for continuous control. The bottleneck in these tasks is the prohibitive amount of exploration required to obtain a learning signal from the initial state of the system. In this work, we leverage physical priors in the form of an approximate system dynamics model to design a curriculum for a model-free policy optimization algorithm. Our Backward Reachability Curriculum (BaRC) begins policy training from states that require a small number of actions to accomplish the task, and expands the initial state distribution backwards in a dynamically-consistent manner once the policy optimization algorithm demonstrates sufficient performance. BaRC is general, in that it can accelerate training of any model-free RL algorithm on a broad class of goal-directed continuous control MDPs. Its curriculum strategy is physically intuitive, easy-to-tune, and allows incorporating physical priors to accelerate training without hindering the performance, flexibility, and applicability of the model-free RL algorithm. We evaluate our approach on two representative dynamic robotic learning problems and find substantial performance improvement relative to previous curriculum generation techniques and naive exploration strategies.


Title: Active Sampling based Safe Identification of Dynamical Systems using Extreme Learning Machines and Barrier Certificates
Key Words: cyber-physical systems  feedforward neural nets  function approximation  Gaussian processes  learning (artificial intelligence)  manipulators  nonlinear dynamical systems  optimisation  robot programming  dynamical system model  robot learning applications  cyber-physical systems  model learning method  ELM learning  invariance property  invariant trajectories  barrier certificates  parameter learning problem  active sampling based safe identification  extreme learning machines  infinite constraint problem  robot arm  barrier constraints  Robots  Trajectory  Safety  Stability analysis  Heuristic algorithms  Neurons  Convergence 
Abstract: Learning the dynamical system (DS) model from data that preserves dynamical system properties is an important problem in many robot learning applications. Typically, the joint data coming from cyber-physical systems, such as robots have some underlying DS properties associated with it, e.g., convergence, invariance to a set, etc. In this paper, a model learning method is developed such that the trajectories of the DS are invariant in a given compact set. Such invariant DS models can be used to generate trajectories of the robot that will always remain in a prescribed set. In order to achieve invariance to a set, Barrier certificates are employed. The DS is approximated using Extreme Learning Machine (ELM), and a parameter learning problem subject to Barrier certificates enforced at all the points in the prescribed set is solved. To solve an infinite constraint problem for enforcing Barrier Certificates at every point in a given compact set, a modified constraint is developed that is sufficient to hold the Barrier certificates in the entire set. An active sampling strategy is formulated to minimize the number of constraints in learning. Simulation results of ELM learning with and without Barrier certificates are presented which show the invariance property being preserved in the ELM learning when learning procedure involves Barrier constraints. The method is validated using experiments conducted on a robot arm recreating invariant trajectories inside a prescribed set.


Title: Navigating Dynamically Unknown Environments Leveraging Past Experience
Key Words: adaptive control  collision avoidance  mobile robots  navigation  autonomous robot navigation  unknown dynamic obstacles  real-time adaptive motion planner  robot motion online  sensed environmental data  limited sensing range  RAMP framework  probabilistic model  unknown dynamic environment  sensing information  RAMP robot  dynamic environment changes  unknown ways  learned probabilistic data  Hilbert maps framework  dynamically unknown environment navigation  Robot sensing systems  Trajectory  Sociology  Statistics  Planning 
Abstract: To enable autonomous robot navigation among unknown dynamic obstacles, a real-time adaptive motion planner (RAMP) plans the robot motion online based on sensing the environment as the robot moves with sensors mounted on the robot. However, the sensed environmental data from the robot's local view is usually incomplete due to occlusions from obstacles and limited sensing range.This paper incorporates learning about the environment into the RAMP framework by leveraging the Hilbert Maps framework to generate a probabilistic model of occupancy of the unknown dynamic environment based on past observations. Utilizing this probabilistic model enables RAMP to reason about trajectory fitness when sensing information is partial and incomplete. This allows the RAMP robot to take advantage of what it has experienced from being in the dynamic environment before to inform its subsequent executions even though the dynamic environment changes in unknown ways. The effectiveness of incorporating such learned probabilistic data into RAMP is shown in both simulation and real experiments.


Title: VPE: Variational Policy Embedding for Transfer Reinforcement Learning
Key Words: learning (artificial intelligence)  Markov processes  pendulums  variational techniques  variational policy embedding  transfer reinforcement Learning  complex problems  deployment conditions  data collection  simulation training  Q-function  master policy  latent variables  latent space  low-dimensional space  simulation-to-real transfer  reinforcement learning methods  Markov decision processes  Optimization  Training  Task analysis  Robots  Adaptation models  Reinforcement learning  Supervised learning 
Abstract: Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.


Title: Automatic Labeled LiDAR Data Generation based on Precise Human Model
Key Words: data analysis  image motion analysis  image recognition  learning (artificial intelligence)  neural nets  optical radar  human recognition  point clouds  ground truth label  automatic labeled data generation pipeline  data generation environments  realistic artificial data  automatic labeled LiDAR data generation  precise human model  deep neural networks  Laser radar  Data models  Three-dimensional displays  Pipelines  Labeling  Legged locomotion  Solid modeling 
Abstract: Following improvements in deep neural networks, state-of-the-art networks have been proposed for human recognition using point clouds captured by LiDAR. However, the performance of these networks strongly depends on the training data. An issue with collecting training data is labeling. Labeling by humans is necessary to obtain the ground truth label; however, labeling requires huge costs. Therefore, we propose an automatic labeled data generation pipeline, for which we can change any parameters or data generation environments. Our approach uses a human model named Dhaiba and a background of Miraikan and consequently generated realistic artificial data. We present 500k + data generated by the proposed pipeline. This paper also describes the specification of the pipeline and data details with evaluations of various approaches.


Title: Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting
Key Words: computer aided instruction  human-robot interaction  image segmentation  learning (artificial intelligence)  manipulators  teaching  video signal processing  teacher-student learning paradigm  interactive video object segmentation  grasping affordances  children learning process  IVOS dataset  teaching signal  human teacher  unstructured environments  robotics  incremental learning  learning affordances  robot manipulation  human robot interaction setting  teacher-student adaptation  adaptation method  manipulation tasks  HRI setup  appearance student network  appearance teacher network  Task analysis  Adaptation models  Motion segmentation  Education  Object segmentation  Benchmark testing 
Abstract: Video object segmentation is an essential task in robot manipulation to facilitate grasping and learning affordances. Incremental learning is important for robotics in unstructured environments. Inspired by the children learning process, human robot interaction (HRI) can be utilized to teach robots about the world guided by humans similar to how children learn from a parent or a teacher. A human teacher can show potential objects of interest to the robot, which is able to self adapt to the teaching signal without providing manual segmentation labels. We propose a novel teacher-student learning paradigm to teach robots about their surrounding environment. A two-stream motion and appearance “teacher” network provides pseudo-labels to adapt an appearance “student” network. The student network is able to segment the newly learned objects in other scenes, whether they are static or in motion. We also introduce a carefully designed dataset that serves the proposed HRI setup, denoted as (I)nteractive (V)ideo (O)bject (S)egmentation. Our IVOS dataset contains teaching videos of different objects, and manipulation tasks. Our proposed adaptation method outperforms the state-of-theart on DAVIS and FBMS with 6.8% and 1.2% in F-measure respectively. It improves over the baseline on IVOS dataset with 46.1% and 25.9% in mIoU.


Title: Designing Worm-inspired Neural Networks for Interpretable Robotic Control
Key Words: brain  manipulator dynamics  mobile robots  neurocontrollers  neurophysiology  nonlinear control systems  recurrent neural nets  search problems  supervised learning  time-varying systems  interpretable robotic control  nonlinear time-varying synaptic links  liquid time-constants dynamics  neuron-pair communication motifs  compact neuronal network structures  sequential robotic tasks  sensory neurons  recurrently-wired interneurons  motor neurons  interpretable dynamics  mobile arm robots  artificial neural network-based control agents  wiring structure  worm-inspired neural networks  liquid time-constant recurrent neural networks  nematode  C. elegans  supervised-learning scheme  search-based algorithm  Neurons  Biological neural networks  Robot sensing systems  Synapses  Correlation  Couplings 
Abstract: In this paper, we design novel liquid time-constant recurrent neural networks for robotic control, inspired by the brain of the nematode, C. elegans. In the worm's nervous system, neurons communicate through nonlinear time-varying synaptic links established amongst them by their particular wiring structure. This property enables neurons to express liquid time-constants dynamics and therefore allows the network to originate complex behaviors with a small number of neurons. We identify neuron-pair communication motifs as design operators and use them to configure compact neuronal network structures to govern sequential robotic tasks. The networks are systematically designed to map the environmental observations to motor actions, by their hierarchical topology from sensory neurons, through recurrently-wired interneurons, to motor neurons. The networks are then parametrized in a supervised-learning scheme by a search-based algorithm. We demonstrate that obtained networks realize interpretable dynamics. We evaluate their performance in controlling mobile and arm robots, and compare their attributes to other artificial neural network-based control agents. Finally, we experimentally show their superior resilience to environmental noise, compared to the existing machine learning-based methods.


Title: Learning Extreme Hummingbird Maneuvers on Flapping Wing Robots
Key Words: aerodynamics  aerospace components  aerospace robotics  aircraft control  control engineering computing  learning (artificial intelligence)  mobile robots  nonlinear control systems  position control  robot dynamics  robot kinematics  stability  extreme aerobatic maneuvers  visual stimulus  180-degree yaw turn  wingbeat frequency  flight control strategy  hybrid control policy  model-based nonlinear control  model-free reinforcement learning policy  hummingbird-like fast evasive maneuvers  extreme hummingbird maneuvers  flapping wing robots  backward translation  posture stabilization  hummingbird robot  frequency 40.0 Hz  time 0.2 s  Aerodynamics  Vehicle dynamics  Uncertainty  Robots  Adaptation models  Torque  Actuators 
Abstract: Biological studies show that hummingbirds can perform extreme aerobatic maneuvers during fast escape. Given a sudden looming visual stimulus at hover, a hummingbird initiates a fast backward translation coupled with a 180-degree yaw turn, which is followed by instant posture stabilization in just under 10 wingbeats. Consider the wingbeat frequency of 40Hz, this aggressive maneuver is carried out in just 0.2 seconds. Inspired by the hummingbirds' near-maximal performance during such extreme maneuvers, we developed a flight control strategy and experimentally demonstrated that such maneuverability can be achieved by an at-scale 12-gram hummingbird robot equipped with just two actuators driving a pair of flapping wings up to 40Hz. The proposed hybrid control policy combines model-based nonlinear control with model-free reinforcement learning. We used the model-based nonlinear control for nominal flight conditions where dynamic models are relatively accurate. During extreme maneuvers when the modeling error becomes unmanageable, we use a model-free reinforcement learning policy trained and optimized in simulation to 'destabilize' the system for peak performance during maneuvering. The hybrid policy manifests a maneuver that is close to that observed in hummingbirds. Direct simulation-to-real transfer is achieved, demonstrating the hummingbird-like fast evasive maneuvers on the at-scale hummingbird robot.


Title: GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization and Mapping
Key Words: cameras  collision avoidance  convolutional neural nets  learning (artificial intelligence)  mobile robots  pose estimation  robot vision  SLAM (robots)  depth estimation system  GEN-SLAM  generative modeling  Deep Learning based system  obstacle avoidance  mobile robot  conventional geometric SLAM  single camera  topological map  camera image  topological location estimation  monocular localization  monocular simultaneous localization and mapping  Cameras  Image reconstruction  Simultaneous localization and mapping  Decoding  Training 
Abstract: We present a Deep Learning based system for the twin tasks of localization and obstacle avoidance essential to any mobile robot. Our system learns from conventional geometric SLAM, and outputs, using a single camera, the topological pose of the camera in an environment, and the depth map of obstacles around it. We use a CNN to localize in a topological map, and a conditional VAE to output depth for a camera image, conditional on this topological location estimation. We demonstrate the effectiveness of our monocular localization and depth estimation system on simulated and real datasets.


Title: Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped
Key Words: control system synthesis  learning (artificial intelligence)  legged locomotion  neurocontrollers  deep reinforcement learning  expert knowledge  domain randomization  stable controllers  high-fidelity simulators  neural network policy  ATRIAS robot  bipedal robots  Neural networks  Hardware  Legged locomotion  Reinforcement learning  Torso  Foot 
Abstract: Learning controllers for bipedal robots is a challenging problem, often requiring expert knowledge and extensive tuning of parameters that vary in different situations. Recently, deep reinforcement learning has shown promise at automatically learning controllers for complex systems in simulation. This has been followed by a push towards learning controllers that can be transferred between simulation and hardware, primarily with the use of domain randomization. However, domain randomization can make the problem of finding stable controllers even more challenging, especially for under actuated bipedal robots. In this work, we explore whether policies learned in simulation can be transferred to hardware with the use of high-fidelity simulators and structured controllers. We learn a neural network policy which is a part of a more structured controller. While the neural network is learned in simulation, the rest of the controller stays fixed, and can be tuned by the expert as needed. We show that using this approach can greatly speed up the rate of learning in simulation, as well as enable transfer of policies between simulation and hardware. We present our results on an ATRIAS robot and explore the effect of action spaces and cost functions on the rate of transfer between simulation and hardware. Our results show that structured policies can indeed be learned in simulation and implemented on hardware successfully. This has several advantages, as the structure preserves the intuitive nature of the policy, and the neural network improves the performance of the hand-designed policy. In this way, we propose a way of using neural networks to improve expert designed controllers, while maintaining ease of understanding.


Title: Unsupervised Gait Phase Estimation for Humanoid Robot Walking*
Key Words: data acquisition  data reduction  humanoid robots  legged locomotion  pattern clustering  phase estimation  robot dynamics  robust control  state estimation  unsupervised learning  unsupervised gait phase estimation  humanoid robot walking  contact detection  feet contact status  proprioceptive sensing  inertial measurement unit  data acquisition  dimensionality reduction  state estimation  unsupervised learning  feature representation  gait phase dynamics  joint encoder  force data  torque data  clustering  robustness  legged robots  Legged locomotion  Humanoid robots  Robot sensing systems  Kinematics  Unsupervised learning 
Abstract: Contact detection is an important topic in contemporary humanoid robotic research. Up to date control and state estimation schemes readily assume that feet contact status is known in advance. In this work, we elaborate on a broader question: in which gait phase is the robot currently in? We introduce an unsupervised learning framework for gait phase estimation based solely on proprioceptive sensing, namely joint encoder, inertial measurement unit and force/torque data. Initially, a meaningful physical explanation on data acquisition is presented. Subsequently, dimensionality reduction is performed to obtain a compact low-dimensional feature representation followed by clustering into three groups, one for each gait phase. The proposed framework is qualitatively and quantitatively assessed in simulation with ground-truth data of uneven/rough terrain walking gaits and insights about the latent gait phase dynamics are drawn. Additionally, its efficacy and robustness is demonstrated when incorporated in leg odometry computation. Since our implementation is based on sensing that is commonly available on humanoids today, we release an open-source ROS/Python package to reinforce further research endeavors.


Title: Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT
Key Words: adaptive control  control engineering computing  control system synthesis  feedback  learning (artificial intelligence)  legged locomotion  mobile robots  PD control  robust control  stability  feedback controllers  bipedal robots  high-dimensional bipedal models  bipedal walking  bipedal control  walking limit cycles  HZD framework  policy learning  adaptive PD controller  stable control policy  robust control policy  RL framework  RABBIT robot model  reinforcement learning  local stability  hybrid zero dynamics  OpenAI gym  MuJoCo physics engine  Legged locomotion  Hip  Rabbits  Robot kinematics  Training  Computational modeling 
Abstract: The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot's reduced order states to a set of parameters that define the desired trajectories for the robot's joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.


Title: Learning Wheel Odometry and IMU Errors for Localization
Key Words: cameras  distance measurement  Gaussian processes  image filtering  Kalman filters  learning (artificial intelligence)  mobile robots  nonlinear filters  path planning  robot vision  wheel odometry  IMU errors  odometry techniques  autonomous robot navigation  robust odometry system  camera  deep learning  variational inference  observation models  state-space systems  Gaussian processes  ground truth  wheel encoders  fiber optic gyro  EKF  wheel speed sensors  inertial measurement unit  extended Kalman filter  Wheels  Kernel  Mobile robots  Sensors  Gaussian processes  Training  Gaussian process  odometry estimation  variational inference  Kalman filter 
Abstract: Odometry techniques are key to autonomous robot navigation, since they enable self-localization in the environment. However, designing a robust odometry system is particularly challenging when camera and LiDAR are uninformative or unavailable. In this paper, we leverage recent advances in deep learning and variational inference to correct dynamical and observation models for state-space systems. The methodology trains Gaussian processes on the residual between the original model and the ground truth, and is applied on publicly available datasets for robot navigation based on two wheel encoders, a fiber optic gyro, and an Inertial Measurement Unit (IMU). We also propose to build an Extended Kalman Filter (EKF) on the learned model using wheel speed sensors and the fiber optic gyro for state propagation, and the IMU to update the estimated state. Experimental results clearly demonstrate that the (learned) corrected models and EKF are more accurate than their original counterparts.


Title: A Multi-Domain Feature Learning Method for Visual Place Recognition
Key Words: feature extraction  image recognition  learning (artificial intelligence)  mobile robots  object recognition  robot vision  computer vision  robotics applications  VPR methods  place recognition performance  environmental factors  end-to-end conditional visual place recognition method  multidomain feature learning method  feature detaching module  environmental condition-related features  feature learning pipeline  multiseason NORDLAND dataset  multiweather GTAV dataset  feature robustness  variant environmental conditions  Feature extraction  Entropy  Visualization  Task analysis  Decoding  Robots  Upper bound 
Abstract: Visual Place Recognition (VPR) is an important component in both computer vision and robotics applications, thanks to its ability to determine whether a place has been visited and where specifically. A major challenge in VPR is to handle changes of environmental conditions including weather, season and illumination. Most VPR methods try to improve the place recognition performance by ignoring the environmental factors, leading to decreased accuracy decreases when environmental conditions change significantly, such as day versus night. To this end, we propose an end-to-end conditional visual place recognition method. Specifically, we introduce the multi-domain feature learning method (MDFL) to capture multiple attribute-descriptions for a given place, and then use a feature detaching module to separate the environmental condition-related features from those that are not. The only label required within this feature learning pipeline is the environmental condition. Evaluation of the proposed method is conducted on the multi-season NORDLAND dataset, and the multi-weather GTAV dataset. Experimental results show that our method improves the feature robustness against variant environmental conditions.


Title: Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network
Key Words: dexterous manipulators  human-robot interaction  image classification  learning (artificial intelligence)  neurocontrollers  pose estimation  robot vision  telerobotics  TeachNet  Shadow dexterous hand  end-to-end deep neural network  intuitive vision-based teleoperation  markerless vision-based teleoperation  dexterous robotic hands  robot joint angles  human hand  visually similar robot hand  consistency loss function  human hands  human-robot training set  labeled depth images  simulated depth images  Shadow C6 robotic hand  pairwise depth images  vision-based teleoperation method  Training  Pose estimation  Three-dimensional displays  Neural networks  Robot sensing systems 
Abstract: In this paper, we present TeachNet, a novel neural network architecture for intuitive and markerless vision-based teleoperation of dexterous robotic hands. Robot joint angles are directly generated from depth images of the human hand that produce visually similar robot hand poses in an end-to-end fashion. The special structure of TeachNet, combined with a consistency loss function, handles the differences in appearance and anatomy between human and robotic hands. A synchronized human-robot training set is generated from an existing dataset of labeled depth images of the human hand and simulated depth images of a robotic hand. The final training set includes 400K pairwise depth images and joint angles of a Shadow C6 robotic hand. The network evaluation results verify the superiority of TeachNet, especially regarding the high-precision condition. Imitation experiments and grasp tasks teleoperated by novice users demonstrate that TeachNet is more reliable and faster than the state-of-the-art vision-based teleoperation method.


Title: Exploiting Human and Robot Muscle Synergies for Human-in-the-loop Optimization of EMG-based Assistive Strategies
Key Words: artificial limbs  biomechanics  electromyography  medical robotics  motion control  optimisation  robot muscle synergies  EMG-based assistive strategies  exoskeleton robot control  Electromyography-based assistive strategies  multiple EMG channels  multiDoF robots  optimization process  human muscles  pneumatic artificial muscle contractions  Bayesian optimization method  human movements  PAMs-driven upper-limb exoskeleton robot  human-in-theloop optimization  human-in-the-loop optimization approach  Muscles  Robots  Optimization  Exoskeletons  Electromyography  Bayes methods  Pneumatic systems 
Abstract: In this study, we propose a novel human-in-the-loop optimization approach for exoskeleton robot control. We develop a method to optimize widely-used Electromyography (EMG)-based assistive strategies. If we use multiple EMG channels to control multi-DoF robots, optimization process becomes complex and requires a large amount of data. To make the optimization tractable, we exploit the synergies both of the human muscles and artificial muscles of the exoskeleton robots to reduce the number of parameters of the assistive strategies. We show that we can extract the synergies not only from the user's muscle activities but from pneumatic artificial muscle (PAMs) contractions of the exoskeleton robot. Then, we adopt a Bayesian optimization method to acquire the parameters for assisting human movements by iteratively identifying the user's preferences of the assistive strategies. We conducted experiments to evaluate our proposed method with a PAMs-driven upper-limb exoskeleton robot. Our method successfully learned assistive strategies from the human-in-theloop optimization with a practicable number of interactions.


Title: Mobile Robotic Painting of Texture
Key Words: image colour analysis  image reconstruction  image texture  learning (artificial intelligence)  mobile robots  neural nets  painting  spraying  mobile robotic painting  mobile robots  robotic paint delivery systems  spray painting  painting tasks  image texture  robotic paint commands  deep learning approach  appearance reconstruction  Painting  Robots  Paints  Ink  Atmospheric modeling  Spraying 
Abstract: Robotic painting is well-established in controlled factory environments, but there is now potential for mobile robots to do functional painting tasks around the everyday world. An obvious first target for such robots is painting a uniform single color. A step further is the painting of textured images. Texture involves a varying appearance, and requires that paint is delivered accurately onto the physical surface to produce the desired effect. Robotic painting of texture is relevant for architecture and in themed environments. A key challenge for robotic painting of texture is to take a desired image as input, and to generate the paint commands to as closely as possible create the desired appearance, according to the robotic capabilities. This paper describes a deep learning approach to take an input ink map of a desired texture, and infer robotic paint commands to produce that texture. We analyze the trade-offs between quality of reconstructed appearance and ease of execution. Our method is general for different kinds of robotic paint delivery systems, but the emphasis here is on spray painting. More generally, the framework can be viewed as an approach for solving a specific class of inverse imaging problems.


Title: Real-Time Minimum Snap Trajectory Generation for Quadcopters: Algorithm Speed-up Through Machine Learning
Key Words: computational complexity  control engineering computing  gradient methods  helicopters  iterative methods  learning (artificial intelligence)  neurocontrollers  trajectory control  human-machine interface  gradient descent method  supervised neural network  computational time  machine learning  quadcopter  iterative methods  real-time minimum snap trajectory generation  smart-tablet interface  Trajectory  Resource management  Real-time systems  Neural networks  Acceleration  Quadratic programming  Nonlinear optics 
Abstract: This paper addresses the problem of generating quadcopter minimum snap trajectories for real time applications. Previous efforts addressed this problem by either employing a gradient descent method, or by greatly sacrificing optimality for faster solutions that are amenable for onboard implementation. In this work, outputs of the gradient descent method are used offline to train a supervised neural network. We show that the use of neural networks results typically in two orders of magnitude reduction in computational time. Our proposed approach can be used for warm-starting onboard implementable iterative methods with an “educated ” initial guess. This work is motivated by the application for human-machine interface in which a human provides desired trajectory through a smart-tablet interface, which has to be translated into a dynamically feasible trajectory for a quadcopter. The proposed solution is tested in thousands of different examples, demonstrating its effectiveness as a booster for minimum snap trajectory generation for quadcopters.


Title: Beauty and the Beast: Optimal Methods Meet Learning for Drone Racing
Key Words: autonomous aerial vehicles  collision avoidance  Kalman filters  learning (artificial intelligence)  mobile robots  navigation  optimisation  predictive control  state estimation  robust flight  previously-unseen race tracks  optimal methods  fast maneuvers  agile maneuvers  dynamic environments  imperfect sensing  state estimation drift  human pilots  unseen track  practice runs  state-of-the-art autonomous navigation algorithms  precise metric map  training data  unseen environment  precise map  expensive data collection  global track layout  coarse gate locations  single demonstration flight  convolutional network  closest gates  extended Kalman filter  maximum-a-posteriori estimates  high-variance estimates  poor observability  visible gates  estimated gate poses  model predictive control  agile flight  autonomous microaerial vehicles  autonomous drone racing  IROS 2018 autonomous drone race competition  Logic gates  Drones  Navigation  Training data  Current measurement  Layout  Uncertainty 
Abstract: Autonomous micro aerial vehicles still struggle with fast and agile maneuvers, dynamic environments, imperfect sensing, and state estimation drift. Autonomous drone racing brings these challenges to the fore. Human pilots can fly a previously unseen track after a handful of practice runs. In contrast, state-of-the-art autonomous navigation algorithms require either a precise metric map of the environment or a large amount of training data collected in the track of interest. To bridge this gap, we propose an approach that can fly a new track in a previously unseen environment without a precise map or expensive data collection. Our approach represents the global track layout with coarse gate locations, which can be easily estimated from a single demonstration flight. At test time, a convolutional network predicts the poses of the closest gates along with their uncertainty. These predictions are incorporated by an extended Kalman filter to maintain optimal maximum-a-posteriori estimates of gate locations. This allows the framework to cope with misleading high-variance estimates that could stem from poor observability or lack of visible gates. Given the estimated gate poses, we use model predictive control to quickly and accurately navigate through the track. We conduct extensive experiments in the physical world, demonstrating agile and robust flight through complex and diverse previously-unseen race tracks. The presented approach was used to win the IROS 2018 Autonomous Drone Race Competition, outracing the second-placing team by a factor of two.


Title: A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning
Key Words: control engineering computing  industrial robots  learning (artificial intelligence)  neural nets  production engineering computing  variable socket position  visual control problem  model-based robotics community  task geometry  off-the-shelf Deep-RL algorithm  narrow-clearance peg-insertion task  deformable clip-insertion task  deep reinforcement learning  haptic control problem  Task analysis  Robots  Sockets  Visualization  Training  Plugs  Feature extraction 
Abstract: Insertion is a challenging haptic and visual control problem with significant practical value for manufacturing. Existing approaches in the model-based robotics community can be highly effective when task geometry is known, but are complex and cumbersome to implement, and must be tailored to each individual problem by a qualified engineer. Within the learning community there is a long history of insertion research, but existing approaches are either too sample-inefficient to run on real robots, or assume access to high-level object features, e.g. socket pose. In this paper we show that relatively minor modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a small number of human demonstrations, allows the robot to quickly learn to solve these tasks efficiently and robustly. Our approach requires no modeling or simulation, no parameterized search or alignment behaviors, no vision system aside from raw images, and no reward shaping. We evaluate our approach on a narrow-clearance peg-insertion task and a deformable clip-insertion task, both of which include variability in the socket position. Our results show that these tasks can be solved reliably on the real robot in less than 10 minutes of interaction time, and that the resulting policies are robust to variance in the socket position and orientation.


Title: Uncertainty-Aware Data Aggregation for Deep Imitation Learning
Key Words: data aggregation  learning (artificial intelligence)  Monte Carlo methods  uncertain systems  uncertainty estimation method  UAIL  uncertainty-aware data aggregation  deep imitation learning  statistical uncertainties  autonomous agents  task execution  safety-critical domains  autonomous driving  uncertainty-aware imitation learning algorithm  end-to-end control systems  Monte Carlo Dropout  control output  end-to-end systems  training data  prior data aggregation algorithms  sub-optimal states  simulated driving tasks  Uncertainty  Data aggregation  Task analysis  Switches  Estimation  Data models 
Abstract: Estimating statistical uncertainties allows autonomous agents to communicate their confidence during task execution and is important for applications in safety-critical domains such as autonomous driving. In this work, we present the uncertainty-aware imitation learning (UAIL) algorithm for improving end-to-end control systems via data aggregation. UAIL applies Monte Carlo Dropout to estimate uncertainty in the control output of end-to-end systems, using states where it is uncertain to selectively acquire new training data. In contrast to prior data aggregation algorithms that force human experts to visit sub-optimal states at random, UAIL can anticipate its own mistakes and switch control to the expert in order to prevent visiting a series of sub-optimal states. Our experimental results from simulated driving tasks demonstrate that our proposed uncertainty estimation method can be leveraged to reliably predict infractions. Our analysis shows that UAIL outperforms existing data aggregation algorithms on a series of benchmark tasks.


Title: Uncertainty Aware Learning from Demonstrations in Multiple Contexts using Bayesian Neural Networks
Key Words: Bayes methods  belief networks  intelligent robots  learning (artificial intelligence)  learning systems  neurocontrollers  Bayesian neural networks  robotic controllers  evaluation conditions  learned controller  testing conditions  high-dimensional simulated domains  real robotic domains  uncertainty based solution  uncertainty aware learning  Uncertainty  Task analysis  Neural networks  Training  Robots  Bayes methods  Measurement uncertainty 
Abstract: Diversity of environments is a key challenge that causes learned robotic controllers to fail due to the discrepancies between the training and evaluation conditions. Training from demonstrations in various conditions can mitigate - but not completely prevent - such failures. Learned controllers such as neural networks typically do not have a notion of uncertainty that allows to diagnose an offset between training and testing conditions, and potentially intervene. In this work, we propose to use Bayesian Neural Networks, which have such a notion of uncertainty. We show that uncertainty can be leveraged to consistently detect situations in high-dimensional simulated and real robotic domains in which the performance of the learned controller would be sub-par. Also, we show that such an uncertainty based solution allows making an informed decision about when to invoke a fallback strategy. One fallback strategy is to request more data. We empirically show that providing data only when requested results in increased data-efficiency.


Title: Learning From Demonstration in the Wild
Key Words: cameras  learning (artificial intelligence)  object detection  traffic engineering computing  video signal processing  uncalibrated camera  learning from demonstration  ViBe  traffic intersection  knowledge expert  video to behaviour  natural behaviour  reward function  hand-coding behaviour  wild  raw videos  naturalistic behaviour  LfD  monocular camera  single camera  traffic scene  Cameras  Trajectory  Roads  Three-dimensional displays  Sensors  Training  Tracking 
Abstract: Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose video to behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.


Title: A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies
Key Words: gradient methods  image coding  learning (artificial intelligence)  mobile robots  path planning  data-efficient framework  sim-to-real transfer  navigation policies  effective visuomotor policies  learning-based system  manual tuning  robot operating  training process  leverage simulation  off-policy data  initial image  lower dimensional latent state  planner modules  meta-learning strategy  adversarial domain transfer  simulated environments  similarly distributed latent representation  fine tuning  encoder + planner  planning performances  navigation tasks  unlabelled random images  Robots  Data models  Planning  Task analysis  Trajectory  Training  Navigation 
Abstract: Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage simulation and off-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is first trained through a meta-learning strategy in simulation. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with only a few real world expert demonstrations, we show successful planning performances in different navigation tasks.


Title: Simulating Emergent Properties of Human Driving Behavior Using Multi-Agent Reward Augmented Imitation Learning
Key Words: behavioural sciences computing  convergence  learning (artificial intelligence)  multi-agent systems  traffic engineering computing  multiagent settings  multiagent imitation learning  human drivers  reward augmentation  imitation learning process  multiagent reward augmented imitation learning  traffic behaviors  imitation learning algorithms  human driving behavior modeling  prior knowledge specification  convergence guarantees  driving policies  Rails  Convergence  Biological system modeling  Trajectory  Computational modeling  Autonomous vehicles 
Abstract: Recent developments in multi-agent imitation learning have shown promising results for modeling the behavior of human drivers. However, it is challenging to capture emergent traffic behaviors that are observed in real-world datasets. Such behaviors arise due to the many local interactions between agents that are not commonly accounted for in imitation learning. This paper proposes Reward Augmented Imitation Learning (RAIL), which integrates reward augmentation into the multi-agent imitation learning framework and allows the designer to specify prior knowledge in a principled fashion. We prove that convergence guarantees for the imitation learning process are preserved under the application of reward augmentation. This method is validated in a driving scenario, where an entire traffic scene is controlled by driving policies learned using our proposed algorithm. Further, we demonstrate improved performance in comparison to traditional imitation learning algorithms both in terms of the local actions of a single agent and the behavior of emergent properties in complex, multi-agent settings.


Title: A Supervised Approach to Predicting Noise in Depth Images
Key Words: cameras  convolutional neural nets  image denoising  image fusion  image reconstruction  image sensors  object detection  pose estimation  robot vision  supervised learning  supervised approach  modern robotic systems  detailed sensor noise models  robotic behavior  scene-dependent pixel-wise dropouts  depth camera simulations  data driven approach  convolutional neural network  no-depth-return pixels  NDP  ground truth depth  noisy depth image  resulting noise-free  noise-free image  depth sensor  cluttered scenes  uncorrupted depth images  noise prediction  scenes reconstruction  CNN  unsupervised domain adaptation baselines  object pose estimation  noise-free depth image  label fusion dataset  Cameras  Image reconstruction  Data models  Noise measurement  Robot vision systems 
Abstract: Modern robotic systems are very complex and need to be tested in simulations with detailed sensor noise models to effectively verify robotic behavior. Depth imagery in particular comes with significant noise in the form of scene-dependent pixel-wise dropouts and distortions. Unfortunately, many depth camera simulations contain limited noise models, or can only support generating realistic depth images of simple scenes, which limits their usefulness in effectively testing perception algorithms. We propose a data driven approach to generate more realistic noise for complex simulated environments by using a convolutional neural network (CNN) to predict which pixels of a simulated noise-free depth image will not have returns (no-depth-return pixels, or NDP). We choose to focus on NDP here, as these dropouts are the most common and dramatic form of depth image noise. To train this network, we use reconstructed real-world scenes from the Label Fusion dataset to provide ground truth depth for each noisy depth image used to scan the scene. We use the resulting noise-free and noisy depth image pairs as labeled examples and train the network to predict which pixels of the noise-free image will be NDP. When used to post-process a simulation of a depth sensor, this system produces realistic depth images, even in cluttered scenes. To demonstrate that our approach successfully closes the reality gap for depth imagery, we show that the popular ICP algorithm for object pose estimation fails more realistically on our CNN-corrupted simulated depth images than on uncorrupted depth images and unsupervised domain adaptation baselines.


Title: Quantum Computation in Robotic Science and Applications
Key Words: cloud computing  intelligent robots  learning (artificial intelligence)  quantum computing  robot programming  quantum computing  cloud services  artificial intelligence  machine learning  robotic scientists  quantum mechanics  quantum computation  intelligent robots  powerful robots  Robot sensing systems  Computers  Optimization  Qubit  Acceleration 
Abstract: Using the effects of quantum mechanics for computing challenges has been an often discussed topic for decades. The frequent successes and early products in this area, which we have seen in recent years, indicate that we are currently entering a new era of computing. This paradigm shift will also impact the work of robotic scientists and the applications of robotics. New possibilities as well as new approaches to known problems will enable the creation of even more powerful and intelligent robots that make use of quantum computing cloud services or co-processors. In this position paper, we discuss potential application areas and also point out open research topics in quantum computing for robotics. We go into detail on the impact of quantum computing in artificial intelligence and machine learning, sensing and perception, kinematics as well as system diagnosis. For each topic we point out where quantum computing could be applied based on results from current research.


Title: A Learning Framework for High Precision Industrial Assembly
Key Words: assembling  optimisation  production engineering computing  supervised learning  learning framework  high precision industrial assembly  reinforcement learning  automatic assembly  supervised learning  assembly tasks  trajectory optimization  actor-critic algorithm  Task analysis  Trajectory  Optimization  Dynamics  Supervised learning  Computational modeling  Space exploration 
Abstract: Automatic assembly has broad applications in industries. Traditional assembly tasks utilize predefined trajectories or tuned force control parameters, which make the automatic assembly time-consuming, difficult to generalize, and not robust to uncertainties. In this paper, we propose a learning framework for high precision industrial assembly. The framework combines both the supervised learning and the reinforcement learning. The supervised learning utilizes trajectory optimization to provide the initial guidance to the policy, while the reinforcement learning utilizes actor-critic algorithm to establish the evaluation system even the supervisor is not accurate. The proposed learning framework is more efficient compared with the reinforcement learning and achieves better stability performance than the supervised learning. The effectiveness of the method is verified by both the simulation and experiment. Experimental videos are available at [1].


Title: Manipulation by Feel: Touch-Based Control with Deep Predictive Models
Key Words: dexterous manipulators  neurocontrollers  predictive control  tactile sensors  unsupervised learning  feel  touch-based control  deep predictive models  touch sensing  dexterous robotic manipulation  tactile sensing  nonprehensile manipulation  general purpose control techniques  accurate physics models  tactile percepts  high-resolution tactile  deep neural network dynamics models  deep tactile MPC  tactile servoing  raw tactile sensor inputs  GelSight-style tactile sensor  learned tactile predictive model  user-specified configurations  goal tactile reading  Task analysis  Predictive models  Tactile sensors  Videos 
Abstract: Touch sensing is widely acknowledged to be important for dexterous robotic manipulation, but exploiting tactile sensing for continuous, non-prehensile manipulation is challenging. General purpose control techniques that are able to effectively leverage tactile sensing as well as accurate physics models of contacts and forces remain largely elusive, and it is unclear how to even specify a desired behavior in terms of tactile percepts. In this paper, we take a step towards addressing these issues by combining high-resolution tactile sensing with data-driven modeling using deep neural network dynamics models. We propose deep tactile MPC, a framework for learning to perform tactile servoing from raw tactile sensor inputs, without manual supervision. We show that this method enables a robot equipped with a GelSight-style tactile sensor to manipulate a ball, analog stick, and 20-sided die, learning from unsupervised autonomous interaction and then using the learned tactile predictive model to reposition each object to user-specified configurations, indicated by a goal tactile reading. Videos, visualizations and the code are available here: https://sites.google.com/view/deeptactilempc.


Title: Ways to Learn a Therapist’s Patient-specific Intervention: Robotics-vs Telerobotics-mediated Hands-on Teaching
Key Words: medical robotics  patient rehabilitation  patient treatment  telerobotics  therapists time  healthcare resources  rehabilitation services  robot-assisted rehabilitation  economical solution  LfD  robotic rehabilitation  learning from demonstration  telerobotic-mediated hands-on teaching  telerobotic-mediated kinesthetic teaching  TMKT  RMKT  Task analysis  Impedance  Medical treatment  Robot kinematics  Robot sensing systems  Service robots 
Abstract: Due to the limitations of therapists time and healthcare resources to cover the increasing demand for rehabilitation services, robot-assisted rehabilitation is becoming an appealing, powerful and economical solution. In our previous research, a solution that combines Learning from Demonstration (LfD) and robotic rehabilitation to save the therapists time and reduce the therapy costs was proposed. In this paper we compare two modalities, Robot-and Telerobotic-Mediated Kinesthetic Teaching (RMKT and TMKT), for implementing LfD in robotic rehabilitation. Our results show that behaviors demonstrated in both modalities are able to be imitated accurately, but demonstrations in TMKT have less repeatability.


Title: Towards Learning Abstract Representations for Locomotion Planning in High-dimensional State Spaces
Key Words: convolutional neural nets  learning (artificial intelligence)  mobile robots  neurocontrollers  path planning  cost function  search-based planning  hybrid driving-stepping locomotion  locomotion planning  high-dimensional state spaces  ground robots  ground structure  movable body parts  robot representation  Planning  Robots  Cost function  Task analysis  Search problems  Acceleration  Tuning 
Abstract: Ground robots which are able to navigate a variety of terrains are needed in many domains. One of the key aspects is the capability to adapt to the ground structure, which can be realized through movable body parts coming along with additional degrees of freedom (DoF). However, planning respective locomotion is challenging since suitable representations result in large state spaces. Employing an additional abstract representation-which is coarser, lower-dimensional, and semantically enriched-can support the planning. While a desired robot representation and action set of such an abstract representation can be easily defined, the cost function requires large tuning efforts. We propose a method to represent the cost function as a CNN. Training of the network is done on generated artificial data, while it generalizes well to the abstraction of real world scenes. We further apply our method to the problem of search-based planning of hybrid driving-stepping locomotion. The abstract representation is used as a powerful informed heuristic which accelerates planning by multiple orders of magnitude.


Title: Neural Network Pile Loading Controller Trained by Demonstration
Key Words: backpropagation  Bayes methods  construction equipment  feedforward neural nets  industrial robots  mobile robots  neurocontrollers  sensors  automated pile loading  robotic wheel loader  control signals  hydrostatic driving pressure  boom control  single hidden layer  training data  bucket filling performance  heuristic automated controller  manual human control  demonstration approach  Bayesian regularization backpropagation algorithm  end-to-end neural network controllers  3D laser scan  Levenberg-Marquardt algorithm  Artificial neural networks  Loading  Robot sensing systems  Wheels  Training  Neurons 
Abstract: This paper presents the development and testing of end-to-end Neural Network (NN) controllers for automated pile loading with a robotic wheel loader. NNs were trained using the Learning from Demonstration approach, i.e. by first recording sensor and control signals during manually-driven pile loading actions. Training made use of three input signals: boom angle, bucket angle and hydrostatic driving pressure; and three output signals: boom control, bucket control and the gas command. Most testing was conducted using NNs with 5 neurons in a single hidden layer, which were able to fill the bucket reasonably well. Qualitative comparisons were made to ascertain how the amount of training data and number of hidden neurons affects bucket filling performance, for NNs trained using both the Levenberg-Marquardt and Bayesian Regularization backpropagation algorithms. Different NNs trained with the same data were also compared. An additional pile transfer experiment compared the performance of an NN controller with a heuristic automated controller and manual human control. By estimating the total volume of material transferred using 3D laser scans, human control was found to have the highest performance, though the NN outperformed the heuristic controller. This indicated that end-to-end NN control trained by demonstration could offer improvement over current heuristic methods for automated pile loading.


Title: Fast Radar Motion Estimation with a Learnt Focus of Attention using Weak Supervision
Key Words: distance measurement  image filtering  image fusion  image segmentation  learning (artificial intelligence)  mobile robots  motion estimation  pose estimation  radar imaging  robot vision  fast radar motion estimation  data association  radar odometry  weak supervision  focus of attention policy  learning algorithm  raw data prefiltering  image segmentation network  radar processing time reduction  field robots  consistent motion estimation  copious annotated measurements  wheel odometry  external ego-motion estimator  short-term sensor coherence  measurement stream  scanning radar  Azimuth  Robot sensing systems  Task analysis  Laser radar  Training  Labeling  radar  sensing  ego-motion estimation  field robotics 
Abstract: This paper is about fast motion estimation with scanning radar. We use weak supervision to train a focus of attention policy which actively down-samples the measurement stream before data association steps are undertaken. At training, we avoid laborious manual labelling by exploiting short-term sensor coherence from multiple poses in the presence of an external ego-motion estimator (for example, wheel odometry). In this way, we generate copious annotated measurements which can be used for training a learning algorithm in a weakly-supervised fashion. We demonstrate the validity of the approach in the context of a Radar Odometry (RO) task, pre-filtering raw data with a popular image segmentation network trained as presented. We evaluate our system against 26 km of data collected in Central Oxford and show consistent motion estimation with greatly reduced radar processing times (by a factor of 2.36).


Title: Learned Map Prediction for Enhanced Mobile Robot Exploration
Key Words: information theory  learning (artificial intelligence)  mobile robots  neural nets  autonomous ground robot  geometric heuristics  information theory  deep learning  mobile robot exploration  generative neural network  2D maps  reinforcement learning  robots behavior  Deep learning  Robot sensing systems  Decoding  Gain measurement  Navigation  Training 
Abstract: We demonstrate an autonomous ground robot capable of exploring unknown indoor environments for reconstructing their 2D maps. This problem has been traditionally tackled by geometric heuristics and information theory. More recently, deep learning and reinforcement learning based approaches have been proposed to learn exploration behavior in an end-to-end manner. We present a method that combines the strengths of these different approaches. Specifically, we employ a state-of-the-art generative neural network to predict unknown regions of a partially explored map, and use the prediction to enhance the exploration in an information-theoretic manner. We evaluate our system in simulation using floor plans of real buildings. We also present comparisons with traditional methods which demonstrate the advantage of our method in terms of exploration efficiency. We retain an advantage over end-to-end learned exploration methods in that the robot's behavior is easily explicable in terms of the predicted map.


Title: Propagation Networks for Model-Based Control Under Partial Observation
Key Words: learning (artificial intelligence)  mobile robots  off-the-shelf physics engines  interaction networks  pairwise interactions  propagation networks  differentiable dynamics model  learnable dynamics model  partially observable scenarios  model-based control  deep reinforcement learning algorithms  robot planning  PropNet  Engines  Task analysis  Force  Robots  Computational modeling  Adaptation models 
Abstract: There has been an increasing interest in learning dynamics simulators for model-based control. Compared with off-the-shelf physics engines, a learnable simulator can quickly adapt to unseen objects, scenes, and tasks. However, existing models like interaction networks only work for fully observable systems; they also only consider pairwise interactions within a single time step, both restricting their use in practical systems. We introduce Propagation Networks (PropNet), a differentiable, learnable dynamics model that handles partially observable scenarios and enables instantaneous propagation of signals beyond pairwise interactions. With these innovations, our propagation networks not only outperform current learnable physics engines in forward simulation, but also achieves superior performance on various control tasks. Compared with existing deep reinforcement learning algorithms, model-based control with propagation networks is more accurate, efficient, and generalizable to novel, partially observable scenes and tasks.


Title: Interactive Trajectory Prediction for Autonomous Driving via Recurrent Meta Induction Neural Network
Key Words: automobiles  estimation theory  learning (artificial intelligence)  mobile robots  recurrent neural nets  road safety  road traffic control  stochastic processes  traffic engineering computing  lane change trajectory  meta-learning framework  autonomous driving data collection  interactive trajectory prediction  interactive driving  autonomous cars  stochastic processes  driving behavior estimation  recurrent neural cell  recurrent meta induction neural network  human-driven cars behaviors  conditional neural process  Trajectory  Automobiles  Autonomous vehicles  Estimation  Generators  Task analysis  Stochastic processes 
Abstract: Interactive driving is challenging but essential for autonomous cars in dense traffic or urban areas. Proper interaction requires understanding and prediction of future trajectories of all neighboring cars around a target vehicle. Current solutions typically assume a certain distribution or stochastic process to approximate human-driven cars' behaviors. To relax this assumption, a Recurrent Meta Induction Network (RMIN) framework is developed. The original Conditional Neural Process (CNP) on which this is based does not consider the sequence of the conditions, due to the permutation invariance requirements for stochastic processes. However, the sequential information is important for the driving behavior estimation. Therefore, in the proposed method, a recurrent neural cell replaces the original demonstration sub-net. The behavior estimation is conditioned on the historical observations for all related cars, including the target car and its surrounding cars. The method is applied to predict the lane change trajectory of a target car in dense traffic areas. The proposed method achieves better results than previous methods and thanks to the meta-learning framework, it can use a smaller dataset, putting fewer demands on autonomous driving data collection.


Title: Improving Keypoint Matching Using a Landmark-Based Image Representation
Key Words: convolutional neural nets  image matching  image representation  learning (artificial intelligence)  landmark-based image representation  visual loop closure verification  multiview geometry  MVG  deep learning  convolutional neural network features  matched landmark pairs  image representation  verification method  keypoint matching method  ConvNet features  Lighting  Proposals  Visualization  Standards  Feature extraction  Simultaneous localization and mapping  Image representation 
Abstract: Motivated by the need to improve the performance of visual loop closure verification via multi-view geometry (MVG) under significant illumination and viewpoint changes, we propose a keypoint matching method that uses landmarks as an intermediate image representation in order to leverage the power of deep learning. In environments with various changes, the traditional verification method via MVG may encounter difficulty because of their inability to generate a sufficient number of correctly matched keypoints. Our method exploits the excellent invariance properties of convolutional neural network (ConvNet) features, which have shown outstanding performance for matching landmarks between images. By generating and matching landmarks first in the images and then matching the keypoints within the matched landmark pairs, we can significantly improve the quality of matched keypoints in terms of precision and recall measures. The proposed method is validated on challenging datasets that involve significant illumination and viewpoint changes, to establish its superior performance to the standard keypoint matching method.


Title: Learning Robust Manipulation Strategies with Multimodal State Transition Models and Recovery Heuristics
Key Words: control engineering computing  learning (artificial intelligence)  manipulator dynamics  mobile robots  optimisation  robot programming  contact dynamics  reinforcement learning  recovery skills  multiple contact state changes  multimodal state transition model  recovery heuristics  contact-based manipulations  robust manipulation strategies learning  skill selections  Task analysis  End effectors  Robustness  Data models  Planning  Clustering algorithms 
Abstract: Robots are prone to making mistakes when performing manipulation tasks in unstructured environments. Robust policies are thus needed to not only avoid mistakes but also to recover from them. We propose a framework for increasing the robustness of contact-based manipulations by modeling the task structure and optimizing a policy for selecting skills and recovery skills. A multimodal state transition model is acquired based on the contact dynamics of the task and the observed transitions. A policy is then learned from the model using reinforcement learning. The policy is incrementally improved by expanding the action space by generating recovery skills with a heuristic. Evaluations on three simulated manipulation tasks demonstrate the effectiveness of the framework. The robot was able to complete the tasks despite multiple contact state changes and errors encountered, increasing the success rate averaged across the tasks from 70.0% to 95.3%.


Title: Accounting for Part Pose Estimation Uncertainties during Trajectory Generation for Part Pick-Up Using Mobile Manipulators
Key Words: grippers  learning (artificial intelligence)  mobile robots  optimisation  path planning  pose estimation  position control  probability  mobile base trajectory  active learning based approach  optimization-based framework  time-optimal trajectories  part pose estimation uncertainties  trajectory generation  part pick-up  mobile manipulators  operation time  gripper  Grippers  Grasping  Trajectory  Uncertainty  Physics  End effectors 
Abstract: To minimize the operation time, mobile manipulators need to pick-up parts while the mobile base and the gripper are moving. The gripper speed needs to be selected to ensure that the pick-up operation does not fail due to uncertainties in part pose estimation. This, in turn, affects the mobile base trajectory. This paper presents an active learning based approach to construct a meta-model to estimate the probability of successful part pick-up for a given level of uncertainty in the part pose estimate. Using this model, we present an optimization-based framework to generate time-optimal trajectories that satisfy the given level of success probability threshold for picking-up the part.


Title: Feedback motion planning of legged robots by composing orbital Lyapunov functions using rapidly-exploring random trees
Key Words: feedback  learning (artificial intelligence)  legged locomotion  Lyapunov methods  neurocontrollers  path planning  random processes  sampling methods  trajectory control  trees (mathematics)  feedback motion planning  legged robots  orbital Lyapunov functions  sampling-based framework  Poincaré section  multiple trajectory optimization problems  rapidly-exploring random tree algorithm  regions of attraction  deep learning neural networks  Limit-cycles  Planning  Legged locomotion  Lyapunov methods  Force  Trajectory optimization 
Abstract: We present a sampling-based framework for feedback motion planning of legged robots. Our framework is based on switching between limit cycles at a fixed instance of motion, the Poincaré section (e.g., apex or touchdown), by finding overlaps between the regions of attraction (ROA) of two limit cycles. First, we assume a candidate orbital Lyapunov function (OLF) and define a ROA at the Poincaré section. Next, we solve multiple trajectory optimization problems, one for each sampled initial condition on the ROA to minimize an energy metric and subject to the exponential convergence of the OLF between two steps. The result is a table of control actions and the corresponding initial conditions at the Poincaré section. Then we develop a control policy for each control action as a function of the initial condition using deep learning neural networks. The control policy is validated by testing on initial conditions sampled on ROA of randomly chosen limit cycles. Finally, the rapidly-exploring random tree algorithm is adopted to plan transitions between the limit cycles using the ROAs. The approach is demonstrated on a hopper model to achieve velocity and height transitions between steps.


Title: LookUP: Vision-Only Real-Time Precise Underground Localisation for Autonomous Mining Vehicles
Key Words: cameras  image sensors  mining  mining industry  mobile robots  neural nets  robot vision  autonomous underground mining vehicles  neural-network-based pixel sampling strategy  visual based technique  real-time accurate localisation system  range sensor-based system  ceiling-facing cameras  Optical imaging  Cameras  Optical sensors  Adaptive optics  Heating systems  Optical network units  Real-time systems 
Abstract: A key capability for autonomous underground mining vehicles is real-time accurate localisation. While significant progress has been made, currently deployed systems have several limitations ranging from dependence on costly additional infrastructure to failure of both visual and range-sensor-based techniques in highly aliased or visually challenging environments. In our previous work, we presented a lightweight coarse vision-based localisation system that could map and then localise to within a few metres in an underground mining environment. However, this level of precision is insufficient for providing a cheaper, more reliable vision-based automation alternative to current range sensor-based systems. Here we present a new precision localisation system dubbed “LookUP”, which learns a neural-network-based pixel sampling strategy for estimating homographies based on ceiling-facing cameras without requiring any manual labelling. This new system runs in real time on limited computation resource and is demonstrated on two different underground mine sites, achieving real time performance at ~5 frames per second and a much improved average localisation error of ~1.2 metre.


Title: Robust object grasping in clutter via singulation
Key Words: function approximation  learning (artificial intelligence)  manipulators  neural nets  object detection  robot vision  robust object grasping  singulation  cluttered environment  collision free grasp affordances  optimal push policies  action-value function approximation  robot training  deep neural network  reinforcement learning  Robots  Task analysis  Image segmentation  Reinforcement learning  Clutter  Neural networks  Collision avoidance 
Abstract: Grasping objects in a cluttered environment is challenging due to the lack of collision free grasp affordances. In such conditions, the target object touches or is covered by other objects in the scene, resulting in a failed grasp. To address this problem, we propose a strategy of singulating the object from its surrounding clutter, which consists of previously unseen objects, by means of lateral pushing movements. We employ reinforcement learning for obtaining optimal push policies given depth observations of the scene. The action-value function(Q-function) is approximated with a deep neural network. We train the robot in simulation and we demonstrate that the transfer of learned policies to the real environment is robust.


Title: Transferring Grasp Configurations using Active Learning and Local Replanning
Key Words: dexterous manipulators  image segmentation  learning (artificial intelligence)  path planning  robot vision  grasp configurations  active learning  prior example objects  similar shapes  geometric shape characteristics  semantic shape characteristics  grasp space  model parts  corresponding grasps  local replanning  point cloud  robotic hands  Shape  Grasping  Semantics  Three-dimensional displays  Robots  Stability analysis  Particle swarm optimization 
Abstract: We present a new approach to transfer grasp configurations from prior example objects to novel objects. We assume the novel and example objects have the same topology and similar shapes. We perform 3D segmentation on these objects using geometric and semantic shape characteristics. We compute a grasp space for each part of the example object using active learning. We build bijective contact mapping between these model parts and compute the corresponding grasps for novel objects. Finally, we assemble the individual parts and use local replanning to adjust grasp configurations while maintaining its stability and physical constraints. Our approach is general, can handle all kind of objects represented using mesh or point cloud and a variety of robotic hands.


Title: Goal-Driven Navigation for Non-holonomic Multi-Robot System by Learning Collision
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  multi-robot systems  goal-driven navigation  nonholonomic multirobot system  learning collision  reinforcement learning  multirobot collision avoidance approach  training agent robots  agent robot  trained policy  multiple obstacle robots  robot simulation  robot experiment  Robots  Collision avoidance  Training  Planning  Task analysis  Servers  Heuristic algorithms 
Abstract: In this paper, we propose the reinforcement learning based multi-robot collision avoidance approach by learning collision. Dynamical path re-planning, which is massively used in classical collision avoidance methods, needs overall information of the environment. Also, training agent robots to avoid the collision and pursue a goal point simultaneously is inefficient since the agent should learn two tasks. As the number of tasks that the agent should learn increases, it is difficult to make the performance of an algorithm consistent, which is known as reproducibility issue. To overcome these limitations, Collision Avoidance by Learning Collision (CALC), which learns collision instead of avoiding an obstacle robot is suggested. To solve the collision avoidance problem efficiently, the proposed method divides the problem into training and planning. In the training algorithm, an agent robot learns how to collide with a single obstacle robot and then generates a trained policy. With the trained policy, the agent can pursue a goal point since the policy leads the agent to `collide' with the goal. Furthermore, by taking action in a reverse way from the trained policy, the agent can avoid multiple obstacle robots in the planning algorithm at once. The proposed method is validated both in the robot simulation and real robot experiment, and compared with the existing collision avoidance method.


Title: An Approach for Semantic Segmentation of Tree-like Vegetation
Key Words: convolutional neural nets  forestry  image classification  image colour analysis  image fusion  image segmentation  learning (artificial intelligence)  vegetation  semantic segmentation  tree-like vegetation  pipeline  single RGB-D image  deep network  multiple convolutional neural network architectures  colour data  asynchronous training approach  3-channel HHA image  late fusion architecture  synthetic dataset  broadleaf trees  tree species  Vegetation  Image segmentation  Image color analysis  Semantics  Vegetation mapping  Training  Robots 
Abstract: This paper presents a pipeline for semantic segmentation of trees into their components. Given a single RGB-D image of a tree, we employ a deep network to predict labels to classify each pixel of the tree into trunk, branches, twigs and leaves. Multiple convolutional neural network architectures to combine the complementary modalities of depth and colour data are investigated. An asynchronous training approach where two networks trained separately on RGB and depth encoded as a 3-channel HHA image are combined using a late fusion architecture with different learning rates performs the best. Training and evaluation are performed on a synthetic dataset of 6 species of broadleaf trees. We further demonstrate the network's generalization capabilities, across various tree species on the synthetic dataset, achieving an accuracy of upto 92.5%. Furthermore, we present a qualitative evaluation of our approach on real-world data.


Title: Learning to Capture a Film-Look Video with a Camera Drone
Key Words: autonomous aerial vehicles  collision avoidance  decision making  image capture  image sensors  image sequences  learning (artificial intelligence)  mobile robots  motion control  object tracking  robot vision  video cameras  video signal processing  film-look video  camera drone  intelligent drones  smarter assistant tools  autonomous aerial filming  camera motion planning  cinematic footages  film-look aerial footage  camera position  automatic filming onboard  data-driven planning approach  image composition  data-driven learning-based approach  professional cameramans intention  decision-making process  Cameras  Drones  Training  Skeleton  Streaming media  Two dimensional displays  Feature extraction 
Abstract: The development of intelligent drones has simplified aerial filming and provided smarter assistant tools for users to capture a film-look footage. Existing methods of autonomous aerial filming either specify predefined camera movements for a drone to capture a footage, or employ heuristic approaches for camera motion planning. However, both predefined movements and heuristically planned motions are hardly able to provide cinematic footages for various dynamic scenarios. In this paper, we propose a data-driven learning-based approach, which can imitate a professional cameraman's intention for capturing a film-look aerial footage of a single subject in real-time. We model the decision-making process of the cameraman with two steps: 1) we train a network to predict the future image composition and camera position, and 2) our system then generates control commands to achieve the desired shot framing. At the system level, we deploy our algorithm on the limited resources of a drone and demonstrate the feasibility of running automatic filming onboard in real-time. Our experiments show how our data-driven planning approach achieves film-look footages and successfully mimics the work of a professional cameraman.


Title: A Fog Robotic System for Dynamic Visual Servoing
Key Words: cloud computing  control engineering computing  Internet  mobile robots  motion control  multi-robot systems  object recognition  position control  robot vision  service robots  telerobotics  visual servoing  dynamic visual  cloud robotics  multiple robots  cloud services  unlimited computation power  network communication  dynamic compliant service robots  human compliant service robots  dynamic self-balancing robot  cloud teleoperation  cloud-based image based visual servoing module  cloud teleoperator  real-time automation system  cloud-edge hybrid design  dynamic robotic control  deep-learning recognition systems  self-balancing service robot  fog robotic object recognition system  Cloud computing  Service robots  Robot sensing systems  Legged locomotion  Visualization 
Abstract: Cloud Robotics is a paradigm where multiple robots are connected to cloud services via Internet to access “unlimited” computation power, at the cost of network communication. However, due to limitations such as network latency and variability, it is difficult to control dynamic, human compliant service robots directly from the cloud. In this work, we combine cloud robotics with an agile edge device to build a Fog Robotic system by leveraging an asynchronous protocol with a “heartbeat” signal. We use the system to enable robust teleoperation of a dynamic self-balancing robot from the cloud. We use the system to pick up boxes from static locations, a task commonly performed in warehouse logistics. To make cloud teleoperation more intuitive and efficient, we program a cloud-based image based visual servoing (IBVS) module to automatically assist the cloud teleoperator during the object pickups. Visual feedbacks, including apriltag recognition and tracking, are performed in the cloud to emulate a Fog Robotic object recognition system for IBVS. We demonstrate the feasibility of a dynamic real-time automation system using this cloud-edge hybrid design, which opens up possibilities of deploying dynamic robotic control with deep-learning recognition systems in Fog Robotics. Finally, we show that Fog Robotics enables the self-balancing service robot to pick up a box automatically from a person under unstructured environments.


Title: Informed Information Theoretic Model Predictive Control
Key Words: nonlinear control systems  nonlinear dynamical systems  optimal control  optimisation  predictive control  robust control  sampling methods  informed sampling distribution  optimized controls  informed information theoretic model predictive control  nonlinear control systems  uncertainties  highly nonlinear dynamics  contextual information  generative models  trajectory control  sampling-based MPC methods  robustness properties  conditional variational autoencoders  autonomous navigation domain  Task analysis  Optimal control  Trajectory  Decoding  Robots  Planning  Cost function 
Abstract: The problem of minimizing cost in nonlinear control systems with uncertainties or disturbances remains a major challenge. Model predictive control (MPC), and in particular sampling-based MPC has recently shown great success in complex domains such as aggressive driving with highly nonlinear dynamics. Sampling-based methods rely on a prior distribution to generate samples in the first place. Obviously, the choice of this distribution highly influences efficiency of the controller. Existing approaches such as sampling around the control trajectory of the previous time step perform suboptimally, especially in multi-modal or highly dynamic settings. In this work, we therefore propose to learn models that generate samples in low-cost areas of the state-space, conditioned on the environment and on contextual information of the task to solve. By using generative models as an informed sampling distribution, our approach exploits guidance from the learned models and at the same time maintains robustness properties of the MPC methods. We use Conditional Variational Autoencoders (CVAE) to learn distributions that imitate samples from a training dataset containing optimized controls. An extensive evaluation in the autonomous navigation domain suggests that replacing previous sampling schemes with our learned models considerably improves performance in terms of path quality and planning efficiency.


Title: Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty
Key Words: Bayes methods  belief networks  control engineering computing  learning (artificial intelligence)  mobile robots  neurocontrollers  robust control  uncertainty-aware robotic perception  explicit generative model  observation distribution  action-conditioned collision prediction task  Bayesian neural network techniques  task-aware generative uncertainty  deep learning  open world  real-world robotic systems  mobile robots  out-of-distribution observations  neural network predictions  robotic perception  approximate Bayesian approach  Uncertainty  Robots  Predictive models  Bayes methods  Neural networks  Training  Collision avoidance 
Abstract: Deep learning provides a powerful tool for robotic perception in the open world. However, real-world robotic systems, especially mobile robots, must be able to react intelligently and safely even in unexpected circumstances. This requires a system that knows what it knows, and can estimate its own uncertainty for unfamiliar, out-of-distribution observations. Approximate Bayesian approaches are commonly used to estimate uncertainty for neural network predictions, but struggle with out-of-distribution observations. Generative models can in principle detect out-of-distribution observations as those with a low estimated density, but overly pessimistic as an uncertainty measure, since the mere presence of an out-of-distribution input does not by itself indicate an unsafe situation. Intuitively, we would like a perception system that can detect when task-salient parts of the image are unfamiliar or uncertain, while ignoring task-irrelevant features. In this paper, we present a method for uncertainty-aware robotic perception that combines generative modeling and model uncertainty. Our method estimates an uncertainty measure about the model's prediction, taking into account an explicit generative model of the observation distribution to handle out-of-distribution inputs. We evaluate our method on an action-conditioned collision prediction task with both simulated and real data, and demonstrate that our approach improves on a variety of Bayesian neural network techniques.


Title: Classifying Pedestrian Actions In Advance Using Predicted Video Of Urban Driving Scenes
Key Words: feature extraction  graphics processing units  image classification  image representation  image sequences  learning (artificial intelligence)  pedestrians  traffic engineering computing  video signal processing  encoder-decoder network models  predicted video  urban driving scenes  traffic scene  pedestrian behaviour  urban pedestrian  binary action classifier  iterative transform  image sequence  GPU  Hidden Markov models  Decoding  Predictive models  Kernel  Training  Automobiles  Iterative decoding 
Abstract: Fig. 1.Generating predictions of a future for a pedestrian attempting to cross the street. We pick out two key frames from the (a) input sequence and the (b) ground truth sequence, 16 frames apart. Image (c) shows our prediction at the same time instant as the ground truth.We explore prediction of urban pedestrian actions by generating a video future of the traffic scene, and show promising results in classifying pedestrian behaviour before it is observed. We compare several encoder-decoder network models that predict 16 frames (400-600 milliseconds of video) from the preceding 16 frames. Our main contribution is a method for learning a sequence of representations to iteratively transform features learnt from the input to the future. Then we use a binary action classifier network for determining a pedestrian's crossing intent from predicted video. Our results show an average precision of 81%, significantly higher than previous methods. The model with the best classification performance runs for 117 ms on commodity GPU, giving an effective look-ahead of 416 ms.


Title: Learning to Write Anywhere with Spatial Transformer Image-to-Motion Encoder-Decoder Networks
Key Words: affine transforms  handwritten character recognition  humanoid robots  image coding  image motion analysis  learning (artificial intelligence)  robot vision  DMP  digit drawings  image-to-motion encoder-decoder networks  convolutional layers  humanoid robot  affine transformed digits  fully differentiable overall network  spatial transformer  motion trajectories  digit images  robot vision  handwritten characters  Trajectory  Robots  Transforms  Task analysis  Writing  Neural networks  Decoding 
Abstract: Learning to recognize and reproduce handwritten characters is already a challenging task both for humans and robots alike, but learning to do the same thing for characters that can be transformed arbitrarily in space, as humans do when writing on a blackboard for instance, significantly ups the ante from a robot vision and control perspective. In previous work we proposed various different forms of encoder-decoder networks that were capable of mapping raw images of digits to dynamic movement primitives (DMPs) such that a robot could learn to translate the digit images into motion trajectories in order to reproduce them in written form. However, even with the addition of convolutional layers in the image encoder, the extent to which these networks are spatially invariant or equivariant is rather limited. In this paper, we propose a new architecture that incorporates both an image-to-motion encoder-decoder and a spatial transformer in a fully differentiable overall network that learns to rectify affine transformed digits in input images into canonical forms, before converting them into DMPs with accompanying motion trajectories that are finally transformed back to match up with the original digit drawings such that a robot can write them in their original forms. We present experiments with various challenging datasets that demonstrate the superiority of the new architecture compared to our previous work and demonstrate its use with a humanoid robot in a real writing task.


Title: Improving Data Efficiency of Self-supervised Learning for Robotic Grasping
Key Words: convolutional neural nets  force feedback  grippers  learning (artificial intelligence)  manipulators  robot vision  fully-convolutional neural network  gripper parameters  inference performance  random grasp success rate  grasp space  systematic manner  typical bin picking scenarios  self-supervised learning  robotic grasping  depth camera input  gripper force feedback  learning algorithm  geometric consistency  undistorted depth images  task space  grasp attempts  data efficiency  training data  Grasping  Grippers  Robot kinematics  Artificial neural networks  Task analysis  Training 
Abstract: Given the task of learning robotic grasping solely based on a depth camera input and gripper force feedback, we derive a learning algorithm from an applied point of view to significantly reduce the amount of required training data. Major improvements in time and data efficiency are achieved by: Firstly, we exploit the geometric consistency between the undistorted depth images and the task space. Using a relative small, fully-convolutional neural network, we predict grasp and gripper parameters with great advantages in training as well as inference performance. Secondly, motivated by the small random grasp success rate of around 3 %, the grasp space was explored in a systematic manner. The final system was learned with 23 000 grasp attempts in around 60 h, improving current solutions by an order of magnitude. For typical bin picking scenarios, we measured a grasp success rate of (96.6 ± 1.0) %. Further experiments showed that the system is able to generalize and transfer knowledge to novel objects and environments.


Title: Online Object and Task Learning via Human Robot Interaction
Key Words: force control  human-robot interaction  learning (artificial intelligence)  motion control  neural nets  deep learning based localization  recognition system  intuitive user interface  3D motion task  hybrid force-vision control module  compliant motion  task learning  human robot interaction  KUKA Innovation Award competition  Hanover Messe 2018  online object learning  incremental object learning module  Robots  Education  Three-dimensional displays  Task analysis  Feature extraction  Human-robot interaction 
Abstract: This work describes the development of a robotic system that acquires knowledge incrementally through human interaction where new objects and motions are taught on the fly. The robotic system developed was one of the five finalists in the KUKA Innovation Award competition and demonstrated during the Hanover Messe 2018 in Germany. The main contributions of the system are i) a novel incremental object learning module - a deep learning based localization and recognition system - that allows a human to teach new objects to the robot, ii) an intuitive user interface for specifying 3D motion task associated with the new object, and iii) a hybrid force-vision control module for performing compliant motion on an unstructured surface. This paper describes the implementation and integration of the main modules of the system and summarizes the lessons learned from the competition.


Title: Color-Coded Fiber-Optic Tactile Sensor for an Elastomeric Robot Skin
Key Words: cameras  robots  tactile sensors  contact localization  robotic perception system  force sensing range  color-coded tactile sensor  tactile exploration  robust tactile sensing  color-coded fiber-optic tactile sensor  elastomeric robot skin  artificial tactile skin  transparent silicone rubber  off-the-shelf color camera  camera POFs  Optical sensors  Tactile sensors  Image color analysis  Cameras 
Abstract: The sense of touch is essential for reliable mapping between the environment and a robot which interacts physically with objects. Presumably, an artificial tactile skin would facilitate safe interaction of the robots with the environment. In this work, we present our color-coded tactile sensor, incorporating plastic optical fibers (POF), transparent silicone rubber and an off-the-shelf color camera. Processing electronics are placed away from the sensing surface to make the sensor robust to harsh environments. Contact localization is possible thanks to the lower number of light sources compared to the number of camera POFs. Classical machine learning techniques and a hierarchical classification scheme were used for contact localization. Specifically, we generated the mapping from stimulation to sensation of a robotic perception system using our sensor. We achieved a force sensing range up to 18 N with the force resolution of around 3.6 N and the spatial resolution of 8 mm. The color-coded tactile sensor is suitable for tactile exploration and might enable further innovations in robust tactile sensing.


Title: Reinforcement Learning in Topology-based Representation for Human Body Movement with Whole Arm Manipulation
Key Words: humanoid robots  human-robot interaction  learning (artificial intelligence)  manipulator dynamics  medical robotics  path planning  patient care  position control  topology-based representation  human body movement  bulky object  WAM  manipulation places  global properties  local contacts  grasping  reinforcement learning problem  robot behavior  human motion  robot-human interaction  topology-based coordinates  torso positions  learned policy  body shapes  dynamic sea rescue scenario  unseen scenarios  differently-shaped humans  whole arm manipulation  Robot kinematics  Humanoid robots  Manipulators  Laplace equations  Torso  Shape 
Abstract: Moving a human body or a large and bulky object may require the strength of whole arm manipulation (WAM). This type of manipulation places the load on the robot's arms and relies on global properties of the interaction to succeed- rather than local contacts such as grasping or non-prehensile pushing. In this paper, we learn to generate motions that enable WAM for holding and transporting of humans in certain rescue or patient care scenarios. We model the task as a reinforcement learning problem in order to provide a robot behavior that can directly respond to external perturbation and human motion. For this, we represent global properties of the robot-human interaction with topology-based coordinates that are computed from arm and torso positions. These coordinates also allow transferring the learned policy to other body shapes and sizes. For training and evaluation, we simulate a dynamic sea rescue scenario and show in quantitative experiments that the policy can solve unseen scenarios with differently-shaped humans, floating humans, or with perception noise. Our qualitative experiments show the subsequent transporting after holding is achieved and we demonstrate that the policy can be directly transferred to a real world setting.


Title: Demonstration-Guided Deep Reinforcement Learning of Control Policies for Dexterous Human-Robot Interaction
Key Words: dexterous manipulators  humanoid robots  human-robot interaction  learning (artificial intelligence)  motion control  neural nets  handshake  hand clap  finger touch  training control policies  human-robot interactions  hand claps  humanoid Shadow Dexterous Hand  robot arm  multiobjective reward function  reward structure  motion capture data  human-human interactions  hand interactions  dexterous human-robot interaction  demonstration-guided deep reinforcement learning  Training  Humanoid robots  Task analysis  Robot kinematics  Reinforcement learning  Convergence 
Abstract: In this paper, we propose a method for training control policies for human-robot interactions such as handshakes or hand claps via Deep Reinforcement Learning. The policy controls a humanoid Shadow Dexterous Hand, attached to a robot arm. We propose a parameterizable multi-objective reward function that allows learning of a variety of interactions without changing the reward structure. The parameters of the reward function are estimated directly from motion capture data of human-human interactions in order to produce policies that are perceived as being natural and human-like by observers. We evaluate our method on three significantly different hand interactions: handshake, hand clap and finger touch. We provide detailed analysis of the proposed reward function and the resulting policies and conduct a large-scale user study, indicating that our policy produces natural looking motions.


Title: Coordinated multi-robot planning while preserving individual privacy
Key Words: collision avoidance  cryptography  data privacy  mobile robots  motion control  multi-robot systems  coordinated multirobot planning  individual privacy  multiple robots  privacy-preserving rendezvous  persistent monitoring  joint plan  collective motions  collective task  joint-collision determination  mobile robots  secure path intersection primitives  homomorphic encryption  Robot kinematics  Collision avoidance  Protocols  Task analysis  Privacy  Encryption 
Abstract: We consider the problem of multiple robots that must cooperate within a shared environment, but which wish to limit the information they disclose during their coordination efforts. Specifically, we examine the problems of privacy-preserving rendezvous and persistent monitoring. In the former, the robots construct a joint plan to have them meet, without either knowing beforehand where or when the meeting will occur. In the latter, multiple robots dynamically cover a region of space-they plan collective motions which are collision-free but with the assurance that agents remain ignorant of the paths of others. Accordingly, the tasks are sort of inverses in that the robots must collectively determine whether their joint paths collide or not, then, using this, achieve their collective task. Other than what is learned by the outcome of the joint-collision determination, the robots possess no details of the other paths. Our approach builds on garbled circuits and homomorphic encryption to realize basic secure path intersection primitives. We present algorithms, a software implementation, and a physical experiment on mobile robots to test the practical feasibility of our approach. We believe that these ideas provide a valuable direction for adoption in small Unmanned Systems belonging to different stakeholders.


Title: Multi-Agent Synchronization Using Online Model-Free Action Dependent Dual Heuristic Dynamic Programming Approach
Key Words: adaptive control  approximation theory  discrete time systems  dynamic programming  iterative methods  learning systems  multi-agent systems  neurocontrollers  nonlinear control systems  optimal control  action dependent dual heuristic dynamic programming schemes  fast solution platforms  unknown models  uncertain dynamical models  online model-free adaptive  dynamic graphical games  approximate the optimal value function  associated model-free control strategy  model-free coupled Bellman optimality equation  multiagent synchronization  online model-free action dependent dual heuristic dynamic programming approach  approximate dynamic programming platforms  agents interaction  communication graphs  policy iteration process  Mathematical model  Adaptation models  Optimal control  Dynamic programming  Synchronization  Games  Neural networks 
Abstract: Approximate dynamic programming platforms are employed to solve dynamic graphical games, where the agents interact among each other using communication graphs in order to achieve synchronization. Although the action dependent dual heuristic dynamic programming schemes provide fast solution platforms for several control problems, their capabilities degrade for systems with unknown or uncertain dynamical models. An online model-free adaptive learning solution based on action dependent dual heuristic dynamic programming is proposed to solve the dynamic graphical games. It employs distributed actor-critic neural networks to approximate the optimal value function and the associated model-free control strategy for each agent. This is done using a policy iteration process where it does not employ any extensive computational effort, as traditionally observed. The duality between the model-free coupled Bellman optimality equation and the underlying coupled Riccati equation is highlighted. This is followed by a graph simulation scenario to test the usefulness of the proposed policy iteration process.


Title: Inverse Reinforcement Learning of Interaction Dynamics from Demonstrations
Key Words: decision theory  human-robot interaction  learning (artificial intelligence)  Markov processes  inverse reinforcement learning methods  high-level sequential tasks  human demonstrations  POMDP policies  interaction dynamics  human-robot interaction domain  structured interactions  partially observable Markov decision process  learning from demonstration  IRL algorithms  reward function learning  MDPIRL algorithm  Robots  Task analysis  Education  Reinforcement learning  Uncertainty  Aircraft navigation  Human-robot interaction 
Abstract: This paper presents a framework to learn the reward function underlying high-level sequential tasks from demonstrations. The purpose of reward learning, in the context of learning from demonstration (LfD), is to generate policies that mimic the demonstrator's policies, thereby enabling imitation learning. We focus on a human-robot interaction (HRI) domain where the goal is to learn and model structured interactions between a human and a robot. Such interactions can be modeled as a partially observable Markov decision process (POMDP) where the partial observability is caused by uncertainties associated with the ways humans respond to different stimuli. The key challenge in finding a good policy in such a POMDP is determining the reward function that was observed by the demonstrator. Existing inverse reinforcement learning (IRL) methods for POMDPs are computationally very expensive and the problem is not well understood. In comparison, IRL algorithms for Markov decision process (MDP) are well defined and computationally efficient. We propose an approach of reward function learning for high-level sequential tasks from human demonstrations where the core idea is to reduce the underlying POMDP to an MDP and apply any efficient MDPIRL algorithm. Our extensive experiments suggest that the reward function learned this way generates POMDP policies that mimic the policies of the demonstrator well.


Title: Learning to Predict the Wind for Safe Aerial Vehicle Planning
Key Words: autonomous aerial vehicles  convolutional neural nets  mobile robots  path planning  sampling methods  wind  safe aerial vehicle planning  local wind  unmanned aerial vehicles  wind environment  relatively low mass  high-resolution wind fields  terrain elevation model  deep convolutional neural network  sampling-based planner  strong wind scenarios  UAV  inflow conditions  prediction error  wind estimation techniques  Wind forecasting  Planning  Computational modeling  Wind speed  Predictive models  Data models 
Abstract: Obtaining an accurate estimate of the local wind remains a significant challenge for small unmanned aerial vehicles (UAVs). Small UAVs often operate at low altitudes near terrain, where the wind environment can be more complex than at higher altitudes. Combined with their relatively low mass, this makes small UAVs particularly susceptible to wind. In this paper we present an approach for predicting high-resolution wind fields based on a terrain elevation model and known inflow conditions. Our approach uses a deep convolutional neural network (CNN) to generate 3D wind estimates. We show that our approach produces wind estimates with lower prediction error than existing methods, and that inference can be performed on an on-board computer in less than two seconds. By providing the wind estimate to a sampling-based planner we show that the improved estimates allow the planner to generate safer paths in strong wind scenarios than with alternative wind estimation techniques.


Title: Visual recognition in the wild by sampling deep similarity functions
Key Words: convolutional neural nets  image representation  learning (artificial intelligence)  mobile robots  object recognition  pattern classification  probability  probabilities  class variability  images range  training exemplars  relative similarities  class predictions  class labels  CNN  test data  training data  recognition system  target class  deep convolutional neural networks  supervised machine learning  autonomous robot  deep similarity functions  visual recognition  Training  Computer aided instruction  Visualization  Robots  Neural networks  Labeling  Task analysis 
Abstract: Recognising relevant objects or object states in its environment is a basic capability for an autonomous robot. The dominant approach to object recognition in images and range images is classification by supervised machine learning, nowadays mostly with deep convolutional neural networks (CNNs). This works well for target classes whose variability can be completely covered with training examples. However, a robot moving in the wild, i.e., in an environment that is not known at the time the recognition system is trained, will often face domain shift: the training data cannot be assumed to exhaustively cover all the within-class variability that will be encountered in the test data. In that situation, learning is in principle possible, since the training set does capture the defining properties, respectively dissimilarities, of the target classes. But directly training a CNN to predict class probabilities is prone to overfitting to irrelevant correlations between the class labels and the specific subset of the target class that is represented in the training set. We explore the idea to instead learn a Siamese CNN that acts as similarity function between pairs of training examples. Class predictions are then obtained by measuring the similarities between a new test instance and the training samples. We show that the CNN embedding correctly recovers the relative similarities to arbitrary class exemplars in the training set. And that therefore few, randomly picked training exemplars are sufficient to achieve good predictions, making the procedure efficient.


Title: Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving
Key Words: computer vision  convolutional neural nets  learning (artificial intelligence)  object detection  road traffic  traffic engineering computing  binary weight object detector  autonomous driving  energy efficiency  on-board object detection  object detectors  low-precision neural networks  computation requirements  memory footprint  binary weight neural networks  BWNs  knowledge transfer method  full-precision teacher network  MobileNet-based binary weight YOLOv2 detectors  pedestrian  cyclist detection  KITTI benchmark  MobileNet-YOLO  DarkNet-YOLO  deep convolutional neural network  word length 1.0 bit  memory size 8.8 MByte to 257.0 MByte  memory size 7.9 MByte to 193.0 MByte  Training  Detectors  Neural networks  Quantization (signal)  Knowledge transfer  Task analysis  Autonomous vehicles 
Abstract: Autonomous driving has harsh requirements of small model size and energy efficiency, in order to enable the embedded system to achieve real-time on-board object detection. Recent deep convolutional neural network based object detectors have achieved state-of-the-art accuracy. However, such models are trained with numerous parameters and their high computational costs and large storage prohibit the deployment to memory and computation resource limited systems. Low-precision neural networks are popular techniques for reducing the computation requirements and memory footprint. Among them, binary weight neural networks (BWNs) are the extreme case which quantizes the float-point into just 1 bit. BWNs are difficult to train and suffer from accuracy deprecation due to the extreme low-bit representation. To address this problem, we propose a knowledge transfer (KT) method to aid the training of BWN using a full-precision teacher network. We built DarkNet- and MobileNet-based binary weight YOLOv2 detectors and conduct experiments on KITTI benchmark for car, pedestrian and cyclist detection. The experimental results show that the proposed method maintains high detection accuracy while reducing the model size of DarkNet-YOLO from 257 MB to 8.8 MB and MobileNet-YOLO from 193 MB to 7.9 MB.


Title: Improving Incremental Planning Performance through Overlapping Replanning and Execution
Key Words: path planning  robots  trajectory control  integrated motion planning  Chekov  trajectory optimization problems  roadmap seed trajectories  motion planning algorithms  control information  incremental planning  Planning  Dynamics  Mobile robots  Trajectory optimization  Collision avoidance 
Abstract: Deployment of motion planning algorithms in practical applications has lagged due to their slow speed in reacting to disturbances. We believe that the best way to address this is to reuse learned planning and control information across queries. In previous work, we introduced Chekov, a reactive, integrated motion planning and execution system that reuses learned information in the form of an enhanced roadmap. We have previously shown how we can use Chekov to formulate trajectory optimization problems that result in superior performance in static environments. In this work, we show how incremental planning can be incorporated into the formulation of optimized trajectories from roadmap seed trajectories. Further, we show how an incremental planner can be adapted to reduce the overhead incurred for replanning when trajectories become invalid during execution.


Title: Multimodal Policy Search using Overlapping Mixtures of Sparse Gaussian Process Prior
Key Words: Bayes methods  Gaussian processes  inference mechanisms  learning (artificial intelligence)  mobile robots  search problems  OMSGPs  optimal policies  reinforcement learning  multimodal policy search algorithm  overlapping mixtures of sparse Gaussian process  Bayesian inference  object grasping  table-sweep tasks  Task analysis  Grasping  Kernel  Gaussian processes  Robots  Inference algorithms  Prediction algorithms 
Abstract: In this paper, we present a novel policy search reinforcement learning algorithm that can deal with multimodality in control policies based on Gaussian processes. Our approach employs Overlapping Mixtures of Gaussian Processes (OMGPs) for a control policy, in which all the GPs in the mixture are global and overlapped in the input space. We first extend the OMGPs by combing sparse pseudo-input GPs as OMSGPs to reduce its computational cost of learning and prediction suitable for policy search. Then, we derive a novel multimodal policy search algorithm based on variational Bayesian inference by placing the OMSGPs as the prior of the multimodal control policy. To validate the effectiveness of our algorithm, we applied it to two typical robotic tasks in simulation: 1) object grasping and 2) table-sweep tasks since they both require the multimodality in the optimal policies. Simulation results demonstrate that our algorithm can efficiently learn multimodal policies even with high dimensional observations.


Title: Data-efficient Learning of Morphology and Controller for a Microrobot
Key Words: Bayes methods  control engineering computing  learning (artificial intelligence)  legged locomotion  microrobots  optimisation  data-efficient learning  robot design  HPC-BBO  hierarchical Bayesian optimization process  morphology configurations  controller learning process  hardware validation  hardware configurations design  6-legged microrobot  Morphology  Optimization  Robots  Bayes methods  Hardware  Process control  Task analysis 
Abstract: Robot design is often a slow and difficult process requiring the iterative construction and testing of prototypes, with the goal of sequentially optimizing the design. For most robots, this process is further complicated by the need, when validating the capabilities of the hardware to solve the desired task, to already have an appropriate controller, which is in turn designed and tuned for the specific hardware. In this paper, we propose a novel approach, HPC-BBO, to efficiently and automatically design hardware configurations, and evaluate them by also automatically tuning the corresponding controller. HPC-BBO is based on a hierarchical Bayesian optimization process which iteratively optimizes morphology configurations (based on the performance of the previous designs during the controller learning process) and subsequently learns the corresponding controllers (exploiting the knowledge collected from optimizing for previous morphologies). Moreover, HPC-BBO can select a “batch” of multiple morphology designs at once, thus parallelizing hardware validation and reducing the number of time-consuming production cycles. We validate HPC-BBO on the design of the morphology and controller for a simulated 6-legged microrobot. Experimental results show that HPC-BBO outperforms multiple competitive baselines, and yields a 360% reduction in production cycles over standard Bayesian optimization, thus reducing the hypothetical manufacturing time of our microrobot from 21 to 4 months.


Title: Generalized Orientation Learning in Robot Task Space
Key Words: control engineering computing  end effectors  humanoid robots  learning (artificial intelligence)  manipulator dynamics  mobile robots  motion control  position control  generalized orientation learning  robot task space  imitation learning  human skills  joint space  end-effector orientation  arbitrary desired points  adapting learned orientation skills  angular velocity  learning Cartesian positions suffices  learning multiple orientation trajectories  kernelized treatment  dynamic movement primitives  Quaternions  Trajectory  Probabilistic logic  Task analysis  Robots  Angular velocity  Transforms 
Abstract: In the context of imitation learning, several approaches have been developed so as to transfer human skills to robots, with demonstrations often represented in Cartesian or joint space. While learning Cartesian positions suffices for many applications, the end-effector orientation is required in many others. However, several crucial issues arising from learning orientations have not been adequately addressed yet. For instance, how can demonstrated orientations be adapted to pass through arbitrary desired points that comprise orientations and angular velocities? In this paper, we propose an approach that is capable of learning multiple orientation trajectories and adapting learned orientation skills to new situations (e.g., via-point and end-point), where both orientation and angular velocity are addressed. Specifically, we introduce a kernelized treatment to alleviate explicit basis functions when learning orientations. Several examples including comparison with the state-of-the-art dynamic movement primitives are provided to verify the effectiveness of our method.


Title: HD Map Change Detection with a Boosted Particle Filter
Key Words: image classification  learning (artificial intelligence)  object detection  particle filtering (numerical methods)  probability  road vehicles  traffic engineering computing  automated driving  landmark readings  probability distribution  HD map change detection  boosted particle filter  change detection algorithm  backend-based stream processing pipeline  floating car data  series-production vehicles  automotive high definition digital map  crowd-based approach  Roads  Global navigation satellite system  Measurement  Visualization  Automobiles  Robot sensing systems  Feature extraction 
Abstract: In this paper, we present a change detection algorithm that can run in real time as part of a backend-based stream processing pipeline. It can process the floating car data collected by series-production vehicles to detect changes in an automotive high definition digital (HD) map used for automated driving. The algorithm uses a particle filter approach with odometry, GNSS and landmark readings to localize the vehicle within the digital map. While all particles together represent the probability distribution for the vehicle's position at a given time, each individual particle also serves as a hypothesis about the vehicle's position. This is used to compute various metrics for how well the current sensor readings match the world model encoded in the HD map. The different metrics are evaluated by a number of weak classifiers that are used as input for a trained Adaboost classifier. The achievable detection rate of a single vehicle is then compared to that of a simple crowd-based approach, where each vehicle votes on whether or not the current section of the road has changed.


Title: Self-Supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment
Key Words: direction-of-arrival estimation  geometry  indoor environment  learning (artificial intelligence)  microphone arrays  mobile robots  geometry features  self-supervision process  ground truth label  pre-collected data  human supervisions  explicit GCC-PHAT features  supervised incremental learning  sound source localization  complex indoor environment  incremental learning framework  mobile robots  human sound source  microphone array  multiple rooms  training data  prediction model  incremental learning scheme  implicit acoustic features  training samples  direction-of-arrival estimation  Feature extraction  Robots  Acoustics  Predictive models  Indoor environment  Microphones  Data models 
Abstract: This paper presents an incremental learning framework for mobile robots localizing the human sound source using a microphone array in a complex indoor environment consisting of multiple rooms. In contrast to conventional approaches that leverage direction-of-arrival (DOA) estimation, the framework allows a robot to accumulate training data and improve the performance of the prediction model over time using an incremental learning scheme. Specifically, we use implicit acoustic features obtained from an auto-encoder together with the geometry features from the map for training. A self-supervision process is developed such that the model ranks the priority of rooms to explore and assigns the ground truth label to the collected data, updating the learned model on-the-fly. The framework does not require pre-collected data and can be directly applied to real-world scenarios without any human supervisions or interventions. In experiments, we demonstrate that the prediction accuracy reaches 67% using about 20 training samples and eventually achieves 90% accuracy within 120 samples, surpassing prior classification-based methods with explicit GCC-PHAT features.


Title: Learning To Grasp Under Uncertainty Using POMDPs
Key Words: grippers  humanoid robots  learning (artificial intelligence)  Markov processes  object detection  recurrent neural nets  robust control  service robots  uncertainty handling  visual sensing  partially observable Markov decision process  grasp policy  deep recurrent neural network  imitation learning  model-based POMDP planning  G3DB object dataset  service robots  far-field sensors  open-loop grasp  tactile sensing  adaptive grasping  robust object grasping strategy  uncertainty handling  Uncertainty  Grippers  Grasping  Planning  Sensors  Shape  Computational modeling 
Abstract: Robust object grasping under uncertainty is an essential capability of service robots. Many existing approaches rely on far-field sensors, such as cameras, to compute a grasp pose and perform open-loop grasp after placing gripper under the pose. This often fails as a result of sensing or environment uncertainty. This paper presents a principled, general and efficient approach to adaptive grasping, using both tactile and visual sensing as feedback. We first model adaptive grasping as a partially observable Markov decision process (POMDP), which handles uncertainty naturally. We solve the POMDP for sampled objects from a set, in order to generate data for learning. Finally, we train a grasp policy, represented as a deep recurrent neural network (RNN), in simulation through imitation learning. By combining model-based POMDP planning and imitation learning, the proposed approach achieves robustness under uncertainty, generalization over many objects, and fast execution. In particular, we show that modeling only a small sample of objects enables us to learn a robust strategy to grasp previously unseen objects of varying shapes and recover from failure over multiple steps. Experiments on the G3DB object dataset in simulation and a smaller object set with a real robot indicate promising results.


Title: Using Geometric Features to Represent Near-Contact Behavior in Robotic Grasping
Key Words: geometry  grippers  learning (artificial intelligence)  kinematic noise  contact points  binary grasp success classifier  hand morphologies  hand-object geometric relationships  near-contact behavior  near-contact stage  feature representations  robotic grasping  geometric features  Measurement  Robots  Grasping  Surface morphology  Geometry  Morphology  Machine learning algorithms 
Abstract: In this paper we define two feature representations for grasping. These representations capture hand-object geometric relationships at the near-contact stage - before the fingers close around the object. Their benefits are: 1) They are stable under noise in both joint and pose variation. 2) They are largely hand and object agnostic, enabling direct comparison across different hand morphologies. 3) Their format makes them suitable for direct application of machine learning techniques developed for images. We validate the representations by: 1) Demonstrating that they can accurately predict the distribution of ε-metric values generated by kinematic noise. I.e., they capture much of the information inherent in contact points and force vectors without the corresponding instabilities. 2) Training a binary grasp success classifier on a real-world data set consisting of 588 grasps.


Title: Offline Policy Iteration Based Reinforcement Learning Controller for Online Robotic Knee Prosthesis Parameter Tuning
Key Words: biomechanics  finite state machines  iterative methods  learning systems  medical robotics  optimal control  prosthetics  offline policy iteration  online robotic knee prosthesis parameter  optimal controller  personalized control  optimal control problems  human-prosthesis system  prototypic robotic prosthesis  approximate policy iteration algorithm  reinforcement learning-based control  near-normal knee kinematics  offline learning  robotic lower limb prosthesis  RL control  control policy  impedance control parameters  prosthesis control parameters  Prosthetics  Knee  Impedance  Kinematics  Reinforcement learning  Legged locomotion 
Abstract: This paper aims to develop an optimal controller that can automatically provide personalized control of robotic knee prosthesis in order to best support gait of individual prosthesis wearers. We introduced a new reinforcement learning (RL) controller for this purpose based on the promising ability of RL controllers to solve optimal control problems through interactions with the environment without requiring an explicit system model. However, collecting data from a human-prosthesis system is expensive and thus the design of a RL controller has to take into account data and time efficiency. We therefore propose an offline policy iteration based reinforcement learning approach. Our solution is built on the finite state machine (FSM) impedance control framework, which is the most used prosthesis control method in commercial and prototypic robotic prosthesis. Under such a framework, we designed an approximate policy iteration algorithm to devise impedance parameter update rules for 12 prosthesis control parameters in order to meet individual users' needs. The goal of the reinforcement learning-based control was to reproduce near-normal knee kinematics during gait. We tested the RL controller obtained from offline learning in real time experiment involving the same able-bodied human subject wearing a robotic lower limb prosthesis. Our results showed that the RL control resulted in good convergent behavior in kinematic states, and the offline learning control policy successfully adjusted the prosthesis control parameters to produce near-normal knee kinematics in 10 updates of the impedance control parameters.


Title: The Importance of Metric Learning for Robotic Vision: Open Set Recognition and Active Learning
Key Words: convolutional neural nets  image classification  learning (artificial intelligence)  robot vision  robotic vision  robotic problems  training set distribution  robotic applications  real-world robotic action  deep metric learning classification system  open set recognition problems  open set active learning approach  active learning problems  deep neural network recognition systems  Training  Robots  Labeling  Semantics  Task analysis  Extraterrestrial measurements 
Abstract: State-of-the-art deep neural network recognition systems are designed for a static and closed world. It is usually assumed that the distribution at test time will be the same as the distribution during training. As a result, classifiers are forced to categorise observations into one out of a set of predefined semantic classes. Robotic problems are dynamic and open world; a robot will likely observe objects that are from outside of the training set distribution. Classifier outputs in robotic applications can lead to real-world robotic action and as such, a practical recognition system should not silently fail by confidently misclassifying novel observations. We show how a deep metric learning classification system can be applied to such open set recognition problems, allowing the classifier to label novel observations as unknown. Further to detecting novel examples, we propose an open set active learning approach that allows a robot to efficiently query a user about unknown observations. Our approach enables a robot to improve its understanding of the true distribution of data in the environment, from a small number of label queries. Experimental results show that our approach significantly outperforms comparable methods in both the open set recognition and active learning problems.


Title: Learning Discriminative Embeddings for Object Recognition on-the-fly
Key Words: image classification  learning (artificial intelligence)  object recognition  lightweight classifier  Computer Vision applications  real-world images datasets  inference time  unseen objects  trained model  Supervised Triplet Loss  separable embeddings  high-end computational resources  CNNs  object recognition on-the-fly  discriminative embeddings  Training  Measurement  Computational modeling  Robots  Computer architecture  Object recognition  Support vector machines 
Abstract: We address the problem of learning to recognize new objects on-the-fly efficiently. When using CNNs, a typical approach for learning new objects is by fine-tuning the model. However, this approach relies on the assumption that the original training set is available and requires high-end computational resources for training the ever-growing dataset efficiently, which can be unfeasible for robots with limited hardware. To overcome these limitations, we propose a new architecture that: 1) Instead of predicting labels, it learns to generate discriminative and separable embeddings of an object's viewpoints by using a Supervised Triplet Loss, which is easier to implement than current smart mining techniques and the trained model can be applied to unseen objects. 2) Infers an object's identity efficiently by utilizing a lightweight classifier in the features embedding space, this keeps the inference time in the order of milliseconds and can be retrained efficiently when new objects are learned. We evaluate our approach on four real-world images datasets used for Robotics and Computer Vision applications: Amazon Robotics Challenge 2017 by MIT-Princeton, T-LESS, ToyBoX, and CORe50 datasets. Code available at [1].


Title: DSNet: Joint Learning for Scene Segmentation and Disparity Estimation
Key Words: feature extraction  image coding  image matching  image segmentation  image sequences  learning (artificial intelligence)  semantic features  deep disparity features  semantic labels  scene segmentation  disparity estimation  scene semantics  optical flow estimation  depth information  dense depth maps  image frames  deep semantic information  disparity feature maps  independent encoding modules  semantic disparity information  multitasking architecture DSNet  ResNet encoding module  Semantics  Estimation  Task analysis  Feature extraction  Optical imaging  Training  Three-dimensional displays 
Abstract: Recently, research works have attempted the joint prediction of scene semantics and optical flow estimation, which demonstrate the mutual improvement between both tasks. Besides, the depth information is also indispensable for the scene understanding, and disparity estimation is necessary for outputting dense depth maps. Such task shares a great similarity with the optical flow estimation since they can all be cast into a problem of capturing the difference at a location of two image frames. However, as far as we know, currently there are few networks for the joint learning of semantic and disparity. Moreover, since deep semantic information and disparity feature maps can learn from each other, we find it unnecessary with two independent encoding modules to separately extract semantic and disparity features. Therefore, we propose a unified multi-tasking architecture DSNet, for the simultaneous estimation of semantic and disparity information. In our model, semantic features, extracted by the encoding module ResNet from the left and right images, are used to obtain the deep disparity features via a novel matching module which performs pixel-to-pixel matching. In addition, we also use the disparity map to perform warp operation on deep features of the right image to deal with the problem of lacking of semantic labels. The effectiveness of our method is demonstrated by extensive experiments.


Title: Unsupervised Learning of Assistive Camera Views by an Aerial Co-robot in Augmented Reality Multitasking Environments
Key Words: augmented reality  autonomous aerial vehicles  cameras  computer displays  mobile robots  unsupervised learning  unsupervised learning  assistive camera views  augmented reality multitasking environments  assistive aerial robot  task domain  head motion  anisotropic spherical sensor  expectation maximization solver  Gaussians  dynamic coverage control law  augmented reality display  human operator  assistive robot  reflex time  task completion time  aerial co-robot  Visualization  Task analysis  Cameras  Robot vision systems 
Abstract: This paper presents a novel method by which an assistive aerial robot can learn the relevant camera views within a task domain through tracking the head motions of a human collaborator. The human's visual field is modeled as an anisotropic spherical sensor, which decays in acuity towards the periphery, and is integrated in time throughout the domain. This data is resampled and fed into an expectation maximization solver in order to estimate the environment's visual interest as a mixture of Gaussians. A dynamic coverage control law directs the robot to capture camera views of the peaks of these Gaussians which is broadcast to an augmented reality display worn by the human operator. An experimental study is presented that assesses the influence of the assistive robot on reflex time, head motion, and task completion time.


Title: Multi-Vehicle Close Enough Orienteering Problem with Bézier Curves for Multi-Rotor Aerial Vehicles
Key Words: aircraft control  autonomous aerial vehicles  remotely operated vehicles  unsupervised learning  travel cost  travel budget  Bézier curves  multivehicle CEOP  multirotor aerial vehicles  maximal velocity  acceleration limits  rewarding target locations  multivehicle close enough orienteering problem  surveillance planning  unsupervised learning  Trajectory  Acceleration  Adaptive arrays  Planning  Unsupervised learning  Optimization  Surveillance 
Abstract: This paper introduces the Close Enough Orienteering Problem (CEOP) for planning missions with multi-rotor aerial vehicles considering their maximal velocity and acceleration limits. The addressed problem stands to select the most rewarding target locations and sequence to visit them in the given limited travel budget. The reward is collected within a non-zero range from a particular target location that allows saving the travel cost, and thus collect more rewards. Hence, we are searching for the fastest trajectories to collect the most valuable rewards such that the motion constraints are not violated, and the travel budget is satisfied. We leverage on existing trajectory parametrization based on Bézier curves recently deployed in surveillance planning using unsupervised learning, and we propose to employ the learning in a solution of the introduced multi-vehicle CEOP. Feasibility of the proposed approach is supported by empirical evaluation and experimental deployment using multi-rotor vehicles.


Title: Reinforcement Learning on Variable Impedance Controller for High-Precision Robotic Assembly
Key Words: assembling  control engineering computing  force control  industrial robots  learning (artificial intelligence)  neural net architecture  position control  robotic assembly  wheels  high-precision robotic assembly  precise robotic manipulation skills  industrial settings  reinforcement learning methods  RL  perceived forces  high-precision tasks  proper operational space force controller  open-source Siemens Robot Learning Challenge  precise force-controlled behavior  delicate force-controlled behavior  variable impedance controller  Task analysis  Gears  Aerospace electronics  Robots  Reinforcement learning  Trajectory  Neural networks 
Abstract: Precise robotic manipulation skills are desirable in many industrial settings, reinforcement learning (RL) methods hold the promise of acquiring these skills autonomously. In this paper, we explicitly consider incorporating operational space force/torque information into reinforcement learning; this is motivated by humans heuristically mapping perceived forces to control actions, which results in completing high-precision tasks in a fairly easy manner. Our approach combines RL with force/torque information by incorporating a proper operational space force controller; where we also exploit different ablations on processing this information. Moreover, we propose a neural network architecture that generalizes to reasonable variations of the environment. We evaluate our method on the open-source Siemens Robot Learning Challenge, which requires precise and delicate force-controlled behavior to assemble a tight-fit gear wheel set.


Title: Every Hop is an Opportunity: Quickly Classifying and Adapting to Terrain During Targeted Hopping
Key Words: control engineering computing  learning (artificial intelligence)  learning systems  mobile robots  pattern classification  robot programming  terrain properties  terrain-informed learning  low shot learning  targeted hopping  task-relevant objectives  hopping robot  control strategies  jumping task  closed-loop jumping  real-world jumping data  terrain classification  online learning experiments  Task analysis  Optimal control  Solids  Force  Robot sensing systems  Solid modeling 
Abstract: Practical use of robots in diverse domains requires programming for, or adapting to, each domain and its unique characteristics. Failure to do so compromises the ability of the robot to achieve task-relevant objectives. Here we describe how the learned terrain reaction force profiles of a hopping robot serve the additional objectives of classifying terrain and quickly learning control strategies to accomplish a jumping task on novel terrain. We show that the reaction forces experienced during closed-loop jumping are sufficient to discriminate between three different terrain types (granular, trampoline, and rigid) when using the learned models as discriminators. Building on this, we show that applying the classification to unknown terrain types leads to faster task completion, where the task objective is to meet a specific jump height. The classification experiments, utilizing real-world jumping data, achieve 95% prediction accuracy. The online learning experiments leverage simulation as there is more control over the terrain properties. Terrain-informed learning achieves the target hop heights more than 2x faster than without terrain knowledge when the prediction is correct, and 1.5x faster when the prediction is incorrect. Thus, applying the closest approximately known terrain knowledge facilitates low shot learning when hopping on unknown terrain.


Title: Semiparametrical Gaussian Processes Learning of Forward Dynamical Models for Navigating in a Circular Maze
Key Words: Gaussian processes  learning (artificial intelligence)  mobile robots  optimisation  regression analysis  robot dynamics  trajectory control  Gaussian process regression  semiparametrical Gaussian processes learning  robot learning  ball trajectories  physics first principles  motion dynamics  dry friction  nonlinear effects  degrees of freedom  circular maze environment  forward dynamical models  Computational modeling  Heuristic algorithms  Servomotors  Navigation  Cameras  Gaussian processes  Dynamics 
Abstract: This paper presents a problem of model learning for the purpose of learning how to navigate a ball to a goal state in a circular maze environment with two degrees of freedom. The motion of the ball in the maze environment is influenced by several non-linear effects such as dry friction and contacts, which are difficult to model physically. We propose a semiparametric model to estimate the motion dynamics of the ball based on Gaussian Process Regression equipped with basis functions obtained from physics first principles. The accuracy of this semiparametric model is shown not only in estimation but also in prediction at n-steps ahead and its compared with standard algorithms for model learning. The learned model is then used in a trajectory optimization algorithm to compute ball trajectories. We propose the system presented in the paper as a benchmark problem for reinforcement and robot learning, for its interesting and challenging dynamics and its relative ease of reproducibility.


Title: Semantic Predictive Control for Explainable and Efficient Policy Learning
Key Words: image motion analysis  image representation  learning (artificial intelligence)  optimisation  predictive control  visual explanation  policy decisions  SPC  future semantic segmentation  multiscale feature maps  guidance model  multiple simulation environments  model-based reinforcement  data efficiency  short time horizons  human-level performance  complex environments  driving policy learning framework  feature representations  sampling-based optimization  semantic predictive control framework  Semantics  Predictive models  Feature extraction  Visualization  Task analysis  Predictive control  Optimization 
Abstract: Visual anticipation of ego and object motion over a short time horizons is a key feature of human-level performance in complex environments. We propose a driving policy learning framework that predicts feature representations of future visual inputs; our predictive model infers not only future events but also semantics, which provide a visual explanation of policy decisions. Our Semantic Predictive Control (SPC) framework predicts future semantic segmentation and events by aggregating multi-scale feature maps. A guidance model assists action selection and enables efficient sampling-based optimization. Experiments on multiple simulation environments show that networks which implement SPC can outperform existing model-based reinforcement learning algorithms in terms of data efficiency and total rewards while providing clear explanations for the policy's behavior.


Title: Adaptive Variance for Changing Sparse-Reward Environments
Key Words: Gaussian processes  learning (artificial intelligence)  optimisation  robot programming  adaptive variance  sparse-reward environments  optimal exploration  Gaussian-parameterized policy  robots  Robots  Task analysis  Reinforcement learning  Training  Adaptation models  Friction  Navigation 
Abstract: Robots that are trained to perform a task in a fixed environment often fail when facing unexpected changes to the environment due to a lack of exploration. We propose a principled way to adapt the policy for better exploration in changing sparse-reward environments. Unlike previous works which explicitly model environmental changes, we analyze the relationship between the value function and the optimal exploration for a Gaussian-parameterized policy and show that our theory leads to an effective strategy for adjusting the variance of the policy, enabling fast adapt to changes in a variety of sparse-reward environments.


Title: Combining Physical Simulators and Object-Based Networks for Control
Key Words: learning (artificial intelligence)  mobile robots  neural nets  object-based neural network  interacting objects  complex control tasks  object shapes  physical simulators  physics engine  robot planning  real-world control problems  complex contact dynamics  hybrid dynamics model  simulator-augmented interaction networks  Physics  Engines  Analytical models  Task analysis  Robots  Predictive models  Mathematical model 
Abstract: Physics engines play an important role in robot planning and control; however, many real-world control problems involve complex contact dynamics that cannot be characterized analytically. Most physics engines therefore employ approximations that lead to a loss in precision. In this paper, we propose a hybrid dynamics model, simulator-augmented interaction networks (SAIN), combining a physics engine with an object-based neural network for dynamics modeling. Compared with existing models that are purely analytical or purely data-driven, our hybrid model captures the dynamics of interacting objects in a more accurate and data-efficient manner. Experiments both in simulation and on a real robot suggest that it also leads to better performance when used in complex control tasks. Finally, we show that our model generalizes to novel environments with varying object shapes and materials.


Title: Using Data-Driven Domain Randomization to Transfer Robust Control Policies to Mobile Robots
Key Words: automobiles  collision avoidance  mobile robots  motion control  probability  robust control  stochastic processes  trajectory control  deep stochastic dynamics model  collision avoidance  1/5 scale agile ground vehicle  robust control policies  vehicle data  collision probability  trajectory tracking accuracy  stochasticity  simple analytic car model  high quality stochastic dynamics model  robot motion trajectories  mobile robots  data-driven domain randomization  Stochastic processes  Data models  Vehicle dynamics  Optimization  Robots  Uncertainty  Computational modeling 
Abstract: This work develops a technique for using robot motion trajectories to create a high quality stochastic dynamics model that is then leveraged in simulation to train control policies with associated performance guarantees. We demonstrate the idea by collecting dynamics data from a 1/5 scale agile ground vehicle, fitting a stochastic dynamics model, and training a policy in simulation to drive around an oval track at up to 6.5 m/s while avoiding obstacles. We show that the control policy can be transferred back to the real vehicle with little loss in predicted performance. We compare this to an approach that uses a simple analytic car model to train a policy in simulation and show that using a model with stochasticity learned from data leads to higher performance in terms of trajectory tracking accuracy and collision probability. Furthermore, we show empirically that simulation-derived performance guarantees transfer to the actual vehicle when executing a policy optimized using a deep stochastic dynamics model fit to vehicle data.


Title: Depth Completion with Deep Geometry and Context Guidance
Key Words: computer vision  convolutional neural nets  feature extraction  learning (artificial intelligence)  object detection  bilateral weight  deep geometry  context guidance  geometry network  context network  single encoder-decoder network  initial propagated depth map  slanted surfaces  convolutional neural network  CNN-based depth completions  local feature extraction  global feature extraction  Three-dimensional displays  Geometry  Feature extraction  Image edge detection  Reliability  Laser radar  Convolution 
Abstract: In this paper, we present an end-to-end convolutional neural network (CNN) for depth completion. Our network consists of a geometry network and a context network. The geometry network, a single encoder-decoder network, learns to optimize a multi-task loss to generate an initial propagated depth map and a surface normal. The complementary outputs allow it to correctly propagate initial sparse depth points in slanted surfaces. The context network extracts a local and a global feature of an image to compute a bilateral weight, which enables it to preserve edges and fine details in the depth maps. At the end, a final output is produced by multiplying the initially propagated depth map with the bilateral weight. In order to validate the effectiveness and the robustness of our network, we performed extensive ablation studies and compared the results against state-of-the-art CNN-based depth completions, where we showed promising results on various scenes.


Title: Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera
Key Words: cameras  image colour analysis  image fusion  image sequences  optical radar  regression analysis  robot vision  semidense annotations  KITTI depth completion benchmark  self-supervised sparse-to-dense  self-supervised depth completion  dense depth image  sparse depth measurements  irregularly spaced pattern  multiple sensor modalities  dense level ground truth depth  pixel-level ground truth depth  self-supervised training framework  sparse depth images  color images  Laser radar  Training  Color  Robot sensing systems  Three-dimensional displays  Convolution  Extraterrestrial measurements 
Abstract: Depth completion, the technique of estimating a dense depth image from sparse depth measurements, has a variety of applications in robotics and autonomous driving. However, depth completion faces 3 main challenges: the irregularly spaced pattern in the sparse depth input, the difficulty in handling multiple sensor modalities (when color images are available), as well as the lack of dense, pixel-level ground truth depth labels for training. In this work, we address all these challenges. Specifically, we develop a deep regression model to learn a direct mapping from sparse depth (and color images) input to dense depth prediction. We also propose a self-supervised training framework that requires only sequences of color and sparse depth images, without the need for dense depth labels. Our experiments demonstrate that the self-supervised framework outperforms a number of existing solutions trained with semi-dense annotations. Furthermore, when trained with semi-dense annotations, our network attains state-of-the-art accuracy and is the winning approach on the KITTI depth completion benchmark at the time of submission.


Title: Multi-Modal Generative Models for Learning Epistemic Active Sensing
Key Words: control engineering computing  learning (artificial intelligence)  mobile robots  multi-agent systems  neural nets  statistical analysis  multimodal deep generative models  coordinated heterogeneous multiagent active sensing  joint latent representation  epistemic active sensing behavior  multimodal variational auto encoder  sensor modalities  multiagent deep reinforcement learning setup  direct reward signal  evidence lower bound  Robot sensing systems  Robot kinematics  Training  Feature extraction  Reinforcement learning 
Abstract: We present a novel approach of multi-modal deep generative models and apply this to coordinated heterogeneous multi-agent active sensing. A major approach to achieve this objective is to train a multi-modal variational Auto Encoder (M2VAE) that integrates the information of different sensor modalities into a joint latent representation. Furthermore, we derive an objective from the M2VAE that enables the maximization of the evidence lower bound via selection of sensor modalities. Using this approach as a direct reward signal to a multi-modal and multi-agent deep reinforcement learning setup leads intuitively to an epistemic active sensing behavior that coordinately resolves the ambiguity of observations.


Title: Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals
Key Words: gradient methods  learning (artificial intelligence)  mobile robots  multi-agent systems  multi-robot systems  navigation  path planning  robot programming  interaction-aware multiagent reinforcement learning  mobile agents  individual goals  optimal policy  decentralized learning  mobile robot navigation  policy gradient algorithms  nonstationary policies  curriculum-based strategy  interactive policy learning  Training  Robots  Games  Markov processes  Reinforcement learning  Navigation  Autonomous vehicles 
Abstract: In a multi-agent setting, the optimal policy of a single agent is largely dependent on the behavior of other agents. We investigate the problem of multi-agent reinforcement learning, focusing on decentralized learning in non-stationary domains for mobile robot navigation. We identify a cause for the difficulty in training non-stationary policies: mutual adaptation to sub-optimal behaviors, and we use this to motivate a curriculum-based strategy for learning interactive policies. The curriculum has two stages. First, the agent leverages policy gradient algorithms to learn a policy that is capable of achieving multiple goals. Second, the agent learns a modifier policy to learn how to interact with other agents in a multi-agent setting. We evaluated our approach on both an autonomous driving lane-change domain and a robot navigation domain.


Title: Active Perception in Adversarial Scenarios using Maximum Entropy Deep Reinforcement Learning
Key Words: belief networks  entropy  learning (artificial intelligence)  multi-agent systems  neural nets  planning (artificial intelligence)  stochastic processes  partial observability  adversary agent  autonomous agent  belief space planning  generative adversary modeling  maximum entropy reinforcement learning  stochastic belief space policy  unmodeled adversarial strategies  maximum entropy deep reinforcement learning  active perception problem  potentially adversarial behaviors  uncertainty modeling  standard chance-constraint partially observable Markov decision  Uncertainty  Games  Autonomous agents  Reinforcement learning  Nash equilibrium  Planning  Adaptation models 
Abstract: We pose an active perception problem where an autonomous agent actively interacts with a second agent with potentially adversarial behaviors. Given the uncertainty in the intent of the other agent, the objective is to collect further evidence to help discriminate potential threats. The main technical challenges are the partial observability of the agent intent, the adversary modeling, and the corresponding uncertainty modeling. Note that an adversary agent may act to mislead the autonomous agent by using a deceptive strategy that is learned from past experiences. We propose an approach that combines belief space planning, generative adversary modeling, and maximum entropy reinforcement learning to obtain a stochastic belief space policy. By accounting for various adversarial behaviors in the simulation framework and minimizing the predictability of the autonomous agent's action, the resulting policy is more robust to unmodeled adversarial strategies. This improved robustness is empirically shown against an adversary that adapts to and exploits the autonomous agent's policy when compared with a standard Chance-Constraint Partially Observable Markov Decision Process robust approach.


Title: Learning Object Localization and 6D Pose Estimation from Simulation and Weakly Labeled Real Images
Key Words: image annotation  image classification  image segmentation  object detection  pose estimation  robot vision  unsupervised learning  6D pose estimation  computer vision models  object localization  multiple domain classifiers  synthetic images  object poses  time-consuming annotations  occluded scenes  cluttered scenes  weak object detector  deep learning approaches  cluttered environments  robust robotic grasping  Feature extraction  Pose estimation  Training  Robots  Heating systems  Solid modeling  Task analysis 
Abstract: Accurate pose estimation is often a requirement for robust robotic grasping and manipulation of objects placed in cluttered, tight environments, such as a shelf with multiple objects. When deep learning approaches are employed to perform this task, they typically require a large amount of training data. However, obtaining precise 6 degrees of freedom for ground-truth can be prohibitively expensive. This work therefore proposes an architecture and a training process to solve this issue. More precisely, we present a weak object detector that enables localizing objects and estimating their 6D poses in cluttered and occluded scenes. To minimize the human labor required for annotations, the proposed detector is trained with a combination of synthetic and a few weakly annotated real images (as little as 10 images per object), for which a human provides only a list of objects present in each image (no time-consuming annotations, such as bounding boxes, segmentation masks and object poses). To close the gap between real and synthetic images, we use multiple domain classifiers trained adversarially. During the inference phase, the resulting class-specific heatmaps of the weak detector are used to guide the search of 6D poses of objects. Our proposed approach is evaluated on several publicly available datasets for pose estimation. We also evaluated our model on classification and localization in unsupervised and semi-supervised settings. The results clearly indicate that this approach could provide an efficient way toward fully automating the training process of computer vision models used in robotics.


Title: Learning Pose Estimation for High-Precision Robotic Assembly Using Simulated Depth Images
Key Words: CAD  cameras  convolutional neural nets  end effectors  industrial manipulators  learning (artificial intelligence)  pose estimation  production engineering computing  robot vision  robotic assembly  3D CAD models  depth camera  deep convolutional neural networks  industrial robotic assembly tasks  depth images  pose estimation learning  two-stage pose estimation procedure  end-effector  Conferences  Automation 
Abstract: Most of industrial robotic assembly tasks today require fixed initial conditions for successful assembly. These constraints induce high production costs and low adaptability to new tasks. In this work we aim towards flexible and adaptable robotic assembly by using 3D CAD models for all parts to be assembled. We focus on a generic assembly task - the Siemens Innovation Challenge - in which a robot needs to assemble a gear-like mechanism with high precision into an operating system. To obtain the millimeter-accuracy required for this task and industrial settings alike, we use a depth camera mounted near the robot's end-effector. We present a high-accuracy two-stage pose estimation procedure based on deep convolutional neural networks, which includes detection, pose estimation, refinement, and handling of near- and full symmetries of parts. The networks are trained on simulated depth images with means to ensure successful transfer to the real robot. We obtain an average pose estimation error of 2.16 millimeters and 0.64 degree leading to 91% success rate for robotic assembly of randomly distributed parts. To the best of our knowledge, this is the first time that the Siemens Innovation Challenge is fully addressed, with all the parts assembled with high success rates.


Title: Learning Monocular Visual Odometry through Geometry-Aware Curriculum Learning
Key Words: distance measurement  image sequences  learning (artificial intelligence)  pose estimation  recurrent neural nets  regression analysis  windowed composition layer  CL-VO  learning monocular visual odometry  optical flow network  geometry-aware objective function  complex geometry problems  geometry-aware Curriculum Learning  Training  Geometry  Cameras  Task analysis  Estimation  Feature extraction  Optical fiber networks 
Abstract: Inspired by the cognitive process of humans and animals, Curriculum Learning (CL) trains a model by gradually increasing the difficulty of the training data. In this paper, we study whether CL can be applied to complex geometry problems like estimating monocular Visual Odometry (VO). Unlike existing CL approaches, we present a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training. To this end, we propose a novel geometry-aware objective function by jointly optimizing relative and composite transformations over small windows via bounded pose regression loss. A cascade optical flow network followed by recurrent network with a differentiable windowed composition layer, termed CL-VO, is devised to learn the proposed objective. Evaluation on three real-world datasets shows superior performance of CL-VO over state-of-the-art feature-based and learning-based VO.


Title: Leveraging Contact Forces for Learning to Grasp
Key Words: grippers  learning (artificial intelligence)  manipulators  grasp acquisition  model-free deep reinforcement learning  control policies  contact sensing  robust grasping  multifingered hand  complex finger coordination  learned policies  grasping policies  contact feedback  open problem  robotics research  noisy observations  sensor feedback  visual feedback  contact forces  Grasping  Robot sensing systems  Uncertainty  Visualization  Grippers 
Abstract: Grasping objects under uncertainty remains an open problem in robotics research. This uncertainty is often due to noisy or partial observations of the object pose or shape. To enable a robot to react appropriately to unforeseen effects, it is crucial that it continuously takes sensor feedback into account. While visual feedback is important for inferring a grasp pose and reaching for an object, contact feedback offers valuable information during manipulation and grasp acquisition. In this paper, we use model-free deep reinforcement learning to synthesize control policies that exploit contact sensing to generate robust grasping under uncertainty. We demonstrate our approach on a multi-fingered hand that exhibits more complex finger coordination than the commonly used two-fingered grippers. We conduct extensive experiments in order to assess the performance of the learned policies, with and without contact sensing. While it is possible to learn grasping policies without contact sensing, our results suggest that contact feedback allows for a significant improvement of grasping robustness under object pose uncertainty and for objects with a complex shape.


Title: Learning Latent Space Dynamics for Tactile Servoing
Key Words: dexterous manipulators  feedback  haptic interfaces  learning (artificial intelligence)  mobile robots  tactile sensors  tactile servoing  tactile sensing information  tactile skin geometry  tactile finger  dexterous robotic manipulation  tactile feedback capability  latent space dynamics learning  contact point tracking  manifold learning  Robot sensing systems  Skin  Aerospace electronics  Two dimensional displays  Three-dimensional displays 
Abstract: To achieve a dexterous robotic manipulation, we need to endow our robot with tactile feedback capability, i.e. the ability to drive action based on tactile sensing. In this paper, we specifically address the challenge of tactile servoing, i.e. given the current tactile sensing and a target/goal tactile sensing - memorized from a successful task execution in the past - what is the action that will bring the current tactile sensing to move closer towards the target tactile sensing at the next time step. We develop a data-driven approach to acquire a dynamics model for tactile servoing by learning from demonstration. Moreover, our method represents the tactile sensing information as to lie on a surface - or a 2D manifold - and perform a manifold learning, making it applicable to any tactile skin geometry. We evaluate our method on a contact point tracking task using a robot equipped with a tactile finger.


Title: PointNetGPD: Detecting Grasp Configurations from Point Sets
Key Words: computational geometry  feature extraction  grippers  image colour analysis  learning (artificial intelligence)  PointNetGPD  end-to-end grasp evaluation  object grasping  3D point cloud  grasp configuration detection  point sets  robot grasp configurations  complex geometric structure  gripper  3D geometry information  deep neural network  RGB-D camera  Three-dimensional displays  Robot sensing systems  Measurement  Grippers  Grasping  Solid modeling  Geometry 
Abstract: In this paper, we propose an end-to-end grasp evaluation model to address the challenging problem of localizing robot grasp configurations directly from the point cloud. Compared to recent grasp evaluation metrics that are based on handcrafted depth features and a convolutional neural network (CNN), our proposed PointNetGPD is lightweight and can directly process the 3D point cloud that locates within the gripper for grasp evaluation. Taking the raw point cloud as input, our proposed grasp evaluation network can capture the complex geometric structure of the contact area between the gripper and the object even if the point cloud is very sparse. To further improve our proposed model, we generate a large-scale grasp dataset with 350k real point cloud and grasps with the YCB object set for training. The performance of the proposed model is quantitatively measured both in simulation and on robotic hardware. Experiments on object grasping and clutter removal show that our proposed model generalizes well to novel objects and outperforms state-of-the-art methods. Code and video are available at https://lianghongzhuo.github.io/PointNetGPD.


Title: Learning Deep Visuomotor Policies for Dexterous Hand Manipulation
Key Words: dexterous manipulators  learning (artificial intelligence)  tactile sensors  touch sensing information  expert demonstration trajectories  high dimensional visual observations  manipulation tasks  imitation learning  on-board sensors  tactile sensors  external tracking  on-board sensing capabilities  in-hand manipulation  multifingered dexterous hands  dexterous hand manipulation  deep visuomotor policies  Task analysis  Visualization  Training  Tactile sensors 
Abstract: Multi-fingered dexterous hands are versatile and capable of acquiring a diverse set of skills such as grasping, in-hand manipulation, and tool use. To fully utilize their versatility in real-world scenarios, we require algorithms and policies that can control them using on-board sensing capabilities, without relying on external tracking or motion capture systems. Cameras and tactile sensors are the most widely used on-board sensors that do not require instrumentation of the world. In this work, we demonstrate an imitation learning based approach to train deep visuomotor policies for a variety of manipulation tasks with a simulated five fingered dexterous hand. These policies directly control the hand using high dimensional visual observations of the world and propreoceptive observations from the robot, and can be trained efficiently with a few hundred expert demonstration trajectories. We also find that using touch sensing information enables faster learning and better asymptotic performance for tasks with high degree of occlusions. Video demonstration of our results are available at: https://sites.google.com/view/hand-vil/.


Title: Learning to Identify Object Instances by Touch: Tactile Recognition via Multimodal Matching
Key Words: image recognition  object recognition  tactile sensors  visual modality  global observation  robotic manipulation  visual object identification  touch-based instance recognition  multimodal recognition  visual observation  tactile observation  multimodal instance recognition problem  GelSight touch sensors  autonomous data collection procedure  tactile observations  tactile recognition  robotic perception  Visualization  Cameras  Grippers  Tactile sensors  Data collection 
Abstract: Much of the literature on robotic perception focuses on the visual modality. Vision provides a global observation of a scene, making it broadly useful. However, in the domain of robotic manipulation, vision alone can sometimes prove inadequate: in the presence of occlusions or poor lighting, visual object identification might be difficult. The sense of touch can provide robots with an alternative mechanism for recognizing objects. In this paper, we study the problem of touch-based instance recognition. We propose a novel framing of the problem as multi-modal recognition: the goal of our system is to recognize, given a visual and tactile observation, whether or not these observations correspond to the same object. To our knowledge, our work is the first to address this type of multi-modal instance recognition problem on such a large-scale with our analysis spanning 98 different objects. We employ a robot equipped with two GelSight touch sensors, one on each finger, and a self-supervised, autonomous data collection procedure to collect a dataset of tactile observations and images. Our experimental results show that it is possible to accurately recognize object instances by touch alone, including instances of novel objects that were never seen during training. Our learned model outperforms other methods on this complex task, including that of human volunteers.


Title: Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost
Key Words: control engineering computing  dexterous manipulators  learning (artificial intelligence)  neural nets  robot programming  deep reinforcement learning  multifingered hands  contact-rich manipulation behavior  model-free deep RL algorithms  complex multifingered manipulation skills  direct deep RL training  model-based control  dexterous manipulation  dexterous multifingered robotic hands  general-purpose robotic manipulators  autonomous control  complex intermittent contact interactions  Task analysis  Valves  Acceleration  Reinforcement learning  Robot sensing systems  Hardware 
Abstract: Dexterous multi-fingered robotic hands can perform a wide range of manipulation skills, making them an appealing component for general-purpose robotic manipulators. However, such hands pose a major challenge for autonomous control, due to the high dimensionality of their configuration space and complex intermittent contact interactions. In this work, we propose deep reinforcement learning (deep RL) as a scalable solution for learning complex, contact rich behaviors with multi-fingered hands. Deep RL provides an end-to-end approach to directly map sensor readings to actions, without the need for task specific models or policy classes. We show that contact-rich manipulation behavior with multi-fingered hands can be learned by directly training with model-free deep RL algorithms in the real world, with minimal additional assumption and without the aid of simulation. We learn to perform a variety of tasks on two different low-cost hardware platforms entirely from scratch, and further study how the learning can be accelerated by using a small number of human demonstrations. Our experiments demonstrate that complex multi-fingered manipulation skills can be learned in the real world in about 4-7 hours for most tasks, and that demonstrations can decrease this to 2-3 hours, indicating that direct deep RL training in the real world is a viable and practical alternative to simulation and model-based control. https:// sites.google.com/view/deeprl-handmanipulation.


Title: Robotics Education and Research at Scale: A Remotely Accessible Robotics Development Platform
Key Words: control engineering computing  educational robots  laboratories  learning (artificial intelligence)  mobile robots  research and development  telerobotics  remotely accessible robotics development platform  KUKA Robot Learning Lab  industrial lightweight robots  Service robots  Robot sensing systems  Robot learning  Mobile robots  Collision avoidance  Hardware 
Abstract: This paper introduces the KUKA Robot Learning Lab at KIT - a remotely accessible robotics testbed. The motivation behind the laboratory is to make state-of-the-art industrial lightweight robots more accessible for education and research. Such expensive hardware is usually not available to students or less privileged researchers to conduct experiments. This paper describes the design and operation of the Robot Learning Lab and discusses the challenges that one faces when making experimental robot cells remotely accessible. Especially safety and security must be ensured, while giving users as much freedom as possible when developing programs to control the robots. A fully automated and efficient processing pipeline for experiments makes the lab suitable for a large amount of users and allows a high usage rate of the robots.


Title: Interactive Open-Ended Object, Affordance and Grasp Learning for Robotic Manipulation
Key Words: dexterous manipulators  end effectors  humanoid robots  learning (artificial intelligence)  object recognition  position control  robot vision  service robots  human-centric environments  end-effector positions  affordance category  grasp configuration  Bayesian approach  learning recognition  object categories  instance-based approach  robotic manipulation  service robots  object perception  grasp affordances  training data  batch learning  end-effector orientations  Task analysis  Three-dimensional displays  Education  Object detection  Object recognition  Service robots 
Abstract: Service robots are expected to autonomously and efficiently work in human-centric environments. For this type of robots, object perception and manipulation are challenging tasks due to need for accurate and real-time response. This paper presents an interactive open-ended learning approach to recognize multiple objects and their grasp affordances concurrently. This is an important contribution in the field of service robots since no matter how extensive the training data used for batch learning, a robot might always be confronted with an unknown object when operating in human-centric environments. The paper describes the system architecture and the learning and recognition capabilities. Grasp learning associates grasp configurations (i.e., end-effector positions and orientations) to grasp affordance categories. The grasp affordance category and the grasp configuration are taught through verbal and kinesthetic teaching, respectively. A Bayesian approach is adopted for learning and recognition of object categories and an instance-based approach is used for learning and recognition of affordance categories. An extensive set of experiments has been performed to assess the performance of the proposed approach regarding recognition accuracy, scalability and grasp success rate on challenging datasets and real-world scenarios.


Title: Dynamic Primitives in Human Manipulation of Non-Rigid Objects
Key Words: biomechanics  human-robot interaction  learning systems  manipulator dynamics  motion control  optimisation  pendulums  position control  sloshing  human manipulation  nonrigid objects  liquid sloshing  horizontal line  virtual environment  human subjects  robotic manipulandum  residual oscillations  humans simplified control  human movements  continuous optimization-based control  control model  flexible objects  motion profile  human profiles  robot control  input shaping model  cart-and-pendulum system  Task analysis  Mathematical model  Oscillators  Trajectory  Robots  Predictive models  Computational modeling 
Abstract: This study examined strategies humans chose to manipulate an object with complex (nonlinear, underactuated) dynamics, such as liquid sloshing in a cup of coffee. The problem was simplified to the well-known cart-and-pendulum system moving on a horizontal line. This model was implemented in a virtual environment and human subjects manipulated the object via a robotic manipulandum. The task was to maneuver the system from rest to arrive at a target position such that no residual oscillations of the pendulum bob remained. Our goal was to test whether humans simplified control by employing dynamic primitives, specifically submovements. Experimental velocity profiles of the human movements were compared to those predicted by three different control models. Two models used continuous optimization-based control, the third control model was based on Input Shaping. Input Shaping is a method for controlling flexible objects by convolving a motion profile with impulses of appropriate amplitude and timing. To evaluate whether humans used Input Shaping, we decomposed the velocity profiles recorded from humans into submovements, as proxies for the convolved impulses. Comparing the motion profiles from the 3 models with the experimentally measured human profiles showed superior performance of the Input Shaping model. These initial results are consistent with our hypothesis that combining dynamic primitives, submovements, is a competent description of human performance and may provide a simpler alternative to computationally complex optimization-based methods of robot control.


Title: Improving Haptic Adjective Recognition with Unsupervised Feature Learning
Key Words: feature extraction  image classification  iterative methods  unsupervised learning  unsupervised feature learning  densely innervated skin  haptics researchers  haptic intelligence  concrete tasks  object recognition  feature learning methods  haptic adjectives  diverse interactions  abstract binary classification tasks  spatio-temporal hierarchical matching pursuit  haptic adjective recognition  Haptic interfaces  Feature extraction  Task analysis  Dictionaries  Matching pursuit algorithms  Robot sensing systems 
Abstract: Humans can form an impression of how a new object feels simply by touching its surfaces with the densely innervated skin of the fingertips. Many haptics researchers have recently been working to endow robots with similar levels of haptic intelligence, but these efforts almost always employ hand-crafted features, which are brittle, and concrete tasks, such as object recognition. We applied unsupervised feature learning methods, specifically K-SVD and Spatio-Temporal Hierarchical Matching Pursuit (ST-HMP), to rich multi-modal haptic data from a diverse dataset. We then tested the learned features on 19 more abstract binary classification tasks that center on haptic adjectives such as smooth and squishy. The learned features proved superior to traditional hand-crafted features by a large margin, almost doubling the average F1 score across all adjectives. Additionally, particular exploratory procedures (EPs) and sensor channels were found to support perception of certain haptic adjectives, underlining the need for diverse interactions and multi-modal haptic data.


Title: Adaptive Probabilistic Vehicle Trajectory Prediction Through Physically Feasible Bayesian Recurrent Neural Network
Key Words: Bayes methods  gradient methods  learning (artificial intelligence)  probability  recurrent neural nets  stochastic processes  traffic engineering computing  Bayesian recurrent neural network  prediction horizon  target human driver  naturalistic car following data  multimodal stochastic feedback gain  particle-filter-based parameter adaptation algorithm  adopted gradient-based training method  embedded physical model  trajectory distribution  Bayesian-neural-network-based policy model  Bayesian recurrent neural network model  driving policy  predicted distribution  physical feasibility  long-term trajectory prediction  autonomous driving  robust safety  adaptive probabilistic vehicle trajectory prediction  Trajectory  Adaptation models  Probabilistic logic  Bayes methods  Vehicle dynamics  Vehicles  Predictive models 
Abstract: Probabilistic vehicle trajectory prediction is essential for robust safety of autonomous driving. Current methods for long-term trajectory prediction cannot guarantee the physical feasibility of predicted distribution. Moreover, their models cannot adapt to the driving policy of the predicted target human driver. In this work, we propose to overcome these two shortcomings by a Bayesian recurrent neural network model consisting of Bayesian-neural-network-based policy model and known physical model of the scenario. Bayesian neural network can ensemble complicated output distribution, enabling rich family of trajectory distribution. The embedded physical model ensures feasibility of the distribution. Moreover, the adopted gradient-based training method allows direct optimization for better performance in long prediction horizon. Furthermore, a particle-filter-based parameter adaptation algorithm is designed to adapt the policy Bayesian neural network to the predicted target online. Effectiveness of the proposed methods is verified with a toy example with multi-modal stochastic feedback gain and naturalistic car following data.


Title: Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control
Key Words: biological tissues  learning (artificial intelligence)  manipulators  medical robotics  mobile robots  predictive control  robot vision  surgery  autonomous tissue manipulation  soft tissue  AI learning  vision strategies  Raven IV surgical robotic system  predictive control algorithms  reinforcement learning  Robots  Heuristic algorithms  Prediction algorithms  Surgery  Neural networks  Task analysis  Aerospace electronics  Robotic Tissue Manipulation  Reinforcement Learning  Learning from Demonstration  Neural Networks  Simulation  Surgery  Automation  Machine Learning  Artificial Intelligence  AI  Raven Surgical Robot  Medical Robotics 
Abstract: Tissue manipulation is a frequently used fundamental subtask of any surgical procedures, and in some cases it may require the involvement of a surgeon's assistant. The complex dynamics of soft tissue as an unstructured environment is one of the main challenges in any attempt to automate the manipulation of it via a surgical robotic system. Two AI learning based model predictive control algorithms using vision strategies are proposed and studied: (1) reinforcement learning and (2) learning from demonstration. Comparison of the performance of these AI algorithms in a simulation setting indicated that the learning from demonstration algorithm can boost the learning policy by initializing the predicted dynamics with given demonstrations. Furthermore, the learning from demonstration algorithm is implemented on a Raven IV surgical robotic system and successfully demonstrated feasibility of the proposed algorithm using an experimental approach. This study is part of a profound vision in which the role of a surgeon will be redefined as a pure decision maker whereas the vast majority of the manipulation will be conducted autonomously by a surgical robotic system. A supplementary video can be found at: http://bionics.seas.ucla.edu/research/surgeryproject17.html.


Title: A Novel Iterative Learning Model Predictive Control Method for Soft Bending Actuators
Key Words: actuators  bending  elasticity  iterative learning control  predictive control  robots  soft bending actuators  soft robots  pseudorigid-body model  bending behavior  learning curve  learning process  soft-elastic composite actuator  iterative learning model predictive control method  Mathematical model  Predictive models  Soft robotics  Actuators  Iterative methods  Predictive control  Computational modeling  Soft Material Robotics  Motion Control  Model Learning for Control 
Abstract: Soft robots attract research interests worldwide. However, its control remains challenging due to the difficulty in sensing and accurate modeling. In this paper, we propose a novel iterative learning model predictive control (ILMPC) method for soft bending actuators. The uniqueness of our approach is the ability to improve model accuracy gradually. In this method, a pseudo-rigid-body model is used to take an initial guess of the bending behavior of the actuator and the model accuracy is improved with iterative learning. Compared with conventional model free iterative learning control (ILC), the proposed method significantly reduces the learning curve. Compared with the model predictive control (MPC), the proposed method does not rely on an accurate model and it will output a satisfactory model after the learning process. A soft-elastic composite actuator (SECA) is used to validate the proposed method. Both simulation and experimental results show that the proposed method outperforms the conventional MPC and ILC.


Title: DeepFusion: Real-Time Dense 3D Reconstruction for Monocular SLAM using Single-View Depth and Gradient Predictions
Key Words: cameras  computerised instrumentation  graphics processing units  image fusion  image reconstruction  learning (artificial intelligence)  minimisation  neurocontrollers  photometry  probability  SLAM (robots)  stereo image processing  target tracking  depth cameras  CNN  DeepFusion  semidense multiview stereo algorithm  gradient predictions  monocular SLAM  single-view depth  keypoint-based maps  camera tracking  dense 3D reconstructions  convolutional neural network  sparse monocular simultaneous localisation and mapping systems  real-time dense 3D reconstruction system  photometric error minimization  GPU  probability  Image reconstruction  Uncertainty  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Real-time systems  Robot vision systems 
Abstract: While the keypoint-based maps created by sparse monocular Simultaneous Localisation and Mapping (SLAM) systems are useful for camera tracking, dense 3D reconstructions may be desired for many robotic tasks. Solutions involving depth cameras are limited in range and to indoor spaces, and dense reconstruction systems based on minimising the photometric error between frames are typically poorly constrained and suffer from scale ambiguity. To address these issues, we propose a 3D reconstruction system that leverages the output of a Convolutional Neural Network (CNN) to produce fully dense depth maps for keyframes that include metric scale. Our system, DeepFusion, is capable of producing real-time dense reconstructions on a GPU. It fuses the output of a semi-dense multiview stereo algorithm with the depth and gradient predictions of a CNN in a probabilistic fashion, using learned uncertainties produced by the network. While the network only needs to be run once per keyframe, we are able to optimise for the depth map with each new frame so as to constantly make use of new geometric constraints. Based on its performance on synthetic and real world datasets, we demonstrate that DeepFusion is capable of performing at least as well as other comparable systems.


Title: Dynamic Hilbert Maps: Real-Time Occupancy Predictions in Changing Environments
Key Words: Hilbert spaces  mobile robots  remotely operated vehicles  real-time occupancy predictions  static occupancy models  continuous occupancy map  high-dimensional feature space  data-efficient model  crowded unstructured outdoor environments  dynamic Hilbert maps  temporal dependencies  3D laser data  2D laser data  Vehicle dynamics  Predictive models  Uncertainty  Dynamics  Indexes  Real-time systems  Clustering algorithms 
Abstract: This paper addresses the problem of learning instantaneous occupancy levels of dynamic environments and predicting future occupancy levels. Due to the complexity of most real environments, such as urban streets or crowded areas, the efficient and robust incorporation of temporal dependencies into otherwise static occupancy models remains a challenge. We propose a method to capture the uncertainty of moving objects and incorporate this uncertainty information into a continuous occupancy map represented in a rich high-dimensional feature space. This data-efficient model not only allows us to learn the occupancy states incrementally, but also makes predictions about what the future occupancy states will be. Experiments performed using 2D and 3D laser data collected from crowded unstructured outdoor environments show that the proposed methodology can accurately predict occupancy states for areas of around 1000 m2 at 10 Hz, making the proposed framework ideal for online applications under real-time constraints.


Title: Actively Improving Robot Navigation On Different Terrains Using Gaussian Process Mixture Models
Key Words: Gaussian processes  mixture models  mobile robots  navigation  path planning  robot navigation  outdoor environments  place-dependent model  aerial image  path planning  Gaussian process mixture model  Robots  Navigation  Vibrations  Mixture models  Energy consumption  Gaussian processes  Planning 
Abstract: Robot navigation in outdoor environments is exposed to detrimental factors such as vibrations or power consumption due to the different terrains on which the robot navigates. In this paper, we address the problem of actively improving navigation by planning paths that aim at reducing over time phenomena such as vibrations during traversal. Our approach uses a Gaussian Process (GP) mixture model and an aerial image of the environment to learn and improve continuously a place-dependent model of such phenomena from the experiences of the robot. We use this model to plan paths that trade-off the exploration of unknown promising regions and the exploitation of known areas where the impact of the detrimental factors on navigation is low, leading to an improved navigation over time. We implemented our approach and thoroughly tested it using real-world data. Our experiments suggest that our approach with no initial information leads the robot, after few runs, to follow paths along which it experiences similar vibrations or energy consumption as if it was following the optimal path computed given the ground truth information.


Title: Experimental Learning of a Lift-Maximizing Central Pattern Generator for a Flapping Robotic Wing
Key Words: aerodynamics  aerospace components  autonomous aerial vehicles  control engineering computing  gradient methods  learning (artificial intelligence)  motion control  multi-agent systems  robot dynamics  robot kinematics  robot programming  experimental learning  lift-maximizing central pattern generator  flapping robotic wing  policy gradient algorithm  dynamically scaled robotic wing  constant Reynolds number  central pattern generator model  CPG  motion controller  rhythmic wing motion patterns  half-stroke symmetry constraint  learning agent  robotic learning  wing kinematic learning  Kinematics  Robots  Oscillators  Trajectory  Servomotors  Heuristic algorithms  Computational modeling 
Abstract: In this work, we present an application of a policy gradient algorithm to a real-time robotic learning problem, where the goal is to maximize the average lift generation of a dynamically scaled robotic wing at a constant Reynolds number (Re). Compared to our previous work, the merit of this work is two-fold. First, a central pattern generator (CPG) model was used as the motion controller, which provided a smooth generation and transition of rhythmic wing motion patterns while the CPG was being updated by the policy gradient, thereby accelerating the sample generation and reducing the total learning time. Second, the kinematics included three degrees of freedom (stroke, deviation, pitching) and were also free of half-stroke symmetry constraint, together they yielded a larger kinematic space which later explored by the policy gradient to maximize the lift generation. The learned wing kinematics used the full range of stroke and deviation to maximize the lift generation, implying that the wing trajectories with larger disk area and lower frequencies were preferred for high lift generation at constant Re. Furthermore, the wing pitching amplitude converged to values between 45°-49° regardless of what the other parameters were. Notably, the learning agent was able to find two locally optimal wing motion patterns, which had distinct shapes of wing trajectory but generated similar cycle-averaged lift.


Title: Design and Implementation of Computer Vision based In-Row Weeding System
Key Words: agricultural robots  agrochemicals  cameras  crops  feature extraction  mobile robots  robot vision  spraying  sustainable development  computer vision  autonomous robotic weeding systems  precision farming  current dependency  herbicides  pesticides  selective spraying  mechanical weed removal modules  environmental pollution  real-time treatment  weeding control system  indeterminate classification delays  in-row weeding system design  sustainability  nonoverlapping multicamera system  terrain conditions  Cameras  Agriculture  Delays  Control systems  Robots  Three-dimensional displays  Tracking 
Abstract: Autonomous robotic weeding systems in precision farming have demonstrated their full potential to alleviate the current dependency on herbicides or pesticides by introducing selective spraying or mechanical weed removal modules, thus reducing the environmental pollution and improving the sustainability. However, most previous works require fast weed detection system to achieve real-time treatment. In this paper, a novel computer vision based weeding control system is presented, where a non-overlapping multi-camera system is introduced to compensate the indeterminate classification delays, thus allowing for more complicated and advanced detection algorithms, e.g. deep learning based methods. The suitable tracking and control strategies are developed to achieve accurate and robust in-row weed treatment, and the performance of the proposed system is evaluated in different terrain conditions in the presence of various delays.


Title: LSTM-based Network for Human Gait Stability Prediction in an Intelligent Robotic Rollator
Key Words: adaptive control  control engineering computing  gait analysis  geriatrics  handicapped aids  image colour analysis  intelligent robots  Kalman filters  laser ranging  learning (artificial intelligence)  medical robotics  mobile robots  nonlinear filters  pose estimation  robust control  state estimation  legs positions  UKF  pose estimation  deep learning  laser range finder data  augmented gait state estimation  human gait stability predictor  user-adaptive control architecture  unscented Kalman filter  body center of mass  long short term memory networks  robust predictions  encoder-decoder sequence  LRF data  nonwearable sensors  multimodal RGB-D  elderly users  intelligent robotic rollator  LSTM-based network  Stability analysis  Legged locomotion  Laser stability  Robot sensing systems  Senior citizens 
Abstract: In this work, we present a novel framework for on-line human gait stability prediction of the elderly users of an intelligent robotic rollator using Long Short Term Memory (LSTM) networks, fusing multimodal RGB-D and Laser Range Finder (LRF) data from non-wearable sensors. A Deep Learning (DL) based approach is used for the upper body pose estimation. The detected pose is used for estimating the body Center of Mass (CoM) using Unscented Kalman Filter (UKF). An Augmented Gait State Estimation framework exploits the LRF data to estimate the legs' positions and the respective gait phase. These estimates are the inputs of an encoder-decoder sequence to sequence model which predicts the gait stability state as Safe or Fall Risk walking. It is validated with data from real patients, by exploring different network architectures, hyperparameter settings and by comparing the proposed method with other baselines. The presented LSTM-based human gait stability predictor is shown to provide robust predictions of the human stability state, and thus has the potential to be integrated into a general user-adaptive control architecture as a fall-risk alarm.


Title: Deep n-Shot Transfer Learning for Tactile Material Classification with a Flexible Pressure-Sensitive Skin
Key Words: convolutional neural nets  haptic interfaces  learning (artificial intelligence)  pattern classification  1-shot learning  knowledge transfer  deep n-shot transfer learning  tactile material classification  flexible pressure-sensitive skin  active sensing tasks  deep end-to-end transfer learning  deep convolutional neural network  superhuman tactile classification performance  Task analysis  Robot sensing systems  Feature extraction  Training  Skin  Learning systems 
Abstract: n-shot learning, i.e., learning a classifier from only few or even one training samples per class, is the ultimate goal in minimizing the cost of sample acquisition. This is esp. important for active sensing tasks like tactile material classification. Achieving high classification accuracy from only few samples is typically possible only when pre-knowledge is used. In n-shot transfer learning, knowledge from pre-training on a large knowledge set with many classes and samples per class has to be transferred to support the training for a given task set with only few samples per new class. In this paper, we show for the first time that deep end-to-end transfer learning is feasible for tactile material classification. Based on the previously presented (TactNet-II) [1], a deep convolutional neural network (CNN) which reaches superhuman tactile classification performance, we adapt state-of-the art deep transfer learning methods. We evaluate the resulting deep n-shot learning methods with a publicly available tactile material data set with 36 materials [1] in a 6-way n-shot learning task with 30 materials in the knowledge set. In 1-shot learning, our deep transfer learning method reaches 75.5% classification accuracy and in 10-shot more than 90%, outperforming classification without knowledge transfer by more than 40%. This results in an up to 15 time reduction in the number of samples needed to reach a desired accuracy level. We also provide insights of the inner workings of the derived deep transfer learning methods.


Title: Towards Effective Tactile Identification of Textures using a Hybrid Touch Approach
Key Words: convolutional neural nets  feature extraction  humanoid robots  image classification  image texture  learning (artificial intelligence)  neurocontrollers  recurrent neural nets  robot vision  tactile sensors  hybrid touch approach  human sense  interacted objects  standard sensory modalities  sliding movements  tactile-based texture classification  machine-learning methods  surface textures  hand-engineered features  recurrent neural network layers  feature representations  tactile data  touch data  tactile identification  sense of touch  touch movements  convolutional neural network layers  iCub platform  Support vector machines  Robot sensing systems  Standards  Machine learning  Recurrent neural networks 
Abstract: The sense of touch is arguably the first human sense to develop. Empowering robots with the sense of touch may augment their understanding of interacted objects and the environment beyond standard sensory modalities (e.g., vision). This paper investigates the effect of hybridizing touch and sliding movements for tactile-based texture classification. We develop three machine-learning methods within a framework to discriminate between surface textures; the first two methods use hand-engineered features, whilst the third leverages convolutional and recurrent neural network layers to learn feature representations from raw data. To compare these methods, we constructed a dataset comprising tactile data from 23 textures gathered using the iCub platform under a loosely constrained setup, i.e., with nonlinear motion. In line with findings from neuroscience, our experiments show that a good initial estimate can be obtained via touch data, which can be further refined via sliding; combining both touch and sliding data results in 98% classification accuracy over unseen test data.


Title: Miniaturization of multistage high dynamic range six-axis force sensor composed of resin material
Key Words: learning (artificial intelligence)  mobile robots  motion control  path planning  robot vision  mobile robots  learning (artificial intelligence)  robot vision  path planning  motion control  medical robotics  optimisation  object detection  position control  collision avoidance  Force sensors  Structural beams  Force  Robot sensing systems  Stress  Strain  Creep 
Abstract: The following topics are dealt with: mobile robots; learning (artificial intelligence); robot vision; path planning; motion control; medical robotics; optimisation; object detection; position control; collision avoidance.


Title: Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots
Key Words: gesture recognition  humanoid robots  human-robot interaction  knowledge based systems  learning (artificial intelligence)  mobile robots  neural nets  speech recognition  text analysis  TED talks  NAO robot  speech text understanding  end-to-end neural network model  learning-based co-speech gesture generation  human labor  rule-based speech-gesture association  humanoid robots  end-to-end learning  robots learn social skills  Robots  Videos  Decoding  Natural languages  Training  Recurrent neural networks  Principal component analysis 
Abstract: Co-speech gestures enhance interaction experiences between humans as well as between humans and robots. Most existing robots use rule-based speech-gesture association, but this requires human labor and prior knowledge of experts to be implemented. We present a learning-based co-speech gesture generation that is learned from 52 h of TED talks. The proposed end-to-end neural network model consists of an encoder for speech text understanding and a decoder to generate a sequence of gestures. The model successfully produces various gestures including iconic, metaphoric, deictic, and beat gestures. In a subjective evaluation, participants reported that the gestures were human-like and matched the speech content. We also demonstrate a co-speech gesture with a NAO robot working in real time.


Title: How Shall I Drive? Interaction Modeling and Motion Planning towards Empathetic and Socially-Graceful Driving
Key Words: game theory  intelligent robots  learning (artificial intelligence)  mobile robots  path planning  predictive control  remotely operated vehicles  AV  human driver  social awareness  passive-aggressive motions  interaction modeling  socially-graceful driving  autonomous vehicles  two-player game  model predictive control  social gracefulness  intent inference  motion planning  Planning  Games  Vehicles  Adaptation models  Inference algorithms  Estimation  Loss measurement 
Abstract: While intelligence of autonomous vehicles (AVs) has significantly advanced in recent years, accidents involving AVs suggest that these autonomous systems lack gracefulness in driving when interacting with human drivers. In the setting of a two-player game, we propose model predictive control based on social gracefulness, which is measured by the discrepancy between the actions taken by the AV and those that could have been taken in favor of the human driver. We define social awareness as the ability of an agent to infer such favorable actions based on knowledge about the other agent's intent, and further show that empathy, i.e., the ability to understand others' intent by simultaneously inferring others' understanding of the agent's self intent, is critical to successful intent inference. Lastly, through an intersection case, we show that the proposed gracefulness objective allows an AV to learn more sophisticated behavior, such as passive-aggressive motions that gently force the other agent to yield.


Title: Customized Object Recognition and Segmentation by One Shot Learning with Human Robot Interaction
Key Words: Big Data  human-robot interaction  image segmentation  learning (artificial intelligence)  neural nets  object detection  object recognition  human robot interaction  robotic applications  deep learning models  labeled training data  pre-defined big data  segmentation method  target object  data generation method  segmentation model  lightweight segmentation net  object recognition  one shot learning  Proposals  Robots  Adaptation models  Training data  Data models  Object recognition  Testing 
Abstract: There are two difficulties to utilize state-of-the-art object recognition/detection/segmentation methods to robotic applications. First, most of the deep learning models heavily depend on large amounts of labeled training data, which are expensive to obtain for each individual application. Second, the object categories must be pre-defined in the dataset, thus not practical to scenarios with varying object categories. To alleviate the reliance on pre-defined big data, this paper proposes a customized object recognition and segmentation method. It aims to recognize and segment any object defined by the user, given only one annotation. There are three steps in the proposed method. First, the user takes an exemplar video of the target object with the robot, defines its name, and mask its boundary on only one frame. Then the robot automatically propagates the annotation through the exemplar video based on a proposed data generation method. In the meantime, a segmentation model continuously updates itself on the generated data. Finally, only a lightweight segmentation net is required at testing stage, to recognize and segment the user-defined object in any scenes.


Title: Object Classification Based on Unsupervised Learned Multi-Modal Features For Overcoming Sensor Failures
Key Words: driver information systems  feature extraction  image classification  image fusion  unsupervised learning  unsupervised learned multimodal features  autonomous driving applications  road users  road side infrastructure  autonomous cars  classification modules  unseen sensor noise  object classification module  total sensor failure  unsupervised feature training  uni-modal classifiers training  multimodal classifiers training  feature space  sensor modalities  decision module  unsupervised learned multi-modal features  Three-dimensional displays  Feature extraction  Robot sensing systems  Computer architecture  Training  Decoding  Convolutional codes 
Abstract: For autonomous driving applications it is critical to know which type of road users and road side infrastructure are present to plan driving manoeuvres accordingly. Therefore autonomous cars are equipped with different sensor modalities to robustly perceive its environment. However, for classification modules based on machine learning techniques it is challenging to overcome unseen sensor noise. This work presents an object classification module operating on unsupervised learned multi-modal features with the ability to overcome gradual or total sensor failure. A two stage approach composed of an unsupervised feature training and a uni-modal and multimodal classifiers training is presented. We propose a simple but effective decision module switching between uni-modal and multi-modal classifiers based on the closeness in the feature space to the training data. Evaluations on the ModelNet 40 data set show that the proposed approach has a 14% accuracy gain compared to a late fusion approach operating on a noisy point cloud data and a 6% accuracy gain when operating on noisy image data.


Title: SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud
Key Words: image segmentation  object detection  optical radar  rendering (computer graphics)  unsupervised learning  LiDAR point cloud  deep-learning-based approaches  point cloud segmentation  SqueezeSetV2  data collection  domain-adaptation training pipeline  domain adaptation pipeline  unsupervised domain adaptation  road-object segmentation  domain-adaptation methods  Three-dimensional displays  Laser radar  Training  Adaptation models  Data models  Pipelines  Sensors 
Abstract: Earlier work demonstrates the promise of deep-learning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2. With an improved model structure, SqueezeSetV2 is more robust against dropout noises in LiDAR point cloud and therefore achieves significant accuracy improvement. Training models for point cloud segmentation requires large amounts of labeled data, which is expensive to obtain. To sidestep the cost of data collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. Existing domain-adaptation methods mainly focus on images and most of them cannot be directly applied to point clouds. We address this problem with a domain-adaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6% over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0% to 57.4%. Our source code and synthetic dataset are open sourced. https://github.com/xuanyuzhou98/SqueezeSegV2.


Title: A Framework for Self-Training Perceptual Agents in Simulated Photorealistic Environments
Key Words: computer games  learning (artificial intelligence)  mobile robots  object recognition  robot vision  virtual reality  self-training perceptual agents  simulated photorealistic environments  high-performance perception  mobile robotic agents  gaming industry  game engines  perceptual agent  virtual environment  task-specific object distribution  description language  learning environments  object recognition  sensory input  robotic system  Task analysis  Robots  Training data  Engines  Virtual environments  Games  Data models  Self-Training Perception  Robotic Simulation  Unreal Engine  Scenario Description 
Abstract: The development of high-performance perception for mobile robotic agents is still challenging. Learning appropriate perception models usually requires extensive amounts of labeled training data that ideally follows the same distribution as the data an agent will encounter in its target task. Recent developments in gaming industry led to game engines able to generate photorealistic environments in real-time, which can be used to realistically simulate the sensory input of an agent.We propose a novel framework which allows the definition of different learning scenarios and instantiates these scenarios in a high quality game engine where a perceptual agent can act and learn in. The scenarios are specified in a newly developed scenario description language that allows the parametrization of the virtual environment and the perceptual agent. New scenarios can be sampled from a task-specific object distribution that allows the automatic generation of extensive amounts of different learning environments for the perceptual agent.We will demonstrate the plausibility of the framework by conducting object recognition experiments on a real robotic system which has been trained within our framework.


Title: Gaussian Processes Model-Based Control of Underactuated Balance Robots
Key Words: control system synthesis  Gaussian processes  mobile robots  pendulums  predictive control  robot dynamics  robust control  trajectory control  control design  robot dynamics  underactuated balance robot  model predictive control  Gaussian process regression model  trajectory tracking  learning-based control  GP model  robustness  Furuta pendulum system  Trajectory  Mathematical model  Computational modeling  Robot kinematics  Predictive models  Trajectory tracking 
Abstract: Control of underactuated balance robot requires external subsystem trajectory tracking and internal unstable subsystem balancing with limited control authority. We present a learning-based control approach for underactuated balance robots. The tracking and balancing control is designed the controller in fast- and slow-time scales. In the slow-time scale, model predictive control is adopted to plan desired internal state profile to achieve external trajectory tracking task. The internal state is then stabilized around the planned profile in the fast-time scale. The control design is based on a learned Gaussian process (GP) regression model without need of a priori knowledge about the robot dynamics. The controller also incorporates the GP model predicted variance to enhance robustness to modeling errors. Experiments are presented using a Furuta pendulum system.


Title: A Fog Robotics Approach to Deep Robot Learning: Application to Object Recognition and Grasp Planning in Surface Decluttering
Key Words: cloud computing  control engineering computing  learning (artificial intelligence)  mobile robots  object recognition  path planning  robot programming  robot vision  storage management  fog robotics  mobile robot  grasp planning model  nonpublic synthetic images  deep object recognition  nonprivate synthetic images  deep models  centralized Cloud Robotics model  surface decluttering  deep robot learning  Cloud computing  Robot sensing systems  Computational modeling  Adaptation models  Security  Data models 
Abstract: The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4× to successfully declutter 86% of objects over 213 attempts.


Title: Robot Communication Via Motion: Closing the Underwater Human-Robot Interaction Loop
Key Words: control engineering computing  human-robot interaction  mobile robots  robot communication  underwater human-robot interaction loop  colored lights  underwater robots  robot-to-human communication methods  body language gestures  communication vector  Robots  Solids  Task analysis  Unmanned underwater vehicles  Communication systems  Human-robot interaction  Hardware 
Abstract: In this paper, we propose a novel method for underwater robot-to-human communication using the motion of the robot as “body language”. To evaluate this system, we develop simulated examples of the system's body language gestures, called kinemes, and compare them to a baseline system using flashing colored lights through a user study. Our work shows evidence that motion can be used as a successful communication vector which is accurate, easy to learn, and quick enough to be used, all without requiring any additional hardware to be added to our platform. We thus contribute to “closing the loop” for human-robot interaction underwater by proposing and testing this system, suggesting a library of possible body language gestures for underwater robots, and offering insight on the design of nonverbal robot-to-human communication methods.


Title: Project AutoVision: Localization and 3D Scene Perception for an Autonomous Vehicle with a Multi-Camera System
Key Words: cameras  geometry  image reconstruction  mobile robots  path planning  remotely operated vehicles  robot vision  stereo image processing  video signal processing  sensor suite  autonomous vehicle  multicamera system  3D scene perception capabilities  self-driving vehicle  autonomous navigation  urban environments  rural environments  exteroceptive sensors  AutoVision project  multiview geometry  Cameras  Sensors  Three-dimensional displays  Calibration  Laser radar  Autonomous vehicles  Lighting 
Abstract: Project AutoVision aims to develop localization and 3D scene perception capabilities for a self-driving vehicle. Such capabilities will enable autonomous navigation in urban and rural environments, in day and night, and with cameras as the only exteroceptive sensors. The sensor suite employs many cameras for both 360-degree coverage and accurate multi-view stereo; the use of low-cost cameras keeps the cost of this sensor suite to a minimum. In addition, the project seeks to extend the operating envelope to include GNSS-less conditions which are typical for environments with tall buildings, foliage, and tunnels. Emphasis is placed on leveraging multi-view geometry and deep learning to enable the vehicle to localize and perceive in 3D space. This paper presents an overview of the project, and describes the sensor suite and current progress in the areas of calibration, localization, and perception.


Title: Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks
Key Words: cameras  image matching  image sequences  motion estimation  object detection  unsupervised learning  video signal processing  monocular depth  multiple masks  unsupervised learning method  depth estimation network  ego-motion estimation network  projection target imaging plane  fine masks  image pixel mismatch  repeated masking  KITTI dataset  low-quality uncalibrated bike video dataset  Image reconstruction  Estimation  Cameras  Unsupervised learning  Training  Simultaneous localization and mapping 
Abstract: A new unsupervised learning method of depth and ego-motion using multiple masks from monocular video is proposed in this paper. The depth estimation network and the ego-motion estimation network are trained according to the constraints of depth and ego-motion without truth values. The main contribution of our method is to carefully consider the occlusion of the pixels generated when the adjacent frames are projected to each other, and the blank problem generated in the projection target imaging plane. Two fine masks are designed to solve most of the image pixel mismatch caused by the movement of the camera. In addition, some relatively rare circumstances are considered, and repeated masking is proposed. To some extent, the method is to use a geometric relationship to filter the mismatched pixels for training, making unsupervised learning more efficient and accurate. The experiments on KITTI dataset show our method achieves good performance in terms of depth and ego-motion. The generalization capability of our method is demonstrated by training on the low-quality uncalibrated bike video dataset and evaluating on KITTI dataset, and the results are still good.


Title: Soil Displacement Terramechanics for Wheel-Based Trenching with a Planetary Rover
Key Words: aerospace robotics  mobile robots  planetary rovers  soil  wheels  trenching  autonomous trenching  front wheel  rear wheel  deep trench  single wheel experiments  driving strategy  closed-form model  digging operations  wheel actuators  planetary exploration rovers  wheel-based trenching  soil displacement terramechanics  Wheels  Soil  Geometry  Deformable models  Mathematical model  Predictive models  Finite element analysis 
Abstract: Planetary exploration rovers are expensive, weight constrained, and cannot be serviced once deployed. Here, we explore one way to increase their capabilities while avoiding the cost, mass, and complexity leading to these issues. We propose to re-use the large wheel actuators for trenching and other digging operations, which will enable a range of missions such as sampling deeper layers of soil. We present a new, closed-form model of the soil displaced by an angled, spinning wheel to analyze the trenching potential of a driving strategy and inform the control of the wheel. The model is demonstrated with single wheel experiments under different driving conditions. The model suggests: that a deep trench does not require large tractive efforts; that the shape of the trench can be controlled; and that a rear wheel has a lower risk of entrapment when trenching than a front wheel. Ultimately this model could be used in a nonprehensile manipulation planning or learning algorithm to enable autonomous trenching.


Title: OmniDRL: Robust Pedestrian Detection using Deep Reinforcement Learning on Omnidirectional Cameras*
Key Words: cameras  computerised instrumentation  image processing  learning (artificial intelligence)  robust pedestrian detection  omnidirectional cameras  computer vision  robotics  deep learning methods  omnidirectional imaging  OmniDRL  deep reinforcement learning  3D bounding boxes  Cameras  Three-dimensional displays  Distortion  Image segmentation  Robot vision systems  Object detection 
Abstract: Pedestrian detection is one of the most explored topics in computer vision and robotics. The use of deep learning methods allowed the development of new and highly competitive algorithms. Deep Reinforcement Learning has proved to be within the state-of-the-art in terms of both detection in perspective cameras and robotics applications. However, for detection in omnidirectional cameras, the literature is still scarce, mostly because of their high levels of distortion. This paper presents a novel and efficient technique for robust pedestrian detection in omnidirectional images. The proposed method uses deep Reinforcement Learning that takes advantage of the distortion in the image. By considering the 3D bounding boxes and their distorted projections into the image, our method is able to provide the pedestrian's position in the world, in contrast to the image positions provided by most state-of-the-art methods for perspective cameras. Our method avoids the need of pre-processing steps to remove the distortion, which is computationally expensive. Beyond the novel solution, our method compares favorably with the state-of-the-art methodologies that do not consider the underlying distortion for the detection task.


Title: 2D3D-Matchnet: Learning To Match Keypoints Across 2D Image And 3D Point Cloud
Key Words: cameras  feature extraction  image classification  image matching  image representation  image retrieval  image sensors  learning (artificial intelligence)  object recognition  pose estimation  solid modelling  visual databases  image-based counterpart  visual pose estimation  2D-3D image  cloud correspondences  end-to-end deep network architecture  query image  3D point cloud reference map  Oxford 2D-3D Patches dataset  Oxford Robotcar dataset  ground truth camera pose  Three-dimensional displays  Two dimensional displays  Pose estimation  Visualization  Cameras  Training  Detectors 
Abstract: Large-scale point cloud generated from 3D sensors is more accurate than its image-based counterpart. However, it is seldom used in visual pose estimation due to the difficulty in obtaining 2D-3D image to point cloud correspondences. In this paper, we propose the 2D3D-MatchNet - an end-to-end deep network architecture to jointly learn the descriptors for 2D and 3D keypoint from image and point cloud, respectively. As a result, we are able to directly match and establish 2D-3D correspondences from the query image and 3D point cloud reference map for visual pose estimation. We create our Oxford 2D-3D Patches dataset from the Oxford Robotcar dataset with the ground truth camera poses and 2D-3D image to point cloud correspondences for training and testing the deep network. Experimental results verify the feasibility of our approach.


Title: Teaching Robots To Draw
Key Words: handwritten character recognition  manipulators  natural language processing  teaching  just-drawn handwritten characters  writing utensil  target stroke  continuous drawing motion  handcrafted rules  predefined paths  stroke-based drawing  teaching robots  manipulator robots  line drawings  Automation  Machine-to-machine communications 
Abstract: In this paper, we introduce an approach which enables manipulator robots to write handwritten characters or line drawings. Given an image of just-drawn handwritten characters, the robot infers a plan to replicate the image with a writing utensil, and then reproduces the image. Our approach draws each target stroke in one continuous drawing motion and does not rely on handcrafted rules or on predefined paths of characters. Instead, it learns to write from a dataset of demonstrations. We evaluate our approach in both simulation and on two real robots. Our model can draw handwritten characters in a variety of languages which are disjoint from the training set, such as Greek, Tamil, or Hindi, and also reproduce any stroke-based drawing from an image of the drawing.


Title: Learning Probabilistic Multi-Modal Actor Models for Vision-Based Robotic Grasping
Key Words: Gaussian processes  learning (artificial intelligence)  manipulators  neural nets  robot vision  statistical distributions  inference time  probabilistic multimodal actor models  vision-based robotic grasping  neural density model  neural network  normalizing flows  complex probability distributions  Gaussian mixture  conditional distribution  4 dimensional action space  Training  Grasping  Robots  Neural networks  Computational modeling  Predictive models  Probability distribution 
Abstract: Many previous works approach vision-based robotic grasping by training a value network that evaluates grasp proposals. These approaches require an optimization process at run-time to infer the best action from the value network. As a result, the inference time grows exponentially as the dimension of action space increases. We propose an alternative method, by directly training a neural density model to approximate the conditional distribution of successful grasp poses from the input images. We construct a neural network that combines Gaussian mixture and normalizing flows, which is able to represent multi-modal, complex probability distributions. We demonstrate on both simulation and real robot that the proposed actor model achieves similar performance compared to the value network using the Cross-Entropy Method (CEM) for inference, on top-down grasping with a 4 dimensional action space. Our actor model reduces the inference time by 3 times compared to the state-of-the-art CEM method. We believe that actor models will play an important role when scaling up these approaches to higher dimensional action spaces.


Title: Self-supervised Learning for Single View Depth and Surface Normal Estimation
Key Words: convolutional neural nets  natural scenes  stereo image processing  supervised learning  single view depth  surface normal estimation  self-supervised learning framework  surface normals  outdoor scenes  fronto-parallel planes  piece-wise smooth depth  surface orientation  natural scenes  piece-wise smooth normals  trained normal network  depth network  realistic smooth normal assumption  self-supervised depth prediction network  convolutional neural networks  depth-normal consistency  Training  Estimation  Geometry  Cameras  Sensors  Visual odometry  Neural networks 
Abstract: In this work we present a self-supervised learning framework to simultaneously train two Convolutional Neural Networks (CNNs) to predict depth and surface normals from a single image. In contrast to most existing frameworks which represent outdoor scenes as fronto-parallel planes at piece-wise smooth depth, we propose to predict depth with surface orientation while assuming that natural scenes have piece-wise smooth normals. We show that a simple depth-normal consistency as a soft-constraint on the predictions is sufficient and effective for training both these networks simultaneously. The trained normal network provides state-of-the-art predictions while the depth network, relying on much realistic smooth normal assumption, outperforms the traditional self-supervised depth prediction network by a large margin on the KITTI benchmark.


Title: Learning to Drive from Simulation without Real World Labels
Key Words: cameras  closed loop systems  control engineering computing  learning (artificial intelligence)  learning systems  mobile robots  road vehicles  robot vision  traffic engineering computing  domain transfer  single-camera control policy  simulation control labels  driving performance  rural roads  urban roads  machine learning systems  simulated environment  vision-based lane  driving policy  rural road  image-to-image translation  autonomous vehicle  open-loop regression metric  Aerospace electronics  Image reconstruction  Task analysis  Semantics  Training  Roads  Measurement 
Abstract: Simulation can be a powerful tool for under-standing machine learning systems and designing methods to solve real-world problems. Training and evaluating methods purely in simulation is often “doomed to succeed” at the desired task in a simulated environment, but the resulting models are incapable of operation in the real world. Here we present and evaluate a method for transferring a vision-based lane following driving policy from simulation to operation on a rural road without any real-world labels. Our approach leverages recent advances in image-to-image translation to achieve domain transfer while jointly learning a single-camera control policy from simulation control labels. We assess the driving performance of this method using both open-loop regression metrics, and closed-loop performance operating an autonomous vehicle on rural and urban roads.


Title: Asymmetric Local Metric Learning with PSD Constraint for Person Re-identification
Key Words: approximation theory  image recognition  iterative methods  learning (artificial intelligence)  matrix algebra  video signal processing  visual databases  asymmetric local metric learning  machine learning  positive sample pairs  adaptive local metric learning method  single distance metric  smooth metric matrix function  linear combinations  learning process  positive semidefinite constraint  person reidentification  metric learning  PSD constraint  UCI databases  GRID database  VIPeR database  CUHK01 database  video monitor  approximation error bound  Measurement  Learning systems  Manifolds  Databases  Symmetric matrices  Machine learning  Machine learning algorithms 
Abstract: Person re-identification is one of the key issues in both machine learning and video monitor application. In particular, defining an appropriate distance metric between the person images is very important. Existing metric learning approaches used in person re-identification either learn a single measure, or ignore the positive semi-definite (PSD) of measurement matrix, at the same time, since the number of negative sample pairs largely exceeds the number of positive sample pairs, some metric learning methods are largely influenced by the sample imbalance. Considering the above issues, we propose a new adaptive local metric learning method with positive semi-definite (PSD) constraint. Unlike existing metric learning methods which learn a single distance metric, we use an approximation error bound of a smooth metric matrix function over the data manifold to learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. Besides, we develop an efficient two stage algorithm that first learns the anchor points and the linear combinations of each instance, then learns the metric matrices of the anchor points. We employ the fast iterative shrinkage-thresholding algorithm which is a fast first-order optimization algorithm in the learning process of the linear combinations as well as the basis metrics of the anchor points. Our metric learning method has excellent performance. We firstly apply the proposed method on 5 UCI databases, which are widely used in machine learning, to test and evaluate the effectiveness of the proposed method. Then the proposed approach is applied for person re-identification, achieving better performance on three challenging databases (GRID, VIPeR, CUHK01) than the existing methods. The experimental results show that the proposed method can prvide the theoretical and practical support for the person re-identification problem.


Title: Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications
Key Words: computer vision  learning (artificial intelligence)  mobile robots  object detection  pose estimation  high posture variance  real-time detection rates  3D object detection domain  mobile application  3D point clouds  robust 3D person detector  posture estimator  mobile robotic applications  computer vision domain  mobile robotics  standing postures  socially aware navigation  deep learning techniques  Kinect2 depth sensor  Three-dimensional displays  Detectors  Feature extraction  Robots  Histograms  Deep learning  Task analysis 
Abstract: Due to recent deep learning techniques, person detection seems to be solved in the computer vision domain, however, it is still an issue in mobile robotics. On a robot only limited computing capacities are available. The challenge gets even more difficult when operating in an environment, with people in poses different from the standard upright ones. In this work the environment of a supermarket is considered. Unlike most scenarios targeted by the community, persons not only occur in standing postures, but also grasping into the shelves or squatting in front of them. Furthermore, people are heavily occluded, e.g. by shopping carts. In such a challenging environment, it is important to perceive people early enough and in real-time in order to enable a socially aware navigation. Classical person detectors often suffer from a high posture variance or do not achieve acceptable real-time detection rates. For this reason, different components from the 3D object detection domain have been used to create a new robust person detector for mobile application. Operating on 3D point clouds allows fast detections in real-time up to our goal distance of ten meters and above using the Kinect2 depth sensor. The detector can even differentiate between typical postures of customers who stand or squat in front of shelves.


Title: Part Segmentation for Highly Accurate Deformable Tracking in Occlusions via Fully Convolutional Neural Networks
Key Words: convolutional neural nets  image filtering  image segmentation  learning (artificial intelligence)  object tracking  pose estimation  robot vision  stereo image processing  visual perception  direct pose estimation  machine learning  direct estimation techniques  geometric tracking methods  robotic applications  observed point clouds  segmentation maps  Fast-FCN network architecture  convolutional neural networks  Three-dimensional displays  Semantics  Computational modeling  Robots  Image segmentation  Computer architecture  Pose estimation 
Abstract: Successfully tracking the human body is an important perceptual challenge for robots that must work around people. Existing methods fall into two broad categories: geometric tracking and direct pose estimation using machine learning. While recent work has shown direct estimation techniques can be quite powerful, geometric tracking methods using point clouds can provide a very high level of 3D accuracy which is necessary for many robotic applications. However these approaches can have difficulty in clutter when large portions of the subject are occluded. To overcome this limitation, we propose a solution based on fully convolutional neural networks (FCN). We develop an optimized Fast-FCN network architecture for our application which allows us to filter observed point clouds and improve tracking accuracy while maintaining interactive frame rates. We also show that this model can be trained with a limited number of examples and almost no manual labelling by using an existing geometric tracker and data augmentation to automatically generate segmentation maps. We demonstrate the accuracy of our full system by comparing it against an existing geometric tracker, and show significant improvement in these challenging scenarios.


Title: Exploiting Trademark Databases for Robotic Object Fetching
Key Words: feature extraction  image sensors  learning (artificial intelligence)  mobile robots  neural nets  object detection  object recognition  robot vision  service robots  shape recognition  trademarks  synthetic data  convolutional neural network logo detector  domain randomization  soft drinks  logo images  large-scale data  household objects  service robots  robotic object  trademark databases  object fetching  Databases  Trademarks  Robots  Detectors  Task analysis  Training  Shape 
Abstract: Service robots require the ability to recognize various household objects in order to carry out certain tasks, such as fetching an object for a person. Manually collecting information on all the objects a robot may encounter in a household is tedious and time-consuming; therefore this paper proposes the use of large-scale data from existing trademark databases. These databases contain logo images and a description of the goods and services the logo was registered under. For example, Pepsi is registered under soft drinks. We extend domain randomization in order to generate synthetic data to train a convolutional neural network logo detector, which outperformed previous logo detectors trained on synthetic data. We also provide a practical implementation for object fetching on a robot, which uses a Kinect and the logo detector to identify the object the human user requested. Tests on this robot indicate promising results, despite not using any real world photos for training.


Title: Object Detection Approach for Robot Grasp Detection
Key Words: convolutional neural nets  grippers  learning (artificial intelligence)  object detection  robot vision  robot platform  object detection approach  robot grasp detection  robot grasping problem  parallel gripper  image data  end-to-end approach  RGB images  transfer learning  adapted network  convolutional neural network based based object detection architecture  Grasping  Robot kinematics  Grippers  Neural networks  Object detection  Task analysis 
Abstract: In this paper, we focus on the robot grasping problem with parallel grippers using image data. For this task, we propose and implement an end-to-end approach. In order to detect the good grasping poses for a parallel gripper from RGB images, we have employed transfer learning for a Convolutional Neural Network (CNN) based object detection architecture. Our obtained results show that, the adapted network either outperforms or is on-par with the state-of-the art methods on a benchmark dataset. We also performed grasping experiments on a real robot platform to evaluate our method's real world performance.


Title: MetaGrasp: Data Efficient Grasping by Affordance Interpreter Network
Key Words: dexterous manipulators  grippers  image colour analysis  inference mechanisms  learning (artificial intelligence)  robot vision  data efficient grasping  data-driven approach  training data  data collection  grasp training system  model inference  antipodal grasp rule  affordance map  ungraspability  grasp affordances  pixel-level affordance interpreter network  quantitative experiments  real-world grasp experiments  qualitative experiments  Grasping  Training  Data collection  Data models  Grippers  Robots  Deep learning 
Abstract: Data-driven approach for grasping shows significant advance recently. But these approaches usually require much training data. To increase the efficiency of grasping data collection, this paper presents a novel grasp training system including the whole pipeline from data collection to model inference. The system can collect effective grasp sample with a corrective strategy assisted by antipodal grasp rule, and we design an affordance interpreter network to predict pixelwise grasp affordance map. We define graspability, ungraspability and background as grasp affordances. The key advantage of our system is that the pixel-level affordance interpreter network trained with only a small number of grasp samples under antipodal rule can achieve significant performance on totally unseen objects and backgrounds. The training sample is only collected in simulation. Extensive qualitative and quantitative experiments demonstrate the accuracy and robustness of our proposed approach. In the real-world grasp experiments, we achieve a grasp success rate of 93% on a set of household items and 91% on a set of adversarial items with only about 6,300 simulated samples. We also achieve 87% accuracy in clutter scenario. Although the model is trained using only RGB image, when changing the background textures, it also performs well and can achieve even 94% accuracy on the set of adversarial objects, which outperforms current state-of-the-art methods.


Title: Video-based Prediction of Hand-grasp Preshaping with Application to Prosthesis Control
Key Words: dexterous manipulators  electromyography  learning (artificial intelligence)  medical control systems  medical signal processing  orthotics  prosthetics  video signal processing  deep learning  automatic prediction  hand prosthesis  video sequences  RGB-D video data  orthotic devices  prosthetic devices  surface electromyography pattern recognition  arbitrary objects  video-based technique  grasp-type selection techniques  prosthesis control  hand-grasp  video-based prediction  Cameras  Prosthetics  Deep learning  Predictive models  Grasping  Object recognition  Pattern recognition 
Abstract: Among the currently available grasp-type selection techniques for hand prostheses, there is a distinct lack of intuitive, robust, low-latency solutions. In this paper we investigate the use of a portable, forearm-mounted, video-based technique for the prediction of hand-grasp preshaping for arbitrary objects. The purpose of this system is to automatically select the grasp-type for the user of the prosthesis, potentially increasing ease-of-use and functionality. This system can be used to supplement and improve existing control strategies, such as surface electromyography (sEMG) pattern recognition, for prosthetic and orthotic devices. We designed and created a suitable dataset consisting of RGB-D video data for 2212 grasp examples split evenly across 7 classes; 6 grasps commonly used in activities of daily living, and an additional no-grasp category. We processed and analyzed the dataset using several state-of-the-art deep learning architectures. Our selected model shows promising results for realistic, intuitive, real-world use, reaching per-frame accuracies on video sequences of up to 95.90% on the validation set. Such a system could be integrated into the palm of a hand prosthesis, allowing an automatic prediction of the grasp-type without requiring any special movements or aiming by the user.


Title: OffsetNet: Deep Learning for Localization in the Lung using Rendered Images
Key Words: cameras  closed loop systems  image reconstruction  image registration  medical computing  medical image processing  phantoms  rendering (computer graphics)  surgery  bronchoscope  update rate  average position error  conserved regions  training dataset  simulated images  simulated domains  conservative thresholds  rendered images  surgical tools  dynamic anatomy  tortuous anatomy  real-time localization  preoperative scan  human operators  closed-loop control  autonomous agents  deep learning architecture  recorded camera images  lung phantom  OffsetNet  time 30.0 min  frequency 47.0 Hz  Lung  Computed tomography  Training  Robot sensing systems  Real-time systems  Cameras  Imaging phantoms 
Abstract: Navigating surgical tools in the dynamic and tortuous anatomy of the lung's airways requires accurate, real-time localization of the tools with respect to the preoperative scan of the anatomy. Such localization can inform human operators or enable closed-loop control by autonomous agents, which would require accuracy not yet reported in the literature. In this paper, we introduce a deep learning architecture, called OffsetNet, to accurately localize a bronchoscope in the lung in real-time. After training on only 30 minutes of recorded camera images in conserved regions of a lung phantom, OffsetNet tracks the bronchoscope's motion on a held-out recording through these same regions at an update rate of 47 Hz and an average position error of 1.4 mm. Because this model performs poorly in less conserved regions, we augment the training dataset with simulated images from these regions. To bridge the gap between camera and simulated domains, we implement domain randomization and a generative adversarial network (GAN). After training on simulated images, OffsetNet tracks the bronchoscope's motion in less conserved regions at an average position error of 2.4 mm, which meets conservative thresholds required for successful tracking.


Title: Using Augmentation to Improve the Robustness to Rotation of Deep Learning Segmentation in Robotic-Assisted Surgical Data
Key Words: data handling  learning (artificial intelligence)  medical computing  medical robotics  recurrent neural nets  robot kinematics  surgery  telerobotics  JIGSAWS dataset  data augmentation  kinematic data  surgical data science  deep learning segmentation  robotic-assisted surgical data  Robotic-Assisted Minimally Invasive Surgery  automated segmentation  data-intensive segmentation algorithms  da Vinci Research Kit  recurrent neural network  Surgery  Task analysis  Training  Robots  Kinematics  Deep learning  Robustness  Surgical Robotics: Laparoscopy  Deep Learning in Robotics and Automation  Rotation augmentation  Network generalization 
Abstract: Robotic-Assisted Minimally Invasive Surgery allows for easy recording of kinematic data, and presents excellent opportunities for data-intensive approaches to assessment of surgical skill, system design, and automation of procedures. However, typical surgical cases result in long data streams, and therefore, automated segmentation into gestures is important. The public release of the JIGSAWS dataset allowed for developing and benchmarking data-intensive segmentation algorithms. However, this dataset is small and the gestures are similar in their structure and directions. This may limit the generalization of the algorithms to real surgical data that are characterized by movements in arbitrary directions. In this paper, we use a recurrent neural network to segment a suturing task, and demonstrate one such generalization problem-limited generalization to rotation. We propose a simple augmentation that can solve this problem without collecting new data, and demonstrate its benefit using: (1) the JIGSAWS dataset, and (2) a new dataset that we recorded with a da Vinci Research Kit. Our study highlights the prospect of using data augmentation in the analysis of kinematic data in surgical data science.


Title: Deep Learning based Motion Prediction for Exoskeleton Robot Control in Upper Limb Rehabilitation
Key Words: biomechanics  electromyography  learning (artificial intelligence)  medical robotics  medical signal processing  mobile robots  motion control  patient rehabilitation  trajectory control  wearable robots  deep learning based motion prediction model  human arm dynamics  surface electromyography  deep learning model  robot arm  exoskeleton robot control  robot-assisted training  motion trajectory  8 degrees-of-freedom upper limb rehabilitation exoskeleton  NTUH-II  user motion prediction  RAT  wireless sensors  sEMG  8DoFs  Manipulators  Exoskeletons  Sensors  Deep learning  Muscles  Training 
Abstract: The synchronization of the movement between exoskeleton robot and human arm is crucial for Robot-assisted training (RAT) in upper limb rehabilitation. In this paper, we propose a deep learning based motion prediction model which is applied to our recently developed 8 degrees-of-freedom (DoFs) upper limb rehabilitation exoskeleton, named NTUH-II. The human arm dynamics and surface electromyography (sEMG) can be first measured by two wireless sensors and used as input of deep learning model to predict user's motion. Then, the prediction can be used as desired motion trajectory of the exoskeleton. As a result, the robot arm can follow the movement on either side of the user's arm in real-time. Various experiments have been conducted to verify the performance of the proposed motion prediction model, and the results show that the proposed motion prediction implementation can reduce the mean absolute error and the average delay time of movement between human arm and robot arm.


Title: Adaptive Gait Planning for Walking Assistance Lower Limb Exoskeletons in Slope Scenarios
Key Words: adaptive control  gait analysis  humanoid robots  legged locomotion  medical robotics  motion control  pendulums  robot dynamics  adaptive gait planning approach  lower-limb walking assistance exoskeletons  human-exoskeleton system  reference foot locations  adaptive gait trajectories  level ground walking  paraplegic patients  slope terrains  stepping locations  dynamic movement primitives  2D linear inverted pendulum model  dynamic gait generator  conventional capture point theory  Legged locomotion  Exoskeletons  Foot  Trajectory  Adaptation models  Force  Planning  Adaptive Gait Planning  Lower-limb Exoskeleton  LIPM  Dynamic Movement Primitives  Slope 
Abstract: Lower-limb exoskeleton has gained considerable interests in walking assistance applications for paraplegic patients. In walking assistance of paraplegic patients, the exoskeleton should have the ability to help patients to walk over different terrains in the daily life, such as slope terrains. One critical issue is how to plan the stepping locations on slopes with different gradients, and generate stable and human-like gaits for patients. This paper proposed an adaptive gait planning approach which can generate gait trajectories adapt to slopes with different gradients for lower-limb walking assistance exoskeletons. We modeled the human-exoskeleton system as a 2D Linear Inverted Pendulum Model (2D-LIPM) with an external force in the two-dimensional sagittal plane, and proposed a Dynamic Gait Generator (DGG) based on an extension of the conventional Capture Point (CP) theory and Dynamic Movement Primitives (DMPs). The proposed approach can dynamically generate reference foot locations for each step on slopes, and human-like adaptive gait trajectories can be reproduced after the learning from demonstrated trajectories that sampled from level ground walking of normal healthy human. We demonstrated the efficiency of the proposed approach on both the Gazebo simulation platform and an exoskeleton named AIDER. Experimental results indicate that the proposed approach is able to provide the ability for exoskeletons to generate appropriate gaits adapt to slopes with different gradients.


Title: Open Loop Position Control of Soft Continuum Arm Using Deep Reinforcement Learning
Key Words: bending  control engineering computing  learning (artificial intelligence)  manipulator dynamics  neural nets  numerical analysis  pneumatic actuators  position control  torsion  open loop position control  deep reinforcement learning  soft robots  nonlinear spatial deformations  inherent actuation  numerical models  soft spatial continuum arm  unidirectional bending deformation  bidirectional torsional deformation  Deep-Q Learning  continuum arm prototype  external loading conditions  Manipulators  Load modeling  Mathematical model  Numerical models  Strain 
Abstract: Soft robots undergo large nonlinear spatial deformations due to both inherent actuation and external loading. The physics underlying these deformations is complex, and often requires intricate analytical and numerical models. The complexity of these models may render traditional model-based control difficult and unsuitable. Model-free methods offer an alternative for analyzing the behavior of such complex systems without the need for elaborate modeling techniques. In this paper, we present a model-free approach for open loop position control of a soft spatial continuum arm, based on deep reinforcement learning. The continuum arm is pneumatically actuated and attains a spatial work-space by a combination of unidirectional bending and bidirectional torsional deformation. We use Deep-Q Learning with experience replay to train the system in simulation. The efficacy and robustness of the control policy obtained from the system is validated both in simulation and on the continuum arm prototype for varying external loading conditions.


Title: Fast Motion Planning for High-DOF Robot Systems Using Hierarchical System Identification
Key Words: actuators  elasticity  human-robot interaction  learning (artificial intelligence)  motion control  path planning  robot dynamics  robot kinematics  reinforcement-learning-based feedback control  underwater swimming robot  line-actuated elastic robot arm  optimization-based motion planning  hierarchical adaptive grid  forward dynamics  articulated robots  soft robots  hierarchical system identification  high-DOF robot systems  fast motion planning  Dynamics  Planning  Heuristic algorithms  Soft robotics  Finite element analysis  Computational modeling 
Abstract: We present an efficient algorithm for motion planning and controlling a robot system with a high number of degrees-of-freedom (DOF). These systems include high-DOF soft robots and articulated robots interacting with a deformable environment. We present a novel technique to accelerate the evaluations of the forward dynamics function by storing the results of costly computations in a hierarchical adaptive grid. Furthermore, we exploit the underactuated properties of the robot systems and build the grid in a low-dimensional space. Our approach approximates the forward dynamics function with guaranteed error bounds and can be used in optimization-based motion planning and reinforcement-learning-based feed-back control. We highlight the performance on two high-DOF robot systems: a line-actuated elastic robot arm and an underwater swimming robot in water. Compared to prior techniques based on exact dynamics evaluation, we observe one to two orders of magnitude improvement in the performance.


Title: Non-parametric Imitation Learning of Robot Motor Skills
Key Words: learning (artificial intelligence)  mobile robots  nonparametric imitation learning  robot motor skills  learning capabilities  learning approach  kernel treatment  human skills  correlation-adaptive imitation learning  collaborative task  Trajectory  Robots  Task analysis  Probabilistic logic  Kernel  Databases  Correlation 
Abstract: Unstructured environments impose several challenges when robots are required to perform different tasks and adapt to unseen situations. In this context, a relevant problem arises: how can robots learn to perform various tasks and adapt to different conditions? A potential solution is to endow robots with learning capabilities. In this line, imitation learning emerges as an intuitive way to teach robots different motor skills. This learning approach typically mimics human demonstrations by extracting invariant motion patterns and subsequently applies these patterns to new situations. In this paper, we propose a novel kernel treatment of imitation learning, which endows the robot with imitative and adaptive capabilities. In particular, due to the kernel treatment, the proposed approach is capable of learning human skills associated with high-dimensional inputs. Furthermore, we study a new concept of correlation-adaptive imitation learning, which allows for the adaptation of correlations exhibited in high-dimensional demonstrated skills. Several toy examples and a collaborative task with a real robot are provided to verify the effectiveness of our approach.


Title: Efficient Humanoid Contact Planning using Learned Centroidal Dynamics Prediction
Key Words: computational complexity  convex programming  humanoid robots  integer programming  legged locomotion  mobile robots  navigation  path planning  robot dynamics  learned centroidal dynamics prediction  humanoid robots  intermittent contact  contact sequence  quasistatic balance criterion  dynamic motions  efficient mixed integer convex programming solvers  dynamic contact sequences  short time horizon contact sequences  dynamic evolution  robot centroidal momenta  dynamically robust contact sequences  search-based contact planner  humanoid contact planning  Dynamics  Planning  End effectors  Legged locomotion  Humanoid robots  Optimization 
Abstract: Humanoid robots dynamically navigate an environment by interacting with it via contact wrenches exerted at intermittent contact poses. Therefore, it is important to consider dynamics when planning a contact sequence. Traditional contact planning approaches assume a quasi-static balance criterion to reduce the computational challenges of selecting a contact sequence over a rough terrain. This however limits the applicability of the approach when dynamic motions are required, such as when walking down a steep slope or crossing a wide gap. Recent methods overcome this limitation with the help of efficient mixed integer convex programming solvers capable of synthesizing dynamic contact sequences. Nevertheless, its exponential-time complexity limits its applicability to short time horizon contact sequences within small environments. In this paper, we go beyond current approaches by learning a prediction of the dynamic evolution of the robot centroidal momenta, which can then be used for quickly generating dynamically robust contact sequences for robots with arms and legs using a search-based contact planner. We demonstrate the efficiency and quality of the results of the proposed approach in a set of dynamically challenging scenarios.


Title: Autonomous Cooperative Flight of Rigidly Attached Quadcopters
Key Words: adaptive control  aircraft control  autonomous aerial vehicles  helicopters  learning (artificial intelligence)  parameter estimation  quadcopter inertial measurement units  IMU  plug and play assembly  reinforcement learning  quadcopters stable operation  autonomous flight  controller parameters  adaptive controller architecture  estimated physical attachment  short online experiments  physical structure  automatic control  online parameter estimation  rigidly attached quadcopters  autonomous cooperative flight  Propellers  Estimation  Acceleration  Parameter estimation  Adaptation models  Force  Measurement units 
Abstract: In this paper, a method for online parameter estimation and automatic control of a system of rigidly attached quadcopters is introduced. First, the method performs an estimation of the physical structure attaching the quadcopters by relying solely on information from the quadcopters' Inertial Measurement Units (IMU). This information is obtained via simple and short online experiments, allowing their plug and play assembly without any human intervention. Then, given the estimated physical attachment's parameters, a stable operation of the quadcopters is achieved via an adaptive controller architecture, where the controller parameters are obtained using Reinforcement Learning. Finally, experimental results validate the proposed method, showing that a correct estimation of the physical structure is obtained allowing the autonomous flight of a pair of attached quadcopters.


Title: The Phoenix Drone: An Open-Source Dual-Rotor Tail-Sitter Platform for Research and Education
Key Words: aerodynamics  aerospace components  aerospace robotics  aircraft control  autonomous aerial vehicles  educational robots  helicopters  microrobots  mobile robots  rotors  educational purposes  design methodology  open-source Phoenix reference design  software design  Phoenix drone  open-source dual-rotor tail-sitter platform  open-source tail-sitter microaerial vehicle platform  dual-rotor design  open-source release  design documents  high-performance tail-sitter  testing  open-source materials  aerodynamics  flight control  state estimation  Open source software  Propellers  Aerodynamics  Vehicle dynamics  Attitude control  Drones  Atmospheric modeling 
Abstract: In this paper, we introduce the Phoenix drone: the first completely open-source tail-sitter micro aerial vehicle (MAV) platform. The vehicle has a highly versatile, dual-rotor design and is engineered to be low-cost and easily extensible/modifiable. Our open-source release includes all of the design documents, software resources, and simulation tools needed to build and fly a high-performance tail-sitter for research and educational purposes.The drone has been developed for precision flight with a high degree of control authority. Our design methodology included extensive testing and characterization of the aerodynamic properties of the vehicle. The platform incorporates many off-the-shelf components and 3D-printed parts, in order to keep the cost down. Nonetheless, the paper includes results from flight trials which demonstrate that the vehicle is capable of very stable hovering and accurate trajectory tracking.Our hope is that the open-source Phoenix reference design will be useful to both researchers and educators. In particular, the details in this paper and the available open-source materials should enable learners to gain an understanding of aerodynamics, flight control, state estimation, software design, and simulation, while experimenting with a unique aerial robot.


Title: High-Bandwidth Control of Twisted String Actuators
Key Words: actuators  adaptive control  control system synthesis  feedforward  parameter estimation  position control  twisted string behavior  adaptive control methodology  TSA-based systems  online parameter estimation  outline adaptive estimation methods  variable controller gain  adaptive control architecture  high-bandwidth control  adaptive control strategies  TSA control system  mechatronics  twisted string actuators  Mathematical model  Robots  Adaptation models  Jacobian matrices  Adaptive control  Task analysis  Actuators  Tendon/Wire Mechanism  Motion Control  Learning and Adaptive Systems 
Abstract: Twisted string actuators are an emerging type of transmission systems that may benefit various applications of robotics and mechatronics. However, control of TSAs in applications that require high bandwidth has attracted comparatively little interest from research community, mainly due to complexity of twisted string behavior. This paper proposes a new adaptive control methodology that allows to sufficiently increase bandwidth of TSA-based systems. We reformulate mathematical model of the TSA into a suitable form for online parameter estimation, outline adaptive estimation methods and propose a method to design variable controller gain that rectifies nonlinearities in the system. we present experimental comparison of proposed adaptive control strategies with two conventional tsa control techniques. experimental results demonstrated that the proposed adaptive control architecture with feedforward speed term was nearly insensitive to increase in input signal frequency while reducing position tracking error by 80%. proposed algorithm can be applied in any tsa control system that has input and output signal measurements.


Title: Pose Graph optimization for Unsupervised Monocular Visual Odometry
Key Words: graph theory  neural nets  optimisation  pose estimation  unsupervised learning  pose graph optimization  unsupervised monocular visual odometry  unsupervised learning  label-free leaning ability  drift correction technique  large-scale odometry estimation  loop closure detection  hybrid VO system  NeuralBundler  temporal loss  spatial photometric loss  multiview 6DoF constraints  cycle consistency loss  global pose graph  local loop 6DoF constraints  KITTI odometry dataset  unsupervised monocular VO estimation  monocular SLAM systems  Optimization  Visual odometry  Simultaneous localization and mapping  Cameras  Training  Neural networks  Estimation 
Abstract: Unsupervised Learning based monocular visual odometry (VO) has lately drawn significant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for large-scale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler, and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efficient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.


Title: Probably Unknown: Deep Inverse Sensor Modelling Radar
Key Words: image segmentation  learning (artificial intelligence)  neural nets  object detection  optical radar  probability  radar computing  radar imaging  autonomous vehicle applications  weather conditions  raw radar power returns  sensor noise  occlusion  Inverse Sensor Model  grid map  grid cell  heteroscedastic uncertainty  deep Inverse Sensor modelling radar  sensor observation  model formulation  standard CFAR filtering approaches  dynamic urban environment  world occupancy  lidar  partial occupancy labels  deep neural network  occupancy probabilities  Uncertainty  Robot sensing systems  Laser radar  Training  Spaceborne radar  Neural networks 
Abstract: Radar presents a promising alternative to lidar and vision in autonomous vehicle applications, able to detect objects at long range under a variety of weather conditions. However, distinguishing between occupied and free space from raw radar power returns is challenging due to complex interactions between sensor noise and occlusion. To counter this we propose to learn an Inverse Sensor Model (ISM) converting a raw radar scan to a grid map of occupancy probabilities using a deep neural network. Our network is selfsupervised using partial occupancy labels generated by lidar, allowing a robot to learn about world occupancy from past experience without human supervision. We evaluate our approach on five hours of data recorded in a dynamic urban environment. By accounting for the scene context of each grid cell our model is able to successfully segment the world into occupied and free space, outperforming standard CFAR filtering approaches. Additionally by incorporating heteroscedastic uncertainty into our model formulation, we are able to quantify the variance in the uncertainty throughout the sensor observation. Through this mechanism we are able to successfully identify regions of space that are likely to be occluded.


Title: Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space
Key Words: augmented reality  convolutional neural nets  image classification  image restoration  image segmentation  learning (artificial intelligence)  mobile robots  object detection  object recognition  robot vision  augmented reality  static structure  image inpainting  dynamic-object-invariant space  vehicles  pedestrians  plausible imagery  multiclass semantic segmentation  inpainting methods  deep learning  generative adversarial model  convolutional network  vision-based robot localization  visual place recognition  Vehicle dynamics  Semantics  Task analysis  Image segmentation  Deep learning  Image reconstruction  Training 
Abstract: In this paper we present an end-to-end deep learning framework to turn images that show dynamic content, such as vehicles or pedestrians, into realistic static frames. This objective encounters two main challenges: detecting all the dynamic objects, and inpainting the static occluded background with plausible imagery. The former challenge is addressed by the use of a convolutional network that learns a multiclass semantic segmentation of the image. The second problem is approached with a conditional generative adversarial model that, taking as input the original dynamic image and its dynamic/static binary mask, is capable of generating the final static image. These generated images can be used for applications such as augmented reality or vision-based robot localization purposes. To validate our approach, we show both qualitative and quantitative comparisons against other state-of-the-art inpainting methods by removing the dynamic objects and hallucinating the static structure behind them. Furthermore, to demonstrate the potential of our results, we carry out pilot experiments that show the benefits of our proposal for visual place recognition.


Title: Autonomous Exploration, Reconstruction, and Surveillance of 3D Environments Aided by Deep Learning
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  multi-robot systems  neural nets  autonomous exploration  surveillance  greedy learning approach  supervised learning approach  level set representation  convolutional neural network  visibility  on-line computational cost  topologically accurate maps  complex 3D environments  frontier-based strategies  potential vantage points  deep learning approaches  obstacle avoidance  local navigation  global exploration problem  3D urban environments  Training  Level set  Surveillance  Three-dimensional displays  Two dimensional displays  Sensors  Convolution 
Abstract: We propose a greedy and supervised learning approach for visibility-based exploration, reconstruction and surveillance. Using a level set representation, we train a convolutional neural network to determine vantage points that maximize visibility. We show that this method drastically reduces the on-line computational cost and determines a small set of vantage points that solve the problem. This enables us to efficiently produce highly-resolved and topologically accurate maps of complex 3D environments. Unlike traditional next-best-view and frontier-based strategies, the proposed method accounts for geometric priors while evaluating potential vantage points. While existing deep learning approaches focus on obstacle avoidance and local navigation, our method aims at finding near-optimal solutions to the more global exploration problem. We present realistic simulations on 2D and 3D urban environments.


Title: GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks
Key Words: cameras  convolutional neural nets  distance measurement  image colour analysis  image motion analysis  image sequences  pose estimation  unsupervised learning  unlabelled RGB image sequences  deep convolutional Generative Adversarial Networks  single-view depth generation network  unsupervised deep VO methods  unsupervised deep monocular visual odometry  depth estimation  supervised deep learning approaches  visual odometry applications  unsupervised deep learning approaches  VO research  generative unsupervised learning framework  multiview pose estimation  6-DoF pose camera motion  Image reconstruction  Pose estimation  Cameras  Deep learning  Training  Feature extraction  Gallium nitride 
Abstract: In the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI [1] and Cityscapes [2] datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.


Title: Fast Instance and Semantic Segmentation Exploiting Local Connectivity, Metric Learning, and One-Shot Detection for Robotics
Key Words: convolutional neural nets  image segmentation  learning (artificial intelligence)  neural net architecture  object detection  robot vision  metric learning  one-shot detection  semantic scene understanding  autonomous robots  dynamic environments  instance segmentation  multitask convolutional neural network architecture  object instances  local connectivity  semantic segmentation  Semantics  Image segmentation  Decoding  Feature extraction  Task analysis  Robots  Object detection 
Abstract: Semantic scene understanding is important for autonomous robots that aim to navigate dynamic environments, manipulate objects, or interact with humans in a natural way. In this paper, we address the problem of jointly performing semantic segmentation as well as instance segmentation in an online fashion, so that autonomous robots can use this information on-the-go and without sacrificing accuracy. We achieve this by exploiting a local connectivity prior of objects in the real world and a multi-task convolutional neural network architecture. The network identifies the individual object instances and their classes without region proposals or pre-segmentation of the images into individual classes. We implemented and thoroughly evaluated our approach, and our experiments suggest that our method can be used to accurately segment instance masks of objects and identify their class in an online fashion.


Title: Priming Deep Pedestrian Detection with Geometric Context
Key Words: computational geometry  feature extraction  learning (artificial intelligence)  neural nets  object detection  pedestrians  deep neural networks  deep object detectors  geometric context  deep pedestrian detection  DNN detectors  DNN feature learning  Cameras  Detectors  Proposals  Three-dimensional displays  Two dimensional displays  Geometry  Context modeling 
Abstract: We investigate the role of geometric context in deep neural networks to establish better pedestrian detectors that are more robust to occlusions. Notwithstanding their demonstrated successes, deep object detectors under-perform in crowded scenes with high intra-category occlusions. One brute-force solution is to collect a large number of labeled training samples under occlusion, but the combinatorial increase in the labeling effort makes it an unaffordable solution. We argue that a promising and complementary direction to solve this problem is to bring geometric context to modulate feature learning in a DNN. We identify that an effective way to leverage geometric context is to induce it in two steps - through early fusion, by guiding region proposal generation to focus on occluded regions, and through late fusion, by penalizing misalignments of bounding boxes in both 2D and 3D. Our experiments on multiple state-of-the-art DNN detectors and detection benchmarks clearly demonstrates that our proposed method outperforms strong baselines by an average of 5%.


Title: Locomotion Planning through a Hybrid Bayesian Trajectory Optimization
Key Words: Bayes methods  computational complexity  Gaussian processes  humanoid robots  learning (artificial intelligence)  legged locomotion  motion control  nonlinear programming  optimal control  path planning  locomotion planning  legged systems  suitable contact schedules  contact sequence  hybrid dynamical system  achievable motions  optimal control problem  computational complexity  motion optimization  plans contacts  contact schedule selection  high-level task descriptors  motion planning nonlinear program  single-legged hopping  task appropriate contact schedules  hybrid Bayesian trajectory optimization  Bayesian optimization  Gaussian process model  bilevel optimization  Optimization  Schedules  Trajectory  Kernel  Task analysis  Robots 
Abstract: Locomotion planning for legged systems requires reasoning about suitable contact schedules. The contact sequence and timings constitute a hybrid dynamical system and prescribe a subset of achievable motions. State-of-the-art approaches cast motion planning as an optimal control problem. In order to decrease computational complexity, one common strategy separates footstep planning from motion optimization and plans contacts using heuristics. In this paper, we propose to learn contact schedule selection from high-level task descriptors using Bayesian Optimization. A bi-level optimization is defined in which a Gaussian Process model predicts the performance of trajectories generated by a motion planning nonlinear program. The agent, therefore, retains the ability to reason about suitable contact schedules, while explicit computation of the corresponding gradients is avoided. We delineate the algorithm in its general form and provide results for planning single-legged hopping. Our method is capable of learning contact schedule transitions that align with human intuition. It performs competitively against a heuristic baseline in predicting task appropriate contact schedules.


Title: Context-Dependent Compensation Scheme to Reduce Trajectory Execution Errors for Industrial Manipulators
Key Words: compensation  end effectors  industrial manipulators  mobile robots  motion control  position control  execution error  end-effector loads  general purpose automated compensation scheme  trajectory errors  learned compensation scheme  context-dependent compensation scheme  automatically generated trajectories  robot model  actuator errors  low production volume applications  reduced trajectory execution errors  Trajectory  Service robots  End effectors  Trajectory tracking  Error correction 
Abstract: Currently, automatically generated trajectories cannot be directly used on tasks that require high execution accuracies due to errors accused by inaccuracies in the robot model, actuator errors, and controller limitations. These trajectories often need manual refinement. This is not economically viable on low production volume applications. Unfortunately, execution errors are dependent on the nature of the trajectory and end-effector loads, and therefore devising a general purpose automated compensation scheme for reducing trajectory errors is not possible. This paper presents a method for analyzing the given trajectory, executing an exploratory physical run for a small portion of the given trajectory, and learning a compensation scheme based on the measured data. The learned compensation scheme is context-dependent and can be used to reduce the execution error. We have demonstrated the feasibility of this approach by conducting physical experiments.


Title: Bioinspired Direct Visual Estimation of Attitude Rates with Very Low Resolution Images using Deep Networks
Key Words: autonomous aerial vehicles  cameras  data visualisation  image resolution  image sensors  learning (artificial intelligence)  mobile robots  neural nets  robot vision  light source direction  artificial neural networks  deep networks  bioinspired visual system sensor  low resolution images  attitude rates  bioinspired direct visual estimation  source code  classical computer vision based method  learning approach  low resolution cameras  Drosophila's ocellar system  hardware setup  UAV  unmanned aerial vehicles  angular rates  Cameras  Robot sensing systems  Visualization  Estimation  Neural networks  Computer vision  Image resolution 
Abstract: In this work we present a bioinspired visual system sensor to estimate angular rates in unmanned aerial vehicles (UAV) using Neural Networks. We have conceived a hardware setup to emulate Drosophila's ocellar system, three simple eyes related to stabilization. This device is composed of three low resolution cameras with a similar spatial configuration as the ocelli. There have been previous approaches based on this ocellar system, most of them considering assumptions such as known light source direction or a punctual light source. In contrast, here we present a learning approach using Artificial Neural Networks in order to recover the system's angular rates indoors and outdoors without previous knowledge. A classical computer vision based method is also derived to be used as a benchmark for the learning approach. The method is validated with a large dataset of images (more than half a million samples) including synthetic and real data. The source code of the algorithms and the datasets used in this paper have been released in an open repository.


Title: Learning ad-hoc Compact Representations from Salient Landmarks for Visual Place Recognition in Underwater Environments
Key Words: convolutional neural nets  feature extraction  image coding  image representation  mobile robots  object recognition  robot vision  SLAM (robots)  unsupervised learning  salient landmarks  visual place recognition  underwater environments  visual attention algorithm  hand-crafted local descriptors  ad hoc descriptor generator  convolutional autoencoder  ad-hoc compact representations  SeqSLAM  FAB-MAP  SURF method  Visualization  Feature extraction  Image color analysis  Training  Robots  Task analysis  Neural networks 
Abstract: In this paper, we propose an approach to learn compact representations from salient landmarks detected by a visual attention algorithm to recognize previously visited places in underwater environments. Instead of using hand-crafted local descriptors as it has been typically done in visual place recognition, we use a convolutional autoencoder to obtain an ad hoc descriptor generator from salient landmarks. The main advantage of using an autoencoder is that it can learn in an unsupervised manner directly from the salient landmarks. In addition, we show that it is possible to do the training with less than 100,000 examples instead of several hundreds of thousands or even millions of labeled examples as in other convolutional architectures. The trained convolutional autoencoder is used to obtain descriptors for salient landmarks that are later utilized in a voting scheme to calculate similarity between images with the objective of finding if a place has already been visited. The proposed method has obtained good results compared to SeqSLAM and FAB-MAP in different datasets obtained from robotic explorations of coral reefs in real life conditions. Moreover, when the visual attention algorithm is used, fewer features are required to get a good performance in terms of precision and recall compared when using the SURF method to extract visual features.


Title: Finding divers with SCUBANet
Key Words: autonomous underwater vehicles  convolutional neural nets  gesture recognition  human-robot interaction  learning (artificial intelligence)  mobile robots  robot vision  diver-diver communication  diver-robot communication  underwater detection dataset  standard diver gestures  diver recognition  diver body-head-hand localization  CNN-based approach  SCUBANet dataset  human-robot communication  diver component recognition  robot-diver communication  human operators  divers finding  RF signal attenuation  gesture visual recognition  per-instance bounding boxes  crowd sourcing  transfer learning  Web-based interface  Standards  Training  Object detection  Robot sensing systems  Computer vision  Human-robot interaction  computer vision  object detection  dataset  underwater  robotics 
Abstract: Robot-diver communication underwater is complicated by the attenuation of RF signals, the complexities of the environment in terms of deploying interaction devices, and issues related to the cognitive loading of human operators. Humans operating underwater have developed a simple yet effective strategy for diver-diver communication based on the visual recognition of gestures. Can a similar approach be effective for diver-robot communication? Here we present experiments with SCUBANet, an underwater detection dataset of body parts associated with diver-robot communication. Given the nature of standard diver gestures, here we concentrate on diver recognition and in particular on diver body-head-hand localization and examine the feasibility of using a CNN-based approach to address this problem. Such data-driven approaches typically require an appropriately annotated dataset. The SCUBANet dataset contains images of object classes commonly encountered during human-robot communication underwater. Object classes are labeled using per-instance bounding boxes. Annotations were created through crowd sourcing via a web-based interface to ease deployment. We provide baseline performance on diver and diver component recognition and localization using transfer learning on three widely available pre-trained models.


Title: Robotic Detection of Marine Litter Using Deep Visual Detection Models
Key Words: autonomous underwater vehicles  convolutional neural nets  learning (artificial intelligence)  marine engineering  marine pollution  mobile robots  neural net architecture  object detection  robotic detection  marine litter  deep visual detection models  trash deposits  aquatic environments  marine ecosystems  autonomous underwater vehicles  AUV  deep-learning algorithms  convolutional neural network architectures  object detection  trained networks  underwater trash removal  Plastics  Training  Oceans  Data models  Object detection  Visualization  Biological system modeling 
Abstract: Trash deposits in aquatic environments have a destructive effect on marine ecosystems and pose a long-term economic and environmental threat. Autonomous underwater vehicles (AUVs) could very well contribute to the solution of this problem by finding and eventually removing trash. This paper evaluates a number of deep-learning algorithms performing the task of visually detecting trash in realistic underwater environments, with the eventual goal of exploration, mapping, and extraction of such debris by using AUVs. A large and publicly-available dataset of actual debris in open-water locations is annotated for training a number of convolutional neural network architectures for object detection. The trained networks are then evaluated on a set of images from other portions of that dataset, providing insight into approaches for developing the detection capabilities of an AUV for underwater trash removal. In addition, the evaluation is performed on three different platforms of varying processing power, which serves to assess these algorithms' fitness for real-time applications.


Title: Deep Local Trajectory Replanning and Control for Robot Navigation
Key Words: collision avoidance  control engineering computing  learning (artificial intelligence)  mobile robots  motion control  navigation  velocity control  robot navigation  hierarchical planning  machine learning  optimal paths  deep local trajectory planner  velocity controller  motion commands  attention mechanisms  nearby pedestrians  map global plan information  sensor data  velocity commands  hand-designed traditional navigation system  deep local trajectory replanning  global planner  Navigation  Robot kinematics  Trajectory  Planning  Robot sensing systems  Laser radar 
Abstract: We present a navigation system that combines ideas from hierarchical planning and machine learning. The system uses a traditional global planner to compute optimal paths towards a goal, and a deep local trajectory planner and velocity controller to compute motion commands. The latter components of the system adjust the behavior of the robot through attention mechanisms such that it moves towards the goal, avoids obstacles, and respects the space of nearby pedestrians. Both the structure of the proposed deep models and the use of attention mechanisms make the system's execution interpretable. Our simulation experiments suggest that the proposed architecture outperforms baselines that try to map global plan information and sensor data directly to velocity commands. In comparison to a hand-designed traditional navigation system, the proposed approach showed more consistent performance.


Title: Learning from Transferable Mechanics Models: Generalizable Online Mode Detection in Underactuated Dexterous Manipulation
Key Words: control engineering computing  dexterous manipulators  grippers  Jacobian matrices  manipulator dynamics  manipulator kinematics  random forests  supervised learning  transferable mechanics models  generalizable online mode detection  underactuated dexterous manipulation  mechanics-inspired framework  fingertip-based planar within-hand manipulation  underactuated robotic gripper  hand-object system  grasp matrix  manipulability metrics  planar manipulation modes  supervised learning model  contact curvatures  prediction transferability  visual approach  gripper models  finger Jacobians  random forests classifier  Grippers  Transmission line matrix methods  Feature extraction  Jacobian matrices  Kinematics  Robot sensing systems 
Abstract: In this work, we investigate a mechanics-inspired framework for describing fingertip-based planar within-hand manipulation with an underactuated robotic gripper. In particular, this framework leverages fundamental mechanics properties of the hand-object system, including basic terms such as local contact curvature as well as more complex features including the grasp matrix and manipulability metrics. These are extracted using a simple visual approach and then in real-time used for predicting planar manipulation modes: namely rolling, dropped, stuck, and sliding. Given a desired cartesian motion for the object, a supervised learning model predicts these four manipulation modes before they occur, allowing us to either avoid or trigger these different behaviors. Since we utilize strictly fundamental properties of the grasp matrix, finger Jacobians, and contact curvatures, we are able to demonstrate prediction transferability between different grippers using our original classifier. In particular, a Random Forests classifier trained on one gripper successfully predicts manipulation modes for grippers with different fingers with 84% accuracy, compared to just 56% from an approach in previous work. Overall, we find that the features designed in our approach better describes fingertip manipulation when precise gripper models are not available.


Title: A Framework for Robot Manipulation: Skill Formalism, Meta Learning and Adaptive Control
Key Words: adaptive control  learning systems  manipulators  adaptive control  adaptive impedance control  meta parameter learning  compatible skill specifications  abstract expert knowledge  quality evaluation metrics  adaptive impedance controller  carefully defined skill formalism  manipulation tasks  learned tasks  learning-based solution  learning force-sensitive robot manipulation skills  submillimeter industrial tolerances  time 20.0 min  Robots  Impedance  Task analysis  Trajectory  Reinforcement learning  Complexity theory  Visualization 
Abstract: In this paper we introduce a novel framework for expressing and learning force-sensitive robot manipulation skills. It is based on a formalism that extends our previous work on adaptive impedance control with meta parameter learning and compatible skill specifications. This way the system is also able to make use of abstract expert knowledge by incorporating process descriptions and quality evaluation metrics. We evaluate various state-of-the-art schemes for meta parameter learning and experimentally compare selected ones. Our results clearly indicate that the combination of our adaptive impedance controller with a carefully defined skill formalism significantly reduces the complexity of manipulation tasks even for learning peg-in-hole with submillimeter industrial tolerances. Overall, the considered system is able to learn variations of this skill in under 20 minutes. In fact, experimentally the system was able to perform the learned tasks without visual feedback faster than humans, leading to the first learning-based solution of complex assembly at such real-world performance.


Title: Learning Action Representations for Self-supervised Visual Exploration
Key Words: cameras  image representation  learning (artificial intelligence)  learning action  self-supervised visual exploration  on-board camera  initial state  self-supervised prediction network  intrinsic rewards  current state-action pair  higher dimensional representations  representational power  transition network  sparse extrinsic rewards  camera view  input actions  action representation module  Training  Navigation  Task analysis  Visualization  Robots  Predictive models  Cameras 
Abstract: Learning to efficiently navigate an environment using only an on-board camera is a difficult task for an agent when the final goal is far from the initial state and extrinsic rewards are sparse. To address this problem, we present a self-supervised prediction network to train the agent with intrinsic rewards that relate to achieving the desired final goal. The network learns to predict its future camera view (the future state) from a current state-action pair through an Action Representation Module that decodes input actions as higher dimensional representations. To increase the representational power of the network during exploration we fuse the responses from the Action Representation Module in the transition network, which predicts the future state. Moreover, to enhance the discrimination capability between predictions from different input actions we introduce joint regression and triplet ranking loss functions. We show that, despite the sparse extrinsic rewards, by learning action representations we achieve a faster training convergence than state-of-the-art methods with only a small increase in the number of the model parameters.


Title: Plug-and-Play: Improve Depth Prediction via Sparse Data Propagation
Key Words: image colour analysis  image segmentation  learning (artificial intelligence)  optical radar  PnP module updates  intermediate feature map  sparse data propagation  RGB image  sparse LiDAR points  plug-and-play module  sparse depths  pretrained depth prediction model  dense depth map  Estimation  Training  Laser radar  Image reconstruction  Predictive models  Robot sensing systems 
Abstract: We propose a novel plug-and-play (PnP) module for improving depth prediction with taking arbitrary patterns of sparse depths as input. Given any pre-trained depth prediction model, our PnP module updates the intermediate feature map such that the model outputs new depths consistent with the given sparse depths. Our method requires no additional training and can be applied to practical applications such as leveraging both RGB and sparse LiDAR points to robustly estimate dense depth map. Our approach achieves consistent improvements on various state-of-the-art methods on indoor (i.e., NYU-v2) and outdoor (i.e., KITTI) datasets. Various types of LiDARs are also synthesized in our experiments to verify the general applicability of our PnP module in practice.


Title: DFNet: Semantic Segmentation on Panoramic Images with Dynamic Loss Weights and Residual Fusion Block
Key Words: feature extraction  image fusion  image segmentation  neural nets  object detection  roads  traffic engineering computing  visual perception  sight images  DFNet  dynamic loss weights  fusion layer  boundary information loss  semantic segmentation  automatic parking  lane markings  parking slots  pavement information  pixel multiplication  PSV dataset  residual fusion block  RFB  Image segmentation  Semantics  Feature extraction  Deep learning  Convolution  Training  Vehicle dynamics 
Abstract: For the domain of self-driving and automatic parking, perception is a basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. Compared with front sight images, panoramic images(PI) can capture more comprehensive pavement information. However, the imbalance of different classes in PI is even more serious. Additionally, the judgment of boundary information between areas is a hard problem in deep models. Therefore, we propose a new model named DFNet to solve these problems. The proposed model has two main contributions, one is dynamic loss weights, and the other is residual fusion block(RFB). DFNet use dynamic loss weights to overcome the negative effect of imbalance dataset, which are calculated according to the pixel number of each class in a batch. RFB is composed of several convolutional layers, a pooling layer, and a fusion layer to combine the feature maps by pixel multiplication, which can reduce boundary information loss. We evaluate our method on PSV dataset, and the achieved advanced results demonstrate the effectiveness of the proposed model.


Title: Anytime Stereo Image Depth Estimation on Mobile Devices
Key Words: learning (artificial intelligence)  mobile computing  stereo image processing  end-to-end learned approach  inference time  mobile devices  stereo depth estimation  memory-constrained devices  disparity prediction  disparity maps  computational constraints  stereo image depth estimation  NVIDIA Jetson TX2 module  AnyNet  Estimation  Image resolution  Feature extraction  Computational modeling  Three-dimensional displays  Cameras  Predictive models 
Abstract: Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process 1242×375 resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error - using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet.


Title: Improved Generalization of Heading Direction Estimation for Aerial Filming Using Semi-Supervised Regression
Key Words: autonomous aerial vehicles  cinematography  image sequences  regression analysis  robot vision  unsupervised learning  video signal processing  improved generalization  semisupervised regression  visual input  data distributions  semisupervised algorithm  generalization ability  autonomous aerial filming  heading direction estimation problem  temporal continuity  unsupervised signal  testing performance  unlabeled sequences  performance improvement  labeled loss  unlabeled loss  moving actor filming  Task analysis  Drones  Training  Estimation  Data models  Cameras  Feature extraction 
Abstract: In the task of Autonomous aerial filming of a moving actor (e.g. a person or a vehicle), it is crucial to have a good heading direction estimation for the actor from the visual input. However, the models obtained in other similar tasks, such as pedestrian collision risk analysis and human-robot interaction, are very difficult to generalize to the aerial filming task, because of the difference in data distributions. Towards improving generalization with less amount of labeled data, this paper presents a semi-supervised algorithm for heading direction estimation problem. We utilize temporal continuity as the unsupervised signal to regularize the model and achieve better generalization ability. This semi-supervised algorithm is applied to both training and testing phases, which increases the testing performance by a large margin. We show that by leveraging unlabeled sequences, the amount of labeled data required can be significantly reduced. We also discuss several important details on improving the performance by balancing labeled and unlabeled loss, and making good combinations. Experimental results show that our approach robustly outputs the heading direction for different types of actor. The aesthetic value of the video is also improved in the aerial filming task.


Title: On the Impact of Uncertainty for Path Planning
Key Words: graph theory  learning (artificial intelligence)  mobile robots  navigation  path planning  probability  travelling salesman problems  path planning  planning paths  uncertain edge  learned classifier  mobile robots  partially-known environments  simulation campaign  real-world maps  planning strategy  traversability estimates  Canadian traveller problem  Robot sensing systems  Navigation  Uncertainty  Path planning  Estimation  Optimized production technology 
Abstract: We consider the problem of planning paths on graphs with some edges whose traversability is uncertain; for each uncertain edge, we are given a probability of being traversable (e.g., by a learned classifier). We categorize different interpretations of the problem that are meaningful for mobile robots navigating partially-known environments, each of which yields a different formalization; we then focus on the case in which the true traversability of an edge is revealed only when the agent visits one of its endpoints (Canadian Traveller Problem). In this context, we design a large simulation campaign on synthetic and real-world maps to study the impact of two different factors: the planning strategy, and the amount of uncertainty (which could depend on the quality of the classifier producing traversability estimates).


Title: Night-to-Day Image Translation for Retrieval-based Localization
Key Words: approximation theory  image retrieval  learning (artificial intelligence)  neural nets  object detection  pose estimation  robot vision  SLAM (robots)  visual databases  geo-tagged images  night-to-day image translation  retrieval-based localization  visual localization  robotics pipelines  image retrieval techniques  database image retrieval  neural models  pose estimation  visual query photo  ToDayGAN  Task analysis  Visualization  Cameras  Feature extraction  Training  Generators  Data models 
Abstract: Visual localization is a key step in many robotics pipelines, allowing the robot to (approximately) determine its position and orientation in the world. An efficient and scalable approach to visual localization is to use image retrieval techniques. These approaches identify the image most similar to a query photo in a database of geo-tagged images and approximate the query's pose via the pose of the retrieved database image. However, image retrieval across drastically different illumination conditions, e.g. day and night, is still a problem with unsatisfactory results, even in this age of powerful neural models. This is due to a lack of a suitably diverse dataset with true correspondences to perform end-to-end learning. A recent class of neural models allows for realistic translation of images among visual domains with relatively little training data and, most importantly, without ground-truth pairings.In this paper, we explore the task of accurately localizing images captured from two traversals of the same area in both day and night. We propose ToDayGAN - a modified image-translation model to alter nighttime driving images to a more useful daytime representation. We then compare the daytime and translated night images to obtain a pose estimate for the night image using the known 6-DOF position of the closest day image. Our approach improves localization performance by over 250% compared the current state-of-the-art, in the context of standard metrics in multiple categories.


Title: Deep Reinforcement Learning of Navigation in a Complex and Crowded Environment with a Limited Field of View
Key Words: cameras  learning (artificial intelligence)  mobile robots  navigation  neural nets  optical radar  path planning  robot vision  deep reinforcement learning-based methods  DRL agents  LSTM agent  Local-Map Critic  LSTM-LMC  wide FOV  single depth camera  mobile robots  lidar devices  DRL method  depth cameras  dynamics randomization technique  Navigation  Mobile robots  Reinforcement learning  Laser radar  Robot sensing systems  Cameras 
Abstract: Mobile robots are required to navigate freely in a complex and crowded environment in order to provide services to humans. For this navigation ability, deep reinforcement learning (DRL)-based methods are gaining increasing attentions. However, existing DRL methods require a wide field of view (FOV), which imposes the usage of high-cost lidar devices. In this paper, we explore the possibility of replacing expensive lidar devices with affordable depth cameras which have a limited FOV. First, we analyze the effect of a limited field of view in the DRL agents. Second, we propose a LSTM agent with Local-Map Critic (LSTM-LMC), which is a novel DRL method to learn efficient navigation in a complex environment with a limited FOV. Lastly, we introduce the dynamics randomization technique to improve the robustness of the DRL agents in the real world. We found that our method with a limited FOV can outperform the methods having a wide FOV but limited memory. We provide the empirical evidence that our method learns to implicitly model the surrounding environment and dynamics of other agents. We also show that a robot with a single depth camera can navigate through a complex real-world environment using our method.


Title: Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics
Key Words: learning (artificial intelligence)  robot dynamics  robust control  robustified controller  robotic tasks  complex dynamics  robot tasks  deep reinforcement learning  simulated environment  learned task  fine-tuning  simulation parameters  nontrivial task  nonrobustified controller  sim-to-real transfer learning  robustified controllers  Task analysis  Robots  Physics  Games  Reinforcement learning  Training  Computational modeling 
Abstract: Learning robot tasks or controllers using deep reinforcement learning has been proven effective in simulations. Learning in simulation has several advantages. For example, one can fully control the simulated environment, including halting motions while performing computations. Another advantage when robots are involved, is that the amount of time a robot is occupied learning a task-rather than being productive-can be reduced by transferring the learned task to the real robot. Transfer learning requires some amount of fine-tuning on the real robot. For tasks which involve complex (non-linear) dynamics, the fine-tuning itself may take a substantial amount of time. In order to reduce the amount of fine-tuning we propose to learn robustified controllers in simulation. Robustified controllers are learned by exploiting the ability to change simulation parameters (both appearance and dynamics) for successive training episodes. An additional benefit for this approach is that it alleviates the precise determination of physics parameters for the simulator, which is a non-trivial task. We demonstrate our proposed approach on a real setup in which a robot aims to solve a maze game, which involves complex dynamics due to static friction and potentially large accelerations. We show that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.


Title: Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight
Key Words: aircraft control  autonomous aerial vehicles  collision avoidance  data analysis  helicopters  learning (artificial intelligence)  mobile robots  robot vision  vision-based autonomous flight  fragile scale quadrotors  small-scale quadrotors  complex physics  air currents  hybrid deep reinforcement learning algorithm  generalizable perception system  nanoaerial vehicle collision avoidance task  real data  simulated data  Data models  Robots  Task analysis  Predictive models  Neural networks  Reinforcement learning  Collision avoidance 
Abstract: Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS.


Title: Crowd-Robot Interaction: Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning
Key Words: human-robot interaction  learning (artificial intelligence)  mobile robots  neural nets  pedestrians  crowded spaces  Human-Human interactions  deep reinforcement learning framework  dense crowds  human dynamics  crowd-aware robot navigation  attention-based deep reinforcement learning  robot operations  crowd-robot interaction  Robots  Navigation  Reinforcement learning  Planning  Task analysis  Human-robot interaction  Biological system modeling 
Abstract: Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a self-attention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot's anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.


Title: Residual Reinforcement Learning for Robot Control
Key Words: continuous systems  control system synthesis  feedback  friction  industrial robots  learning (artificial intelligence)  learning systems  mechanical contact  motion control  robot dynamics  first-order physical modeling  brittle controllers  inaccurate controllers  reinforcement learning methods  continuous robot controllers  control signals  robot control problems  modern manufacturing  control design  feedback control methods  control policy  residual reinforcement learning  rigid body equations of motion  contacts  friction  robot learning  unstable objects  block assembly task  Robots  Task analysis  Feedback control  Reinforcement learning  Mathematical model  Manufacturing  Adaptive control 
Abstract: Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.


Title: A Reinforcement Learning Approach for Control of a Nature-Inspired Aerial Vehicle
Key Words: autonomous aerial vehicles  function approximation  gradient methods  learning (artificial intelligence)  neural nets  position control  three-term control  nature-inspired aerial vehicle  position controller  UAV  fixed-wing aircraft  neural network function approximators  reinforcement learning agent  learned controller  deep deterministic policy gradients  PID controller  Ape-X distributed prioritized experience replay  multi-rotors  underactuated nature-inspired unmanned aerial vehicle  body contrary  Training  Aerodynamics  Neural networks  Mathematical model  Reinforcement learning  Drag  Prototypes 
Abstract: In this work, reinforcement learning is used to develop a position controller for an underactuated nature-inspired Unmanned Aerial Vehicle (UAV). This particular configuration of UAVs achieves lift by spinning its entire body contrary to standard multi-rotors or fixed-wing aircraft. Deep Deterministic Policy Gradients (DDPG) with Ape-X Distributed Prioritized Experience Replay was used to train neural network function approximators that were implemented as the final control policy. The reinforcement learning agent was trained in simulations and directly ported over to real-life hardware. Position control tests were performed on the learned control policy and compared to a baseline PID controller. The learned controller was found to exhibit better control over the inherent oscillations that arise from the non-linear dynamics of the platform.


Title: Formal Policy Learning from Demonstrations for Reachability Properties
Key Words: closed loop systems  feedback  learning (artificial intelligence)  predictive control  reachability analysis  formal behavioral specifications  counterexample-guided iterative loop  receding horizon model-predictive controllers  demonstrator actions  formally-verified policies  formal policy learning from demonstrations  reachability properties  structured closed-loop policies  MPC  cost-to-go function  Robots  Trajectory  Training data  Predictive models  Real-time systems  Measurement 
Abstract: We consider the problem of learning structured, closed-loop policies (feedback laws) from demonstrations in order to control under-actuated robotic systems, so that formal behavioral specifications such as reaching a target set of states are satisfied. Our approach uses a “counterexample-guided” iterative loop that involves the interaction between a policy learner, a demonstrator and a verifier. The learner is responsible for querying the demonstrator in order to obtain the training data to guide the construction of a policy candidate. This candidate is analyzed by the verifier and either accepted as correct, or rejected with a counterexample. In the latter case, the counterexample is used to update the training data and further refine the policy.The approach is instantiated using receding horizon model-predictive controllers (MPCs) as demonstrators. Rather than using regression to fit a policy to the demonstrator actions, we extend the MPC formulation with the gradient of the cost-to-go function evaluated at sample states in order to constrain the set of policies compatible with the behavior of the demonstrator. We demonstrate the successful application of the resulting policy learning schemes on two case studies and we show how simple, formally-verified policies can be inferred starting from a complex and unverified nonlinear MPC implementations. As a further benefit, the policies are many orders of magnitude faster to implement when compared to the original MPCs.


Title: FastDepth: Fast Monocular Depth Estimation on Embedded Systems
Key Words: autonomous aerial vehicles  cameras  computational complexity  embedded systems  estimation theory  image colour analysis  image segmentation  image sensors  learning (artificial intelligence)  microrobots  mobile robots  neural nets  object detection  robot vision  low-latency decoder  NYU Depth v2 dataset  real-time monocular depth estimation  deep neural network  embedded platform  microaerial vehicle  embedded systems  robotic tasks  obstacle detection  single RGB image  monocular cameras  lightweight encoder-decoder network architecture  computational complexity  FastDepth  fast monocular depth estimation  depth sensing  deep neural networks  Estimation  Decoding  Neural networks  Runtime  Convolution  Complexity theory  Task analysis 
Abstract: Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.


Title: Lifelong Learning for Heterogeneous Multi-Modal Tasks
Key Words: learning (artificial intelligence)  pattern classification  heterogeneous modalities  learned classifier  multimodal task  multimodal lifelong learning framework  consecutive multimodal learning tasks  multimodal lifelong learning problem  heterogeneous multimodal tasks  heterogeneous multimodal fusion  material recognition task  online dictionary learning algorithm  Task analysis  Dictionaries  Knowledge based systems  Machine learning  Encoding  Optimization  Robots 
Abstract: In this work, we investigate the lifelong learning problem from the viewpoint of heterogeneous multi-modal fusion. The main challenges come from the fact that the common representation between heterogeneous modalities should be persistently learned and the learned classifier for each multi-modal task should be persistently updated. To address this problem, we construct a multi-modal lifelong learning framework which deals with the consecutive multi-modal learning tasks and develop an efficient online dictionary learning algorithm to solve the multi-modal lifelong learning problem. Finally, we perform experimental validation on a complicated material recognition task and show the promising results.


Title: Nonlinear System Identification of Soft Robot Dynamics Using Koopman Operator Theory
Key Words: autoregressive processes  identification  learning (artificial intelligence)  mean square error methods  mobile robots  neural nets  nonlinear control systems  nonlinear dynamical systems  pneumatic actuators  regression analysis  robot dynamics  state-space methods  soft robot dynamics  Koopman operator theory  large-scale data collection  system identification method  nonlinear dynamical systems  linear regression  linear representation  infinite-dimensional space  nonlinear system identification methods  dynamic model  pneumatic soft robot arm  linear state space model  nonlinear Hammerstein-Wiener model  neural network  total normalized-root-mean-square error  NRMSE  Soft robotics  Nonlinear dynamical systems  Predictive models  Linear regression  Data models  Tuning 
Abstract: Soft robots are challenging to model due in large part to the nonlinear properties of soft materials. Fortunately, this softness makes it possible to safely observe their behavior under random control inputs, making them amenable to large-scale data collection and system identification. This paper implements and evaluates a system identification method based on Koopman operator theory in which models of nonlinear dynamical systems are constructed via linear regression of observed data by exploiting the fact that every nonlinear system has a linear representation in the infinite-dimensional space of real-valued functions called observables. The approach does not suffer from some of the shortcomings of other nonlinear system identification methods, which typically require the manual tuning of training parameters and have limited convergence guarantees. A dynamic model of a pneumatic soft robot arm is constructed via this method, and used to predict the behavior of the real system. The total normalized-root-mean-square error (NRMSE) of its predictions is lower than that of several other identified models including a neural network, NLARX, nonlinear Hammerstein-Wiener, and linear state space model.


Title: Data Driven Inverse Kinematics of Soft Robots using Local Models
Key Words: computational complexity  data visualisation  learning (artificial intelligence)  mobile robots  path planning  robot kinematics  data driven inverse kinematics  soft robot  computational approaches  data quantity  memory complexity  time complexity  actuation system  visual markers  motion planning  motion control  learning cube environment  Shape  Soft robotics  Sensors  Kinematics  Calibration  Computational modeling 
Abstract: Soft robots are advantageous in terms of flexibility, safety and adaptability. It is challenging to find efficient computational approaches for planning and controlling their motion. This work takes a direct data-driven approach to learn the kinematics of the three-dimensional shape of a soft robot, by using visual markers. No prior information about the robot at hand is required. The model is oblivious to the design of the robot and type of actuation system. This allows adaptation to erroneous manufacturing. We present a highly versatile and inexpensive learning cube environment for collecting and analysing data. We prove that using multiple, lower order models of data opposed to one global, higher order model, will reduce the required data quantity, time complexity and memory complexity significantly without compromising accuracy. Further, our approach allows for embarrassingly parallelism. Yielding an overall much more simple and efficient approach.


Title: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation
Key Words: motion estimation  pose estimation  SLAM (robots)  photometric loss  self-supervised ego-motion estimation  accurate relative pose  SLAM  self-supervised learning framework  image depth  photometric error  systematic error  realistic scenes  geometric loss  matching loss  self-supervised framework  unsupervised egomotion estimation methods  Simultaneous localization and mapping  Estimation  Geometry  Cameras  Motion estimation  Visualization  Visual odometry 
Abstract: Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised egomotion estimation methods by a large margin. The code and data are available at https://github.com/hlzz/DeepMatchVO.


Title: Vision-based Control of a Quadrotor in User Proximity: Mediated vs End-to-End Learning Approaches
Key Words: autonomous aerial vehicles  helicopters  learning (artificial intelligence)  robot vision  state estimation  vision-based control  quadrotor  onboard camera  control signals  high-level state estimation  learning approaches  Task analysis  Drones  Robot sensing systems  Training  Cameras  Computer architecture 
Abstract: We consider the task of controlling a quadrotor to hover in front of a freely moving user, using input data from an onboard camera. On this specific task we compare two widespread learning paradigms: a mediated approach, which learns a high-level state from the input and then uses it for deriving control signals; and an end-to-end approach, which skips high-level state estimation altogether. We show that despite their fundamental difference, both approaches yield equivalent performance on this task. We finally qualitatively analyze the behavior of a quadrotor implementing such approaches.


Title: Activity recognition in manufacturing: The roles of motion capture and sEMG+inertial wearables in detecting fine vs. gross motion
Key Words: assembling  feature extraction  human-robot interaction  image classification  image motion analysis  industrial robots  manufacturing systems  production engineering computing  sensor fusion  wearable computers  unimodal sensor data  UCSD-MIT Human Motion dataset  Vicon motion capture system  gross motion recognition  sensor modalities  sEMG  human activity recognition  inertial wearables  fine motion detection  manufacturing  safety-critical environments  assembly tasks  HAR algorithms  Robot sensing systems  Task analysis  Wearable sensors  Grasping  Automotive engineering  Tracking 
Abstract: In safety-critical environments, robots need to reliably recognize human activity to be effective and trust-worthy partners. Since most human activity recognition (HAR) approaches rely on unimodal sensor data (e.g. motion capture or wearable sensors), it is unclear how the relationship between the sensor modality and motion granularity (e.g. gross or fine) of the activities impacts classification accuracy. To our knowledge, we are the first to investigate the efficacy of using motion capture as compared to wearable sensor data for recognizing human motion in manufacturing settings. We introduce the UCSD-MIT Human Motion dataset, composed of two assembly tasks that entail either gross or fine-grained motion. For both tasks, we compared the accuracy of a Vicon motion capture system to a Myo armband using three widely used HAR algorithms. We found that motion capture yielded higher accuracy than the wearable sensor for gross motion recognition (up to 36.95%), while the wearable sensor yielded higher accuracy for fine-grained motion (up to 28.06%). These results suggest that these sensor modalities are complementary, and that robots may benefit from systems that utilize multiple modalities to simultaneously, but independently, detect gross and fine-grained motion. Our findings will help guide researchers in numerous fields of robotics including learning from demonstration and grasping to effectively choose sensor modalities that are most suitable for their applications.


Title: Redundant Perception and State Estimation for Reliable Autonomous Racing
Key Words: automobiles  driver information systems  image colour analysis  learning (artificial intelligence)  particle filtering (numerical methods)  pose estimation  SLAM (robots)  state estimation  vehicle dynamics  reliable autonomous racing  sensor failure  critical consequences  state estimation approaches  autonomous race car  track delimiting objects  sensor modalities  learning-based approaches  camera data  redundant perception inputs  probabilistic failure detection algorithm  real-world racing conditions  slip dynamics  particle filter based SLAM algorithm  pose estimates  velocity 90.0 km/h  Robot sensing systems  Cameras  Three-dimensional displays  Automobiles  Image color analysis  Laser radar  Reliability 
Abstract: In autonomous racing, vehicles operate close to the limits of handling and a sensor failure can have critical consequences. To limit the impact of such failures, this paper presents the redundant perception and state estimation approaches developed for an autonomous race car. Redundancy in perception is achieved by estimating the color and position of the track delimiting objects using two sensor modalities independently. Specifically, learning-based approaches are used to generate color and pose estimates, from LiDAR and camera data respectively. The redundant perception inputs are fused by a particle filter based SLAM algorithm that operates in real-time. Velocity is estimated using slip dynamics, with reliability being ensured through a probabilistic failure detection algorithm. The sub-modules are extensively evaluated in real-world racing conditions using the autonomous race car gotthard driverless, achieving lateral accelerations up to 1. 7G and a top speed of 90km/h.


Title: A Simple Adaptive Tracker with Reminiscences
Key Words: convex programming  gradient methods  learning (artificial intelligence)  object detection  object tracking  VOT benchmark dataset  OTB benchmark dataset  gradient-based convex optimization  MTCF  appearance changes  adaptive tracker  temporal windows  base trackers  ensemble method  visual object tracking  correlation filters  reminiscences  visual appearance  video history  Correlation  Visualization  Object tracking  History  Standards  Convolution  Training 
Abstract: Correlation filters have provided exceptional results in the field of visual object tracking in the past few years. However, these methods typically learn a single filter to be robust to many different appearance changes, which can be challenging. We propose a simple solution to this problem by utilizing an ensemble method of base trackers trained on different temporal windows of the video history. The proposed tracker, called MTCF, exhibits the following features: i) it can be trained using gradient-based convex optimization; ii) it is robust to short-term and long-term changes in visual appearance. MTCF performs on par with or outperforms state-of-the-art trackers on the OTB and the VOT benchmark datasets. We present an extensive analysis of the performance of MTCF on these benchmark datasets.


Title: Learning-driven Coarse-to-Fine Articulated Robot Tracking
Key Words: computer vision  edge detection  image colour analysis  image sequences  learning (artificial intelligence)  manipulators  object tracking  coarse-to-fine articulated robot tracking  articulated tracking approach  robotic manipulators  visual cues  subpixel-level accurate correspondences  discriminative depth information  coarse-to-fine articulated state estimator  robot state distribution  depth image  Schunk SDH2 hand interacting  edge tracking  depth keypoints  articulated model fitting  colour edge correspondences  RGB-D sequences  KUICA LWR arm  palm position estimation  Visualization  Optimization  Manipulators  Image edge detection  Two dimensional displays  Robot sensing systems 
Abstract: In this work we present an articulated tracking approach for robotic manipulators, which relies only on visual cues from colour and depth images to estimate the robot's state when interacting with or being occluded by its environment. We hypothesise that articulated model fitting approaches can only achieve accurate tracking if subpixel-level accurate correspondences between observed and estimated state can be established. Previous work in this area has exclusively relied on either discriminative depth information or colour edge correspondences as tracking objective and required initialisation from joint encoders. In this paper we propose a coarse-to-fine articulated state estimator, which relies only on visual cues from colour edges and learned depth keypoints, and which is initialised from a robot state distribution predicted from a depth image. We evaluate our approach on four RGB-D sequences showing a KUICA LWR arm with a Schunk SDH2 hand interacting with its environment and demonstrate that this combined keypoint and edge tracking objective can estimate the palm position with an average error of 2. 5cm without using any joint encoder sensing.


Title: 2D LiDAR Map Prediction via Estimating Motion Flow with GRU
Key Words: feature extraction  image sequences  learning (artificial intelligence)  motion estimation  optical radar  radar imaging  recurrent neural nets  LiDAR-FlowNet model  motion flow based method  2D LiDAR map prediction  optical flow  recurrent neural network  motion flow estimation  gated recurrent unit  robotics navigation  path planning  Laser radar  Two dimensional displays  Dynamics  Logic gates  Recurrent neural networks  Training  Robot sensing systems 
Abstract: It is a significant problem to predict the 2D LiDAR map at next moment for robotics navigation and path-planning. To tackle this problem, we resort to the motion flow between adjacent maps, as motion flow is a powerful tool to process and analyze the dynamic data, which is named optical flow in video processing. However, unlike video, which contains abundant visual features in each frame, a 2D LiDAR map lacks distinctive local features. To alleviate this challenge, we propose to estimate the motion flow based on deep neural networks inspired by its powerful representation learning ability in estimating the optical flow of the video. To this end, we design a recurrent neural network based on gated recurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can encode the temporal dynamic information, our LiDAR-FlowNet can estimate motion flow between the current map and the unknown next map only from the current frame and previous frames. A self-supervised strategy is further designed to train the LiDAR-FlowNet model effectively, while no training data need to be manually annotated. With the estimated motion flow, it is straightforward to predict the 2D LiDAR map at the next moment. Experimental results verify the effectiveness of our LiDAR-FlowNet as well as the proposed training strategy. The results of the predicted LiDAR map also show the advantages of our motion flow based method.


Title: Robot eye-hand coordination learning by watching human demonstrations: a task function approximation approach
Key Words: feedback  function approximation  learning (artificial intelligence)  manipulators  robot vision  visual servoing  human demonstrations  task function approximation approach  robot eye-hand coordination learning method  visual task specification  inverse reinforcement learning  learned reward model  uncalibrated visual servoing  hand-engineered task specification  traditional UVS controller  learned policy  robot platforms  Task analysis  Robot kinematics  Videos  Training  Visualization  Visual servoing 
Abstract: We present a robot eye-hand coordination learning method that can directly learn visual task specification by watching human demonstrations. Task specification is represented as a task function, which is learned using inverse reinforcement learning(IRL [1]) by inferring a reward model from state transitions. The learned reward model is then used as continuous feedbacks in an uncalibrated visual servoing(UVS [2]) controller designed for the execution phase. Our proposed method can directly learn from raw videos, which removes the need for hand-engineered task specification. Benefiting from the use of a traditional UVS controller, the training on real robot only happens at initial Jacobian estimation which takes an average of 4-7 seconds for a new task. Besides, the learned policy is independent from a particular robot, thus has the potential of fast adapting to other robot platforms. Various experiments were designed to show that, for a task with certain DOFs, our method can adapt to task/environment changes in target positions, backgrounds, illuminations, and occlusions.


Title: Modeling and Analysis of Motion Data from Dynamically Positioned Vessels for Sea State Estimation
Key Words: convolutional neural nets  data analysis  fast Fourier transforms  feature extraction  learning (artificial intelligence)  marine engineering  position control  recurrent neural nets  sensitivity analysis  sensor fusion  ships  state estimation  time series  long dependency  ship motion data  convolutional neural network  frequency features  feature fusion layer  raw time series data  hand-engineered features  sensitivity analysis method  data preprocessing  ship motion dataset  SeaStateNet  sea state estimation  dynamically positioned vessels  autonomous ship  deep neural network model  time-invariant feature extraction  long-short-term memory recurrent neural network  fast Fourier transform block  Sea state  Marine vehicles  Time series analysis  Estimation  Sensors  Feature extraction  Data models 
Abstract: Developing a reliable model to identify the sea state is significant for the autonomous ship. This paper introduces a novel deep neural network model (SeaStateNet) to estimate the sea state based on the ship motion data from dynamically positioned vessels. The SeaStateNet mainly consists of three components: an Long-Short-Term Memory (LSTM) recurrent neural network to capture the long dependency in the ship motion data; a convolutional neural network (CNN) to extract time-invariant features; and a Fast Fourier Transform (FFT) block to extract frequency features. A feature fusion layer is designed to learn the degree affected by each component. The proposed model is applied directly to the raw time series data, without needing of any hand-engineered features. A sensitivity analysis (SA) method is applied to assess the influence of data preprocessing. Through benchmark test and experiment on ship motion dataset, SeaStateNet is verified effective for sea state estimation. The investigation on real-time test further shows the practicality of the proposed model.


Title: Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning
Key Words: decision making  interactive systems  learning (artificial intelligence)  multi-agent systems  neural nets  probability  hyperparameter values  adversarial learning  interaction-aware multiagent tracking  probabilistic behavior prediction  motion planning  intelligent systems  multiple interactive agents  distribution learning  decision making  generative adversarial network  Generators  Gallium nitride  Predictive models  Training  Generative adversarial networks  State estimation  Optimization 
Abstract: In order to enable high-quality decision making and motion planning of intelligent systems such as robotics and autonomous vehicles, accurate probabilistic predictions for surrounding interactive objects is a crucial prerequisite. Although many research studies have been devoted to making predictions on a single entity, it remains an open challenge to forecast future behaviors for multiple interactive agents simultaneously. In this work, we take advantage of the Generative Adversarial Network (GAN) due to its capability of distribution learning and propose a generic multi-agent probabilistic prediction and tracking framework which takes the interactions among multiple entities into account, in which all the entities are treated as a whole. However, since GAN is very hard to train, we make an empirical research and present the relationship between training performance and hyperparameter values with a numerical case study. The results imply that the proposed model can capture both the mean, variance and multi-modalities of the groundtruth distribution. Moreover, we apply the proposed approach to a real-world task of vehicle behavior prediction to demonstrate its effectiveness and accuracy. The results illustrate that the proposed model trained by adversarial learning can achieve a better prediction performance than other state-of-the-art models trained by traditional supervised learning which maximizes the data likelihood. The well-trained model can also be utilized as an implicit proposal distribution for particle filtered based Bayesian state estimation.


Title: Quantifying the Reality Gap in Robotic Manipulation Tasks
Key Words: image motion analysis  manipulators  mobile robots  robot vision  robotic manipulation tasks  Kinova robotic manipulator  motion capture system  manipulation-oriented robotic tasks  robotic reaching task  robotic interaction task  quantitative data  Physics  Engines  Task analysis  Hardware  Manipulators  Computational modeling 
Abstract: We quantify the accuracy of various simulators compared to a real world robotic reaching and interaction task. Simulators are used in robotics to design solutions for real world hardware without the need for physical access. The `reality gap' prevents solutions developed or learnt in simulation from performing well, or at all, when transferred to real-world hardware. Making use of a Kinova robotic manipulator and a motion capture system, we record a ground truth enabling comparisons with various simulators, and present quantitative data for various manipulation-oriented robotic tasks. We show the relative strengths and weaknesses of numerous contemporary simulators, highlighting areas of significant discrepancy, and assisting researchers in the field in their selection of appropriate simulators for their use cases.


Title: Augmenting Action Model Learning by Non-Geometric Features
Key Words: Gaussian processes  grippers  human-robot interaction  learning (artificial intelligence)  manipulators  mixture models  mobile robots  teaching  action model learning  nongeometric features  manipulation actions  action-induced reactions  measured liquid levels  explicit case dependent programming  external features  dynamic system  action imitation  geometric trajectory  real-world robot experiments  Gaussian mixture model representation  Robots  Trajectory  Task analysis  Liquids  Force measurement  Grippers  Dynamics 
Abstract: Learning from demonstration is a powerful tool for teaching manipulation actions to a robot. It is, however, an unsolved problem how to consider knowledge about the world and action-induced reactions such as forces imposed onto the gripper or measured liquid levels during pouring without explicit and case dependent programming. In this paper, we present a novel approach to include such knowledge directly in form of measured features. To this end, we use action demonstrations together with external features to learn a motion encoded by a dynamic system in a Gaussian Mixture Model (GMM) representation. Accordingly, during action imitation, the system is able to couple the geometric trajectory of the motion to measured features in the scene. We demonstrate the feasibility of our approach with a broad range of external features in real-world robot experiments including a drinking, a handover and a pouring task.


Title: Skill Acquisition via Automated Multi-Coordinate Cost Balancing
Key Words: convex programming  intelligent robots  learning (artificial intelligence)  learning framework  MCCB  point-to-point movement skills  multiple differential coordinates  local geometric properties  convex optimization problem  multicoordinate cost function  complex skill datasets  skill acquisition  automated multicoordinate cost balancing  Trajectory  Laplace equations  Cost function  Task analysis  Encoding  Robots 
Abstract: We propose a learning framework, named Multi-Coordinate Cost Balancing (MCCB), to address the problem of acquiring point-to-point movement skills from demonstrations. MCCB encodes demonstrations simultaneously in multiple differential coordinates that specify local geometric properties. MCCB generates reproductions by solving a convex optimization problem with a multi-coordinate cost function and linear constraints on the reproductions, such as initial, target, and via points. Further, since the relative importance of each coordinate system in the cost function might be unknown for a given skill, MCCB learns optimal weighting factors that balance the cost function. We demonstrate the effectiveness of MCCB via detailed experiments conducted on one handwriting dataset and three complex skill datasets.


Title: Real-time Multisensory Affordance-based Control for Adaptive Object Manipulation
Key Words: haptic interfaces  learning (artificial intelligence)  manipulators  robot vision  adaptive object manipulation  RMAC  multisensory inputs  real-time multisensory affordance-based control  affordance models  Robot sensing systems  Hidden Markov models  Trajectory  Adaptation models  Real-time systems  Data models 
Abstract: We address the challenge of how a robot can adapt its actions to successfully manipulate objects it has not previously encountered. We introduce Real-time Multisensory Affordance-based Control (RMAC), which enables a robot to adapt existing affordance models using multisensory inputs. We show that using the combination of haptic, audio, and visual information with RMAC allows the robot to learn afforance models and adaptively manipulate two very different objects (drawer, lamp), in multiple novel configurations. Offline evaluations and real-time online evaluations show that RMAC allows the robot to accurately open different drawer configurations and turn-on novel lamps with an average accuracy of 75%.


Title: Learning Behavior Trees From Demonstration
Key Words: control engineering computing  decision trees  learning (artificial intelligence)  robot programming  LfD methods  decision trees  household cleaning task  Robotic Learning from Demonstration  primitive actions  Fetch robot  human teaching  robot programming  behavior trees  Task analysis  Decision trees  Service robots  Hidden Markov models  Real-time systems  Games 
Abstract: Robotic Learning from Demonstration (LfD) allows anyone, not just experts, to program a robot for an arbitrary task. Many LfD methods focus on low level primitive actions such as manipulator trajectories. Complex multistep task with many primitive actions must be learned from demonstration if LfD is to encompass the full range of task a user may desire. Existing methods represent the high level task in various forms including, finite state machines, decision trees, formal logic, among others. Behavior trees are proposed as an alternative representation of high level task. Behavior trees are an execution model for the control of a robot designed for real time execution, modularity, and, consequently, transparency. Real time execution allows the robot to reactively perform the task. Modularity allows the reuse of learned primitive actions and high level task in new situations, speeding up the process of learning in new scenarios. Transparency allows users to understand and interactively modify the learned model. Behavior trees are used to represent high level tasks by building on the relationship it has with decision trees. We demonstrate a human teaching our Fetch robot a household cleaning task.


Title: Leveraging Temporal Reasoning for Policy Selection in Learning from Demonstration
Key Words: graph theory  learning by example  probability  temporal reasoning  policy selection  temporal reasoning model  Allen's interval algebra  sequential relations  parallel temporal relations  probabilistic inference  temporal context graph  learning from demonstration  perceptual aliasing  Task analysis  Hidden Markov models  Cognition  Context modeling  Robots  Algebra  Probabilistic logic 
Abstract: High-level human activities often have rich temporal structures that determine the order in which atomic actions are executed. We propose the Temporal Context Graph (TCG), a temporal reasoning model that integrates probabilistic inference with Allen's interval algebra, to capture these temporal structures. TCGs are capable of modeling tasks with cyclical atomic actions and consisting of sequential and parallel temporal relations. We present Learning from Demonstration as the application domain where the use of TCGs can improve policy selection and address the problem of perceptual aliasing. Experiments validating the model are presented for learning two tasks from demonstration that involve structured human-robot interactions. The source code for this implementation is available at https://github.com/AssistiveRoboticsUNH/TCG.


Title: Imitating Human Search Strategies for Assembly
Key Words: collision avoidance  end effectors  human-robot interaction  learning (artificial intelligence)  manipulator dynamics  mobile robots  motion control  position control  robot vision  alignment tasks  human demonstrations  state invariant dynamics model  exploration distribution  search trajectory  deterministic ergodic control  position domains  superposed forces  learnt strategy  3D electricity socket task  search task  human search strategies  Trajectory  Task analysis  Robot sensing systems  Search problems  Sockets  Impedance 
Abstract: We present a Learning from Demonstration method for teaching robots to perform search strategies imitated from humans in scenarios where alignment tasks fail due to position uncertainty. The method utilizes human demonstrations to learn both a state invariant dynamics model and an exploration distribution that captures the search area covered by the demonstrator. We present two alternative algorithms for computing a search trajectory from the exploration distribution, one based on sampling and another based on deterministic ergodic control. We augment the search trajectory with forces learnt through the dynamics model to enable searching both in force and position domains. An impedance controller with superposed forces is used for reproducing the learnt strategy. We experimentally evaluate the method on a KUKA LWR4+ performing a 2D peg-in-hole and a 3D electricity socket task. Results show that the proposed method can, with only few human demonstrations, learn to complete the search task.


Title: Learning Robust Manipulation Skills with Guided Policy Search via Generative Motor Reflexes
Key Words: learning (artificial intelligence)  manipulators  neural nets  search problems  trajectory control  robust manipulation skills  control policies  complex manipulation tasks  high-dimensional neural networks  robot actions  real-world trajectory samples  resulting neural networks  policy representation  robust actions  broader state space  state-dependent motor reflex  similar motor reflexes  real-world manipulation tasks  guided policy search  generative motor reflexes map states  state-action policies  Neural networks  Trajectory  Robots  Robustness  Reinforcement learning  Space exploration  Training 
Abstract: Guided Policy Search enables robots to learn control policies for complex manipulation tasks efficiently. Therein, the control policies are represented as high-dimensional neural networks which derive robot actions based on states. However, due to the small number of real-world trajectory samples in Guided Policy Search, the resulting neural networks are only robust in the neighbourhood of the trajectory distribution explored by real-world interactions. In this paper, we present a new policy representation called Generative Motor Reflexes, which is able to generate robust actions over a broader state space compared to previous methods. In contrast to prior state-action policies, Generative Motor Reflexes map states to parameters for a state-dependent motor reflex, which is then used to derive actions. Robustness is achieved by generating similar motor reflexes for many states. We evaluate the presented method in simulated and real-world manipulation tasks, including contact-rich peg-in-hole tasks. Using these evaluation tasks, we show that policies represented as Generative Motor Reflexes lead to robust manipulation skills also outside the explored trajectory distribution with less training needs compared to previous methods.


Title: Incremental Learning of Spatial-Temporal Features in Human Motion Patterns with Mixture Model for Planning Motion of a Collaborative Robot in Assembly Lines
Key Words: assembling  industrial robots  learning (artificial intelligence)  mobile robots  motion control  product quality  productivity  spatial-temporal features  human motion patterns  collaborative robot  assembly lines  incremental learning system  planning motion  products quality  workers behavior  Task analysis  Hidden Markov models  Mixture models  Predictive models  Data models  Trajectory  Adaptation models 
Abstract: Collaborative robots are expected to work in cooperation with humans to improve productivity and maintain the quality of products. In the previous study, we have proposed an incremental learning system for adaptively scheduling a motion of the collaborative robot based on a worker's behavior. Although this system could model the worker's motion pattern precisely and robustly without collecting the worker's data in advance, it required two different models for modeling the worker's spatial and temporal features respectively and was not well considered for generalization. In this paper, we extend the previous incremental learning system by integrating the spatial and temporal models using a mixture model. In addition, we install a new incremental learning algorithm which improves a generalization capability of the mixture model and avoids overfitting in the situation where the prior information is limited. Implementing the proposed algorithm, we evaluate the effectiveness of the proposed system by experiments for several workers and for several assembly processes.


Title: Learning Quickly to Plan Quickly Using Modular Meta-Learning
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  path planning  multiobject manipulation problems  continuous state  continuous operator parameters  state description  discrete parameters  single specializer  modular meta-learning approach  action spaces  3D pick-and-place tasks  Task analysis  Planning  Robots  Skeleton  Search problems  Companies  Neural networks 
Abstract: Multi-object manipulation problems in continuous state and action spaces can be solved by planners that search over sampled values for the continuous parameters of operators. The efficiency of these planners depends critically on the effectiveness of the samplers used, but effective sampling in turn depends on details of the robot, environment, and task. Our strategy is to learn functions called speciatizers that generate values for continuous operator parameters, given a state description and values for the discrete parameters. Rather than trying to learn a single specializer for each operator from large amounts of data on a single task, we take a modular meta-learning approach. We train on multiple tasks and learn a variety of specializers that, on a new task, can be quickly adapted using relatively little data - thus, our system learns quickly to plan quickly using these specializers. We validate our approach experimentally in simulated 3D pick-and-place tasks with continuous state and action spaces. Visit http://tinyurl.com/chitnis-icra-19 for a supplementary video.


Title: Deep Multi-Sensory Object Category Recognition Using Interactive Behavioral Exploration
Key Words: convolutional neural nets  learning (artificial intelligence)  object recognition  recurrent neural nets  robot vision  multisensory object category recognition  interactive behavioral exploration  deep learning methodology  visual data  haptic sensory data  haptic data  auditory data  sensory modality  visual information  dominant modality  auditory networks  convolutional neural networks  haptic networks  tensor-train gated recurrent unit network  robot category recognition  Haptic interfaces  Robot sensing systems  Visualization  Convolution  Network architecture  Neural networks 
Abstract: When identifying an object and its properties, humans use features from multiple sensory modalities produced when manipulating the object. Motivated by this cognitive process, we propose a deep learning methodology for object category recognition which uses visual, auditory, and haptic sensory data coupled with exploratory behaviors (e.g., grasping, lifting, pushing, etc.). In our method, as the robot performs an action on an object, it uses a Tensor-Train Gated Recurrent Unit network to process its visual data, and Convolutional Neural Networks to process haptic and auditory data. We propose a novel strategy to train a single neural network that inputs video, audio and haptic data, and demonstrate that its performance is better than separate neural networks for each sensory modality. The proposed method was evaluated on a dataset in which the robot explored 100 different objects, each belonging to one of 20 categories. While the visual information was the dominant modality for most categories, adding the additional haptic and auditory networks further improves the robot's category recognition accuracy. For some of the behaviors, our approach outperforms the previous published baseline for the dataset which used handcrafted features for each modality. We also show that a robot does not need the sensory data from the entire interaction, but instead can make a good prediction early on during behavior execution.


Title: Discontinuity-Sensitive Optimal Control Learning by Mixture of Experts
Key Words: approximation theory  function approximation  iterative methods  learning (artificial intelligence)  neural nets  nonlinear control systems  optimal control  optimisation  discontinuity-sensitive optimal control learning  machine learning method  parametric input  problem parameters  optimal solutions  problem-optimum map  discrete homotopy classes  control switching  MoE  standard neural networks  dynamic vehicle control problems  nonlinear optimal control problems  function approximators  mixture of experts model  trajectory prediction  Trajectory  Training  Optimal control  Neural networks  Optimization  Standards  Predictive models 
Abstract: This paper proposes a machine learning method to predict the solutions of related nonlinear optimal control problems given some parametric input, such as the initial state. The map between problem parameters to optimal solutions is called the problem-optimum map, and is often discontinuous due to nonconvexity, discrete homotopy classes, and control switching. This causes difficulties for traditional function approximators such as neural networks, which assume continuity of the underlying function. This paper proposes a mixture of experts (MoE) model composed of a classifier and several regressors, where each regressor is tuned to a particular continuous region. A novel training approach is proposed that trains classifier and regressors independently. MoE greatly outperforms standard neural networks, and achieves highly reliable trajectory prediction (over 99.5% accuracy) in several dynamic vehicle control problems.


Title: Wormhole Learning
Key Words: cameras  image colour analysis  image sensors  infrared detectors  learning (artificial intelligence)  object detection  object detector  invariance properties  visible-light RGB camera  infrared sensor  temporary sensor  infrared detector  RGB-inferred labels  infrared-inferred labels  transfer learning  wormhole learning  pretrained RGB detector  Detectors  Task analysis  Robot sensing systems  Cameras  Lighting  Training  Mutual information 
Abstract: Typically, to enlarge the operating domain of an object detector, more labeled training data is required. We describe a method called wormhole learning, which allows to extend the operating domain without additional data, but only with temporary access to an auxiliary sensor with certain invariance properties. We describe the instantiation of this principle with a regular visible-light RGB camera as the main sensor, and an infrared sensor as the temporary sensor. We start with a pre-trained RGB detector; then we train the infrared detector based on the RGB-inferred labels; finally we re-train the RGB detector based on the infrared-inferred labels. After these two transfer-learning steps, the RGB detector has enlarged its operating domain by inheriting part of the invariance to illumination of the infrared sensor; in particular, the RGB detector is now able to see much better at night. We analyze the wormhole learning phenomenon by bounding the possible gain in accuracy using mutual information properties of the two sensors and considered operating domain.


Title: Position control of medical cable-driven flexible instruments by combining machine learning and kinematic analysis
Key Words: cables (mechanical)  control engineering computing  endoscopes  flexible manipulators  learning (artificial intelligence)  manipulator kinematics  medical computing  medical robotics  position control  robot kinematics  surgery  medical cable-driven flexible instruments  machine learning  kinematic analysis  cable transmissions  medical endoscopic systems  flexible medical robotic systems  open-loop accuracy  Position Inverse Kinematic Model  off-line learning  learning process  STRAS medical robot  position control  kinematic models  hysteresis effects  Instruments  Hysteresis motors  Hysteresis  Training  Kinematics  Robot sensing systems 
Abstract: Non-linearities in cable transmissions are important limitations for the accurate control of flexible instruments used in medical endoscopic systems. Hysteresis effects greatly impact the accuracy of conventional kinematic models. This is especially critical for implementing automatic motions in flexible medical robotic systems. In this paper, we propose a method for improving open-loop accuracy of flexible instruments by implementing a Position Inverse Kinematic Model which is able to take into account hysteresis effects. In order to avoid complex physical modeling, the method relies on the off-line learning of the behavior of the instruments. Basic knowledge of the kinematic is also incorporated in the learning process in order to make it fast. The validity of the approach is demonstrated by the execution of 2D and 3D trajectories with the instruments of the STRAS medical robot. The accuracy is shown to be significantly improved with respect to other learning-based methods.


Title: Online Learning for Proactive Obstacle Avoidance with Powered Transfemoral Prostheses
Key Words: adaptive control  collision avoidance  kinematics  medical control systems  prosthetics  regression analysis  regression model  kinematic data  obstacle avoidance system  obstacle avoidance success rates  prosthetic limb  adaptive system  powered prosthetic limbs  stumble recovery systems  powered prostheses  direct knee control  mechanically-passive transfemoral prosthetic limbs  powered transfemoral prostheses  proactive obstacle avoidance  online learning  amputee subject  obstacle negotiation success rate  trip avoidance system  nonamputee subject  Collision avoidance  Trajectory  Knee  Prosthetics  Legged locomotion  Sensitivity  Feature extraction 
Abstract: Avoiding obstacles poses a significant challenge for amputees using mechanically-passive transfemoral prosthetic limbs due to their lack of direct knee control. In contrast, powered prostheses can potentially improve obstacle avoidance via their ability to add energy to the system. In past work, researchers have proposed stumble recovery systems for powered prosthetic limbs that provide assistance in the event of a trip. However, these systems only aid recovery after an obstacle has disrupted the user's gait and do not proactively help the amputee avoid obstacles. To address this problem, we designed an adaptive system that learns online to use kinematic data from the prosthetic limb to detect the user's obstacle avoidance intent in early swing. When the system detects an obstacle, it alters the planned swing trajectory to help avoid trips. Additionally, the system uses a regression model to predict the required knee flexion angle for the trip response. We validated the system by comparing obstacle avoidance success rates with and without the obstacle avoidance system. For a non-amputee subject wearing the prosthesis through an adapter, the trip avoidance system improved the obstacle negotiation success rate from 37% to 89%, while an amputee subject improved his success rate from 35% to 71% when compared to utilizing minimum jerk trajectories for the knee and ankle joints.


Title: Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments
Key Words: autonomous underwater vehicles  geophysical image processing  image representation  object detection  oceanographic techniques  probability  robot vision  SLAM (robots)  unsupervised learning  co-robotic exploration  bandwidth tunable technique  real-time probabilistic scene modeling  communication constrained environments  deep sea  scene complexity  bandwidth requirements  underwater robot  high-level semantic scene constructs  artificially constructed tank environment  science interests  unsupervised scene model impact  resulting scene model  coral reef  bandwidth constraints  scene maps streaming  Robot sensing systems  Bandwidth  Oceans  Data models  Visualization  Bayes methods 
Abstract: This paper proposes a bandwidth tunable technique for real-time probabilistic scene modeling and mapping to enable co-robotic exploration in communication constrained environments such as the deep sea. The parameters of the system enable the user to characterize the scene complexity represented by the map, which in turn determines the bandwidth requirements. The approach is demonstrated using an underwater robot that learns an unsupervised scene model of the environment and then uses this scene model to communicate the spatial distribution of various high-level semantic scene constructs to a human operator. Preliminary experiments in an artificially constructed tank environment as well as simulated missions over a 10m×10m coral reef using real data show the tunability of the maps to different bandwidth constraints and science interests. To our knowledge this is the first paper to quantity how the free parameters of the unsupervised scene model impact both the scientific utility of and bandwidth required to communicate the resulting scene model.


Title: UWStereoNet: Unsupervised Learning for Depth Estimation and Color Correction of Underwater Stereo Imagery
Key Words: cameras  computerised instrumentation  feature extraction  geophysical image processing  image colour analysis  image matching  image resolution  image restoration  light propagation  neural nets  oceanographic techniques  spatial variables measurement  stereo image processing  unsupervised learning  visual perception  unsupervised learning  stereo cameras  navigation  underwater robotic systems  constrained camera geometry  feature detection  underwater light propagation lead  deep learning  underwater image restoration  unsupervised deep neural network  input raw color underwater stereo imagery  color corrected imagery  underwater image formation  image processing techniques  depth estimation  stereo vision algorithms  disparity estimation  DNN  Image color analysis  Estimation  Image restoration  Cameras  Attenuation  Stereo vision  Deep learning 
Abstract: Stereo cameras are widely used for sensing and navigation of underwater robotic systems. They can provide high resolution color views of a scene; the constrained camera geometry enables metrically accurate depth estimation; they are also relatively cost-effective. Traditional stereo vision algorithms rely on feature detection and matching to enable triangulation of points for estimating disparity. However, for underwater applications, the effects of underwater light propagation lead to image degradation, reducing image quality and contrast. This makes it especially challenging to detect and match features, especially from varying viewpoints. Recently, deep learning has shown success in end-to-end learning of dense disparity maps from stereo images. Still, many state-of-the-art methods are supervised and require ground truth depth or disparity, which is challenging to gather in subsea environments. Simultaneously, deep learning has also been applied to the problem of underwater image restoration. Again, it is difficult or impossible to gather real ground truth data for this problem. In this work, we present an unsupervised deep neural network (DNN) that takes input raw color underwater stereo imagery and outputs dense depth maps and color corrected imagery of underwater scenes. We leverage a model of the process of underwater image formation, image processing techniques, as well as the geometric constraints inherent to the stereo vision problem to develop a modular network that outperforms existing methods.


Title: A Framework for On-line Learning of Underwater Vehicles Dynamic Models
Key Words: marine navigation  mobile robots  regression analysis  robot dynamics  support vector machines  tracking  underwater vehicles  vehicle dynamics  on-line learning  underwater vehicles dynamic models  accurate tracking controllers  navigation algorithms  high fidelity performance  robot dynamics  incremental support vector regression method  Robots  Vehicle dynamics  Adaptation models  Computational modeling  Heuristic algorithms  Support vector machines  Data models 
Abstract: Learning the dynamics of robots from data can help achieve more accurate tracking controllers, or aid their navigation algorithms. However, when the actual dynamics of the robots change due to external conditions, on-line adaptation of their models is required to maintain high fidelity performance. In this work, a framework for on-line learning of robot dynamics is developed to adapt to such changes. The proposed framework employs an incremental support vector regression method to learn the model sequentially from data streams. In combination with the incremental learning, strategies for including and forgetting data are developed to obtain better generalization over the whole state space. The framework is tested in simulation and real experimental scenarios demonstrating its adaptation capabilities to changes in the robot's dynamics.


Title: Incorporating End-to-End Speech Recognition Models for Sentiment Analysis
Key Words: emotion recognition  human-robot interaction  learning (artificial intelligence)  natural language processing  pattern classification  recurrent neural nets  speech recognition  text analysis  linguistic modality  expressed emotion  spoken text  ground-truth transcriptions  speech recognition mistakes  automatic speech recognition output  character-level recurrent neural network  sentiment recognition  acoustic modality  binary sentiment classification task  emotion recognition  synergistic effect  auditory transcribed text  transcribed text  sentiment intensity  end-to-end speech recognition models  sentiment analysis  ASR systems  multimodal corpus of sentiment intensity  MOSI  Acoustics  Feature extraction  Speech recognition  Training  Computational modeling  Computer architecture  Emotion recognition 
Abstract: Previous work on emotion recognition demonstrated a synergistic effect of combining several modalities such as auditory, visual, and transcribed text to estimate the affective state of a speaker. Among these, the linguistic modality is crucial for the evaluation of an expressed emotion. However, manually transcribed spoken text cannot be given as input to a system practically. We argue that using ground-truth transcriptions during training and evaluation phases leads to a significant discrepancy in performance compared to real-world conditions, as the spoken text has to be recognized on the fly and can contain speech recognition mistakes. In this paper, we propose a method of integrating an automatic speech recognition (ASR) output with a character-level recurrent neural network for sentiment recognition. In addition, we conduct several experiments investigating sentiment recognition for human-robot interaction in a noise-realistic scenario which is challenging for the ASR systems. We quantify the improvement compared to using only the acoustic modality in sentiment recognition. We demonstrate the effectiveness of this approach on the Multimodal Corpus of Sentiment Intensity (MOSI) by achieving 73,6% accuracy in a binary sentiment classification task, exceeding previously reported results that use only acoustic input. In addition, we set a new state-of-the-art performance on the MOSI dataset (80.4% accuracy, 2% absolute improvement).


Title: Improved Optical Flow for Gesture-based Human-robot Interaction
Key Words: feature extraction  gesture recognition  human-robot interaction  neural nets  service robots  gesture interaction  human motion  gesture-based human-robot interaction  house service robot  practical robot applications  gesture recognition methods  optical flow estimation method  speed-accuracy trade-off  deep learning-based methods  feature extractors  midway features  Estimation  Optical imaging  Feature extraction  Gesture recognition  Robots  Optical fiber networks  Human-robot interaction 
Abstract: Gesture interaction is a natural way of communicating with a robot as an alternative to speech. Gesture recognition methods leverage optical flow in order to understand human motion. However, while accurate optical flow estimation (i.e., traditional) methods are costly in terms of runtime, fast estimation (i.e., deep learning) methods' accuracy can be improved. In this paper, we present a pipeline for gesture-based human-robot interaction that uses a novel optical flow estimation method in order to achieve an improved speed-accuracy trade-off. Our optical flow estimation method introduces four improvements to previous deep learning-based methods: strong feature extractors, attention to contours, midway features, and a combination of these three. This results in a better understanding of motion, and a finer representation of silhouettes. In order to evaluate our pipeline, we generated our own dataset, MIBURI, which contains gestures to command a house service robot. In our experiments, we show how our method improves not only optical flow estimation, but also gesture recognition, offering a speed-accuracy trade-off more realistic for practical robot applications.


Title: Decentralization of Multiagent Policies by Learning What to Communicate
Key Words: decentralised control  multi-agent systems  multi-robot systems  neurocontrollers  optimisation  multiagent policies  agent architecture  training methodology  neural networks  task-oriented communication semantics  communication-unaware expert policy  perimeter defense game  communication constraints  collaborative tasks  optimization  decentralization  Neural networks  Pins  Task analysis  Training  Games  Computer architecture  Decoding 
Abstract: Effective communication is required for teams of robots to solve sophisticated collaborative tasks. In practice it is typical for both the encoding and semantics of communication to be manually defined by an expert; this is true regardless of whether the behaviors themselves are bespoke, optimization based, or learned. We present an agent architecture and training methodology using neural networks to learn task-oriented communication semantics based on the example of a communication-unaware expert policy. A perimeter defense game illustrates the system's ability to handle dynamically changing numbers of agents and its graceful degradation in performance as communication constraints are tightened or the expert's observability assumptions are broken.


Title: Acquisition of Word-Object Associations from Human-Robot and Human-Human Dialogues
Key Words: human-robot interaction  knowledge acquisition  learning (artificial intelligence)  robot programming  human-robot dialogues  word-object associations  human-human dialogues  robotic system  AI agents  cross-situational learning  Instruction-based word learning  Robots  Semantics  Syntactics  Learning systems  Linguistics  Education  Computer architecture 
Abstract: Past work on acquisition of word-object associations in robots has focused on either fast instruction-based methods which accept highly constrained input or gradual cross-situational learning methods, but not a mixture of both. In this paper, we present an integrated robotic system which allows for a combination of these methods to contribute to the task of learning the labels of objects in AI agents. We demonstrate the expanded word learning capabilities in the outcome system and how learning from both human-human and human-robot dialogues can be achieved in one integrated system.


Title: Security-Aware Synthesis of Human-UAV Protocols
Key Words: autonomous aerial vehicles  command and control systems  control engineering computing  formal verification  learning (artificial intelligence)  military aircraft  stochastic games  security-aware synthesis  human-UAV protocols  collaboration protocols  human-unmanned aerial vehicle  geolocation task  stochastic game-based model  stealthy false-data injection attacks  collected experimental data  human-UAV coalition  H-UAV protocol synthesis  human operators  UAV hidden-information constraint  RESCHU-SA testbed  geolocation strategies  model checkers  command and control systems  Games  Task analysis  Protocols  Geology  Stochastic processes  Security  Global Positioning System 
Abstract: In this work, we synthesize collaboration protocols for human-unmanned aerial vehicle (H-UAV) command and control systems, where the human operator aids in securing the UAV by intermittently performing geolocation tasks to confirm its reported location. We first present a stochastic game-based model for the system that accounts for both the operator and an adversary capable of launching stealthy false-data injection attacks, causing the UAV to deviate from its path. We also describe a synthesis challenge due to the UAV's hidden-information constraint. Next, we perform human experiments using a developed RESCHU-SA testbed to recognize the geolocation strategies that operators adopt. Furthermore, we deploy machine learning techniques on the collected experimental data to predict the correctness of a geolocation task at a given location based on its geographical features. By representing the model as a delayed-action game and formalizing the system objectives, we utilize off-the-shelf model checkers to synthesize protocols for the human-UAV coalition that satisfy these objectives. Finally, we demonstrate the usefulness of the H-UAV protocol synthesis through a case study where the protocols are experimentally analyzed and further evaluated by human operators.


Title: Underwater Communication Using Full-Body Gestures and Optimal Variable-Length Prefix Codes
Key Words: convolutional neural nets  decoding  encoding  gesture recognition  learning (artificial intelligence)  mobile robots  multi-robot systems  protocols  variable length codes  optimal variable-length prefix codes  interrobot communication  action sequences  swimming robot  communication protocol  whole-body gestures  radio-denied environments  passive communication  full-body gestures  underwater communication  robot gesture execution  classical decoding methods  observer robot  convolutional network  natural activity  Robots  Encoding  Decoding  Three-dimensional displays  Visualization  Heuristic algorithms  Target tracking 
Abstract: In this paper we consider inter-robot communication in the context of joint activities. In particular, we focus on convoying and passive communication for radio-denied environments by using whole-body gestures to provide cues regarding future actions. We develop a communication protocol whereby information described by codewords is transmitted by a series of actions executed by a swimming robot. These action sequences are chosen to optimize robustness and transmission duration given the observability, natural activity of the robot and the frequency of different messages. Our approach uses a convolutional network to make core observations of the pose of the robot being tracked, which is sending messages. The observer robot then uses an adaptation of classical decoding methods to infer a message that is being transmitted. The system is trained and validated using simulated data, tested in the pool and is targeted for deployment in the open ocean. Our decoder achieves.94 precision and.66 recall on real footage of robot gesture execution recorded in a swimming pool.


Title: Learning Recursive Bayesian Nonparametric Modeling of Moving Targets via Mobile Decentralized Sensors
Key Words: Bayes methods  Gaussian processes  learning (artificial intelligence)  recursive estimation  sensor fusion  mobile decentralized sensors  multisensor applications  GP recursive fusion law  recursive DPGP fusion approach  data fusion  Gaussian processes  recursive Bayesian nonparametric modeling  Dirichlet process  Sensor fusion  Computational modeling  Gaussian processes  Time measurement  Kinematics  Velocity measurement 
Abstract: Bayesian nonparametric models, such as the Dirichlet Process Gaussian Process (DPGP), have been shown very effective at learning models of dynamic targets exclusively from data. Previous work on batch DPGP learning and inference, however, ceases to be efficient in multi-sensor applications that require decentralized measurements to be obtained sequentially over time. Batch processing, in this case, leads to redundant computations that may hinder online applicability. This paper develops a recursive approach for DPGP learning and inference in which a novel Dirichlet Process prior based on Wasserstein metric is used for measuring the similarity between multiple Gaussian Processes (GPs). Combined with the GP recursive fusion law, the proposed recursive DPGP fusion approach enables efficient online data fusion. The problem of active sensing for recursive DPGP learning and inference is also investigated by uncertainty reduction via expected mutual information. Simulation and experimental results show that the proposed approach successfully learns the models of moving targets and outperforms existing benchmark methods.


Title: Sound-Indicated Visual Object Detection for Robotic Exploration
Key Words: acoustic signal detection  audio signal processing  microphones  mobile robots  neural net architecture  object detection  source separation  supervised learning  robotic exploration  microphones  cameras  physical world  visual modalities  audio modalities  robotic platforms  robotic sound-indicated visual object detection framework  two-stream weakly-supervised deep learning architecture  sounding object localization  AudioSet  Visualization  Object detection  Robots  Feature extraction  Task analysis  Semantics  Deep learning 
Abstract: Robots are usually equipped with microphones and cameras to perceive and understand the physical world. Though visual object detection technology has achieved great success, the detection in other modalities remains unsolved. In this paper, we establish a novel robotic sound-indicated visual object detection framework, and develop a two-stream weakly-supervised deep learning architecture to connect the visual and audio modalities for localizing the sounding object. A dataset is constructed from the AudioSet to validate the proposed method and some promising applications are demonstrated on robotic platforms.


Title: HG-DAgger: Interactive Imitation Learning with Human Experts
Key Words: learning (artificial intelligence)  HG-DAgger  interactive imitation learning  behavioral cloning  data mismatch  DAgger algorithm  sampling schemes  action labels  autonomous driving task  corrective actions  Safety  Cloning  Training  Measurement  Trajectory  Task analysis 
Abstract: Imitation learning has proven to be useful for many real-world problems, but approaches such as behavioral cloning suffer from data mismatch and compounding error issues. One attempt to address these limitations is the DAgger algorithm, which uses the state distribution induced by the novice to sample corrective actions from the expert. Such sampling schemes, however, require the expert to provide action labels without being fully in control of the system. This can decrease safety and, when using humans as experts, is likely to degrade the quality of the collected labels due to perceived actuator lag. In this work, we propose HG-DAgger, a variant of DAgger that is more suitable for interactive imitation learning from human experts in real-world systems. In addition to training a novice policy, HG-DAgger also learns a safety threshold for a model-uncertainty-based risk metric that can be used to predict the performance of the fully trained novice in different regions of the state space. We evaluate our method on both a simulated and real-world autonomous driving task, and demonstrate improved performance over both DAgger and behavioral cloning.


Title: Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds
Key Words: convolutional neural nets  feature extraction  graph theory  image segmentation  learning (artificial intelligence)  stereo image processing  hierarchical depthwise graph convolutional neural network  3D semantic segmentation  point clouds  point cloud semantic segmentation  depthwise convolution  pointwise convolution  local feature extraction  local features  global features  graph convolution  depthwise graph convolution  Three-dimensional displays  Convolution  Feature extraction  Semantics  Memory management  Shape  Convolutional neural networks 
Abstract: This paper proposes a hierarchical depthwise graph convolutional neural network (HDGCN) for point cloud semantic segmentation. The main chanllenge for learning on point clouds is to capture local structures or relationships. Graph convolution has the strong ability to extract local shape information from neighbors. Inspired by depthwise convolution, we propose a depthwise graph convolution which requires less memory consumption compared with the previous graph convolution. While depthwise graph convolution aggregates features channel-wisely, pointwise convolution is used to learn features across different channels. A customized block called DGConv is specially designed for local feature extraction based on depthwise graph convolution and pointwise convolution. The DGConv block can extract features from points and transfer features to neighbors while being invariant to different point orders. HDGCN is constructed by a series of DGConv blocks using a hierarchical structure which can extract both local and global features of point clouds. Experiments show that HDGCN achieves the state-of-the-art performance in the indoor dataset S3DIS and the outdoor dataset Paris-Lille-3D.


Title: CELLO-3D: Estimating the Covariance of ICP in the Real World
Key Words: covariance analysis  covariance matrices  data analysis  image registration  iterative methods  state estimation  CELLO-3D  state estimation frameworks  closed-form covariance estimation algorithms  data-driven approach  uncertainty estimation  closed-form solutions  covariance estimation and learning through likelihood optimization framework  iterative closest point registrations  3D datasets  ICP registrations  real 3D point clouds  Three-dimensional displays  Estimation  Linear programming  Uncertainty  Prediction algorithms  Measurement  Computational modeling 
Abstract: The fusion of Iterative Closest Point (ICP) registrations in existing state estimation frameworks relies on an accurate estimation of their uncertainty. In this paper, we study the estimation of this uncertainty in the form of a covariance. First, we scrutinize the limitations of existing closed-form covariance estimation algorithms over 3D datasets. Then, we set out to estimate the covariance of ICP registrations through a data-driven approach, with over 5100000 registrations on 1020 pairs from real 3D point clouds. We assess our solution upon a wide spectrum of environments, ranging from structured to unstructured and indoor to outdoor. The capacity of our algorithm to predict covariances is accurately assessed, as well as the usefulness of these estimations for uncertainty estimation over trajectories. The proposed method estimates covariances better than existing closed-form solutions, and makes predictions that are consistent with observed trajectories.


Title: Unsupervised Out-of-context Action Understanding
Key Words: convolutional neural nets  image sequences  motion estimation  unsupervised learning  human action  video sequence  unsupervised label  synthetic databases  unsupervised learning method  O2CA ground truth  SURREAL-O2CA  unsupervised out-of-context action understanding  Databases  Unsupervised learning  Sports  Context modeling  Legged locomotion  Video sequences 
Abstract: The paper presents an unsupervised out-of-context action (O2CA) paradigm that is based on facilitating understanding by separately presenting both human action and context within a video sequence. As a means of generating an unsupervised label, we comprehensively evaluate responses from action-based (ActionNet) and context-based (ContextNet) convolutional neural networks (CNNs). Additionally, we have created three synthetic databases based on the human action (UCF101, HMDB51) and motion capture (mocap) (SURREAL) datasets. We then conducted experimental comparisons between our approach and conventional approaches. We also compared our unsupervised learning method with supervised learning using an O2CA ground truth given by synthetic data. From the results obtained, we achieved a 96.8 score on Synth-UCF, a 96.8 score on Synth-HMDB, and 89.0 on SURREAL-O2CA with F-score.


Title: Learning to Drive in a Day
Key Words: learning (artificial intelligence)  mobile robots  road safety  road traffic control  road vehicles  robot vision  autonomous driving tasks  single monocular image  safety driver  model-free deep reinforcement learning algorithm  lane following  on-vehicle  Reinforcement learning  Autonomous vehicles  Task analysis  Markov processes  Global Positioning System  Sensors  Training 
Abstract: We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.


Title: Generating Adversarial Driving Scenarios in High-Fidelity Simulators
Key Words: automobile industry  Bayes methods  computer vision  control engineering computing  learning (artificial intelligence)  mobile robots  optimisation  program testing  road traffic  road vehicles  traffic engineering computing  transportation  self-driving policy  simulated pedestrians  self-driving behavior  high-fidelity simulators  public roads  software tests  self-driving software  adversarial self-driving scenarios  self-driving vehicles  transportation systems  simulated driving scenarios  driving scenario generation  self-driving car industry  Bayesian optimization  vision-based imitation learning  Optimization  Accidents  Rendering (computer graphics)  Bayes methods  Reinforcement learning  Roads  Trajectory 
Abstract: In recent years self-driving vehicles have become more commonplace on public roads, with the promise of bringing safety and efficiency to modern transportation systems. Increasing the reliability of these vehicles on the road requires an extensive suite of software tests, ideally performed on high-fidelity simulators, where multiple vehicles and pedestrians interact with the self-driving vehicle. It is therefore of critical importance to ensure that self-driving software is assessed against a wide range of challenging simulated driving scenarios. The state of the art in driving scenario generation, as adopted by some of the front-runners of the self-driving car industry, still relies on human input [1]. In this paper we propose to automate the process using Bayesian optimization to generate adversarial self-driving scenarios that expose poorly-engineered or poorly-trained self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. We show that by incorporating the generated scenarios into the training set of the self-driving policy, and by fine-tuning the policy using vision-based imitation learning we obtain safer self-driving behavior.


Title: Data-Driven Contact Clustering for Robot Simulation
Key Words: control engineering computing  force sensors  learning (artificial intelligence)  multilayer perceptrons  optimisation  pattern clustering  robots  data-driven contact clustering  rigid-body robot simulation  multilayer perceptron network  constraint-based optimization contact solver  contact simulation  data-driven learning-based contact clustering  force sensors  torque sensors  MLP network  Force  Numerical models  Robot sensing systems  Data models  Numerical stability  Optimization 
Abstract: We propose a novel data-driven learning-based contact clustering (i.e., of contact points and contact normals) framework for rigid-body robot simulation, with its accuracy established/verified by real experimental data. We first construct an experimental robotic setup with force/torque (F/T) sensors to collect real contact motion/force data. We then design a multilayer perceptron (MLP) network for the contact clustering based on the full motion and force/torque information of the contacts. We also adopt the constraint-based optimization contact solver to facilitate the learning of our MLP network during the training. Our proposed data-driven/learning-based contact clustering framework is then verified against the experimental setup, compared with other techniques/simulators and shown to significantly (or meaningfully) enhance the accuracy of contact simulation as compared to them.


Title: Force-based Heterogeneous Traffic Simulation for Autonomous Vehicle Testing
Key Words: computer simulation  control engineering computing  driver information systems  mobile robots  road safety  road traffic control  road vehicles  traffic engineering computing  self-driving tests  force-based concept  heterogenous traffic simulation  realistic urban environment  personal mobility devices  pedestrians  autonomous vehicles  traffic control  high-fidelity driving simulator  autonomous vehicle testing  force-based heterogeneous traffic simulation  Force  Autonomous vehicles  Roads  Acceleration  Bicycles  Testing  Urban areas 
Abstract: Recent failures in real-world self-driving tests have suggested a paradigm shift from directly learning in real-world roads to building a high-fidelity driving simulator as an alternative, effective, and safe tool to handle intricate traffic environments in urban areas. To date, traffic simulation can construct virtual urban environments with various weather conditions, day and night, and traffic control for autonomous vehicle testing. However, mutual interactions between autonomous vehicles and pedestrians are rarely modeled in existing simulators. Besides vehicles and pedestrians, the usage of personal mobility devices is increasing in congested cities as an alternative to the traditional transport system. A simulator that considers all potential road-users in a realistic urban environment is urgently desired. In this work, we propose a novel, extensible, and microscopic method to build heterogenous traffic simulation using the force-based concept. This force-based approach can accurately replicate the sophisticated behaviors of various road users and their interactions through a simple and unified way. Furthermore, we validate our approach through simulation experiments and comparisons to the popular simulators currently used for research and development of autonomous vehicles.


Title: Customizing Object Detectors for Indoor Robots
Key Words: control engineering computing  convolutional neural nets  helicopters  neurocontrollers  object detection  robot vision  indoor robots  object detection models  convolutional neural networks  large-scale labeled datasets  training data  DUNet  dense upscaled network  Detectors  Object detection  Robots  Feature extraction  Labeling  Computer architecture  Task analysis 
Abstract: Object detection models based on convolutional neural networks (CNNs) demonstrate impressive performance when trained on large-scale labeled datasets. While a generic object detector trained on such a dataset performs adequately in applications where the input data is similar to user photographs, the detector performs poorly on small objects, particularly ones with limited training data or imaged from uncommon viewpoints. Also, a specific room will have many objects that are missed by standard object detectors, frustrating a robot that continually operates in the same indoor environment.This paper describes a system for rapidly creating customized object detectors. Data is collected from a quadcopter that is teleoperated with an interactive interface. Once an object is selected, the quadcopter autonomously photographs the object from multiple viewpoints to collect data to train DUNet (Dense Upscaled Network), our proposed model for learning customized object detectors from scratch given limited data. Our experiments compare the performance of learning models from scratch with DUNet vs. fine tuning existing state of the art object detectors, both on our indoor robotics domain and on standard datasets.


Title: Semi Supervised Deep Quick Instance Detection and Segmentation
Key Words: convolutional neural nets  data acquisition  image segmentation  object detection  supervised learning  class-agnostic object detection  semisupervised labeling  occlusion aware clutter synthesis  online learning  semisupervised deep quick instance detection  semisupervised deep quick learning framework  data acquisition  convolutional neural network  pixelwise semantic segmentation  Image segmentation  Task analysis  Object segmentation  Clutter  Semantics  Labeling  Object detection 
Abstract: In this paper, we present a semi supervised deep quick learning framework for instance detection and pixelwise semantic segmentation of images in a dense clutter of items. The framework can quickly and incrementally learn novel items in an online manner by real-time data acquisition and generating corresponding ground truths on its own. To learn various combinations of items, it can synthesize cluttered scenes, in real time. The overall approach is based on the tutor-child analogy in which a deep network (tutor) is pretrained for class-agnostic object detection which generates labeled data for another deep network (child). The child utilizes a customized convolutional neural network head for the purpose of quick learning. There are broadly four key components of the proposed framework: semi supervised labeling, occlusion aware clutter synthesis, a customized convolutional neural network head, and instance detection. The initial version of this framework was implemented during our participation in Amazon Robotics Challenge (ARC), 2017. Our system was ranked 3rd rd, 4th and 5 th worldwide in pick, stow-pick and stow task respectively. The proposed framework is an improved version over ARC'17 where novel features such as instance detection and online learning has been added.


Title: Risk Averse Robust Adversarial Reinforcement Learning
Key Words: learning (artificial intelligence)  optimisation  probability  risk averse robust adversarial reinforcement learning  deep reinforcement learning  computer games  robotic control  automotive accidents  optimization  probability  RARARL  self-driving vehicle controller  Reinforcement learning  Training  Mathematical model  Robustness  Autonomous vehicles  Task analysis  Accidents 
Abstract: Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary. Supplementary materials are available at https://sites.google.com/view/rararl.


Title: Early Failure Detection of Deep End-to-End Control Policy by Reinforcement Learning
Key Words: belief networks  control engineering computing  convolutional neural nets  learning (artificial intelligence)  learning systems  observability  predictive control  learned control policies  reinforcement learning  end-to-end imitation  predictive uncertainty  model predictive controller  fully-observable vision-based partially-observable systems  deep convolutional Bayesian neural networks  deep end-to-end control policy  Bayesian networks  mean value  corrective action  partial state observability  Uncertainty  Bayes methods  Task analysis  Neural networks  Safety  Training  Autonomous vehicles 
Abstract: We propose the use of Bayesian networks, which provide both a mean value and an uncertainty estimate as output, to enhance the safety of learned control policies under circumstances in which a test-time input differs significantly from the training set. Our algorithm combines reinforcement learning and end-to-end imitation learning to simultaneously learn a control policy as well as a threshold over the predictive uncertainty of the learned model, with no hand-tuning required. Corrective action, such as a return of control to the model predictive controller or human expert, is taken before the failure of tasks, when the uncertainty threshold is exceeded. We validate our method on fully-observable and vision-based partially-observable systems using cart-pole and autonomous driving simulations using deep convolutional Bayesian neural networks. We demonstrate that our method is robust to uncertainty resulting from varying system dynamics as well as from partial state observability.


Title: Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning
Key Words: approximation theory  control engineering computing  dynamic programming  gradient methods  learning (artificial intelligence)  mobile robots  optimal control  partial differential equations  dynamic programming equation  contraction mapping  Hamilton-Jacobi safety analysis  control-theoretic safety analysis  optimal safety policy  quantitative safety analysis  reinforcement learning techniques  time-discounted modification  optimal control problems  robust optimal control theory  autonomous robotic systems  policy gradient techniques  value learning  Safety  Automation  Reinforcement learning  Robots  Optimal control  Jacobian matrices  Reachability analysis 
Abstract: Safety analysis is a necessary component in the design and deployment of autonomous robotic systems. Techniques from robust optimal control theory, such as Hamilton-Jacobi reachability analysis, allow a rigorous formalization of safety as guaranteed constraint satisfaction. Unfortunately, the computational complexity of these tools for general dynamical systems scales poorly with state dimension, making existing tools impractical beyond small problems. Modern reinforcement learning methods have shown promising ability to find approximate yet proficient solutions to optimal control problems in complex and high-dimensional systems, however their application has in practice been restricted to problems with an additive payoff over time, unsuitable for reasoning about safety. In recent work, we introduced a time-discounted modification of the problem of maximizing the minimum payoff over time, central to safety analysis, through a modified dynamic programming equation that induces a contraction mapping. Here, we show how a similar contraction mapping can render reinforcement learning techniques amenable to quantitative safety analysis as tools to approximate the safe set and optimal safety policy. This opens a new avenue of research connecting control-theoretic safety analysis and the reinforcement learning domain. We validate the correctness of our formulation by comparing safety results computed through Q-learning to analytic and numerical solutions, and demonstrate its scalability by learning safe sets and control policies for simulated systems of up to 18 state dimensions using value learning and policy gradient techniques.


Title: A Friction-Based Kinematic Model for Skid-Steer Wheeled Mobile Robots
Key Words: friction  mobile robots  robot dynamics  robot kinematics  wheels  friction-based kinematic model  skid-steer drive systems  mobile robot platforms  normal operation  slippages  forward kinematics  slip prediction  skid-steer wheeled mobile robots  translational prediction error  rotational prediction error  skid-steer robot  system identification  model learning research  Mobile robots  Kinematics  Wheels  Force  Predictive models  Acceleration 
Abstract: Skid-steer drive systems are widely used in mobile robot platforms. Such systems are subject to significant slippage and skidding during normal operation due to their nature. The ability to predict and compensate for such slippages in the forward kinematics of these types of robots is of great importance and provides the means for accurate control and safe navigation. In this work, we propose a new kinematic model capable of slip prediction for skid-steer wheeled mobile robots (SSWMRs). The proposed model outperforms the state-of-the-art in terms of both translational and rotational prediction error on a dataset composed of more than 6 km worth of trajectories traversed by a skid-steer robot. We also publicly release our dataset to serve as a benchmark for system identification and model learning research for SSWMRs.


Title: DMP Based Trajectory Tracking for a Nonholonomic Mobile Robot With Automatic Goal Adaptation and Obstacle Avoidance
Key Words: collision avoidance  fuzzy logic  gradient methods  learning (artificial intelligence)  Lyapunov methods  manipulator dynamics  mobile robots  motion control  position control  radial basis function networks  stability  steering systems  trajectory control  Dynamic Movement Primitive  motion planning  robot manipulator  nonholonomic mobile robot  Radial Basis Function Networks  robot goal position  Lyapunov stability theory-based analysis  dynamic obstacles  automatic goal adaptation  RBFN  DMP  gradient descent  static obstacles  trajectory tracking  damped spring model  steering angle dynamics  fuzzy logic  Mobile robots  Trajectory  Mathematical model  Robot kinematics  Collision avoidance  Dynamics 
Abstract: Dynamic Movement Primitive (DMP) which is popular for motion planning of a robot manipulator, has been adapted for a nonholonomic mobile robot to track the desired trajectory. DMP is a simple damped spring model with a forcing function, which learns the trajectory. The damped spring model attracts the robot towards the goal position, and the forcing function forces the robot to follow the given trajectory. Two Radial Basis Function Networks (RBFNs) have been used to learn the forcing function associated with the DMP model. Weight update laws are derived using the gradient descent approach to train the RBFNs. Fuzzy logic based steering angle dynamics is proposed to handle the asymmetric nature of an obstacle. The proposed scheme is capable enough to generate a smooth trajectory in the presence of an obstacle even when start and goal positions are altered, without losing the spatial information embedded while training. The convergence of the robot goal position has been shown using Lyapunov stability theory-based analysis. The approach has been extended to multiple static and dynamic obstacles for the successful convergence of the robot at the goal position. Both simulation and experimental results are provided to confirm the efficacy of the proposed scheme.


Title: Multimodal Spatio-Temporal Information in End-to-End Networks for Automotive Steering Prediction
Key Words: cameras  driver information systems  image sequences  road safety  steering systems  visual input data  onboard vehicle camera  empirical comparison  spatial spatio-temporal  real-life driver  predicted steering command  recurrent multimodal model  steering correction concept  multimodal spatio-temporal information  end-to-end networks  automotive steering prediction  end-to-end steering problem  Optical imaging  Adaptive optics  Computational modeling  Kernel  Training  Roads  Cameras  Autonomous steering  deep learning  spatio-temporal model  multimodal input  optical flow  RNN-LSTM 
Abstract: We study the end-to-end steering problem using visual input data from an onboard vehicle camera. An empirical comparison between spatial, spatio-temporal and multimodal models is performed assessing each concept's performance from two points of evaluation. First, how close the model is in predicting and imitating a real-life driver's behavior, second, the smoothness of the predicted steering command. The latter is a newly proposed metric. Building on our results, we propose a new recurrent multimodal model. The suggested model has been tested on a custom dataset recorded by BMW, as well as the public dataset provided by Udacity. Results show that it outperforms previously released scores. Further, a steering correction concept from off-lane driving through the inclusion of correction frames is presented. We show that our suggestion leads to promising results empirically.


Title: Safe Reinforcement Learning With Model Uncertainty Estimates
Key Words: Bayes methods  collision avoidance  control engineering computing  learning (artificial intelligence)  neural nets  safety  model uncertainty estimates  current autonomous systems  strong reliance  black box predictions  deep neural networks  DNNs  unpredictable results  far-from-distribution test data  distributional shift  safety-critical applications  pedestrians  state-of-the-art extraction methods  Bayesian neural networks  MC-Dropout  computationally tractable uncertainty estimates  parallelizable uncertainty estimates  uncertainty-aware navigation  collision avoidance policy  unseen behavior  uncertainty-unaware baseline  safe reinforcement learning framework  Uncertainty  Collision avoidance  Neural networks  Computational modeling  Training  Data models  Reinforcement learning 
Abstract: Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.


Title: Improving dual-arm assembly by master-slave compliance
Key Words: force control  human-robot interaction  industrial manipulators  learning (artificial intelligence)  manipulator dynamics  motion control  multi-robot systems  robotic assembly  telerobotics  learning method  master-slave compliance  dual-arm assembly task  compliance parameters  human demonstration  compliant motions  assembly tasks  convergence region  alignment task  compliant axes  orientation error  single teleoperated manipulator  manipulators compliant  total joint motions  Task analysis  Manipulators  Tools  Jamming  Wrist  Trajectory 
Abstract: In this paper we show how different choices regarding compliance affect a dual-arm assembly task. In addition, we present how the compliance parameters can be learned from a human demonstration. Compliant motions can be used in assembly tasks to mitigate pose errors originating from, for example, inaccurate grasping. We present analytical background and accompanying experimental results on how to choose the center of compliance to enhance the convergence region of an alignment task. Then we present the possible ways of choosing the compliant axes for accomplishing alignment in a scenario where orientation error is present. We show that an earlier presented Learning from Demonstration method can be used to learn motion and compliance parameters of an impedance controller for both manipulators. The learning requires a human demonstration with a single teleoperated manipulator only, easing the execution of demonstration and enabling usage of manipulators at difficult locations as well. Finally, we experimentally verify our claim that having both manipulators compliant in both rotation and translation can accomplish the alignment task with less total joint motions and in shorter time than moving one manipulator only. In addition, we show that the learning method produces the parameters that achieve the best results in our experiments.


Title: REPLAB: A Reproducible Low-Cost Arm Benchmark for Robotic Learning
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  robot programming  robot vision  reproducible low-cost arm benchmark  robotic learning  vision-based manipulation benchmark  robot arm  robotics  grasping benchmark  evaluation protocol  standardized evaluation  machine learning  REPLAB  Robots  Benchmark testing  Task analysis  Grasping  Hardware  Cameras  Calibration 
Abstract: Standardized evaluation measures have aided in the progress of machine learning approaches in disciplines such as computer vision and machine translation. In this paper, we make the case that robotic learning would also benefit from benchmarking, and present a template for a vision-based manipulation benchmark. Our benchmark is built on “REPLAB,” a reproducible and self-contained hardware stack (robot arm, camera, and workspace) that costs about 2000 USD and occupies a cuboid of size 70x40x60 cm. Each REPLAB cell may be assembled within a few hours. Through this low-cost, compact design, REPLAB aims to drive wide participation by lowering the barrier to entry into robotics and to enable easy scaling to many robots. We envision REPLAB as a framework for reproducible research across manipulation tasks, and as a step in this direction, we define a grasping benchmark consisting of a task definition, evaluation protocol, performance measures, and a dataset of over 50,000 grasp attempts. We implement, evaluate, and analyze several previously proposed grasping approaches to establish baselines for this benchmark. Project page with assembly instructions, additional details, and videos: https://goo.gl/5F9dP4.


Title: Self-Supervised Surgical Tool Segmentation using Kinematic Information
Key Words: calibration  convolutional neural nets  endoscopes  image classification  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  optimisation  pose estimation  robot vision  surgery  training labels  optimization method  unknown hand-eye calibration  imprecise kinematic model  fully-convolutional neural network  endoscopic images  flexible robotized endoscopy system  self-supervised surgical tool segmentation  kinematic information  task automation  minimally invasive surgical operations  modern machine learning methods  manually-annotated images  surgical context  patient-to-patient differences  annotated data  self-supervised approach  robot-assisted context  pose estimation  subtask automation  hand-eye calibration  pixel-wise classification  Tools  Image segmentation  Kinematics  Shape  Robot kinematics  Cost function 
Abstract: Surgical tool segmentation in endoscopic images is the first step towards pose estimation and (sub-)task automation in challenging minimally invasive surgical operations. While many approaches in the literature have shown great results using modern machine learning methods such as convolutional neural networks, the main bottleneck lies in the acquisition of a large number of manually-annotated images for efficient learning. This is especially true in surgical context, where patient-to-patient differences impede the overall generalizability. In order to cope with this lack of annotated data, we propose a self-supervised approach in a robot-assisted context. To our knowledge, the proposed approach is the first to make use of the kinematic model of the robot in order to generate training labels. The core contribution of the paper is to propose an optimization method to obtain good labels for training despite an unknown hand-eye calibration and an imprecise kinematic model. The labels can subsequently be used for fine-tuning a fully-convolutional neural network for pixel-wise classification. As a result, the tool can be segmented in the endoscopic images without needing a single manually-annotated image. Experimental results on phantom and in vivo datasets obtained using a flexible robotized endoscopy system are very promising.


Title: Needle Localization for Robot-assisted Subretinal Injection based on Deep Learning
Key Words: biomedical optical imaging  eye  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  needles  optical tomography  surgery  visual feedback  microscope-integrated optical coherence tomography  robotic subretinal injection  needle segment  retinal surface  OCT volumetric images  MI-OCT  needle detection  human surgeons  robot-assisted surgery  high surgical precision  deep learning  robot-assisted subretinal injection  needle localization  Needles  Retina  Surgery  Image segmentation  Robots  Microscopy  Agriculture 
Abstract: Subretinal injection is known to be a complicated task for ophthalmologists to perform, the main sources of difficulties are the fine anatomy of the retina, insufficient visual feedback, and high surgical precision. Image guided robot-assisted surgery is one of the promising solutions that bring significant surgical enhancement in treatment outcome and reduces the physical limitations of human surgeons. In this paper, we demonstrate a robust framework for needle detection and localization in subretinal injection using microscope-integrated Optical Coherence Tomography (MI-OCT) based on deep learning. The proposed method consists of two main steps: a) the preprocessing of OCT volumetric images; b) needle localization in the processed images. The first step is to coarsely localize the needle position based on the needle information above the retinal surface and crop the original image into a small region of interest (ROI). Afterward, the cropped small image is fed into a well trained network for detection and localization of the needle segment. The entire framework is extensively validated in ex-vivo pig eye experiments with robotic subretinal injection. The results show that the proposed method can localize the needle accurately with a confidence of 99.2%.


Title: Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing
Key Words: blood vessels  cameras  diseases  image matching  intelligent robots  learning (artificial intelligence)  manipulator dynamics  medical image processing  medical robotics  mobile robots  multi-robot systems  robot vision  stents  stereo image processing  robotic platforms  autonomous personalized stent graft manufacturing  stereo vision systems  personalized stent-graft manufacturing  robotic arm  dynamic stereo microscope  static wide angle view stereo webcam  multiple stereo camera configuration  sewing process  stereo matching  feature identifications  visual-servoing system  real-time intelligent robotic control  visual guidance  automatic control  robotic personalized stent graft manufacturing  AAA patient  hybrid vision system  abdominal aortic aneurysms patient  DDPG  reinforcement learning  object localization  Microscopy  Robot kinematics  Needles  Webcams  Manipulators  Aneurysm 
Abstract: Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.


Title: Towards 3D Path Planning from a Single 2D Fluoroscopic Image for Robot Assisted Fenestrated Endovascular Aortic Repair
Key Words: blood vessels  computerised tomography  diagnostic radiography  image registration  image segmentation  medical image processing  medical robotics  path planning  phantoms  CT scans  computed tomography  2D intra-operative AAA skeletons  graph matching method  3D preoperative AAA  3D distance error  skeleton length  skeleton deformation  real-time 3D robotic path planning  Abdominal Aortic Aneurysm  skeleton instantiation framework  2D fluoroscopic images  fenestrated endovascular aortic repair  single 2D fluoroscopic image  Three-dimensional displays  Two dimensional displays  Skeleton  Indexes  Robots  Arteries  Path planning 
Abstract: The current standard of intra-operative navigation during Fenestrated Endovascular Aortic Repair (FEVAR) calls for the need of 3D alignments between inserted devices and aortic branches. The navigation commonly via 2D fluoroscopic images, lacks anatomical information, resulting in longer operation hours and radiation exposure. In this paper, a skeleton instantiation framework of Abdominal Aortic Aneurysm (AAA) from a single 2D fluoroscopic image is introduced for real-time 3D robotic path planning. A graph matching method is proposed to establish the correspondences between the 3D preoperative and 2D intra-operative AAA skeletons, and then the two skeletons are registered by skeleton deformation and regularization in respect to skeleton length and smoothness. Furthermore, deep learning was used to segment 3D preoperative AAA from Computed Tomography (CT) scans to facilitate the framework automation. Simulation, phantom and patient AAA data sets have been used to validate the proposed framework. 3D distance error of 2mm was achieved in the phantom setup. Performance advantages were also achieved in terms of accuracy, robustness and time-efficiency.


Title: Visual Robot Task Planning
Key Words: learning (artificial intelligence)  mobile robots  Monte Carlo methods  neural net architecture  planning (artificial intelligence)  robot vision  tree searching  visual robot task planning  visual information  planning algorithm  neural network architecture  Monte Carlo tree search  block-stacking simulation  Task analysis  Planning  Visualization  Predictive models  Robots  Transforms  Computer architecture 
Abstract: Prospection is key to solving challenging problems in new environments, but it has not been deeply explored as applied to task planning for perception-driven robotics. We propose visual robot task planning, where we take in an input image and must generate a sequence of high-level actions and associated observations that achieve some task. In this paper, we describe a neural network architecture and associated planning algorithm that (1) learns a representation of the world that can generate prospective futures, (2) uses this generative model to simulate the result of sequences of high-level actions in a variety of environments, and (3) evaluates these actions via a variant of Monte Carlo Tree Search to find a viable solution to a particular problem. Our approach allows us to visualize intermediate motion goals and learn to plan complex activity from visual information, and used this to generate and visualize task plans on held-out examples of a block-stacking simulation.


Title: Visual Representations for Semantic Target Driven Navigation
Key Words: image representation  image segmentation  learning (artificial intelligence)  mobile robots  navigation  path planning  robot vision  visual representations  semantic target driven navigation  semantic visual navigation  semantic segmentation  domain adaptation  computer vision algorithms  robot  deep network  navigation policy learning  Navigation  Visualization  Semantics  Training  Adaptation models  Robots  Task analysis 
Abstract: What is a good visual representation for navigation? We study this question in the context of semantic visual navigation, which is the problem of a robot finding its way through a previously unseen environment to a target object, e.g. go to the refrigerator. Instead of acquiring a metric semantic map of an environment and using planning for navigation, our approach learns navigation policies on top of representations that capture spatial layout and semantic contextual cues. We propose to use semantic segmentation and detection masks as observations obtained by state-of-the-art computer vision algorithms and use a deep network to learn the navigation policy. The availability of equitable representations in simulated environments enables joint training using real and simulated data and alleviates the need for domain adaptation or domain randomization commonly used to tackle the sim-to-real transfer of the learned policies. Both the representation and the navigation policy can be readily applied to real non-synthetic environments as demonstrated on the Active Vision Dataset [1]. Our approach successfully gets to the target in 54% of the cases in unexplored environments, compared to 46% for a non-learning based approach, and 28% for a learning-based baseline.


Title: Deep Object-Centric Policies for Autonomous Driving
Key Words: computer games  convolutional neural nets  data visualisation  learning (artificial intelligence)  traffic engineering computing  object-centric models  object instances  end-to-end learning  Grand Theft Auto V simulator  object-agnostic methods  object-centric policies  autonomous driving  visuomotor skills  deep neural networks  robotics tasks  intuitive visualization  Berkeley DeepDrive Video dataset  Task analysis  Training  Taxonomy  Automobiles  Autonomous vehicles  Feature extraction  Robots 
Abstract: While learning visuomotor skills in an end-to-end manner is appealing, deep neural networks are often uninterpretable and fail in surprising ways. For robotics tasks, such as autonomous driving, models that explicitly represent objects may be more robust to new scenes and provide intuitive visualizations. We describe a taxonomy of “object-centric” models which leverage both object instances and end-to-end learning. In the Grand Theft Auto V simulator, we show that object-centric models outperform object-agnostic methods in scenes with other vehicles and pedestrians, even with an imperfect detector. We also demonstrate that our architectures perform well on real-world environments by evaluating on the Berkeley DeepDrive Video dataset, where an object-centric model outperforms object-agnostic models in the low-data regimes.


Title: Neural Autonomous Navigation with Riemannian Motion Policy
Key Words: collision avoidance  geometry  learning (artificial intelligence)  mobile robots  neurocontrollers  optimal control  predictive control  robot vision  image-based autonomous navigation technique  Riemannian motion policy framework  vehicular control  deep learning  policy structure  data complexity  modeling error  end-to-end learning  neural autonomous navigation  local geometry  RMP representation  indoor obstacle avoidance  Gibson environment  optimal control commands  visual images  control point RMPs  deep neural network  Acceleration  Geometry  Autonomous robots  Measurement  Neural networks  Kinematics 
Abstract: End-to-end learning for autonomous navigation has received substantial attention recently as a promising method for reducing modeling error. However, its data complexity, especially around generalization to unseen environments, is high. We introduce a novel image-based autonomous navigation technique that leverages in policy structure using the Riemannian Motion Policy (RMP) framework for deep learning of vehicular control. We design a deep neural network to predict control point RMPs of the vehicle from visual images, from which the optimal control commands can be computed analytically. We show that our network trained in the Gibson environment can be used for indoor obstacle avoidance and navigation on a real RC car, and our RMP representation generalizes better to unseen environments than predicting local geometry or predicting control commands directly.


Title: Two-Stage Transfer Learning for Heterogeneous Robot Detection and 3D Joint Position Estimation in a 2D Camera Image Using CNN
Key Words: calibration  cameras  collision avoidance  convolutional neural nets  dexterous manipulators  feature extraction  image classification  image colour analysis  image segmentation  learning (artificial intelligence)  multi-robot systems  robot vision  two-stage transfer learning approach  multiobjective convolutional neural network  heterogeneous robot arms  eye-to-hand calibration  universal robots  two-stage transfer learning  2D colour image  collision avoidance algorithms  fixed robot-camera setups  collision detection  factory floors  collaborative robots  2D camera image  3D joint position estimation  heterogeneous robot detection  multiobjective CNN  data collection approach  Robot kinematics  Collision avoidance  Cameras  Robot vision systems  Calibration 
Abstract: Collaborative robots are becoming more common on factory floors as well as regular environments, however, their safety still is not a fully solved issue. Collision detection does not always perform as expected and collision avoidance is still an active research area. Collision avoidance works well for fixed robot-camera setups, however, if they are shifted around, Eye-to-Hand calibration becomes invalid making it difficult to accurately run many of the existing collision avoidance algorithms. We approach the problem by presenting a stand-alone system capable of detecting the robot and estimating its position, including individual joints, by using a simple 2D colour image as an input, where no Eye-to-Hand calibration is needed. As an extension of previous work, a two-stage transfer learning approach is used to re-train a multi-objective convolutional neural network (CNN) to allow it to be used with heterogeneous robot arms. Our method is capable of detecting the robot in real-time and new robot types can be added by having significantly smaller training datasets compared to the requirements of a fully trained network. We present data collection approach, the structure of the multi-objective CNN, the two-stage transfer learning training and test results by using real robots from Universal Robots, Kuka, and Franka Emika. Eventually, we analyse possible application areas of our method together with the possible improvements.


Title: Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks
Key Words: learning (artificial intelligence)  manipulators  haptic feedback  visual feedback  robot controller  deep reinforcement learning  high-dimensional inputs  sample complexity  multimodal representation  sensory inputs  policy learning  peg insertion task  self-supervised learning  multimodal representations  contact-rich manipulation tasks  Task analysis  Robot sensing systems  Haptic interfaces  Visualization  Geometry  Reinforcement learning 
Abstract: Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. We present results in simulation and on a real robot.


Title: Deep Visuo-Tactile Learning: Estimation of Tactile Properties from Images
Key Words: control engineering computing  end effectors  feature extraction  image colour analysis  learning (artificial intelligence)  mobile robots  neural nets  robot vision  tactile sensors  visual perception  deep visuo-tactile learning  tactile sensor data  Webcam  tactile properties  visual perception  encoder-decoder network  latent variables  tactile features  visual features  RGB images  uSkin tactile sensor  end-effector  feature space  Sawyer robot  Tactile sensors  Training  Time series analysis  Decoding 
Abstract: Estimation of tactile properties from vision, such as slipperiness or roughness, is important to effectively interact with the environment. These tactile properties help us decide which actions we should choose and how to perform them. E.g., we can drive slower if we see that we have bad traction or grasp tighter if an item looks slippery. We believe that this ability also helps robots to enhance their understanding of the environment, and thus enables them to tailor their actions to the situation at hand. We therefore propose a model to estimate the degree of tactile properties from visual perception alone (e.g., the level of slipperiness or roughness). Our method extends a encoder-decoder network, in which the latent variables are visual and tactile features. In contrast to previous works, our method does not require manual labeling, but only RGB images and the corresponding tactile sensor data. All our data is collected with a webcam and uSkin tactile sensor mounted on the end-effector of a Sawyer robot, which strokes the surfaces of 25 different materials. We show that our model generalizes to materials not included in the training data by evaluating the feature space, indicating that it has learned to associate important tactile properties with images.


Title: Variational End-to-End Navigation and Localization
Key Words: cameras  Global Positioning System  learning (artificial intelligence)  mobile robots  navigation  path planning  probability  variational techniques  point-topoint navigation algorithms  full-scale autonomous vehicle  localization algorithm  variational end-to-end navigation  deep learning  autonomous vehicle control  raw sensory data  navigation instruction  end-to-end driving networks  point-to-point navigation  probabilistic localization  noisy GPS data  raw camera data  higher level roadmaps  probability distribution  deterministic control command  rough localization  real-world driving data  variational network  Navigation  Roads  Cameras  Robot sensing systems  Visualization  Partitioning algorithms 
Abstract: Deep learning has revolutionized the ability to learn “end-to-end” autonomous vehicle control directly from raw sensory data. While there have been recent extensions to handle forms of navigation instruction, these works are unable to capture the full distribution of possible actions that could be taken and to reason about localization of the robot within the environment. In this paper, we extend end-to-end driving networks with the ability to perform point-to-point navigation as well as probabilistic localization using only noisy GPS data. We define a novel variational network capable of learning from raw camera data of the environment as well as higher level roadmaps to predict (1) a full probability distribution over the possible control commands; and (2) a deterministic control command capable of navigating on the route specified within the map. Additionally, we formulate how our model can be used to localize the robot according to correspondences between the map and the observed visual road topology, inspired by the rough localization that human drivers can perform. We test our algorithms on real-world driving data that the vehicle has never driven through before, and integrate our point-topoint navigation algorithms onboard a full-scale autonomous vehicle for real-time performance. Our localization algorithm is also evaluated over a new set of roads and intersections to demonstrates rough pose localization even in situations without any GPS prior.


Title: Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience
Key Words: learning (artificial intelligence)  mobile robots  real world experience  simulation parameter distribution  policy training  policy transfer  policy behavior  sim-to-real loop  simulation randomization  swing-peg-in-hole  cabinet drawer opening  Adaptation models  Training  Data models  Robots  Computational modeling  Trajectory  Task analysis 
Abstract: We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https://sites.google.com/view/simopt.


Title: Robust Learning of Tactile Force Estimation through Robot Interaction
Key Words: control engineering computing  force feedback  learning (artificial intelligence)  neural nets  robots  tactile sensors  learned force model  robust learning  tactile force estimation  robot interaction  analytic models  robust model  SynTouch BioTac sensor  neural networks  voxelized input feature layer  spatial signals  sensor surface  robust tactile force model  force torque sensor  FT sensor  force inference  planar pushing task  force direction  force estimation  tactile sensor signals  force feedback grasp controller  Force  Robot sensing systems  Biological system modeling  Task analysis  Biosensors  Neural networks 
Abstract: Current methods for estimating force from tactile sensor signals are either inaccurate analytic models or task-specific learned models. In this paper, we explore learning a robust model that maps tactile sensor signals to force. We specifically explore learning a mapping for the SynTouch BioTac sensor via neural networks. We propose a voxelized input feature layer for spatial signals and leverage information about the sensor surface to regularize the loss function. To learn a robust tactile force model that transfers across tasks, we generate ground truth data from three different sources: (1) the BioTac rigidly mounted to a force torque (FT) sensor, (2) a robot interacting with a ball rigidly attached to the same FT sensor, and (3) through force inference on a planar pushing task by formalizing the mechanics as a system of particles and optimizing over the object motion. A total of 140k samples were collected from the three sources. We achieve a median angular accuracy of 3.5 degrees in predicting force direction (66% improvement over the current state of the art) and a median magnitude accuracy of 0.06 N (93% improvement) on a test dataset. Additionally, we evaluate the learned force model in a force feedback grasp controller performing object lifting and gentle placement. Our results can be found on https: //sites.google.com/view/tactile-force.


Title: Learning Scene Geometry for Visual Localization in Challenging Conditions
Key Words: feature extraction  image colour analysis  image retrieval  learning (artificial intelligence)  object detection  robot vision  daytime images  learning scene geometry  visual localization  outdoor large scale image  cross-season  learned global image descriptor  scene geometry information  depth map  query image  localization accuracy  cross-weather  long-term localization scenario  night images  winter localization sequence  summer localization sequence  Training  Decoding  Feature extraction  Robots  Image reconstruction  Geometry  Visualization 
Abstract: We propose a new approach for outdoor large scale image based localization that can deal with challenging scenarios like cross-season, cross-weather, day/night and long-term localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We are able to increase recall@1 performances by 2.15% on cross-weather and long-term localization scenario and by 4.24% points on a challenging winter/summer localization sequence versus state-of-the-art methods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images.


Title: Transfer Learning for Surgical Task Segmentation
Key Words: learning (artificial intelligence)  medical robotics  surgery  transfer learning  surgical task segmentation  segmentation points  manually labeled data  correlated features  segmentation rule  high segmentation rates  Task analysis  Motion segmentation  Trajectory  Feature extraction  Needles  Surgery 
Abstract: In this paper, we present a novel approach for surgical task segmentation. A segmentation policy learns the correlations between features and segmentation points from manually labeled data. The most correlated features and rules for segmenting them are identified and learned. These form a complete set of segmentation policy. The proposed approach is developed to segment new but similar tasks through transfer learning. It is verified through applying the segmentation rule learned from the labeled data to segment other tasks. The performance of the proposed algorithm was evaluated by comparing the results against the ground truths. Experimental results demonstrate that our approach can achieve high segmentation rates with an accuracy of between 68.8% - 81.8%.


Title: Networked Operation of a UAV Using Gaussian Process-Based Delay Compensation and Model Predictive Control
Key Words: autonomous aerial vehicles  compensation  control nonlinearities  delays  Gaussian processes  mobile robots  networked control systems  nonlinear control systems  path planning  predictive control  stability  state estimation  state feedback  time-varying systems  Gaussian process-based delay compensation  model predictive control  time-varying network delay  UAV control system  delayed state feedback  multirotor-type UAVs  networked control system  UAV networked operation  path planning  state estimation  Delays  Servers  Uplink  Downlink  Predictive models  Unmanned aerial vehicles  Mathematical model 
Abstract: This study addresses an operation of unmanned aerial vehicles (UAVs) in a network environment where there is time-varying network delay. The network delay entails undesirable effects on the stability of the UAV control system due to delayed state feedback and outdated control input. Although several networked control algorithms have been proposed to deal with the network delay, most existing studies have assumed that the plant dynamics is known and simple, or the network delay is constant. These assumptions are improper to multirotor-type UAVs because of their nonlinearity and time-sensitive characteristics. To deal with these problems, we propose a networked control system using model predictive control (MPC) designed under the consideration of multirotor characteristics. We also apply a Gaussian process (GP) to learn an unknown nonlinear model, which increases the accuracy of path planning and state estimation. Flight experiments show that the proposed algorithm successfully compensates the network delay and Gaussian process learning improves the UAV's path tracking performance.


Title: Flappy Hummingbird: An Open Source Dynamic Simulation of Flapping Wing Robots and Animals
Key Words: aerodynamics  aerospace components  aerospace robotics  aircraft control  autonomous aerial vehicles  closed loop systems  control system synthesis  learning (artificial intelligence)  microrobots  mobile robots  motion control  nonlinear control systems  robot dynamics  robot kinematics  stability  vehicle dynamics  flappy hummingbird  open source dynamic simulation  flapping Wing robots  hummingbirds  extraordinary flight performance  stable hovering maneuvering  aggressive maneuvering  conventional small scale man-made vehicles  FWMAVs  performance gap  open source high fidelity dynamic simulation  optimization  flight control  at-scale hummingbird robot  system identification  dynamic response  open-loop  loop systems  simulated flights  experimental flights  highly nonlinear flight dynamics  control problems  control algorithms  linear controller  control policy  simulation-to-real transfer  physical robot  flapping wing microair vehicles  Aerodynamics  Robots  Vehicle dynamics  Force  Torque  Animals 
Abstract: Insects and hummingbirds exhibit extraordinary flight performance and can simultaneously master seemingly conflicting goals: stable hovering and aggressive maneuvering, which are unmatched by conventional small scale man-made vehicles. Flapping Wing Micro Air Vehicles (FWMAVs) hold great promise for closing this performance gap. However, design and control of such systems remain challenging. Here, we present an open source high fidelity dynamic simulation for FWMAVs. The simulator serves as a testbed for the design, optimization and flight control of FWMAVs. To validate the simulation, we recreated the at-scale hummingbird robot developed in our lab in the simulation. System identification was performed to obtain the model parameters. Force generation and dynamic response of open-loop and closed loop systems between simulated and experimental flights were compared. The unsteady aerodynamics and the highly nonlinear flight dynamics present challenging control problems for conventional and learning control algorithms such as Reinforcement Learning. The interface of the simulation is fully compatible with OpenAI Gym environment. As a benchmark study, we present a linear controller for hovering stabilization and a Deep Reinforcement Learning control policy for goal-directed maneuvering. Finally, we demonstrate direct simulation-to-real transfer of both control policies onto the physical robot, further demonstrating the fidelity of the simulation.


Title: Visual Repetition Sampling for Robot Manipulation Planning
Key Words: collision avoidance  Gaussian processes  image sampling  learning (artificial intelligence)  manipulators  mobile robots  robot vision  trees (mathematics)  Gaussian mixture models  rapidly-exploring random tree  biased sampling methods  real-time applications  longer planning times  optimization-based methods  complex environments  sampling-based motion planners  robot manipulation  visual repetition sampling  RRT motion planner  sampling efficiency  visual input  GMM  Planning  Databases  Task analysis  Visualization  Robots  Probability distribution  Feature extraction 
Abstract: One of the main challenges in sampling-based motion planners is to find an efficient sampling strategy. While methods such as Rapidly-exploring Random Tree (RRT) have shown to be more reliable in complex environments than optimization-based methods, they often require longer planning times, which reduces their usability for real-time applications. Recently, biased sampling methods have shown to remedy this issue. For example Gaussian Mixture Models (GMMs) have been used to sample more efficiently in feasible regions of the configuration space. Once the GMM is learned, however, this approach does not adapt its biases to individual planning scene during inference. Hence, we propose in this work a more efficient sampling strategy to further bias the GMM based on visual input upon query. We employ an autoencoder trained entirely in simulation to extract features from depth images and use the latent representation to adjust the weights of each mixture components in the GMM. We show empirically that this improves the sampling efficiency of an RRT motion planner in both real and simulated scenes.


Title: SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation
Key Words: convolutional neural nets  image classification  image motion analysis  image resolution  image sampling  learning (artificial intelligence)  pose estimation  self-supervised monocular depth estimation  self-supervised monocular depth prediction  subpixel convolutional layer extension  depth super-resolution  high-resolution disparities  low-resolution convolutional features  flip-augmentation layer  single-image super-resolution  deep learning methods  super-resolved monocular depth estimation  public KITTI benchmark  pose estimation  Estimation  Convolutional codes  Cameras  Spatial resolution  Three-dimensional displays  Training 
Abstract: Recent techniques in self-supervised monocular depth estimation are approaching the performance of supervised methods, but operate in low resolution only. We show that high resolution is key towards high-fidelity self-supervised monocular depth prediction. Inspired by recent deep learning methods for Single-Image Super-Resolution, we propose a subpixel convolutional layer extension for depth super-resolution that accurately synthesizes high-resolution disparities from their corresponding low-resolution convolutional features. In addition, we introduce a differentiable flip-augmentation layer that accurately fuses predictions from the image and its horizontally flipped version, reducing the effect of left and right shadow regions generated in the disparity map due to occlusions. Both contributions provide significant performance gains over the state-of-the-art in self-supervised depth and pose estimation on the public KITTI benchmark. A video of our approach can be found at https://youtu.be/jKNgBeBMx0I.


Title: Improving Underwater Obstacle Detection using Semantic Image Segmentation
Key Words: feature extraction  image classification  image enhancement  image matching  image segmentation  learning (artificial intelligence)  mobile robots  path planning  robot vision  stereo image processing  image-based underwater obstacle detection  sparse stereo point clouds  monocular semantic image segmentation  cluttered underwater environments  robust robotic path planning  feature-based stereo matching  learning-based segmentation  robust obstacle map  direct binary learning  underwater obstacles  multiclass learning approach  binary map  sparse stereo matching  3D obstacle maps  coral reef environments  image-wide obstacle detection  dynamic objects  image-based obstacle maps  Image segmentation  Semantics  Cameras  Three-dimensional displays  Real-time systems  Training  Robots 
Abstract: This paper presents two novel approaches for improving image-based underwater obstacle detection by combining sparse stereo point clouds with monocular semantic image segmentation. Generating accurate image-based obstacle maps in cluttered underwater environments, such as coral reefs, are essential for robust robotic path planning and navigation. However, these maps can be challenged by factors including visibility, lighting and dynamic objects (e.g. fish) that may lead to falsely identified free space or dynamic objects which trajectory planners may react to undesirably. We propose combining feature-based stereo matching with learning-based segmentation to produce a more robust obstacle map. This approach considers direct binary learning of the presence or absence of underwater obstacles, as well as a multiclass learning approach to classify their distance (near, mid and far) in the scene. An enhancement to the binary map is also shown by including depth information from sparse stereo matching to produce 3D obstacle maps of the scene. The performance is evaluated using field data collected in cluttered, and at times, visually degraded coral reef environments. The results show improved image-wide obstacle detection, rejection of transient objects (such as fish), and range estimation compared to feature-based sparse and dense stereo point clouds alone.


Title: Long-Term Occupancy Grid Prediction Using Recurrent Neural Networks
Key Words: convolutional neural nets  learning (artificial intelligence)  Monte Carlo methods  neural net architecture  optical radar  recurrent neural nets  long-term occupancy grid prediction  recurrent neural networks  scene evolution  automated driving  Lidar grid fusion  birds eye view  RNNs  CNN architecture  convolutional long short-term memories  ConvLSTMs  Monte Carlo approach  Vehicle dynamics  Training  Predictive models  Task analysis  Computer architecture  Recurrent neural networks  Correlation 
Abstract: We tackle the long-term prediction of scene evolution in a complex downtown scenario for automated driving based on Lidar grid fusion and recurrent neural networks (RNNs). A bird's eye view of the scene, including occupancy and velocity, is fed as a sequence to a RNN which is trained to predict future occupancy. The nature of prediction allows generation of multiple hours of training data without the need of manual labeling. Thus, the training strategy and loss function are designed for long sequences of real-world data (unbalanced, continuously changing situations, false labels, etc.). The deep CNN architecture comprises convolutional long short-term memories (ConvLSTMs) to separate static from dynamic regions and to predict dynamic objects in future frames. Novel recurrent skip connections show the ability to predict small occluded objects, i.e. pedestrians, and occluded static regions. Spatio-temporal correlations between grid cells are exploited to predict multimodal future paths and interactions between objects. Experiments also quantity improvements to our previous network, a Monte Carlo approach, and literature.


Title: Personalized Online Learning of Whole-Body Motion Classes using Multiple Inertial Measurement Units
Key Words: body sensor networks  feature extraction  image classification  image motion analysis  learning (artificial intelligence)  pose estimation  personalized online learning  whole-body motion classes  online action classification  machine learning applications  personal behavior patterns  offline average user models  personalized models  motion sequences  inertial measuring units  Adaptation models  Task analysis  Sensors  Computational modeling  Data models  Hardware  Training 
Abstract: Online action classification is an important field of research, enabling the particularly interesting application scenario of controlling wearable devices which actively support the user's motions. The majority of machine learning applications of real-world systems are based on pre-trained average-user models without any personalization. Our long-term goal is to provide a system that adapts to its user's personal behavior patterns on the fly and in real-time. Ideally, we want to initiate a continuous collaboration between the system and the user where both alternatively adjust to each other to maximize the system's utility. Such tasks are not feasible with static models. In this paper, we investigate the potential and benefits of personalized online learning in the task of online action classification. We record motion sequences of different subjects wearing the Xsens bodysuit, which incorporates multiple inertial measuring units, enabling a fine-grained discrimination of motions. On this basis, we first perform a feature selection, showing that only a few sensors are necessary to achieve a high classification performance. Subsequently, we compare the recognition capabilities of offline average user models against personalized models trained in an online way. Our experiments conclude that personalized models require only few data to outperform average user systems and are particularly valuable for applications with limited computational hardware which rely on the raw sensor inputs only.


Title: Knowledge is Never Enough: Towards Web Aided Deep Open World Recognition
Key Words: data mining  learning (artificial intelligence)  mobile robots  object recognition  robot vision  deep learning architecture  deep network  deep extension  nonparametric model  autonomous mining  robot platform  deep open world recognition  visual knowledge gaps  open set recognition  visual modules  Robots  Visualization  Training  Feature extraction  Task analysis  Artificial neural networks  Semantics 
Abstract: While today's robots are able to perform sophisticated tasks, they can only act on objects they have been trained to recognize. This is a severe limitation: any robot will inevitably see new objects in unconstrained settings, and thus will always have visual knowledge gaps. However, standard visual modules are usually built on a limited set of classes and are based on the strong prior that an object must belong to one of those classes. Identifying whether an instance does not belong to the set of known categories (i.e. open set recognition), only partially tackles this problem, as a truly autonomous agent should be able not only to detect what it does not know, but also to extend dynamically its knowledge about the world. We contribute to this challenge with a deep learning architecture that can dynamically update its known classes in an end-to-end fashion. The proposed deep network, based on a deep extension of a non-parametric model, detects whether a perceived object belongs to the set of categories known by the system and learns it without the need to retrain the whole system from scratch. Annotated images about the new category can be provided by an `oracle' (i.e. human supervision), or by autonomous mining of the Web. Experiments on two different databases and on a robot platform demonstrate the promise of our approach.


Title: Inferring Robot Morphology from Observation of Unscripted Movement
Key Words: image colour analysis  learning (artificial intelligence)  manipulator kinematics  mobile robots  motion control  multi-robot systems  recurrent neural nets  robot vision  low-cost RGB-D camera output  task sharing  shared communication protocol  centralized planner  shared action  kinematic model  large-scale data  RNN-based methods  unscripted movement observation  robots morphological structure  Robot sensing systems  Robot kinematics  Three-dimensional displays  Morphology  Task analysis  Manipulators 
Abstract: Task sharing between heterogeneous robots currently requires a priori capability knowledge, a shared communication protocol, or a centralized planner. However, in practice, when two robots are brought together, the effort required to construct shared action and structure models can be significant. In this paper, we describe our approach to determining the kinematic model of a robot based purely on observation of unscripted movement. We describe construction of large-scale data simulating low-cost RGB-D camera output, and application of two different RNN-based methods to the learning problem. Our results suggest that this is an efficient and effective way to determine a robot's morphological structure without requiring communication or pre-existing knowledge of its capabilities.


Title: Joint Learning of Instance and Semantic Segmentation for Robotic Pick-and-Place with Heavy Occlusions in Clutter
Key Words: feature extraction  image segmentation  learning (artificial intelligence)  manipulators  robot vision  image-level reasoning  joint learning model  visible region masks  occluded region masks  instance occlusion segmentation  semantic occlusion segmentation  instance segmentation model  feature extractor  robotic pick-and-place tasks  Feature extraction  Image segmentation  Semantics  Task analysis  Robots  Cognition  Predictive models 
Abstract: We present joint learning of instance and semantic segmentation for visible and occluded region masks. Sharing the feature extractor with instance occlusion segmentation, we introduce semantic occlusion segmentation into the instance segmentation model. This joint learning fuses the instance-and image-level reasoning of the mask prediction on the different segmentation tasks, which was missing in the previous work of learning instance segmentation only (instance-only). In the experiments, we evaluated the proposed joint learning comparing the instance-only learning on the test dataset. We also applied the joint learning model to 2 different types of robotic pick-and-place tasks (random and target picking) and evaluated its effectiveness to achieve real-world robotic tasks.


Title: Weakly Supervised Recognition of Surgical Gestures
Key Words: feature extraction  Gaussian processes  gesture recognition  image classification  image motion analysis  image representation  image segmentation  medical image processing  medical robotics  robot kinematics  surgery  trajectory control  unsupervised learning  action recognition  surgical trajectories  ground truth annotations  gesture recognition  surgical demonstrations  kinematic trajectories  surgical robots  surgical gestures  automatic segmentation  surgical skill assessment  surgical automation  unsupervised learning methods  action units  supervised recognition approaches  GMM-based algorithm  task-agnostic initialization methods  Needles  Trajectory  Kinematics  Measurement  Tools  Task analysis  Robots  Classification  Gaussian Mixture Models  robotic surgery  kinematics  surgical gesture recognition 
Abstract: Kinematic trajectories recorded from surgical robots contain information about surgical gestures and potentially encode cues about surgeon's skill levels. Automatic segmentation of these trajectories into meaningful action units could help to develop new metrics for surgical skill assessment as well as to simplify surgical automation. State-of-the-art methods for action recognition relied on manual labelling of large datasets, which is time consuming and error prone. Unsupervised methods have been developed to overcome these limitations. However, they often rely on tedious parameter tuning and perform less well than supervised approaches, especially on data with high variability such as surgical trajectories. Hence, the potential of weak supervision could be to improve unsupervised learning while avoiding manual annotation of large datasets. In this paper, we used at a minimum one expert demonstration and its ground truth annotations to generate an appropriate initialization for a GMM-based algorithm for gesture recognition. We showed on real surgical demonstrations that the latter significantly outperforms standard task-agnostic initialization methods. We also demonstrated how to improve the recognition accuracy further by redefining the actions and optimising the inputs.


Title: ClusterNav: Learning-Based Robust Navigation Operating in Cluttered Environments
Key Words: assisted living  geriatrics  human-robot interaction  learning (artificial intelligence)  mobile robots  navigation  path planning  robust control  service robots  telerobotics  nonexpert users  ClusterNav  cluttered environments  robust autonomous navigation  social robots  elderly users  traditional model-based navigation techniques  stable theoretical foundation  practical foundation  autonomous operation  domestic environments  acceptable behaviour  novel learning-based technique  geometric representation  acceptable manner  elderly care facility  traditional model-based approach  learning-based robust navigation  Trajectory  Navigation  Robot kinematics  Clustering algorithms  Reinforcement learning 
Abstract: Robust autonomous navigation is one of the most important aspects in the acceptance of social robots by elderly users. Traditional model-based navigation techniques provide a stable theoretical and practical foundation for autonomous operation in domestic environments, but fall short in achieving human-like, acceptable behaviour while still being able to robustly navigate cluttered environments. In this work, we propose ClusterNav, a novel learning-based technique for navigation. Our technique consists of teaching the robot how it should move in the environment in a human-like manner, capturing key features of this demonstration in a geometric representation of the environment. This representation is then used to generate new trajectories for execution, allowing the robot move in an acceptable manner. We have tested our technique in a real environment in an elderly care facility, comparing it with the traditional model-based approach. Tests involved both expert and non-expert users teleoperating the robot. Results show that ClusterNav is capable of navigating the environment, achieving better similarity with the reference trajectories and higher execution speed when compared to the model-based approach.


Title: Adaptive motor control and learning in a spiking neural network realised on a mixed-signal neuromorphic processor
Key Words: control engineering computing  feedback  learning (artificial intelligence)  mobile robots  neural chips  neurocontrollers  adaptive motor control  mixed-signal neuromorphic processor  neuromorphic computing  biological neural networks  spiking neural network architecture  sensory feedback  control rotational velocity  robotic vehicle  correct motor command  miniature mobile vehicle  two-layer spiking neural network  neuromorphic chip  purely neuromorphic motor control  spiking neurons  neuromorphic device  on-chip plastic synaptic weights  Neuromorphics  Neurons  Biological neural networks  Computer architecture  Robot sensing systems  Sociology 
Abstract: Neuromorphic computing is a new paradigm for design of both the computing hardware and algorithms inspired by biological neural networks. The event-based nature and the inherent parallelism make neuromorphic computing a promising paradigm for building efficient neural network based architectures for control of fast and agile robots. In this paper, we present a spiking neural network architecture that uses sensory feedback to control rotational velocity of a robotic vehicle. When the velocity reaches the target value, the mapping from the target velocity of the vehicle to the correct motor command, both represented in the spiking neural network on the neuromorphic device, is autonomously stored on the device using on-chip plastic synaptic weights. We validate the controller using a wheel motor of a miniature mobile vehicle and inertia measurement unit as the sensory feedback and demonstrate online learning of a simple “inverse model” in a two-layer spiking neural network on the neuromorphic chip. The prototype neuromorphic device that features 256 spiking neurons allows us to realise a simple proof of concept architecture for the purely neuromorphic motor control and learning. The architecture can be easily scaled-up if a larger neuromorphic device is available.


Title: Adaptive Genomic Evolution of Neural Network Topologies (AGENT) for State-to-Action Mapping in Autonomous Agents
Key Words: autonomous aerial vehicles  collision avoidance  genetic algorithms  learning (artificial intelligence)  neurocontrollers  autonomous agents  neural networks  NN  evolutionary algorithm  state-to-action mapping model  neuroevolution process  population diversity  unmanned aerial vehicle collision avoidance problem  adaptive genomic evolution  neural network topologies  augmented topologies formalism  Open AI platform  UAV collision avoidance problem  Genomics  Topology  Sociology  Statistics  Artificial neural networks  Network topology 
Abstract: Neuroevolution is a process of training neural networks (NN) through an evolutionary algorithm, usually to serve as a state-to-action mapping model in control or reinforcement learning-type problems. This paper builds on the Neuro Evolution of Augmented Topologies (NEAT) formalism that allows designing topology and weight evolving NNs. Fundamental advancements are made to the neuroevolution process to address premature stagnation and convergence issues, central among which is the incorporation of automated mechanisms to control the population diversity and average fitness improvement within the neuroevolution process. Insights into the performance and efficiency of the new algorithm is obtained by evaluating it on three benchmark problems from the Open AI platform and an Unmanned Aerial Vehicle (UAV) collision avoidance problem.


Title: End to End Learning of a Multi-Layered Snn Based on R-Stdp for a Target Tracking Snake-Like Robot
Key Words: learning (artificial intelligence)  mobile robots  neurocontrollers  path planning  target tracking  multilayered SNN  target tracking snake-like robot  end-to-end learning approach  R-STDP  SNN controller  target tracking tasks  reward-modulated spike-timing-dependent plasticity  multilayered spiking neural network  learning algorithms  lateral tracking  Neurons  Target tracking  Task analysis  Synapses  Robot sensing systems  Training 
Abstract: This paper introduces an end-to-end learning approach based on Reward-modulated Spike-Timing-Dependent Plasticity (R-STDP) for a multi-layered spiking neural network (SNN). As a case study, a snake-like robot is used as an agent to perform target tracking tasks on the basis of our proposed approach. Since the key of R-STDP is to use rewards to modulate synapse strengthens, we first propose a general way to propagate the reward back through a multi-layered SNN. Upon the proposed approach, we build up an SNN controller that drives a snake-like robot for performing target tracking tasks. We demonstrate the practicability and advantage of our approach in terms of lateral tracking accuracy by comparing it to other state-of-the-art learning algorithms for SNNs based on R-STDP.


Title: DeepSignals: Predicting Intent of Drivers Through Visual Signals
Key Words: alarm systems  belief networks  image sequences  learning (artificial intelligence)  neural nets  psychology  traffic engineering computing  video signal processing  driver intention detection  emergency flashers  turn signals  stops  lane changes  sudden events  visual signals  DeepSignals  temporal information  spatial information  deep neural network  video sequences  potentially critical reaction time  Feature extraction  Convolution  Vehicles  Visualization  Streaming media  Computer architecture  Logic gates 
Abstract: Detecting the intention of drivers is an essential task in self-driving, necessary to anticipate sudden events like lane changes and stops. Turn signals and emergency flashers communicate such intentions, providing seconds of potentially critical reaction time. In this paper, we propose to detect these signals in video sequences by using a deep neural network that reasons about both spatial and temporal information. Our experiments on more than a million frames show high per-frame accuracy in very challenging scenarios.


Title: Go with the Flow: Exploration and Mapping of Pedestrian Flow Patterns from Partial Observations
Key Words: mobile robots  navigation  path planning  pedestrians  stochastic processes  pedestrian flow patterns  partial observations  safe robot navigation  spatial constraints  temporal constraints  multiple Poisson processes  long-term pedestrian datasets  uninformed exploration strategies  human motion patterns  robot navigation  Robots  Uncertainty  Predictive models  Buildings  Probabilistic logic  Data models  Collision avoidance 
Abstract: Understanding how people are likely to behave in an environment is a key requirement for efficient and safe robot navigation. However, mobile platforms are subject to spatial and temporal constraints, meaning that only partial observations of human activities are typically available to a robot, while the activity patterns of people in a given environment may also change at different times. To address these issues we present as the main contribution an exploration strategy for acquiring models of pedestrian flows, which decides not only the locations to explore but also the times when to explore them. The approach is driven by the uncertainty from multiple Poisson processes built from past observations. The approach is evaluated using two long-term pedestrian datasets, comparing its performance against uninformed exploration strategies. The results show that when using the uncertainty in the exploration policy, model accuracy increases, enabling faster learning of human motion patterns.


Title: Memory Efficient Experience Replay for Streaming Learning
Key Words: cognition  learning (artificial intelligence)  neural nets  neurophysiology  learning settings  memory efficient experience replay  supervised machine learning  static settings  data stream  streaming learning  conventional deep neural networks  full rehearsal  memory efficient rehearsal  ExStream algorithm  Clustering algorithms  Training  Prototypes  Robots  Buffer storage  Partitioning algorithms  Neural networks 
Abstract: In supervised machine learning, an agent is typically trained once and then deployed. While this works well for static settings, robots often operate in changing environments and must quickly learn new things from data streams. In this paradigm, known as streaming learning, a learner is trained online, in a single pass, from a data stream that cannot be assumed to be independent and identically distributed (iid). Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting. An old fix to both of these issues is rehearsal. To learn a new example, rehearsal mixes it with previous examples, and then this mixture is used to update the DNN. Full rehearsal is slow and memory intensive because it stores all previously observed examples, and its effectiveness for preventing catastrophic forgetting has not been studied in modern DNNs. Here, we describe the ExStream algorithm for memory efficient rehearsal and compare it to alternatives. We find that full rehearsal can eliminate catastrophic forgetting in a variety of streaming learning settings, with ExStream performing well using far less memory and computation.


Title: RoboCSE: Robot Common Sense Embedding
Key Words: belief networks  embedded systems  home automation  mobile robots  service robots  statistical analysis  RoboCSE  robot common sense embedding  autonomous service robots  semantic knowledge  AI2Thor  statistical significant  home environment simulator  Word2Vec  Bayesian logic network  MatterPort3D  Semantics  Training  Robot sensing systems  Cognition  Computational modeling  Training data 
Abstract: Autonomous service robots require computational frameworks that allow them to generalize knowledge to new situations in a manner that models uncertainty while scaling to real-world problem sizes. The Robot Common Sense Embedding (RoboCSE) showcases a class of computational frameworks, multi-relational embeddings, that have not been leveraged in robotics to model semantic knowledge. We validate RoboCSE on a realistic home environment simulator (AI2Thor) to measure how well it generalizes learned knowledge about object affordances, locations, and materials. Our experiments show that RoboCSE can perform prediction better than a baseline that uses pre-trained embeddings, such as Word2Vec, achieving statistically significant improvements while using orders of magnitude less memory than our Bayesian Logic Network baseline. In addition, we show that predictions made by RoboCSE are robust to significant reductions in data available for training as well as domain transfer to MatterPort3D, achieving statistically significant improvements over a baseline that memorizes training data.


Title: Neural Lander: Stable Drone Landing Control Using Learned Dynamics
Key Words: aerodynamics  aircraft control  autonomous aerial vehicles  control system synthesis  feedback  helicopters  learning (artificial intelligence)  linearisation techniques  neurocontrollers  nonlinear control systems  robust control  trajectory control  cross-table trajectory tracking cases  Neural Lander  stable drone landing control  learned dynamics  precise near-ground trajectory control  multirotor drones  complex aerodynamic effects  multirotor airflow  complex effects  smooth landing  robust nonlinear controller  control performance  nominal dynamics model  Deep Neural Network  high-order interactions  Lipschitz constant  Lipschitz property  nonlinear feedback linearization controller  DNN-based nonlinear feedback controller  arbitrarily large neural nets  Baseline Nonlinear Tracking Controller  Aerodynamics  Stability analysis  Rotors  Neural networks  Trajectory tracking  Training  Vehicle dynamics 
Abstract: Precise near-ground trajectory control is difficult for multi-rotor drones, due to the complex aerodynamic effects caused by interactions between multi-rotor airflow and the environment. Conventional control methods often fail to properly account for these complex effects and fall short in accomplishing smooth landing. In this paper, we present a novel deep-learning-based robust nonlinear controller (Neural-Lander) that improves control performance of a quadrotor during landing. Our approach combines a nominal dynamics model with a Deep Neural Network (DNN) that learns high-order interactions. We apply spectral normalization (SN) to constrain the Lipschitz constant of the DNN. Leveraging this Lipschitz property, we design a nonlinear feedback linearization controller using the learned model and prove system stability with disturbance rejection. To the best of our knowledge, this is the first DNN-based nonlinear feedback controller with stability guarantees that can utilize arbitrarily large neural nets. Experimental results demonstrate that the proposed controller significantly outperforms a Baseline Nonlinear Tracking Controller in both landing and cross-table trajectory tracking cases. We also empirically show that the DNN generalizes well to unseen data outside the training domain.


Title: Distributional Deep Reinforcement Learning with a Mixture of Gaussians
Key Words: Gaussian processes  learning (artificial intelligence)  statistical distributions  discrete distribution  softmax parametrization  KL divergence loss  discretization hyperparameters  Atari games  distributional deep reinforcement learning  mixture density network  return distribution  mixtures of Gaussians  Jensen-Tsallis distance  autonomous vehicle driving  Measurement  Reinforcement learning  Games  Approximation algorithms  Neural networks  Autonomous vehicles  Stochastic processes 
Abstract: In this paper, we propose a novel distributional reinforcement learning (RL) method which models the distribution of the sum of rewards using a mixture density network. Recently, it has been shown that modeling the randomness of the return distribution leads to better performance in Atari games and control tasks. Despite the success of the prior work, it has limitations which come from the use of a discrete distribution. First, it needs a projection step and softmax parametrization for the distribution, since it minimizes the KL divergence loss. Secondly, its performance depends on discretization hyperparameters such as the number of atoms and bounds of the support which require domain knowledge. We mitigate these problems with the proposed parameterization, a mixture of Gaussians. Furthermore, we propose a new distance metric called the Jensen-Tsallis distance, which allows the computation of the distance between two mixtures of Gaussians in a closed form. We have conducted various experiments to validate the proposed method, including Atari games and autonomous vehicle driving.


Title: Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning
Key Words: learning (artificial intelligence)  legged locomotion  optimisation  control policy  walking gaits  deep reinforcement learning  learning-based approaches  control network  design distribution  controller access  design parameters  legged locomotion  physical design  Physical design  Optimization  Reinforcement learning  Training  Legged locomotion  Task analysis 
Abstract: The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learning-based approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical-i.e., by picking a design and training a control policy for it. Since training these policies is time-consuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that jointly optimizes over the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines across different settings.


Title: Surgical instrument segmentation for endoscopic vision with data fusion of cnn prediction and kinematic pose
Key Words: convolutional neural nets  endoscopes  feature extraction  image segmentation  learning (artificial intelligence)  medical image processing  object tracking  particle filtering (numerical methods)  pose estimation  sensor fusion  surgery  endoscopic vision  data fusion  robust surgical instrument segmentation  instrument segmentation method  convolutional neural networks prediction  kinematic pose information  CNN model ToolNet-C  convolutional feature extractor  pixel-wise segmentor  labeled images  silhouette projection  instrument body  endoscopic image  shape matching likelihood  accurate silhouette mask  final segmentation output  surgical navigation system  debrider instrument  unlabeled images  Instruments  Image segmentation  Feature extraction  Kinematics  Training  Shape  Real-time systems 
Abstract: The real-time and robust surgical instrument segmentation is an important issue for endoscopic vision. We propose an instrument segmentation method fusing the convolutional neural networks (CNN) prediction and the kinematic pose information. First, the CNN model ToolNet-C is designed, which cascades a convolutional feature extractor trained over numerous unlabeled images and a pixel-wise segmentor trained on few labeled images. Second, the silhouette projection of the instrument body onto the endoscopic image is implemented based on the measured kinematic pose. Third, the particle filter with the shape matching likelihood and the weight suppression is proposed for data fusion, whose estimate refines the kinematic pose. The refined pose determines an accurate silhouette mask, which is the final segmentation output. The experiments are conducted with a surgical navigation system, several animal-tissue backgrounds, and a debrider instrument.


Title: Inferring Compact Representations for Efficient Natural Language Understanding of Robot Instructions
Key Words: control engineering computing  graph theory  human-robot interaction  inference mechanisms  learning (artificial intelligence)  mobile robots  natural language processing  probability  robot programming  compact environment model  perceptual classifiers  succinct instruction-specific environment representation  probabilistic graphical models  natural language symbol grounding  robot instructions  approximate inference algorithms  natural language understanding  environment-related information  human-robot interaction  compact representations  Grounding  Natural languages  Computational modeling  Adaptation models  Robots  Semantics  Task analysis 
Abstract: The speed and accuracy with which robots are able to interpret natural language is fundamental to realizing effective human-robot interaction. A great deal of attention has been paid to developing models and approximate inference algorithms that improve the efficiency of language understanding. However, existing methods still attempt to reason over a representation of the environment that is flat and unnecessarily detailed, which limits scalability. An open problem is then to develop methods capable of producing the most compact environment model sufficient for accurate and efficient natural language understanding. We propose a model that leverages environment-related information encoded within instructions to identify the subset of observations and perceptual classifiers necessary to perceive a succinct, instruction-specific environment representation. The framework uses three probabilistic graphical models trained from a corpus of annotated instructions to infer salient scene semantics, perceptual classifiers, and grounded symbols. Experimental results on two robots operating in different environments demonstrate that by exploiting the content and the structure of the instructions, our method learns compact environment representations that significantly improve the efficiency of natural language symbol grounding.


Title: Improving Grounded Natural Language Understanding through Human-Robot Dialog
Key Words: grammars  human-robot interaction  interactive systems  learning (artificial intelligence)  natural language processing  natural language understanding  human-robot dialog  virtual setting  Amazon Mechanical Turk  physical robot platform  concept grounding  language parsing  robot actions  natural language commands  end-to-end pipeline  perceptual concepts  language constructions  human environments  human commands  Semantics  Task analysis  Grounding  Natural languages  Robot sensing systems  Pipelines 
Abstract: Natural language understanding for robotics can require substantial domain- and platform-specific engineering. For example, for mobile robots to pick-and-place objects in an environment to satisfy human commands, we can specify the language humans use to issue such commands, and connect concept words like red can to physical object properties. One way to alleviate this engineering for a new domain is to enable robots in human environments to adapt dynamically-continually learning new language constructions and perceptual concepts. In this work, we present an end-to-end pipeline for translating natural language commands to discrete robot actions, and use clarification dialogs to jointly improve language parsing and concept grounding. We train and evaluate this agent in a virtual setting on Amazon Mechanical Turk, and we transfer the learned agent to a physical robot platform to demonstrate it in the real world.


Title: Prospection: Interpretable plans from language by predicting the future
Key Words: control engineering computing  learning (artificial intelligence)  natural language processing  robot programming  high-level human instructions  natural-language command  crowd-sourcing  plan fidelity  representations learning  robot agent  Task analysis  Robots  Training  Natural languages  Visualization  Planning  Predictive models 
Abstract: High-level human instructions often correspond to behaviors with multiple implicit steps. In order for robots to be useful in the real world, they must be able to to reason over both motions and intermediate goals implied by human instructions. In this work, we propose a framework for learning representations that convert from a natural-language command to a sequence of intermediate goals for execution on a robot. A key feature of this framework is prospection, training an agent not just to correctly execute the prescribed command, but to predict a horizon of consequences of an action before taking it. We demonstrate the fidelity of plans generated by our framework when interpreting real, crowd-sourced natural language commands for a robot in simulated scenes.


Title: An Interactive Scene Generation Using Natural Language
Key Words: dexterous manipulators  discrete event systems  learning (artificial intelligence)  natural language processing  natural scenes  text analysis  interactive scene generation  robotic drawing  discrete event system  MSCOCO evaluation dataset  CIDEr metric  ROUGH-L metric  METEOR metric  Amazon Mechanical Turk  natural language descriptions  Layout  Dogs  Robots  Generators  Natural language processing  Semantics  Training 
Abstract: Scene generation is an important step of robotic drawing. Recent works have shown success in scene generation conditioned on text using a variety of approaches, with which the generated scenes cannot be revised after its generation. To allow modification on generated scenes, we model the scene generation process as a discrete event system. Instead of training text-to-pixel mappings using large datasets, the proposed approach uses object instances retrieved from the Internet to synthesize scenes. Evaluated on 128 experiments using MSCOCO evaluation dataset, the result shows the scene generation performance has been increased by 197%, 22.3%, and 55.7% compared with the state of the art approach on three standard metrics (CIDEr, ROUGH-L, METEOR), respectively. Human evaluation conducted on Amazon Mechanical Turk shows over 80% of generated scenes are considered to have higher recognizability and better alignment with natural language descriptions than baseline works.


Title: Efficient Generation of Motion Plans from Attribute-Based Natural Language Instructions Using Dynamic Constraint Mapping
Key Words: graph theory  human-robot interaction  learning (artificial intelligence)  mobile robots  motion control  natural language processing  optimisation  path planning  robot programming  dynamic constraint mapping  robot motion planning  dynamic grounding graph  7-DOF Fetch robot  factor graph  optimization-based motion planning  parametric constraints  attribute-based natural language instructions  motion plans  Robots  Grounding  Natural languages  Cost function  Planning  Dynamics  Heuristic algorithms 
Abstract: We present an algorithm for combining natural language processing (NLP) and fast robot motion planning to automatically generate robot movements. Our formulation uses a novel concept called Dynamic Constraint Mapping to transform complex, attribute-based natural language instructions into appropriate cost functions and parametric constraints for optimization-based motion planning. We generate a factor graph from natural language instructions called the Dynamic Grounding Graph (DGG), which takes latent parameters into account. The coefficients of this factor graph are learned based on conditional random fields (CRFs) and are used to dynamically generate the constraints for motion planning. We map the cost function directly to the motion parameters of the planner and compute smooth trajectories in dynamic scenes. We highlight the performance of our approach in a simulated environment and via a human interacting with a 7-DOF Fetch robot using intricate language commands including negation, orientation specification, and distance constraints.


Title: Learning from Extrapolated Corrections
Key Words: control engineering computing  extrapolation  function approximation  learning (artificial intelligence)  robot programming  extrapolated corrections  cost functions  user guidance  extrapolation problem  online function approximation  function space  nonEuclidean norms  robot learning  Trajectory  Cost function  Robot kinematics  Function approximation  Kernel  Estimation 
Abstract: Our goal is to enable robots to learn cost functions from user guidance. Often it is difficult or impossible for users to provide full demonstrations, so corrections have emerged as an easier guidance channel. However, when robots learn cost functions from corrections rather than demonstrations, they have to extrapolate a small amount of information - the change of a waypoint along the way - to the rest of the trajectory. We cast this extrapolation problem as online function approximation, which exposes different ways in which the robot can interpret what trajectory the person intended, depending on the function space used for the approximation. Our simulation results and user study suggest that using function spaces with non-Euclidean norms can better capture what users intend, particularly if environments are uncluttered. This, in turn, can lead to the robot learning a more accurate cost function and improves the user's subjective perceptions of the robot.


Title: Merging Position and orientation Motion Primitives
Key Words: image motion analysis  learning (artificial intelligence)  mobile robots  motion control  path planning  position control  robot programming  stability  orientation motion primitives  complex robotic trajectories  time series  dynamical systems  merging position  stability analysis  merging sequential motion primitives  pose trajectories  Quaternions  Robots  Stability analysis  Trajectory  Task analysis  Acceleration  Clocks 
Abstract: In this paper, we focus on generating complex robotic trajectories by merging sequential motion primitives. A robotic trajectory is a time series of positions and orientations ending at a desired target. Hence, we first discuss the generation of converging pose trajectories via dynamical systems, providing a rigorous stability analysis. Then, we present approaches to merge motion primitives which represent both the position and the orientation part of the motion. Developed approaches preserve the shape of each learned movement and allow for continuous transitions among succeeding motion primitives. Presented methodologies are theoretically described and experimentally evaluated, showing that it is possible to generate a smooth pose trajectory out of multiple motion primitives.


Title: Learning Haptic Exploration Schemes for Adaptive Task Execution
Key Words: end effectors  haptic interfaces  human-robot interaction  learning (artificial intelligence)  mobile robots  robot vision  teaching  haptic exploration schemes  adaptive task execution  compliant robots  kinesthetic teaching  programming physical interactions  force data  force sensing  autonomous exploration strategies  object targeted manner  learning framework  adaptive robot  systematically changing environment  generated behavior  haptic exploration skills  relative manipulation skills  manipulation task  adaptive task structure  unseen object locations  teaching strategy  Grippers  Robot sensing systems  Task analysis  Force  Motion segmentation 
Abstract: The recent generation of compliant robots enables kinesthetic teaching of novel skills by human demonstration. This enables strategies to transfer tasks to the robot in a more intuitive way than conventional programming interfaces. Programming physical interactions can be achieved by manually guiding the robot to learn the behavior from the motion and force data. To let the robot react to changes in the environment, force sensing can be used to identify constraints and act accordingly. While autonomous exploration strategies in the whole workspace are time consuming, we propose a way to learn these schemes from human demonstrations in an object targeted manner. The presented teaching strategy and the learning framework allow to generate adaptive robot behaviors relying on the robot's sense of touch in a systematically changing environment. A generated behavior consists of a hierarchical representation of skills, where haptic exploration skills are used to touch the environment with the end effector, and relative manipulation skills, which are parameterized according to previous exploration events. The effectiveness of the approach has been proven in a manipulation task, where the adaptive task structure is able to generalize to unseen object locations. The robot autonomously manipulates objects without relying on visual feedback.


Title: Learning Motion Trajectories from Phase Space Analysis of the Demonstration
Key Words: approximation theory  dexterous manipulators  learning (artificial intelligence)  linear systems  motion control  path planning  regression analysis  trajectory control  closed form solutions  task generalization  phase space analysis  motion trajectories  kinematic based task  dynamic motion  phase space model  sequential trajectory task  energy-based analysis  linear time invariant equations  trajectory segments  linear piece-wise regression method  Trajectory  Mathematical model  Dynamics  Task analysis  Motion segmentation  Adaptation models  Space exploration 
Abstract: A major goal of learning from demonstration is task generalization via observation of a teacher. In this paper, we propose a novel framework for learning motion from a single demonstration. Our approach reconstructs the demonstrated trajectory's phase space curve via a linear piece-wise regression method. We approximate dynamics of trajectory segments with linear time invariant equations, each yielding closed form solutions. We show convergence to desired phase space states via an energy-based analysis. The robustness of the model is evaluated on a robot for a sequential trajectory task. Additionally, we show the advantages that the phase space model has over the dynamic motion primitive for a kinematic based task.


Title: I Can See Clearly Now: Image Restoration via De-Raining
Key Words: drops  image denoising  image restoration  image segmentation  learning (artificial intelligence)  rain  stereo image processing  traffic engineering computing  realrain dataset  image restoration  image reconstruction  computer-generated adherent water droplets  streaks  CamVid road marking segmentation dataset  Cityscapes semantic segmentation datasets  stereo dataset  denoising generator training  de-raining  lens  Rain  Image segmentation  Lenses  Task analysis  Computational modeling  Roads  Generators 
Abstract: We present a method for improving segmentation tasks on images affected by adherent rain drops and streaks. We introduce a novel stereo dataset recorded using a system that allows one lens to be affected by real water droplets while keeping the other lens clear. We train a denoising generator using this dataset and show that it is effective at removing the effect of real water droplets, in the context of image reconstruction and road marking segmentation. To further test our de-noising approach, we describe a method of adding computer-generated adherent water droplets and streaks to any images, and use this technique as a proxy to demonstrate the effectiveness of our model in the context of general semantic segmentation. We benchmark our results using the CamVid road marking segmentation dataset, Cityscapes semantic segmentation datasets and our own realrain dataset, and show significant improvement on all tasks.


Title: Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs
Key Words: computer vision  control engineering computing  convolutional neural nets  image segmentation  learning (artificial intelligence)  public domain software  robot vision  deployment interface  existing robotics codebase  label prediction  open-source training  CNNs  semantic segmentation labels each pixel  class label  convolutional neural networks  high-quality open-source frameworks  neural network implementation  semantic segmentation task  modular approach  robotic platform  CNN approach  robotics research  open-source codebase  semantic segmentation CNN  semantic annotation  Bonnet framework  Python  TensorFlow  C++ library  Semantics  Training  C++ languages  Tools  Robot sensing systems  Hardware 
Abstract: The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.


Title: Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations
Key Words: computer vision  image annotation  image reconstruction  image segmentation  learning (artificial intelligence)  depth estimation  asymmetric annotations  deep learning models  sensory information extractors  generic GPU cards  asymmetric datasets  real-time semantic segmentation network  floating point operations  hard knowledge distillation  dense 3D semantic reconstruction  Task analysis  Semantics  Estimation  Real-time systems  Adaptation models  Image segmentation  Robots 
Abstract: Deployment of deep learning models in robotics as sensory information extractors can be a daunting task to handle, even using generic GPU cards. Here, we address three of its most prominent hurdles, namely, i) the adaptation of a single model to perform multiple tasks at once (in this work, we consider depth estimation and semantic segmentation crucial for acquiring geometric and semantic understanding of the scene), while ii) doing it in real-time, and iii) using asymmetric datasets with uneven numbers of annotations per each modality. To overcome the first two issues, we adapt a recently proposed real-time semantic segmentation network, making changes to further reduce the number of floating point operations. To approach the third issue, we embrace a simple solution based on hard knowledge distillation under the assumption of having access to a powerful `teacher' network. We showcase how our system can be easily extended to handle more tasks, and more datasets, all at once, performing depth estimation and segmentation both indoors and outdoors with a single model. Quantitatively, we achieve results equivalent to (or better than) current state-of-the-art approaches with one forward pass costing just 13ms and 6.5 GFLOPs on 640×480 inputs. This efficiency allows us to directly incorporate the raw predictions of our network into the SemanticFusion framework [1] for dense 3D semantic reconstruction of the scene.


Title: Real-Time Monocular Object-Model Aware Sparse SLAM
Key Words: cameras  convolutional neural nets  feature extraction  learning (artificial intelligence)  mobile robots  object detection  robot vision  SLAM (robots)  mobile robotics  sparse point-based SLAM methods  CNN-based plane detector  semantic SLAM  simultaneous localization and mapping  camera localization  deep-learned object detector  CNN network  semantic objects representation  monocular object-model aware sparse SLAM framework  Simultaneous localization and mapping  Semantics  Image reconstruction  Real-time systems  Cameras  Three-dimensional displays 
Abstract: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in mobile robotics. While sparse point-based SLAM methods provide accurate camera localization, the generated maps lack semantic information. On the other hand, state of the art object detection methods provide rich information about entities present in the scene from a single image. This work incorporates a real-time deep-learned object detector to the monocular SLAM framework for representing generic objects as quadrics that permit detections to be seamlessly integrated while allowing the real-time performance. Finer reconstruction of an object, learned by a CNN network, is also incorporated and provides a shape prior for the quadric leading further refinement. To capture the structure of the scene, additional planar landmarks are detected by a CNN-based plane detector and modelled as independent landmarks in the map. Extensive experiments support our proposed inclusion of semantic objects and planar structures directly in the bundle-adjustment of SLAM - Semantic SLAM- that enriches the reconstructed map semantically, while significantly improving the camera localization.


Title: Robust low-overlap 3-D point cloud registration for outlier rejection
Key Words: feature extraction  image registration  iterative methods  Markov processes  stereo image processing  hidden Markov random field model  iterative closest point algorithm  outlier rejection  3D point cloud registration  Hidden Markov models  Three-dimensional displays  Cloud computing  Probabilistic logic  Robot sensing systems  Solid modeling  Measurement 
Abstract: When registering 3-D point clouds it is expected that some points in one cloud do not have corresponding points in the other cloud. These non-correspondences are likely to occur near one another, as surface regions visible from one sensor pose are obscured or out of frame for another. In this work, a hidden Markov random field model is used to capture this prior within the framework of the iterative closest point algorithm. The EM algorithm is used to estimate the distribution parameters and learn the hidden component memberships. Experiments are presented demonstrating that this method outperforms several other outlier rejection methods when the point clouds have low or moderate overlap.


Title: Continuous Value Iteration (CVI) Reinforcement Learning and Imaginary Experience Replay (IER) For Learning Multi-Goal, Continuous Action and State Space Controllers
Key Words: iterative methods  learning (artificial intelligence)  optimal control  state-space methods  continuous action  state space controllers  goal space  optimal value functions  nonparametric estimators  multiple arbitrary goals  real-world voltage controlled robot  nonobservable Cartesian task space  multigoal  model-free reinforcement learning algorithm  Aerospace electronics  Robot kinematics  Task analysis  Trajectory  Mathematical model  Voltage control 
Abstract: This paper presents a novel model-free Reinforcement Learning algorithm for learning behavior in continuous action, state, and goal spaces. The algorithm approximates optimal value functions using non-parametric estimators. It is able to efficiently learn to reach multiple arbitrary goals in deterministic and nondeterministic environments. To improve generalization in the goal space, we propose a novel sample augmentation technique. Using these methods, robots learn faster and overall better controllers. We benchmark the proposed algorithms using simulation and a real-world voltage controlled robot that learns to maneuver in a non-observable Cartesian task space.


Title: What am I touching? Learning to classify terrain via haptic sensing
Key Words: control engineering computing  haptic interfaces  image classification  learning (artificial intelligence)  legged locomotion  pattern clustering  robot vision  terrain mapping  haptic sensing  mobile robots  real-world outdoors applications  robot control  optimal terrain negotiation  terrain classification  terrain identification  legged robot foot  fixed-length step  controlled environment  clustering method  robot perception  machine learning  Legged locomotion  Robot sensing systems  Computer architecture  Convolution  Force  Foot 
Abstract: Mobile robots are becoming very popular in real-world outdoors applications, where there are many challenges in robot control and perception. One of the most critical problems is to characterise the terrain traversed by the robot. This knowledge is indispensable for optimal terrain negotiation. Currently, most approaches are performing terrain classification from vision, but there is not enough research on terrain identification from a direct interaction of the robot with the environment. In our work, we proposed new methods for classification of force/torque data from an interaction of the legged robot foot with the ground, gathered during the walking process. We provided machine learning methods for terrain classification from raw force/torque signals for which we achieved 93% accuracy on a challenging dataset with 160 minutes of recorded fixed-length steps. We also worked on a dataset where the assumption of a fixed-length step is not valid. In this case, the final result is around 80% of accuracy. The most important fact is that the data in both cases was recorded while the robot was walking, no particular movements or controlled environment were needed. Additionally, we also proposed a clustering method which allows us to learn about the class membership based on the recorded data only, without any human supervision.


Title: Depth Generation Network: Estimating Real World Depth from Stereo and Depth Images*
Key Words: image colour analysis  learning (artificial intelligence)  stereo image processing  Depth Generation Network  real world Depth  dense depth estimation  deep-learning technique  stereo RGB images  stereo pairs  depth ground-truth  stereo setting parameters  image pairs  supervision learning  synthetic depth maps  relative dense depth  stereo geometric settings  optic settings  epipolar geometric cues  DGN  falling things dataset  variational method  Training  Estimation  Three-dimensional displays  Data models  Fats  Cameras  Robots 
Abstract: In this work, we propose the Depth Generation Network (DGN) to address the problem of dense depth estimation by exploiting the variational method and the deep-learning technique. In particular, we focus on improving the feasibility of depth estimation under complex scenarios given stereo RGB images, where the stereo pairs and/or depth ground-truth captured by real sensors may be deteriorated; the stereo setting parameters may be unavailable or unreliable, hence hamper efforts to establish the correspondence between image pairs via supervision learning or epipolar geometric cues. Instead of relying on real data, we supervise the training of our model using synthetic depth maps generated by the simulator, which deliver complex scenes and reliable data with ease. Two non-trivial challenges, i.e., (i) attaining reasonable amount yet realistic samples for training, and (ii) developing a model that adapts to both synthetic and real scenes arise, whereas in this work we mainly deal with the later one yet leveraging state-of-the-art Falling Things (FAT) dataset to overcome the first. Experiments on FAT and KITTI datasets demonstrate that our model estimates relative dense depth in fine details, potentially generalizable to real scenes without knowing the stereo geometric and optic settings.


Title: GraspFusion: Realizing Complex Motion by Learning and Fusing Grasp Modalities with Instance Segmentation
Key Words: grippers  image fusion  image matching  image segmentation  learning (artificial intelligence)  object detection  deep learning  multimodal grippers  simultaneous pinch  multimodal grasp fusion  object-class-agnostic grasp  modality detection  object-class-agnostic instance segmentation  grasp template matching  object manipulation  object geometry  grasp modalities  instance segmentation  integrated system  Image segmentation  Grippers  Grasping  Motion segmentation  Image color analysis  Task analysis 
Abstract: Recent progress of deep learning improved the capability of a robot to find a proper grasp of a novel object for different grasp modalities (e.g., pinch and suction). While these previous studies consider multiple modalities separately, several studies develop multi-modal grippers that can achieve simultaneous pinch and suction grasp (multi-modal grasp fusion) for more capable and stable object manipulation. However, the previous studies with these grippers restrict the situations: simple object geometry and uncluttered environments. To overcome these difficulties, we propose a system that consists of: 1) object-class-agnostic grasp modality detection; 2) object-class-agnostic instance segmentation; and 3) grasp template matching for different modalities. The key idea of our work is the introduction of instance segmentation to fuse multiple modalities regarding each instance eluding a grasp of multiple objects at once. In the experiments, we evaluated the proposed system on the real-world picking task in clutter. The experimental results show that the effectiveness of modality detection, instance segmentation, and the integrated system as a whole.


Title: Factored Contextual Policy Search with Bayesian optimization
Key Words: learning (artificial intelligence)  robots  search problems  truly complex tasks  locally learned policies  data-efficient learning  parametric context space  contextual policy representation  target contexts  task objectives  target position  environment contexts  contextual policy search algorithms  Bayesian optimization approach  active learning settings  faster learning  factored contextual policy search  scarce data  task contexts  Task analysis  Trajectory  Optimization  Bayes methods  Robot kinematics  Entropy 
Abstract: Scarce data is a major challenge to scaling robot learning to truly complex tasks, as we need to generalize locally learned policies over different task contexts. Contextual policy search offers data-efficient learning and generalization by explicitly conditioning the policy on a parametric context space. In this paper, we further structure the contextual policy representation. We propose to factor contexts into two components: target contexts that describe the task objectives, e.g. target position for throwing a ball; and environment contexts that characterize the environment, e.g. initial position or mass of the ball. Our key observation is that experience can be directly generalized over target contexts. We show that this can be easily exploited in contextual policy search algorithms. In particular, we apply factorization to a Bayesian optimization approach to contextual policy search both in sampling-based and active learning settings. Our simulation results show faster learning and better generalization in various robotic domains. See our supplementary video: https://youtu.be/IIJTbBAOufDY.


Title: Probabilistic Active Filtering for Object Search in Clutter
Key Words: Gaussian processes  graph theory  grippers  image filtering  learning (artificial intelligence)  probability  robot vision  search problems  complex state-action space  Gaussian process active filtering strategy  object search  state dynamics  object search problem  large-scale model  heavy occlusions  clutter  probabilistic active filtering  Search problems  Training  Robots  Task analysis  Probabilistic logic  Clutter  Uncertainty 
Abstract: This paper proposes a probabilistic approach for object search in clutter. Due to heavy occlusions, it is vital for an agent to be able to gradually reduce uncertainty in observations of the objects in its workspace by systematically rearranging them. Probabilistic methodologies present a promising sample-efficient alternative to handle the massively complex state-action space that inherently comes with this problem, avoiding the need for both exhaustive training samples and the accompanying heuristics for traversing a large-scale model during runtime. We approach the object search problem by extending a Gaussian Process active filtering strategy with an additional model for capturing state dynamics as the objects are moved over the course of the activity. This allows viable models to be built upon relatively scarce training data, while the complexity of the action space is also reduced by shifting objects over relatively short distances. Validation in both simulation and with a real Baxter robot with a limited number of training samples demonstrates the efficacy of the proposed approach.


Title: Robust 3D Object Classification by Combining Point Pair Features and Graph Convolution
Key Words: convolutional neural nets  feature extraction  graph theory  image classification  image matching  image reconstruction  learning (artificial intelligence)  object classification  vital semantic information  high-level tasks  point pair features  modern deep learning methods  discriminative features  graph convolutional networks  Stanford 3D indoor dataset  robust 3D object classification  Three-dimensional displays  Deep learning  Feature extraction  Solid modeling  Sensors  Robots  Convolution 
Abstract: Object classification is an important capability for robots as it provides vital semantic information that underpin most practical high-level tasks. Classic handcrafted features, such as point pair features, have demonstrated their robustness for this task. Combining these features with modern deep learning methods provide discriminative features that are rotation invariant and robust to various sources of noise. In this work, we aim to improve the descriptiveness of point pair features while retaining their robustness. We propose a method to achieve more structured sampling of pairs and combine this information through the use of graph convolutional networks. We introduce a novel attention model based on a repeatable local reference frame. Experiments show that our approach significantly improves the state of the art for object classification on large scale reconstruction such as the Stanford 3D indoor dataset and ScanNet and obtains competitive accuracy on the artificial dataset ModelNet.


Title: Discrete Rotation Equivariance for Point Cloud Recognition
Key Words: feature extraction  image recognition  learning (artificial intelligence)  discrete rotation equivariance  point cloud recognition  point clouds  deep networks  deep learning architecture  rotation group  rotated inputs  point cloud based networks  Three-dimensional displays  Two dimensional displays  Task analysis  Feature extraction  Robots  Deep learning  Computer architecture 
Abstract: Despite the recent active research on processing point clouds with deep networks, few attention has been on the sensitivity of the networks to rotations. In this paper, we propose a deep learning architecture that achieves discrete SO(2)/SO(3) rotation equivariance for point cloud recognition. Specifically, the rotation of an input point cloud with elements of a rotation group is similar to shuffling the feature vectors generated by our approach. The equivariance is easily reduced to invariance by eliminating the permutation with operations such as maximum or average. Our method can be directly applied to any existing point cloud based networks, resulting in significant improvements in their performance for rotated inputs. We show state-of-the-art results in the classification tasks with various datasets under both SO(2) and SO(3) rotations. In addition, we further analyze the necessary conditions of applying our approach to PointNet [1] based networks.


Title: MVX-Net: Multimodal VoxelNet for 3D Object Detection
Key Words: cameras  image colour analysis  image fusion  learning (artificial intelligence)  neural nets  object detection  stereo image processing  MVX-net  multimodal VoxelNet  3D object detection  neural network architectures  point cloud data  point cloud modalities  state-of-the-art multimodal algorithms  3D detection categories  simple single stage network  early-fusion approach  VoxelNet architecture  PointFusion  VoxelFusion  RGB modalities  KITTI dataset  birds eye view  Three-dimensional displays  Two dimensional displays  Feature extraction  Laser radar  Object detection  Proposals  Fuses 
Abstract: Many recent works on 3D object detection have focused on designing neural network architectures that can consume point cloud data. While these approaches demonstrate encouraging performance, they are typically based on a single modality and are unable to leverage information from other modalities, such as a camera. Although a few approaches fuse data from different modalities, these methods either use a complicated pipeline to process the modalities sequentially, or perform late-fusion and are unable to learn interaction between different modalities at early stages. In this work, we present PointFusion and VoxelFusion: two simple yet effective early-fusion approaches to combine the RGB and point cloud modalities, by leveraging the recently introduced VoxelNet architecture. Evaluation on the KITTI dataset demonstrates significant improvements in performance over approaches which only use point cloud data. Furthermore, the proposed method provides results competitive with the state-of-the-art multimodal algorithms, achieving top-2 ranking in five of the six birds eye view and 3D detection categories on the KITTI benchmark, by using a simple single stage network.


Title: Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data
Key Words: CAD  convolutional neural nets  image coding  image colour analysis  image enhancement  image resolution  image segmentation  learning (artificial intelligence)  masks  object tracking  category-agnostic instance segmentation  hand-labeled data  object tracking  automated dataset generation  network training  computer vision research  RGB imaging  synthetic depth data sensors  unknown object segmentation  SD mask R-CNN  high-resolution synthetic depth imaging  synthetic depth mask R-CNN  unknown 3D object segmentation  3D CAD models  domain randomization  point cloud clustering baselines  COCO benchmarks  hand-labeled RGB datasets  instance-specific grasping pipeline  synthetic training dataset  Image segmentation  Training  Solid modeling  Three-dimensional displays  Robots  Cameras  Grasping 
Abstract: The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.


Title: Multi-Modal Geometric Learning for Grasping and Manipulation
Key Words: computational geometry  control engineering computing  convolutional neural nets  learning (artificial intelligence)  manipulators  solid modelling  multimodal geometric learning  robotic manipulation tasks  3D convolutional neural network  captured depth information  object geometry  visual-tactile approaches  3D models  tactile information  geometric prediction  geometric reasoning  Geometry  Robot sensing systems  Three-dimensional displays  Shape  Training  Grasping 
Abstract: This work provides an architecture that incorporates depth and tactile information to create rich and accurate 3D models useful for robotic manipulation tasks. This is accomplished through the use of a 3D convolutional neural network (CNN). Offline, the network is provided with both depth and tactile information and trained to predict the object's geometry, thus filling in regions of occlusion. At runtime, the network is provided a partial view of an object. Tactile information is acquired to augment the captured depth information. The network can then reason about the object's geometry by utilizing both the collected tactile and depth information. We demonstrate that even small amounts of additional tactile information can be incredibly helpful in reasoning about object geometry. This is particularly true when information from depth alone fails to produce an accurate geometric prediction. Our method is benchmarked against and outperforms other visual-tactile approaches to general geometric reasoning. We also provide experimental results comparing grasping success with our method.


Title: Realizing Learned Quadruped Locomotion Behaviors through Kinematic Motion Primitives
Key Words: control engineering computing  gradient methods  learning (artificial intelligence)  legged locomotion  motion control  principal component analysis  robot kinematics  robot programming  quadruped locomotion behavior learning  single gait learning  Stoch  policy gradient  PCA  quadrupedal walking  walking gaits  robust locomotion behaviors  D-RL  deep reinforcement learning  kMPs  kinematic motion primitives  Legged locomotion  Trajectory  Computational modeling  Kinematics  Optimization  Training 
Abstract: Humans and animals are believed to use a very minimal set of trajectories to perform a wide variety of tasks including walking. Our main objective in this paper is two fold 1) Obtain an effective tool to realize these basic motion patterns for quadrupedal walking, called the kinematic motion primitives (kMPs), via trajectories learned from deep reinforcement learning (D-RL) and 2) Realize a set of behaviors, namely trot, walk, gallop and bound from these kinematic motion primitives in our custom four legged robot, called the “Stoch”. D-RL is a data driven approach, which has been shown to be very effective for realizing all kinds of robust locomotion behaviors, both in simulation and in experiment. On the other hand, kMPs are known to capture the underlying structure of walking and yield a set of derived behaviors. We first generate walking gaits from D-RL, which uses policy gradient based approaches. We then analyze the resulting walking by using principal component analysis. We observe that the kMPs extracted from PCA followed a similar pattern irrespective of the type of gaits generated. Leveraging on this underlying structure, we then realize walking in Stoch by a straightforward reconstruction of joint trajectories from kMPs. This type of methodology improves the transferability of these gaits to real hardware, lowers the computational overhead on-board, and also avoids multiple training iterations by generating a set of derived behaviors from a single learned gait.


Title: Single-shot Foothold Selection and Constraint Evaluation for Quadruped Locomotion
Key Words: control engineering computing  convolutional neural nets  geometry  legged locomotion  motion control  optimal control  robot dynamics  robot kinematics  single-shot foothold selection  constraint evaluation  quadruped locomotion  optimal footholds  legged systems  swing leg  local elevation map  kinematic constraints  convolutional neural network  geometrical characteristics  Legged locomotion  Foot  Kinematics  Robot kinematics  Collision avoidance  Neural networks 
Abstract: In this paper, we propose a method for selecting the optimal footholds for legged systems. The goal of the proposed method is to find the best foothold for the swing leg on a local elevation map. First, we evaluate the geometrical characteristics of each cell on the elevation map, checks kinematic constraints and collisions. Then, we apply the Convolutional Neural Network to learn the relationship between the local elevation map and the quality of potential footholds. During execution time, the controller obtains the qualitative measurement of each potential foothold from the neural model. This method evaluates hundreds of potential footholds and checks multiple constraints in a single step which takes 10 ms on a standard computer without GPU. The experiments were carried out on a quadruped robot walking over rough terrain in both simulation and real robotic platforms.


Title: Learning Primitive Skills for Mobile Robots
Key Words: intelligent robots  learning (artificial intelligence)  legged locomotion  multi-robot systems  robot vision  robot soccer small-size domain  tactical level team strategies  high-level team strategies  individual robot ball-based skills  robot primitive skills  continuous action space  hardware fidelity  learned skills  mobile robots  hand-coding algorithms  training parameters  mobile robot system  deep reinforcement learning algorithm  primitive skills  task performance  learning algorithms  Robot kinematics  Sports  Task analysis  Training  Legged locomotion 
Abstract: Achieving effective task performance on real mobile robots is a great challenge when hand-coding algorithms, both due to the amount of effort involved and manually tuned parameters required for each skill. Learning algorithms instead have the potential to lighten up this challenge by using one single set of training parameters for learning different skills, but the question of the feasibility of such learning in real robots remains a research pursuit. We focus on a kind of mobile robot system - the robot soccer “small-size” domain, in which tactical and high-level team strategies build upon individual robot ball-based skills. In this paper, we present our work using a Deep Reinforcement Learning algorithm to learn three real robot primitive skills in continuous action space: go-to-ball, turn-and-shoot and shoot-goalie, for which there is a clear success metric to reach a destination or score a goal. We introduce the state and action representation, as well as the reward and network architecture. We describe our training and testing using a simulator of high physical and hardware fidelity. Then we test the policies trained from simulation on real robots. Our results show that the learned skills achieve an overall better success rate at the expense of taking 0.29 seconds slower on average for all three skills. In the end, we show that our policies trained in simulation have good performance on real robots by directly transferring the policy.


Title: Continuous Control for High-Dimensional State Spaces: An Interactive Learning Approach
Key Words: interactive systems  learning (artificial intelligence)  interactive learning approach  deep reinforcement learning  corrective advice communicated by humans  DRL agent  human training effort  D-COACH framework  human corrective feedback  human knowledge  machine learning methods  reward function  simulated environments  robotics applications  complex decision-making problems  high-dimensional state spaces  continuous control  Training  Robots  Shape  Task analysis  Adaptation models  Decoding  Machine learning 
Abstract: Deep Reinforcement Learning (DRL) has become a powerful methodology to solve complex decision-making problems. However, DRL has several limitations when used in real-world problems (e.g., robotics applications). For instance, long training times are required and cannot be accelerated in contrast to simulated environments, and reward functions may be hard to specify/model and/or to compute. Moreover, the transfer of policies learned in a simulator to the real-world has limitations (reality gap). On the other hand, machine learning methods that rely on the transfer of human knowledge to an agent have shown to be time efficient for obtaining well performing policies and do not require a reward function. In this context, we analyze the use of human corrective feedback during task execution to learn policies with high-dimensional state spaces, by using the D-COACH framework, and we propose new variants of this framework. D-COACH is a Deep Learning based extension of COACH (COrrective Advice Communicated by Humans), where humans are able to shape policies through corrective advice. The enhanced version of DCOACH, which is proposed in this paper, largely reduces the time and effort of a human for training a policy. Experimental results validate the efficiency of the D-COACH framework in three different problems (simulated and with real robots), and show that its enhanced version reduces the human training effort considerably, and makes it feasible to learn policies within periods of time in which a DRL agent do not reach any improvement.


Title: A Predictive Reward Function for Human-Like Driving Based on a Transition Model of Surrounding Environment
Key Words: decision making  image processing  learning (artificial intelligence)  mobile robots  neural nets  road traffic  road vehicles  robot vision  traffic engineering computing  autonomous driving vehicles  deep predictive network  predictive reward function  human-like driving  decision making  vehicle control  traffic flow  occupancy grid image  prediction network training  real driving data  reinforcement learning agent training  deep neural networks  Autonomous vehicles  Roads  Predictive models  Reinforcement learning  Decision making  Object detection  Autonomous driving  Prediction  Reward  Deep Learning  Reinforcement Learning  naturalistic Driving Data 
Abstract: Driving is a complex task that requires the perception of the surrounding environment, decision making and control of the vehicle. Human drivers predict how surrounding objects move and decide an appropriate driving behavior. As with human drivers, autonomous driving vehicles should consider the condition of the surrounding environment and behave naturally so as not to disturb the traffic flow. We propose a reward function for learning how natural the driving is based on the hypothesis that the movement of surrounding vehicles becomes unpredictable when the ego vehicle takes an unnatural driving behavior. The reward function is based on the prediction error of a deep predictive network that models the transition of the surrounding environment. Occupancy grid image is used to perceive the surrounding environment and the predictions up to two seconds are used to calculate the reward function. We evaluated the reward function using both simulated and the real world data. We trained the prediction network using real driving data and trained a reinforcement learning agent based on the reward function. Then we compared the speed planned by the agent and a human driver, which showed a correlation of 0.52. We also confirmed the benefit of taking prediction into account by observing the behavior of the agent in a specific traffic scenario.


Title: ADAPS: Autonomous Driving Via Principled Simulations
Key Words: hierarchical systems  remotely operated vehicles  road traffic control  robust control  ADAPS  autonomous driving  robust control policy  autonomous vehicles  simulation platforms  learning mechanism  hierarchical control policy  DAGGER method  Task analysis  Training  Accidents  Autonomous vehicles  Training data  Trajectory  Learning systems 
Abstract: Autonomous driving has gained significant advancements in recent years. However, obtaining a robust control policy for driving remains challenging as it requires training data from a variety of scenarios, including rare situations (e.g., accidents), an effective policy architecture, and an efficient learning mechanism. We propose ADAPS for producing robust control policies for autonomous vehicles. ADAPS consists of two simulation platforms in generating and analyzing accidents to automatically produce labeled training data, and a memoryenabled hierarchical control policy. Additionally, ADAPS offers a more efficient online learning mechanism that reduces the number of iterations required in learning compared to existing methods such as DAGGER [1]. We present both theoretical and experimental results. The latter are produced in simulated environments, where qualitative and quantitative results are generated to demonstrate the benefits of ADAPS.


Title: A Data-driven Approach for Fast Simulation of Robot Locomotion on Granular Media
Key Words: control engineering computing  data analysis  granular materials  legged locomotion  mechanical contact  optimisation  shear modulus  stick-slip  data-driven approach  robot locomotion  granular media  semiempirical approach  contact model  stick-slip behavior  rigid objects  granular grains  granular substrate  optimization-based contact force  contact solver  contact wrenches  fast simulation  convex volume  frictional dissipation  plausible interaction response  Substrates  Computational modeling  Force  Media  Legged locomotion  Foot 
Abstract: In this paper, we propose a semi-empirical approach for simulating robot locomotion on granular media. We first develop a contact model based on the stick-slip behavior between rigid objects and granular grains, which is then learned through running extensive experiments. The contact model represents all possible contact wrenches that the granular substrate can provide as a convex volume, which our method formulates as constraints in an optimization-based contact force solver. During simulation, granular substrates are treated as rigid objects that allow penetration and the contact solver solves for wrenches that maximize frictional dissipation. We show that our method is able to simulate plausible interaction response with several granular media at interactive rates.


Title: A Classification-based Approach for Approximate Reachability
Key Words: approximation theory  computational complexity  controllability  nonlinear control systems  optimal control  pattern classification  reachability analysis  goal satisfaction  safety verification  nonlinear systems  computational complexity  restrictive problem classes  optimal controller  HJ reachability problem  control-affine systems  dynamical systems  reachability value function  classification-based approach  approximate reachability  Hamilton-Jacobi reachability analysis  simple binary classifiers  grid-based methodologies  physical quadrotor navigation task  Optimal control  Reachability analysis  Tools  Neural networks  Safety  System dynamics 
Abstract: Hamilton-Jacobi (HJ) reachability analysis has been developed over the past decades into a widely-applicable tool for determining goal satisfaction and safety verification in nonlinear systems. While HJ reachability can be formulated very generally, computational complexity can be a serious impediment for many systems of practical interest. Much prior work has been devoted to computing approximate solutions to large reachability problems, yet many of these methods may only apply to very restrictive problem classes, do not generate controllers, and/or can be extremely conservative. In this paper, we present a new method for approximating the optimal controller of the HJ reachability problem for control-affine systems. While also a specific problem class, many dynamical systems of interest are, or can be well approximated, by control-affine models. We explicitly avoid storing a representation of the reachability value function, and instead learn a controller as a sequence of simple binary classifiers. We compare our approach to existing grid-based methodologies in HJ reachability and demonstrate its utility on several examples, including a physical quadrotor navigation task.


Title: Online Deep Learning for Improved Trajectory Tracking of Unmanned Aerial Vehicles Using Expert Knowledge
Key Words: autonomous aerial vehicles  learning (artificial intelligence)  neurocontrollers  trajectory control  input-output dataset  deep neural network-based controller  trained DNN  expert knowledge  learning-based approach  trajectory tracking performance  online deep learning  unmanned aerial vehicles  online learning-based control method  trajectory tracking  Training  Fuzzy logic  Unmanned aerial vehicles  Trajectory  Real-time systems  Trajectory tracking 
Abstract: This work presents an online learning-based control method for improved trajectory tracking of unmanned aerial vehicles using both deep learning and expert knowledge. The proposed method does not require the exact model of the system to be controlled, and it is robust against variations in system dynamics as well as operational uncertainties. The learning is divided into two phases: offline (pre-)training and online (post-)training. In the former, a conventional controller performs a set of trajectories and, based on the input-output dataset, the deep neural network (DNN)-based controller is trained. In the latter, the trained DNN, which mimics the conventional controller, controls the system. Unlike the existing papers in the literature, the network is still being trained for different sets of trajectories which are not used in the training phase of DNN. Thanks to the rule-base, which contains the expert knowledge, the proposed framework learns the system dynamics and operational uncertainties in real-time. The experimental results show that the proposed online learning-based approach gives better trajectory tracking performance when compared to the only offline trained network.


Title: One-Shot Learning of Multi-Step Tasks from Observation via Activity Localization in Auxiliary Video
Key Words: inference mechanisms  learning (artificial intelligence)  video signal processing  one-shot learning  multistep tasks  activity localization  reinforcement learning  action policies learning  reward functions inference  user-segmented demonstration  auxiliary video data  Task analysis  Reinforcement learning  Neural networks  Training  Robot sensing systems  Training data 
Abstract: Due to burdensome data requirements, learning from demonstration often falls short of its promise to allow users to quickly and naturally program robots. Demonstrations are inherently ambiguous and incomplete, making correct generalization to unseen situations difficult without a large number of demonstrations in varying conditions. By contrast, humans are often able to learn complex tasks from a single demonstration (typically observations without action labels) by leveraging context learned over a lifetime. Inspired by this capability, our goal is to enable robots to perform one-shot learning of multi-step tasks from observation by leveraging auxiliary video data as context. Our primary contribution is a novel system that achieves this goal by: (1) using a single user-segmented demonstration to define the primitive actions that comprise a task, (2) localizing additional examples of these actions in unsegmented auxiliary videos via a metalearning-based approach, (3) using these additional examples to learn a reward function for each action, and (4) performing reinforcement learning on top of the inferred reward functions to learn action policies that can be combined to accomplish the task. We empirically demonstrate that a robot can learn multi-step tasks more effectively when provided auxiliary video, and that performance greatly improves when localizing individual actions, compared to learning from unsegmented videos.


Title: LVIS: Learning from Value Function Intervals for Contact-Aware Robot Controllers
Key Words: concave programming  feedback  humanoid robots  integer programming  learning (artificial intelligence)  mechanical contact  mobile robots  neurocontrollers  optimal control  predictive control  robot dynamics  tree searching  LVIS  contact-aware robot controllers  guided policy search  high-dimensional systems  nonconvex trajectory optimization  local minima  optimal policy  independently-optimized samples  optimal value function  mixed-integer programs  global optimality  interval samples  terminal cost  feedback control  learning from value function intervals  controller training  global mixed-integer optimization  nonuniqueness issue  branch-and-bound algorithm  neural net training  learned cost-to-go  one-step model-predictive controller  piecewise affine models  cart-pole system  planar humanoid robot  Humanoid robots  Trajectory optimization  Neural networks  Force  Training 
Abstract: Guided policy search is a popular approach for training controllers for high-dimensional systems, but it has a number of pitfalls. Non-convex trajectory optimization has local minima, and non-uniqueness in the optimal policy itself can mean that independently-optimized samples do not describe a coherent policy from which to train. We introduce LVIS, which circumvents the issue of local minima through global mixed-integer optimization and the issue of non-uniqueness through learning the optimal value function rather than the optimal policy. To avoid the expense of solving the mixed-integer programs to full global optimality, we instead solve them only partially, extracting intervals containing the true cost-to-go from early termination of the branch-and-bound algorithm. These interval samples are used to weakly supervise the training of a neural net which approximates the true cost-to-go. Online, we use that learned cost-to-go as the terminal cost of a one-step model-predictive controller, which we solve via a small mixed-integer optimization. We demonstrate LVIS on piecewise affine models of a cart-pole system with walls and a planar humanoid robot and show that it can be applied to a fundamentally hard problem in feedback control-control through contact.


