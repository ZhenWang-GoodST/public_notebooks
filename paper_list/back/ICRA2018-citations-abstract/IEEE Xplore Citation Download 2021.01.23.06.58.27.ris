TY  - CONF
TI  - A Synchronization Scheme for Position Control of Multiple Rope-Climbing Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3736
EP  - 3741
AU  - G. Sun
AU  - X. Li
AU  - P. Li
AU  - Y. Meng
AU  - Y. Zhou
AU  - E. Xu
AU  - Y. Liu
PY  - 2018
KW  - actuators
KW  - asymptotic stability
KW  - manipulators
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - robot dynamics
KW  - singularly perturbed systems
KW  - single rope-climbing robot
KW  - multiple rope-climbing robots
KW  - position control
KW  - Robot kinematics
KW  - Synchronization
KW  - Task analysis
KW  - Actuators
KW  - Mathematical model
KW  - Position control
KW  - multiple Rope-Climbing Robots
KW  - climbing robots
KW  - motion control
DO  - 10.1109/ICRA.2018.8460484
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The ability of rope-climbing robots in aloft operation is limited by its self-supporting and locomotion ability. In many applications, a given task is also too complex to be achieved by a single rope-climbing robot acting alone. The solution of multiple rope-climbing robots can overcome the limitations. However, existing control methods for rope-climbing robots are limited to single robot, and the open issue of coordination between multiple rope-climbing robots has not been systematically addressed. This paper presents a new synchronization scheme for position control of multiple rope-climbing robots, such that each robot moves to the corresponding desired position while synchronizing the heights between each other. Maintaining the same height is very important to guarantee the stability of the task-oriented manipulator installed among multiple robots, when it is performing the manipulation task. The development of the proposed controller is based on the singular perturbation approach, by treating the fast actuator dynamics as a perturbation of the slow robot dynamics, such that the lowest control complexity is achieved. The exponential stability of the overall system that consists of the fast and slow subsystems is proved by using Tikhonov' s theorem. Experimental results are presented to illustrate the performance of the proposed controller.
ER  - 

TY  - CONF
TI  - Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3750
EP  - 3757
AU  - A. Zeng
AU  - S. Song
AU  - K. Yu
AU  - E. Donlon
AU  - F. R. Hogan
AU  - M. Bauza
AU  - D. Ma
AU  - O. Taylor
AU  - M. Liu
AU  - E. Romo
AU  - N. Fazeli
AU  - F. Alet
AU  - N. C. Dafle
AU  - R. Holladay
AU  - I. Morena
AU  - P. Qu Nair
AU  - D. Green
AU  - I. Taylor
AU  - W. Liu
AU  - T. Funkhouser
AU  - A. Rodriguez
PY  - 2018
KW  - grippers
KW  - image classification
KW  - image matching
KW  - object recognition
KW  - robot vision
KW  - robotic pick-and-place
KW  - image classification framework
KW  - 2017 Amazon Robotics Challenge
KW  - MIT-Princeton Team system
KW  - category-agnostic affordance prediction algorithm
KW  - cross-domain image matching
KW  - Grasping
KW  - Robots
KW  - Clutter
KW  - Grippers
KW  - Robustness
KW  - Proposals
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461044
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses a category-agnostic affordance prediction algorithm to select and execute among four different grasping primitive behaviors. It then recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional training data. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT-Princeton Team system that took 1st place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http://arc.cs.princeton.edu.
ER  - 

TY  - CONF
TI  - Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3758
EP  - 3765
AU  - R. Rahmatizadeh
AU  - P. Abolghasemi
AU  - L. Bölöni
AU  - S. Levine
PY  - 2018
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - recurrent neural nets
KW  - robot vision
KW  - nonprehensile manipulation
KW  - recurrent neural network
KW  - raw images
KW  - VAE-GAN-based reconstruction
KW  - autoregressive multimodal action prediction
KW  - complex manipulation tasks
KW  - towel
KW  - weight
KW  - reconstruction-based regularization
KW  - vision-based multitask manipulation
KW  - end-to-end learning
KW  - multitask learning
KW  - low-cost robotic arm
KW  - robot arm trajectories
KW  - complex picking and placing tasks
KW  - Task analysis
KW  - Robots
KW  - Feature extraction
KW  - Neural networks
KW  - Image reconstruction
KW  - Training
KW  - Visualization
DO  - 10.1109/ICRA.2018.8461076
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We propose a technique for multi-task learning from demonstration that trains the controller of a low-cost robotic arm to accomplish several complex picking and placing tasks, as well as non-prehensile manipulation. The controller is a recurrent neural network using raw images as input and generating robot arm trajectories, with the parameters shared across the tasks. The controller also combines VAE-GAN-based reconstruction with autoregressive multimodal action prediction. Our results demonstrate that it is possible to learn complex manipulation tasks, such as picking up a towel, wiping an object, and depositing the towel to its previous position, entirely from raw images with direct behavior cloning. We show that weight sharing and reconstruction-based regularization substantially improve generalization and robustness, and training on multiple tasks simultaneously increases the success rate on all tasks.
ER  - 

TY  - CONF
TI  - Learning 6-DOF Grasping Interaction via Deep Geometry-Aware 3D Representations
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3766
EP  - 3773
AU  - X. Yan
AU  - J. Hsu
AU  - M. Khansari
AU  - Y. Bai
AU  - A. Pathak
AU  - A. Gupta
AU  - J. Davidson
AU  - H. Lee
PY  - 2018
KW  - convolution
KW  - dexterous manipulators
KW  - geometry
KW  - grippers
KW  - image reconstruction
KW  - image representation
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - optimisation
KW  - recurrent neural nets
KW  - virtual reality
KW  - outcome prediction model
KW  - 3D shape modeling
KW  - DGGN
KW  - analysis-by-synthesis optimization
KW  - 6-DOF grasping net
KW  - virtual reality
KW  - sensory annotations
KW  - data augmentation strategy
KW  - CNN
KW  - 3D occupancy grid
KW  - mental geometry-aware representation
KW  - deep geometry-aware grasping network
KW  - 3D geometry prediction
KW  - grasping interaction learning
KW  - parallel jaw gripper
KW  - RGBD input
KW  - internal geometry-aware representation
KW  - Grasping
KW  - Three-dimensional displays
KW  - Shape
KW  - Geometry
KW  - Solid modeling
KW  - Two dimensional displays
KW  - Robots
DO  - 10.1109/ICRA.2018.8460609
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper focuses on the problem of learning 6- DOF grasping with a parallel jaw gripper in simulation. Our key idea is constraining and regularizing grasping interaction learning through 3D geometry prediction. We introduce a deep geometry-aware grasping network (DGGN) that decomposes the learning into two steps. First, we learn to build mental geometry-aware representation by reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via generative 3D shape modeling. Second, we learn to predict grasping outcome with its internal geometry-aware representation. The learned outcome prediction model is used to sequentially propose grasping solutions via analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best of our knowledge, we are presenting for the first time a method to learn a 6-DOF grasping net from RGBD input; (2) We build a grasping dataset from demonstrations in virtual reality with rich sensory and interaction annotations. This dataset includes 101 everyday objects spread across 7 categories, additionally, we propose a data augmentation strategy for effective learning; (3) We demonstrate that the learned geometry-aware representation leads to about 10% relative performance improvement over the baseline CNN on grasping objects from our dataset. (4) We further demonstrate that the model generalizes to novel viewpoints and object instances.
ER  - 

TY  - CONF
TI  - Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3774
EP  - 3781
AU  - J. Hatori
AU  - Y. Kikuchi
AU  - S. Kobayashi
AU  - K. Takahashi
AU  - Y. Tsuboi
AU  - Y. Unno
AU  - W. Ko
AU  - J. Tan
PY  - 2018
KW  - industrial robots
KW  - interactive systems
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - natural language interfaces
KW  - natural language processing
KW  - object detection
KW  - robot vision
KW  - natural language processing technologies
KW  - unconstrained spoken instructions
KW  - instruction ambiguity
KW  - physical industrial robot arm
KW  - natural instructions
KW  - object picking task
KW  - real-world objects
KW  - unconstrained spoken language instructions
KW  - spoken natural language
KW  - human instructions
KW  - comprehensive system
KW  - Task analysis
KW  - Object recognition
KW  - Natural languages
KW  - Object detection
KW  - Feature extraction
KW  - Service robots
DO  - 10.1109/ICRA.2018.8460699
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Comprehension of spoken natural language is an essential skill for robots to communicate with humans effectively. However, handling unconstrained spoken instructions is challenging due to (1) complex structures and the wide variety of expressions used in spoken language, and (2) inherent ambiguity of human instructions. In this paper, we propose the first comprehensive system for controlling robots with unconstrained spoken language, which is able to effectively resolve ambiguity in spoken instructions. Specifically, we integrate deep learning-based object detection together with natural language processing technologies to handle unconstrained spoken instructions, and propose a method for robots to resolve instruction ambiguity through dialogue. Through our experiments on both a simulated environment as well as a physical industrial robot arm, we demonstrate the ability of our system to understand natural instructions from human operators effectively, and show how higher success rates of the object picking task can be achieved through an interactive clarification process.
ER  - 

TY  - CONF
TI  - Translating Videos to Commands for Robotic Manipulation with Deep Recurrent Neural Networks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3782
EP  - 3788
AU  - A. Nguyen
AU  - D. Kanoulas
AU  - L. Muratore
AU  - D. G. Caldwell
AU  - N. G. Tsagarakis
PY  - 2018
KW  - control engineering computing
KW  - convolution
KW  - feature extraction
KW  - feedforward neural nets
KW  - humanoid robots
KW  - mobile robots
KW  - recurrent neural nets
KW  - video signal processing
KW  - feature extraction
KW  - video translation
KW  - CNN
KW  - RNN
KW  - full-size humanoid robot WALK-MAN
KW  - manipulation tasks
KW  - translation module
KW  - visual features
KW  - encoder-decoder architecture
KW  - RNN layers
KW  - deep Convolutional Neural Networks
KW  - input video frames
KW  - deep features
KW  - command
KW  - Deep Recurrent Neural Networks
KW  - robotic manipulation
KW  - Videos
KW  - Robots
KW  - Feature extraction
KW  - Task analysis
KW  - Visualization
KW  - Recurrent neural networks
KW  - Logic gates
DO  - 10.1109/ICRA.2018.8460857
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a new method to translate videos to commands for robotic manipulation using Deep Recurrent Neural Networks (RNN). Our framework first extracts deep features from the input video frames with a deep Convolutional Neural Networks (CNN). Two RNN layers with an encoder-decoder architecture are then used to encode the visual features and sequentially generate the output words as the command. We demonstrate that the translation accuracy can be improved by allowing a smooth transaction between two RNN layers and using the state-of-the-art feature extractor. The experimental results on our new challenging dataset show that our approach outperforms recent methods by a fair margin. Furthermore, we combine the proposed translation module with the vision and planning system to let a robot perform various manipulation tasks. Finally, we demonstrate the effectiveness of our framework on a full-size humanoid robot WALK-MAN.
ER  - 

TY  - CONF
TI  - Distributed Learning for the Decentralized Control of Articulated Mobile Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3789
EP  - 3794
AU  - G. Sartoretti
AU  - Y. Shi
AU  - W. Paivine
AU  - M. Travers
AU  - H. Choset
PY  - 2018
KW  - decentralised control
KW  - distributed control
KW  - learning systems
KW  - mobile robots
KW  - multi-agent systems
KW  - highly cluttered evaluation environments
KW  - decentralized control architectures
KW  - central pattern generators
KW  - spatially distributed portions
KW  - articulated bodies
KW  - system-level objectives
KW  - reinforcement learning
KW  - independent agents
KW  - parallel environments
KW  - meta-level agent
KW  - homogeneous decentralized control
KW  - articulated locomotion
KW  - distributed learning
KW  - asynchronous advantage actor-critic algorithm
KW  - A3C
KW  - decentralized control policies
KW  - independently controlled portion
KW  - autonomous decentralized compliant control framework
KW  - compliant control baseline
KW  - articulated mobile robots
KW  - Shape
KW  - Decentralized control
KW  - Aerospace electronics
KW  - Robot kinematics
KW  - Admittance
KW  - Hardware
DO  - 10.1109/ICRA.2018.8460802
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Decentralized control architectures, such as those conventionally defined by central pattern generators, independently coordinate spatially distributed portions of articulated bodies to achieve system-level objectives. State of the art distributed algorithms for reinforcement learning employ a different but conceptually related idea; independent agents simultaneously coordinating their own behaviors in parallel environments while asynchronously updating the policy of a system-or, rather, meta-level agent. This work, to the best of the authors' knowledge, is the first to explicitly explore the potential relationship between the underlying concepts in homogeneous decentralized control for articulated locomotion and distributed learning. We present an approach that leverages the structure of the asynchronous advantage actor-critic (A3C) algorithm to provide a natural framework for learning decentralized control policies on a single platform. Our primary contribution shows an individual agent in the A3C algorithm can be defined by an independently controlled portion of the robot's body, thus enabling distributed learning on a single platform for efficient hardware implementation. To this end, we show how the system is trained offline using hardware experiments implementing an autonomous decentralized compliant control framework. Our experimental results show that the trained agent outperforms the compliant control baseline by more than 40% in terms of steady progression through a series of randomized, highly cluttered evaluation environments.
ER  - 

TY  - CONF
TI  - Neural Task Programming: Learning to Generalize Across Hierarchical Tasks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3795
EP  - 3802
AU  - D. Xu
AU  - S. Nair
AU  - Y. Zhu
AU  - J. Gao
AU  - A. Garg
AU  - L. Fei-Fei
AU  - S. Savarese
PY  - 2018
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - neural task programming
KW  - unseen tasks
KW  - sequential tasks
KW  - robot manipulation tasks
KW  - bottom-level programs
KW  - hierarchical neural program
KW  - finer sub-task specifications
KW  - task specification
KW  - neural program induction
KW  - few-shot learning
KW  - NTP
KW  - novel robot learning framework
KW  - hierarchical tasks
KW  - Task analysis
KW  - Programming
KW  - Robots
KW  - Sorting
KW  - Semantics
KW  - Topology
KW  - Data models
DO  - 10.1109/ICRA.2018.8460689
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well towards unseen tasks with increasing lengths, variable topologies, and changing objectives.stanfordvl.github.io/ntp/.
ER  - 

TY  - CONF
TI  - Sim-to-Real Transfer of Robotic Control with Dynamics Randomization
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3803
EP  - 3810
AU  - X. B. Peng
AU  - M. Andrychowicz
AU  - W. Zaremba
AU  - P. Abbeel
PY  - 2018
KW  - calibration
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - robot dynamics
KW  - sim-to-real transfer
KW  - robotic control
KW  - dynamics randomization
KW  - training agents
KW  - training process
KW  - robotic arm
KW  - calibration error
KW  - Robots
KW  - Training
KW  - Adaptation models
KW  - Task analysis
KW  - Trajectory
KW  - Data models
KW  - Robustness
DO  - 10.1109/ICRA.2018.8460528
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this “reality gap”. By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.
ER  - 

TY  - CONF
TI  - Topomap: Topological Mapping and Navigation Based on Visual SLAM Maps
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3818
EP  - 3825
AU  - F. Blochliger
AU  - M. Fehr
AU  - M. Dymczyk
AU  - T. Schneider
AU  - R. Siegwart
PY  - 2018
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - three-dimensional topological map
KW  - noisy sparse point cloud
KW  - convex free-space clusters
KW  - global planning
KW  - mobile robotic platform
KW  - Topomap
KW  - visual SLAM
KW  - visual robot navigation
KW  - navigation task
KW  - sparse feature-based map
KW  - path planning algorithms
KW  - visual simultaneous localization and mapping system
KW  - Navigation
KW  - Visualization
KW  - Simultaneous localization and mapping
KW  - Path planning
KW  - Three-dimensional displays
KW  - Planning
DO  - 10.1109/ICRA.2018.8460641
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Visual robot navigation within large-scale, semistructured environments deals with various challenges such as computation intensive path planning algorithms or insufficient knowledge about traversable spaces. Moreover, many state-of-the-art navigation approaches only operate locally instead of gaining a more conceptual understanding of the planning objective. This limits the complexity of tasks a robot can accomplish and makes it harder to deal with uncertainties that are present in the context of real-time robotics applications. In this work, we present Topomap, a framework which simplifies the navigation task by providing a map to the robot which is tailored for path planning use. This novel approach transforms a sparse feature-based map from a visual Simultaneous Localization And Mapping (SLAM) system into a three-dimensional topological map. This is done in two steps. First, we extract occupancy information directly from the noisy sparse point cloud. Then, we create a set of convex free-space clusters, which are the vertices of the topological map. We show that this representation improves the efficiency of global planning, and we provide a complete derivation of our algorithm. Planning experiments on real world datasets demonstrate that we achieve similar performance as RRT* with significantly lower computation times and storage requirements. Finally, we test our algorithm on a mobile robotic platform to prove its advantages.
ER  - 

TY  - CONF
TI  - PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor Fusion and Hardware Co-Design
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3826
EP  - 3832
AU  - Z. Zhang
AU  - S. Liu
AU  - G. Tsai
AU  - H. Hu
AU  - C. Chu
AU  - F. Zheng
PY  - 2018
KW  - cameras
KW  - image fusion
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - synchronisation
KW  - embedded simultaneous localization and mapping algorithm
KW  - multi-core processor
KW  - public visual-inertial datasets
KW  - PerceptIn Robotics Vision System
KW  - Hardware Co-Design
KW  - advanced visual-inertial SLAM System
KW  - state-of-the-art visual-inertial algorithms
KW  - additional sensor modalities
KW  - inertial measurements
KW  - visual measurements
KW  - flexible sensor fusion approach
KW  - PIRVS software features
KW  - precise hardware synchronization
KW  - global-shutter stereo camera
KW  - PIRVS hardware
KW  - visual-inertial computing hardware
KW  - Simultaneous localization and mapping
KW  - Hardware
KW  - Cameras
KW  - Feature extraction
KW  - Synchronization
KW  - Visualization
DO  - 10.1109/ICRA.2018.8460672
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we present the PerceptIn Robotics Vision System (PIRVS), a visual-inertial computing hardware with embedded simultaneous localization and mapping (SLAM) algorithm. The PIRVS hardware is equipped with a multi-core processor, a global-shutter stereo camera, and an IMU with precise hardware synchronization. The PIRVS software features a flexible sensor fusion approach to not only tightly integrate visual measurements with inertial measurements and also to loosely couple with additional sensor modalities. It runs in real-time on both PC and the PIRVS hardware. We perform a thorough evaluation of the proposed system using multiple public visual-inertial datasets. Experimental results demonstrate that our system reaches comparable accuracy of state-of-the-art visual-inertial algorithms on PC, while being more efficient on the PIRVS hardware.
ER  - 

TY  - CONF
TI  - ProSLAM: Graph SLAM from a Programmer's Perspective
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3833
EP  - 3840
AU  - D. Schlegel
AU  - M. Colosi
AU  - G. Grisetti
PY  - 2018
KW  - C++ language
KW  - data structures
KW  - graph theory
KW  - public domain software
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - Graph SLAM
KW  - data structures
KW  - C++ programming language
KW  - standard libraries
KW  - lightweight open-source stereo visual SLAM system
KW  - programmer
KW  - ProSLAM
KW  - algorithmic aspects
KW  - mathematical aspects
KW  - highly modular system
KW  - Simultaneous localization and mapping
KW  - Visualization
KW  - Three-dimensional displays
KW  - Cameras
KW  - Data structures
KW  - Benchmark testing
DO  - 10.1109/ICRA.2018.8461180
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper we present ProSLAM, a lightweight open-source stereo visual SLAM system designed with simplicity in mind. This work stems from the experience gathered by the authors while teaching SLAM and aims at providing a highly modular system that can be easily implemented and understood. Rather than focusing on the well known mathematical aspects of stereo visual SLAM, we highlight the data structures and the algorithmic aspects required to realize such a system. We implemented ProSLAM using the C++ programming language in combination with a minimal set of standard libraries. The results of a thorough validation performed on several standard benchmark datasets show that ProSLAM achieves precision comparable to state-of-the-art approaches, while requiring substantially less computation.
ER  - 

TY  - CONF
TI  - Talk Resource-Efficiently to Me: Optimal Communication Planning for Distributed Loop Closure Detection
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3841
EP  - 3848
AU  - M. Giamou
AU  - K. Khosoussi
AU  - J. P. How
PY  - 2018
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - SLAM (robots)
KW  - cooperative simultaneous localization and mapping
KW  - inter-robot loop closures
KW  - general resource-efficiency communication planning
KW  - sensory data sharing
KW  - distributed loop closure detection
KW  - optimal communication planning
KW  - CSLAM
KW  - Robot sensing systems
KW  - Distributed databases
KW  - Planning
KW  - Trajectory
KW  - Visualization
KW  - Metadata
DO  - 10.1109/ICRA.2018.8460783
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Due to the distributed nature of cooperative simultaneous localization and mapping (CSLAM), detecting inter-robot loop closures necessitates sharing sensory data with other robots. A naïve approach to data sharing can easily lead to a waste of mission-critical resources. This paper investigates the logistical aspects of CSLAM. Particularly, we present a general resource-efficient communication planning framework that takes into account both the total amount of exchanged data and the induced division of labor between the participating robots. Compared to other state-of-the-art approaches, our framework is able to verify the same set of potential inter-robot loop closures while exchanging considerably less data and influencing the induced workloads. We develop a fast algorithm for finding globally optimal communication policies, and present theoretical analysis to characterize the necessary and sufficient conditions under which simpler strategies are optimal. The proposed framework is extensively evaluated with data from the KITTI odometry benchmark datasets.
ER  - 

TY  - CONF
TI  - StaticFusion: Background Reconstruction for Dense RGB-D SLAM in Dynamic Environments
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3849
EP  - 3856
AU  - R. Scona
AU  - M. Jaimez
AU  - Y. R. Petillot
AU  - M. Fallon
AU  - D. Cremers
PY  - 2018
KW  - cameras
KW  - image colour analysis
KW  - image filtering
KW  - image motion analysis
KW  - image reconstruction
KW  - image segmentation
KW  - image sensors
KW  - image sequences
KW  - motion estimation
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - frame-to-model alignment
KW  - 3D model estimation
KW  - outlier filtering techniques
KW  - moving object detection
KW  - camera pose tracking
KW  - probabilistic static-dynamic segmentation
KW  - background structure reconstruction
KW  - dynamic scenes
KW  - static environments
KW  - dynamic sequences
KW  - static sequences
KW  - camera motion estimation
KW  - weighted dense RGB-D fusion
KW  - current RGB-D image pair
KW  - implicit robust penalisers
KW  - background structure
KW  - robust dense RGB-D SLAM
KW  - visual SLAM
KW  - dynamic environments
KW  - Cameras
KW  - Robustness
KW  - Image segmentation
KW  - Motion segmentation
KW  - Dynamics
KW  - Three-dimensional displays
KW  - Image reconstruction
DO  - 10.1109/ICRA.2018.8460681
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Dynamic environments are challenging for visual SLAM as moving objects can impair camera pose tracking and cause corruptions to be integrated into the map. In this paper, we propose a method for robust dense RGB-D SLAM in dynamic environments which detects moving objects and simultaneously reconstructs the background structure. While most methods employ implicit robust penalisers or outlier filtering techniques in order to handle moving objects, our approach is to simultaneously estimate the camera motion as well as a probabilistic static/dynamic segmentation of the current RGB-D image pair. This segmentation is then used for weighted dense RGB-D fusion to estimate a 3D model of only the static parts of the environment. By leveraging the 3D model for frame-to-model alignment, as well as static/dynamic segmentation, camera motion estimation has reduced overall drift - as well as being more robust to the presence of dynamics in the scene. Demonstrations are presented which compare the proposed method to related state-of-the-art approaches using both static and dynamic sequences. The proposed method achieves similar performance in static environments and improved accuracy and robustness in dynamic scenes.
ER  - 

TY  - CONF
TI  - Vision Based Collaborative Path Planning for Micro Aerial Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3889
EP  - 3895
AU  - S. Vemprala
AU  - S. Saripalli
PY  - 2018
KW  - cameras
KW  - collision avoidance
KW  - covariance matrices
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - trees (mathematics)
KW  - microaerial vehicles
KW  - collaborative path-planning framework
KW  - localization uncertainty
KW  - two-step planning framework
KW  - visual-fidelity aerial vehicle simulator
KW  - Planning
KW  - Uncertainty
KW  - Cameras
KW  - Three-dimensional displays
KW  - Collaboration
KW  - Optimization
KW  - Path planning
DO  - 10.1109/ICRA.2018.8462910
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we present a collaborative path-planning framework for a group of micro aerial vehicles that are capable of localizing through vision. Each of the micro aerial vehicles is assumed to be equipped with a forward facing monocular camera. The vehicles initially use their captured images to build 3D maps through common features; and subsequently track these features to localize through 3D-2D correspondences. The planning algorithm, while connecting start locations to provided goal locations, also aims to reduce the localization uncertainty of the vehicles in the group. To achieve this, we develop a two-step planning framework: the first step attempts to build an improved map of the environment by solving the next-best-view problem for multiple cameras. We express this as a black-box optimization problem and solve it using the Covariance Matrix Adaption evolution strategy (CMA-ES). Once an improved map is available, the second stage of the planning framework performs belief space planning for the vehicles individually using the rapidly exploring random belief tree (RRBT) algorithm. Through the RRBT approach, the planner generates paths that ensure feature visibility while attempting to optimize path cost and reduce localization uncertainty. We validate our approach using experiments conducted in a high visual-fidelity aerial vehicle simulator, Microsoft AirSim.
ER  - 

TY  - CONF
TI  - Semi-Dense Visual-Inertial Odometry and Mapping for Quadrotors with SWAP Constraints
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3904
EP  - 3909
AU  - W. Liu
AU  - G. Loianno
AU  - K. Mohta
AU  - K. Daniilidis
AU  - V. Kumar
PY  - 2018
KW  - helicopters
KW  - inertial navigation
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - state estimation
KW  - stereo image processing
KW  - semidense visual-inertial odometry
KW  - quadrotors
KW  - SWAP constraints
KW  - autonomous navigation capabilities
KW  - dense 3D maps
KW  - indoor environments
KW  - visual inertial state estimation
KW  - microaerial vehicles
KW  - size, weight, and power constraints
KW  - stereo camera
KW  - Cameras
KW  - Three-dimensional displays
KW  - Visual odometry
KW  - Optimization
KW  - Navigation
KW  - Robot vision systems
DO  - 10.1109/ICRA.2018.8463163
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Micro Aerial Vehicles have the potential to assist humans in real life tasks involving applications such as smart homes, search and rescue, and architecture construction. To enhance autonomous navigation capabilities these vehicles need to be able to create dense 3D maps of the environment, while concurrently estimating their own motion. In this paper, we are particularly interested in small vehicles that can navigate cluttered indoor environments. We address the problem of visual inertial state estimation, control and 3D mapping on platforms with Size, Weight, And Power (SWAP) constraints. The proposed approach is validated through experimental results on a 250 g, 22 cm diameter quadrotor equipped only with a stereo camera and an IMU with a computationally-limited CPU showing the ability to autonomously navigate, while concurrently creating a 3D map of the environment.
ER  - 

TY  - CONF
TI  - Approximation Algorithms for Tours of Orientation-Varying View Cones
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3910
EP  - 3915
AU  - N. Stefas
AU  - P. A. Plonski
AU  - V. Isler
PY  - 2018
KW  - approximation theory
KW  - computational complexity
KW  - travelling salesman problems
KW  - apex angle
KW  - 3D traveling salesman problem
KW  - shorter tours
KW  - tilted Cone-TSPN problem
KW  - planar surface
KW  - apex points
KW  - inverted cone views
KW  - shortest tour
KW  - orientation-varying view cones
KW  - approximation algorithms
KW  - Approximation algorithms
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Traveling salesman problems
KW  - Lakes
KW  - Animals
KW  - Cameras
DO  - 10.1109/ICRA.2018.8462908
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper considers the problem of finding the shortest tour to cover a given set of inverted cone views with apex angle α and height H when their apex points lie on a planar surface. This is a novel variant of the 3D Traveling Salesman Problem with intersecting Neighborhoods (TSPN) called Cone-TSPN. When the cones are allowed to tilt by an angle c we have the tilted Cone-TSPN problem, to which we present an algorithm that returns a solution with an approximation ratio of O (1+tan α/1-tan ϵ tan α (1 + log max(H)/min(H)). We demonstrate through simulations that our algorithm can be implemented in a practical way and by exploiting the structure of the cones we can achieve shorter tours. Finally, we present results from covering a reflective surface (lake area) that shows the importance of selecting different view angles under strong sunlight specularities.
ER  - 

TY  - CONF
TI  - Geometric Calibration of an OCT Imaging System
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3993
EP  - 3999
AU  - M. Ourak
AU  - B. Tamadazte
AU  - G. J. Laurent
AU  - N. Andreff
PY  - 2018
KW  - biological organs
KW  - biomedical optical imaging
KW  - calibration
KW  - medical image processing
KW  - optical tomography
KW  - optical coherence tomography
KW  - OCT geometric calibration method
KW  - OCT imaging system
KW  - spectral domain OCT system
KW  - 3D images
KW  - calibration model
KW  - spectral distortions
KW  - optical path
KW  - OCT images formation
KW  - optical biopsies
KW  - OCT medical imaging system
KW  - Optical distortion
KW  - Distortion
KW  - Optical imaging
KW  - Mirrors
KW  - Adaptive optics
KW  - Two dimensional displays
KW  - Three-dimensional displays
DO  - 10.1109/ICRA.2018.8463171
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper deals with an OCT (optical coherence tomography) geometric calibration method. OCT medical imaging system has received a growing interest during the last two decades. In medical purposes, OCT images are generally called optical biopsies which allows in-vivo investigation almost similar to a histopathological study. The physician can rely on the OCT images to establish a rapid and direct diagnosis. But the OCT images formation suffered numerous distortions due in particular to the optical path, from the source to the viewed sample passing through the two reflecting mirrors and a scan objective. The obtained optical biopsies include several spectral and geometric distortions. The proposed calibration model aims to compensate the geometrical ones. More precisely, two models were developed allowing the correction of both 2D images (B-Scan slices) and 3D images (volume). These models were experimentally validated (in both artificial and biological samples) using a spectral domain OCT system. It has demonstrated a significant enhancement of the OCT images accuracy.
ER  - 

TY  - CONF
TI  - Marker-Based Registration for Large Deformations - Application to Open Liver Surgery
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4007
EP  - 4012
AU  - Y. Adagolodjo
AU  - N. Golse
AU  - E. Vibert
AU  - M. De Mathelin
AU  - S. Cotin
AU  - H. Courtecuisse
PY  - 2018
KW  - augmented reality
KW  - computerised tomography
KW  - image registration
KW  - liver
KW  - medical image processing
KW  - surgery
KW  - tumours
KW  - augmented reality system
KW  - marker-based method
KW  - liver resection surgery
KW  - realtime tracking algorithm
KW  - nonrigid initial registration method
KW  - preoperative model
KW  - open surgery
KW  - open liver surgery
KW  - marker-based registration
KW  - Surgery
KW  - Strain
KW  - Liver
KW  - Cameras
KW  - Deformable models
KW  - Biological system modeling
DO  - 10.1109/ICRA.2018.8462909
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper introduces an Augmented Reality (AR) system for open liver surgery. Although open surgery remains the gold-standard for the treatment of complex tumors and central lesions, technological issues actually prevent using AR with sufficient accuracy for clinical use. We propose a markers-based method allowing for the tracking and the deformation of a preoperative model in real-time during the surgery. Markers are manually placed on the surface of the organ after opening the abdominal cavity, and tracked in real-time by a set of infrared cameras. Our framework is composed of both a nonrigid initial registration method, providing an estimation of the location of the markers in the preoperative model, and a realtime tracking algorithm to deform the model during the surgery (even for large deformation or partial occlusion of the organ). The method is validated on both synthetic and ex-vivo samples; in addition, we demonstrate its applicability in the operating room during a liver resection surgery on a human patient. Preliminary studies provided promising results to improve the location of tumors, and to help surgeons into planning the ideal resection intraoperatively.
ER  - 

TY  - CONF
TI  - Real-Time Image-Guided Cooperative Robotic Assist Device for Deep Anterior Lamellar Keratoplasty
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4013
EP  - 4018
AU  - M. Draelos
AU  - B. Keller
AU  - G. Tang
AU  - A. Kuo
AU  - K. Hauser
AU  - J. Izatt
PY  - 2018
KW  - biomechanics
KW  - biomedical equipment
KW  - biomedical optical imaging
KW  - eye
KW  - manipulators
KW  - medical robotics
KW  - optical tomography
KW  - surgery
KW  - robot arm
KW  - graft rejection risk
KW  - chronic immunosuppression comorbidities
KW  - corneal transplantation
KW  - promising technique
KW  - deep anterior lamellar keratoplasty
KW  - time image-guided cooperative robotic
KW  - perforation-free needle depth
KW  - DALK needle insertions
KW  - real-time OCT segmentation
KW  - posterior corneal boundary virtual fixture
KW  - optical coherence tomography imaging
KW  - robot-assisted solution
KW  - inadequate needle depth
KW  - Needles
KW  - Surgery
KW  - Robot sensing systems
KW  - Visualization
KW  - Tools
KW  - Cornea
KW  - Cooperative control
KW  - medical robotics
DO  - 10.1109/ICRA.2018.8463153
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Deep anterior lamellar keratoplasty (DALK) is a promising technique for corneal transplantation that avoids the chronic immunosuppression comorbidities and graft rejection risk associated with penetrating keratoplasty (PKP), the standard procedure. In DALK, surgeons must insert a needle 90% through the 500 μm cornea without penetrating its underlying membrane. This pushes surgeons to their manipulation and visualization limits such that 59% of DALK attempts fail due to corneal perforation or inadequate needle depth. We propose a robot-assisted solution to jointly solve the manipulation and visualization challenges using a cooperatively-controlled, precise robot arm and live optical coherence tomography (OCT) imaging, respectively. Our system features an interface handle, with which the surgeon and robot cooperatively hold the tool, and a posterior corneal boundary virtual fixture driven by real-time OCT segmentation. A study in which three operators performed DALK needle insertions manually and cooperatively in ex vivo human corneas demonstrated an 84% improvement in perforation-free needle depth without an increased perforation rate.
ER  - 

TY  - CONF
TI  - Hall Effect Sensing Workspace Estimation with Non-Permanent Magnetic Needle for Eye Anesthesia Training System via Robotic Experiments
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4019
EP  - 4024
AU  - K. Borvorntanajanya
AU  - J. Suthakorn
PY  - 2018
KW  - eye
KW  - Hall effect devices
KW  - Kalman filters
KW  - manipulators
KW  - medical robotics
KW  - needles
KW  - position control
KW  - sensor arrays
KW  - sensors
KW  - surgery
KW  - Hall effect sensing workspace
KW  - nonpermanent magnetic needle
KW  - eye anesthesia training system
KW  - robotic experiments
KW  - eye surgery
KW  - needle tip tracking system
KW  - ophthalmic anesthesia training
KW  - anesthesia needle
KW  - magnetized needle tip
KW  - Hall-effect sensor array
KW  - orbital structure model
KW  - Hall-effect sensors
KW  - ophthalmic anesthesia pathway
KW  - needle tip position
KW  - commercial robotic manipulator
KW  - developed system
KW  - Needles
KW  - Robot sensing systems
KW  - Magnetic flux
KW  - Anesthesia
KW  - Orbits
DO  - 10.1109/ICRA.2018.8461015
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Ophthalmic anesthesia is an important preparation for eye surgery. The conventional practice is performed blind in a cadaver under the supervision of an experienced surgeon. This paper introduces a needle tip tracking system for ophthalmic anesthesia training without major modification of an anesthesia needle. The study presents a prototyped system to track a magnetized needle tip using Hall-effect sensor array. The orbital structure model was embedded with Hall-effect sensors after considering the sensing workspace and ophthalmic anesthesia pathway. The extended Kalman filter was used to calculate needle tip position. A commercial robotic manipulator was used to model the characteristics of sensor and accuracy of the developed system. A prototype can detect needle tip position with a root-mean-square deviation around 1.80 mm. As a result, the system is capable of providing needle tip positions for training purposes.
ER  - 

TY  - CONF
TI  - Precision Needle Tip Localization Using Optical Coherence Tomography Images for Subretinal Injection
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4033
EP  - 4040
AU  - M. Zhou
AU  - K. Huang
AU  - A. Eslami
AU  - H. Roodaki
AU  - D. Zapp
AU  - M. Maier
AU  - C. P. Lohmann
AU  - A. Knoll
AU  - M. A. Nasseri
PY  - 2018
KW  - biological tissues
KW  - biomedical equipment
KW  - biomedical optical imaging
KW  - eye
KW  - medical image processing
KW  - medical robotics
KW  - needles
KW  - optical tomography
KW  - surgery
KW  - robot-assisted subretinal injection
KW  - needle tip localization
KW  - microsurgery
KW  - microscope-integrated intraoperative optical coherence tomography
KW  - insertion depth
KW  - subretinal visual feedback
KW  - optical coherence tomography images
KW  - Needles
KW  - Retina
KW  - Surgery
KW  - Microscopy
KW  - Robots
KW  - Probes
DO  - 10.1109/ICRA.2018.8460745
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Subretinal injection is a delicate and complex microsurgery, which requires surgeons to inject the therapeutic substance in a pre-operatively defined and intra-operatively updated subretinal target area. Due to the lack of subretinal visual feedback, it is hard to sense the insertion depth during the procedure, thus affecting the results of surgical outcome and hindering the widespread use of this treatment. This paper presents a novel approach to estimate the 3D position of the needle under the retina using the information from microscope-integrated Intraoperative Optical Coherence Tomography (iOCT). We evaluated our approach on both tissue phantom and ex-vivo porcine eyes. Evaluation results show that the average error in distance measurement is 4.7 μm (maximum of 16.5 μm). We furthermore, verified the feasibility of the proposed method to track the insertion depth of needle in robot-assisted subretinal injection.
ER  - 

TY  - CONF
TI  - Proprioceptive Inference for Dual-Arm Grasping of Bulky Objects Using RoboSimian
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4049
EP  - 4056
AU  - M. Burkhardt
AU  - S. Karumanchi
AU  - K. Edelberg
AU  - J. W. Burdick
AU  - P. Backes
PY  - 2018
KW  - Bayes methods
KW  - dexterous manipulators
KW  - humanoid robots
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - position control
KW  - robot kinematics
KW  - sensors
KW  - torque measurement
KW  - nasa jet propulsion laboratorys
KW  - robosimian
KW  - JPL
KW  - supporting manipulator
KW  - cumbersome objects
KW  - data-driven Bayesian models
KW  - inferred object properties
KW  - dual-arm lifting
KW  - bulky object
KW  - dual-arm grasping
KW  - proprioceptive inference
KW  - Manipulators
KW  - Probabilistic logic
KW  - Rotation measurement
KW  - Grasping
KW  - Shape
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8460776
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This work demonstrates dual-arm lifting of bulky objects based on inferred object properties (center of mass (COM) location, weight, and shape) using proprioception (i.e. force torque measurements). Data-driven Bayesian models describe these quantities, which enables subsequent behaviors to depend on confidence of the learned models. Experiments were conducted using the NASA Jet Propulsion Laboratory's (JPL) RoboSimian to lift a variety of cumbersome objects ranging in mass from 7kg to 25kg. The position of a supporting second manipulator was determined using a particle set and heuristics that were derived from inferred object properties. The supporting manipulator decreased the initial manipulator's load and distributed the wrench load more equitably across each manipulator, for each bulky object. Knowledge of the objects came from pure proprioception (i.e. without reliance on vision or other exteroceptive sensors) throughout the experiments.
ER  - 

TY  - CONF
TI  - Compact and High Performance Torque-Controlled Actuators and its Implementation to Disaster Response Robot
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4057
EP  - 4063
AU  - Y. Kanemoto
AU  - T. Yoshiike
AU  - M. Muromachi
AU  - M. Osada
PY  - 2018
KW  - actuators
KW  - disasters
KW  - electromagnetic interference
KW  - gears
KW  - legged locomotion
KW  - rescue robots
KW  - strain gauges
KW  - torque control
KW  - high performance torque-controlled actuators
KW  - disaster response robot
KW  - scattered debris
KW  - axial compactness
KW  - torque sensors
KW  - torque control
KW  - analog digital converter board
KW  - differential control
KW  - joint torque
KW  - torque ripple
KW  - torque-controlled legged robot
KW  - disaster environments
KW  - harmonic drive gear
KW  - electromagnetic interference
KW  - strain gauges
KW  - Torque
KW  - Actuators
KW  - Robot sensing systems
KW  - Torque measurement
KW  - Strain measurement
DO  - 10.1109/ICRA.2018.8460789
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Applying robots in narrow and cluttered disaster environments such as oil refineries requires a slim body and a wide range of motion. It is also necessary to have abilities to absorb unexpected contact with the environment and to walk on scattered debris. In this paper we propose new compact and high performance torque-controlled actuators for legged robots to satisfy the above mentioned requirements. For axial compactness, torque sensors are designed as ring-shaped thin cylinders surrounding motors or gears with strain gauges for sensing. To achieve broad bandwidth of torque control, we introduced an analog differentiator circuit into an analog digital converter (ADC) board in order to suppress noise in the differential control of joint torque. We also propose methods to reduce torque ripple caused by the deformation of the harmonic drive gear and electromagnetic interference (EMI) from a motor and a motor driver. Finally, experiments of a collision with objects and movement on scattered debris were executed with a fully torque-controlled legged robot built with the proposed actuators.
ER  - 

TY  - CONF
TI  - High Dynamic Range Sensing by a Multistage Six-Axis Force Sensor with Stopper Mechanism
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4065
EP  - 4070
AU  - D. Okumura
AU  - S. Sakaino
AU  - T. Tsuji
PY  - 2018
KW  - force measurement
KW  - force sensors
KW  - seals (stoppers)
KW  - torque measurement
KW  - high dynamic range six-axis force-torque sensor
KW  - HDR six-axis force-torque sensor
KW  - force measurement
KW  - stopper mechanism
KW  - multistage six-axis force sensor
KW  - high dynamic range sensing
KW  - HDR measurement
KW  - overload protection mechanism
KW  - low-rigidity flexure element
KW  - high-rigidity flexure element
KW  - Robot sensing systems
KW  - Force sensors
KW  - Force
KW  - Strain
KW  - Force measurement
KW  - Mathematical model
DO  - 10.1109/ICRA.2018.8460571
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper describes the design of a high dynamic range (HDR) six-axis force/torque sensor. The sensor is composed of a high-rigidity flexure element detecting large force and a low-rigidity flexure element detecting small force. The overload on the low-rigidity flexure element is prevented by an overload protection mechanism. An HDR measurement is achieved by combining the outputs of the two flexure elements. A loading test for the designed sensor is performed, and the results indicate that the six-axis sensor measures force with a dynamic range from 0.01N to 1000 N.
ER  - 

TY  - CONF
TI  - Principal Components of Touch
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4071
EP  - 4078
AU  - K. Aquilina
AU  - D. A. W. Barton
AU  - N. F. Lepora
PY  - 2018
KW  - biology computing
KW  - data visualisation
KW  - principal component analysis
KW  - sensor arrays
KW  - tactile sensors
KW  - touch (physiological)
KW  - vibrissal arrays
KW  - PCA
KW  - touch
KW  - complex robotic manipulation
KW  - tactile sensor arrays
KW  - principal component analysis
KW  - visualisation approach
KW  - k-NN
KW  - Euclidean distance
KW  - Principal component analysis
KW  - Tactile sensors
KW  - Data visualization
KW  - Sensor arrays
KW  - Pins
DO  - 10.1109/ICRA.2018.8461045
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Our human sense of touch enables us to manipulate our surroundings; therefore, complex robotic manipulation will require artificial tactile sensing. Typically tactile sensor arrays are used in robotics, implying that a straightforward way of interpreting multidimensional data is required. In this paper we present a simple visualisation approach based on applying principal component analysis (PCA) to systematically collected sets of tactile data. We apply the visualisation approach to 4 different types of tactile sensor, encompassing fingertips and vibrissal arrays. The results show that PCA can reveal structure and regularities in the tactile data, which also permits the use of simple classifiers such as k-NN to achieve good inference. Additionally, the Euclidean distance in principal component space gives a measure of sensitivity, which can aid visualisation and also be used to find regions in the tactile input space where the sensor is able to perceive with higher accuracy. We expect that these observations will generalise, and thus offer the potential for novel control methods based on touch.
ER  - 

TY  - CONF
TI  - Design and Force-Tracking Impedance Control of a 2-DOF Wall-Cleaning Manipulator Using Disturbance Observer and Sliding Mode Control
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4079
EP  - 4084
AU  - T. Kim
AU  - S. Yoo
AU  - H. S. Kim
AU  - J. Kim
PY  - 2018
KW  - control system synthesis
KW  - force control
KW  - manipulator dynamics
KW  - motion control
KW  - observers
KW  - position control
KW  - variable structure systems
KW  - disturbance observer
KW  - sliding mode control
KW  - SMC
KW  - position-based force
KW  - FTIC
KW  - constant contact force
KW  - force tracking capability
KW  - force-tracking impedance control
KW  - 2-degree-of-freedom
KW  - 2-DOF wall-cleaning manipulator
KW  - Force
KW  - Manipulator dynamics
KW  - Brushes
KW  - Impedance
KW  - Dynamics
KW  - Shape
DO  - 10.1109/ICRA.2018.8460897
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents design and force-tracking impedance control of 2-degree-of-freedom (DOF) wall-cleaning manipulator equipped with disturbance observer (DOB) and sliding mode control (SMC). In order to keep in contact with various shapes of walls, the proposed manipulator is designed to ensure 2-DOF motions of translation and tilting by using ball screws. The position-based force tracking impedance control (FTIC) is first adopted for the proposed manipulator not only to interact with walls in a desired dynamic behavior but also to maintain a constant contact force. Also, to improve the force tracking capability of proposed manipulator against different walls and brushes for manipulator, the FTIC is combined with the disturbance observer (DOB) and the sliding mode control (SMC). Extensive experiments prove that although different brushes used for manipulator rotate against varying shapes of walls, the proposed manipulator can keep a constant contact force within a bound of ± 4.5 N by virtue of the proposed FTIC equipped with the DOB and the SMC.
ER  - 

TY  - CONF
TI  - Artistic Pen Drawing on an Arbitrary Surface Using an Impedance-Controlled Robot
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4085
EP  - 4090
AU  - D. Song
AU  - T. Lee
AU  - Y. J. Kim
PY  - 2018
KW  - art
KW  - computational geometry
KW  - curve fitting
KW  - manipulators
KW  - position control
KW  - splines (mathematics)
KW  - vectors
KW  - surface-reconstruction
KW  - position control
KW  - vector-graphics engine
KW  - Bézier spline curves
KW  - artistic pen drawing
KW  - impedance control
KW  - seven-degree-of-freedom manipulator
KW  - pen strokes
KW  - pen art
KW  - semiautonomous robotic pen-drawing system
KW  - impedance-controlled robot
KW  - arbitrary surface
KW  - Surface impedance
KW  - Robot sensing systems
KW  - Service robots
KW  - Surface reconstruction
KW  - Rendering (computer graphics)
KW  - Art
DO  - 10.1109/ICRA.2018.8461084
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a semi-autonomous robotic pen-drawing system that is capable of creating pen art on an arbitrary surface with varying thickness of pen strokes but without reconstructing the surface explicitly. Our robotic system relies on an industrial, seven-degree-of-freedom (7DoF) manipulator that can be both position- and impedance-controlled. We use a vector-graphics engine to take an artist's pen drawing as input and generate Bézier spline curves with varying offsets. In order to estimate geometric details of the target, unknown surface, during drawing, we rely on incremental and adaptive sampling on the surface using a combination of position and impedance control. Then, our control algorithm physically replicates this drawing on any arbitrary, continuous surface by impedance-controlling the manipulator. We demonstrate that our system can create visually-pleasing and complicated artistic pen drawings on general surfaces without explicit surface-reconstruction nor visual feedback.
ER  - 

TY  - CONF
TI  - Detection and Control of Contact Force Transients in Robotic Manipulation Without a Force Sensor
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4091
EP  - 4096
AU  - M. Karlsson
AU  - A. Robertsson
AU  - R. Johansson
PY  - 2018
KW  - force sensors
KW  - industrial robots
KW  - manipulators
KW  - recurrent neural nets
KW  - torque
KW  - recurrent neural network
KW  - RNN
KW  - industrial robot
KW  - force transient detection
KW  - robot joint torques
KW  - force sensor
KW  - robotic manipulation
KW  - contact force transients
KW  - Robot sensing systems
KW  - Transient analysis
KW  - Training
KW  - Switches
KW  - Task analysis
KW  - Data models
DO  - 10.1109/ICRA.2018.8461104
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this research, it is shown that robot joint torques can be used to recognize contact force transients induced during robotic manipulation, thus detecting when a task is completed. The approach does not assume any external sensor, which is a benefit compared to the state of the art. The joint torque data are used as input to a recurrent neural network (RNN), and the output of the RNN indicates whether the task is completed. A real-time application for force transient detection is developed, and verified experimentally on an industrial robot.
ER  - 

TY  - CONF
TI  - Unsupervised Learning of Hierarchical Models for Hand-Object Interactions
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4097
EP  - 4102
AU  - X. Xie
AU  - H. Liu
AU  - M. Edmonds
AU  - F. Gaol
AU  - S. Qi
AU  - Y. Zhu
AU  - B. Rothrock
AU  - S. Zhu
PY  - 2018
KW  - image segmentation
KW  - pose estimation
KW  - support vector machines
KW  - tactile sensors
KW  - unsupervised learning
KW  - force vectors
KW  - unsupervised manner
KW  - event labeling sequences
KW  - manipulation event segmentation
KW  - hierarchical models
KW  - hand-object interactions
KW  - contact forces
KW  - unsupervised learning approach
KW  - manipulation event parsing
KW  - low-cost easy-to-replicate tactile glove
KW  - temporal grammar model
KW  - Force
KW  - Grammar
KW  - Robot sensing systems
KW  - Task analysis
KW  - Motion segmentation
KW  - Unsupervised learning
DO  - 10.1109/ICRA.2018.8461214
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Contact forces of the hand are visually unobservable, but play a crucial role in understanding hand-object interactions. In this paper, we propose an unsupervised learning approach for manipulation event segmentation and manipulation event parsing. The proposed framework incorporates hand pose kinematics and contact forces using a low-cost easy-to-replicate tactile glove. We use a temporal grammar model to capture the hierarchical structure of events, integrating extracted force vectors from the raw sensory input of poses and forces. The temporal grammar is represented as a temporal And-Or graph (T-AOG), which can be induced in an unsupervised manner. We obtain the event labeling sequences by measuring the similarity between segments using the Dynamic Time Alignment Kernel (DTAK). Experimental results show that our method achieves high accuracy in manipulation event segmentation, recognition and parsing by utilizing both pose and force data.
ER  - 

TY  - CONF
TI  - Decoupled Motion Control of Wearable Robot for Rejecting Human Induced Disturbances
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4103
EP  - 4110
AU  - F. Y. Wu
AU  - H. H. Asada
PY  - 2018
KW  - artificial limbs
KW  - dexterous manipulators
KW  - medical robotics
KW  - mobile robots
KW  - motion control
KW  - position control
KW  - wearable robots
KW  - human arm
KW  - human induced disturbances
KW  - data-driven latent space impedance control method
KW  - latent space impedance controller
KW  - wearable robotic fingers
KW  - decoupled motion control
KW  - wearable extra limbs
KW  - human movement
KW  - self-standing robots
KW  - single-handed object manipulation
KW  - 5G mobile communication
KW  - Conferences
KW  - Automation
KW  - Australia
DO  - 10.1109/ICRA.2018.8461109
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - When a human performs a task with the assistance of wearable extra limbs, the human movement for performing the task may inadvertently disturb the position and orientation of the robot base, making it difficult for the robot to properly carry out its objective. Therefore, unlike self-standing robots, a wearable robot must not only assist the user without interfering or prohibiting the natural human movement, but also have the capability to detect and reject disturbances caused by the wearer's motion. This paper examines such a situation, where the human attempts to twist open a bottle while a pair of robotic fingers mounted on the same arm holds the bottle in place. As the human arm rotates to twist the cap, the robot and consequently the bottle would rotate in that same direction, which makes separation of the cap from the bottle almost impossible. To compensate for the human induced disturbances, a data-driven latent space impedance control method is developed such that the robot can secure the bottle and at the same time allow natural human movement to be carried out during manipulation. Simulation and experiments have demonstrated the efficacy of the latent space impedance controller to enable single-handed object manipulation with the assistance of wearable robotic fingers.
ER  - 

TY  - CONF
TI  - Estimation of Object Orientation Using Conductive Ink and Fabric Based Multilayered Tactile Sensor
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4135
EP  - 4141
AU  - G. Ponraj
AU  - H. Ren
PY  - 2018
KW  - accelerometers
KW  - dexterous manipulators
KW  - force sensors
KW  - grippers
KW  - manipulator kinematics
KW  - tactile sensors
KW  - fabric based multilayered tactile sensor
KW  - hard materials
KW  - soft materials
KW  - robotic hand gripper
KW  - kinesthetic sensation
KW  - grasped object orientation
KW  - tactile sensing
KW  - rigid inertial measurement units
KW  - object orientation estimation
KW  - conductive silver ink
KW  - conductive fabric
KW  - fabric based sensors
KW  - hard surfaces
KW  - soft surfaces
KW  - manipulator kinematics
KW  - object manipulation tasks
KW  - Ink
KW  - Fabrics
KW  - Tactile sensors
KW  - Piezoresistance
KW  - Silver
KW  - Fabric Tactile sensor
KW  - Multilayered sensor
KW  - Tilt sensing
KW  - Conductive silver ink
DO  - 10.1109/ICRA.2018.8461031
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Robots, either made from hard or soft materials, are being used increasingly for unknown object manipulation tasks in unstructured environments. To hold an object firmly and do the required task, the orientation of the object with respect to the robotic hand gripper is one of the necessary key information. Humans can sense the orientation of an object based on vision, kinesthetic sensation, or touch sensation. Similarly, robots shall also be able to estimate grasped object orientation just based on tactile sensing without knowing manipulator kinesthetic or kinematics. Existing sensors are mostly based on vision or rigid inertial measurement units (such as accelerometers, gyroscopes, magnetometers), which require additional setup and are typically not compliant with the arbitrary surfaces of the unknown obj ects. This paper discusses a new approach for object orientation estimation from a multilayered tactile sensor based on conductive silver ink and conductive fabric. Fabric based sensors are flexible and can confer to both hard and soft surfaces. Experimental results show that the tactile sensor developed is able to estimate the tilt angle without any information about manipulator kinematics.
ER  - 

TY  - CONF
TI  - Magnified Force Sensory Substitution for Telemanipulation via Force-Controlled Skin Deformation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4142
EP  - 4148
AU  - Y. Kamikawa
AU  - N. Enayati
AU  - A. M. Okamura
PY  - 2018
KW  - force control
KW  - force feedback
KW  - force sensors
KW  - haptic interfaces
KW  - medical robotics
KW  - skin
KW  - surgery
KW  - tactile sensors
KW  - telerobotics
KW  - magnified force sensory substitution
KW  - teleoperation systems
KW  - kinesthetic force feedback systems
KW  - force-controlled tactile skin deformation
KW  - tangential force
KW  - normal force
KW  - sensory substitution device
KW  - skin deformation force feedback
KW  - maximum stable kinesthetic force feedback
KW  - da Vinci Research Kit teleoperation system
KW  - force magnification
KW  - interaction force
KW  - magnified force feedback
KW  - force feedback maximized performance
KW  - force-controlled skin deformation feedback
KW  - magnified kinesthetic force feedback
KW  - Force
KW  - Force feedback
KW  - Manipulators
KW  - Robot sensing systems
KW  - Skin
KW  - Strain
KW  - Force sensors
KW  - Haptics and Haptic Interfaces
KW  - Surgical Robotics
KW  - Laparoscopy
KW  - Force Control
KW  - Telerobotics and Teleoperation
DO  - 10.1109/ICRA.2018.8460810
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Teleoperation systems could benefit from force sensory substitution when kinesthetic force feedback systems are too bulky or expensive, and when they cause instability by magnifying force feedback. We aim to magnify force feedback using sensory substitution via force-controlled tactile skin deformation, using a device with the ability to provide tangential and normal force directly to the fingerpads. The sensory substitution device is able to provide skin deformation force feedback over ten times the maximum stable kinesthetic force feedback on a da Vinci Research Kit teleoperation system. We evaluated the effect of this force magnification in two experimental tasks where the goal was to minimize interaction force with the environment. In a peg transfer task, magnified force feedback using sensory substitution improved participants' performance for force magnifications up to ten times, but decreased performance for higher force magnifications. In a tube connection task, sensory substitution that doubled the force feedback maximized performance; there was no improvement at the larger magnifications. These experiments demonstrate that magnified force feedback using sensory substitution via force-controlled skin deformation feedback can decrease applied forces similarly to magnified kinesthetic force feedback during teleoperation.
ER  - 

TY  - CONF
TI  - Obstacle-Aided Navigation of a Soft Growing Robot
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4165
EP  - 4172
AU  - J. D. Greer
AU  - L. H. Blumenschein
AU  - A. M. Okamura
AU  - E. W. Hawkes
PY  - 2018
KW  - collision avoidance
KW  - mobile robots
KW  - path planning
KW  - obstacle-aided navigation
KW  - soft growing robot
KW  - obstacle avoidance
KW  - robot path planning
KW  - soft robots
KW  - intentional obstacle collisions
KW  - soft robot navigation
KW  - robot-obstacle interaction
KW  - tip-extending soft robot
KW  - obstacle interaction model
KW  - account obstacle collisions
KW  - Collision avoidance
KW  - Computational modeling
KW  - Kinematics
KW  - Pneumatic systems
KW  - Soft robotics
KW  - Navigation
DO  - 10.1109/ICRA.2018.8460777
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - For many types of robots, avoiding obstacles is necessary to prevent damage to the robot and environment. As a result, obstacle avoidance has historically been an important problem in robot path planning and control. Soft robots represent a paradigm shift with respect to obstacle avoidance because their low mass and compliant bodies can make collisions with obstacles inherently safe. Here we consider the benefits of intentional obstacle collisions for soft robot navigation. We develop and experimentally verify a model of robot-obstacle interaction for a tip-extending soft robot. Building on the obstacle interaction model, we develop an algorithm to determine the path of a growing robot that takes into account obstacle collisions. We find that obstacle collisions can be beneficial for open-loop navigation of growing robots because the obstacles passively steer the robot, both reducing the uncertainty of the location of the robot and directing the robot to targets that do not lie on a straight path from the starting point. Our work shows that for a robot with predictable and safe interactions with obstacles, target locations in a cluttered, mapped environment can be reached reliably by simply setting the initial trajectory. This has implications for the control and design of robots with minimal active steering.
ER  - 

TY  - CONF
TI  - Color-Based Sensing of Bending Deformation on Soft Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4181
EP  - 4187
AU  - R. B. N. Scharff
AU  - R. M. Doornbusch
AU  - X. L. Klootwijk
AU  - A. A. Doshi
AU  - E. L. Doubrovski
AU  - J. Wu
AU  - J. M. P. Geraedts
AU  - C. C. L. Wang
PY  - 2018
KW  - image colour analysis
KW  - pneumatic actuators
KW  - robots
KW  - signal generators
KW  - three-dimensional printing
KW  - signal generator
KW  - color sensors
KW  - soft pneumatic actuators
KW  - soft actuators
KW  - multicolor 3D printing
KW  - soft robots
KW  - bending deformation
KW  - color-based sensing
KW  - Color
KW  - Strain
KW  - Actuators
KW  - Robot sensing systems
KW  - Signal generators
KW  - Soft robotics
DO  - 10.1109/ICRA.2018.8460521
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper introduces a novel approach for sensing the bending deformation on soft robots by leveraging multicolor 3D printing. The measurement of deformation enables to complete the feedback loop of deformation control on soft actuators. The working principle of our approach is based on using compact color sensors to detect deformation that is visualized by the change of color ratios. Two novel designs are presented to generate color signals on 3D printed objects, which we call an external signal generator and an internal signal generator. Signal processing and calibration methods are developed to transform the raw RGB-data into a meaningful deformation metric. Our experimental tests taken on soft pneumatic actuators verify that color signals can be stably generated and captured to indicate the bending deformation. The results also demonstrate the usability of this sensing approach in deformation control.
ER  - 

TY  - CONF
TI  - Modelling and Control of a Novel Soft Crawling Robot Based on a Dielectric Elastomer Actuator
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4188
EP  - 4193
AU  - J. Cao
AU  - W. Liang
AU  - Q. Ren
AU  - U. Gupta
AU  - F. Chen
AU  - J. Zhu
PY  - 2018
KW  - elastomers
KW  - electric actuators
KW  - electroactive polymer actuators
KW  - feedback
KW  - feedforward
KW  - mobile robots
KW  - motion control
KW  - viscoelasticity
KW  - dielectric elastomer actuator
KW  - soft crawling robot
KW  - inchworms
KW  - viscoelasticity
KW  - feedforward plus feedback control scheme
KW  - motion control
KW  - Force
KW  - Actuators
KW  - Soft robotics
KW  - Friction
KW  - Steady-state
KW  - Electrodes
DO  - 10.1109/ICRA.2018.8460784
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Soft robots have recently evoked extensive attention due to their abilities to work effectively in unstructured environments. As an actuation technology of soft robots, dielectric elastomers exhibit many intriguing attributes such as large strain and high energy density. This work presents a novel dielectric elastomer based soft crawling robot inspired by inchworms. To fill the need of control of the soft robot, a model describing the interaction between the dielectric elastomer actuator and the environment is proposed, which takes inertia, viscoelasticity and friction into consideration. The model can well describe the robot's dynamic performances and the modelling approach used here can be extended to other dielectric elastomer actuators with complicated geometries for control purposes. The obtained model allows us to design a feedforward plus feedback control scheme for the robot to achieve desired motion. Simulation shows fast response and good tracking performances which are further confirmed by the experiments.
ER  - 

TY  - CONF
TI  - Geometry-based Direct Simulation for Multi-Material Soft Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4194
EP  - 4199
AU  - G. Fang
AU  - C. Matte
AU  - T. Kwok
AU  - C. C. L. Wang
PY  - 2018
KW  - calibration
KW  - deformation
KW  - design engineering
KW  - elasticity
KW  - geometry
KW  - manipulators
KW  - motion control
KW  - optimisation
KW  - pneumatic actuators
KW  - shapes (structures)
KW  - three-dimensional printing
KW  - material properties
KW  - deformation simulation
KW  - deformed shape
KW  - geometry-based direct simulation
KW  - multimaterial soft robots
KW  - soft materials
KW  - motion simulation
KW  - robots fabrication
KW  - numerical optimization
KW  - pneumatic actuators
KW  - cable-driven
KW  - calibration
KW  - design engineering
KW  - manipulators
KW  - 3D-printing
KW  - elasticity
KW  - Shape
KW  - Soft robotics
KW  - Strain
KW  - Computational modeling
KW  - Optimization
KW  - Numerical models
KW  - Deformable models
DO  - 10.1109/ICRA.2018.8461088
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Robots fabricated by soft materials can provide higher flexibility and thus better safety while interacting with natural objects with low stiffness such as food and human beings. However, as many more degrees of freedom are introduced, the motion simulation of a soft robot becomes cumbersome, especially when large deformations are presented. Moreover, when the actuation is defined by geometry variation, it is not easy to obtain the exact loads and material properties to be used in the conventional methods of deformation simulation. In this paper, we present a direct approach to take the geometric actuation as input and compute the deformed shape of soft robots by numerical optimization using a geometry-based algorithm. By a simple calibration, the properties of multiple materials can be modeled geometrically in the framework. Numerical and experimental tests have been conducted to demonstrate the performance of our approach on both cable-driven and pneumatic actuators in soft robotics.
ER  - 

TY  - CONF
TI  - Incorporate Oblique Muscle Contractions to Strengthen Soft Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4200
EP  - 4205
AU  - X. Wang
AU  - H. Faraji
AU  - Y. Mengüç
PY  - 2018
KW  - actuators
KW  - biomechanics
KW  - muscle
KW  - robot kinematics
KW  - shear strength
KW  - wearable robots
KW  - soft robotics
KW  - shear forces
KW  - muscle arrangements
KW  - incompressible property
KW  - biological hydrostatic skeletons
KW  - longitudinal muscles
KW  - transverse muscles
KW  - oblique arrangement
KW  - shape-independent load-carrying capability
KW  - actuation mechanisms
KW  - oblique muscle contractions
KW  - flexibility
KW  - Muscles
KW  - Skeleton
KW  - Strain
KW  - Shape
KW  - Soft robotics
KW  - Force
KW  - Frequency modulation
DO  - 10.1109/ICRA.2018.8461139
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - For the state-of-the-art of soft robotics, the current actuation mechanisms cannot produce shear forces, neither are the current stiffening mechanisms adaptive to various deformations. Consequently, the soft robots gain strength at the price of losing flexibility. To fill this gap, we proposed a new mechanism based on the muscle arrangements and incompressible property identified in biological hydrostatic skeletons. Beside longitudinal and transverse muscles, the proposed mechanism includes the oblique arrangement which is proved to play an indispensable role of producing shear forces. The effectiveness of the new mechanism is demonstrated through a benchmark problem - carrying a distributed load at the initial horizontal configuration, thus indicating an improved direction to realise shape-independent load-carrying capability of soft robotics. Furthermore, the proposed mechanism may explain how elephants coordinate the two contradicting properties, strength and flexibility, during their trunk manipulations.
ER  - 

TY  - CONF
TI  - Efficient FEM-Based Simulation of Soft Robots Modeled as Kinematic Chains
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4206
EP  - 4213
AU  - M. Pozzi
AU  - E. Miguel
AU  - R. Deimel
AU  - M. Malvezzi
AU  - B. Bickel
AU  - O. Brock
AU  - D. Prattichizzo
PY  - 2018
KW  - dexterous manipulators
KW  - finite element analysis
KW  - pneumatic actuators
KW  - soft manipulation
KW  - soft hands
KW  - environmental constraints
KW  - object surfaces
KW  - simulation technologies
KW  - triple-layered simulation framework
KW  - dynamic properties
KW  - lumped parameter model
KW  - fast simulate soft fingers
KW  - soft pneumatic fingers
KW  - soft robots modeled
KW  - kinematic chains
KW  - robotic manipulation
KW  - grasping
KW  - force closure
KW  - single posture
KW  - contact-rich
KW  - FEM-based simulation
KW  - FEM simulation data
KW  - Computational modeling
KW  - Actuators
KW  - Deformable models
KW  - Finite element analysis
KW  - Data models
KW  - Object oriented modeling
KW  - Robots
DO  - 10.1109/ICRA.2018.8461106
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In the context of robotic manipulation and grasping, the shift from a view that is static (force closure of a single posture) and contact-deprived (only contact for force closure is allowed, everything else is obstacle) towards a view that is dynamic and contact-rich (soft manipulation) has led to an increased interest in soft hands. These hands can easily exploit environmental constraints and object surfaces without risk, and safely interact with humans, but present also some challenges. Designing them is difficult, as well as predicting, modelling, and “programming” their interactions with the objects and the environment. This paper tackles the problem of simulating them in a fast and effective way, leveraging on novel and existing simulation technologies. We present a triple-layered simulation framework where dynamic properties such as stiffness are determined from slow but accurate FEM simulation data once, and then condensed into a lumped parameter model that can be used to fast simulate soft fingers and soft hands. We apply our approach to the simulation of soft pneumatic fingers.
ER  - 

TY  - CONF
TI  - Evaluating the Quality of Non-Prehensile Balancing Grasps
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4215
EP  - 4220
AU  - R. Krug
AU  - Y. Bekiroglu
AU  - D. Kragic
AU  - M. A. Roa
PY  - 2018
KW  - dexterous manipulators
KW  - grippers
KW  - manipulator dynamics
KW  - mobile robots
KW  - nonprehensile balancing grasps
KW  - wrench-based quality metric
KW  - force-closure grasps
KW  - autonomous robotic applications
KW  - manipulation
KW  - prediction capability
KW  - dexterity
KW  - Task analysis
KW  - Measurement
KW  - Force
KW  - Robots
KW  - Grasping
KW  - Friction
KW  - Predictive models
DO  - 10.1109/ICRA.2018.8461078
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Assessing grasp quality and, subsequently, predicting grasp success is useful for avoiding failures in many autonomous robotic applications. In addition, interest in nonprehensile grasping and manipulation has been growing as it offers the potential for a large increase in dexterity. However, while force-closure grasping has been the subject of intense study for many years, few existing works have considered quality metrics for non-prehensile grasps. Furthermore, no studies exist to validate them in practice. In this work we use a real-world data set of non-prehensile balancing grasps and use it to experimentally validate a wrench-based quality metric by means of its grasp success prediction capability. The overall accuracy of up to 84 % is encouraging and in line with existing results for force-closure grasps.
ER  - 

TY  - CONF
TI  - Transferring Grasping Skills to Novel Instances by Latent Space Non-Rigid Registration
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4229
EP  - 4236
AU  - D. Rodriguez
AU  - C. Cogswell
AU  - S. Koo
AU  - S. Behnke
PY  - 2018
KW  - image registration
KW  - inference mechanisms
KW  - intelligent robots
KW  - shape recognition
KW  - latent space nonrigid transformation
KW  - coherent point drift approach
KW  - class-level knowledge
KW  - grasping motions
KW  - shape parameters
KW  - low-dimensional latent space
KW  - subspace methods
KW  - nonrigid registration method
KW  - grasping skills
KW  - Shape
KW  - Grasping
KW  - Strain
KW  - Aerospace electronics
KW  - Three-dimensional displays
KW  - Robots
KW  - Coherence
DO  - 10.1109/ICRA.2018.8461169
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Robots acting in open environments need to be able to handle novel objects. Based on the observation that objects within a category are often similar in their shapes and usage, we propose an approach for transferring grasping skills from known instances to novel instances of an object category. Correspondences between the instances are established by means of a non-rigid registration method that combines the Coherent Point Drift approach with subspace methods. The known object instances are modeled using a canonical shape and a transformation which deforms it to match the instance shape. The principle axes of variation of these deformations define a low-dimensional latent space. New instances can be generated through interpolation and extrapolation in this shape space. For inferring the shape parameters of an unknown instance, an energy function expressed in terms of the latent variables is minimized. Due to the class-level knowledge of the object, our method is able to complete novel shapes from partial views. Control poses for generating grasping motions are transferred efficiently to novel instances by the estimated non-rigid transformation.
ER  - 

TY  - CONF
TI  - Grasping Objects Big and Small: Human Heuristics Relating Grasp-Type and Object Size
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4237
EP  - 4242
AU  - A. Kothari
AU  - J. Morrow
AU  - V. Thrasher
AU  - K. Engle
AU  - R. Balasubramanian
AU  - C. Grimm
PY  - 2018
KW  - biomechanics
KW  - dexterous manipulators
KW  - motion control
KW  - human heuristics
KW  - grasp-type
KW  - object size
KW  - online data collection method
KW  - human intuition
KW  - survey questions
KW  - adopted taxonomy
KW  - wrist orientation
KW  - common grasps
KW  - object height
KW  - robot hand size
KW  - confidence-interval based polytope
KW  - object shape space
KW  - potential pre-grasps
KW  - grasping objects
KW  - fundamental object shapes
KW  - Shape
KW  - Taxonomy
KW  - Robots
KW  - Grasping
KW  - Planning
KW  - Videos
KW  - Data collection
DO  - 10.1109/ICRA.2018.8460860
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents an online data collection method that captures human intuition about what grasp types are preferred for different fundamental object shapes and sizes. Survey questions are based on an adopted taxonomy that combines grasp pre-shape, approach, wrist orientation, object shape, orientation and size which covers a large swathe of common grasps. For example, the survey identifies at what object height or width dimension (normalized by robot hand size) the human prefers to use a two finger precision grasp versus a three-finger power grasp. This information is represented as a confidence-interval based polytope in the object shape space. The result is a database that can be used to quickly find potential pre-grasps that are likely to work, given an estimate of the object shape and size.
ER  - 

TY  - CONF
TI  - Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4243
EP  - 4250
AU  - K. Bousmalis
AU  - A. Irpan
AU  - P. Wohlhart
AU  - Y. Bai
AU  - M. Kelcey
AU  - M. Kalakrishnan
AU  - L. Downs
AU  - J. Ibarz
AU  - P. Pastor
AU  - K. Konolige
AU  - S. Levine
AU  - V. Vanhoucke
PY  - 2018
KW  - image colour analysis
KW  - manipulators
KW  - neurocontrollers
KW  - robot vision
KW  - deep robotic grasping
KW  - off-the-shelf simulators
KW  - ground-truth annotations
KW  - randomized simulated environments
KW  - domain adaptation methods
KW  - grasping system
KW  - raw monocular RGB images
KW  - pixel-level domain adaptation
KW  - real-world grasping performance
KW  - annotated visual grasping datasets
KW  - GraspGAN
KW  - generative adversial network
KW  - Grasping
KW  - Robots
KW  - Training
KW  - Feature extraction
KW  - Adaptation models
KW  - Cameras
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8460875
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.
ER  - 

TY  - CONF
TI  - Coordination of Intrinsic and Extrinsic Degrees of Freedom in Soft Robotic Grasping
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4251
EP  - 4256
AU  - C. Erdogan
AU  - A. Schröder
AU  - O. Brock
PY  - 2018
KW  - adaptive control
KW  - grippers
KW  - manipulators
KW  - motion control
KW  - hand capabilities
KW  - soft RBO Hand 2
KW  - predefined motion
KW  - finger movements
KW  - compliant robot
KW  - soft robotic grasping
KW  - human grasping
KW  - movement patterns
KW  - adaptive intrinsic/extrinsic motion
KW  - Robot kinematics
KW  - Grasping
KW  - Wrist
KW  - Manipulators
KW  - Protocols
KW  - Kinematics
DO  - 10.1109/ICRA.2018.8461075
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We demonstrate that moving the wrist while the fingers perform a grasp increases performance. The coordination shapes the interactions between the fingers, the object and its environment to extend the hand capabilities (e.g. higher payload and precision). We evaluated our hypothesis with a human grasping study where the volunteers grasped objects by moving the soft RBO Hand 2 while its fingers closed in a predefined motion. We limited their ability to coordinate their motion with the finger movements using a compliant robot attached to the hand, and observed that their grasp success decreases with increased constraints. We also successfully transferred one of the observed movement patterns to the robot, indicating that adaptive intrinsic/extrinsic motion increases robotic grasp performance as well.
ER  - 

TY  - CONF
TI  - Reinforcement Learning for 4-Finger-Gripper Manipulation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4257
EP  - 4262
AU  - M. Ojer De Andres
AU  - M. Mahdi Ghazaei Ardakani
AU  - A. Robertsson
PY  - 2018
KW  - grippers
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - manipulator dynamics
KW  - motion control
KW  - path planning
KW  - high-level discrete actions
KW  - Q-learning
KW  - rhythmic Dynamic Movement Primitives
KW  - 4-finger-gripper manipulator
KW  - hierarchical planning
KW  - Reinforcement Learning
KW  - 4-finger-gripper manipulation
KW  - hierarchical-planning approach
KW  - Robots
KW  - Trajectory
KW  - Task analysis
KW  - Planning
KW  - Heuristic algorithms
KW  - Learning (artificial intelligence)
KW  - Approximation algorithms
DO  - 10.1109/ICRA.2018.8461153
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In the framework of robotics, Reinforcement Learning (RL) deals with the learning of a task by the robot itself. This paper presents a hierarchical-planning approach in which the robot learns the optimal behavior for different levels in a decoupled way. For high-level discrete actions, Q-learning was chosen, whereas for the low level we utilize Policy Improvement with Path Integrals (PI2) algorithm to learn the parameters of policies, represented by rhythmic Dynamic Movement Primitives (DMPs). The paper studies the case of a 4-finger-gripper manipulator, which performs the task of continuously spinning a ball around a desired axis. The results demonstrate the efficacy of the hierarchical planning and the increased performance of the task when PI2 is used in conjunction with rhythmic DMPs in a real environment.
ER  - 

TY  - CONF
TI  - Popcorn-Driven Robotic Actuators
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4271
EP  - 4276
AU  - S. Ceron
AU  - A. Kurumunda
AU  - E. Garg
AU  - M. Kim
AU  - T. Yeku
AU  - K. Pctersen
PY  - 2018
KW  - actuators
KW  - compressive strength
KW  - friction
KW  - granular flow
KW  - granular materials
KW  - grippers
KW  - ignition
KW  - mixtures
KW  - robot dynamics
KW  - inter-granular friction
KW  - granular fluids
KW  - jamming actuators
KW  - popcorn-driven actuation
KW  - robotics
KW  - popcorn-driven robotic actuators
KW  - popcorn kernels
KW  - expansion ratio
KW  - transition temperature
KW  - compression strength
KW  - hot oil
KW  - hot air
KW  - direct contact
KW  - heated Nichrome wire
KW  - popping force
KW  - biodegradability
KW  - Kernel
KW  - Heating systems
KW  - Robots
KW  - Jamming
KW  - Actuators
KW  - Force
KW  - Wires
DO  - 10.1109/ICRA.2018.8461147
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Popcorn kernels are a natural, edible, and inexpensive material that has the potential to rapidly expand with high force upon application of heat. Although this transition is irreversible, it carries potential for several robotic applications. Here, we examine relevant characteristics of three types of kernels including expansion ratio, transition temperature, popping force, compression strength, and biodegradability. We test the viability of popping by hot oil, hot air, microwaves, and direct contact with heated Nichrome wire. As kernels can change from regular to (larger) irregular shapes, we examine the change in inter-granular friction and propose their use as granular fluids in jamming actuators, without the need for a vacuum pump. Furthermore, as a proof-of-concept, we also demonstrate the use of popcorn-driven actuation in soft, compliant, and rigid-link grippers. Serving as a first introduction of popcorn into robotics, we hope this paper will inspire novel mechanisms for multi-functional designs.
ER  - 

TY  - CONF
TI  - A Hybrid Dynamic-Regenerative Damping Scheme for Energy Regeneration in Variable Impedance Actuators
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4277
EP  - 4282
AU  - F. wu
AU  - M. Howard
PY  - 2018
KW  - actuators
KW  - damping
KW  - electromagnetic devices
KW  - energy conservation
KW  - regenerative braking
KW  - vibration control
KW  - VIA
KW  - variable damping module design
KW  - numerical simulations
KW  - energy consumption
KW  - energy efficiency
KW  - variable impedance actuators
KW  - dynamic-regenerative damping scheme
KW  - dynamic braking
KW  - regenerative braking effect
KW  - energy regeneration
KW  - dissipated energy
KW  - Damping
KW  - Resistance
KW  - Actuators
KW  - DC motors
KW  - Task analysis
KW  - Robots
DO  - 10.1109/ICRA.2018.8460207
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Increasing research efforts have been made to improve the energy efficiency of variable impedance actuators (VIAs) through reduction of energy consumption. However, the harvesting of dissipated energy in such systems remains under-explored. This study proposes a novel variable damping module design enabling energy regeneration in VIAs by exploiting the regenerative braking effect of DC motors. The proposed damping module uses four switches to combine regenerative and dynamic braking, in a hybrid approach that enables energy regeneration without reduction in the range of damping achievable. Numerical simulations and a physical experiment are presented in which the proposed module shows an optimal trade-off between task-performance and energy efficiency.
ER  - 

TY  - CONF
TI  - Screw-Powered Propulsion in Granular Media: An Experimental and Computational Study
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4283
EP  - 4288
AU  - A. Thoesen
AU  - S. Ramirez
AU  - H. Marvi
PY  - 2018
KW  - aerospace propulsion
KW  - blades
KW  - design engineering
KW  - discrete element method
KW  - fasteners
KW  - glass
KW  - granular materials
KW  - planetary rovers
KW  - propellers
KW  - shafts
KW  - space vehicles
KW  - tracked vehicles
KW  - transportation
KW  - vehicle dynamics
KW  - thrust force
KW  - granular media
KW  - screw-powered propulsion
KW  - industrial processes
KW  - pontoon shaft
KW  - arctic media
KW  - tracked vehicles
KW  - screw design
KW  - soda-lime glass beads
KW  - screw-propelled vehicles
KW  - transportation
KW  - dewatering
KW  - blades damage
KW  - blade sinkage
KW  - lunar rover design
KW  - aqueous media
KW  - angular velocity
KW  - double-helix Archimedes screw generating propulsive force
KW  - miniaturized exploration vehicle
KW  - discrete element modeling software
KW  - size 5.0 cm
KW  - size 8.0 cm
KW  - size 10.0 cm
KW  - size 1.8 mm to 2.2 mm
KW  - size 4 cm
KW  - Fasteners
KW  - Friction
KW  - Force
KW  - Glass
KW  - Media
KW  - Young's modulus
KW  - Propulsion
DO  - 10.1109/ICRA.2018.8460916
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Screw-Propelled Vehicles (SPV's) have been widely used for terrestrial applications such as transportation over mud, snow, and amphibious environments. Similar vehicles have also been applied to industrial processes such as dewatering. Typical designs rely on a large pontoon shaft and relatively small blades to prevent unwanted sinkage or blade damage. These types of vehicles were considered during the design of the first lunar rover, given their success in aqueous and arctic media and simplicity compared to tracked vehicles. Studies have looked at the mobility of SPV's on the surface of granular media but there are not any computational and experimental studies on propulsive buried screws. Understanding the role of screw design and its angular velocity on thrust force is key to the advancement and control of SPV's. This study presents experimental and computational results of a submerged, double-helix Archimedes screw generating propulsive force against a bed of soda-lime glass beads. Thus, this research forms the basis for design of a future miniaturized exploration vehicle for space applications. In our study, we used two different screw designs (5 cm radius, 10 cm length, 63 and 44 degrees helix angle corresponding to 4 cm and 8 cm pitch, respectively) submerged in 2mm glass beads (90% roundness with sizes 1.8 mm to 2.2 mm), For both screws, a similar trend is observed between rotational speed and thrust force. We used EDEM, a Discrete Element Modeling (DEM) software for computational studies of the screw interactions with granular media. There is 5-20% discrepancy between our computational and experimental results. We will discuss possible sources of error and the potential for using DEM as a design tool for SPV's.
ER  - 

TY  - CONF
TI  - Axially and Radially Expandable Modular Helical Soft Actuator for Robotic Implantables
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4297
EP  - 4304
AU  - E. R. Perez-Guagnelli
AU  - S. Nejus
AU  - J. Yu
AU  - S. Miyashita
AU  - Y. Liu
AU  - D. D. Damian
PY  - 2018
KW  - biological tissues
KW  - biomedical materials
KW  - cellular biophysics
KW  - elastomers
KW  - medical robotics
KW  - pneumatic actuators
KW  - prosthetics
KW  - surgery
KW  - radially expandable modular helical soft actuator
KW  - axially expandable modular helical soft actuator
KW  - elastomeric strands
KW  - elongation
KW  - tissue regeneration
KW  - long-gap esophageal atresia condition
KW  - soft pneumatic actuator
KW  - soft robots
KW  - modular soft basic constituents
KW  - human body
KW  - biomedical engineering
KW  - robotic implantables
KW  - soft medical robots
KW  - pressure 19.0 kPa
KW  - Actuators
KW  - Implants
KW  - Soft robotics
KW  - Esophagus
KW  - Surgery
KW  - Mathematical model
DO  - 10.1109/ICRA.2018.8461239
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Soft robotics has advanced the field of biomedical engineering by creating safer technologies for interfacing with the human body. One of the challenges in this field is the realization of modular soft basic constituents and accessible assembly methods to increase the versatility of soft robots. We present a soft pneumatic actuator composed of two elastomeric strands that provide interdependent axial and radial expansion due to the modularity of the components and their helical arrangement. The actuator reaches 35% of elongation with respect to its initial height and both chambers achieve forces of 1N at about 19kPa. We describe the design, fabrication, modeling and benchtop testing of the soft actuator towards realizing 3D functional structures with potential medical applications. An example of application for soft medical robots is tissue regenerative for the long-gap esophageal atresia condition.
ER  - 

TY  - CONF
TI  - Displacement Amplifier Mechanism for Piezoelectric Actuators Design Using SIMP Topology Optimization Approach
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4305
EP  - 4311
AU  - T. Schlinquer
AU  - A. Mohand-Ousaid
AU  - M. Rakotondrabe
PY  - 2018
KW  - finite element analysis
KW  - optimisation
KW  - piezoelectric actuators
KW  - Rhombus mechanism
KW  - SIMP topology optimization method
KW  - displacement range
KW  - inherent crystalline properties piezoelectric actuators
KW  - SIMP topology optimization approach
KW  - piezoelectric actuators design
KW  - displacement amplifier mechanism
KW  - Optimization
KW  - Topology
KW  - Force
KW  - Piezoelectric actuators
KW  - Sensitivity
KW  - Mathematical model
KW  - piezoelectric actuators
KW  - optimal design
KW  - compliant structure
KW  - SIMP topology optimization
DO  - 10.1109/ICRA.2018.8460183
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Due to their inherent crystalline properties piezoelectric actuators have a limited deformation. This intrinsic drawback deprives to exploit the potential of these actuators such as, high bandwidth and high resolution in applications that require large displacement range. To overcome this limitation, classical as well as systematic approaches were proposed to design amplification mechanisms. The classical approach leads to empirical mechanisms which are not trivial and needs much experience and intuition. In contrast, systematic approach uses topology optimization method which permits to automatically derive optimal designs that can satisfy specified performances and imposed constraints simultaneously, this with a reasonable time and cost. This paper proposes the design of a mechanism devoted to amplify the displacement of a piezoelectric actuators (PEA). Based on the SIMP topology optimization method, the approach permits to derive a design with a displacement amplification ratio of 4.5, which is higher than with the existing method of Rhombus mechanism. Both finite element (FE) simulation and experimental results confirm and demonstrate the efficiency of the approach.
ER  - 

TY  - CONF
TI  - Clothoid-Based Global Path Planning for Autonomous Vehicles in Urban Scenarios
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4312
EP  - 4318
AU  - J. A. R. Silva
AU  - V. Grassi
PY  - 2018
KW  - curve fitting
KW  - mobile robots
KW  - path planning
KW  - road traffic
KW  - road vehicles
KW  - roads
KW  - autonomous vehicles
KW  - urban scenario
KW  - intelligent vehicles
KW  - kinematic constraints
KW  - continuous-curvature paths
KW  - low curvature derivatives
KW  - clothoid-based global path planning
KW  - road network representation
KW  - Roads
KW  - Path planning
KW  - Geometry
KW  - Autonomous vehicles
KW  - Wheels
KW  - Kinematics
KW  - Computational modeling
DO  - 10.1109/ICRA.2018.8461201
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Intelligent vehicles require an efficient way to compute a feasible path that connects its current localization to a destination point. To achieve that, the knowledge about the road network, including its geometry, is important since connections between roads can be used in the path planning. This work consists on computing a feasible global path for autonomous vehicles with kinematic constraints. Piecewise linear continuous-curvature paths composed of clothoids, circular arcs, and straight lines are used for this purpose. Low curvature derivatives provide comfort to passengers. This approach provides a compact road network representation as only the parameters of the curves are stored. A real urban scenario with straight and curved roads, multiple lanes, intersections, and roundabouts is modeled and the proposed approach is validated. As a result of the proposed approaches, door-to-door global continuous-curvature paths are generated considering the shortest traveled distance.
ER  - 

TY  - CONF
TI  - Surface-Based Exploration for Autonomous 3D Modeling
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4319
EP  - 4326
AU  - S. Song
AU  - S. Jo
PY  - 2018
KW  - mobile robots
KW  - path planning
KW  - solid modelling
KW  - surface reconstruction
KW  - 3D models
KW  - exploration algorithm
KW  - autonomous 3D modeling
KW  - path planning problem
KW  - exploration path
KW  - low-confidence surfaces
KW  - reconstructed surfaces
KW  - volumetric model
KW  - volumetric map
KW  - mobile robot
KW  - Surface reconstruction
KW  - Computational modeling
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Inspection
KW  - Mobile robots
KW  - Solid modeling
DO  - 10.1109/ICRA.2018.8460862
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this study, we addressed a path planning problem of a mobile robot to construct highly accurate 3D models of an unknown environment. Most studies have focused on exploration approaches, which find the most informative viewpoint or trajectories by analyzing a volumetric map. However, the completion of a volumetric map does not necessarily describe the completion of a 3D model. A highly complicated structure sometimes cannot be represented as a volumetric model. We propose a novel exploration algorithm that considers not only a volumetric map but also reconstructed surfaces. Unlike previous approaches, we evaluate the model completeness according to the quality of the reconstructed surfaces and extract low-confidence surfaces. The surface information is used to guide the computation of the exploration path. Experimental results showed that the proposed algorithm performed better than other state-of-the-art exploration methods and especially improved the completeness and confidence of the 3D models.
ER  - 

TY  - CONF
TI  - Departure and Conflict Management in Multi-Robot Path Coordination
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4327
EP  - 4333
AU  - P. Lertkultanon
AU  - J. Yang
AU  - H. Pham
AU  - Q. Pham
PY  - 2018
KW  - aircraft control
KW  - airports
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - automatic aircraft taxiing coordination
KW  - driver-less cars coordination
KW  - no-backward-movement constraint
KW  - complex conflict situations
KW  - Charles de Gaulle airport
KW  - multirobot path coordination
KW  - Robot kinematics
KW  - Planning
KW  - Aircraft
KW  - Collision avoidance
KW  - Airports
KW  - Automobiles
DO  - 10.1109/ICRA.2018.8460587
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper addresses the problem of multi-robot path coordination, considering specific features that arise in applications such as automatic aircraft taxiing or driver-less cars coordination. The first feature is departure events: when robots arrive at their destinations (e.g. the runway for takeoff), they can be removed from the coordination diagram. The second feature is the “no-backward-movement” constraint: the robots can only move forward on their assigned paths. These features can interact to give rise to complex conflict situations, which existing planners are unable to solve in practical time. We propose a set of algorithms to efficiently account for these features and validate these algorithms on a realistic model of Charles de Gaulle airport.
ER  - 

TY  - CONF
TI  - A Single-Planner Approach to Multi-Modal Humanoid Mobility
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4334
EP  - 4341
AU  - A. Dornbush
AU  - K. Vijayakumar
AU  - S. Bardapurkar
AU  - F. Islam
AU  - M. Ito
AU  - M. Likhachev
PY  - 2018
KW  - humanoid robots
KW  - legged locomotion
KW  - path planning
KW  - planning (artificial intelligence)
KW  - search problems
KW  - planning efforts
KW  - planning process
KW  - single-planner approach
KW  - multimodal humanoid mobility
KW  - configuration space
KW  - humanoid robot
KW  - single search process
KW  - search spaces
KW  - adaptive dimensionality
KW  - Planning
KW  - Task analysis
KW  - Aerospace electronics
KW  - Legged locomotion
KW  - Superluminescent diodes
KW  - Humanoid robots
DO  - 10.1109/ICRA.2018.8461134
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this work, we present an approach to planning for humanoid mobility. Humanoid mobility is a challenging problem, as the configuration space for a humanoid robot is intractably large, especially if the robot is capable of performing many types of locomotion. For example, a humanoid robot may be able to perform such tasks as bipedal walking, crawling, and climbing. Our approach is to plan for all these tasks within a single search process. This allows the search to reason about all the capabilities of the robot at any point, and to derive the complete solution such that the plan is guaranteed to be feasible. A key observation is that we often can roughly decompose a mobility task into a sequence of smaller tasks, and focus planning efforts to reason over much smaller search spaces. To this end, we leverage the results of a recently developed framework for planning with adaptive dimensionality, and incorporate the capabilities of available controllers directly into the planning process. The resulting planner can also be run in an interleaved fashion alongside execution so that time spent idle is much reduced.
ER  - 

TY  - CONF
TI  - Information Based Mobile Sensor Planning for Source Term Estimation of a Non-Continuous Atmospheric Release
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4342
EP  - 4347
AU  - M. Hutchinson
AU  - C. Liu
AU  - W. Chen
PY  - 2018
KW  - air pollution
KW  - atmospheric chemistry
KW  - atmospheric techniques
KW  - Bayes methods
KW  - chemical sensors
KW  - disperse systems
KW  - hazardous materials
KW  - inverse problems
KW  - mobile sensor planning
KW  - Bayes' theorem
KW  - static sensors
KW  - single mobile sensor
KW  - chemical sensor
KW  - dispersion parameters
KW  - Gaussian puff dispersion model
KW  - meteorological information
KW  - inverse problem
KW  - hazardous material
KW  - noncontinuous atmospheric release
KW  - source term estimation
KW  - Robot sensing systems
KW  - Dispersion
KW  - Atmospheric modeling
KW  - Unmanned aerial vehicles
KW  - Position measurement
KW  - Wind speed
KW  - Mathematical model
DO  - 10.1109/ICRA.2018.8460686
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Ahstract- This paper presents a method to estimate the original location and the mass of an instantaneous release of hazardous material into the atmosphere. It is formulated as an inverse problem, where concentration observations from a mobile sensor are fused with meteorological information and a Gaussian puff dispersion model to characterise the source. Bayes' theorem is used to estimate the parameters of the release taking into account the uncertainty that exists in the dispersion parameters and meteorological variables. An information based reward is used to guide an unmanned aerial vehicle equipped with a chemical sensor to the expected most informative measurement locations. Simulation results compare the performance between a single mobile sensor with various amounts of static sensors.
ER  - 

TY  - CONF
TI  - Collision-Free Motion Planning for Human-Robot Collaborative Safety Under Cartesian Constraint
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4348
EP  - 4354
AU  - J. Chen
AU  - K. Song
PY  - 2018
KW  - collision avoidance
KW  - control system synthesis
KW  - end effectors
KW  - human-robot interaction
KW  - image segmentation
KW  - Kalman filters
KW  - nearest neighbour methods
KW  - robot vision
KW  - human-robot collaborative safety
KW  - Cartesian constraint
KW  - real-time motion planning
KW  - control design
KW  - robotic arm
KW  - multiple KinectV2 depth cameras
KW  - robot workspace
KW  - collision avoidance
KW  - robot end effector
KW  - collision-free motion planning method
KW  - 6-DOF robot arm
KW  - Kalman filter
KW  - K-nearest neighbor searching algorithm
KW  - K-nearest neighbor searching algorithm
KW  - Robots
KW  - Collision avoidance
KW  - Force
KW  - Planning
KW  - Three-dimensional displays
KW  - Collaboration
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8460185
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a real-time motion planning and control design of a robotic arm for human-robot collaborative safety. A novel collision-free motion planning method is proposed not only to keep robot body from colliding with objects but also preserve the execution of robot's original task under the Cartesian constraint of the environment. Multiple KinectV2 depth cameras are utilized to model and track dynamic obstacles (e.g. Humans and objects) inside the robot workspace. Depth images are applied to generate point cloud of segmented objects in the environment. A K-nearest neighbor (KNN) searching algorithm is used to cluster and find the closest point from the obstacle to the robot. Then a Kalman filter is applied to estimate the obstacle position and velocity. For the collision avoidance in collaborative operation, attractive and repulsive potential is generated for robot end effector based on the task specification and obstacle observation. Practical experiments show that the 6-DOF robot arm can effectively avoid an obstacle in a constrained environment and complete the original task.
ER  - 

TY  - CONF
TI  - Sampling-Based Motion Planning with μ-Calculus Specifications Without Steering
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4355
EP  - 4360
AU  - L. Larocque
AU  - J. Liu
PY  - 2018
KW  - boundary-value problems
KW  - feedback
KW  - formal verification
KW  - linear quadratic control
KW  - motion control
KW  - path planning
KW  - probability
KW  - sampling methods
KW  - temporal logic
KW  - sampling-based motion planning
KW  - temporal logic specifications
KW  - linear dynamics
KW  - two-point boundary value problem
KW  - asymptotically optimal planning algorithm SST
KW  - local deterministic μ-calculus model
KW  - motion planning algorithm
KW  - deterministic μ-calculus specifications
KW  - multiple Kripke structures
KW  - abstracted Kripke structure
KW  - state-space
KW  - linear-quadratic regulator feedback control policy
KW  - complex liveness specification
KW  - steering function
KW  - kinodynamic planning algorithm SST
KW  - LQR feedback control policy
KW  - Calculus
KW  - Planning
KW  - Model checking
KW  - Trajectory
KW  - Reactive power
KW  - Lattices
DO  - 10.1109/ICRA.2018.8460769
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - While using temporal logic specifications with motion planning has been heavily researched, the reliance on having an available steering function is impractical and often suited only to basic problems with linear dynamics. This is because a steering function is a solution to an optimal two-point boundary value problem (OBVP); to our knowledge, it is nearly impossible to find an analytic solution to such problems in many cases. Addressing this issue, we have developed a means of combining the asymptotically optimal and probabilistically complete kinodynamic planning algorithm SST* with a local deterministic μ-calculus model checking procedure to create a motion planning algorithm with deterministic μ-calculus specifications that does not rely on a steering function. The procedure involves combining only the most pertinent information from multiple Kripke structures in order to create one abstracted Kripke structure storing the best paths to all possible proposition regions of the state-space. A linear-quadratic regulator (LQR) feedback control policy is then used to track these best paths, effectively connecting the trajectories found from multiple Kripke structures. Simulations demonstrate that it is possible to satisfy a complex liveness specification for infinitely often reaching specified regions of state-space using only forward propagation.
ER  - 

TY  - CONF
TI  - Generating Vibration Free Rest-to-Rest Trajectories for Configuration Dependent Dynamic Systems via 3-Segmented Input Shaping
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4361
EP  - 4366
AU  - D. K. Thomsen
AU  - R. S. Knudsen
AU  - D. Brandt
AU  - O. Balling
AU  - X. Zhang
PY  - 2018
KW  - bang-bang control
KW  - path planning
KW  - position control
KW  - configuration dependent dynamics
KW  - vibration-free RTR trajectory generation
KW  - bang-coast-bang trajectory
KW  - system dynamics
KW  - piece wise shaping
KW  - trajectory segmentation strategy
KW  - 3-Segmented Input Shaping
KW  - configuration dependent dynamic systems
KW  - free rest-to-rest trajectories
KW  - Trajectory
KW  - Vibrations
KW  - Acceleration
KW  - Motion segmentation
KW  - System dynamics
KW  - Numerical simulation
DO  - 10.1109/ICRA.2018.8460865
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a new method to generate vibration free rest-to-rest (RTR) trajectories for configuration dependent dynamic systems, such as robots, cranes or machine tools. The new method named 3-Segmented Input Shaping is based on a combination of the widely known Input Shaping method and a new trajectory segmentation strategy for piece wise shaping of the trajectory. The new segmentation strategy facilitates the capability of accounting for variations in system dynamics during motion by shaping acceleration and deceleration profiles with individual frequencies. In this paper the new segmentation strategy is used in combination with the bang-coast-bang (BCB) trajectory. The generated trajectories are described in closed form, hence requires no optimization and thereby provides strong computational performance. The new method is verified by numerical simulations and detailed analysis and shows great potential in vibration-free RTR trajectory generation for systems with configuration dependent dynamics.
ER  - 

TY  - CONF
TI  - A Model-Based Hierarchical Controller for Legged Systems Subject to External Disturbances
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4375
EP  - 4382
AU  - G. Xin
AU  - H. Lin
AU  - J. Smith
AU  - O. Cebe
AU  - M. Mistry
PY  - 2018
KW  - control system synthesis
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - position control
KW  - robot dynamics
KW  - robot kinematics
KW  - robust control
KW  - model-based hierarchical controller
KW  - model-based controller
KW  - projected inverse dynamics controller
KW  - control law
KW  - constrained space controller
KW  - unknown external disturbances
KW  - impedance controller
KW  - legged systems
KW  - unconstrained component
KW  - contact forces
KW  - force sensors
KW  - torque sensors
KW  - ANYmal quadruped platform
KW  - contact locations
KW  - legged robots
KW  - Task analysis
KW  - Force
KW  - Legged locomotion
KW  - Aerospace electronics
KW  - Dynamics
KW  - Jacobian matrices
DO  - 10.1109/ICRA.2018.8461172
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Legged robots have many potential applications in real-world scenarios where the tasks are too dangerous for humans, and compliance is needed to protect the system against external disturbances and impacts. In this paper, we propose a model-based controller for hierarchical tasks of legged systems subject to external disturbance. The control framework is based on projected inverse dynamics controller, such that the control law is decomposed into two orthogonal subspaces, i.e., the constrained and the unconstrained subspaces. The unconstrained component controls multiple desired tasks with impedance responses. The constrained space controller maintains the contact subject to unknown external disturbances, without the use of any force/torque sensing at the contact points. By explicitly modelling the external force, our controller is robust to external disturbances and errors arising from incorrect dynamic model information. The main contributions of this paper include (1) incorporating an impedance controller to control external disturbances and allow impedance shaping to adjust the behaviour of the motion under external disturbances, (2) optimising contact forces within the constrained subspace that also takes into account the external disturbances without using force/torque sensors at the contact locations. The techniques are evaluated on the ANYmal quadruped platform under a variety of scenarios.
ER  - 

TY  - CONF
TI  - Fore-Aft Leg Specialization Controller for a Dynamic Quadruped
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4383
EP  - 4390
AU  - J. M. Brown
AU  - C. P. Carbiener
AU  - J. Nicholson
AU  - N. Hemenway
AU  - J. L. Pusey
AU  - J. E. Clark
PY  - 2018
KW  - legged locomotion
KW  - robot dynamics
KW  - trajectory control
KW  - fore-aft leg specialization controller
KW  - running animals
KW  - robotic counterparts
KW  - functional dynamic decomposition
KW  - Dynamic Quadruped
KW  - trajectory-based controller
KW  - Legged locomotion
KW  - Trajectory
KW  - Force
KW  - Springs
KW  - Vehicle dynamics
KW  - Robot kinematics
DO  - 10.1109/ICRA.2018.8460763
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Many running animals, unlike their robotic counterparts, have distinct morphologies and functional roles for their front and rear legs. In this paper we present a new control approach for a 5kg autonomous dynamic quadruped that explicitly encodes separate roles for each contralateral pair of legs. This controller utilizes a functional dynamic decomposition similar to Raibert's three part control law, but focuses on fore-aft leg specialization to regulate the robot's performance. The velocity of this controller, which exceeds 5 body lengths per sec, is compared with an improved trajectory-based controller and shown to be significantly more robust to changes in environment.
ER  - 

TY  - CONF
TI  - Contact Model Fusion for Event-Based Locomotion in Unstructured Terrains
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4399
EP  - 4406
AU  - G. Bledt
AU  - P. M. Wensing
AU  - S. Ingersoll
AU  - S. Kim
PY  - 2018
KW  - discrete time systems
KW  - finite state machines
KW  - force control
KW  - Kalman filters
KW  - legged locomotion
KW  - motion control
KW  - observers
KW  - robot dynamics
KW  - robot kinematics
KW  - robust control
KW  - contact detection
KW  - contact state estimation
KW  - MIT Cheetah 3 robot
KW  - dynamic modeling
KW  - kinematic
KW  - Event-Based Finite State Machine
KW  - Kalman Filtering
KW  - contact priors
KW  - proprioceptive force control estimates
KW  - generalized-momentum disturbance observer
KW  - discrete-time extension
KW  - contact initiation
KW  - terrain geometry
KW  - contact models
KW  - contact transitions
KW  - unstructured environments
KW  - legged robots
KW  - unstructured terrains
KW  - event-based locomotion
KW  - contact model fusion
KW  - time 4.0 ms to 5.0 ms
KW  - Legged locomotion
KW  - Robot sensing systems
KW  - Force
KW  - Disturbance observers
KW  - Robustness
DO  - 10.1109/ICRA.2018.8460904
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - As legged robots are sent into unstructured environments, the ability to robustly manage contact transitions will be a critical skill. This paper introduces an approach to probabilistically fuse contact models, managing uncertainty in terrain geometry, dynamic modeling, and kinematics to improve the robustness of contact initiation at touchdown. A discrete-time extension of the generalized-momentum disturbance observer is presented to increase the accuracy of proprioceptive force control estimates. This information is fused with other contact priors under a framework of Kalman Filtering to increase robustness of the method. This approach results in accurate contact detection with 99.3 % accuracy and a small 4-5ms delay. Using this new detector, an Event-Based Finite State Machine is implemented to deal with unexpected early and late contacts. This allows the robot to traverse cluttered environments by modifying the control actions for each individual leg based on the estimated contact state rather than adhering to a rigid time schedule regardless of actual contact state. Experiments with the MIT Cheetah 3 robot show the success of both the detection algorithm, as well as the Event-Based FSM while making unexpected contacts during trotting.
ER  - 

TY  - CONF
TI  - Single-Image Footstep Prediction for Versatile Legged Locomotion
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4407
EP  - 4413
AU  - W. Zhang
AU  - K. Hauser
PY  - 2018
KW  - convolution
KW  - feedforward neural nets
KW  - image colour analysis
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - path planning
KW  - prediction theory
KW  - robot kinematics
KW  - sensors
KW  - versatile legged locomotion
KW  - robots
KW  - longterm routes
KW  - horizontal terrain
KW  - vertical terrain
KW  - onboard sensors
KW  - vantage points
KW  - strongly foreshortened images
KW  - terrain features
KW  - viewing angle
KW  - convolutional neural network method
KW  - arbitrary tilt angles
KW  - route planner
KW  - plausible plans
KW  - rock climbing gyms
KW  - walking robots
KW  - climbing robots
KW  - single image footstep prediction
KW  - distance angle
KW  - valid handhold prediction
KW  - foothold locations prediction
KW  - single RGB+D images
KW  - learning techniques
KW  - flat ground
KW  - stairs
KW  - walls
KW  - Cameras
KW  - Legged locomotion
KW  - Planning
KW  - Robot kinematics
KW  - Three-dimensional displays
KW  - Rocks
DO  - 10.1109/ICRA.2018.8460999
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Walking and climbing robots need to plan longterm routes on both horizontal and vertical terrain, but onboard sensors take images from vantage points that provide strongly foreshortened images that cause the appearance of terrain features to vary greatly by distance and viewing angle. This paper presents a convolutional neural network (CNN) method for predicting valid handhold and foothold locations from single RGB+D images taken at arbitrary tilt angles. Experiments show that the method predicts holds more accurately than comparable learning techniques, and that a route planner based on these predictions generates plausible plans for flat ground, stairs, and walls in rock climbing gyms.
ER  - 

TY  - CONF
TI  - Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4422
EP  - 4429
AU  - R. Hartley
AU  - J. Mangelson
AU  - L. Gan
AU  - M. Ghaffari Jadidi
AU  - J. M. Walls
AU  - R. M. Eustice
AU  - J. W. Grizzle
PY  - 2018
KW  - graph theory
KW  - legged locomotion
KW  - object tracking
KW  - optimisation
KW  - robot kinematics
KW  - robot vision
KW  - state estimation
KW  - robotic perception systems
KW  - robot state-estimation
KW  - Agility Robotics
KW  - Cassie-series robot
KW  - forward kinematic factor
KW  - factor graph framework
KW  - preintegrated contact factor
KW  - kinematic factors
KW  - legged robots
KW  - state-estimation technique
KW  - visual tracking
KW  - Robot sensing systems
KW  - Legged locomotion
KW  - Kinematics
KW  - Optimization
KW  - Foot
KW  - Cameras
DO  - 10.1109/ICRA.2018.8460748
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - State-of-the-art robotic perception systems have achieved sufficiently good performance using Inertial Measurement Units (IMUs), cameras, and nonlinear optimization techniques, that they are now being deployed as technologies. However, many of these methods rely significantly on vision and often fail when visual tracking is lost due to lighting or scarcity of features. This paper presents a state-estimation technique for legged robots that takes into account the robot's kinematic model as well as its contact with the environment. We introduce forward kinematic factors and preintegrated contact factors into a factor graph framework that can be incrementally solved in real-time. The forward kinematic factor relates the robot's base pose to a contact frame through noisy encoder measurements. The preintegrated contact factor provides odometry measurements of this contact frame while accounting for possible foot slippage. Together, the two developed factors constrain the graph optimization problem allowing the robot's trajectory to be estimated. The paper evaluates the method using simulated and real sensory IMU and kinematic data from experiments with a Cassie-series robot designed by Agility Robotics. These preliminary experiments show that using the proposed method in addition to IMU decreases drift and improves localization accuracy, suggesting that its use can enable successful recovery from a loss of visual tracking.
ER  - 

TY  - CONF
TI  - Learning to Parse Natural Language to Grounded Reward Functions with Weak Supervision
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4430
EP  - 4436
AU  - E. C. Williams
AU  - N. Gopalan
AU  - M. Rhee
AU  - S. Tellex
PY  - 2018
KW  - grammars
KW  - lambda calculus
KW  - learning (artificial intelligence)
KW  - natural language processing
KW  - robots
KW  - trees (mathematics)
KW  - parse weights
KW  - validation-driven perceptron weight updates
KW  - goal-condition learning approach
KW  - grounded reward functions
KW  - language representations
KW  - weighted linear Combinatory Categorial Grammar semantic parser
KW  - CCG lexicon
KW  - parse trees
KW  - robot behaviors
KW  - Cleanup World domain
KW  - natural language parsing
KW  - goal-state reward functions
KW  - lambda calculus
KW  - Natural languages
KW  - Semantics
KW  - Task analysis
KW  - Planning
KW  - Robots
KW  - Trajectory
KW  - Navigation
DO  - 10.1109/ICRA.2018.8460937
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In order to intuitively and efficiently collaborate with humans, robots must learn to complete tasks specified using natural language. We represent natural language instructions as goal-state reward functions specified using lambda calculus. Using reward functions as language representations allows robots to plan efficiently in stochastic environments. To map sentences to such reward functions, we learn a weighted linear Combinatory Categorial Grammar (CCG) semantic parser. The parser, including both parameters and the CCG lexicon, is learned from a validation procedure that does not require execution of a planner, annotating reward functions, or labeling parse trees, unlike prior approaches. To learn a CCG lexicon and parse weights, we use coarse lexical generation and validation-driven perceptron weight updates using the approach of Artzi and Zettlemoyer [4]. We present results on the Cleanup World domain [18] to demonstrate the potential of our approach. We report an F1 score of 0.82 on a collected corpus of 23 tasks containing combinations of nested referential expressions, comparators and object properties with 2037 corresponding sentences. Our goal-condition learning approach enables an improvement of orders of magnitude in computation time over a baseline that performs planning during learning, while achieving comparable results. Further, we conduct an experiment with just 6 labeled demonstrations to show the ease of teaching a robot behaviors using our method. We show that parsing models learned from small data sets can generalize to commands not seen during training.
ER  - 

TY  - CONF
TI  - Deep Haptic Model Predictive Control for Robot-Assisted Dressing
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4437
EP  - 4444
AU  - Z. Erickson
AU  - H. M. Clever
AU  - G. Turk
AU  - C. K. Liu
AU  - C. C. Kemp
PY  - 2018
KW  - assisted living
KW  - clothing
KW  - control engineering computing
KW  - end effectors
KW  - handicapped aids
KW  - haptic interfaces
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - optimal control
KW  - predictive control
KW  - service robots
KW  - physical human-robot interaction
KW  - people with disabilities
KW  - controller objective function
KW  - deep predictive model
KW  - prediction horizon
KW  - PR2 robot
KW  - physics-based simulation
KW  - dressing assistance
KW  - garment
KW  - deep recurrent model
KW  - nonrigid garments
KW  - physical implications
KW  - robot-assisted dressing
KW  - deep haptic model predictive control
KW  - Robot sensing systems
KW  - Haptic interfaces
KW  - End effectors
KW  - Clothing
KW  - Predictive models
DO  - 10.1109/ICRA.2018.8460656
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Robot-assisted dressing offers an opportunity to benefit the lives of many people with disabilities, such as some older adults. However, robots currently lack common sense about the physical implications of their actions on people. The physical implications of dressing are complicated by non-rigid garments, which can result in a robot indirectly applying high forces to a person's body. We present a deep recurrent model that, when given a proposed action by the robot, predicts the forces a garment will apply to a person's body. We also show that a robot can provide better dressing assistance by using this model with model predictive control. The predictions made by our model only use haptic and kinematic observations from the robot's end effector, which are readily attainable. Collecting training data from real world physical human-robot interaction can be time consuming, costly, and put people at risk. Instead, we train our predictive model using data collected in an entirely self-supervised fashion from a physics-based simulation. We evaluated our approach with a PR2 robot that attempted to pull a hospital gown onto the arms of 10 human participants. With a 0.2s prediction horizon, our controller succeeded at high rates and lowered applied force while navigating the garment around a persons fist and elbow without getting caught. Shorter prediction horizons resulted in significantly reduced performance with the sleeve catching on the participants' fists and elbows, demonstrating the value of our model's predictions. These behaviors of mitigating catches emerged from our deep predictive model and the controller objective function, which primarily penalizes high forces.
ER  - 

TY  - CONF
TI  - EmoRL: Continuous Acoustic Emotion Classification Using Deep Reinforcement Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4445
EP  - 4450
AU  - E. Lakomkin
AU  - M. A. Zamani
AU  - C. Weber
AU  - S. Magg
AU  - S. Wermter
PY  - 2018
KW  - acoustic signal processing
KW  - emotion recognition
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - EmoRL model
KW  - audio signal
KW  - continuous acoustic emotion classification
KW  - deep reinforcement learning
KW  - acoustically expressed emotions
KW  - deep neural network-based models
KW  - affective state evaluation
KW  - real-time communication scenario
KW  - human-robot interaction
KW  - Acoustics
KW  - Robots
KW  - Adaptation models
KW  - Logic gates
KW  - Feature extraction
KW  - Predictive models
KW  - Recurrent neural networks
DO  - 10.1109/ICRA.2018.8461058
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Acoustically expressed emotions can make communication with a robot more efficient. Detecting emotions like anger could provide a clue for the robot indicating unsafe/undesired situations. Recently, several deep neural network-based models have been proposed which establish new state-of-the-art results in affective state evaluation. These models typically start processing at the end of each utterance, which not only requires a mechanism to detect the end of an utterance but also makes it difficult to use them in a real-time communication scenario, e.g. human-robot interaction. We propose the EmoRL model that triggers an emotion classification as soon as it gains enough confidence while listening to a person speaking. As a result, we minimize the need for segmenting the audio signal for classification and achieve lower latency as the audio signal is processed incrementally. The method is competitive with the accuracy of a strong baseline model, while allowing much earlier prediction.
ER  - 

TY  - CONF
TI  - Temporal Spatial Inverse Semantics for Robots Communicating with Humans
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4451
EP  - 4458
AU  - Z. Gong
AU  - Y. Zhang
PY  - 2018
KW  - control engineering computing
KW  - human-robot interaction
KW  - natural language processing
KW  - Amazon MTurk
KW  - TeSIS
KW  - natural language sentences
KW  - extended sentence structure
KW  - spatial context information
KW  - human listeners
KW  - temporal spatial inverse semantics
KW  - temporal context
KW  - Semantics
KW  - Grounding
KW  - Pallets
KW  - Natural languages
KW  - Tires
KW  - Service robots
DO  - 10.1109/ICRA.2018.8460754
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Effective communication between humans often embeds both temporal and spatial context. While spatial context captures the geographic settings of objects in the environment, temporal context describes their changes over time. In this paper, we propose temporal spatial inverse semantics (TeSIS) to extend the inverse semantics approach to also consider the temporal context for robots communicating with humans. Inverse semantics generates natural language requests while taking into account how well the human listeners would interpret those requests given the current spatial context. Compared to inverse semantics, our approach incorporates also temporal context by referring to spatial context information in the past. To achieve this, we extend the sentence structure in inverse semantics to generate sentences that can refer to not only the current but also previous states of the environment. A new metric based on the extended sentence structure is developed by breaking a single sentence into multiple independent sentences that refer to environment states at different times. Using this approach, we are able to generate sentences such as “Please pick up the cup beside the oven that was on the dining table”. To evaluate our approach, we randomly generate scenarios in an experimental domain. Each scenario includes the description of the current and several immediate previous states. Natural language sentences are then generated for these scenarios using both inverse semantics that uses only the spatial context and our approach. Amazon MTurk is used to compare the sentences generated and results show that TeSIS achieves better accuracy, sometimes by a significant margin, than the baseline.
ER  - 

TY  - CONF
TI  - Brain-Computer Interface Meets ROS: A Robotic Approach to Mentally Drive Telepresence Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4459
EP  - 4464
AU  - G. Beraldo
AU  - M. Antonello
AU  - A. Cimolato
AU  - E. Menegatti
AU  - L. Tonin
PY  - 2018
KW  - brain
KW  - brain-computer interfaces
KW  - collision avoidance
KW  - control engineering computing
KW  - geriatrics
KW  - handicapped aids
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - medical signal processing
KW  - mobile robots
KW  - operating systems (computers)
KW  - patient rehabilitation
KW  - position control
KW  - robot programming
KW  - telerobotics
KW  - video streaming
KW  - noninvasive Brain-Computer Interface
KW  - Robot Operating System
KW  - telepresence robot
KW  - mobile device
KW  - human brain signals
KW  - severe physical disabilities
KW  - elderly people
KW  - BCI user
KW  - robot position control
KW  - obstacle avoidance
KW  - video streaming
KW  - Navigation
KW  - Robot sensing systems
KW  - Task analysis
KW  - Telepresence
KW  - Brain-computer interfaces
DO  - 10.1109/ICRA.2018.8460578
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper shows and evaluates a novel approach to integrate a non-invasive Brain-Computer Interface (BCI) with the Robot Operating System (ROS) to mentally drive a telepresence robot. Controlling a mobile device by using human brain signals might improve the quality of life of people suffering from severe physical disabilities or elderly people who cannot move anymore. Thus, the BCI user can actively interact with relatives and friends located in different rooms thanks to a video streaming connection to the robot. To facilitate the control of the robot via BCI, we explore new ROS-based algorithms for navigation and obstacle avoidance in order to make the system safer and more reliable. In this regard, the robot exploits two maps of the environment, one for localization and one for navigation, and both are used as additional visual feedback for the BCI user to control the robot position. Experimental results show a decrease of the number of commands needed to complete the navigation task, suggesting a reduction user's cognitive workload. The novelty of this work is to provide a first evidence of an integration between BCI and ROS that can simplify and foster the development of software for BCI driven robotics devices.
ER  - 

TY  - CONF
TI  - FaNeuRobot: A Framework for Robot and Prosthetics Control Using the NeuCube Spiking Neural Network Architecture and Finite Automata Theory
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4465
EP  - 4472
AU  - K. Kumarasinghe
AU  - M. Owen
AU  - D. Taylor
AU  - N. Kasabov
AU  - C. Kit
PY  - 2018
KW  - brain
KW  - brain-computer interfaces
KW  - electroencephalography
KW  - finite automata
KW  - handicapped aids
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - medical signal processing
KW  - muscle
KW  - neural nets
KW  - neurophysiology
KW  - brain-machine interface
KW  - central nervous system
KW  - biomedical signal
KW  - finite automata theory
KW  - NeuCube evolving spiking neural network architecture
KW  - robust prosthetic control
KW  - anthropomorphic mechanical design
KW  - noninvasive BMI
KW  - muscle atrophy
KW  - motor control framework
KW  - anthropomorphic design
KW  - prosthetic limbs
KW  - limb amputation
KW  - prosthetics control
KW  - Prosthetics
KW  - Muscles
KW  - Electroencephalography
KW  - DC motors
KW  - Grasping
KW  - Bones
KW  - Thumb
DO  - 10.1109/ICRA.2018.8460197
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Limb amputation is a global problem. Prosthetic limbs can enhance the quality of life of amputees. To this end, anthropomorphic design and intuitive manipulation are two essential requirements. This paper presents a motor control framework for prosthetic control through Brain-Machine Interface (BMI) using Finite Automata Theory, and NeuCube Evolving Spiking Neural Network (SNN) architecture. Voluntary control of prosthetics requires decoding motor commands from the Central Nervous System of the amputee. Selection of the most suitable biomedical signal depends on many parameters such as level of amputation and muscle atrophy. Non-invasive BMI's have the possibility of supporting a wider range of amputees as it extracts the motor commands from the brain. In this paper, we present a proof of concept study on whether a cognitive computational model that is inspired by the motor control of the human body through muscle synergies combined with an anthropomorphic mechanical design, can result in accurate and robust prosthetic control through a noninvasive BMI. In future, learning of a complex Finite Automata that reflects complex upper limb motor behaviours will be investigated.
ER  - 

TY  - CONF
TI  - Human in the Loop of Robot Learning: EEG-Based Reward Signal for Target Identification and Reaching Task
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4473
EP  - 4480
AU  - L. Schiatti
AU  - J. Tessadori
AU  - N. Deshpande
AU  - G. Barresi
AU  - L. C. King
AU  - L. S. Mattos
PY  - 2018
KW  - brain
KW  - control engineering computing
KW  - electroencephalography
KW  - human-robot interaction
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - robot programming
KW  - target reaching task
KW  - assistive technologies
KW  - shared control
KW  - intelligent robotic device
KW  - electrophysiological measures
KW  - error detection
KW  - Error-related Potentials
KW  - semiautonomous system
KW  - online robot learning task
KW  - detected ErrP
KW  - robot learning loop
KW  - optimal policy learning
KW  - shared autonomy
KW  - reinforcement learning framework
KW  - Electroencephalography
KW  - Training
KW  - Graphical user interfaces
KW  - Microsoft Windows
KW  - Testing
KW  - Robot learning
DO  - 10.1109/ICRA.2018.8460551
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Shared control and shared autonomy play an important role in assistive technologies, allowing the offloading of the cognitive burden required for control from the user to the intelligent robotic device. In this context, electrophysiological measures of error detection, directly measured from a person's brain activity as Error-related Potentials (ErrPs), can be exploited to provide passive adaptation of an external semi-autonomous system to the human. This concept was implemented in an online robot learning task, where user's evaluation of the robot's actions, in terms of detected ErrP, was exploited to update a reward function in a Reinforcement Learning (RL) framework. Results from both simulated and experimental studies show that the introduction of human evaluation in the robot learning loop allows for: (1) the acceleration of optimal policy learning in a target reaching task, (2) the introduction of a further degree of control in robot learning, namely identification of one among multiple targets, according to the user's will. Overall, presented results support the potential of human-robot co-adaptive and co-operative strategies to develop human-centered assistive technologies.
ER  - 

TY  - CONF
TI  - Incremental Adversarial Domain Adaptation for Continually Changing Environments
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4489
EP  - 4495
AU  - M. Wulfmeier
AU  - A. Bewley
AU  - I. Posner
PY  - 2018
KW  - feature extraction
KW  - image segmentation
KW  - unsupervised learning
KW  - machine learning models
KW  - robotics applications
KW  - alignment step
KW  - feature distribution
KW  - GAN training
KW  - continuous appearance shifts
KW  - continually changing environments
KW  - incremental adversarial domain adaptation
KW  - generative adversarial network
KW  - traversable-path segmentation task
KW  - unsupervised domain adaptation
KW  - Training
KW  - Task analysis
KW  - Adaptation models
KW  - Robots
KW  - Gallium nitride
KW  - Mathematical model
KW  - Lighting
DO  - 10.1109/ICRA.2018.8460982
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.
ER  - 

TY  - CONF
TI  - DeepVP: Deep Learning for Vanishing Point Detection on 1 Million Street View Images
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4496
EP  - 4503
AU  - C. Chang
AU  - J. Zhao
AU  - L. Itti
PY  - 2018
KW  - cameras
KW  - convolution
KW  - edge detection
KW  - feedforward neural nets
KW  - image classification
KW  - learning (artificial intelligence)
KW  - object detection
KW  - algorithmic vanishing point detector
KW  - DeepVP
KW  - Google street view image dataset
KW  - camera parameters
KW  - deep learning
KW  - deep vanishing point system
KW  - CNN classification problem
KW  - inferred ground-truth vanishing points
KW  - convolutional neural network
KW  - vanishing point detection
KW  - Roads
KW  - Cameras
KW  - Google
KW  - Machine learning
KW  - Videos
KW  - Image segmentation
KW  - Computer architecture
DO  - 10.1109/ICRA.2018.8460499
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We propose a novel approach to detect vanishing points in images using a convolutional neural network (CNN) trained on a newly collected Google street-view image dataset. By utilizing the camera parameters and road direction data from Google street view, we collected a total of 1,053,425 images with inferred ground-truth vanishing points, along 23 worldwide routes totaling 125,165 kilometers. We then formulate vanishing point detection as a CNN classification problem using an output layer with 225 discrete possible vanishing point locations. Experimental results show that our deep vanishing point system outperforms the state-of-the-art algorithmic vanishing point detector. We achieved 99% accuracy in recovering the horizon line and 92% in locating the vanishing point within a ±5-degree range.
ER  - 

TY  - CONF
TI  - Deep Lidar CNN to Understand the Dynamics of Moving Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4504
EP  - 4509
AU  - V. Vaquero
AU  - A. Sanfeliu
AU  - F. Moreno-Noguer
PY  - 2018
KW  - image colour analysis
KW  - image sensors
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - optical radar
KW  - semantic networks
KW  - pretext tasks
KW  - test time
KW  - including distilled image information
KW  - standard image-based optical flow
KW  - novel lidar-flow feature
KW  - semantic information
KW  - image data
KW  - consecutive lidar scans
KW  - testing time
KW  - CNN architecture
KW  - external observed vehicles
KW  - observer vehicle
KW  - proprio-motion
KW  - autonomous cars
KW  - Deep Learning solutions
KW  - RGB images
KW  - semantically rich information
KW  - Autonomous Driving
KW  - perception technologies
KW  - Deep lidar CNN
KW  - Laser radar
KW  - Task analysis
KW  - Vehicle dynamics
KW  - Three-dimensional displays
KW  - Dynamics
KW  - Machine learning
KW  - Semantics
DO  - 10.1109/ICRA.2018.8460554
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Perception technologies in Autonomous Driving are experiencing their golden age due to the advances in Deep Learning. Yet, most of these systems rely on the semantically rich information of RGB images. Deep Learning solutions applied to the data of other sensors typically mounted on autonomous cars (e.g. lidars or radars) are not explored much. In this paper we propose a novel solution to understand the dynamics of moving vehicles of the scene from only lidar information. The main challenge of this problem stems from the fact that we need to disambiguate the proprio-motion of the “observer” vehicle from that of the external “observed” vehicles. For this purpose, we devise a CNN architecture which at testing time is fed with pairs of consecutive lidar scans. However, in order to properly learn the parameters of this network, during training we introduce a series of so-called pretext tasks which also leverage on image data. These tasks include semantic information about vehicleness and a novel lidar-flow feature which combines standard image-based optical flow with lidar scans. We obtain very promising results and show that including distilled image information only during training, allows improving the inference results of the network at test time, even when image data is no longer used.
ER  - 

TY  - CONF
TI  - Optimization Beyond the Convolution: Generalizing Spatial Relations with End-to-End Metric Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4510
EP  - 4516
AU  - P. Jund
AU  - A. Eitel
AU  - N. Abdo
AU  - W. Burgard
PY  - 2018
KW  - convolution
KW  - distance learning
KW  - feedforward neural nets
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - pose estimation
KW  - solid modelling
KW  - robots
KW  - arbitrary spatial relations
KW  - sizes
KW  - shapes
KW  - distance metric learning
KW  - 3D point clouds
KW  - metric space
KW  - object poses
KW  - arbitrary target relation
KW  - domestic environments
KW  - convolution
KW  - gradient based optimization
KW  - neural network
KW  - geometric models
KW  - continuous spectrum
KW  - end to end metric learning
KW  - Measurement
KW  - Three-dimensional displays
KW  - Robots
KW  - Optimization
KW  - Transforms
KW  - Shape
KW  - Convolution
DO  - 10.1109/ICRA.2018.8460220
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - To operate intelligently in domestic environments, robots require the ability to understand arbitrary spatial relations between objects and to generalize them to objects of varying sizes and shapes. In this work, we present a novel end-to-end approach to generalize spatial relations based on distance metric learning. We train a neural network to transform 3D point clouds of objects to a metric space that captures the similarity of the depicted spatial relations, using only geometric models of the objects. Our approach employs gradient-based optimization to compute object poses in order to imitate an arbitrary target relation by reducing the distance to it under the learned metric. Our results based on simulated and real-world experiments show that the proposed method enables robots to generalize spatial relations to unknown objects over a continuous spectrum.
ER  - 

TY  - CONF
TI  - Constructing Category-Specific Models for Monocular Object-SLAM
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4517
EP  - 4524
AU  - P. Parkhiya
AU  - R. Khawad
AU  - J. K. Murthy
AU  - B. Bhowmick
AU  - K. M. Krishna
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - mobile robots
KW  - object detection
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - category-specific models
KW  - real-time object-oriented SLAM
KW  - monocular camera
KW  - object-level models
KW  - category-level models
KW  - object deformations
KW  - discriminative object features
KW  - category models
KW  - object landmark observations
KW  - generic monocular SLAM framework
KW  - 2D object features
KW  - sparse feature-based monocular SLAM
KW  - object instance retrieval
KW  - instance-independent monocular object-SLAM system
KW  - feature-based SLAM methods
KW  - time 2.0 d
KW  - time 3.0 d
KW  - Solid modeling
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Object oriented modeling
KW  - Pipelines
KW  - Two dimensional displays
KW  - Shape
DO  - 10.1109/ICRA.2018.8460816
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fty, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show - to the best of our knowledge - first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.
ER  - 

TY  - CONF
TI  - DPDB-Net: Exploiting Dense Connections for Convolutional Encoders
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4525
EP  - 4531
AU  - G. L. Oliveira
AU  - W. Burgard
AU  - T. Brox
PY  - 2018
KW  - convolutional codes
KW  - decoding
KW  - image classification
KW  - image coding
KW  - image segmentation
KW  - neural net architecture
KW  - dense connections
KW  - Cam Vid dataset
KW  - Freiburg Forest dataset
KW  - convolutional encoders
KW  - multiple segmentation tasks
KW  - feature map explosion
KW  - residual network architecture
KW  - dense block
KW  - DPDB-Net
KW  - Dual-Path Dense-Block Network
KW  - encoder-decoder architectures
KW  - feature re-usage
KW  - dense networks
KW  - multiple classification tasks
KW  - feature exploration
KW  - densely connected networks
KW  - Decoding
KW  - Computer architecture
KW  - Semantics
KW  - Task analysis
KW  - Explosions
KW  - Image segmentation
KW  - Forestry
DO  - 10.1109/ICRA.2018.8461089
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Densely connected networks for classification enable feature exploration and result in state-of-the-art performance on multiple classification tasks. The alternative to dense networks is the residual network which enables feature re-usage. In this work, we combine these orthogonal concepts for encoder-decoder architectures, which we call Dual-Path Dense-Block Network (DPDB-Net). We introduce a dense block which incorporates feature re-usage and new feature exploration in the encoder. Moreover, we discuss that feature re-usage by the residual network architecture leads to a feature map explosion in the decoder and, thus, is not advantageous in this part of the network. We evaluated our proposed architecture in multiple segmentation tasks and report state-of-the-art performance on the Freiburg Forest dataset and competitive results on the Cam Vid dataset.
ER  - 

TY  - CONF
TI  - The Hands-Free Push-Cart: Autonomous Following in Front by Predicting User Trajectory Around Obstacles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4548
EP  - 4554
AU  - P. Nikdel
AU  - R. Shrestha
AU  - R. Vaughan
PY  - 2018
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - object detection
KW  - trajectory control
KW  - autonomous mobile robot
KW  - walking user
KW  - autonomous push-carts
KW  - multimodal person detection
KW  - human-motion model
KW  - obstacle mapper
KW  - human tracker
KW  - human motion model
KW  - robot motion planner
KW  - robot motion controller
KW  - industrial entertainment applications
KW  - domestic entertainment applications
KW  - hands-free push-cart
KW  - predicting user trajectory
KW  - Robot sensing systems
KW  - Legged locomotion
KW  - Cameras
KW  - Tracking
KW  - Trajectory
KW  - Predictive models
DO  - 10.1109/ICRA.2018.8461181
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper demonstrates an autonomous mobile robot that follows a walking user while staying ahead of them. Despite several useful applications for autonomous push-carts, this problem has received much less attention than the easier problem of following from behind. In contrast to previous work, we use multi-modal person detection and a human-motion model that considers obstacles to predict the future path of the user. We implement the system with a modular architecture of obstacle mapper, human tracker, human motion model, robot motion planner and robot motion controller. We report on the performance of the robot in real-world experiments. We believe that approaches to this largely overlooked problem could be useful in real industrial, domestic and entertainment applications in the near future.
ER  - 

TY  - CONF
TI  - Socially Constrained Tracking in Crowded Environments Using Shoulder Pose Estimates
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4555
EP  - 4562
AU  - A. Virgona
AU  - A. Alempijevic
AU  - T. Vidal-Calleja
PY  - 2018
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - robot vision
KW  - inner city train station
KW  - robotic technologies
KW  - 2D pose
KW  - motion capture system
KW  - person tracking framework
KW  - human environments
KW  - shoulder pose estimates
KW  - crowded environments
KW  - pose errors
KW  - lab environment
KW  - Target tracking
KW  - Sensors
KW  - Cameras
KW  - Robustness
KW  - Task analysis
KW  - Head
DO  - 10.1109/ICRA.2018.8461030
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Detecting and tracking people is a key requirement in the development of robotic technologies intended to operate in human environments. In crowded environments such as train stations this task is particularly challenging due the high numbers of targets and frequent occlusions. In this paper we present a framework for detecting and tracking humans in such crowded environments in terms of 2D pose ( x, y, θ). The main contributions are a method for extracting pose from the most visible parts of the body in a crowd, the head and shoulders, and a tracker which leverages social constraints regarding peoples orientation, movement and proximity to one another, to improve robustness in this challenging environment. The framework is evaluated on two datasets: one captured in a lab environment with ground truth obtained using a motion capture system, and the other captured in a busy inner city train station. Pose errors are reported against the ground truth and the tracking results are then compared with a state-of-the-art person tracking framework.
ER  - 

TY  - CONF
TI  - Anticipating Many Futures: Online Human Motion Prediction and Generation for Human-Robot Interaction
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4563
EP  - 4570
AU  - J. Bütepage
AU  - H. Kjellström
AU  - D. Kragic
PY  - 2018
KW  - human-robot interaction
KW  - image coding
KW  - image colour analysis
KW  - image motion analysis
KW  - image representation
KW  - probability
KW  - robot vision
KW  - motion patterns
KW  - kinematic cues
KW  - natural human motion
KW  - human-robot interaction
KW  - online human motion prediction
KW  - target prediction
KW  - RGB depth images
KW  - skeletal data
KW  - conditional variational autoencoder
KW  - time 300.0 ms to 500.0 ms
KW  - Trajectory
KW  - Task analysis
KW  - Robot kinematics
KW  - Predictive models
KW  - Computational modeling
KW  - Training data
DO  - 10.1109/ICRA.2018.8460651
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Fluent and safe interactions of humans and robots require both partners to anticipate the others' actions. The bottleneck of most methods is the lack of an accurate model of natural human motion. In this work, we present a conditional variational autoencoder that is trained to predict a window of future human motion given a window of past frames. Using skeletal data obtained from RGB depth images, we show how this unsupervised approach can be used for online motion prediction for up to 1660 ms. Additionally, we demonstrate online target prediction within the first 300-500 ms after motion onset without the use of target specific training data. The advantage of our probabilistic approach is the possibility to draw samples of possible future motion patterns. Finally, we investigate how movements and kinematic cues are represented on the learned low dimensional manifold.
ER  - 

TY  - CONF
TI  - Joint Long-Term Prediction of Human Motion Using a Planning-Based Social Force Approach
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4571
EP  - 4577
AU  - A. Rudenko
AU  - L. Palmieri
AU  - K. O. Arras
PY  - 2018
KW  - collision avoidance
KW  - Markov processes
KW  - mobile robots
KW  - motion control
KW  - multi-agent systems
KW  - multi-robot systems
KW  - random processes
KW  - stochastic processes
KW  - motion trajectories
KW  - planning-based approach
KW  - dynamic objects
KW  - planning-based social force approach
KW  - joint long-term prediction
KW  - individual agent velocities
KW  - social forces
KW  - weighted random walk algorithm
KW  - stochastic motion policies
KW  - long-term predictions
KW  - multiple agents
KW  - joint motion
KW  - local interactions
KW  - long-term human motion prediction
KW  - dynamic environments
KW  - intelligent vehicles
KW  - mobile robots
KW  - Trajectory
KW  - Prediction algorithms
KW  - Force
KW  - Predictive models
KW  - Planning
KW  - Stochastic processes
KW  - Robots
DO  - 10.1109/ICRA.2018.8460527
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The ability to perceive and predict future positions of dynamic objects is essential for mobile robots and intelligent vehicles in dynamic environments. In this paper, we present a novel planning-based approach for long-term human motion prediction that accounts for local interactions and can accurately predict joint motion of multiple agents. Long-term predictions are handled using an MDP formulation that computes a set of stochastic motion policies. To obtain distributions over future motion trajectories, we sample the policies with a weighted random walk algorithm in which each person is locally influenced by social forces from other nearby agents. Unlike related work, the algorithm is environment-aware, can account for individual agent velocities, requires no training phase and makes joint predictions for multiple agents. Experiments in simulation and with real data show that our method makes more accurate predictions than two state-of-the-art methods in terms of probabilistic and geometrical performance measures.
ER  - 

TY  - CONF
TI  - Negotiating with a Robot: Analysis of Regulatory Focus Behavior
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4578
EP  - 4594
AU  - A. Cruz-Maya
AU  - A. Tapus
PY  - 2018
KW  - control engineering computing
KW  - geriatrics
KW  - man-machine systems
KW  - psychology
KW  - robots
KW  - user interfaces
KW  - persuasive communication skills
KW  - social psychology theory
KW  - promotion behavior
KW  - prevention behavior
KW  - neutral behavior
KW  - companion robots
KW  - caregivers
KW  - elderly people
KW  - decisions taking
KW  - regulatory focus behavior
KW  - prevention focus
KW  - body gestures
KW  - negotiation scenario
KW  - Robots
KW  - Senior citizens
KW  - Psychology
KW  - Games
KW  - Speech recognition
KW  - Tracking
KW  - Human-robot interaction
DO  - 10.1109/ICRA.2018.8460611
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Companion robots are more and more taking the role of caregivers for elderly people. Elderly people sometimes take the advice given by their family members or caregivers as a criticism. In this context, persuasive communication skills could be helpful. A social psychology theory called Regulatory Focus states that people have one of two inclinations when taking decisions: Promotion or Prevention Focus. Also, based on these inclinations, people can be influenced by the way the message is sent, including the speed of the speech and the amplitude of body gestures. In this paper, we analyze the influence of Regulatory Focus on a negotiation scenario, using 3 conditions: (1) a robot with a promotion behavior, (2) a robot with a prevention behavior, and (3) a robot with a neutral behavior. Our results support the results found in the psychology literature related to Regulatory Focus, suggesting that Promotion participants were more influenced by the robot showing a Promotion based behavior. Moreover, Prevention participants were more relaxed on the condition with the robot showing a Prevention based behavior, and accepted the biggest concession between the initial and final offer.
ER  - 

TY  - CONF
TI  - Multi3: Multi-Sensory Perception System for Multi-Modal Child Interaction with Multiple Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4585
EP  - 4592
AU  - A. Tsiami
AU  - P. Koutras
AU  - N. Efthymiou
AU  - P. P. Filntisis
AU  - G. Potamianos
AU  - P. Maragos
PY  - 2018
KW  - computer games
KW  - control engineering computing
KW  - educational robots
KW  - gesture recognition
KW  - human-robot interaction
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - speech recognition
KW  - robotic platforms
KW  - child-robot interaction scenarios
KW  - Multi3
KW  - robotic sensory
KW  - perception capabilities
KW  - speech recognition modules
KW  - gesture recognition modules
KW  - modular multirobot architecture
KW  - action recognition modules
KW  - indoors interaction scenarios
KW  - child-robot interaction scene
KW  - multiple Kinect-based system
KW  - multiple robots
KW  - Multimodal child interaction
KW  - Multisensory perception system
KW  - Speech recognition
KW  - Trajectory
KW  - Robot sensing systems
KW  - Microphone arrays
DO  - 10.1109/ICRA.2018.8461210
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Child-robot interaction is an interdisciplinary research area that has been attracting growing interest, primarily focusing on edutainment applications. A crucial factor to the successful deployment and wide adoption of such applications remains the robust perception of the child's multi-modal actions, when interacting with the robot in a natural and untethered fashion. Since robotic sensory and perception capabilities are platform-dependent and most often rather limited, we propose a multiple Kinect-based system to perceive the child-robot interaction scene that is robot-independent and suitable for indoors interaction scenarios. The audio-visual input from the Kinect sensors is fed into speech, gesture, and action recognition modules, appropriately developed in this paper to address the challenging nature of child-robot interaction. For this purpose, data from multiple children are collected and used for module training or adaptation. Further, information from the multiple sensors is fused to enhance module performance. The perception system is integrated in a modular multi-robot architecture demonstrating its flexibility and scalability with different robotic platforms. The whole system, called Multi3, is evaluated, both objectively at the module level and subjectively in its entirety, under appropriate child-robot interaction scenarios containing several carefully designed games between children and robots.
ER  - 

TY  - CONF
TI  - Multi-Robot Coordination in Dynamic Environments Shared with Humans
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4593
EP  - 4600
AU  - Z. Talebpour
AU  - A. Martinoli
PY  - 2018
KW  - human-robot interaction
KW  - mobile robots
KW  - multi-robot systems
KW  - navigation
KW  - path planning
KW  - market-based framework
KW  - coordination mechanism
KW  - social costs
KW  - bid evaluations
KW  - realistic environment
KW  - human-aware navigation
KW  - human-agnostic planning
KW  - social constraints
KW  - multirobot coordination
KW  - dynamic environments
KW  - social human-populated environments
KW  - multirobot task allocation problem
KW  - static humans
KW  - moving humans
KW  - high-fidelity simulator
KW  - localization noise
KW  - static people
KW  - blocked passages
KW  - human-aware planning
KW  - human-agnostic navigation
KW  - robot experiments
KW  - MRTA metrics
KW  - Robot kinematics
KW  - Task analysis
KW  - Navigation
KW  - Planning
KW  - Resource management
KW  - Measurement
DO  - 10.1109/ICRA.2018.8460978
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This work addresses multi-robot coordination in social human-populated environments using a market-based framework for solving the Multi-Robot Task Allocation (MRTA) problem. Humans are considered in the proposed coordination mechanism by means of accounting for social costs in bid evaluations and requesting collaboration in socially blocking situations. Initially, the effect of a realistic environment with varying number of static/moving humans on the behavior and performance of our method is studied through an extensive suite of experiments in a high-fidelity simulator. Results show that the total traveled distance and time are increased when humans are present in the environments. Localization noise is also increased particularly in the case of static people. In the second series of experiments, a number of problematic cases resulting in longer modified paths, blocked passages, and long waits have been investigated. A comparative study targeting human-agnostic navigation and planning, human-aware navigation and human-agnostic planning, and human-aware navigation and planning has been conducted. Both simulated and real robot experiments confirm the effectiveness of accounting for humans at both team and individual levels. This leads to respecting social constraints as well as achieving a better performance based on MRTA metrics.
ER  - 

TY  - CONF
TI  - Social Attention: Modeling Attention in Human Crowds
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4601
EP  - 4607
AU  - A. Vemula
AU  - K. Muelling
AU  - J. Oh
PY  - 2018
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - publicly available crowd datasets
KW  - trained attention model
KW  - Social Attention
KW  - human crowds
KW  - robots
KW  - human predictable trajectories
KW  - human trajectory prediction
KW  - Trajectory
KW  - Navigation
KW  - Predictive models
KW  - Robots
KW  - Collision avoidance
KW  - Dynamics
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8460504
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Robots that navigate through human crowds need to be able to plan safe, efficient, and human predictable trajectories. This is a particularly challenging problem as it requires the robot to predict future human trajectories within a crowd where everyone implicitly cooperates with each other to avoid collisions. Previous approaches to human trajectory prediction have modeled the interactions between humans as a function of proximity. However, that is not necessarily true as some people in our immediate vicinity moving in the same direction might not be as important as other people that are further away, but that might collide with us in the future. In this work, we propose Social Attention, a novel trajectory prediction model that captures the relative importance of each person when navigating in the crowd, irrespective of their proximity. We demonstrate the performance of our method against a state-of-the-art approach on two publicly available crowd datasets and analyze the trained attention model to gain a better understanding of which surrounding agents humans attend to, when navigating in a crowd.
ER  - 

TY  - CONF
TI  - Fully Convolutional Neural Networks for Road Detection with Multiple Cues Integration
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4608
EP  - 4613
AU  - X. Han
AU  - J. Lu
AU  - C. Zhao
AU  - H. Li
PY  - 2018
KW  - convergence
KW  - convolution
KW  - feature extraction
KW  - feedforward neural nets
KW  - gradient methods
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optical radar
KW  - position control
KW  - convolutional neural networks
KW  - multiple cues integration
KW  - autonomous driving
KW  - deep learning
KW  - road detection algorithms
KW  - pre-trained Resnet-lOl
KW  - RGB images
KW  - CNN
KW  - feature maps extraction
KW  - Lidar scanner
KW  - position map
KW  - image gradient
KW  - convergence
KW  - KITTI benchmark
KW  - Roads
KW  - Feature extraction
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Fuses
KW  - Network architecture
DO  - 10.1109/ICRA.2018.8460663
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Road detection from images is a key task in autonomous driving. The recent advent of deep learning (and in particular, CNN or convolutional neural networks) has greatly improved the performance of road detection algorithms. In this paper, we show how to fuse multiple different cues under the same convolutional network framework. Specifically, we adopt a pre-trained Resnet-lOl to extract feature maps from RGB images; we then connect it with three extra deconvolution layers. These deconvolution layers is trained conditioning on appropriate image cues, and in our case they are a height image (i.e. elevation map obtained by e.g. Lidar scanner), image gradient, and position map. We also design two skip layers to speed up the convergence. Experiments on KITTI benchmark show competitive performance of our new networks.
ER  - 

TY  - CONF
TI  - A Visual-Inertial Approach to Human Gait Estimation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4614
EP  - 4621
AU  - A. Ahmed
AU  - S. Roumeliotis
PY  - 2018
KW  - cameras
KW  - gait analysis
KW  - image motion analysis
KW  - inertial navigation
KW  - pose estimation
KW  - head-mounted IMU-camera
KW  - batch least-squares algorithm
KW  - human motion models
KW  - inertial data
KW  - visual data
KW  - human gait estimation
KW  - minimal sensors-based system
KW  - VICON motion capture system
KW  - gait models
KW  - inertial measurement units
KW  - Foot
KW  - Trajectory
KW  - Computational modeling
KW  - Sensors
KW  - Legged locomotion
KW  - Visualization
KW  - Estimation
DO  - 10.1109/ICRA.2018.8460871
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper addresses the problem of gait estimation using visual and inertial data, as well as human motion models. Specifically, a batch least-squares (BLS) algorithm is presented that fuses data from a minimal set of sensors [two inertial measurement units (IMUs), one on each foot, and a head-mounted IMU-camera pair] along with motion constraints corresponding to the different walking states, to estimate the person's head and feet poses. Subsequently, gait models are employed to solve for the lower-body's posture and generate its animation. Experimental results against the VICON motion capture system demonstrate the accuracy of the proposed minimal sensors-based system for determining a person's motion.
ER  - 

TY  - CONF
TI  - A Deep Learning Based Behavioral Approach to Indoor Autonomous Navigation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4646
EP  - 4653
AU  - G. Sepulveda
AU  - J. C. Niebles
AU  - A. Soto
PY  - 2018
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigational behaviors
KW  - deep learning architectures
KW  - semantic abstraction
KW  - navigation tasks
KW  - navigational missions
KW  - behavioral approach
KW  - indoor autonomous navigation
KW  - semantically rich graph representation
KW  - indoor robotic navigation
KW  - semantic locations
KW  - Navigation
KW  - Semantics
KW  - Visualization
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Measurement
DO  - 10.1109/ICRA.2018.8460646
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a semantically rich graph representation for indoor robotic navigation. Our graph representation encodes: semantic locations such as offices or corridors as nodes, and navigational behaviors such as enter office or cross a corridor as edges. In particular, our navigational behaviors operate directly from visual inputs to produce motor controls and are implemented with deep learning architectures. This enables the robot to avoid explicit computation of its precise location or the geometry of the environment, and enables navigation at a higher level of semantic abstraction. We evaluate the effectiveness of our representation by simulating navigation tasks in a large number of virtual environments. Our results show that using a simple sets of perceptual and navigational behaviors, the proposed approach can successfully guide the way of the robot as it completes navigational missions such as going to a specific office. Furthermore, our implementation shows to be effective to control the selection and switching of behaviors.
ER  - 

TY  - CONF
TI  - Footstep Planning in Rough Terrain for Bipedal Robots Using Curved Contact Patches
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4662
EP  - 4669
AU  - D. Kanoulas
AU  - A. Stumpf
AU  - V. S. Raghavan
AU  - C. Zhou
AU  - A. Toumpa
AU  - O. Von Stryk
AU  - D. G. Caldwell
AU  - N. G. Tsagarakis
PY  - 2018
KW  - humanoid robots
KW  - legged locomotion
KW  - off-road vehicles
KW  - path planning
KW  - robot vision
KW  - rough terrain
KW  - rough terrain stepping
KW  - WALK-MAN humanoid robot
KW  - flat foothold contact analysis
KW  - rough local terrain surfaces
KW  - curved patch modeling system
KW  - 6DoF footstep sequences
KW  - black box walking controller
KW  - proper environment modeling
KW  - visual perception
KW  - foothold placements
KW  - exteroceptive perception
KW  - curved contact patches
KW  - bipedal robots
KW  - footstep planning
KW  - Planning
KW  - Three-dimensional displays
KW  - Rough surfaces
KW  - Surface roughness
KW  - Robot sensing systems
KW  - Navigation
DO  - 10.1109/ICRA.2018.8460561
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Bipedal robots have gained a lot of locomotion capabilities the past few years, especially in the control level. Navigation over complex and unstructured environments using exteroceptive perception, is still an active research topic. In this paper, we present a footstep planning system to produce foothold placements, using visual perception and proper environment modeling, given a black box walking controller. In particular, we extend a state-of-the-art search-based planning approach (ARA*) that produces 6DoF footstep sequences in 3D space for flat uneven terrain, to also handle rough curved surfaces, e.g. rocks. This is achieved by integrating both a curved patch modeling system for rough local terrain surfaces and a flat foothold contact analysis based on visual range input data, into the existing planning framework. The system is experimentally validated using real-world point clouds, while rough terrain stepping demonstrations are presented on the WALK-MAN humanoid robot, in simulation.
ER  - 

TY  - CONF
TI  - Robust and Precise Vehicle Localization Based on Multi-Sensor Fusion in Diverse City Scenes
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4670
EP  - 4677
AU  - G. Wan
AU  - X. Yang
AU  - R. Cai
AU  - H. Li
AU  - Y. Zhou
AU  - H. Wang
AU  - S. Song
PY  - 2018
KW  - Global Positioning System
KW  - Kalman filters
KW  - optical radar
KW  - road vehicles
KW  - satellite navigation
KW  - sensor fusion
KW  - LiDAR
KW  - localization system
KW  - GNSS RTK module
KW  - urban downtown
KW  - complementary sensors
KW  - precise localization system
KW  - robust localization system
KW  - precise vehicle localization
KW  - localization measurements
KW  - error-state Kalman filter
KW  - ambiguity resolution success rate
KW  - multisensor fusion framework
KW  - size 60.0 km
KW  - size 5.0 cm to 10.0 cm
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Estimation
KW  - Sensors
KW  - Global navigation satellite system
KW  - Autonomous vehicles
KW  - Robustness
DO  - 10.1109/ICRA.2018.8461224
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system.
ER  - 

TY  - CONF
TI  - Safe Distributed Lane Change Maneuvers for Multiple Autonomous Vehicles Using Buffered Input Cells
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4678
EP  - 4684
AU  - M. Wang
AU  - Z. Wang
AU  - S. Paudel
AU  - M. Schwager
PY  - 2018
KW  - collision avoidance
KW  - computational geometry
KW  - decision making
KW  - feedback
KW  - mobile robots
KW  - position control
KW  - road safety
KW  - road vehicles
KW  - safe distributed lane change maneuvers
KW  - multiple autonomous vehicles
KW  - reciprocal collision avoidance method
KW  - autonomous cars
KW  - linear dynamics
KW  - buffered input cell
KW  - Voronoi cell
KW  - Voronoi diagrams
KW  - vehicles control input
KW  - control stack
KW  - freeway driving scenario
KW  - decision-making layer
KW  - trajectory planning layer
KW  - feedback controller
KW  - BIC method
KW  - human-driven car
KW  - Collision avoidance
KW  - Robots
KW  - Vehicle dynamics
KW  - Aerospace electronics
KW  - Traffic control
KW  - Autonomous vehicles
KW  - Planning
DO  - 10.1109/ICRA.2018.8460898
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper introduces the Buffered Input Cell as a reciprocal collision avoidance method for multiple vehicles with high-order linear dynamics, extending recently proposed methods based on the Buffered Voronoi Cell [1] and generalized Voronoi diagrams [2]. We prove that if each vehicle's control input remains in its Buffered Input Cell at each time step, collisions will be avoided indefinitely. The method is fast, reactive, and only requires that each vehicle measures the relative position of neighboring vehicles. We incorporate this collision avoidance method as one layer of a complete lane change control stack for autonomous cars in a freeway driving scenario. The lane change control stack comprises a decision-making layer, a trajectory planning layer, a trajectory following feedback controller, and the Buffered Input Cell for collision avoidance. We show in simulations that collisions are avoided with multiple vehicles simultaneously changing lanes on a freeway. We also show in simulations that autonomous cars using the BIC method effectively avoid collisions with an aggressive human-driven car.
ER  - 

TY  - CONF
TI  - Deep Predictive Models for Collision Risk Assessment in Autonomous Driving
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4685
EP  - 4692
AU  - M. Strickland
AU  - G. Fainekos
AU  - H. B. Amor
PY  - 2018
KW  - Bayes methods
KW  - collision avoidance
KW  - decision making
KW  - driver information systems
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - recurrent neural nets
KW  - risk management
KW  - video streaming
KW  - Deep Predictive Models
KW  - collision risk assessment
KW  - autonomous driving
KW  - predictive approach
KW  - assisted driving
KW  - deep predictive model
KW  - video streams
KW  - RGB images
KW  - temporal information
KW  - multi-modal information
KW  - proprioceptive state
KW  - Bayesian convolutional LSTM
KW  - decision making
KW  - Predictive models
KW  - Accidents
KW  - Stochastic processes
KW  - Uncertainty
KW  - Bayes methods
KW  - Cameras
KW  - Tensile stress
DO  - 10.1109/ICRA.2018.8461160
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we investigate a predictive approach for collision risk assessment in autonomous and assisted driving. A deep predictive model is trained to anticipate imminent accidents from traditional video streams. In particular, the model learns to identify cues in RGB images that are predictive of hazardous upcoming situations. In contrast to previous work, our approach incorporates (a) temporal information during decision making, (b) multi-modal information about the environment, as well as the proprioceptive state and steering actions of the controlled vehicle, and (c) information about the uncertainty inherent to the task. To this end, we discuss Deep Predictive Models and present an implementation using a Bayesian Convolutional LSTM. Experiments in a simple simulation environment show that the approach can learn to predict impending accidents with reasonable accuracy, especially when multiple cameras are used as input sources.
ER  - 

TY  - CONF
TI  - End-to-End Driving Via Conditional Imitation Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4693
EP  - 4700
AU  - F. Codevilla
AU  - M. Müller
AU  - A. López
AU  - V. Koltun
AU  - A. Dosovitskiy
PY  - 2018
KW  - collision avoidance
KW  - learning systems
KW  - mobile robots
KW  - road traffic control
KW  - driving policy functions
KW  - conditional imitation learning
KW  - sensorimotor coordination
KW  - vision-based driving
KW  - robotic truck
KW  - driving policies
KW  - deep networks
KW  - high-level navigational commands
KW  - urban driving
KW  - high-level command input
KW  - condition imitation
KW  - Robot sensing systems
KW  - Task analysis
KW  - Vehicles
KW  - Cameras
KW  - Roads
KW  - Navigation
DO  - 10.1109/ICRA.2018.8460487
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands.
ER  - 

TY  - CONF
TI  - VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4701
EP  - 4708
AU  - M. Bojarski
AU  - A. Choromanska
AU  - K. Choromanski
AU  - B. Firner
AU  - L. J. Ackel
AU  - U. Muller
AU  - P. Yeres
AU  - K. Zieba
PY  - 2018
KW  - data visualisation
KW  - feedforward neural nets
KW  - learning (artificial intelligence)
KW  - object detection
KW  - traffic engineering computing
KW  - video signal processing
KW  - convolutional neural network
KW  - irrelevant information
KW  - prediction decision
KW  - CNN-based systems
KW  - steering self-driving cars
KW  - visualization method
KW  - valuable debugging tool
KW  - theoretical arguments
KW  - input pixels
KW  - individual pixels
KW  - visualization tool
KW  - NVIDIA neural-network-based end-to-end learning system
KW  - autonomous driving
KW  - VisualBackProp
KW  - public road video data
KW  - layer-wise relevance propagation approach
KW  - similar visualization results
KW  - PilotNet steering decision
KW  - relevant object capture
KW  - Neurons
KW  - Visualization
KW  - Deconvolution
KW  - Tools
KW  - Biological neural networks
KW  - Roads
KW  - Data visualization
DO  - 10.1109/ICRA.2018.8461053
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper proposes a new method, that we call VisualBackProp, for visualizing which sets of pixels of the input image contribute most to the predictions made by the convolutional neural network (CNN). The method heavily hinges on exploring the intuition that the feature maps contain less and less irrelevant information to the prediction decision when moving deeper into the network. The technique we propose is dedicated for CNN-based systems for steering self-driving cars and is therefore required to run in real-time. This makes the proposed visualization method a valuable debugging tool which can be easily used during both training and inference. We justify our approach with theoretical arguments and confirm that the proposed method identifies sets of input pixels, rather than individual pixels, that collaboratively contribute to the prediction. We utilize the proposed visualization tool in the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet. We demonstrate that VisualBackProp determines which elements in the road image most influence PilotNet's steering decision and indeed captures relevant objects on the road. The empirical evaluation furthermore shows the plausibility of the proposed approach on public road video data as well as in other applications and reveals that it compares favorably to the layer-wise relevance propagation approach, i.e. it obtains similar visualization results and achieves order of magnitude speed-ups.
ER  - 

TY  - CONF
TI  - Predicting Ego-Vehicle Paths from Environmental Observations with a Deep Neural Network
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4709
EP  - 4716
AU  - U. Baumann
AU  - C. Guiser
AU  - M. Herman
AU  - J. M. Zollner
PY  - 2018
KW  - driver information systems
KW  - feature extraction
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - road vehicles
KW  - static vehicle environment
KW  - grid-based prediction
KW  - varying assistance tasks
KW  - baseline approaches
KW  - environmental observations
KW  - deep neural network
KW  - advanced driver assistance systems
KW  - predictive model
KW  - road topologies
KW  - environmental properties
KW  - path extraction
KW  - ego-vehicle path prediction
KW  - ego-vehicle motion
KW  - Predictive models
KW  - Vehicles
KW  - Sensors
KW  - Roads
KW  - Trajectory
KW  - Data models
KW  - Motion measurement
DO  - 10.1109/ICRA.2018.8460704
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Advanced driver assistance systems allow for increasing user comfort and safety by sensing the environment and anticipating upcoming hazards. Often, this requires to accurately predict how situations will change. Recent approaches make simplifying assumptions on the predictive model of the Ego-Vehicle motion or assume prior knowledge, such as road topologies, to be available. However, in many urban areas this assumption is not satisfied. Furthermore, temporary changes (e.g. construction areas, vehicles parked on the street) are not considered by such models. Since many cars observe the environment with several different sensors, predictive models can benefit from them by considering environmental properties. In this work, we present an approach for an Ego-Vehicle path prediction from such sensor measurements of the static vehicle environment. Besides proposing a learned model for predicting the driver's multi-modal future path as a grid-based prediction, we derive an approach for extracting paths from it. In driver assistance systems both can be used to solve varying assistance tasks. The proposed approach is evaluated on real driving data and outperforms several baseline approaches.
ER  - 

TY  - CONF
TI  - Learning Steering Bounds for Parallel Autonomous Systems
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4717
EP  - 4724
AU  - A. Amini
AU  - L. Paull
AU  - T. Balch
AU  - S. Karaman
AU  - D. Rus
PY  - 2018
KW  - Bayes methods
KW  - cameras
KW  - control engineering computing
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - mixture models
KW  - mobile robots
KW  - neural nets
KW  - path planning
KW  - road vehicles
KW  - robot vision
KW  - steering systems
KW  - parallel autonomous systems
KW  - deep learning
KW  - autonomous driving task
KW  - camera data input
KW  - autonomous navigation
KW  - vehicle control
KW  - continuous control probability distribution
KW  - deep neural network based algorithm
KW  - steering angles
KW  - parallel autonomy setting
KW  - driving conditions
KW  - variational Bayesian methods
KW  - steering bounds learning
KW  - end-to-end learning
KW  - steering control options
KW  - Gaussian mixture models
KW  - Autonomous vehicles
KW  - Navigation
KW  - Neural networks
KW  - Probability distribution
KW  - Decision making
KW  - Machine learning
KW  - Bayes methods
DO  - 10.1109/ICRA.2018.8461253
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Deep learning has been successfully applied to “end-to-end” learning of the autonomous driving task, where a deep neural network learns to predict steering control commands from camera data input. However, the learned representations do not support higher-level decision making required for autonomous navigation, nor the uncertainty estimates required for parallel autonomy, where vehicle control is shared between human and robot. This paper tackles the problem of learning a representation to predict a continuous control probability distribution, and thus steering control options and bounds for those options, which can be used for autonomous navigation. Each mode of the distribution encodes a possible macro-action that the system could execute at that instant, and the covariances of the modes place bounds on safe steering control values. Our approach has the added advantage of being trained on unlabeled data collected from inexpensive cameras. The deep neural network based algorithm generates a probability distribution over the space of steering angles, from which we leverage Variational Bayesian methods to extract a mixture model and compute the different possible actions in the environment. A bound, which the autonomous vehicle must respect in our parallel autonomy setting, is then computed for each of these actions. We evaluate our approach on a challenging dataset containing a wide variety of driving conditions, and show that our algorithm is capable of parameterizing Gaussian Mixture Models for possible actions, and extract steering bounds with a mean error of only 2 degrees. Additionally, we demonstrate our system working on a full scale autonomous vehicle and evaluate its ability to successful handle various different parallel autonomy situations.
ER  - 

TY  - CONF
TI  - End to End Learning of Spiking Neural Network Based on R-STDP for a Lane Keeping Vehicle
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4725
EP  - 4732
AU  - Z. Bing
AU  - C. Meschede
AU  - K. Huang
AU  - G. Chen
AU  - F. Rohrbein
AU  - M. Akl
AU  - A. Knoll
PY  - 2018
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - road vehicles
KW  - robot vision
KW  - SLAM (robots)
KW  - spiking neural network
KW  - lane keeping vehicle
KW  - mobile applications
KW  - mobile robot applications
KW  - reward-modulated spike-timing-dependent-plasticity
KW  - reinforcement learning
KW  - Pioneer robot
KW  - lane information
KW  - robot tasks control
KW  - end to end learning approach
KW  - R-STDP
KW  - SNNs training
KW  - neuromorphic vision sensor
KW  - lateral localization accuracy
KW  - Voltage control
KW  - Task analysis
KW  - Robot sensing systems
KW  - Training
KW  - Synapses
KW  - Neurons
DO  - 10.1109/ICRA.2018.8460482
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Learning-based methods have demonstrated clear advantages in controlling robot tasks, such as the information fusion abilities, strong robustness, and high accuracy. Meanwhile, the on-board systems of robots have limited computation and energy resources, which are contradictory with state-of-the-art learning approaches. They are either too lightweight to solve complex problems or too heavyweight to be used for mobile applications. On the other hand, training spiking neural networks (SNNs) with biological plausibility has great potentials of performing fast computation and energy efficiency. However, the lack of effective learning rules for SNNs impedes their wide usage in mobile robot applications. This paper addresses the problem by introducing an end to end learning approach of spiking neural networks for a lane keeping vehicle. We consider the reward-modulated spike-timing-dependent-plasticity (R-STDP) as a promising solution in training SNNs, since it combines the advantages of both reinforcement learning and the well-known STDP. We test our approach in three scenarios that a Pioneer robot is controlled to keep lanes based on an SNN. Specifically, the lane information is encoded by the event data from a neuromorphic vision sensor. The SNN is constructed using R-STDP synapses in an all-to-all fashion. We demonstrate the advantages of our approach in terms of the lateral localization accuracy by comparing with other state-of-the-art learning algorithms based on SNNs.
ER  - 

TY  - CONF
TI  - A Dual-Modal Vision-Based Tactile Sensor for Robotic Hand Grasping
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4740
EP  - 4745
AU  - B. Fang
AU  - F. Sun
AU  - C. Yang
AU  - H. Xue
AU  - W. Chen
AU  - C. Zhang
AU  - D. Guo
AU  - H. Liu
PY  - 2018
KW  - backpropagation
KW  - CCD image sensors
KW  - elastomers
KW  - force measurement
KW  - image texture
KW  - neural nets
KW  - robots
KW  - shape recognition
KW  - tactile sensors
KW  - robotic hand grasping
KW  - force vector distribution
KW  - transparent elastomer
KW  - transparent acrylic board
KW  - CCD camera
KW  - reflective membrane
KW  - markers array
KW  - object contact surface
KW  - backpropagation neural network
KW  - local binary pattern algorith
KW  - force magnitude
KW  - force direction
KW  - surface texture sensing
KW  - dual-modal vision-based tactile sensor
KW  - texture recognition rate
KW  - texture information
KW  - Force
KW  - Tactile sensors
KW  - Neurons
KW  - Cameras
KW  - Light emitting diodes
DO  - 10.1109/ICRA.2018.8461007
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Humans' fingertips can perceive not only the magnitude and the direction of force but also the texture of object. When we grasp an object, the surface texture sensing of the fingertip helps us recognize the object and the force feeling that is parallel to the skin helps us grasp stably. Focusing on these points, we have developed a dual-modal vision-based tactile sensor that can measure the texture of object and a distribution of force vectors. The tactile sensor consists of a transparent elastomer, a camera, a piece of transparent acrylic board, LEDs and supporting structures. A reflective membrane and markers array are on the surface of the elastomer. An applied force on the elastic body results in movements of the markers, which are acquired by the CCD camera. In addition, the shape and texture of the object's contact surface can be reflected by the membrane deformations. The distribution of force vectors is determined by the BP neural network. The local binary pattern algorithm using captured images calculates the texture information. This paper reports experimental evaluation results concerning accuracy of determination of magnitude, direction of force, and texture recognition rate.
ER  - 

TY  - CONF
TI  - Adapting the Goals/Questions/Metrics (GQM) Method for Applications in Robot Design
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4746
EP  - 4751
AU  - C. McGinn
AU  - E. Bourke
AU  - T. O'Kelly
AU  - M. F. Cullinan
PY  - 2018
KW  - grippers
KW  - service robots
KW  - Goals/Questions/Metrics method
KW  - robot design
KW  - advanced robots
KW  - resource-intensive activity
KW  - research teams
KW  - complex robot systems
KW  - design metrics
KW  - design tool
KW  - design-orientated GQM method
KW  - bespoke robotic gripper
KW  - service robot
KW  - GQM principles
KW  - robotics applications
KW  - Measurement
KW  - Prototypes
KW  - Grippers
KW  - Service robots
KW  - Robot sensing systems
KW  - Planning
DO  - 10.1109/ICRA.2018.8460630
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Developing advanced robots can be a resource-intensive activity that creates many challenges for research teams. There is a need to formulate new techniques for systematically designing complex robot systems, especially in cases where high adaptability is needed and design metrics cannot be explicitly specified in advance. This research explores how the Goals/Questions/Metrics (GQM) method, a well-established technique for process measurement, can be modified for use as a design tool in robotics. To illustrate how a design-orientated GQM method may be used in practice, a sample use-case is given detailing how the approach was applied to the task of developing a bespoke robotic gripper for a service robot. The study provides an early indication that the adoption of GQM principles by designers can have significant benefits in robotics applications. However, further investigation is needed to better understand the magnitude and scope of any improvements.
ER  - 

TY  - CONF
TI  - The Exchange of Knowledge Using Cloud Robotics
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4768
EP  - 4775
AU  - A. K. Bozcuoğlu
AU  - G. Kazhoyan
AU  - Y. Furuta
AU  - S. Stelter
AU  - M. Beetz
AU  - K. Okada
AU  - M. Inaba
PY  - 2018
KW  - cloud computing
KW  - ontologies (artificial intelligence)
KW  - robots
KW  - ontologies
KW  - OPENEASE cloud engine
KW  - web-based user interface
KW  - Fetch robot
KW  - PR2 robots
KW  - execution logs
KW  - knowledge exchange
KW  - encyclopedic knowledge
KW  - cloud application
KW  - crowd-sourcing
KW  - cloud robotics
KW  - Robots
KW  - Ontologies
KW  - Semantics
KW  - Task analysis
KW  - Containers
KW  - Cloud computing
KW  - Cognition
DO  - 10.1109/ICRA.2018.8460187
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - To enable robots to perform human-level tasks flexibly in varying conditions, we need a mechanism that allows them to exchange knowledge between themselves for crowd-sourcing the knowledge gap problem. One approach to achieve this is to equip a cloud application with a range of encyclopedic knowledge (i.e. ontologies) and execution logs of different robots performing the same tasks in different environments. In this paper, we show how knowledge exchange between robots can be done using OPENEASE as the cloud application. We equipped OPENEASE with ontologies about the kitchen domain, execution logs of three robots operating in two different kitchens, and semantic descriptions of both environments. By addressing two different use cases, we show that two PR2 robots and one Fetch robot can successfully adapt each other's plan parameters and sub symbolic data to the experiments that they are conducting.
ER  - 

TY  - CONF
TI  - Dry Stacking for Automated Construction with Irregular Objects
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4782
EP  - 4789
AU  - V. Thangavelu
AU  - Y. Liu
AU  - M. Saboia
AU  - N. Napp
PY  - 2018
KW  - assembly planning
KW  - brick
KW  - building management systems
KW  - buildings (structures)
KW  - construction industry
KW  - disasters
KW  - heuristic programming
KW  - mechanical stability
KW  - robotic assembly
KW  - statistical analysis
KW  - structural engineering
KW  - automated construction
KW  - irregular objects
KW  - dry stacked structures
KW  - disaster areas
KW  - remote environments
KW  - assembly planning process
KW  - bricks
KW  - heuristics programming
KW  - Shape
KW  - Stacking
KW  - Planning
KW  - Stability analysis
KW  - Two dimensional displays
KW  - Building materials
DO  - 10.1109/ICRA.2018.8460562
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We describe a method for automatically building structures from stacked, irregularly shaped objects. This is a simplified model for the problem of building dry stacked structures (i.e. no mortar) from found stones. Although automating such construction methods would be ideally suited for disaster areas or remote environments, currently such structures need to be built by skilled masons. No practical methods for automating the assembly planning process are known. The problem is challenging since each assembly action can be drawn from a continuous space poses for an object and several local geometric and physical considerations strongly affect the overall stability. We show that structures that are built following a stacking order for perfect bricks can accommodate a limited amount of irregularity, however, their performance degrades quickly when objects deviate from their ideal shape. We present a strategy for stacking irregular shapes that first considers geometric and physical constraints to find a small set of feasible actions and then further refines this set by using heuristics gathered from instructional literature for masons. The proposed method of choosing assembly actions allows construction with objects that contain a significant amount of variation.
ER  - 


