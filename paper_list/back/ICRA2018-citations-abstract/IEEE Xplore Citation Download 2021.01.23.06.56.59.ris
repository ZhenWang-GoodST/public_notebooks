TY  - CONF
TI  - Practical Motion Segmentation for Urban Street View Scenes
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1879
EP  - 1886
AU  - C. Rubino
AU  - A. Del Bue
AU  - T. Chin
PY  - 2018
KW  - cameras
KW  - image motion analysis
KW  - image segmentation
KW  - image sequences
KW  - traffic engineering computing
KW  - video signal processing
KW  - generic motion segmentation algorithms
KW  - KITTI dataset
KW  - video sequence annotation
KW  - restricted camera movement
KW  - application-specific factors
KW  - urban environment
KW  - image-based motion segmentation
KW  - urban street view scenes
KW  - practical motion segmentation
KW  - realistic motion segmentation benchmark dataset
KW  - Motion segmentation
KW  - Trajectory
KW  - Computer vision
KW  - Cameras
KW  - Transmission line matrix methods
KW  - Semantics
KW  - Computational modeling
DO  - 10.1109/ICRA.2018.8460993
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Though a long-studied problem, motion segmentation has yet to migrate into practical applications. We argue that a vital step towards that goal lies in addressing motion segmentation for the specific setting of interest. To this end, this paper presents a new approach for image-based motion segmentation in the case of vehicles navigating inside an urban environment. We exploit two application-specific factors - the restricted camera movement and the known type of moving objects - to deal with the two major limiting factors - missing data and strong perspective effects - that affect most previous “generic” motion segmentation algorithms. By constraining the geometry and exploiting known semantic classes in the scene, we achieve much higher accuracy than previous approaches. In addition to the novel algorithm, we contribute a more realistic motion segmentation benchmark dataset for moving platforms by annotating real video sequences from the KITTI dataset. Experiments on this dataset and other synthetic data confirm the effectiveness of the proposed approach.
ER  - 

TY  - CONF
TI  - SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1887
EP  - 1893
AU  - B. Wu
AU  - A. Wan
AU  - X. Yue
AU  - K. Keutzer
PY  - 2018
KW  - computer games
KW  - feedforward neural nets
KW  - image classification
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - object detection
KW  - optical radar
KW  - pattern clustering
KW  - Grand Theft Auto
KW  - video game
KW  - autonomous driving
KW  - road-objects
KW  - semantic segmentation
KW  - 3D LiDAR point cloud
KW  - real-time road-object segmentation
KW  - recurrent CRF
KW  - realistic training data
KW  - LiDAR simulator
KW  - extra training data
KW  - 3D bounding boxes
KW  - point-wise segmentation labels
KW  - CNN model
KW  - instance-level labels
KW  - point-wise label map
KW  - transformed LiDAR point cloud
KW  - convolutional neural networks
KW  - SqueezeSeg
KW  - end-to-end pipeline
KW  - point-wise classification problem
KW  - LiDAR point clouds
KW  - time 8.2 ms to 9.2 ms
KW  - time 3.0 d
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Computational modeling
KW  - Pipelines
KW  - Autonomous vehicles
KW  - Semantics
KW  - Clustering algorithms
DO  - 10.1109/ICRA.2018.8462926
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a point-wise classification problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7±0.5 ms per frame), highly desirable for autonomous driving. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code is open-source released1. The paper is accompanied by a video2 containing a high level introduction and demonstrations of this work.
ER  - 

TY  - CONF
TI  - Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1894
EP  - 1900
AU  - D. Barnes
AU  - W. Maddern
AU  - G. Pascoe
AU  - I. Posner
PY  - 2018
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image classification
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion estimation
KW  - object detection
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - self-supervised distractor learning
KW  - robust monocular visual odometry
KW  - self-supervised approach
KW  - distractors
KW  - camera images
KW  - cluttered urban environments
KW  - per-pixel ephemerality mask
KW  - depth map
KW  - deep convolutional network
KW  - monocular visual odometry pipeline
KW  - sparse features
KW  - dense photometric matching
KW  - metric-scale VO
KW  - single camera
KW  - robust VO methods
KW  - odometry drift
KW  - egomotion estimation
KW  - moving vehicles
KW  - urban traffic
KW  - vehicle motion
KW  - ephemerality
KW  - offline multisession mapping approaches
KW  - Three-dimensional displays
KW  - Cameras
KW  - Robustness
KW  - Visual odometry
KW  - Motion estimation
KW  - Entropy
KW  - Training data
DO  - 10.1109/ICRA.2018.8460564
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a self-supervised approach to ignoring “distractors” in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.
ER  - 

TY  - CONF
TI  - Semantic Mapping with Omnidirectional Vision
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1901
EP  - 1907
AU  - L. F. Posada
AU  - A. Velasquez-Lopez
AU  - F. Hoffmann
AU  - T. Bertram
PY  - 2018
KW  - cameras
KW  - distortion
KW  - image classification
KW  - image fusion
KW  - image segmentation
KW  - image sensors
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - omnidirectional vision
KW  - omnidirectional images
KW  - robust segmentation
KW  - occupancy grid maps
KW  - inverse sensor model
KW  - nonlinear distortions
KW  - omnidirectional camera mirror
KW  - place category classifier
KW  - range-based occupancy grid
KW  - dense semantic map
KW  - bird eye view
KW  - visual semantic mapping framework
KW  - robot local free space
KW  - Semantics
KW  - Sensors
KW  - Cameras
KW  - Robots
KW  - Buildings
KW  - Mirrors
KW  - Visualization
DO  - 10.1109/ICRA.2018.8461165
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a purely visual semantic mapping framework using omnidirectional images. The approach rests upon the robust segmentation of the robot's local free space, replacing conventional range sensors for the generation of occupancy grid maps. The perceptions are mapped into a bird's eye view allowing an inverse sensor model directly by removing the non-linear distortions of the omnidirectional camera mirror. The system relies on a place category classifier to label the navigation relevant categories: room, corridor, doorway, and open room. Each place class maintains a separated grid map that are fused with the range-based occupancy grid for building a dense semantic map.
ER  - 

TY  - CONF
TI  - Semantic Segmentation from Limited Training Data
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1908
EP  - 1915
AU  - A. Milan
AU  - T. Pham
AU  - K. Vijay
AU  - D. Morrison
AU  - A. W. Tow
AU  - L. Liu
AU  - J. Erskine
AU  - R. Grinover
AU  - A. Gurman
AU  - T. Hunn
AU  - N. Kelly-Boxall
AU  - D. Lee
AU  - M. McTaggart
AU  - G. Rallos
AU  - A. Razjigaev
AU  - T. Rowntree
AU  - T. Shen
AU  - R. Smith
AU  - S. Wade-McCue
AU  - Z. Zhuang
AU  - C. Lehnert
AU  - G. Lin
AU  - I. Reid
AU  - P. Corke
AU  - J. Leitner
PY  - 2018
KW  - convolution
KW  - image classification
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - object recognition
KW  - recurrent neural nets
KW  - robot vision
KW  - limited training data
KW  - robotic perception
KW  - cluttered scenes
KW  - shiny surfaces
KW  - transparent surfaces
KW  - robust perception pipeline
KW  - data acquisition
KW  - deep metric learning approach
KW  - semantic-agnostic boundary detection
KW  - pixel-wise voting
KW  - fully-supervised semantic segmentation approach
KW  - ARC 2017 dataset
KW  - Amazon Robotics Challenge 2017
KW  - dataset collection
KW  - deep convolutional neural networks
KW  - Task analysis
KW  - Image segmentation
KW  - Training
KW  - Semantics
KW  - Robots
KW  - Measurement
KW  - Three-dimensional displays
DO  - 10.1109/ICRA.2018.8461082
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present our approach for robotic perception in cluttered scenes that led to winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small objects with shiny and transparent surfaces, the biggest challenge of the 2017 competition was the introduction of unseen categories. In contrast to traditional approaches which require large collections of annotated data and many hours of training, the task here was to obtain a robust perception pipeline with only few minutes of data acquisition and training time. To that end, we present two strategies that we explored. One is a deep metric learning approach that works in three separate steps: semantic-agnostic boundary detection, patch classification and pixel-wise voting. The other is a fully-supervised semantic segmentation approach with efficient dataset collection. We conduct an extensive analysis of the two methods on our ARC 2017 dataset. Interestingly, only few examples of each class are sufficient to fine-tune even very deep convolutional neural networks for this specific task.
ER  - 

TY  - CONF
TI  - Planning Ergonomic Sequences of Actions in Human-Robot Interaction
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1916
EP  - 1923
AU  - B. Busch
AU  - M. Toussaint
AU  - M. Lopes
PY  - 2018
KW  - ergonomics
KW  - human-robot interaction
KW  - multi-agent systems
KW  - multi-robot systems
KW  - path planning
KW  - optimization formulation
KW  - ergonomic situations
KW  - human-robot interaction
KW  - human-robot collaboration
KW  - motion planning problem
KW  - multiagent case
KW  - human robot
KW  - Ergonomics
KW  - Task analysis
KW  - Robot kinematics
KW  - Planning
KW  - Cost function
DO  - 10.1109/ICRA.2018.8462927
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we define the problem of human-robot collaboration as a combined task and motion planning problem which is extended to the multi-agent case (human and robot). Our proposed approach allows us to explicitly take into account ergonomic cost, synchrony and concurrency of behavior in an optimization formulation. We show simulated results as well as an experiment with a real robot combined with a user study. Results show that optimizing over a sequence of actions leads to more ergonomic situations.
ER  - 

TY  - CONF
TI  - Proposal of Collaboration Safety in a Coexistence Environment of Human and Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1924
EP  - 1930
AU  - M. Dohi
AU  - K. Okada
AU  - I. Maeda
AU  - S. Fujitani
AU  - T. Fujita
PY  - 2018
KW  - Big Data
KW  - factory automation
KW  - human-robot interaction
KW  - industrial robots
KW  - Internet of Things
KW  - manufacturing systems
KW  - production engineering computing
KW  - safety
KW  - solution-oriented industrial society
KW  - high technological capabilities
KW  - next-generation manufacturing systems
KW  - flexible productivity
KW  - human-robot collaboration
KW  - robot revolution
KW  - collaboration safety
KW  - safety level
KW  - IoT technology
KW  - optimization
KW  - big data
KW  - progressing digitalization
KW  - 4th industrial revolution
KW  - manufacturing sites
KW  - Collaboration
KW  - Service robots
KW  - Production
KW  - Hazards
KW  - Optimization
KW  - Robot Safety
KW  - Industrial Robots
KW  - Factory Automation
DO  - 10.1109/ICRA.2018.8460869
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Whereas various things are connected via networks thanks to IoT technology, and the era of the 4th industrial revolution which realizes optimization and efficiency by utilizing AI and big data is emerging, manufacturing systems are also significantly transforming globally. In addition to the robot revolution, along with progressing digitalization, manufacturing sites in Japan are changing, aiming at establishment of “Connected Industries” that is a solution-oriented industrial society, based on the high technological capabilities. In order to respond timely to diversifying demands of customers, it is necessary to build a next-generation manufacturing systems that realizes flexible and high productivity, such as human-robot collaboration, and it is becoming difficult to respond to the necessity by conventional safety concept. Therefore, in order to realize the 4th industrial revolution, the robot revolution, and “Connected Industries,” it is essential to establish a new safety concept corresponding to the next-generation manufacturing systems to ensure safety. This paper introduces the safety concept to be changed along with the evolution of manufacturing sites, and proposes a new safety concept, which realizes collaboration safety of humans and robots, and an outline of its safety level, for the first time in the world.
ER  - 

TY  - CONF
TI  - A Robust Method to Predict Temporal Aspects of Actions by Observation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1931
EP  - 1938
AU  - E. Hourdakis
AU  - P. Trahanias
PY  - 2018
KW  - human-robot interaction
KW  - manipulators
KW  - robust method
KW  - temporal properties
KW  - ground truth data
KW  - temporal aspec prediction
KW  - time-constrained tasks
KW  - wiping
KW  - table
KW  - vegetable chopping
KW  - floor cleaning
KW  - Task analysis
KW  - Frequency modulation
KW  - Motion segmentation
KW  - Predictive models
KW  - Process control
DO  - 10.1109/ICRA.2018.8461067
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The ability to predict the duration of an activity can enable a robot to plan its behaviors ahead, interact seamlessly with other humans, by coordinating its actions, and allocate effort and resources to tasks that are time-constrained or critical. Despite its usefulness, models that examine the temporal properties of an activity remain relatively unexplored. In the current paper we present, to the best of our knowledge, the first method that can estimate temporal properties of an activity by observation. We evaluate it on three use-cases (i) wiping a table, (ii) chopping vegetables and (iii) cleaning the floor, using ground truth data from real demonstrations, and show that it can make predictions with high accuracy and little training. In addition, we investigate different methods to approximate the progress of each task, and demonstrate how a model can generalize, by reusing part of it in different activities.
ER  - 

TY  - CONF
TI  - Augmented Reality for Feedback in a Shared Control Spraying Task
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1939
EP  - 1946
AU  - J. Elsdon
AU  - Y. Demiris
PY  - 2018
KW  - augmented reality
KW  - calibration
KW  - control engineering computing
KW  - feedback
KW  - industrial robots
KW  - mobile robots
KW  - spraying
KW  - handheld spraying robot
KW  - industrial robots
KW  - task awareness
KW  - Microsoft Hololens system
KW  - motion capture system
KW  - augmented reality spraying task
KW  - calibration routine
KW  - augmented reality interfaces
KW  - feedback
KW  - logical approach
KW  - target regions
KW  - shared control methods
KW  - shared control spraying task
KW  - time 4.0 s
KW  - Task analysis
KW  - Robots
KW  - Spraying
KW  - Augmented reality
KW  - Calibration
KW  - Paints
KW  - Headphones
DO  - 10.1109/ICRA.2018.8461179
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Using industrial robots to spray structures has been investigated extensively, however interesting challenges emerge when using handheld spraying robots. In previous work we have demonstrated the use of shared control of a handheld spraying robot to assist a user in a 3D spraying task. In this paper we demonstrate the use of Augmented Reality Interfaces to increase the user's progress and task awareness. We describe our solutions to challenging calibration issues between the Microsoft Hololens system and a motion capture system without the need for well defined markers or careful alignment on the part of the user. Error relative to the motion capture system was shown to be 10mm after only a 4 second calibration routine. Secondly we outline a logical approach for visualising liquid density for an augmented reality spraying task, this system allows the user to see target regions to complete, areas that are complete and areas that have been overdosed clearly. Finally we produced a user study to investigate the level of assistance that a handheld robot utilising shared control methods should provide during a spraying task. Using a handheld spraying robot with a moving spray head did not aid the user much over simply actuating spray nozzle for them. Compared to manual control the automatic modes significantly reduced the task load experienced by the user and significantly increased the quality of the result of the spraying task, reducing the error by 33-45%.
ER  - 

TY  - CONF
TI  - Interactive Robot Knowledge Patching Using Augmented Reality
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1947
EP  - 1954
AU  - H. Liu
AU  - Y. Zhang
AU  - W. Si
AU  - X. Xie
AU  - Y. Zhu
AU  - S. Zhu
PY  - 2018
KW  - augmented reality
KW  - data visualisation
KW  - decision making
KW  - human-robot interaction
KW  - knowledge representation
KW  - learning (artificial intelligence)
KW  - robot programming
KW  - Augmented Reality
KW  - Temporal And-Or graph
KW  - robot program
KW  - interactive robot teaching
KW  - AR interface
KW  - knowledge representation
KW  - comprehensive visualizations
KW  - decision making process
KW  - Microsoft HoloLens
KW  - interactive robot knowledge patching
KW  - Task analysis
KW  - Decision making
KW  - Knowledge representation
KW  - Visualization
KW  - Robot sensing systems
KW  - Grammar
DO  - 10.1109/ICRA.2018.8462837
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a novel Augmented Reality (AR) approach, through Microsoft HoloLens, to address the challenging problems of diagnosing, teaching, and patching interpretable knowledge of a robot. A Temporal And-Or graph (T-AOG) of opening bottles is learned from human demonstration and programmed to the robot. This representation yields a hierarchical structure that captures the compositional nature of the given task, which is highly interpretable for the users. By visualizing the knowledge structure represented by a T-AOG and the decision making process by parsing the T-AOG, the user can intuitively understand what the robot knows, supervise the robot's action planner, and monitor visually latent robot states (e.g., the force exerted during interactions). Given a new task, through such comprehensive visualizations of robot's inner functioning, users can quickly identify the reasons of failures, interactively teach the robot with a new action, and patch it to the current knowledge structure. In this way, the robot is capable of solving similar but new tasks only through minor modifications provided by the users interactively. This process demonstrates the interpretability of our knowledge representation and the effectiveness of the AR interface.
ER  - 

TY  - CONF
TI  - Using Constrained Optimization for Real-Time Synchronization of Verbal and Nonverbal Robot Behavior
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1955
EP  - 1961
AU  - A. E. Vijayan
AU  - S. Alexanderson
AU  - J. Beskow
AU  - I. Leite
PY  - 2018
KW  - computer animation
KW  - humanoid robots
KW  - image motion analysis
KW  - mobile robots
KW  - motion control
KW  - optimisation
KW  - robot vision
KW  - constrained optimization
KW  - real-time synchronization
KW  - motion re-targeting techniques
KW  - virtual character animation research
KW  - joint angular velocities
KW  - robot motion
KW  - re-targeted motion sequences
KW  - humanoid robot
KW  - joint motion trajectories
KW  - verbal behavior synchronization
KW  - nonverbal behavior synchronization
KW  - verbal robot behavior
KW  - nonverbal robot behavior
KW  - Optimization
KW  - Trajectory
KW  - Angular velocity
KW  - Synchronization
KW  - Dynamics
KW  - Robot motion
DO  - 10.1109/ICRA.2018.8462828
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Most of the motion re-targeting techniques are grounded on virtual character animation research, which means that they typically assume that the target embodiment has unconstrained joint angular velocities. However, because robots often do have such constraints, traditional re-targeting approaches can originate irregular delays in the robot motion. With the goal of ensuring synchronization between verbal and nonverbal behavior, this paper proposes an optimization framework for processing re-targeted motion sequences that addresses constraints such as joint angle and angular velocities. The proposed framework was evaluated on a humanoid robot using both objective and subjective metrics. While the analysis of the joint motion trajectories provides evidence that our framework successfully performs the desired modifications to ensure verbal and nonverbal behavior synchronization, results from a perceptual study showed that participants found the robot motion generated by our method more natural, elegant and lifelike than a control condition.
ER  - 

TY  - CONF
TI  - Learning Task-Based Instructional Policy for Excavator-Like Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1962
EP  - 1969
AU  - H. Maske
AU  - E. Kieson
AU  - G. Chowdhary
AU  - C. Abramson
PY  - 2018
KW  - excavators
KW  - learning by example
KW  - mobile robots
KW  - learning from demonstration
KW  - demonstration trajectories automatic segmentation
KW  - hydraulic actuated scaled excavator robot
KW  - complex truck loading task
KW  - nongeneric policy model
KW  - mapping continuous state action trajectories
KW  - expert demonstration
KW  - task-based instructional policy
KW  - Task analysis
KW  - Robots
KW  - Trajectory
KW  - Hidden Markov models
KW  - Load modeling
KW  - Actuators
KW  - Loading
DO  - 10.1109/ICRA.2018.8462923
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We explore beyond existing work in learning from demonstration by asking the question: “Can robots learn to guide?”, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct humans in executing complex task? As a solution, we propose learning of instructional policy (πI) that maps the state to an instruction for a human. To learn πI, we define action primitives that addresses the challenge of mapping continuous state action trajectories to human parse-able instructions. Action primitives are demonstrated to be very effective in automatic segmentation of demonstration trajectories into fewer repetitive and reusable segments, and a highly scalable approach in comparison to the existing state-of-the art. Finally, we construct a non-generic policy model as a generative model for instructional policies to generate instruction for an entire task. With few modifications, the proposed model is demonstrated to perform autonomous execution of complex truck loading task on hydraulic actuated scaled excavator robot. Guidance approach is tested based on a controlled group study involving 75 participants, who learn to perform the same task.
ER  - 

TY  - CONF
TI  - Playdough to Roombots: Towards a Novel Tangible User Interface for Self-reconfigurable Modular Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1970
EP  - 1977
AU  - M. Mutlu
AU  - S. Hauser
AU  - A. Bernardino
AU  - A. Ijspeert
PY  - 2018
KW  - control engineering computing
KW  - furniture
KW  - mobile robots
KW  - robot dynamics
KW  - user interfaces
KW  - self-reconfigurable modular robots
KW  - shape-shift
KW  - self-reconfigurable furniture
KW  - intuitive user interface
KW  - tangible user interface
KW  - Roombots shape
KW  - 3D shape scanning
KW  - SRMR system
KW  - Shape
KW  - Three-dimensional displays
KW  - User interfaces
KW  - Solid modeling
KW  - Robots
KW  - Planning
KW  - Buildings
KW  - tangible user interface
KW  - self-reconfigurable modular robots
KW  - deformable material
KW  - shape formation
DO  - 10.1109/ICRA.2018.8461248
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - One of the main strengths of self-reconfigurable modular robots (SRMR) is their ability to shape-shift and dynamically change their morphology. In the case of our SRMR system “Roombots”, these shapes can be quite arbitrary for a great variety of tasks while the major utility is envisioned to be self-reconfigurable furniture. As such, the ideas and inspirations from users quickly need to be translated into the final Roombots shape. This involves a multitude of separate processes and - most importantly - requires an intuitive user interface. Our current approach led to the development of a tangible user interface (TUI) which involves 3D-scanning of a shape formed by modeling clay and the necessary steps to prepare the digitized model to be formed by Roombots. The system is able to generate a solution in less than two minutes for our target use as demonstrated with various examples.
ER  - 

TY  - CONF
TI  - A Hierarchical Model for Action Recognition Based on Body Parts
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1978
EP  - 1985
AU  - Z. Shao
AU  - Y. Li
AU  - Y. Guo
AU  - J. Yang
AU  - Z. Wang
PY  - 2018
KW  - gesture recognition
KW  - image motion analysis
KW  - image representation
KW  - object detection
KW  - vectors
KW  - hierarchical model
KW  - body parts
KW  - human action recognition
KW  - human actions
KW  - human skeleton
KW  - discriminative body-parts selection
KW  - Fisher vectors
KW  - hierarchical representations
KW  - hierarchical RRV descriptors
KW  - Rotation and Relative Velocity descriptors
KW  - Skeleton
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Trajectory
KW  - Couplings
KW  - Biological system modeling
DO  - 10.1109/ICRA.2018.8460516
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - As increasing attention is paid on human action recognition from skeleton data, this paper focuses on such tasks by proposing a hierarchical model to discover the structure information of body-parts involved in human actions. Considering human actions as simultaneous motions of different body-parts of the human skeleton, we propose a hierarchical model to simultaneously apply discriminative body-parts selection at a same scale and group coupling of bundles of body-parts at different scales, while we decompose the human skeleton into a hierarchy of body-parts of varying scales. To represent such hierarchy of body-parts, we accordingly build a hierarchical RRV (Rotation and Relative Velocity) descriptors. The hierarchical representations encoded by Fisher vectors of the hierarchical RRV descriptors are properly formulated into the hierarchical model via the proposed hierarchical mixed norm, to apply sparse selection of body-parts and regularize the structure of such hierarchy of body-parts. The extensive evaluations on three challenging datasets demonstrate the effectiveness of our proposed approach, which achieves superior performance compared to state-of-the-art results on different sizes of datasets, showing it is more widely applicable than existing approaches.
ER  - 

TY  - CONF
TI  - 3D Human Pose Estimation in RGBD Images for Robotic Task Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1986
EP  - 1992
AU  - C. Zimmermann
AU  - T. Welschehold
AU  - C. Dornhege
AU  - W. Burgard
AU  - T. Brox
PY  - 2018
KW  - image colour analysis
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - service robots
KW  - monocular 3D pose estimation
KW  - service robot
KW  - color images
KW  - robust human keypoint detectors
KW  - robotic task learning
KW  - RGBD images
KW  - 3D human pose estimation
KW  - human teacher
KW  - PR2 robot
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Color
KW  - Pose estimation
KW  - Robot sensing systems
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8462833
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher.
ER  - 

TY  - CONF
TI  - Safe and Efficient Human-Robot Collaboration Part I: Estimation of Human Arm Motions
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1993
EP  - 1999
AU  - R. Weitschat
AU  - J. Ehrensperger
AU  - M. Maier
AU  - H. Aschemann
PY  - 2018
KW  - human-robot interaction
KW  - manipulator dynamics
KW  - motion control
KW  - path planning
KW  - human arm motions
KW  - fenceless robot cells
KW  - safety requirements
KW  - robot motions
KW  - control-oriented dynamic model
KW  - human-robot collaboration
KW  - cycle times
KW  - admissible path velocity
KW  - Collision avoidance
KW  - Manipulators
KW  - Kinematics
KW  - Collaboration
KW  - Safety
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461190
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - A significant barrier regarding a successful implementation of fenceless robot cells into manufacturing areas with humans is given by the inefficiency due to safety requirements. Robot motions have to be slowed down so that an unexpected collision with a human does not result in human injuries. This velocity reduction leads to longer cycle times and, hence, fenceless robot cells turn out as uneconomic. In this paper, a new approach for human-robot collaboration in assembly tasks is presented. For a better performance of the robot, methods are investigated on how the robot can exploit a maximum performance while maintaining the safety of collaborating humans. For this purpose, the kinematics and dynamics of a human arm are described by a control-oriented dynamic model to determine its capability and reachability. Successful experiments validate the dynamic model as well as a corresponding projection approach for calculating possible movements of the human arm that may lead to a collision with the robot. Finally, this information is used to calculate an admissible path velocity that minimizes the danger of human injuries.
ER  - 

TY  - CONF
TI  - Robust Real-Time 3D Person Detection for Indoor and Outdoor Applications
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2000
EP  - 2006
AU  - R. Hanten
AU  - P. Kuhlmann
AU  - S. Otte
AU  - A. Zell
PY  - 2018
KW  - graphics processing units
KW  - human computer interaction
KW  - mobile robots
KW  - object detection
KW  - robot vision
KW  - stereo image processing
KW  - mobile robotics
KW  - 3D sensor types
KW  - indoor applications
KW  - outdoor applications
KW  - outdoor scenarios
KW  - indoor scenarios
KW  - smaller robotic systems
KW  - single CPU thread
KW  - multiple CPU cores
KW  - human interaction
KW  - robotic applications
KW  - robust person detection
KW  - robust real-time 3D person detection
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Feature extraction
KW  - Visualization
KW  - Pipelines
KW  - Real-time systems
DO  - 10.1109/ICRA.2018.8461257
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Fast and robust person detection is one of the most important tasks for robotic applications involving human interaction. Particularly in mobile robotics this task is still challenging. Though there are already reliable and real-time capable approaches, they are usually computationally expensive. They either require GPUs or multiple CPU cores in order to work properly. Furthermore, some of the approaches are designed for special environments and sensor types, which reduces general applicability. In this work, we present a robust, generic and lightweight solution for real-time 3D person detection. Since our approach requires only a single CPU thread, it can be run as a background process and is suitable for smaller robotic systems. We demonstrate applicability to indoor and outdoor scenarios using different 3D sensor types separately. Moreover, we are able to show that the proposed method outperforms other state-of-the-art approaches, including a DCNN.
ER  - 

TY  - CONF
TI  - Pedestrian Feature Generation in Fish-Eye Images via Adversary
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2007
EP  - 2012
AU  - Y. Qian
AU  - M. Yang
AU  - C. Wang
AU  - B. Wang
PY  - 2018
KW  - driver information systems
KW  - feature extraction
KW  - image classification
KW  - object detection
KW  - pedestrians
KW  - FSTN
KW  - pedestrian detection
KW  - pedestrian feature generation
KW  - advanced driver assistance systems
KW  - ADAS
KW  - training
KW  - fish eye spatial transformer network
KW  - fish eye images
KW  - feature maps
KW  - robust
KW  - deformation
KW  - ETH
KW  - KITTI pedestrian datasets
KW  - adversarial network
KW  - fish eye pedestrian detectors
KW  - Detectors
KW  - Training
KW  - Feature extraction
KW  - Mathematical model
KW  - Cameras
KW  - Proposals
KW  - Convolution
DO  - 10.1109/ICRA.2018.8460565
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Pedestrian detection in fish-eye images is always an important problem in advanced driver assistance systems (ADAS). In conventional methods, pedestrian detectors will be trained using fish-eye images. But it is hard to collect and label enough fish-eye images manually. Therefore, a new strategy for training fish-eye pedestrian detectors using images from normal pedestrian datasets is proposed in this work. Concretely, Fish-eye Spatial Transformer Network (FSTN) is designed to generate pedestrian features in fish-eye images. FSTN aims to simulate distorted pedestrian features on the feature maps. Then the entire network is trained via adversary. FSTN is trained to generate examples which are difficult for pedestrian detectors to classify. So that the detectors are more robust to the deformation. FSTN can be embedded into state-of-the-art detectors easily. And the entire pedestrian detector, where the FSTN embedded, can be trained end to end via adversary. Moreover, experiments on ETH and KITTI pedestrian datasets show the slight accuracy improvement of pedestrian detection in fish-eye images using adversarial network compared with conventional methods.
ER  - 

TY  - CONF
TI  - Eye on You: Fusing Gesture Data from Depth Camera and Inertial Sensors for Person Identification
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2021
EP  - 2026
AU  - W. Chang
AU  - C. Wu
AU  - R. Y. Tsai
AU  - K. C. Lin
AU  - Y. Tseng
PY  - 2018
KW  - biometrics (access control)
KW  - cameras
KW  - gesture recognition
KW  - image fusion
KW  - EOY
KW  - asynchronous timing
KW  - coordinate calibration
KW  - environmental constraints
KW  - IoT applications
KW  - person identification
KW  - gesture data
KW  - fusion algorithms
KW  - inertial measurement unit
KW  - wearable sensors
KW  - 3D depth camera
KW  - data fusion approach
KW  - Eye On You
KW  - remote PID
KW  - reliable PID
KW  - recognition rate
KW  - Skeleton
KW  - Cameras
KW  - Sensors
KW  - Correlation
KW  - Iris recognition
KW  - Feature extraction
KW  - Motion segmentation
DO  - 10.1109/ICRA.2018.8462924
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Person identification (PID) is a key issue in many IoT applications. It has long been studied and achieved by technologies such as RFID and face/fingerprint/iris recognition. These approaches, however, have their limitations due to environmental constraints (such as lighting and obstacles) or require close contact to specific devices. Therefore, their recognition rates highly depend on use scenarios. To enable reliable and remote PID, in this work, we present EOY (Eye On You)1, a data fusion approach that combines two kinds of sensors, a 3D depth camera and wearable sensors embedded with inertial measurement unit (IMU). Since these two kinds of data share common features, we are able to fuse them to conduct PID. Further, the result can be transferred to a mobile platform (such as robot) since we have less constraints on devices. To realize EOY, we develop fusion algorithms to address practical challenges, such as asynchronous timing and coordinate calibration. The experimental evaluation shows that EOY can achieve the recognition rate of 95% and is very robust even in crowded areas.
ER  - 

TY  - CONF
TI  - Human Motion Capture Using a Drone
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2027
EP  - 2033
AU  - X. Zhou
AU  - S. Liu
AU  - G. Pavlakos
AU  - V. Kumar
AU  - K. Daniilidis
PY  - 2018
KW  - calibration
KW  - cameras
KW  - image motion analysis
KW  - image reconstruction
KW  - image sensors
KW  - mobile robots
KW  - robot vision
KW  - motion capture systems
KW  - calibrated cameras
KW  - indoor environments
KW  - on-board RGB camera
KW  - autonomously flying drone
KW  - 3D human MoCap
KW  - drone-based system
KW  - human motion capture
KW  - consumer drone
KW  - motion reconstruction
KW  - reconstruction algorithm
KW  - Cameras
KW  - Drones
KW  - Two dimensional displays
KW  - Three-dimensional displays
KW  - Image reconstruction
KW  - Tracking
KW  - Joints
DO  - 10.1109/ICRA.2018.8462830
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Current motion capture (MoCap) systems generally require markers and multiple calibrated cameras, which can be used only in constrained environments. In this work we introduce a drone-based system for 3D human MoCap. The system only needs an autonomously flying drone with an on-board RGB camera and is usable in various indoor and outdoor environments. A reconstruction algorithm is developed to recover full-body motion from the video recorded by the drone. We argue that, besides the capability of tracking a moving subject, a flying drone also provides fast varying viewpoints, which is beneficial for motion reconstruction. We evaluate the accuracy of the proposed system using our new DroCap dataset and also demonstrate its applicability for MoCap in the wild using a consumer drone.
ER  - 

TY  - CONF
TI  - Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2034
EP  - 2039
AU  - D. Isele
AU  - R. Rahimi
AU  - A. Cosgun
AU  - K. Subramanian
AU  - K. Fujimura
PY  - 2018
KW  - learning systems
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - road vehicles
KW  - autonomous vehicles
KW  - unsignaled intersections
KW  - Deep RL
KW  - intersection handling problem
KW  - deep reinforcement learning system
KW  - occluded intersections
KW  - active sensing behaviors
KW  - Autonomous vehicles
KW  - Automobiles
KW  - Machine learning
KW  - Safety
KW  - Navigation
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA.2018.8461233
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of Deep Reinforcement Learning to handle intersection problems. Using recent advances in Deep RL, we are able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate and have limited ability to generalize. We then explore a system's ability to learn active sensing behaviors to enable navigating safely in the case of occlusions. Our analysis, provides insight into the intersection handling problem, the solutions learned by the network point out several shortcomings of current rule-based methods, and the failures of our current deep reinforcement learning system point to future research directions.
ER  - 

TY  - CONF
TI  - Autonomous Vehicle Navigation in Rural Environments Without Detailed Prior Maps
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2040
EP  - 2047
AU  - T. Ort
AU  - L. Paull
AU  - D. Rus
PY  - 2018
KW  - collision avoidance
KW  - least squares approximations
KW  - mobile robots
KW  - recursive filters
KW  - robot vision
KW  - sensors
KW  - prior maps
KW  - positive societal impact
KW  - autonomous vehicle navigating
KW  - recursive filtering approach
KW  - navigate road networks
KW  - least-squares residual approach
KW  - vehicle frame
KW  - sensor-based perception system
KW  - global navigation
KW  - sparse topological maps
KW  - mapless driving framework
KW  - autonomous navigation
KW  - autonomous driving technology
KW  - transmit detailed maps
KW  - urban areas
KW  - rural environments
KW  - autonomous vehicle navigation
KW  - Roads
KW  - Navigation
KW  - Autonomous vehicles
KW  - Trajectory
KW  - Robot sensing systems
KW  - Reliability
DO  - 10.1109/ICRA.2018.8460519
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - State-of-the-art autonomous driving systems rely heavily on detailed and highly accurate prior maps. However, outside of small urban areas, it is very challenging to build, store, and transmit detailed maps since the spatial scales are so large. Furthermore, maintaining detailed maps of large rural areas can be impracticable due to the rapid rate at which these environments can change. This is a significant limitation for the widespread applicability of autonomous driving technology, which has the potential for an incredibly positive societal impact. In this paper, we address the problem of autonomous navigation in rural environments through a novel mapless driving framework that combines sparse topological maps for global navigation with a sensor-based perception system for local navigation. First, a local navigation goal within the sensor view of the vehicle is chosen as a waypoint leading towards the global goal. Next, the local perception system generates a feasible trajectory in the vehicle frame to reach the waypoint while abiding by the rules of the road for the segment being traversed. These trajectories are updated to remain in the local frame using the vehicle's odometry and the associated uncertainty based on the least-squares residual and a recursive filtering approach, which allows the vehicle to navigate road networks reliably, and at high speed, without detailed prior maps. We demonstrate the performance of the system on a full-scale autonomous vehicle navigating in a challenging rural environment and benchmark the system on a large amount of collected data.
ER  - 

TY  - CONF
TI  - Design of an Autonomous Racecar: Perception, State Estimation and System Integration
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2048
EP  - 2055
AU  - M. I. Valls
AU  - H. F. C. Hendrikx
AU  - V. J. F. Reijgwart
AU  - F. V. Meier
AU  - I. Sa
AU  - R. Dubé
AU  - A. Gawel
AU  - M. Bürki
AU  - R. Siegwart
PY  - 2018
KW  - mobile robots
KW  - road vehicles
KW  - state estimation
KW  - modular redundant sub-systems
KW  - lateral accelerations
KW  - longitudinal accelerations
KW  - flüela driverless
KW  - onboard sensing
KW  - Formula Student Driverless competition
KW  - system integration
KW  - autonomous racecar
KW  - Automobiles
KW  - State estimation
KW  - Robot sensing systems
KW  - Current measurement
KW  - Laser radar
KW  - Wheels
KW  - Global Positioning System
DO  - 10.1109/ICRA.2018.8462829
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper introduces jlüela driverless: the first autonomous racecar to win a Formula Student Driverless competition. In this competition, among other challenges, an autonomous racecar is tasked to complete 10 laps of a previously unknown racetrack as fast as possible and using only onboard sensing and computing. The key components of flüela's design are its modular redundant sub-systems that allow robust performance despite challenging perceptual conditions or partial system failures. The paper presents the integration of key components of our autonomous racecar, i.e., system design, EKF-based state estimation, LiDAR-based perception, and particle filter-based SLAM. We perform an extensive experimental evaluation on real-world data, demonstrating the system's effectiveness by outperforming the next-best ranking team by almost half the time required to finish a lap. The autonomous racecar reaches lateral and longitudinal accelerations comparable to those achieved by experienced human drivers.
ER  - 

TY  - CONF
TI  - Dynamic Occupancy Grid Prediction for Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic Labeling
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2056
EP  - 2063
AU  - S. Hoermann
AU  - M. Bach
AU  - K. Dietmayer
PY  - 2018
KW  - Bayes methods
KW  - convolution
KW  - feedforward neural nets
KW  - filtering theory
KW  - intelligent transportation systems
KW  - mobile robots
KW  - Monte Carlo methods
KW  - road traffic
KW  - time series
KW  - traffic engineering computing
KW  - unsupervised learning
KW  - complex interactions
KW  - dynamic occupancy grid prediction
KW  - urban autonomous driving
KW  - deep learning approach
KW  - long-term situation prediction
KW  - intelligent vehicles
KW  - complex downtown scenarios
KW  - multiple road users
KW  - motor vehicles
KW  - Bayesian filtering technique
KW  - environment representation
KW  - machine learning
KW  - deep convolutional neural network
KW  - spatially distributed velocity estimates
KW  - raw data sequence
KW  - input time series
KW  - multiple sensors
KW  - convolutional neural networks
KW  - road user interaction
KW  - pixel-wise balancing
KW  - static cells
KW  - dynamic cells
KW  - unsupervised learning character
KW  - pedestrians
KW  - bikes
KW  - distributed velocity estimation
KW  - Monte-Carlo simulation
KW  - Vehicle dynamics
KW  - Machine learning
KW  - Sensor fusion
KW  - Roads
KW  - Time series analysis
KW  - Laser radar
DO  - 10.1109/ICRA.2018.8460874
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Long-term situation prediction plays a crucial role for intelligent vehicles. A major challenge still to overcome is the prediction of complex downtown scenarios with multiple road users, e.g., pedestrians, bikes, and motor vehicles, interacting with each other. This contribution tackles this challenge by combining a Bayesian filtering technique for environment representation, and machine learning as long-term predictor. More specifically, a dynamic occupancy grid map is utilized as input to a deep convolutional neural network. This yields the advantage of using spatially distributed velocity estimates from a single time step for prediction, rather than a raw data sequence, alleviating common problems dealing with input time series of multiple sensors. Furthermore, convolutional neural networks have the inherent characteristic of using context information, enabling the implicit modeling of road user interaction. Pixel-wise balancing is applied in the loss function counteracting the extreme imbalance between static and dynamic cells. One of the major advantages is the unsupervised learning character due to fully automatic label generation. The presented algorithm is trained and evaluated on multiple hours of recorded sensor data and compared to Monte-Carlo simulation. Experiments show the ability to model complex interactions.
ER  - 

TY  - CONF
TI  - Realtime Vehicle and Pedestrian Tracking for Didi Udacity Self-Driving Car Challenge
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2064
EP  - 2069
AU  - A. Buyval
AU  - A. Gabdullin
AU  - R. Mustafin
AU  - I. Shimchik
PY  - 2018
KW  - image fusion
KW  - object tracking
KW  - optical radar
KW  - pedestrians
KW  - road vehicles
KW  - traffic engineering computing
KW  - road scene evaluation subsystem
KW  - vehicle tracking
KW  - image processing
KW  - LIDAR processing
KW  - pedestrian tracking
KW  - DiDi-Udacity Self-Driving Challenge datasets
KW  - frequency 25.0 Hz
KW  - Radar tracking
KW  - Laser radar
KW  - Cameras
KW  - Three-dimensional displays
KW  - Sensor fusion
DO  - 10.1109/ICRA.2018.8460913
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The efficiency and execution time of a road scene evaluation subsystem directly influences a self-driving car's control effectiveness. This article presents a novel approach to fuse data from various sensors (camera, LIDAR, radar, IMU) for pedestrian and vehicle tracking. Our approach, thanks to modern methods of image processing and power of GPU for LIDAR processing, achieves 25Hz update frequency for tracking. The system has been tested on the DiDi-Udacity Self-Driving Challenge datasets.
ER  - 

TY  - CONF
TI  - End-to-End Race Driving with Deep Reinforcement Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2070
EP  - 2075
AU  - M. Jaritz
AU  - R. de Charette
AU  - M. Toromanoff
AU  - E. Perot
AU  - F. Nashashibi
PY  - 2018
KW  - automobiles
KW  - cameras
KW  - computer games
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - traffic engineering computing
KW  - deep reinforcement learning
KW  - mediated perception
KW  - object recognition
KW  - scene understanding
KW  - learning strategies
KW  - RGB image
KW  - forward facing camera
KW  - car control
KW  - road structures
KW  - end-to-end race driving
KW  - reinforcement learning algorithm
KW  - legal speed limits
KW  - asynchronous actor critic framework
KW  - rally game
KW  - temperature 3.0 C
KW  - Automobiles
KW  - Training
KW  - Brakes
KW  - Games
KW  - Roads
KW  - Physics
KW  - Computer architecture
DO  - 10.1109/ICRA.2018.8460934
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.
ER  - 

TY  - CONF
TI  - Scalable Decision Making with Sensor Occlusions for Autonomous Driving
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2076
EP  - 2081
AU  - M. Bouton
AU  - A. Nakhaei
AU  - K. Fujimura
AU  - M. J. Kochenderfer
PY  - 2018
KW  - collision avoidance
KW  - decision making
KW  - Markov processes
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - road safety
KW  - road vehicles
KW  - road users
KW  - scalable decision making
KW  - sensor occlusions
KW  - POMDP solution techniques
KW  - optimal avoidance strategy
KW  - decomposition method
KW  - computational cost
KW  - partially observable Markov decision process
KW  - robust navigation
KW  - autonomous driving
KW  - Automobiles
KW  - Uncertainty
KW  - Roads
KW  - Acceleration
KW  - Autonomous vehicles
KW  - Approximation algorithms
KW  - Robot sensing systems
DO  - 10.1109/ICRA.2018.8460914
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Autonomous driving in urban areas requires avoiding other road users with only partial observability of the environment. Observations are only partial because obstacles can occlude the field of view of the sensors. The problem of robust and efficient navigation under uncertainty can be framed as a partially observable Markov decision process (POMDP). In order to bypass the computational cost of scaling the formulation to avoiding multiple road users, this paper demonstrates a decomposition method that leverages the optimal avoidance strategy for a single user. We evaluate the performance of two POMDP solution techniques augmented with the decomposition method for scenarios involving a pedestrian crosswalk and an intersection.
ER  - 

TY  - CONF
TI  - Situation Assessment for Planning Lane Changes: Combining Recurrent Models and Prediction
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2082
EP  - 2088
AU  - O. Scheel
AU  - L. Schwarz
AU  - N. Navab
AU  - F. Tombari
PY  - 2018
KW  - automobiles
KW  - driver information systems
KW  - intelligent transportation systems
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - prediction theory
KW  - recurrent neural nets
KW  - road safety
KW  - road traffic
KW  - road traffic control
KW  - fully autonomous cars
KW  - complex scenes
KW  - dynamic scenes
KW  - car following scenarios
KW  - situation assessment algorithm
KW  - lane changing
KW  - recurrent models
KW  - driver-assistance systems
KW  - bidirectional recurrent neural network
KW  - intelligent driver model
KW  - lane changes planning
KW  - maneuvers planning
KW  - driving situations classification
KW  - deep learning architecture
KW  - long short-term memory units
KW  - Automobiles
KW  - Planning
KW  - Machine learning
KW  - Predictive models
KW  - Prediction algorithms
KW  - Autonomous automobiles
DO  - 10.1109/ICRA.2018.8462838
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - One of the greatest challenges towards fully autonomous cars is the understanding of complex and dynamic scenes. Such understanding is needed for planning of maneuvers, especially those that are particularly frequent such as lane changes. While in recent years advanced driver-assistance systems have made driving safer and more comfortable, these have mostly focused on car following scenarios, and less on maneuvers involving lane changes. In this work we propose a situation assessment algorithm for classifying driving situations with respect to their suitability for lane changing. For this, we propose a deep learning architecture based on a Bidirectional Recurrent Neural Network, which uses Long Short-Term Memory units, and integrates a prediction component in the form of the Intelligent Driver Model. We prove the feasibility of our algorithm on the publicly available NGSIM datasets, where we outperform existing methods.
ER  - 

TY  - CONF
TI  - Design and Analysis of a Novel Underwater Glider - RoBuoy
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2089
EP  - 2094
AU  - T. Ranganathan
AU  - S. Aravazhi
AU  - S. Mishra
AU  - A. Thondiyath
PY  - 2018
KW  - autonomous underwater vehicles
KW  - design engineering
KW  - mathematical analysis
KW  - mechatronics
KW  - mobile robots
KW  - oceanographic equipment
KW  - variable buoyancy method
KW  - autonomous underwater vehicles
KW  - underwater robots
KW  - mechatronic system
KW  - underwater gliders RoBuoy
KW  - metallic bellows
KW  - integrated mathematical model
KW  - wings
KW  - open loop performance
KW  - power efficient
KW  - actuated metallic bellows
KW  - parts fouling
KW  - optimized dimensions
KW  - Actuators
KW  - Buoyancy
KW  - Bellows
KW  - Surges
KW  - Pistons
KW  - Unmanned underwater vehicles
KW  - Prototypes
DO  - 10.1109/ICRA.2018.8462921
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Underwater gliders are special class of autonomous underwater vehicles (AUVs) proven to be power efficient with better range and endurance compared to the conventional underwater robots. Most of the existing underwater gliders use `change of mass' based variable buoyancy (VB) method in which the overall system architecture and construction are complex. A novel underwater glider RoBuoy based on the `change of volume' concept of variable buoyancy method is presented here. RoBuoy uses actuated metallic bellows to change the volume which makes the system simple and modular in construction without any compromise in the performance. It uses minimal number of parts compared to the existing gliders which reduces the overall complexity of the system. Also, most of the conventional gliders use the external fluid for its working which may result in corrosion or fouling of parts and requires frequent maintenance. In the proposed glider, all the vital parts required for its working, apart from the sensing payloads are enclosed inside the hull, thereby increasing the durability. In this paper, a detailed design of RoBuoy is discussed with its possible modes of operation. An integrated mathematical model considering the individual dynamics of the actuator, hull/fuselage, and the wings has been developed and the open loop performance of the glider is studied at different input conditions. An experimental prototype has been designed and fabricated based on optimized dimensions, with the required mechatronic system. Experiments have been conducted and the results prove the feasibility of the concept.
ER  - 

TY  - CONF
TI  - Modeling Speed-, Load-, and Position-Dependent Friction Effects in Strain Wave Gears
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2095
EP  - 2102
AU  - A. Wahrburg
AU  - S. Klose
AU  - D. Clever
AU  - T. Groth
AU  - S. Moberg
AU  - J. Styrud
AU  - H. Ding
PY  - 2018
KW  - force control
KW  - friction
KW  - gears
KW  - industrial robots
KW  - position control
KW  - velocity control
KW  - strain wave gears
KW  - robotic joint
KW  - position-dependent friction effects
KW  - industrial robots
KW  - load-dependent friction effects
KW  - sensorless force control
KW  - speed-dependent friction effects
KW  - Friction
KW  - Load modeling
KW  - Torque
KW  - Gravity
KW  - Strain
KW  - Gears
KW  - Robots
DO  - 10.1109/ICRA.2018.8461043
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Strain wave gears are frequently used in small and medium size industrial robots. In order to describe and quantify friction effects in gearboxes of such type, a structurally simple, yet powerful model is proposed taking into account both speed-and load-dependent friction effects. Moreover, position-dependent disturbances in a robotic joint are considered. An identification procedure is presented that allows to separate the individual components of the model and identify them subsequently. The effectiveness of the model and identification procedure is validated using experimental data gathered from four different robotic joints of varying size. Furthermore, the benefits of improved friction modeling are shown by means of different applications, including smooth lead-through programming and sensorless force control.
ER  - 

TY  - CONF
TI  - Real-Time Identification of Robot Payload Using a Multirate Quaternion-Based Kalman Filter and Recursive Total Least-Squares
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2103
EP  - 2109
AU  - S. Farsoni
AU  - C. T. Landi
AU  - F. Ferraguti
AU  - C. Secchi
AU  - M. Bonfè
PY  - 2018
KW  - end effectors
KW  - industrial manipulators
KW  - Kalman filters
KW  - least squares approximations
KW  - recursive estimation
KW  - robot kinematics
KW  - least-squares process
KW  - multirate quaternion-based Kalman filter
KW  - recursive total least-squares
KW  - inertial parameters
KW  - rigid load
KW  - robot kinematics
KW  - inertial sensors
KW  - robot payload real-time identification
KW  - end-effector
KW  - industrial manipulator
KW  - Kalman filters
KW  - Quaternions
KW  - Robot kinematics
KW  - Service robots
KW  - Robot sensing systems
KW  - Estimation
DO  - 10.1109/ICRA.2018.8461167
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The paper describes an estimation and identification procedure that allows to reconstruct the inertial parameters of a rigid load attached to the end-effector of an industrial manipulator. In particular, the proposed method adopts a multirate quaternion-based Kalman filter, fusing measurements obtained from robot kinematics and inertial sensors at possibly different sampling frequencies, to estimate linear accelerations and angular velocities/accelerations of the load. Then, a recursive total least-squares (RTLS) process is executed to identify the load parameters. Both steps of the estimation and identification procedure are performed in real-time, without the need for offline post-processing of measured data.
ER  - 

TY  - CONF
TI  - Optimal Active Sensing with Process and Measurement Noise
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2118
EP  - 2125
AU  - M. Cognetti
AU  - P. Salaris
AU  - P. Robuffo Giordano
PY  - 2018
KW  - covariance matrices
KW  - eigenvalues and eigenfunctions
KW  - gradient methods
KW  - Kalman filters
KW  - mobile robots
KW  - noise measurement
KW  - nonlinear filters
KW  - observability
KW  - optimisation
KW  - path planning
KW  - Riccati equations
KW  - trajectory control
KW  - estimation algorithm
KW  - OG
KW  - nonnegligible process noise
KW  - largest eigenvalue
KW  - optimal active sensing
KW  - measurement noise
KW  - nonlinear differentially flat system
KW  - online gradient descent method
KW  - optimal trajectories
KW  - maximum estimation uncertainty
KW  - Kalman Filter
KW  - onboard sensors
KW  - observability gramian
KW  - inversely proportional
KW  - posteriori covariance matrix
KW  - continuous Riccati equation
KW  - unicycle robot
KW  - Robot sensing systems
KW  - Estimation
KW  - Trajectory
KW  - Eigenvalues and eigenfunctions
KW  - Observability
DO  - 10.1109/ICRA.2018.8460476
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The goal of this paper is to increase the estimation performance of an Extended Kalman Filter for a nonlinear differentially flat system by planning trajectories able to maximize the amount of information gathered by onboard sensors in presence of both process and measurement noises. In a previous work, we presented an online gradient descent method for planning optimal trajectories along which the smallest eigenvalue of the Observability Gramian (OG) is maximized. As the smallest eigenvalue of the OG is inversely proportional to the maximum estimation uncertainty, its maximization reduces the maximum estimation uncertainty of any estimation algorithm employed during motion. However, the OG does not consider the process noise that, instead, in several applications is far from being negligible. For this reason, this paper proposes a novel solution able to cope with non-negligible process noise: this is achieved by minimizing the largest eigenvalue of the a posteriori covariance matrix obtained by solving the Continuous Riccati Equation as a measure of the total available information. This minimization is expected to maximize the information gathered by the outputs while, at the same time, limiting as much as possible the negative effects of the process noise. We apply our method to a unicycle robot. The comparison between the novel method and the one of our previous work (which did not consider process noise) shows significant improvements in the obtained estimation accuracy.
ER  - 

TY  - CONF
TI  - Encoderless Gimbal Calibration of Dynamic Multi-Camera Clusters
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2126
EP  - 2133
AU  - C. L. Choi
AU  - J. Rebello
AU  - L. Koppel
AU  - P. Ganti
AU  - A. Das
AU  - S. L. Waslander
PY  - 2018
KW  - angular measurement
KW  - calibration
KW  - cameras
KW  - encoderless gimbal calibration
KW  - dynamic multiCamera Clusters
KW  - Dynamic Camera Clusters
KW  - multicamera systems
KW  - cameras
KW  - joint angle measurements
KW  - time-varying transformation
KW  - static camera
KW  - motor encoders
KW  - transformation chain
KW  - encoderless gimbal mechanism
KW  - online estimation
KW  - Cameras
KW  - Calibration
KW  - Robot vision systems
KW  - Estimation
KW  - Reluctance motors
KW  - Kinematics
KW  - Vehicle dynamics
DO  - 10.1109/ICRA.2018.8462920
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Dynamic Camera Clusters (DCCs) are multi-camera systems where one or more cameras are mounted on actuated mechanisms such as a gimbal. Existing methods for DCC calibration rely on joint angle measurements to resolve the time-varying transformation between the dynamic and static camera. This information is usually provided by motor encoders, however, joint angle measurements are not always readily available on off-the-shelf mechanisms. In this paper, we present an encoderless approach for DCC calibration which simultaneously estimates the kinematic parameters of the transformation chain as well as the unknown joint angles. We also demonstrate the integration of an encoderless gimbal mechanism with a state-of-the art VIO algorithm, and show the extensions required in order to perform simultaneous online estimation of the joint angles and vehicle localization state. The proposed calibration approach is validated both in simulation and on a physical DCC composed of a 2-DOF gimbal mounted on a UAV. Finally, we show the experimental results of the calibrated mechanism integrated into the OKVIS VIO package, and demonstrate successful online joint angle estimation while maintaining localization accuracy that is comparable to a standard static multi-camera configuration.
ER  - 

TY  - CONF
TI  - Stickman: Towards a Human Scale Acrobatic Robot
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2134
EP  - 2140
AU  - M. T. Pope
AU  - S. Christensen
AU  - D. Christensen
AU  - A. Simeonov
AU  - G. Imahara
AU  - G. Niemeyer
PY  - 2018
KW  - humanoid robots
KW  - laser ranging
KW  - mobile robots
KW  - motion control
KW  - pendulums
KW  - gravity-driven pendulum launch
KW  - two degree of freedom robot
KW  - autonomous robot
KW  - mobile robot
KW  - acrobatic techniques
KW  - acrobatic capability
KW  - laser range-finder
KW  - somersaulting stunts
KW  - gymnastic arts
KW  - human scale acrobatic robot
KW  - stickman
KW  - Robot sensing systems
KW  - Angular velocity
KW  - Mathematical model
KW  - Aerodynamics
DO  - 10.1109/ICRA.2018.8462836
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Human performers have developed impressive acrobatic techniques over thousands of years of practicing the gymnastic arts. At the same time, robots have started to become more mobile and autonomous, and can begin to imitate these stunts in dramatic and informative ways. We present a simple two degree of freedom robot that uses a gravity-driven pendulum launch and produces a variety of somersaulting stunts. The robot uses an IMU and a laser range-finder to estimate its state mid-flight and actuates to change its motion both on and and off the pendulum. We discuss the dynamics of this behavior in a framework of acrobatic capability and present experimental results.
ER  - 

TY  - CONF
TI  - 3D Lidar-IMU Calibration Based on Upsampled Preintegrated Measurements for Motion Distortion Correction
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2149
EP  - 2155
AU  - C. Le Gentil
AU  - T. Vidal-Calleja
AU  - S. Huang
PY  - 2018
KW  - calibration
KW  - cameras
KW  - image motion analysis
KW  - optical radar
KW  - optimisation
KW  - probability
KW  - sampling methods
KW  - stereo image processing
KW  - interpolated inertial measurements
KW  - lidar scan
KW  - lidar point-to-plane distances
KW  - upsampled preintegrated measurements
KW  - motion distortion correction
KW  - probabilistic framework
KW  - lidar-IMU sensing system
KW  - motion distortion
KW  - on-manifold optimisation
KW  - 3D lidar-IMU calibration
KW  - Laser radar
KW  - Calibration
KW  - Distortion
KW  - Robot sensing systems
KW  - Three-dimensional displays
KW  - Distortion measurement
KW  - Motion measurement
DO  - 10.1109/ICRA.2018.8460179
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we present a probabilistic framework to recover the extrinsic calibration parameters of a lidar-IMU sensing system. Unlike global-shutter cameras, lidars do not take single snapshots of the environment. Instead, lidars collect a succession of 3D-points generally grouped in scans. If these points are assumed to be expressed in a common frame, this becomes an issue when the sensor moves rapidly in the environment causing motion distortion. The fundamental idea of our proposed framework is to use preintegration over interpolated inertial measurements to characterise the motion distortion in each lidar scan. Moreover, by using a set of planes as a calibration target, the proposed method makes use of lidar point-to-plane distances to jointly calibrate and localise the system using on-manifold optimisation. The calibration does not rely on a predefined target as arbitrary planes are detected and modelled in the first lidar scan. Simulated and real data are used to show the effectiveness of the proposed method.
ER  - 

TY  - CONF
TI  - High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2156
EP  - 2163
AU  - K. Park
AU  - S. Kim
AU  - K. Sohn
PY  - 2018
KW  - feedforward neural nets
KW  - optical radar
KW  - radar imaging
KW  - stereo image processing
KW  - LiDAR
KW  - compact convolution module
KW  - dense stereo depth information
KW  - sparse 3D LiDAR
KW  - deep convolutional neural network architecture
KW  - stereo fusion
KW  - high-precision depth estimation
KW  - off-the-shelf stereo algorithm
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Estimation
KW  - Computer architecture
KW  - Sensors
KW  - Reliability
KW  - Image color analysis
DO  - 10.1109/ICRA.2018.8461048
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LiDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.
ER  - 

TY  - CONF
TI  - Sampled-Point Network for Classification of Deformed Building Element Point Clouds
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2164
EP  - 2169
AU  - J. Chen
AU  - Y. K. Cho
AU  - J. Ueda
PY  - 2018
KW  - disasters
KW  - feature extraction
KW  - image classification
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - robot vision
KW  - object recognition
KW  - post-disaster urban areas
KW  - search-and-rescue robots
KW  - deformed building element point clouds
KW  - point network
KW  - synthetically-deformed object datasets
KW  - point sorting
KW  - point coordinates
KW  - classification network
KW  - deformed building elements
KW  - 3D class recognition
KW  - point cloud input
KW  - disaster relief operations
KW  - potentially-deformed objects
KW  - unstructured environments
KW  - point cloud data
KW  - 3D point cloud
KW  - physical site information
KW  - Three-dimensional displays
KW  - Strain
KW  - Object recognition
KW  - Deformable models
KW  - Machine learning
KW  - Feature extraction
KW  - Buildings
DO  - 10.1109/ICRA.2018.8461095
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Search-and-rescue (SAR) robots operating in post-disaster urban areas need to accurately identify physical site information to perform navigation, mapping and manipulation tasks. This can be achieved by acquiring a 3D point cloud of the environment and performing object recognition from the point cloud data. However, this task is complicated by the unstructured environments and potentially-deformed objects encountered during disaster relief operations. Current 3D object recognition methods rely on point cloud input acquired under suitable conditions and do not consider deformations such as outlier noise, bending and truncation. This work introduces a deep learning architecture for 3D class recognition from point clouds of deformed building elements. The classification network, consisting of stacked convolution and average pooling layers applied directly to point coordinates, was trained using point clouds sampled from a database of mesh models. The proposed method achieves robustness to input variability using point sorting, resampling, and rotation normalization techniques. Experimental results on synthetically-deformed object datasets show that the proposed method outperforms the conventional deep learning methods in terms of classification accuracy and computational efficiency.
ER  - 

TY  - CONF
TI  - Recognizing Objects in-the-Wild: Where do we Stand?
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2170
EP  - 2177
AU  - M. Reza Loghmani
AU  - B. Caputo
AU  - M. Vincze
PY  - 2018
KW  - cameras
KW  - image classification
KW  - image colour analysis
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - object recognition
KW  - robot vision
KW  - multiview object dataset
KW  - RGB-D camera
KW  - deep convolutional networks
KW  - Web images
KW  - robotic system
KW  - autonomous agents
KW  - good visual perceptual systems
KW  - robotic vision research communities
KW  - human-populated environments
KW  - robot vision
KW  - real-life robotic data
KW  - object classification
KW  - deep representations
KW  - object recognition algorithms
KW  - real-life application
KW  - mobile robot
KW  - Task analysis
KW  - Clutter
KW  - Visualization
KW  - Mobile robots
KW  - Cameras
KW  - Robot vision systems
DO  - 10.1109/ICRA.2018.8460985
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The ability to recognize objects is an essential skill for a robotic system acting in human-populated environments. Despite decades of effort from the robotic and vision research communities, robots are still missing good visual perceptual systems, preventing the use of autonomous agents for realworld applications. The progress is slowed down by the lack of a testbed able to accurately represent the world perceived by the robot in-the-wild. In order to fill this gap, we introduce a large-scale, multi-view object dataset collected with an RGB-D camera mounted on a mobile robot. The dataset embeds the challenges faced by a robot in a real-life application and provides a useful tool for validating object recognition algorithms. Besides describing the characteristics of the dataset, the paper evaluates the performance of a collection of well-established deep convolutional networks on the new dataset and analyzes the transferability of deep representations from Web images to robotic data. Despite the promising results obtained with such representations, the experiments demonstrate that object classification with real-life robotic data is far from being solved. Finally, we provide a comparative study to analyze and highlight the open challenges in robot vision, explaining the discrepancies in the performance.
ER  - 

TY  - CONF
TI  - A Controlled-Delay Event Camera Framework for On-Line Robotics
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2178
EP  - 2183
AU  - A. Glover
AU  - V. Vasco
AU  - C. Bartolozzi
PY  - 2018
KW  - cameras
KW  - humanoid robots
KW  - image sensors
KW  - mobile robots
KW  - robot vision
KW  - controlled-delay event camera framework
KW  - dynamic robotics
KW  - low latency response
KW  - high dynamic range
KW  - inherent compression
KW  - visual signal
KW  - real-time performance
KW  - off-line datasets
KW  - camera resolution
KW  - latency-free operation
KW  - event-driven framework
KW  - iCub robot
KW  - algorithm processing rate
KW  - actual event-rate
KW  - algorithm performance
KW  - Cameras
KW  - Robot vision systems
KW  - Delays
KW  - Corner detection
KW  - Visualization
DO  - 10.1109/ICRA.2018.8460541
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Event cameras offer many advantages for dynamic robotics due to their low latency response to motion, high dynamic range, and inherent compression of the visual signal. Many algorithms easily achieve real-time performance when testing on off-line datasets, however with an increase in camera resolution and applications on fast-moving robots, latency-free operation is not guaranteed. The event-rate is not constant, but is proportional to the amount of movement in the scene, or the velocity of the camera itself. Recently, algorithms have instead reported a maximum event-rate that can be achieved in real-time. In this paper we present the event-driven framework used on the iCub robot, which closes the loop between algorithm processing rate and the actual event-rate of the camera in order to smoothly control and limit the latency, while allowing the algorithm to degrade gracefully when large bursts of events occur. We show two algorithms that process events differently from each other and demonstrate the trade-off between latency and algorithm performance that the framework provides.
ER  - 

TY  - CONF
TI  - Gemsketch: Interactive Image-Guided Geometry Extraction from Point Clouds
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2184
EP  - 2191
AU  - M. Maghoumi
AU  - J. J. LaVioia
AU  - K. Desingh
AU  - O. Chadwicke Jenkins
PY  - 2018
KW  - feature extraction
KW  - interactive systems
KW  - solid modelling
KW  - multiple-view point clouds
KW  - cuboids
KW  - interactive system
KW  - interactive image-guided geometry extraction
KW  - gemsketch
KW  - Three-dimensional displays
KW  - Shape
KW  - Geometry
KW  - Solid modeling
KW  - Tools
KW  - Cameras
KW  - Data mining
DO  - 10.1109/ICRA.2018.8460532
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We introduce an interactive system for extracting the geometries of generalized cylinders and cuboids from single-or multiple-view point clouds. Our proposed method is intuitive and only requires the object's silhouettes to be traced by the user. Leveraging the user's perceptual understanding of what an object looks like, our proposed method is capable of extracting accurate models, even in the presence of occlusion, clutter or incomplete point cloud data, while preserving the original object's details and scale. We demonstrate the merits of our proposed method through a set of experiments on a public RGB-D dataset. We extracted 16 objects from the dataset using at most two views of each object. Our extracted models represent a high degree of visual similarity to the original objects. Further, we achieved a mean normalized Hausdorff distance of 5.66% when comparing our extracted models with the dataset's ground truths.
ER  - 

TY  - CONF
TI  - Where can i do this? Geometric Affordances from a Single Example with the Interaction Tensor
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2192
EP  - 2199
AU  - E. Ruiz
AU  - W. Mayol-Cuevas
PY  - 2018
KW  - computational geometry
KW  - feature extraction
KW  - image capture
KW  - image colour analysis
KW  - image representation
KW  - tensors
KW  - geometric affordance
KW  - interaction tensor
KW  - tensor field representation
KW  - bisector surface representation
KW  - surface points
KW  - directional vectors
KW  - cognitive robots
KW  - autonomous robots
KW  - RGB-D sensors
KW  - Tensile stress
KW  - Robots
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Task analysis
KW  - Shape
KW  - Tools
DO  - 10.1109/ICRA.2018.8462835
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper introduces and evaluates a new tensor field representation to express the geometric affordance of one object relative to another, a key competence for Cognitive and Autonomous robots. We expand the bisector surface representation to one that is weight-driven and that retains the provenance of surface points with directional vectors. We also incorporate the notion of affordance keypoints which allow for faster decisions at a point of query and with a compact and straightforward descriptor. Using a single interaction example, we are able to generalize to previously-unseen scenarios; both synthetic and also real scenes captured with RGB-D sensors. Evaluations also include crowdsourcing comparisons that confirm the validity of our affordance proposals, which agree on average 84 % of the time with human judgments, that is 20-40 % better than the baseline methods.
ER  - 

TY  - CONF
TI  - A Low-Cost Navigation Strategy for Yield Estimation in Vineyards
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2200
EP  - 2205
AU  - G. Riggio
AU  - C. Fantuzzi
AU  - C. Secchi
PY  - 2018
KW  - agriculture
KW  - image colour analysis
KW  - image resolution
KW  - object detection
KW  - yield estimation
KW  - grape varieties
KW  - vineyard management
KW  - low-cost navigation strategy
KW  - navigation algorithm
KW  - grape pictures
KW  - low-cost autonomous system
KW  - RGB camera
KW  - RGB image processing
KW  - Navigation
KW  - Pipelines
KW  - Yield estimation
KW  - Cameras
KW  - Robot vision systems
KW  - Lasers
DO  - 10.1109/ICRA.2018.8462839
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Accurate yield estimation is very important for improving the vineyard management, the quality of the grapes and the health of the vines. The most common systems use RGB image processing for achieving a good estimation. In order to collect images, robots or farming vehicles can be equipped with a RGB camera. In this paper, we propose a low-cost autonomous system which can navigate through a vineyard while collecting grape pictures in order to provide a yield estimation. Our system uses only a laser scanner to detect the row and follows it until its end, then it navigates towards the next one, exploiting the knowledge of the vineyard. The navigation algorithm was tested both in simulation and in a real environment with good results. Furthermore, a yield estimation of two different grape varieties is presented.
ER  - 

TY  - CONF
TI  - Object Detection for Cattle Gait Tracking
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2206
EP  - 2213
AU  - J. Gardenier
AU  - J. Underwood
AU  - C. Clark
PY  - 2018
KW  - convolution
KW  - dairy products
KW  - dairying
KW  - feature extraction
KW  - feedforward neural nets
KW  - gait analysis
KW  - image sampling
KW  - object detection
KW  - veterinary medicine
KW  - kinematic gait features
KW  - object detection
KW  - cattle gait tracking
KW  - health issue
KW  - locomotion score
KW  - widespread commercial adoption
KW  - sensor configuration
KW  - flight sensors
KW  - rotary milking dairy
KW  - lameness detection systems
KW  - cattle kinematics
KW  - CNN
KW  - cartesian space
KW  - Cows
KW  - Feature extraction
KW  - Kinematics
KW  - Laser radar
KW  - Robot sensing systems
KW  - Three-dimensional displays
KW  - Australia
DO  - 10.1109/ICRA.2018.8460523
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Lameness in cattle is a health issue where gait is modified to minimise pain. Cattle are currently visually assessed for locomotion score, which provides the degree of lameness for individual animals. This subjective method is costly in terms of labour, and its level of accuracy and ability to detect small changes in locomotion that is critical for early detection of lameness and associated intervention. Current automatic lameness detection systems found in literature have not yet met the ultimate goal of widespread commercial adoption. We present a sensor configuration to record cattle kinematics towards automatic lameness detection. This configuration features four Time of Flight sensors to view cattle from above and from one side as they exit an automatic rotary milking dairy. Two dimensional near infrared images sampled from 223 cows passing through the system were used to train a Faster R-CNN to detect hooves (F1-score = 0.90) and carpal/tarsal joints (Fl-score = 0.85). The depth images were used to project these detected key points into Cartesian space where they were tracked to obtain individual trajectories per limb. The results show that kinematic gait features can be successfully obtained as a first and important step towards objective, accurate, automatic lameness detection.
ER  - 

TY  - CONF
TI  - Routing Algorithms for Robot Assisted Precision Irrigation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2221
EP  - 2228
AU  - T. C. Thayer
AU  - S. Vougioukas
AU  - K. Goldberg
AU  - S. Carpin
PY  - 2018
KW  - computational complexity
KW  - graph theory
KW  - greedy algorithms
KW  - irrigation
KW  - mobile robots
KW  - orienteering problem
KW  - NP-hard
KW  - routing algorithms
KW  - robot assisted precision irrigation
KW  - temporal budget
KW  - possible motions
KW  - spatially distributed sites
KW  - battery charge
KW  - optimization problem
KW  - irrigation adjustments
KW  - robots navigate
KW  - commercial vineyard
KW  - Irrigation
KW  - Routing
KW  - Approximation algorithms
KW  - Robot sensing systems
KW  - Navigation
DO  - 10.1109/ICRA.2018.8461242
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - When robots navigate through vineyards to perform irrigation adjustments, an optimization problem emerges whereby robots are tasked with performing adjustments having the highest cumulative outcome within a given temporal budget due to limited battery charge. To this end, the robot needs to reach a set of spatially distributed sites, and the specific structure of the vineyard imposes various constraints on possible motions. In this paper we first demonstrate that this type of orienteering problem remains NP-hard even for the restricted class of graphs associated with precision irrigation. Then, we devise and analyze two greedy heuristics informed by the problem we consider. Finally, these algorithms are evaluated on settings associated with a commercial vineyard and we show that our methods favorably compare to solutions proposed in the past.
ER  - 

TY  - CONF
TI  - Real-Time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2229
EP  - 2235
AU  - A. Milioto
AU  - P. Lottes
AU  - C. Stachniss
PY  - 2018
KW  - agricultural machinery
KW  - agrochemicals
KW  - convolution
KW  - crops
KW  - feedforward neural nets
KW  - image colour analysis
KW  - image segmentation
KW  - industrial robots
KW  - robot vision
KW  - precision agriculture robots leveraging background knowledge
KW  - precision farming robots
KW  - CNN-based semantic segmentation
KW  - crop fields
KW  - sugar beet plants
KW  - RGB data
KW  - vegetation indexes
KW  - real-time semantic segmentation
KW  - trigger weeding actions
KW  - agricultural robot operator
KW  - Agriculture
KW  - Vegetation mapping
KW  - Semantics
KW  - Real-time systems
KW  - Soil
KW  - Indexes
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8460962
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Precision farming robots, which target to reduce the amount of herbicides that need to be brought out in the fields, must have the ability to identify crops and weeds in real time to trigger weeding actions. In this paper, we address the problem of CNN-based semantic segmentation of crop fields separating sugar beet plants, weeds, and background solely based on RGB data. We propose a CNN that exploits existing vegetation indexes and provides a classification in real time. Furthermore, it can be effectively re-trained to so far unseen fields with a comparably small amount of training data. We implemented and thoroughly evaluated our system on a real agricultural robot operating in different fields in Germany and Switzerland. The results show that our system generalizes well, can operate at around 20 Hz, and is suitable for online operation in the fields.
ER  - 

TY  - CONF
TI  - Robustly Adjusting Indoor Drip Irrigation Emitters with the Toyota HSR Robot
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2236
EP  - 2243
AU  - R. Berenstein
AU  - R. Fox
AU  - S. McKinley
AU  - S. Carpin
AU  - K. Goldberg
PY  - 2018
KW  - computer vision
KW  - grippers
KW  - irrigation
KW  - manipulators
KW  - mobile robots
KW  - computer vision
KW  - build-in hand camera
KW  - modular Emitter Localization Device
KW  - lightweight Emitter Localization Device
KW  - Toyota HSR mobile manipulator robot
KW  - commercial buildings
KW  - indoor plants
KW  - Toyota HSR robot
KW  - indoor drip irrigation emitters
KW  - emitter axis
KW  - gripper axis
KW  - Cameras
KW  - Irrigation
KW  - Robot vision systems
KW  - Manipulators
KW  - Grippers
DO  - 10.1109/ICRA.2018.8460969
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Indoor plants in homes and commercial buildings such as malls, offices, airports, and hotels, can benefit from precision irrigation to maintain healthy growth and reduce water consumption. As active valves are too costly, and ongoing precise manual adjustment of drip emitters is impractical, we explore how the Toyota HSR mobile manipulator robot can autonomously adjust low-cost passive emitters. To provide sufficient accuracy for gripper alignment, we designed a lightweight, modular Emitter Localization Device (ELD) with cameras and LEDs that can be non-invasively mounted on the arm. This paper presents details of the design, algorithms, and experiments with adjusting emitters using a two-phase procedure: (1) aligning the robot base using the build-in hand camera, and (2) aligning the gripper axis with the emitter axis using the ELD. We report success rates and sensitivity analysis to tune computer vision parameters and joint motor gains. Experiments suggest that emitters can be adjusted with 95 % success rate in approximately 20 seconds.
ER  - 

TY  - CONF
TI  - Preliminary Study of Twisted String Actuation Through a Conduit Toward Soft and Wearable Actuation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2260
EP  - 2265
AU  - B. Suthar
AU  - M. Usman
AU  - H. Seong
AU  - I. Gaponov
AU  - J. Ryu
PY  - 2018
KW  - actuators
KW  - cables (mechanical)
KW  - friction
KW  - lubrication
KW  - mobile robots
KW  - wearable robots
KW  - twisted string actuation
KW  - conduit
KW  - wearable actuation
KW  - TSAs
KW  - modern engineering
KW  - robotic applications
KW  - conventional cable sliding transmission
KW  - lubricated twisting
KW  - friction
KW  - mobile robots
KW  - Friction
KW  - Force
KW  - Mathematical model
KW  - Cable shielding
KW  - DC motors
KW  - Power cables
KW  - Robots
DO  - 10.1109/ICRA.2018.8460589
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Twisted string actuators (TSAs) are gaining popularity in modern engineering and robotic applications. However, in conventional actuators of this type, the twisted part of the string should not be in contact with any surfaces or objects because this may interfere with the propagation of twisting. This imposes significant constraint on potential applications of TSAs. In this paper, we investigated the feasibility of using TSAs inside conduit and demonstrated that the twists of the string can be fully propagated through the sheath and that consistent periodic behavior of the twisted string can be achieved. In addition, we investigated input-output position and force characteristics of TSAs for various deflection angles of the conduit, effect of lubrication on transmission efficiency, and compared it with conventional cable sliding transmission. We found that TSA has higher transmission efficiency than sliding due to decreased friction between the string and conduit, which is further improved by lubrication. We have managed to achieve 85 % of force transmission efficiency for the case of lubricated twisting, as opposed to the 71.74% for lubricated sliding.
ER  - 

TY  - CONF
TI  - Stiffness Decomposition and Design Optimization of Under-Actuated Tendon-Driven Robotic Systems
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2266
EP  - 2272
AU  - M. Kim
AU  - J. Park
AU  - J. Kim
AU  - M. Kim
PY  - 2018
KW  - compliant mechanisms
KW  - design engineering
KW  - elasticity
KW  - manipulator dynamics
KW  - motion control
KW  - stiffness decomposition
KW  - design optimization
KW  - systematic design framework
KW  - under-actuated tendon-driven robotic systems
KW  - free motion
KW  - contact task
KW  - configuration space
KW  - UATD robotic systems
KW  - actuation
KW  - active tendons
KW  - un-actuated space
KW  - passive compliance
KW  - UATD robotic finger
KW  - contact wrench
KW  - Robots
KW  - Tendons
KW  - Pulleys
KW  - Routing
KW  - Springs
KW  - Grasping
KW  - Actuators
DO  - 10.1109/ICRA.2018.8462906
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a novel systematic design framework for general under-actuated tendon-driven (UATD) robotic systems to exhibit desired behaviors both during the free motion and the contact task. For this, we propose stiffness decomposition, which enables us to completely decompose the configuration space of the UATD robotic systems into the actuated space (with full actuation via active tendons) and the un-actuated space (with no actuation, only with passive compliance and contact wrench). The behavior in the actuated space is then fully-controllable, thus, the attainment of the desired behaviors, particularly those during the contact task, hinges upon that in the un-actuated space. For this, relying on the stiffness decomposition, we optimize the design parameters (e.g., tendon routing, pulley radius, passive compliance, etc.) to ensure the deformation in the un-actuated space as directional (e.g., for adaptive grasping) and minimized (e.g., pushing with posture maintained) for different contact wrench sets as possible, while also rendering the free motion to be as compliant and backdrivable as possible. The presented framework is then applied to design a UATD robotic finger and experimentally verified with the robot able to mimic the behavior of human index finger both during the free motion and pinch-pushing.
ER  - 

TY  - CONF
TI  - Design and Development of Effective Transmission Mechanisms on a Tendon Driven Hand Orthosis for Stroke Patients
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2281
EP  - 2287
AU  - S. Park
AU  - L. Weber
AU  - L. Bishop
AU  - J. Stein
AU  - M. Ciocarlie
PY  - 2018
KW  - biomechanics
KW  - fatigue
KW  - orthotics
KW  - patient rehabilitation
KW  - three-dimensional printing
KW  - 3D-printed artificial finger
KW  - moment arms
KW  - fatigue
KW  - spasticity
KW  - wearable tendon-driven hand orthosis
KW  - exoskeletons
KW  - transmission efficiency
KW  - low-profile design
KW  - effective transmission mechanisms
KW  - stroke patients
KW  - joint angle characteristics
KW  - finger joints
KW  - effective force transmission
KW  - Tendons
KW  - Force
KW  - Exoskeletons
KW  - Robots
KW  - Electron tubes
KW  - Task analysis
KW  - Thumb
DO  - 10.1109/ICRA.2018.8461069
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Tendon-driven hand orthoses have advantages over exoskeletons with respect to wearability and safety because of their low-profile design and ability to fit a range of patients without requiring custom joint alignment. However, no existing study on a wearable tendon-driven hand orthosis for stroke patients presents evidence that such devices can overcome spasticity given repeated use and fatigue, or discusses transmission efficiency. In this study, we propose two designs that provide effective force transmission by increasing moment arms around finger joints. We evaluate the designs with geometric models and experiment using a 3D-printed artificial finger to find force and joint angle characteristics of the suggested structures. We also perform clinical tests with stroke patients to demonstrate the feasibility of the designs. The testing supports the hypothesis that the proposed designs efficiently elicit extension of the digits in patients with spasticity as compared to existing baselines.
ER  - 

TY  - CONF
TI  - Discovering a Library of Rhythmic Gaits for Spherical Tensegrity Locomotion
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2290
EP  - 2295
AU  - C. Rennie
AU  - K. E. Bekris
PY  - 2018
KW  - Bayes methods
KW  - gait analysis
KW  - Gaussian processes
KW  - legged locomotion
KW  - Monte Carlo methods
KW  - motion control
KW  - pattern classification
KW  - regression analysis
KW  - significant control challenges
KW  - high-dimensionality
KW  - nonlinear nature
KW  - effective parameterization
KW  - rhythmic gaits
KW  - periodic control signals
KW  - rhythmic control signals
KW  - gait parameters
KW  - parameter space
KW  - Bayesian Optimization
KW  - parameter sample
KW  - gait discovery process
KW  - spherical tensegrity locomotion
KW  - tensegrity robots
KW  - rigid elements
KW  - soft elements
KW  - locomotion capabilities
KW  - central pattern generators
KW  - Gaussian Process regression model
KW  - Robots
KW  - Optimization
KW  - Aerospace electronics
KW  - Bayes methods
KW  - Shape
KW  - Angular velocity
KW  - Libraries
DO  - 10.1109/ICRA.2018.8460873
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Tensegrity robots, which combine both rigid and soft elements, provide exciting new locomotion capabilities but introduce significant control challenges given their high-dimensionality and non-linear nature. This work first defines an effective parameterization of a spherical tensegrity for generating rhythmic gaits based on Central Pattern Generators (cp G). This allows the definition of periodic and rhythmic control signals, while exposing only five gait parameters. Then, this work proposes a framework for optimizing such gaits by exploring the parameter space through Bayesian Optimization on an underlying Gaussian Process regression model. The objective is to provide gaits that allow the platform to move along different directions with high velocity. Additionally, kNN binary classifiers are trained to estimate whether a parameter sample will result in an effective gait. The classification biases the sampling toward subspaces likely to yield effective gaits. An asynchronous communication layer is defined between the optimization and classification processes. The proposed gait discovery process is shown to efficiently optimize the parameters of gaits defined given the novel CPG architecture and outperforms less holistic approaches and Monte Carlo sampling.
ER  - 

TY  - CONF
TI  - Line-Based Global Localization of a Spherical Camera in Manhattan Worlds
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2296
EP  - 2303
AU  - T. Goto
AU  - S. Pathak
AU  - Y. Ji
AU  - H. Fujii
AU  - A. Yamashita
AU  - H. Asama
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - gradient methods
KW  - Hough transforms
KW  - SLAM (robots)
KW  - 3D line map
KW  - complicated six degrees of freedom search
KW  - 2D line information
KW  - line-based global localization
KW  - spherical Hough representation
KW  - 6 DoF localization process
KW  - Manhattan world assumption
KW  - 3D-2D line correspondences
KW  - spherical-gradient filtering
KW  - spherical image
KW  - indoor environment
KW  - camera position
KW  - global environmental information
KW  - spherical camera
KW  - indoor localization
KW  - indoor spaces
KW  - Cameras
KW  - Three-dimensional displays
KW  - Image edge detection
KW  - Robustness
KW  - Robot vision systems
KW  - Solid modeling
KW  - Estimation
DO  - 10.1109/ICRA.2018.8460920
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Localization is an important task for mobile service robots in indoor spaces. In this research, we propose a novel technique for indoor localization using a spherical camera. Spherical cameras can obtain a complete view of the surroundings allowing the use of global environmental information. We take advantage of this in order to estimate camera position and the orientation with respect to a known 3D line map of an indoor environment, using a single image. We robustly extract 2D line information from the spherical image via spherical-gradient filtering and match it to 3D line information in the line map. Our method requires no information about the 3D-2D line correspondences. In order to avoid a complicated six degrees of freedom (6 DoF) search for position and orientation, we use a Manhattan world assumption to decompose the line information in the image. The 6 DoF localization process is divided into two phases. First, we estimate the orientation by extracting the three principle directions from the image. Then, the position is estimated by robustly matching the distribution of lines between the image and the 3D model via a spherical Hough representation. This decoupled search can robustly localize a spherical camera using a single image, as we demonstrate experimentally.
ER  - 

TY  - CONF
TI  - Optimizing Placement and Number of RF Beacons to Achieve Better Indoor Localization
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2304
EP  - 2311
AU  - R. Falque
AU  - M. Patel
AU  - J. Biehl
PY  - 2018
KW  - indoor radio
KW  - mobility management (mobile radio)
KW  - optimisation
KW  - RF beacons
KW  - RF signal propagation
KW  - indoor infrastructure
KW  - cost function
KW  - Radio Frequency beacons
KW  - indoor localization
KW  - Sensors
KW  - Optimization
KW  - Wireless sensor networks
KW  - Lattices
KW  - Genetic algorithms
KW  - Radio frequency
KW  - RF signals
KW  - Indoor localization
KW  - Beacon deployment
KW  - Bluetooth Low Energy (BLE)
KW  - k-Coverage
KW  - Wireless sensor networks (WSN)
DO  - 10.1109/ICRA.2018.8460202
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we propose a novel solution to optimize the deployment of Radio Frequency (RF) beacons for the purpose of indoor localization. We propose a system that optimizes both the number of beacons and their placement in a given environment. We propose a novel cost-function, called CovBsm, that allows to simultaneously optimize the 3-coverage while maximizing the beacon spreading. Using this cost function, we propose a framework that maximize both the number of beacons and their placement in a given environment. The proposed solution accounts for the indoor infrastructure and its influence on the RF signal propagation by embedding a realistic simulator into the optimization process.
ER  - 

TY  - CONF
TI  - Robust Target-Relative Localization with Ultra-Wideband Ranging and Communication
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2312
EP  - 2319
AU  - T. Nguyen
AU  - A. Hanif Zaini
AU  - C. Wang
AU  - K. Guo
AU  - L. Xie
PY  - 2018
KW  - aircraft communication
KW  - aircraft navigation
KW  - altimeters
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - Kalman filters
KW  - nonlinear filters
KW  - position control
KW  - sensors
KW  - target tracking
KW  - ultra wideband communication
KW  - quadcopter
KW  - autonomous flight
KW  - Extended Kalman Filter
KW  - UWB ranging measurements
KW  - onboard sensors
KW  - altimeters
KW  - optical flow
KW  - UWB based communication capability
KW  - robust target-relative localization
KW  - ultra-wideband ranging communication
KW  - Ultra-wideband ranging sensors
KW  - Distance measurement
KW  - Sensors
KW  - Antenna measurements
KW  - Robustness
KW  - Iron
KW  - Robots
KW  - Kalman filters
DO  - 10.1109/ICRA.2018.8460844
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper we propose a method to achieve relative positioning and tracking of a target by a quadcopter using Ultra-wideband (UWB) ranging sensors, which are strategically installed to help retrieve both relative position and bearing between the quadcopter and target. To achieve robust localization for autonomous flight even with uncertainty in the speed of the target, two main features are developed. First, an estimator based on Extended Kalman Filter (EKF) is developed to fuse UWB ranging measurements with data from onboard sensors including inertial measurement unit (IMU), altimeters and optical flow. Second, to properly handle the coupling of the target's orientation with the range measurements, UWB based communication capability is utilized to transfer the target's orientation to the quadcopter. Experiments results demonstrate the ability of the quadcopter to control its position relative to the target autonomously in both cases when the target is static and moving.
ER  - 

TY  - CONF
TI  - Visual Odometry Using a Homography Formulation with Decoupled Rotation and Translation Estimation Using Minimal Solutions
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2320
EP  - 2327
AU  - B. Guan
AU  - P. Vasseur
AU  - C. Demonceaux
AU  - F. Fraundorfer
PY  - 2018
KW  - distance measurement
KW  - estimation theory
KW  - image matching
KW  - matrix algebra
KW  - motion estimation
KW  - pose estimation
KW  - robot vision
KW  - motion estimation
KW  - decoupled rotation
KW  - optimal inlier set
KW  - histogram voting
KW  - RANSAC step
KW  - KITTI data set
KW  - road driving scenarios
KW  - homography formulation
KW  - visual odometry
KW  - exhaustive search
KW  - motion hypothesis
KW  - translation estimation
KW  - dominant ground plane
KW  - Estimation
KW  - Visual odometry
KW  - Mathematical model
KW  - Cameras
KW  - Histograms
KW  - Gravity
KW  - Motion estimation
DO  - 10.1109/ICRA.2018.8460747
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper we present minimal solutions for two-view relative motion estimation based on a homography formulation. By assuming a known vertical direction (e.g. from an IMU) and assuming a dominant ground plane we demonstrate that rotation and translation estimation can be decoupled. This result allows us to reduce the number of point matches needed to compute a motion hypothesis. We then derive different algorithms based on this decoupling that allow an efficient estimation. We also demonstrate how these algorithms can be used efficiently to compute an optimal inlier set using exhaustive search or histogram voting instead of a traditional RANSAC step. Our methods are evaluated on synthetic data and on the KITTI data set, demonstrating that our methods are well suited for visual odometry in road driving scenarios.
ER  - 

TY  - CONF
TI  - Local Nearest Neighbor Integrity Risk Evaluation for Robot Navigation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2328
EP  - 2333
AU  - G. Duenas Arana
AU  - M. Joerger
AU  - M. Spenko
PY  - 2018
KW  - computational complexity
KW  - feature extraction
KW  - mobile robots
KW  - nearest neighbour methods
KW  - risk analysis
KW  - sensor fusion
KW  - association faults
KW  - nearest neighbor data association algorithm
KW  - upper bound
KW  - nearest neighbor integrity risk evaluation
KW  - robot localization
KW  - integrity risk prediction
KW  - robot navigation
KW  - data association algorithms
KW  - feature extraction
KW  - Feature extraction
KW  - Technological innovation
KW  - Covariance matrices
KW  - Robots
KW  - Upper bound
KW  - Safety
KW  - Noise measurement
DO  - 10.1109/ICRA.2018.8460762
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper describes the design of a new integrity risk prediction/monitoring methodology for robot localization that uses feature extraction and data association algorithms. The work specifically addresses incorrect association faults when employing a local nearest neighbor data association algorithm. This approach is more efficient and easier to implement than previous work. The methodology is tested in simulation, showing that the computed upper bound on integrity risk is a performance metric capable of providing warnings when the safety of the system cannot be guaranteed.
ER  - 

TY  - CONF
TI  - Aided Inertial Navigation with Geometric Features: Observability Analysis
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2334
EP  - 2340
AU  - Y. Yang
AU  - G. Huang
PY  - 2018
KW  - cameras
KW  - inertial navigation
KW  - Monte Carlo methods
KW  - observability
KW  - optical radar
KW  - radar imaging
KW  - radionavigation
KW  - sonar imaging
KW  - stereo image processing
KW  - Monte Carlo simulations
KW  - VINS
KW  - plane features
KW  - bearing measurements
KW  - point features
KW  - bearing sensor
KW  - vision-aided INS
KW  - generic exteroceptive range
KW  - inertial navigation systems
KW  - observability analysis
KW  - Observability
KW  - Jacobian matrices
KW  - Sensors
KW  - Position measurement
KW  - Gravity
KW  - Rotation measurement
KW  - Current measurement
DO  - 10.1109/ICRA.2018.8460670
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we perform observability analysis for inertial navigation systems (INS) aided by generic exteroceptive range and/or bearing sensors with different geometric features including points, lines and planes. While the observability of vision-aided INS (VINS, which uses camera as a bearing sensor) with point features has been extensively studied in the literature, we analytically show that the same observability property remains if using generic range and/or bearing measurements, and if global measurements are also available, as expected, some unobservable directions dismiss. We study in-depth the effects of four degenerate motions on the system observability. In particular, building upon the observability analysis of the aided INS with point features, we perform observability analysis for the same system but with line and plane features, respectively, and show that there exist 5 (and 6) unobservable directions for a single line (and plane) feature. Moreover, we, for the first time, analytically derive the unobservable directions for the cases of multiple lines/planes. We validate our analysis through Monte Carlo simulations.
ER  - 

TY  - CONF
TI  - Omnidirectional CNN for Visual Place Recognition and Navigation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2341
EP  - 2348
AU  - T. Wang
AU  - H. Huang
AU  - J. Lin
AU  - C. Hu
AU  - K. Zeng
AU  - M. Sun
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - feedforward neural nets
KW  - image matching
KW  - image retrieval
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object recognition
KW  - pose estimation
KW  - robot vision
KW  - visual place recognition
KW  - place exemplars
KW  - recognition method
KW  - omnidirectional cameras
KW  - visual input
KW  - matched place exemplar
KW  - closest place exemplar
KW  - relative distance
KW  - retrieved closest place
KW  - omnidirectional view
KW  - powerful O-CNN
KW  - Omnidirectional CNN
KW  - virtual world datasets
KW  - real-world datasets
KW  - omnidirectional convolutional neural network
KW  - camera pose variation
KW  - Visualization
KW  - Navigation
KW  - Robots
KW  - Measurement
KW  - Cameras
KW  - Feature extraction
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8463173
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Visual place recognition is challenging, especially when only a few place exemplars are given. To mitigate the challenge, we consider place recognition method using omnidirectional cameras and propose a novel Omnidirectional Convolutional Neural Network (O-CNN) to handle severe camera pose variation. Given a visual input, the task of the O-CNN is not to retrieve the matched place exemplar, but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place. With the ability to estimate relative distance, a heuristic policy is proposed to navigate a robot to the retrieved closest place. Note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance. To train a powerful O-CNN, we build a virtual world for training on a large scale. We also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently. Finally, our experimental results confirm that our method achieves state-of-the-art accuracy and speed with both the virtual world and real-world datasets.
ER  - 

TY  - CONF
TI  - Addressing Challenging Place Recognition Tasks Using Generative Adversarial Networks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2349
EP  - 2355
AU  - Y. Latif
AU  - R. Garg
AU  - M. Milford
AU  - I. Reid
PY  - 2018
KW  - feature extraction
KW  - image recognition
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - visual perception
KW  - perception task
KW  - place recognition tasks
KW  - simultaneous localization and mapping
KW  - SLAM
KW  - coupled Generative Adversarial Networks
KW  - domain translation task
KW  - Task analysis
KW  - Gallium nitride
KW  - Lighting
KW  - Generators
KW  - Image recognition
KW  - Feature extraction
KW  - Visualization
DO  - 10.1109/ICRA.2018.8461081
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Place recognition is an essential component of Simultaneous Localization And Mapping (SLAM). Under severe appearance change, reliable place recognition is a difficult perception task since the same place is perceptually very different in the morning, at night, or over different seasons. This work addresses place recognition as a domain translation task. Using a pair of coupled Generative Adversarial Networks (GANs), we show that it is possible to generate the appearance of one domain (such as summer) from another (such as winter) without requiring image-to-image correspondences across the domains. Mapping between domains is learned from sets of images in each domain without knowing the instance-to-instance correspondence by enforcing a cyclic consistency constraint. In the process, meaningful feature spaces are learned for each domain, the distances in which can be used for the task of place recognition. Experiments show that learned features correspond to visual similarity and can be effectively used for place recognition across seasons.
ER  - 

TY  - CONF
TI  - Re-Deployment Algorithms for Multiple Service Robots to Optimize Task Response
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2356
EP  - 2363
AU  - A. Sadeghi
AU  - S. L. Smith
PY  - 2018
KW  - approximation theory
KW  - computational complexity
KW  - greedy algorithms
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - service robots
KW  - N task arrivals
KW  - service tasks
KW  - redeployment cost
KW  - one-stage greedy algorithm
KW  - constant-factor approximation algorithm
KW  - service cost
KW  - multiple service robots
KW  - autonomous robots
KW  - re-deployment algorithms
KW  - task response optimization
KW  - NP-hard
KW  - Robots
KW  - Task analysis
KW  - Time factors
KW  - Approximation algorithms
KW  - Probability distribution
KW  - Measurement
KW  - Vehicle dynamics
DO  - 10.1109/ICRA.2018.8460726
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper focuses on the problem of deploying a set of autonomous robots to efficiently service tasks that arrive sequentially in an environment over time. Each task is serviced when the robot visits the corresponding task location. Robots can then redeploy while waiting for the next task to arrive. The objective is to redeploy the robots taking into account the next N task arrivals. We seek to minimize a linear combination of the expected cost to service tasks and the redeployment cost between task arrivals. In the single robot case, we propose a one-stage greedy algorithm and prove its optimality. For multiple robots, the problem is NP-hard, and we propose two constant-factor approximation algorithm, one for the problem with a horizon of two task arrivals and the other for the infinite horizon when redeployment cost is weighted more heavily than service cost. Finally, we present extensive benchmarking results to characterize both solution quality and runtime.
ER  - 

TY  - CONF
TI  - Multi-Agent Time-Based Decision-Making for the Search and Action Problem
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2365
EP  - 2372
AU  - T. Miki
AU  - M. Popović
AU  - A. Gawel
AU  - G. Hitz
AU  - R. Siegwart
PY  - 2018
KW  - computational complexity
KW  - control engineering computing
KW  - decision making
KW  - multi-agent systems
KW  - multi-robot systems
KW  - probability
KW  - rescue robots
KW  - search-and-rescue
KW  - task allocation
KW  - probabilistic reasoning
KW  - Gazebo-based environmenT
KW  - multiagent time-based decision-making
KW  - Mohamed Bin Zayed International Robotics Challenge
KW  - near-optimal decisions
KW  - agent action
KW  - allocated budget
KW  - time constraints
KW  - decentralized multiagent decision-making framework
KW  - computational complexity
KW  - task selection
KW  - missions present several challenges
KW  - robotic applications
KW  - action problem
KW  - Task analysis
KW  - Search problems
KW  - Decision making
KW  - Planning
KW  - Time factors
KW  - Robots
KW  - Probabilistic logic
DO  - 10.1109/ICRA.2018.8460996
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Many robotic applications, such as search-and-rescue, require multiple agents to search for and perform actions on targets. However, such missions present several challenges, including cooperative exploration, task selection and allocation, time limitations, and computational complexity. To address this, we propose a decentralized multi-agent decision-making framework for the search and action problem with time constraints. The main idea is to treat time as an allocated budget in a setting where each agent action incurs a time cost and yields a certain reward. Our approach leverages probabilistic reasoning to make near-optimal decisions leading to maximized reward. We evaluate our method in the search, pick, and place scenario of the Mohamed Bin Zayed International Robotics Challenge (MBZIRC), by using a probability density map and reward prediction function to assess actions. Extensive simulations show that our algorithm outperforms benchmark strategies, and we demonstrate system integration in a Gazebo-based environment, validating the framework's readiness for field application.
ER  - 

TY  - CONF
TI  - Multi-robot Dubins Coverage with Autonomous Surface Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2373
EP  - 2379
AU  - N. Karapetyan
AU  - J. Moulton
AU  - J. S. Lewis
AU  - A. Quattrini Li
AU  - J. M. O'Kane
AU  - I. Rekleitis
PY  - 2018
KW  - computational complexity
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - remotely operated vehicles
KW  - travelling salesman problems
KW  - multirobot Dubins coverage
KW  - aerial monitoring
KW  - single robot approaches
KW  - multirobot approaches
KW  - Dubins vehicle kinematics
KW  - environmental monitoring
KW  - multirobot team
KW  - Dubins vehicles
KW  - NP-complete problems
KW  - salesman problem-k-TSP-formulation
KW  - autonomous surface vehicles
KW  - large scale coverage operations
KW  - marine exploration
KW  - Robot sensing systems
KW  - Clustering algorithms
KW  - Task analysis
KW  - Lakes
KW  - Multi-robot systems
KW  - Kinematics
DO  - 10.1109/ICRA.2018.8460661
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In large scale coverage operations, such as marine exploration or aerial monitoring, single robot approaches are not ideal, as they may take too long to cover a large area. In such scenarios, multi-robot approaches are preferable. Furthermore, several real world vehicles are non-holonomic, but can be modeled using Dubins vehicle kinematics. This paper focuses on environmental monitoring of aquatic environments using Autonomous Surface Vehicles (ASVs). In particular, we propose a novel approach for solving the problem of complete coverage of a known environment by a multi-robot team consisting of Dubins vehicles. It is worth noting that both multi-robot coverage and Dubins vehicle coverage are NP-complete problems. As such, we present two heuristics methods based on a variant of the traveling salesman problem-k-TSP-formulation and clustering algorithms that efficiently solve the problem. The proposed methods are tested both in simulations to assess their scalability and with a team of ASVs operating on a 200 km2 lake to ensure their applicability in real world.
ER  - 

TY  - CONF
TI  - How Many Robots are Enough: A Multi-Objective Genetic Algorithm for the Single-Objective Time-Limited Complete Coverage Problem
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2380
EP  - 2387
AU  - X. Zhou
AU  - H. Wang
AU  - B. Ding
PY  - 2018
KW  - computational complexity
KW  - genetic algorithms
KW  - multi-robot systems
KW  - trees (mathematics)
KW  - multiobjective genetic algorithm
KW  - multirobot complete coverage problem
KW  - task-allocation
KW  - number-fixed problem
KW  - multiobjective GA
KW  - Mofint
KW  - single-objective time-limited complete coverage problem
KW  - Robots
KW  - Vegetation
KW  - Task analysis
KW  - Genetic algorithms
KW  - Resource management
KW  - Optimization
KW  - Approximation algorithms
DO  - 10.1109/ICRA.2018.8461028
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Complete coverage, which is the foundation of many robotic applications, aims to cover an area as quickly as possible. This study investigates the time-limited version of multi-robot complete coverage problem, that is, to find the least number of robots and allocate tasks properly to them such that they can finish a known mission within the time limit. This version of problem can be tackled straightforwardly based on optimizing the task-allocation to a fixed number of robots and enumerating the number. However, the number-fixed problem is NP-hard and the existing algorithm for the number-fixed problem allows intersecting tasks (possibly causing robots' interference) and endures high approximation factor. In this study, the time-limited complete coverage problem is tackled with a multi-objective approach, instead of enumerating robots' number and optimizing each number-fixed problem one by one. The multi-objective GA, Mofint, at first estimates the lower and upper bounds of the number of robots. It abstracts each task as a weighted node of a graph. Then, Mofint evolves individuals, each individual being a forest containing a certain number (within the bounds) of non-intersecting trees. Mofint can finally obtain higher precision than existing work with less time: the approximation factor for Mofint is 1.1 to 1.5 times the ideal allocation when robots' number is fixed, while for existing work is 1.5 to 2. Due to its higher precision, the least number of robots obtained in the experiments by Mofint is 0.6 times of existing work.
ER  - 

TY  - CONF
TI  - Joint Multi-Policy Behavior Estimation and Receding-Horizon Trajectory Planning for Automated Urban Driving
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2388
EP  - 2394
AU  - B. Zhou
AU  - W. Schwarting
AU  - D. Rus
AU  - J. Alonso-Mora
PY  - 2018
KW  - collision avoidance
KW  - Markov processes
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - road vehicles
KW  - multipolicy decision-making
KW  - traffic participants
KW  - planned trajectory
KW  - ego-vehicle
KW  - safe trajectories
KW  - multiple motion policies
KW  - receding-horizon planner
KW  - simulated multivehicle intersection scenarios
KW  - joint multipolicy behavior
KW  - automated urban driving
KW  - urban environments
KW  - autonomous vehicle
KW  - multiple motion hypothesis
KW  - joint behavior estimation
KW  - observable Markov decision processes
KW  - receding-horizon control
KW  - receding-horizon trajectory planning
KW  - Trajectory
KW  - Planning
KW  - Estimation
KW  - Space vehicles
KW  - Uncertainty
KW  - Roads
KW  - Computational modeling
DO  - 10.1109/ICRA.2018.8461138
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - When driving in urban environments, an autonomous vehicle must account for the interaction with other traffic participants. It must reason about their future behavior, how its actions affect their future behavior, and potentially consider multiple motion hypothesis. In this paper we introduce a method for joint behavior estimation and trajectory planning that models interaction and multi-policy decision-making. The method leverages Partially Observable Markov Decision Processes to estimate the behavior of other traffic participants given the planned trajectory for the ego-vehicle, and Receding-Horizon Control for generating safe trajectories for the ego-vehicle. To achieve safe navigation we introduce chance constraints over multiple motion policies in the receding-horizon planner. These constraints account for uncertainty over the behavior of other traffic participants. The method is capable of running in real-time and we show its performance and good scalability in simulated multi-vehicle intersection scenarios.
ER  - 

TY  - CONF
TI  - Robust Environmental Mapping by Mobile Sensor Networks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2395
EP  - 2402
AU  - H. Park
AU  - J. Liu
AU  - M. Johnson-Roberson
AU  - R. Vasudevan
PY  - 2018
KW  - Bayes methods
KW  - computational geometry
KW  - environmental factors
KW  - fires
KW  - mobile radio
KW  - mobile robots
KW  - wireless sensor networks
KW  - environmental mapping
KW  - ground truth distribution
KW  - Voronoi diagram
KW  - ad-hoc communication
KW  - human safety
KW  - satisfactory convergence
KW  - autonomous agents
KW  - mapping tasks
KW  - terrain elevation
KW  - physical quantities
KW  - forest fires
KW  - hazardous chemical leakages
KW  - spatial map
KW  - mobile sensor networks
KW  - decentralized manner
KW  - disjoint regions
KW  - hardware failures
KW  - short-range sensors
KW  - mobile robots
KW  - environmental parameters
KW  - robust spatial mapping
KW  - Bayesian approach
KW  - Robot sensing systems
KW  - Robustness
KW  - Mutual information
KW  - Computational modeling
KW  - Mobile robots
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461034
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Constructing a spatial map of environmental parameters is a crucial step to preventing hazardous chemical leakages, forest fires, or while estimating a spatially distributed physical quantities such as terrain elevation. Although prior methods can do such mapping tasks efficiently via dispatching a group of autonomous agents, they are unable to ensure satisfactory convergence to the underlying ground truth distribution in a decentralized manner when any of the agents fail. Since the types of agents utilized to perform such mapping are typically inexpensive and prone to failure, this results in poor overall mapping performance in real-world applications, which can in certain cases endanger human safety. This paper presents a Bayesian approach for robust spatial mapping of environmental parameters by deploying a group of mobile robots capable of ad-hoc communication equipped with short-range sensors in the presence of hardware failures. Our approach first utilizes a variant of the Voronoi diagram to partition the region to be mapped into disjoint regions that are each associated with at least one robot. These robots are then deployed in a decentralized manner to maximize the likelihood that at least one robot detects every target in their associated region despite a non-zero probability of failure. A suite of simulation results is presented to demonstrate the effectiveness and robustness of the proposed method when compared to existing techniques.
ER  - 

TY  - CONF
TI  - Best Response Model Predictive Control for Agile Interactions Between Autonomous Ground Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2403
EP  - 2410
AU  - G. Williams
AU  - B. Goldfain
AU  - P. Drews
AU  - J. M. Rehg
AU  - E. A. Theodorou
PY  - 2018
KW  - game theory
KW  - information theory
KW  - mobile robots
KW  - nonlinear control systems
KW  - predictive control
KW  - remotely operated vehicles
KW  - stochastic systems
KW  - best response model predictive control
KW  - AutoRally platforms
KW  - nonlinear stochastic systems
KW  - information theoretic model predictive control algorithm
KW  - iterated best response
KW  - game theoretic notion
KW  - autonomous control
KW  - autonomous ground vehicles
KW  - Games
KW  - Stochastic processes
KW  - Predictive control
KW  - Nash equilibrium
KW  - Optimization
KW  - Vehicle dynamics
KW  - Prediction algorithms
DO  - 10.1109/ICRA.2018.8462831
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We introduce an algorithm for autonomous control of multiple fast ground vehicles operating in close proximity to each other. The algorithm is based on a combination of the game theoretic notion of iterated best response, and an information theoretic model predictive control algorithm designed for non-linear stochastic systems. We test the algorithm on two one-fifth scale AutoRally platforms traveling at speeds upwards of 8 meters per second, while maintaining a following distance of under two meters from bumper-to-bumper.
ER  - 

TY  - CONF
TI  - A Deep Incremental Boltzmann Machine for Modeling Context in Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2411
EP  - 2416
AU  - F. I. Doğan
AU  - H. Çelikkanat
AU  - S. Kalkan
PY  - 2018
KW  - Boltzmann machines
KW  - learning (artificial intelligence)
KW  - pattern classification
KW  - contextual model
KW  - context layer
KW  - scene classification benchmark
KW  - nonincremental models
KW  - deep incremental Boltzmann machine
KW  - robots
KW  - context modeling efforts
KW  - fixed structure
KW  - incremental deep model
KW  - Neurons
KW  - Context modeling
KW  - Hidden Markov models
KW  - Robots
KW  - Computational modeling
KW  - Adaptation models
KW  - Data models
DO  - 10.1109/ICRA.2018.8462925
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Context is an essential capability for robots that are to be as adaptive as possible in challenging environments. Although there are many context modeling efforts, they assume a fixed structure and number of contexts. In this paper, we propose an incremental deep model that extends Restricted Boltzmann Machines. Our model gets one scene at a time, and gradually extends the contextual model when necessary, either by adding a new context or a new context layer to form a hierarchy. We show on a scene classification benchmark that our method converges to a good estimate of the contexts of the scenes, and performs better or on-par on several tasks compared to other incremental models or non-incremental models.
ER  - 

TY  - CONF
TI  - Accelerating Model Learning with Inter-Robot Knowledge Transfer
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2417
EP  - 2424
AU  - N. Makondo
AU  - B. Rosman
AU  - O. Hasegawa
PY  - 2018
KW  - learning (artificial intelligence)
KW  - manipulator dynamics
KW  - multi-robot systems
KW  - robot programming
KW  - training transfer models
KW  - online learning
KW  - inverse dynamics model
KW  - model learning
KW  - inter-robot knowledge transfer
KW  - multirobot setting
KW  - trajectory tracking tasks
KW  - robot inverse dynamics model
KW  - tabula rasa learning
KW  - robot learning
KW  - Interbotix PhantomX Pincher arm
KW  - Kuka youBot arm
KW  - Adaptation models
KW  - Manipulator dynamics
KW  - Data models
KW  - Acceleration
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461218
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Online learning of a robot's inverse dynamics model for trajectory tracking necessitates an interaction between the robot and its environment to collect training data. This is challenging for physical robots in the real world, especially for humanoids and manipulators due to their large and high dimensional state and action spaces, as a large amount of data must be collected over time. This can put the robot in danger when learning tabula rasa and can also be a time-intensive process especially in a multi-robot setting, where each robot is learning its model from scratch. We propose accelerating learning of the inverse dynamics model for trajectory tracking tasks in this multi-robot setting using knowledge transfer, where robots share and re-use data collected by preexisting robots, in order to speed up learning for new robots. We propose a scheme for collecting a sample of correspondences from the robots for training transfer models, and demonstrate, in simulations, the benefit of knowledge transfer in accelerating online learning of the inverse dynamics model between several robots, including between a low-cost Interbotix PhantomX Pincher arm, and a more expensive and relatively heavier Kuka youBot arm. We show that knowledge transfer can save up to 63% of training time of the youBot arm compared to learning from scratch, and about 58% for the lighter Pincher arm.
ER  - 

TY  - CONF
TI  - Online Learning of a Memory for Learning Rates
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2425
EP  - 2432
AU  - F. Meier
AU  - D. Kappler
AU  - S. Schaal
PY  - 2018
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - optimisation
KW  - pattern classification
KW  - learning rates
KW  - learning process
KW  - memory model
KW  - optimal learning rate landscape
KW  - task specific optimization
KW  - meta-learner
KW  - internal memory
KW  - optimization tasks
KW  - meta-learning algorithm speeds
KW  - learning control tasks
KW  - online learning settings
KW  - gradient behaviors
KW  - gradient-based optimizer
KW  - MNIST classification
KW  - Task analysis
KW  - Optimization
KW  - Prediction algorithms
KW  - Robots
KW  - Transforms
KW  - Computational modeling
KW  - Predictive models
DO  - 10.1109/ICRA.2018.8460625
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The promise of learning to learn for robotics rests on the hope that by extracting some information about the learning process itself we can speed up subsequent similar learning tasks. Here, we introduce a computationally efficient online meta-learning algorithm that builds and optimizes a memory model of the optimal learning rate landscape from previously observed gradient behaviors. While performing task specific optimization, this memory of learning rates predicts how to scale currently observed gradients. After applying the gradient scaling our meta-learner updates its internal memory based on the observed effect its prediction had. Our meta-learner can be combined with any gradient-based optimizer, learns on the fly and can be transferred to new optimization tasks. In our evaluations we show that our meta-learning algorithm speeds up learning of MNIST classification and a variety of learning control tasks, either in batch or online learning settings.
ER  - 

TY  - CONF
TI  - Learning Coupled Forward-Inverse Models with Combined Prediction Errors
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2433
EP  - 2439
AU  - D. Koert
AU  - G. Maeda
AU  - G. Neumann
AU  - J. Pcters
PY  - 2018
KW  - learning (artificial intelligence)
KW  - robots
KW  - multiple solutions
KW  - inverse space
KW  - forward models
KW  - paired forward-inverse models
KW  - multiple modules
KW  - local minima
KW  - training multiple models-that
KW  - monolithic complex network
KW  - efficient alternative
KW  - multiple simple models
KW  - complex models
KW  - unstructured environments
KW  - combined prediction errors
KW  - coupled forward-inverse models
KW  - Inverse problems
KW  - Computational modeling
KW  - Data models
KW  - Predictive models
KW  - Adaptation models
KW  - Robots
KW  - Context modeling
DO  - 10.1109/ICRA.2018.8460675
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Challenging tasks in unstructured environments require robots to learn complex models. Given a large amount of information, learning multiple simple models can offer an efficient alternative to a monolithic complex network. Training multiple models-that is, learning their parameters and their responsibilities-has been shown to be prohibitively hard as optimization is prone to local minima. To efficiently learn multiple models for different contexts, we thus develop a new algorithm based on expectation maximization (EM). In contrast to comparable concepts, this algorithm trains multiple modules of paired forward-inverse models by using the prediction errors of both forward and inverse models simultaneously. In particular, we show that our method yields a substantial improvement over only considering the errors of the forward models on tasks where the inverse space contains multiple solutions.
ER  - 

TY  - CONF
TI  - DEFO-NET: Learning Body Deformation Using Generative Adversarial Networks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2440
EP  - 2447
AU  - Z. Wang
AU  - S. Rosa
AU  - L. Xie
AU  - B. Yang
AU  - S. Wang
AU  - N. Trigoni
AU  - A. Markham
PY  - 2018
KW  - finite element analysis
KW  - image colour analysis
KW  - image reconstruction
KW  - mobile robots
KW  - robot vision
KW  - RGB-D image
KW  - finite element methods
KW  - mobile robots
KW  - single depth view
KW  - physical finite element model simulator
KW  - autonomous robots
KW  - Generative Adversarial networks
KW  - body deformation
KW  - DEFO-NET
KW  - Strain
KW  - Gallium nitride
KW  - Force
KW  - Deformable models
KW  - Robots
KW  - Training
KW  - Generators
DO  - 10.1109/ICRA.2018.8462832
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Modelling the physical properties of everyday objects is a fundamental prerequisite for autonomous robots. We present a novel generative adversarial network (DEFO-NET), able to predict body deformations under external forces from a single RGB-D image. The network is based on an invertible conditional Generative Adversarial Network (IcGAN) and is trained on a collection of different objects of interest generated by a physical finite element model simulator. Defo-netinherits the generalisation properties of GANs. This means that the network is able to reconstruct the whole 3-D appearance of the object given a single depth view of the object and to generalise to unseen object configurations. Contrary to traditional finite element methods, our approach is fast enough to be used in real-time applications. We apply the network to the problem of safe and fast navigation of mobile robots carrying payloads over different obstacles and floor materials. Experimental results in real scenarios show how a robot equipped with an RGB-D camera can use the network to predict terrain deformations under different payload configurations and use this to avoid unsafe areas.
ER  - 

TY  - CONF
TI  - Bodily Aware Soft Robots: Integration of Proprioceptive and Exteroceptive Sensors
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2448
EP  - 2453
AU  - G. Soter
AU  - A. Conn
AU  - H. Hauser
AU  - J. Rossiter
PY  - 2018
KW  - cameras
KW  - convolution
KW  - dexterous manipulators
KW  - mobile robots
KW  - object tracking
KW  - path planning
KW  - recurrent neural nets
KW  - sensors
KW  - bodily aware soft robots
KW  - exteroceptive sensors
KW  - proprioceptive sensors
KW  - bend sensors
KW  - visual sensor
KW  - nonlinearity
KW  - octopus-inspired arm
KW  - camera record
KW  - arm capturing
KW  - internal sensory signals
KW  - stacked convolutional autoencoder
KW  - CAE
KW  - recurrent neural network
KW  - RNN
KW  - motion
KW  - Convolution
KW  - Soft robotics
KW  - Robot sensing systems
KW  - Visualization
KW  - Recurrent neural networks
DO  - 10.1109/ICRA.2018.8463169
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Being aware of our body has great importance in our everyday life. It helps us to complete difficult tasks, such as movement in a dark room or grasping a complex object. These skills are important for robots as well, however, robotic bodily awareness is still an open question, and the nonlinearity of soft robots adds even more complexity. In this paper, we address this problem and present a novel method to implement bodily awareness into a real soft robot by the integration of its exteroceptive and proprioceptive sensors. We use an octopus-inspired arm as an example where the proprioceptive representation is approximated by four bend sensors integrated into the soft body, while a camera records the movement of the arm capturing its exteroceptive representation. The internal sensory signals are mapped to the visual information using a combination of a stacked convolutional autoencoder (CAE) and a recurrent neural network (RNN). As a result, the soft robot can learn to estimate and, therefore, to imagine its motion even when its visual sensor is not available.
ER  - 

TY  - CONF
TI  - Deep Learning a Quadrotor Dynamic Model for Multi-Step Prediction
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2454
EP  - 2459
AU  - N. Mohajerin
AU  - M. Mozifian
AU  - S. Waslander
PY  - 2018
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - predictive control
KW  - recurrent neural nets
KW  - robot dynamics
KW  - robot kinematics
KW  - trajectory control
KW  - quadrotor dynamic model
KW  - motion prediction
KW  - dynamic systems
KW  - long horizons
KW  - deep learning
KW  - deep recurrent neural networks
KW  - quadrotor motion model
KW  - initial system state
KW  - motor speeds
KW  - prediction horizon
KW  - recurrent neural network state initialization
KW  - quadrotor vehicle flights
KW  - indoor flight arena
KW  - hybrid network architecture
KW  - system identification methods
KW  - robust state predictions
KW  - time 2.0 s
KW  - frequency 100.0 Hz
KW  - Mathematical model
KW  - Predictive models
KW  - Vehicle dynamics
KW  - Aerodynamics
KW  - Recurrent neural networks
KW  - Training
DO  - 10.1109/ICRA.2018.8460840
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We develop a multi-step motion prediction modeling method for dynamic systems over long horizons using deep learning. Building on previous work, we propose a novel hybrid network architecture, by combining deep recurrent neural networks with a quadrotor motion model created using classic system identification methods. The proposed model takes only the initial system state and motor speeds over the prediction horizon as inputs and returns robust state predictions for up to two seconds of motion at 100 Hz. We employ recurrent neural network state initialization during training, to exploit real-world dataset collected from quadrotor vehicle flights in an indoor flight arena. Our experiments demonstrate that the proposed hybrid network model consistently outperforms both black box and rigid body dynamics predictions over single and multi-step prediction scenarios, with an order of magnitude improvements in velocity estimates in particular.
ER  - 

TY  - CONF
TI  - Safe Learning of Quadrotor Dynamics Using Barrier Certificates
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2460
EP  - 2465
AU  - L. Wang
AU  - E. A. Theodorou
AU  - M. Egerstedt
PY  - 2018
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - Gaussian processes
KW  - helicopters
KW  - learning systems
KW  - mobile robots
KW  - nonlinear control systems
KW  - probability
KW  - uncertain systems
KW  - complex dynamical systems
KW  - accurate nonlinear models
KW  - data-driven approach
KW  - Gaussian processes
KW  - learning process
KW  - barrier certificates
KW  - safe learning
KW  - learning controller
KW  - quadrotor dynamics
KW  - Safety
KW  - Control systems
KW  - Computational modeling
KW  - Gaussian processes
KW  - Adaptation models
KW  - Lyapunov methods
KW  - System dynamics
DO  - 10.1109/ICRA.2018.8460471
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - To effectively control complex dynamical systems, accurate nonlinear models are typically needed. However, these models are not always known. In this paper, we present a data-driven approach based on Gaussian processes that learns models of quadrotors operating in partially unknown environments. What makes this challenging is that if the learning process is not carefully controlled, the system will go unstable, i.e., the quadcopter will crash. To this end, barrier certificates are employed for safe learning. The barrier certificates establish a non-conservative forward invariant safe region, in which high probability safety guarantees are provided based on the statistics of the Gaussian Process. A learning controller is designed to efficiently explore those uncertain states and expand the barrier certified safe region based on an adaptive sampling scheme. Simulation results are provided to demonstrate the effectiveness of the proposed approach.
ER  - 

TY  - CONF
TI  - Data-Efficient Decentralized Visual SLAM
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2466
EP  - 2473
AU  - T. Cieslewski
AU  - S. Choudhary
AU  - D. Scaramuzza
PY  - 2018
KW  - cameras
KW  - data mining
KW  - graph theory
KW  - image sensors
KW  - multi-robot systems
KW  - optimisation
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - decentralized visual SLAM system
KW  - decentralized SLAM components
KW  - data-efficient decentralized visual SLAM
KW  - pose-graph optimization method
KW  - data association scales
KW  - robot count
KW  - data transfers
KW  - robots
KW  - map data
KW  - visual SLAM systems exchange
KW  - versatile cameras
KW  - lightweight cameras
KW  - cheap cameras
KW  - multirobot applications
KW  - mapping
KW  - Supplementary Material Data
KW  - Simultaneous localization and mapping
KW  - Visualization
KW  - Optimization
KW  - Pose estimation
KW  - Trajectory
KW  - Bandwidth
DO  - 10.1109/ICRA.2018.8461155
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code. Supplementary Material Data and code are at: https://github.com/uzh-rpg/dslam_open.
ER  - 

TY  - CONF
TI  - A Linear Least Square Initialization Method for 3D Pose Graph Optimization Problem
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2474
EP  - 2479
AU  - S. M. Nasiri
AU  - H. Moradi
AU  - R. Hosseini
PY  - 2018
KW  - approximation theory
KW  - computer vision
KW  - graph theory
KW  - image reconstruction
KW  - iterative methods
KW  - least squares approximations
KW  - optimisation
KW  - pose estimation
KW  - SLAM (robots)
KW  - important optimization problem
KW  - machine vision applications
KW  - 3D SLAM
KW  - graph corresponds
KW  - PGO problem
KW  - relative noisy observation
KW  - PGO solvers
KW  - state-of-the-art initialization methods
KW  - low noise problems
KW  - measurement noise
KW  - iterative methods
KW  - high noise problems
KW  - PGO optimization problem
KW  - iterative least-squares method
KW  - linear least square initialization method
KW  - pose graph optimization
KW  - least-square problem
KW  - Robots
KW  - Integrated circuits
KW  - Cost function
KW  - Iterative methods
KW  - Three-dimensional displays
KW  - Estimation
KW  - Pose Graph Optimization
KW  - Least square
KW  - 3D SLAM
KW  - Initialization method
DO  - 10.1109/ICRA.2018.8460741
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Pose Graph Optimization (PGO) is an important optimization problem arising in robotics and machine vision applications like 3D reconstruction and 3D SLAM. Each node of pose graph corresponds to an orientation and a location. The PGO problem finds orientations and locations of the nodes from relative noisy observation between nodes. Recent investigations show that well-known iterative PGO solvers need good initialization to converge to good solutions. However, we observed that state-of-the-art initialization methods obtain good initialization only in low noise problems, and they fail in challenging problems having more measurement noise. Consequently, iterative methods may converge to bad solutions in high noise problems. In this paper, a new method for obtaining orientations in the PGO optimization problem is presented. Like other well-known methods the initial locations are obtained from the result of a least-squares problem. The proposed method iteratively approximates the problem around current estimation and converts it to a least-squares problem. Therefore, the method can be seen as an iterative least-squares method which is computationally efficient. Simulation results show that the proposed initialization method helps the most well-known iterative solver to obtain better optima and significantly outperform other solvers in some cases.
ER  - 

TY  - CONF
TI  - IMLS-SLAM: Scan-to-Model Matching Based on 3D Data
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2480
EP  - 2485
AU  - J. Deschaud
PY  - 2018
KW  - collision avoidance
KW  - mobile robots
KW  - optical radar
KW  - remotely operated vehicles
KW  - road traffic control
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - robotics community
KW  - stereo cameras
KW  - depth sensors
KW  - Velodyne LiDAR
KW  - autonomous driving
KW  - low-drift SLAM algorithm
KW  - 3D LiDAR data
KW  - scan-to-model matching framework
KW  - specific sampling strategy
KW  - LiDAR scans
KW  - Velodyne HDL32
KW  - Velodyne HDL64
KW  - global drift
KW  - IMLS-SLAM
KW  - 3D data
KW  - simultaneous localization and mapping
KW  - localized LiDAR sweeps
KW  - IMLS surface representation
KW  - implicit moving least squares
KW  - size 4.0 km
KW  - size 16.0 m
KW  - time 10.0 year
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Simultaneous localization and mapping
KW  - Two dimensional displays
KW  - Iterative closest point algorithm
KW  - Observability
DO  - 10.1109/ICRA.2018.8460653
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The Simultaneous Localization And Mapping (SLAM) problem has been well studied in the robotics community, especially using mono, stereo cameras or depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the last 10 years to be very useful to perceive the environment in autonomous driving, but few methods exist that directly use these 3D data for odometry. We present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method relies on a scan-to-model matching framework. We first have a specific sampling strategy based on the LiDAR scans. We then define our model as the previous localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface representation. We show experiments with the Velodyne HDL32 with only 0.40% drift over a 4 km acquisition without any loop closure (i.e., 16 m drift after 4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and ranked among the best methods (against mono, stereo and LiDAR methods) with a global drift of only 0.69%.
ER  - 

TY  - CONF
TI  - ApriISAM: Real-Time Smoothing and Mapping
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2486
EP  - 2493
AU  - X. Wang
AU  - R. Marcotte
AU  - G. Ferrer
AU  - E. Olson
PY  - 2018
KW  - error analysis
KW  - matrix decomposition
KW  - mobile robots
KW  - SLAM (robots)
KW  - sparse matrices
KW  - fixed computational budget
KW  - dynamic variable reordering algorithm
KW  - ApriISAM
KW  - real-time smoothing
KW  - online robots
KW  - incremental SLAM algorithms
KW  - batch algorithms
KW  - absolute error
KW  - incremental Cholesky factorizations
KW  - marginalization order
KW  - iSAM
KW  - re-linearize
KW  - Simultaneous localization and mapping
KW  - Heuristic algorithms
KW  - Smoothing methods
KW  - Sparse matrices
KW  - Clustering algorithms
KW  - Real-time systems
DO  - 10.1109/ICRA.2018.8461072
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - For online robots, incremental SLAM algorithms offer huge potential computational savings over batch algorithms. The dominant incremental algorithms are iSAM and iSAM2 which offer radically different approaches to computing incremental updates, balancing issues like 1) the need to re-linearize, 2) changes in the desirable variable marginalization order, and 3) the underlying conceptual approach (i.e. the “matrix” story versus the “factor graph” story). In this paper, we propose a new incremental algorithm that computes solutions with lower absolute error and generally provides lower error solutions for a fixed computational budget than either iSAM or iSAM2. Key to AprilSAM's performance are a new dynamic variable reordering algorithm for fast incremental Cholesky factorizations, a method for reducing the work involved in backsubstitutions, and a new algorithm for deciding between incremental and batch updates.
ER  - 

TY  - CONF
TI  - Fast Nonlinear Approximation of Pose Graph Node Marginalization
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2494
EP  - 2501
AU  - D. Ta
AU  - N. Banerjee
AU  - S. Eick
AU  - S. Lenser
AU  - M. E. Munich
PY  - 2018
KW  - approximation theory
KW  - graph theory
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - pose estimation
KW  - SLAM (robots)
KW  - pose graph node marginalization
KW  - longterm localization
KW  - longterm mapping
KW  - longterm navigation
KW  - pose graph structure
KW  - absolute-to relative-pose spaces
KW  - pose-composition approach scaled version
KW  - approximate subgraph
KW  - fast nonlinear approximation method
KW  - Topology
KW  - Jacobian matrices
KW  - Covariance matrices
KW  - Gaussian distribution
KW  - Simultaneous localization and mapping
KW  - Approximation methods
DO  - 10.1109/ICRA.2018.8460979
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a fast nonlinear approximation method for marginalizing out nodes on pose graphs for longterm simultaneous localization, mapping, and navigation. Our approximation preserves the pose graph structure to leverage the rich literature of pose graphs and optimization schemes. By re-parameterizing from absolute-to relative-pose spaces, our method does not suffer from the choice of linearization points as in previous works. We then join our approximation process with a scaled version of the recently-demoted pose-composition approach. Our approach eschews the expenses of many state-of-the-art convex optimization schemes through our efficient and simple O(N2) implementation for a given known topology of the approximate subgraph. We demonstrate its speed and near optimality in practice by comparing against state-of-the-art techniques on popular datasets.
ER  - 

TY  - CONF
TI  - A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2502
EP  - 2509
AU  - J. Delmerico
AU  - D. Scaramuzza
PY  - 2018
KW  - aerospace robotics
KW  - cameras
KW  - Global Positioning System
KW  - image capture
KW  - mobile robots
KW  - pose estimation
KW  - robot vision
KW  - state estimation
KW  - monocular visual-inertial odometry algorithms
KW  - state estimation algorithms
KW  - computational constraints
KW  - inertial measurement units
KW  - VIO algorithms
KW  - single-board computer systems
KW  - flying robots
KW  - pose estimation
KW  - cameras
KW  - IMUs
KW  - motion capture
KW  - global positioning systems
KW  - MSCKF
KW  - OKVIS
KW  - ROVIO
KW  - VINS-Mono
KW  - SVO-MSF
KW  - SVO-GTSAM
KW  - hardware configurations
KW  - EuRoC datasets
KW  - six degree of freedom
KW  - 6 DoF
KW  - State estimation
KW  - Visualization
KW  - Optimization
KW  - Pipelines
KW  - Hardware
KW  - Robot sensing systems
DO  - 10.1109/ICRA.2018.8460664
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Flying robots require a combination of accuracy and low latency in their state estimation in order to achieve stable and robust flight. However, due to the power and payload constraints of aerial platforms, state estimation algorithms must provide these qualities under the computational constraints of embedded hardware. Cameras and inertial measurement units (IMUs) satisfy these power and payload constraints, so visual-inertial odometry (VIO) algorithms are popular choices for state estimation in these scenarios, in addition to their ability to operate without external localization from motion capture or global positioning systems. It is not clear from existing results in the literature, however, which VIO algorithms perform well under the accuracy, latency, and computational constraints of a flying robot with onboard state estimation. This paper evaluates an array of publicly-available VIO pipelines (MSCKF, OKVIS, ROVIO, VINS-Mono, SVO+MSF, and SVO+GTSAM) on different hardware configurations, including several single-board computer systems that are typically found on flying robots. The evaluation considers the pose estimation accuracy, per-frame processing time, and CPU and memory load while processing the EuRoC datasets, which contain six degree of freedom (6DoF) trajectories typical of flying robots. We present our complete results as a benchmark for the research community.
ER  - 

TY  - CONF
TI  - Direct Sparse Visual-Inertial Odometry Using Dynamic Marginalization
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2510
EP  - 2517
AU  - L. Von Stumberg
AU  - V. Usenko
AU  - D. Cremers
PY  - 2018
KW  - cameras
KW  - distance measurement
KW  - geometry
KW  - image motion analysis
KW  - image sequences
KW  - optimisation
KW  - pose estimation
KW  - optimization
KW  - IMU information
KW  - intensity gradients
KW  - photometric error
KW  - key-point based systems
KW  - photometric IMU measurement errors
KW  - sparse scene geometry
KW  - camera poses
KW  - direct sparse visual-inertial odometry
KW  - VI-DSO
KW  - dynamic marginalization
KW  - visual-inertial system
KW  - IMU data
KW  - gravity direction
KW  - Cameras
KW  - Optimization
KW  - Visualization
KW  - Gravity
KW  - Measurement
KW  - Geometry
KW  - Three-dimensional displays
DO  - 10.1109/ICRA.2018.8462905
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present VI-DSO, a novel approach for visual-inertial odometry, which jointly estimates camera poses and sparse scene geometry by minimizing photometric and IMU measurement errors in a combined energy functional. The visual part of the system performs a bundle-adjustment like optimization on a sparse set of points, but unlike key-point based systems it directly minimizes a photometric error. This makes it possible for the system to track not only corners, but any pixels with large enough intensity gradients. IMU information is accumulated between several frames using measurement preintegration and is inserted into the optimization as an additional constraint between keyframes. We explicitly include scale and gravity direction into our model and jointly optimize them together with other variables such as poses. As the scale is often not immediately observable using IMU data this allows us to initialize our visual-inertial system with an arbitrary scale instead of having to delay the initialization until everything is observable. We perform partial marginalization of old variables so that updates can be computed in a reasonable time. In order to keep the system consistent we propose a novel strategy which we call “dynamic marginalization”. This technique allows us to use partial marginalization even in cases where the initial scale estimate is far from the optimum. We evaluate our method on the challenging EuRoC dataset, showing that VI-DSO outperforms the state of the art.
ER  - 

TY  - CONF
TI  - A Monocular SLAM System Leveraging Structural Regularity in Manhattan World
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2518
EP  - 2525
AU  - H. Li
AU  - J. Yao
AU  - J. Bazin
AU  - X. Lu
AU  - Y. Xing
AU  - K. Liu
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - mobile robots
KW  - optimisation
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - rotation optimization strategy
KW  - parallelism
KW  - global binding method
KW  - absolute rotation
KW  - relative rotation
KW  - translation optimization strategy leveraging coplanarity
KW  - coplanar features
KW  - relative translation
KW  - optimal absolute translation
KW  - 3D line optimization strategy
KW  - structural line segments
KW  - structural features
KW  - structural feature-based optimization module
KW  - 3D map
KW  - structural regularity
KW  - optimization strategies
KW  - monocular SLAM systems
KW  - Manhattan World
KW  - camera poses
KW  - Three-dimensional displays
KW  - Cameras
KW  - Optimization
KW  - Parallel processing
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Estimation
DO  - 10.1109/ICRA.2018.8463165
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The structural features in Manhattan world encode useful geometric information of parallelism, orthogonality and/or coplanarity in the scene. By fully exploiting these structural features, we propose a novel monocular SLAM system which provides accurate estimation of camera poses and 3D map. The foremost contribution of the proposed system is a structural feature-based optimization module which contains three novel optimization strategies. First, a rotation optimization strategy using the parallelism and orthogonality of 3D lines is presented. We propose a global binding method to compute an accurate estimation of the absolute rotation of the camera. Then we propose an approach for calculating the relative rotation to further refine the absolute rotation. Second, a translation optimization strategy leveraging coplanarity is proposed. Coplanar features are effectively identified, and we leverage them by a unified model handling both points and lines to calculate the relative translation, and then the optimal absolute translation. Third, a 3D line optimization strategy utilizing parallelism, orthogonality and coplanarity simultaneously is proposed to obtain an accurate 3D map consisting of structural line segments with low computational complexity. Experiments in man-made environments have demonstrated that the proposed system outperforms existing state-of-the-art monocular SLAM systems in terms of accuracy and robustness.
ER  - 

TY  - CONF
TI  - Visual Saliency-Aware Receding Horizon Autonomous Exploration with Application to Aerial Robotics
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2526
EP  - 2533
AU  - T. Dang
AU  - C. Papachristos
AU  - K. Alexis
PY  - 2018
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - trees (mathematics)
KW  - visual saliency-aware receding horizon autonomous exploration
KW  - reobserving salient regions
KW  - environment exploration rate
KW  - robot endurance
KW  - random tree
KW  - two-step optimization paradigm
KW  - salient objects
KW  - path planner
KW  - visual attention
KW  - aerial robotics
KW  - Visualization
KW  - Robot sensing systems
KW  - Planning
KW  - Computational modeling
KW  - Path planning
DO  - 10.1109/ICRA.2018.8460992
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a novel strategy for autonomous visual saliency-aware receding horizon exploration of unknown environments using aerial robots. Through a model of visual attention, incrementally built maps are annotated regarding the visual importance and saliency of different objects and entities in the environment. Provided this information, a path planner that simultaneously optimizes for exploration of unknown space, and also directs the robot's attention to focus on the most salient objects, is developed. Following a two-step optimization paradigm, the algorithm first samples a random tree and identifies the branch maximizing for new volume to be explored. The first viewpoint of this path is then provided as a reference to the second planning step. Within that, a new tree is spanned, admissible branches arriving at the reference viewpoint while respecting a time budget dependent on the robot endurance and its environment exploration rate are found and evaluated in terms of reobserving salient regions at sufficient resolution. The best branch is then selected and executed by the robot, and the whole process is iteratively repeated. The proposed method is evaluated regarding its ability to provide increased attention toward salient objects, is verified to run onboard a small aerial robot, and is demonstrated in a set of challenging experimental studies.
ER  - 

TY  - CONF
TI  - Perception-aware Receding Horizon Navigation for MAVs
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2534
EP  - 2541
AU  - Z. Zhang
AU  - D. Scaramuzza
PY  - 2018
KW  - aircraft control
KW  - collision avoidance
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - robot vision
KW  - state estimation
KW  - perception-aware receding horizon navigation
KW  - microaerial vehicle
KW  - state estimation uncertainty
KW  - perception-aware receding horizon approach
KW  - monocular state estimation
KW  - candidate trajectories
KW  - perception quality
KW  - collision probability
KW  - receding horizon navigation framework
KW  - improved state estimation accuracy
KW  - goal-reaching task
KW  - purely-reactive navigation system
KW  - MAV
KW  - Trajectory
KW  - State estimation
KW  - Planning
KW  - Navigation
KW  - Cameras
KW  - Measurement
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461133
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - To reach a given destination safely and accurately, a micro aerial vehicle needs to be able to avoid obstacles and minimize its state estimation uncertainty at the same time. To achieve this goal, we propose a perception-aware receding horizon approach. In our method, a single forward-looking camera is used for state estimation and mapping. Using the information from the monocular state estimation and mapping system, we generate a library of candidate trajectories and evaluate them in terms of perception quality, collision probability, and distance to the goal. The best trajectory to execute is then selected as the one that maximizes a reward function based on these three metrics. To the best of our knowledge, this is the first work that integrates active vision within a receding horizon navigation framework for a goal reaching task. We demonstrate by simulation and real-world experiments on an actual quadrotor that our active approach leads to improved state estimation accuracy in a goal-reaching task when compared to a purely-reactive navigation system, especially in difficult scenes (e.g., weak texture).
ER  - 

TY  - CONF
TI  - Viewpoint-Tolerant Place Recognition Combining 2D and 3D Information for UAV Navigation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2542
EP  - 2549
AU  - F. Maffra
AU  - Z. Chen
AU  - M. Chli
PY  - 2018
KW  - autonomous aerial vehicles
KW  - distance measurement
KW  - geometry
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - stereo image processing
KW  - 3D information
KW  - UAV navigation
KW  - Unmanned Aerial Vehicles
KW  - vision-based odometry
KW  - loop-closure detection
KW  - place recognition framework
KW  - local 3D geometry
KW  - viewpoint-tolerant place recognition
KW  - 2D Information
KW  - hand-held datasets
KW  - perceptual aliasing
KW  - binary features
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Visualization
KW  - Vocabulary
KW  - Image recognition
KW  - Navigation
DO  - 10.1109/ICRA.2018.8460786
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The booming interest in Unmanned Aerial Vehicles (UAV s) is fed by their potentially great impact, however progress is hindered by their limited perception capabilities. While vision-based odometry was shown to run successfully onboard UAV s, loop-closure detection to correct for drift or to recover from tracking failures, has so far, proven particularly challenging for UAVs. At the heart of this is the problem of viewpoint-tolerant place recognition; in stark difference to ground robots, UAVs can revisit a scene from very different viewpoints. As a result, existing approaches struggle greatly as the task at hand violates underlying assumptions in assessing scene similarity. In this paper, we propose a place recognition framework, which exploits both efficient binary features and noisy estimates of the local 3D geometry, which are anyway computed for visual-inertial odometry onboard the UAV. Attaching both an appearance and a geometry signature to each `location', the proposed approach demonstrates unprecedented recall for perfect precision as well as high quality loop-closing transformations on both flying and hand-held datasets exhibiting large viewpoint and appearance changes as well as perceptual aliasing.
ER  - 

TY  - CONF
TI  - Flexible Stereo: Constrained, Non-Rigid, Wide-Baseline Stereo Vision for Fixed-Wing Aerial Platforms
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2550
EP  - 2557
AU  - T. Hinzmann
AU  - T. Taubner
AU  - R. Siegwart
PY  - 2018
KW  - aerospace components
KW  - angular velocity measurement
KW  - autonomous aerial vehicles
KW  - cameras
KW  - collision avoidance
KW  - Kalman filters
KW  - nonlinear filters
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - landing maneuvers
KW  - wing model
KW  - probability density function
KW  - measured deviations
KW  - nominal relative baseline transformation
KW  - relative pose measurements
KW  - relative perspective N-point problem
KW  - inertial measurement units
KW  - highly accurate baseline transformations
KW  - flexible stereo
KW  - wide-baseline stereo vision
KW  - fixed-wing aerial platforms
KW  - computationally efficient method
KW  - visual-inertial sensor rigs
KW  - fixed-wing unmanned aerial vehicle
KW  - estimated relative poses
KW  - highly accurate depth maps
KW  - obstacle avoidance
KW  - low-altitude flights
KW  - extended Kalman filter
KW  - Cameras
KW  - Visualization
KW  - Mathematical model
KW  - Quaternions
KW  - Unmanned aerial vehicles
KW  - Real-time systems
KW  - Accelerometers
DO  - 10.1109/ICRA.2018.8461085
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper proposes a computationally efficient method to estimate the time-varying relative pose between two visual-inertial sensor rigs mounted on the flexible wings of a fixed-wing unmanned aerial vehicle (UAV). The estimated relative poses are used to generate highly accurate depth maps in real-time and can be employed for obstacle avoidance in low-altitude flights or landing maneuvers. The approach is structured as follows: Initially, a wing model is identified by fitting a probability density function to measured deviations from the nominal relative baseline transformation. At runtime, the prior knowledge about the wing model is fused in an Extended Kalman filter (EKF) together with relative pose measurements obtained from solving a relative perspective N-point problem (PNP), and the linear accelerations and angular velocities measured by the two inertial measurement units (IMU) which are rigidly attached to the cameras. Results obtained from extensive synthetic experiments demonstrate that our proposed framework is able to estimate highly accurate baseline transformations and depth maps.
ER  - 

TY  - CONF
TI  - Visual-Inertial Navigation Algorithm Development Using Photorealistic Camera Simulation in the Loop
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2566
EP  - 2573
AU  - T. Sayre-McCord
AU  - W. Guerra
AU  - A. Antonini
AU  - J. Arneberg
AU  - A. Brown
AU  - G. Cavalheiro
AU  - Y. Fang
AU  - A. Gorodetsky
AU  - D. McCoy
AU  - S. Quilter
AU  - F. Riether
AU  - E. Tal
AU  - Y. Terzioglu
AU  - L. Carlone
AU  - S. Karaman
PY  - 2018
KW  - aerospace computing
KW  - autonomous aerial vehicles
KW  - computer vision
KW  - control engineering computing
KW  - image sensors
KW  - inertial navigation
KW  - rendering (computer graphics)
KW  - virtual reality
KW  - vision-based perception
KW  - virtual reality
KW  - visual-inertial navigation algorithm
KW  - high-rate cameras
KW  - image simulation system
KW  - photorealistic camera simulation
KW  - agile maneuvering
KW  - on-board inertial measurement unit
KW  - rapidly prototype computing
KW  - on-board camera images
KW  - NVIDIA Jetson Tegra X1 system-on-chip compute module
KW  - inertial sensors
KW  - microUAV platform
KW  - vision-in-the-loop control algorithms
KW  - agile microUnmanned Aerial Vehicles
KW  - Cameras
KW  - Sensors
KW  - Visualization
KW  - Real-time systems
KW  - Unmanned aerial vehicles
KW  - Navigation
KW  - Solid modeling
DO  - 10.1109/ICRA.2018.8460692
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The development of fast, agile micro Unmanned Aerial Vehicles (UAVs) has been limited by (i) on-board computing hardware restrictions, (ii) the lack of sophisticated vision-based perception and vision-in-the-loop control algorithms, and (iii) the absence of development environments where such systems and algorithms can be rapidly and easily designed, implemented, and validated. Here, we first present a new micro UAV platform that integrates high-rate cameras, inertial sensors, and an NVIDIA Jetson Tegra X1 system-on-chip compute module that boasts 256 GPU cores. The UAV mechanics and electronics were designed and built in house, and are described in detail. Second, we present a novel “virtual reality” development environment, in which photorealistically-rendered synthetic on-board camera images are generated in real time while the UAV is in flight. This development environment allows us to rapidly prototype computing and sensing hardware as well as perception and control algorithms, using real physics, real interoceptive sensor data (e.g., from the on-board inertial measurement unit), and synthetic exteroceptive sensor data (e.g., from synthetic cameras). Third, we demonstrate repeated agile maneuvering with closed-loop vision-based perception and control algorithms, which we have developed using this environment.
ER  - 

TY  - CONF
TI  - Development and Implementation of High Power Hexapole Magnetic Tweezer System for Micromanipulations
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2670
EP  - 2675
AU  - X. Zhang
AU  - H. Kim
AU  - L. W. Rogowski
AU  - S. Sheckman
AU  - M. JunKim
PY  - 2018
KW  - coils
KW  - collision avoidance
KW  - electromagnetic actuators
KW  - magnetic devices
KW  - micromanipulators
KW  - microrobots
KW  - mobile robots
KW  - non-Newtonian fluids
KW  - radiation pressure
KW  - three-dimensional printing
KW  - electromagnetic coil
KW  - 3D printed magnetic yokes
KW  - double layer structure
KW  - power source
KW  - microscale robotic swimmer manipulations
KW  - high power hexapole magnetic tweezer system
KW  - tapering-tipped magnetic poles
KW  - Cartesian coordinate system
KW  - 3D micromanipulations
KW  - Newtonian fluid
KW  - nonNewtonian fluid
KW  - obstacle avoidance
KW  - Magnetic flux
KW  - Magnetic resonance imaging
KW  - Magnetic fields
KW  - Saturation magnetization
KW  - Magnetic hysteresis
KW  - Force
KW  - Power supplies
DO  - 10.1109/ICRA.2018.8463175
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents the design, development and implementation of a novel, high power hexapole magnetic tweezer system for 3D micromanipulations. Six tapering-tipped magnetic poles are deployed in a tilted Cartesian coordinate system, with an electromagnetic coil on each for actuation, connected by two 3D printed magnetic yokes to form a double layer structure. The power source is integrated to the magnetic tweezer system through a control algorithm on the software level; image processing was used for experiment analysis. Because of the high magnetic field that the magnetic coils can generate, the working space in the system is relatively larger than other similar designs, which provides better performance on microscale robotic swimmer manipulations. Simulations and experiments performed in this paper demonstrate the agile and powerful manipulation of microswimmers with desired control input to follow complex trajectories, avoid obstacles and move against micro-flow in the samples. We prove that the developed hexapole magnetic tweezer has enough power and controllability to guide microswimmers in Newtonian and Non-Newtonian fluid environments. The system will be optimized continuously and implemented into cell penetration experiments. Finally, the application will be deployed into in vivo based environments.
ER  - 

TY  - CONF
TI  - Robotic Immobilization of Motile Sperm
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2676
EP  - 2681
AU  - Z. Zhang
AU  - C. Dai
AU  - J. Huang
AU  - X. Wang
AU  - J. Liu
AU  - J. Zhang
AU  - S. Moskovtsev
AU  - C. Librach
AU  - K. Jarvi
AU  - Y. Sun
PY  - 2018
KW  - cell motility
KW  - cellular biophysics
KW  - manipulators
KW  - medical robotics
KW  - microorganisms
KW  - position control
KW  - servomechanisms
KW  - visual servoing
KW  - robotic sperm immobilization
KW  - visual servo control
KW  - sperm velocity
KW  - sperm orientation
KW  - sperm tail positions
KW  - robotic system
KW  - proximal sperms
KW  - sperm head
KW  - motile cells
KW  - motile sperm
KW  - robotic immobilization
KW  - Robots
KW  - Head
KW  - Visualization
KW  - Target tracking
KW  - Servosystems
KW  - Glass
DO  - 10.1109/ICRA.2018.8462912
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Manipulation of motile cells such as bacteria and sperm is required in both cell biology and clinical applications. For immobilizing a motile sperm, the sperm head and tail positions must be accurately tracked, interference of proximal sperms on the target sperm must be tackled, and the orientation of the sperm must be properly aligned with the manipulation tool in order not to damage the sperm head where DNA is contained. Manual operation of sperm immobilization has stringent skill requirements, and both manual operation and existing robotic sperm immobilization suffer from inconsistent success rates and incapability of manipulating sperms swimming in all directions. This paper presents a robotic system for fully automated tracking, orientation control, and immobilization of motile sperms. Algorithms were developed for robustly tracking the sperm head and estimating the sperm tail positions under interfering conditions. A new visual servo control strategy was developed to enable the robotic system to actively adjust sperm orientation for immobilizing a sperm swimming in any direction. Experimental results from robotic immobilization of 400 sperms confirmed that the robotic system achieved a consistent success rate of 94.5 %, independent of sperm velocity or swimming direction.
ER  - 

TY  - CONF
TI  - Automated Non-Invasive Measurement of Sperm Motility and Morphology Parameters
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2682
EP  - 2687
AU  - C. Sheng Dai
AU  - Z. Zhang
AU  - J. Huang
AU  - X. Wang
AU  - W. Meng
AU  - J. Zhang
AU  - S. Moskovtsev
AU  - C. Librach
AU  - K. Jarvi
AU  - Y. Sun
PY  - 2018
KW  - biological techniques
KW  - biomedical measurement
KW  - cell motility
KW  - filtering theory
KW  - image reconstruction
KW  - image segmentation
KW  - medical image processing
KW  - probability
KW  - target tracking
KW  - motile cells
KW  - automation techniques
KW  - noninvasive measurement
KW  - adapted joint probabilistic data association filter
KW  - multisperm tracking
KW  - inherent inhomogeneous image intensity
KW  - quadratic cost function method
KW  - DIC image reconstruction
KW  - sperm motility measurement
KW  - differential interference contrast imaging method
KW  - sperm morphology measurement
KW  - image intensity
KW  - sperm subcellular structures
KW  - single sperm motility
KW  - sperm morphology parameters
KW  - illumination effect
KW  - Morphology
KW  - Switches
KW  - Head
KW  - Target tracking
KW  - Microscopy
KW  - Robots
DO  - 10.1109/ICRA.2018.8461252
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Measuring the motility and morphology parameters of motile cells is important for revealing their functional characteristics. This paper presents automation techniques that, for the first time, enable automated, non-invasive measurement of motility and morphology parameters of individual sperms. Compared to the status quo of qualitative estimation of single sperm's motility and morphology based on embryologists' empirical experience, the automation techniques provide quantitative data in nearly real time. An adapted joint probabilistic data association filter (JPDAF) was used for multi-sperm tracking and tackled challenges of identifying sperms that intersect or have small spatial distances. Since the standard differential interference contrast (DIC) imaging method has side illumination effect which causes inherent inhomogeneous image intensity and poses difficulties for accurate sperm morphology measurement, we integrated total variation norm into the quadratic cost function method, which together effectively removed inhomogeneous image intensity and retained sperm's subcellular structures after DIC image reconstruction. In order to relocate the same sperm of interest identified under low magnification after switching to high magnification, coordinate transformation was conducted to handle the changes in the field of view caused by magnification switch. Experimental results demonstrated an accuracy of 95.6% in sperm motility measurement and errors <;10% in morphology measurement.
ER  - 

TY  - CONF
TI  - Construction of Hepatic Lobule-Like Vascular Network by Using Magnetic Fields
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2688
EP  - 2693
AU  - E. Kim
AU  - M. Takeuchi
AU  - W. Atou
AU  - Y. Iwamoto
AU  - T. Nomura
AU  - T. Kozuka
AU  - A. Hasegawa
AU  - A. Ichikawa
AU  - T. Fukuda
PY  - 2018
KW  - biomagnetism
KW  - biomedical materials
KW  - blood vessels
KW  - cellular biophysics
KW  - ferrites
KW  - gels
KW  - liver
KW  - molecular biophysics
KW  - proteins
KW  - tissue engineering
KW  - cell viability
KW  - multilayered structure
KW  - different magnetic poles
KW  - magnetic tweezer
KW  - magnetizer
KW  - alginate gel fibers
KW  - magnetic fibers
KW  - kinds veins
KW  - portal vein
KW  - central vein
KW  - 3D cellular structure
KW  - transporting required nutrients
KW  - magnetic fields
KW  - vascular network
KW  - hepatic lobule-like
KW  - temperature 22.0 degC
KW  - time 3.0 d
KW  - Steel
KW  - Optical fiber networks
KW  - Veins
KW  - Magnetic flux
KW  - Three-dimensional displays
KW  - Electromagnetics
KW  - Toroidal magnetic fields
DO  - 10.1109/ICRA.2018.8460628
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Fabrication of vascular network is an important research for transporting required nutrients and oxygen to the artificial tissues. In this paper, we propose a novel method to construct a hepatic lobule-like vascular network in a 3D cellular structure. The network is simply constructed by three types of veins, central vein, portal vein, and sinusoids. To realize these kinds veins, we utilize two different sizes of steel rods and magnetic fibers for delivering nutrients in 3D cellular structure. Alginate gel fibers embedding ferrite particles are prepared as the same length and are magnetized by magnetizer at 3T. A magnetic tweezer with seven poles is proposed to generate sufficient forces that can manipulate magnetized fibers. Here, two types of rods are magnetized to different magnetic poles in order to attract opposite the end of fibers. This manipulation process is performed in fibrinogen and thrombin solution with liver cells (RLC-18). After solidification of the solution, we deposit solutions with cells and fibers repeatedly, and therefore, a multi-layered structure can be constructed. In addition, we investigate cell a viability in fibrin gel according to the depth of the gel. The result is that the deeper the depth of the gel is, the lower the cell viability is. The cell viability is conducted in several condition. As a result, at the low temperature (here at 22 °C), the viability of cell is increased.
ER  - 

TY  - CONF
TI  - A Framework for Sensorless Tissue Motion Tracking in Robotic Endomicroscopy Scanning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2694
EP  - 2699
AU  - P. Triantafyllou
AU  - P. Wisanuvej
AU  - S. Giannarou
AU  - J. Liu
AU  - G. Yang
PY  - 2018
KW  - biological tissues
KW  - biomedical optical imaging
KW  - endoscopes
KW  - image resolution
KW  - image segmentation
KW  - laser applications in medicine
KW  - medical image processing
KW  - medical robotics
KW  - optical microscopy
KW  - probe-based confocal laser endomicroscopy
KW  - robotic endomicroscopy scanning
KW  - image-quality metric
KW  - sensorless tissue motion tracking
KW  - ex vivo porcine tissue validate
KW  - autonomous endomicroscopy scanning
KW  - pCLE robotic tool
KW  - novel sensorless framework
KW  - sensorless approaches
KW  - endomicroscopy probe
KW  - robotic manipulation
KW  - tissue deformation
KW  - probe-tissue contact force
KW  - Probes
KW  - Tools
KW  - Force
KW  - Robot sensing systems
KW  - Strain
DO  - 10.1109/ICRA.2018.8462907
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Recent advances in probe-based Confocal Laser Endomicroscopy (pCLE) enable real-time, in situ and in vivo tissue assessment at the micro scale. The limited field-of-view offered by pCLE necessitates the use of mosaicking to allow for accurate tissue characterization from the incoming image stream. However, mosaicking requires a series of contiguous good-quality images, which is particularly challenging because probe-tissue distance must be maintained within a very narrow working range at all times and probe-tissue contact force must be kept to a minimum so that tissue deformation is avoided. Robotic manipulation of the endomicroscopy probe has provided partial solution to these challenges, but sensorless approaches have not been thoroughly investigated up to date. This paper proposes a novel sensorless framework that uses a single non-reference image-quality metric to learn an approximation of tissue motion and subsequently track it. Moreover, a pCLE robotic tool for autonomous endomicroscopy scanning is designed and used for testing and validation purposes. Experiments on lens paper and ex vivo porcine tissue validate the philosophy of the framework.
ER  - 

TY  - CONF
TI  - SAT-C: An Efficient Control Strategy for Assembly of Heterogeneous Stress-Engineered MEMS Microrobots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2700
EP  - 2707
AU  - V. Foroutan
AU  - F. Farzami
AU  - D. Erricolo
AU  - R. Majumdar
AU  - I. Paprotny
PY  - 2018
KW  - controllability
KW  - microrobots
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - position control
KW  - control primitives
KW  - control pulses
KW  - microrobotic systems
KW  - control policy
KW  - planar assembly
KW  - efficient control strategy
KW  - heterogeneous stress-engineered MEMS microrobots
KW  - efficient control framework
KW  - controllable microrobots
KW  - theoretical control strategy
KW  - multiple-shapes microassembly
KW  - arbitrary initial configuration
KW  - power delivery waveform
KW  - nonholonomic unicycles
KW  - multiple macroscale robots
KW  - direct drive wheels
KW  - Robots
KW  - Hysteresis
KW  - Microassembly
KW  - Micromechanical devices
KW  - Voltage control
KW  - Bandwidth
KW  - Actuators
DO  - 10.1109/ICRA.2018.8460481
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a new efficient control framework for controlling groups of heterogeneous stress-engineered MEMS microrobots for accomplishing micro-assembly. The objective is to maximize the number of controllable microrobots in the system while keeping the number of external global signals as low as possible. This work proposes a theoretical control strategy that could complete multiple-shapes microassembly from arbitrary initial configuration where all the control primitives can be accompanied with a constant number (O(1)) of control pulses of the power delivery waveform. We focus on microrobotic systems that can be modeled as nonholonomic unicycles. We validate the control policy with hardware experiments for implementing planar assembly using multiple macroscale robots with direct drive wheels. These results lay the foundation for developing new methods to control of a large number of MEMS microrobots.
ER  - 

TY  - CONF
TI  - Robotic Intracellular Manipulation: 3D Navigation and Measurement Inside a Single Cell
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2716
EP  - 2721
AU  - X. Wang
AU  - M. Luo
AU  - C. Ho
AU  - Z. Zhang
AU  - Q. Zhao
AU  - C. Dai
AU  - Y. Sun
PY  - 2018
KW  - biomagnetism
KW  - biomechanics
KW  - bioMEMS
KW  - Brownian motion
KW  - cancer
KW  - cellular biophysics
KW  - force control
KW  - medical robotics
KW  - micromanipulators
KW  - optical microscopy
KW  - patient treatment
KW  - position control
KW  - predictive control
KW  - magnetic bead
KW  - cell nucleus minor axes
KW  - cell nucleus major axes
KW  - stiffness polarity
KW  - sub-micrometer object
KW  - tissue level
KW  - untethered technique
KW  - 3D navigation
KW  - robotic intracellular manipulation
KW  - force-displacement data
KW  - Brownian motion-imposed constraint
KW  - high-resolution confocal microscopy
KW  - slow visual feedback
KW  - generalized predictive controller
KW  - single human bladder cancer cell
KW  - piconewton force control
KW  - sub-micrometer position control
KW  - magnetic micromanipulation task
KW  - size 0.7 mum
KW  - frequency 1.0 Hz
KW  - distance 0.43 mum
KW  - Magnetic resonance imaging
KW  - Magnetic levitation
KW  - Force
KW  - Magnetic noise
KW  - Magnetic shielding
KW  - Magnetic devices
KW  - Magnetic separation
DO  - 10.1109/ICRA.2018.8463170
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Magnetic micromanipulation is an untethered technique and has enabled numerous applications in the scale of millimeters to micrometers from the tissue level to cell level. However, existing systems are not capable of maneuvering a sub-micrometer object for precise force control, preventing the realization of intracellular manipulation or `fantastic voyage' inside a single cell. The magnetic micromanipulation task achieved in this work is sub-micrometer position control and piconewton force control of a sub-micron (0.7 μm) magnetic bead inside a single human bladder cancer cell (RT4). The magnetic bead was 3D positioned in the cell using a generalized predictive controller that effectively tackled the control challenge caused by the slow visual feedback (1 Hz) from high-resolution confocal microscopy. The average positioning error was quantified to be 0.43 μm, which is slightly larger than Brownian motion-imposed constraint (0.31 μm). The system is capable of three-dimensionally applying a maximum force of 60 pN with a resolution of 4 pN. In experiments, a 0.7 μm magnetic bead was controlled to move from an initial position in a cell to target positions on the cell nucleus. Force-displacement data were obtained from multiple locations along the cell nucleus' major and minor axes. The results revealed, for the first time, significantly higher stiffness exists in the cell nucleus' major axis than the minor axis. This stiffness polarity was likely attributed to the aligned stress fibers of actin filament inside the cells.
ER  - 

TY  - CONF
TI  - ViTac: Feature Sharing Between Vision and Tactile Sensing for Cloth Texture Recognition
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2722
EP  - 2727
AU  - S. Luo
AU  - W. Yuan
AU  - E. Adelson
AU  - A. G. Cohn
AU  - R. Fuentes
PY  - 2018
KW  - covariance analysis
KW  - feature extraction
KW  - image recognition
KW  - image texture
KW  - neural nets
KW  - touch (physiological)
KW  - tactile data
KW  - cloth textures
KW  - good recognition performance
KW  - perception performance
KW  - tactile sensing
KW  - shared representation space
KW  - feature sharing
KW  - cloth texture recognition
KW  - multimodal sensing ability
KW  - tactile images
KW  - Deep Maximum Covariance Analysis
KW  - learned features
KW  - DMCA framework
KW  - unimodal data
KW  - joint latent space
KW  - Gelsight sensor
KW  - deep neural networks
KW  - sensing modalities
KW  - Visualization
KW  - Tactile sensors
KW  - Cameras
KW  - Task analysis
KW  - Surface topography
DO  - 10.1109/ICRA.2018.8460494
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Vision and touch are two of the important sensing modalities for humans and they offer complementary information for sensing the environment. Robots could also benefit from such multi-modal sensing ability. In this paper, addressing for the first time (to the best of our knowledge) texture recognition from tactile images and vision, we propose a new fusion method named Deep Maximum Covariance Analysis (DMCA) to learn a joint latent space for sharing features through vision and tactile sensing. The features of camera images and tactile data acquired from a GelSight sensor are learned by deep neural networks. But the learned features are of a high dimensionality and are redundant due to the differences between the two sensing modalities, which deteriorates the perception performance. To address this, the learned features are paired using maximum covariance analysis. Results of the algorithm on a newly collected dataset of paired visual and tactile data relating to cloth textures show that a good recognition performance of greater than 90% can be achieved by using the proposed DMCA framework. In addition, we find that the perception performance of either vision or tactile sensing can be improved by employing the shared representation space, compared to learning from unimodal data.
ER  - 

TY  - CONF
TI  - Calibration and Analysis of Tactile Sensors as Slip Detectors
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2744
EP  - 2751
AU  - K. Van Wyk
AU  - J. Falco
PY  - 2018
KW  - calibration
KW  - force sensors
KW  - neural nets
KW  - slip
KW  - tactile sensors
KW  - long short-term memory neural networks
KW  - high-quality slip detection
KW  - tactile technologies
KW  - electro-mechanical resistance
KW  - sensing mechanics
KW  - tactile sensing
KW  - robust slip detectors
KW  - sensor behavior
KW  - sensory responses
KW  - spectral analysis
KW  - sensory data points
KW  - systematic data collection process
KW  - robust slip detection
KW  - tactile-based slip detection
KW  - secondary force modulation protocols
KW  - human hand
KW  - mechanical transients
KW  - tactile afferents
KW  - Force
KW  - Tactile sensors
KW  - Detectors
KW  - Robustness
KW  - Spectral analysis
KW  - tactile sensors
KW  - slip detection
KW  - neural networks
KW  - deep learning
DO  - 10.1109/ICRA.2018.8461117
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The existence of tactile afferents sensitive to slip-related mechanical transients in the human hand augments the robustness of grasping through secondary force modulation protocols. Despite this knowledge and the fact that tactile-based slip detection has been researched for decades, robust slip detection is still not an out-of-the-box capability for any commercially available tactile sensor. This research seeks to bridge this gap with a comprehensive study addressing several aspects of slip detection. In particular, key developments include a systematic data collection process yielding millions of sensory data points, a spectral analysis of sensory responses providing insight into sensor behavior, and the application of Long Short-Term Memory (LSTM) neural networks to produce robust slip detectors from three commercially available sensors capable of tactile sensing. The sensing mechanics behind these sensors are all fundamentally different and leverage principles in electro-mechanical resistance, optics, and hydro-acoustics. Critically, slip detection performance of the tactile technologies is quantified through a measurement methodology that unveils the effects of data window size, sampling rate, material type, slip speed, and sensor manufacturing variability. Results indicate that the investigated commercial tactile sensors are inherently capable of high-quality slip detection.
ER  - 

TY  - CONF
TI  - Voronoi Features for Tactile Sensing: Direct Inference of Pressure, Shear, and Contact Locations
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2752
EP  - 2757
AU  - L. Cramphorn
AU  - J. Lloyd
AU  - N. F. Lepora
PY  - 2018
KW  - calibration
KW  - computational geometry
KW  - computerised instrumentation
KW  - inference mechanisms
KW  - optical sensors
KW  - pressure measurement
KW  - pressure sensors
KW  - tactile sensors
KW  - transducers
KW  - Voronoi tessellation
KW  - visualisation mode
KW  - optical tactile sensor
KW  - shear magnitude
KW  - tactile contact
KW  - object grasping
KW  - manipulation
KW  - perception
KW  - contact location inference
KW  - pressure location inference
KW  - shear location inference
KW  - transducing method
KW  - local shear measurement
KW  - TacTip
KW  - calibration
KW  - complex systems
KW  - robot hands
KW  - Pins
KW  - Tactile sensors
KW  - Optical sensors
KW  - Data visualization
KW  - Biomedical optical imaging
KW  - Strain
DO  - 10.1109/ICRA.2018.8460644
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - There are a wide range of features that tactile contact provides, each with different aspects of information that can be used for object grasping, manipulation, and perception. In this paper inference of some key tactile features, tip displacement, contact location, shear direction and magnitude, is demonstrated by introducing a novel method of transducing a third dimension to the sensor data via Voronoi tessellation. The inferred features are displayed throughout the work in a new visualisation mode derived from the Voronoi tessellation; these visualisations create easier interpretation of data from an optical tactile sensor that measures local shear from displacement of internal pins (the TacTip). The output values of tip displacement and shear magnitude are calibrated to appropriate mechanical units and validate the direction of shear inferred from the sensor. We show that these methods can infer the direction of shear to ~2.3° without the need for training a classifier or regressor. The approach demonstrated here will increase the versatility and generality of the sensors and thus allow sensor to be used in more unstructured and unknown environments, as well as improve the use of these tactile sensors in more complex systems such as robot hands.
ER  - 

TY  - CONF
TI  - Learning Manipulation Graphs from Demonstrations Using Multimodal Sensory Signals
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2758
EP  - 2765
AU  - Z. Su
AU  - O. Kroemer
AU  - G. E. Loeb
AU  - G. S. Sukhatme
AU  - S. Schaal
PY  - 2018
KW  - dexterous manipulators
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - tactile sensors
KW  - erroneous contact states
KW  - manipulation demonstrations
KW  - motor primitive
KW  - manipulation task
KW  - insertion tasks
KW  - learned manipulation graphs
KW  - robust manipulation executions
KW  - sensory goals
KW  - multimodal sensory signals
KW  - complex contact manipulation tasks
KW  - contact state
KW  - contact state information
KW  - Barrett arm
KW  - BioTacs
KW  - contact changes
KW  - Robot sensing systems
KW  - Task analysis
KW  - Fasteners
KW  - Trajectory
KW  - Motion segmentation
KW  - Vibrations
DO  - 10.1109/ICRA.2018.8461121
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Complex contact manipulation tasks can be decomposed into sequences of motor primitives. Individual primitives often end with a distinct contact state, such as inserting a screwdriver tip into a screw head or loosening it through twisting. To achieve robust execution, the robot should be able to verify that the primitive's goal has been reached as well as disambiguate it from erroneous contact states. In this paper, we introduce and evaluate a framework to autonomously construct manipulation graphs from manipulation demonstrations. Our manipulation graphs include sequences of motor primitives for performing a manipulation task as well as corresponding contact state information. The sensory models for the contact states allow the robot to verify the goal of each motor primitive as well as detect erroneous contact changes. The proposed framework was experimentally evaluated on grasping, unscrewing, and insertion tasks on a Barrett arm and hand equipped with two BioTacs. The results of our experiments indicate that the learned manipulation graphs achieve more robust manipulation executions by confirming sensory goals as well as discovering and detecting novel failure modes.
ER  - 

TY  - CONF
TI  - ExoSense: Measuring Manipulation in a Wearable Manner
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2774
EP  - 2781
AU  - E. Battaglia
AU  - M. G. Catalano
AU  - G. Grioli
AU  - M. Bianchi
AU  - A. Bicchi
PY  - 2018
KW  - design engineering
KW  - dexterous manipulators
KW  - end effectors
KW  - force control
KW  - force measurement
KW  - force sensors
KW  - grippers
KW  - manipulator dynamics
KW  - medical robotics
KW  - position control
KW  - tactile sensors
KW  - torque
KW  - end-effector posture measurements
KW  - human internal grasp force variations
KW  - robotic hand
KW  - passive hand exoskeleton
KW  - wearable manner
KW  - design engineering
KW  - forces measurement
KW  - exosense
KW  - robotic manipulators control
KW  - fingertip wearable force
KW  - torque sensing system
KW  - position control
KW  - Kinematics
KW  - Exoskeletons
KW  - Robot sensing systems
KW  - Force
KW  - Calibration
DO  - 10.1109/ICRA.2018.8460498
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Grasp and manipulation is a complex task, deceivingly simple to accomplish for humans in everyday life, yet challenging to implement in a robotic hand. There is a trend in literature to use information obtained from studies on human grasp for the design and control of robotic manipulators. However, the effectiveness of such approach is dependent on the measurement tools that are available for use with human hands. While there are many sensing solutions that are designed for this purpose, obtaining a complete set of measurements of forces during grasp interaction is still challenging. In this work we aim to bridge this gap by introducing ExoSense, a passive hand exoskeleton. This device can provide position and orientation of the fingertips and, when integrated with the fingertip wearable force/torque sensing system ThimbleSense, a complete characterization of manipulation in terms of generalized forces and position of contacts on each fingertip in a completely wearable and unconstrained manner. After validating the device in terms of end-effector posture measurements and overall accuracy of grasp measurements, we report on a preliminary experiment aiming to show the potentialities of the system to study human internal grasp force variations and for neuroscientific investigation in general.
ER  - 

TY  - CONF
TI  - Robotizing Double-Bar Ankle-Foot Orthosis
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2782
EP  - 2787
AU  - T. Noda
AU  - A. Takai
AU  - T. Teramae
AU  - E. Hirookai
AU  - K. Hase
AU  - J. Morimoto
PY  - 2018
KW  - actuators
KW  - cables (mechanical)
KW  - gait analysis
KW  - iterative learning control
KW  - medical robotics
KW  - muscle
KW  - orthotics
KW  - patient rehabilitation
KW  - pneumatic actuators
KW  - position control
KW  - pulleys
KW  - shafts
KW  - springs (mechanical)
KW  - double-bar ankle-foot orthosis
KW  - post-stroke gait rehabilitation
KW  - double-bar AFO
KW  - rehabilitation facilities
KW  - pneumatic actuator
KW  - Bowden cable force-transmission system
KW  - modular joint system
KW  - Modular Exoskeletal Joint
KW  - MEJ
KW  - hollow shaft
KW  - AFO's pivot
KW  - Bowden cables
KW  - contraction forces
KW  - actuation scheme
KW  - Nested-cylinder Pneumatic Artificial Muscle system
KW  - PAM
KW  - ideal actuation system
KW  - exoskeletal robots
KW  - NcPAM houses
KW  - cable-tensioning spring
KW  - cable tension
KW  - cable stopper
KW  - ankle-joint trajectory tracking performances
KW  - integrated system
KW  - iterative learning control
KW  - Robots
KW  - Force
KW  - Actuators
KW  - Mechanical cables
KW  - Exoskeletons
KW  - Torque
KW  - Springs
DO  - 10.1109/ICRA.2018.8462911
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper introduces an approach that robotizes an ankle-foot orthosis (AFO). In particular, toward post-stroke gait rehabilitation, we robotize a double-bar AFO, which is widely used in rehabilitation facilities, by newly designing a modular joint, a pneumatic actuator, and a Bowden cable force-transmission system. Our modular joint system, called the Modular Exoskeletal Joint (MEJ), has a hollow shaft for simple attachment to an AFO's pivot. We designed MEJ to compactly house an encoder that is built in a bearing in a pulley. We adopted Bowden cables to transmit contraction forces from an actuator to the MEJ. As an actuation scheme, we developed the Nested-cylinder Pneumatic Artificial Muscle (NcPAM) system. Even though PAMs are mechanically compliant and lightweight, they can still generate a large force. Therefore, they can provide an ideal actuation system for exoskeletal robots. The nested-cylinder in NcPAM houses a cable-tensioning spring to properly maintain small cable tension for passive movements and a cable stopper to connect the PAM and the cable for properly transmitting the large force generated by PAM. We show the ankle-joint trajectory tracking performances of this integrated system using iterative learning control.
ER  - 

TY  - CONF
TI  - Design and Benchtop Validation of a Powered Knee-Ankle Prosthesis with High-Torque, Low-Impedance Actuators
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2788
EP  - 2795
AU  - T. Elery
AU  - S. Rezazadeh
AU  - C. Nesler
AU  - J. Doan
AU  - H. Zhu
AU  - R. D. Gregg
PY  - 2018
KW  - actuators
KW  - artificial limbs
KW  - gait analysis
KW  - motion control
KW  - open loop systems
KW  - torque control
KW  - powered knee-ankle prosthesis
KW  - low-impedance actuators
KW  - high torque density actuators
KW  - low-reduction transmissions
KW  - low-speed motor
KW  - low mechanical impedance
KW  - high backdrivability
KW  - robotic prosthetic legs
KW  - negligible unmodeled actuator dynamics
KW  - power regeneration
KW  - free-swinging knee tests
KW  - open-loop impedance control tests
KW  - intrinsic impedance
KW  - joint impedance
KW  - torque feedback
KW  - powered knee-and-ankle transfemoral prosthetic leg
KW  - actuation styles
KW  - free-swinging knee motion
KW  - size 3.0 nm
KW  - Actuators
KW  - Gears
KW  - Legged locomotion
KW  - Knee
KW  - Torque
KW  - Brushless DC motors
DO  - 10.1109/ICRA.2018.8461259
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper describes the design of a powered knee-and-ankle transfemoral prosthetic leg, which implements high torque density actuators with low-reduction transmissions. The low reduction of the transmission coupled with a high-torque and low-speed motor creates an actuator with low mechanical impedance and high backdrivability. This style of actuation presents several possible benefits over modern actuation styles implemented in emerging robotic prosthetic legs. Such benefits include free-swinging knee motion, compliance with the ground, negligible unmodeled actuator dynamics, and greater potential for power regeneration. Benchtop validation experiments were conducted to verify some of these benefits. Backdrive and free-swinging knee tests confirm that both joints can be backdriven by small torques (~3 Nm). Bandwidth tests reveal that the actuator is capable of achieving frequencies required for walking and running. Lastly, open-loop impedance control tests prove that the intrinsic impedance and unmodeled dynamics of the actuator are sufficiently small to control joint impedance without torque feedback.
ER  - 


