TY  - CONF
TI  - Internally-Balanced Magnetic Mechanisms Using a Magnetic Spring for Producing a Large Amplified Clamping Force
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1840
EP  - 1846
AU  - T. Shimizu
AU  - K. Tadakuma
AU  - M. Watanabe
AU  - E. Takane
AU  - M. Konyo
AU  - S. Tadokoro
PY  - 2020
KW  - clamps
KW  - control system synthesis
KW  - force control
KW  - magnetic devices
KW  - mobile robots
KW  - multi-robot systems
KW  - permanent magnets
KW  - springs (mechanical)
KW  - magnetic spring
KW  - magnetic mechanisms
KW  - clamping force
KW  - permanent magnet
KW  - force control
KW  - attractive force
KW  - internally-balanced magnetic unit
KW  - magnetic devices
KW  - internal force
KW  - internally-balanced magnetic mechanisms
KW  - IB magnet
KW  - nonlinear spring
KW  - unlike-pole pair
KW  - wall-climbing robots
KW  - ceiling-dangling drones
KW  - modular swarm robots
KW  - robotic clamp
KW  - Springs
KW  - Force
KW  - Magnetic noise
KW  - Magnetic shielding
KW  - Magnetic levitation
KW  - Magnetic liquids
KW  - Magnetic separation
KW  - Mechanism Design of Manipulators
KW  - Force Control
DO  - 10.1109/ICRA40945.2020.9197151
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To detach a permanent magnet using a control force much smaller than its original attractive force, the internally-balanced magnetic unit (IB Magnet) was invented. It has been applied to magnetic devices such as wall-climbing robots, ceiling-dangling drones, and modular swarm robots. In contrast to its significant reduction rate with regard to the control force, the IB Magnet has two major problems in its nonlinear spring, which serves the purpose of cancelling out the internal force on the magnet. These problems include the complicated design procedure and the trade-off relationship between balancing the precision and the volume of the mechanism. This paper proposes a principle for a new balancing method for the IB Magnet. This method uses a like-pole pair of magnets as a magnetic spring, whose repulsive force should equal the attractive force of an unlike-pole pair. To verify the proposed principle, a prototype of the IB Magnet was designed using a magnetic spring and verified through experiments such that its reduction rate is comparable to those of conventional IB Magnets. Moreover, a robotic clamp was developed as an application example that contains the proposed IB Magnets as its internal mechanism.
ER  - 

TY  - CONF
TI  - A Continuum Manipulator with Closed-form Inverse Kinematics and Independently Tunable Stiffness
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1847
EP  - 1853
AU  - B. Zhao
AU  - L. Zeng
AU  - B. Wu
AU  - K. Xu
PY  - 2020
KW  - control system synthesis
KW  - dexterous manipulators
KW  - end effectors
KW  - manipulator kinematics
KW  - rigidity
KW  - continuum manipulator
KW  - closed-form inverse kinematics
KW  - independently tunable stiffness
KW  - compliant structures
KW  - articulated manipulator design
KW  - manipulator end-effector
KW  - analytical inverse kinematics
KW  - Manipulators
KW  - Kinematics
KW  - Shape
KW  - Electron tubes
KW  - Friction
KW  - Payloads
DO  - 10.1109/ICRA40945.2020.9196688
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Continuum manipulators can accomplish various tasks in confined spaces, benefiting from their compliant structures and improved dexterity. Confined and unstructured spaces may require both enhanced stiffness of a continuum manipulator for precision and payload, as well as compliance for safe interaction. Thus, studies have been consistently dedicated to design continuum or articulated manipulators with tunable stiffness to adapt to different operating conditions. This paper presents a continuum manipulator with independently tunable stiffness where the stiffness variation does not affect the movement of the manipulator's end-effector. Moreover, the proposed continuum manipulator is found to have analytical inverse kinematics. The design concept, analytical kinematics, system construction and experimental characterizations are presented. The results showed that the manipulator's stiffness can be increased up to 3.61 times of the minimal value, demonstrating the effectiveness of the proposed idea.
ER  - 

TY  - CONF
TI  - Design and Compensation Control of a Flexible Instrument for Endoscopic Surgery
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1860
EP  - 1866
AU  - W. Hong
AU  - A. Schmitz
AU  - W. Bai
AU  - P. Berthet-Rayne
AU  - L. Xie
AU  - G. -Z. Yang
PY  - 2020
KW  - actuators
KW  - compensation
KW  - end effectors
KW  - endoscopes
KW  - grippers
KW  - medical robotics
KW  - surgery
KW  - flexible instrument
KW  - endoscopic surgery
KW  - snake-like robots
KW  - flexible tendon-driven instruments
KW  - microsurgical tasks
KW  - standard endoscopic surgeries
KW  - articulated wrists
KW  - distal-roll gripper
KW  - compensation control scheme
KW  - end-effector rolling motion
KW  - Tendons
KW  - Instruments
KW  - Grippers
KW  - Gears
KW  - Robots
KW  - Joints
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9196955
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Snake-like robots for endoscopic surgery make it possible to reach deep-seated lesions. With the use of small flexible tendon-driven instruments, it is possible to perform bimanual micro-surgical tasks that are challenging for standard endoscopic surgeries. Existing devices, however, lack articulated wrists and rolling motion of the end-effector. This paper presents a new instrument design with a distal-roll gripper for snake-like robots. The developed 5 DoFs miniaturized instruments with a diameter of 3 mm enable the deployment into narrow endoluminal channels. Issues related to actuation coupling, tendon slack, and backlash are addressed. Experimental results show that the distal-roll gripper can rotate 106°, and the actuated joints can achieve good repeatability and accuracy with the proposed compensation control scheme.
ER  - 

TY  - CONF
TI  - Distance and Steering Heuristics for Streamline-Based Flow Field Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1867
EP  - 1873
AU  - K. Y. Cadmus To
AU  - C. Yoo
AU  - S. Anstee
AU  - R. Fitch
PY  - 2020
KW  - computational fluid dynamics
KW  - flow simulation
KW  - marine robots
KW  - mobile robots
KW  - path planning
KW  - robot dynamics
KW  - artificial flow field
KW  - East Australian current
KW  - streamline-based flow field planning
KW  - motion planning
KW  - streamline-based planning
KW  - fluid dynamics
KW  - travel distance
KW  - incompressible flows
KW  - ocean currents
KW  - distance functions
KW  - Euclidean distance
KW  - stream function
KW  - steering heuristics
KW  - ocean prediction data
KW  - autonomous marine robots
KW  - Planning
KW  - Aerospace electronics
KW  - Vehicle dynamics
KW  - Two dimensional displays
KW  - Space exploration
KW  - Space vehicles
KW  - Oceans
DO  - 10.1109/ICRA40945.2020.9196555
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion planning for vehicles under the influence of flow fields can benefit from the idea of streamline-based planning, which exploits ideas from fluid dynamics to achieve computational efficiency. Important to such planners is an efficient means of computing the travel distance and direction between two points in free space, but this is difficult to achieve in strong incompressible flows such as ocean currents. We propose two useful distance functions in analytical form that combine Euclidean distance with values of the stream function associated with a flow field, and with an estimation of the strength of the opposing flow between two points. Further, we propose steering heuristics that are useful for steering towards a sampled point. We evaluate these ideas by integrating them with RRT* and comparing the algorithm's performance with state-of-the-art methods in an artificial flow field and in actual ocean prediction data in the region of the dominant East Australian Current between Sydney and Brisbane. Results demonstrate the method's computational efficiency and ability to find high-quality paths outperforming state-of-the-art methods, and show promise for practical use with autonomous marine robots.
ER  - 

TY  - CONF
TI  - Enhancing Coral Reef Monitoring Utilizing a Deep Semi-Supervised Learning Approach
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1874
EP  - 1880
AU  - M. Modasshir
AU  - I. Rekleitis
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - marine engineering
KW  - object detection
KW  - supervised learning
KW  - deep neural network
KW  - sample extraction
KW  - coral object dataset
KW  - coral reef monitoring
KW  - deep semisupervised learning approach
KW  - coral species detection
KW  - convolutional neural network-based object detector
KW  - Detectors
KW  - Training
KW  - Object detection
KW  - Monitoring
KW  - Tracking
KW  - Predictive models
KW  - Semantics
DO  - 10.1109/ICRA40945.2020.9196528
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Coral species detection underwater is a challenging problem. There are many cases when even the experts (marine biologists) fail to recognize corals, hence limiting ground truth annotation for training a robust detection system. Identifying coral species is fundamental for enabling the monitoring of coral reefs, a task currently performed by humans, which can be automated with the use of underwater robots. By employing temporal cues using a tracker on a high confidence prediction by a convolutional neural network-based object detector, we augment the collected dataset for the retraining of the object detector. However, using trackers to extract examples also introduces hard or mislabelled samples, which is counterproductive and will deteriorate the performance of the detector. In this work, we show that employing a simple deep neural network to filter out hard or mislabelled samples can help regulate sample extraction. We empirically evaluate our approach in a coral object dataset, collected via an Autonomous Underwater Vehicle (AUV) and human divers, that shows the benefit of incorporating extracted examples obtained from tracking. This work also demonstrates how controlling sample generation by tracking using a simple deep neural network can further improve an object detector.
ER  - 

TY  - CONF
TI  - DOB-Net: Actively Rejecting Unknown Excessive Time-Varying Disturbances
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1881
EP  - 1887
AU  - T. Wang
AU  - W. Lu
AU  - Z. Yan
AU  - D. Liu
PY  - 2020
KW  - control system synthesis
KW  - feedback
KW  - learning (artificial intelligence)
KW  - neurocontrollers
KW  - observers
KW  - optimal control
KW  - position control
KW  - recurrent neural nets
KW  - robots
KW  - robot control capabilities
KW  - disturbance dynamics observer network
KW  - controller network
KW  - conventional DOB mechanisms
KW  - recurrent neural networks
KW  - optimal control signals
KW  - conventional feedback controllers
KW  - DOB-Net
KW  - disturbance OB-server network
KW  - observer-integrated reinforcement learning
KW  - Observers
KW  - History
KW  - Vehicle dynamics
KW  - Robots
KW  - Optimization
KW  - Optimal control
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196641
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an observer-integrated Reinforcement Learning (RL) approach, called Disturbance OB-server Network (DOB-Net), for robots operating in environments where disturbances are unknown and time-varying, and may frequently exceed robot control capabilities. The DOB-Net integrates a disturbance dynamics observer network and a controller network. Originated from conventional DOB mechanisms, the observer is built and enhanced via Recurrent Neural Networks (RNNs), encoding estimation of past values and prediction of future values of unknown disturbances in RNN hidden state. Such encoding allows the controller generate optimal control signals to actively reject disturbances, under the constraints of robot control capabilities. The observer and the controller are jointly learned within policy optimization by advantage actor critic. Numerical simulations on position regulation tasks have demonstrated that the proposed DOB-Net significantly outperforms conventional feedback controllers and classical RL policy.
ER  - 

TY  - CONF
TI  - Demonstration of Autonomous Nested Search for Local Maxima Using an Unmanned Underwater Vehicle
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1888
EP  - 1895
AU  - A. Branch
AU  - J. McMahon
AU  - G. Xu
AU  - M. V. Jakuba
AU  - C. R. German
AU  - S. Chien
AU  - J. C. Kinsey
AU  - A. D. Bowen
AU  - K. P. Hand
AU  - J. S. Seewald
PY  - 2020
KW  - autonomous underwater vehicles
KW  - oceanographic equipment
KW  - oceanographic techniques
KW  - search problems
KW  - hydrothermal plume model
KW  - hydrothermal vent emissions
KW  - local maxima
KW  - autonomous nested search method
KW  - hydrothermal venting
KW  - sufficient autonomy
KW  - mission concept
KW  - solar system
KW  - extra-terrestrial life
KW  - Ocean World
KW  - unmanned underwater vehicle
KW  - Vents
KW  - Oceans
KW  - Vehicle dynamics
KW  - Underwater vehicles
KW  - Earth
KW  - Numerical models
KW  - Base stations
DO  - 10.1109/ICRA40945.2020.9196625
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Ocean Worlds represent one of the best chances for extra-terrestrial life in our solar system. A new mission concept must be developed to explore these oceans. This mission would require traversing the 10s of km thick icy shell and releasing a submersible into the ocean below. During the transit of the icy shell and the exploration of the ocean, the vehicle(s) would be out of contact with Earth for weeks or potentially months at a time. During this time the vehicle must have sufficient autonomy to locate and study scientific targets of interest. One such target of interest is hydrothermal venting. We have previously developed an autonomous nested search method to locate and investigate sources of hydrothermal venting by locating local maxima in hydrothermal vent emissions. In this work we demonstrate this approach on board an OceanServer Iver2 AUV in Chesapeake Bay, MD using simulated sensor data from a hydrothermal plume model. This represents the first step towards the deployment of this approach in conditions analogous to those that we might expect on an Ocean World.
ER  - 

TY  - CONF
TI  - Towards distortion based underwater domed viewport camera calibration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1896
EP  - 1902
AU  - E. Iscar
AU  - M. Johnson-Roberson
PY  - 2020
KW  - calibration
KW  - cameras
KW  - computer vision
KW  - geometry
KW  - image motion analysis
KW  - image reconstruction
KW  - motion estimation
KW  - optical transfer function
KW  - photogrammetry
KW  - stereo image processing
KW  - image-world correspondences
KW  - refractive calibration methods
KW  - photogrammetry techniques
KW  - motion estimation
KW  - projective geometry
KW  - image formation process
KW  - refraction
KW  - light rays
KW  - housing interface
KW  - nonlinear effects
KW  - systematic errors
KW  - spherical domes
KW  - camera centers
KW  - distortion based underwater domed viewport camera calibration
KW  - point spread function
KW  - PSF
KW  - optical system
KW  - 3D reconstructions
KW  - Cameras
KW  - Calibration
KW  - Three-dimensional displays
KW  - Optical distortion
KW  - Solid modeling
KW  - Distortion
KW  - Adaptive optics
DO  - 10.1109/ICRA40945.2020.9197036
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Photogrammetry techniques used for 3D reconstructions and motion estimation from images are based on projective geometry that models the image formation process. However, in the underwater setting, refraction of light rays at the housing interface introduce non-linear effects in the image formation. These effects produce systematic errors if not accounted for, and severely degrade the quality of the acquired images. In this paper, we present a novel approach to the calibration of cameras inside spherical domes with large offsets between dome and camera centers. Such large offsets not only amplify the effect of refraction, but also introduce blur in the image that corrupts feature extractors used to establish image-world correspondences in existing refractive calibration methods. We propose using the point spread function (PSF) as a complete description of the optical system and introduce a procedure to recover the camera pose inside the dome based on the measurement of the distortions. Results on a collected dataset show the method is capable of recovering the camera pose with high accuracy.
ER  - 

TY  - CONF
TI  - How far are Pneumatic Artificial Muscles from biological muscles?
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1909
EP  - 1915
AU  - O. Mohseni
AU  - F. Gagey
AU  - G. Zhao
AU  - A. Seyfarth
AU  - M. A. Sharbafi
PY  - 2020
KW  - biomechanics
KW  - elasticity
KW  - electroactive polymer actuators
KW  - legged locomotion
KW  - muscle
KW  - pneumatic actuators
KW  - pneumatic artificial muscles
KW  - biological muscles
KW  - artificial copies
KW  - force generation mechanism
KW  - PAM force-length
KW  - additive passive parallel elastic element
KW  - PAM dynamic behaviors
KW  - dynamic muscle-like model
KW  - living creatures
KW  - multiplicative formulation
KW  - two-segmented leg
KW  - legged robots
KW  - Muscles
KW  - Mathematical model
KW  - Force
KW  - Biological system modeling
KW  - Robots
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197177
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - There is a long history demonstrating humans' tendency to create artificial copies of living creatures. For moving machines called robots, actuators play a key role in developing human-like movements. Among different types of actuation, PAMs (pneumatic artificial muscles) are known as the most similar ones to biological muscles. In addition to similarities in force generation mechanism (tension based), the well-accepted argumentation from Klute et al., states that the PAM force-length (fl) behavior is close to biological muscles, while the force-velocity (fv) pattern is different. Using the multiplicative formulation of the pressure (as an activation term), fl and fv beside an additive passive parallel elastic element, we present a new model of PAM. This muscle-based model can predict PAM dynamic behaviors with high precision. With a second experiment on a two-segmented leg, the proposed model is verified to predict the generated forces of PAMs in an antagonistic arrangement. Such a dynamic muscle-like model of artificial muscles can be used for the design and control of legged robots to generate robust, efficient and versatile gaits.
ER  - 

TY  - CONF
TI  - Shared Control Templates for Assistive Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1956
EP  - 1962
AU  - G. Quere
AU  - A. Hagengruber
AU  - M. Iskandar
AU  - S. Bustamante
AU  - D. Leidner
AU  - F. Stulp
AU  - J. Vogel
PY  - 2020
KW  - assisted living
KW  - handicapped aids
KW  - manipulators
KW  - medical robotics
KW  - mobile robots
KW  - path planning
KW  - service robots
KW  - constraint-based shared control
KW  - specific user command mappings
KW  - task execution
KW  - impairments
KW  - control interface
KW  - motor disability
KW  - light-weight robotic manipulators
KW  - assistive robotics
KW  - shared control templates
KW  - low-dimensional interface
KW  - high-dimensional tasks
KW  - human-readable format
KW  - state transitions
KW  - Task analysis
KW  - Robot kinematics
KW  - Manipulators
KW  - Wheelchairs
KW  - Manifolds
KW  - Rehabilitation robotics
DO  - 10.1109/ICRA40945.2020.9197041
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Light-weight robotic manipulators can be used to restore the manipulation capability of people with a motor disability. However, manipulating the environment poses a complex task, especially when the control interface is of low bandwidth, as may be the case for users with impairments. Therefore, we propose a constraint-based shared control scheme to define skills which provide support during task execution. This is achieved by representing a skill as a sequence of states, with specific user command mappings and different sets of constraints being applied in each state. New skills are defined by combining different types of constraints and conditions for state transitions, in a human-readable format. We demonstrate its versatility in a pilot experiment with three activities of daily living. Results show that even complex, high-dimensional tasks can be performed with a low-dimensional interface using our shared control approach.
ER  - 

TY  - CONF
TI  - Enabling Robots to Understand Incomplete Natural Language Instructions Using Commonsense Reasoning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1963
EP  - 1969
AU  - H. Chen
AU  - H. Tan
AU  - A. Kuntz
AU  - M. Bansal
AU  - R. Alterovitz
PY  - 2020
KW  - common-sense reasoning
KW  - control engineering computing
KW  - human-robot interaction
KW  - natural language processing
KW  - text analysis
KW  - environmental context
KW  - natural language instruction
KW  - unconstrained natural language
KW  - language-model-based commonsense reasoning
KW  - commonsense knowledge
KW  - spoken natural language
KW  - LMCR
KW  - parsing
KW  - verb frames
KW  - unstructured textual corpora
KW  - robot
KW  - Natural languages
KW  - Robot sensing systems
KW  - Cognition
KW  - Task analysis
KW  - Semantics
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197315
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Enabling robots to understand instructions provided via spoken natural language would facilitate interaction between robots and people in a variety of settings in homes and workplaces. However, natural language instructions are often missing information that would be obvious to a human based on environmental context and common sense, and hence does not need to be explicitly stated. In this paper, we introduce Language-Model-based Commonsense Reasoning (LMCR), a new method which enables a robot to listen to a natural language instruction from a human, observe the environment around it, and automatically fill in information missing from the instruction using environmental context and a new commonsense reasoning approach. Our approach first converts an instruction provided as unconstrained natural language into a form that a robot can understand by parsing it into verb frames. Our approach then fills in missing information in the instruction by observing objects in its vicinity and leveraging commonsense reasoning. To learn commonsense reasoning automatically, our approach distills knowledge from large unstructured textual corpora by training a language model. Our results show the feasibility of a robot learning commonsense knowledge automatically from web-based textual corpora, and the power of learned commonsense reasoning models in enabling a robot to autonomously perform tasks based on incomplete natural language instructions.
ER  - 

TY  - CONF
TI  - A Holistic Approach in Designing Tabletop Robot’s Expressivity
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1970
EP  - 1976
AU  - R. Gomez
AU  - D. Szapiro
AU  - L. Merino
AU  - K. Nakamura
PY  - 2020
KW  - computer animation
KW  - control engineering computing
KW  - control system synthesis
KW  - humanoid robots
KW  - robot modalities
KW  - zoomorphic-designed robots
KW  - robot design
KW  - animated characters
KW  - table top robot
KW  - animation techniques
KW  - robot hardware
KW  - Haru
KW  - Animation
KW  - Hardware
KW  - Neck
KW  - Light emitting diodes
KW  - Mouth
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9197016
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Defining a robot's expressivity is a difficult task that requires thoughtful consideration of the potential of various robot modalities and a model of communication that humans understand. Humanoid and zoomorphic-designed robots can easily take cues from human and animals, respectively when designing their expressivity. However, a robot design that is neither human nor animal-like does not have a clear model to follow in terms of designing expressivity. Animation presents a potential model in these circumstances as animated characters in movies take various forms, sizes, shapes and styles, and are successful in defining expressivity that is widely accepted across different languages and cultures. In this paper, we discuss the development and design of the expressivity of Haru, a table top robot that is neither human nor animal-like and the application of animation expertise to the holistic treatment of the different modalities. The method maximizes animation techniques and expertise normally applied to movies to generate expressivity that is then transferred to the robot hardware. Experimental results show that the robot's expressivity generated using our method is easily understood and are preferred to the conventional approach of generating expressions.
ER  - 

TY  - CONF
TI  - DirtNet: Visual Dirt Detection for Autonomous Cleaning Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1977
EP  - 1983
AU  - R. Bormann
AU  - X. Wang
AU  - J. Xu
AU  - J. Schmidt
PY  - 2020
KW  - cleaning
KW  - feature extraction
KW  - object detection
KW  - robot vision
KW  - service robots
KW  - office item detection system
KW  - YOLOv3 framework
KW  - visual dirt detection
KW  - autonomous cleaning
KW  - vacuum cleaning
KW  - professional cleaning robots
KW  - DirtNet
KW  - Cleaning
KW  - Object detection
KW  - Robots
KW  - Training
KW  - Task analysis
KW  - Image resolution
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196559
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual dirt detection is becoming an important capability of modern professional cleaning robots both for optimizing their wet cleaning results and for facilitating demand-oriented daily vacuum cleaning. This paper presents a robust, fast, and reliable dirt and office item detection system for these tasks based on an adapted YOLOv3 framework. Its superiority over state-of-the-art dirt detection systems is demonstrated in several experiments. The paper furthermore features a dataset generator for creating any number of realistic training images from a small set of real scene, dirt, and object examples.
ER  - 

TY  - CONF
TI  - Semantic Linking Maps for Active Visual Object Search
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1984
EP  - 1990
AU  - Z. Zeng
AU  - A. Röfer
AU  - O. C. Jenkins
PY  - 2020
KW  - inference mechanisms
KW  - manipulators
KW  - mobile robots
KW  - robot vision
KW  - search problems
KW  - Semantic Linking Maps model
KW  - target object
KW  - landmark objects
KW  - probabilistic inter-object spatial relations
KW  - hybrid search strategy
KW  - SLiM-based search strategy
KW  - Fetch mobile manipulation robot
KW  - mobile robots
KW  - common human environments
KW  - unseen target objects
KW  - reasoning
KW  - search space
KW  - common spatial relations
KW  - active visual object search strategy
KW  - Search problems
KW  - Robots
KW  - Probabilistic logic
KW  - Semantics
KW  - Buildings
KW  - Inference algorithms
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196830
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We aim for mobile robots to function in a variety of common human environments. Such robots need to be able to reason about the locations of previously unseen target objects. Landmark objects can help this reasoning by narrowing down the search space significantly. More specifically, we can exploit background knowledge about common spatial relations between landmark and target objects. For example, seeing a table and knowing that cups can often be found on tables aids the discovery of a cup. Such correlations can be expressed as distributions over possible pairing relationships of objects. In this paper, we propose an active visual object search strategy method through our introduction of the Semantic Linking Maps (SLiM) model. SLiM simultaneously maintains the belief over a target object's location as well as landmark objects' locations, while accounting for probabilistic inter-object spatial relations. Based on SLiM, we describe a hybrid search strategy that selects the next best view pose for searching for the target object based on the maintained belief. We demonstrate the efficiency of our SLiM-based search strategy through comparative experiments in simulated environments. We further demonstrate the realworld applicability of SLiM-based search in scenarios with a Fetch mobile manipulation robot.
ER  - 

TY  - CONF
TI  - Active Depth Estimation: Stability Analysis and its Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2002
EP  - 2008
AU  - R. T. Rodrigues
AU  - P. Miraldo
AU  - D. V. Dimarogonas
AU  - A. Pedro Aguiar
PY  - 2020
KW  - cameras
KW  - image sequences
KW  - Lyapunov methods
KW  - mobile robots
KW  - robot vision
KW  - solid modelling
KW  - stability
KW  - SfM
KW  - incremental active depth estimation
KW  - chronological sequence
KW  - image frames
KW  - camera actuation
KW  - stability analysis
KW  - control inputs
KW  - image plane
KW  - vision-controlled structure-from-motion scheme
KW  - depth estimation filter
KW  - Lyapunov theory
KW  - Cameras
KW  - Three-dimensional displays
KW  - Stability analysis
KW  - Asymptotic stability
KW  - Convergence
KW  - Estimation error
DO  - 10.1109/ICRA40945.2020.9196670
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recovering the 3D structure of the surrounding environment is an essential task in any vision-controlled Structure-from-Motion (SfM) scheme. This paper focuses on the theoretical properties of the SfM, known as the incremental active depth estimation. The term incremental stands for estimating the 3D structure of the scene over a chronological sequence of image frames. Active means that the camera actuation is such that it improves estimation performance. Starting from a known depth estimation filter, this paper presents the stability analysis of the filter in terms of the control inputs of the camera. By analyzing the convergence of the estimator using the Lyapunov theory, we relax the constraints on the projection of the 3D point in the image plane when compared to previous results. Nonetheless, our method is capable of dealing with the cameras' limited field-of-view constraints. The main results are validated through experiments with simulated data.
ER  - 

TY  - CONF
TI  - VALID: A Comprehensive Virtual Aerial Image Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2009
EP  - 2016
AU  - L. Chen
AU  - F. Liu
AU  - Y. Zhao
AU  - W. Wang
AU  - X. Yuan
AU  - J. Zhu
PY  - 2020
KW  - computer vision
KW  - feature extraction
KW  - image classification
KW  - image segmentation
KW  - object detection
KW  - stereo image processing
KW  - aerial imagery
KW  - unmanned aerial vehicle tasks
KW  - single ground truth type
KW  - virtual environment
KW  - high-resolution images
KW  - virtual scenes
KW  - comprehensive virtual aerial image dataset
KW  - visual ground truth data
KW  - Image segmentation
KW  - Semantics
KW  - Task analysis
KW  - Object detection
KW  - Image color analysis
KW  - Benchmark testing
KW  - Labeling
DO  - 10.1109/ICRA40945.2020.9197186
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Aerial imagery plays an important role in land-use planning, population analysis, precision agriculture, and unmanned aerial vehicle tasks. However, existing aerial image datasets generally suffer from the problem of inaccurate labeling, single ground truth type, and few category numbers. In this work, we implement a simulator that can simultaneously acquire diverse visual ground truth data in the virtual environment. Based on that, we collect a comprehensive Virtual AeriaL Image Dataset named VALID, consisting of 6690 high-resolution images, all annotated with panoptic segmentation on 30 categories, object detection with oriented bounding box, and binocular depth maps, collected in 6 different virtual scenes and 5 various ambient conditions (sunny, dusk, night, snow and fog). To our knowledge, VALID is the first aerial image dataset that can provide panoptic level segmentation and complete dense depth maps. We analyze the characteristics of VALID and evaluate state-of-the-art methods for multiple tasks to provide reference baselines. The experiment results demonstrate that VALID is well presented and challenging. The dataset is available at https://sites.google.com/view/valid-dataset/.
ER  - 

TY  - CONF
TI  - Intensity Scan Context: Coding Intensity and Geometry Relations for Loop Closure Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2095
EP  - 2101
AU  - H. Wang
AU  - C. Wang
AU  - L. Xie
PY  - 2020
KW  - computational geometry
KW  - feature extraction
KW  - image matching
KW  - mobile robots
KW  - optical radar
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - intensity scan context
KW  - light detection and ranging sensor
KW  - discard intensity reading
KW  - geometric relation
KW  - intensity structure re-identification
KW  - coding intensity
KW  - simultaneous localization and mapping
KW  - LiDAR sensor
KW  - 3D loop closure detection
KW  - geometrical only descriptor matching
KW  - place recognition
KW  - robot navigation
KW  - fast point feature histogram
KW  - Geometry
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Histograms
KW  - Simultaneous localization and mapping
KW  - Rough surfaces
KW  - Surface roughness
DO  - 10.1109/ICRA40945.2020.9196764
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Loop closure detection is an essential and challenging problem in simultaneous localization and mapping (SLAM). It is often tackled with light detection and ranging (LiDAR) sensor due to its view-point and illumination invariant properties. Existing works on 3D loop closure detection often leverage on matching of local or global geometrical-only descriptors which discard intensity reading. In this paper we explore the intensity property from LiDAR scan and show that it can be effective for place recognition. We propose a novel global descriptor, intensity scan context (ISC), that explores both geometry and intensity characteristics. To improve the efficiency for loop closure detection, an efficient two-stage hierarchical re-identification process is proposed, including binary-operation based fast geometric relation retrieval and intensity structure re-identification. Thorough experiments including both local experiment and public datasets test have been conducted to evaluate the performance of the proposed method. Our method achieves better recall rate and recall precision than existing geometric-only methods.
ER  - 

TY  - CONF
TI  - TextSLAM: Visual SLAM with Planar Text Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2102
EP  - 2108
AU  - B. Li
AU  - D. Zou
AU  - D. Sartori
AU  - L. Pei
AU  - W. Yu
PY  - 2020
KW  - augmented reality
KW  - data visualisation
KW  - navigation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - text analysis
KW  - text detection
KW  - text object integration
KW  - augmented reality
KW  - navigation
KW  - scene understanding
KW  - illumination-invariant photometric error
KW  - TextSLAM
KW  - text detection
KW  - text-based visual SLAM
KW  - 3D text maps
KW  - visual SLAM pipeline
KW  - planar text features
KW  - visual SLAM system
KW  - planar feature
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Visualization
KW  - Feature extraction
KW  - Navigation
KW  - Cameras
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197233
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose to integrate text objects in man-made scenes tightly into the visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to treat each detected text as a planar feature which is rich of textures and semantic meanings. The text feature is compactly represented by three parameters and integrated into visual SLAM by adopting the illumination-invariant photometric error. We also describe important details involved in implementing a full pipeline of text-based visual SLAM. To our best knowledge, this is the first visual SLAM method tightly coupled with the text features. We tested our method in both indoor and outdoor environments. The results show that with text features, the visual SLAM system becomes more robust and produces much more accurate 3D text maps that could be useful for navigation and scene understanding in robotic or augmented reality applications.
ER  - 

TY  - CONF
TI  - FlowNorm: A Learning-based Method for Increasing Convergence Range of Direct Alignment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2109
EP  - 2115
AU  - K. Wang
AU  - K. Wang
AU  - S. Shen
PY  - 2020
KW  - image registration
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - patch alignments
KW  - global information
KW  - learning-based network
KW  - local information
KW  - learning-based method
KW  - direct alignment
KW  - camera poses
KW  - photometric error
KW  - nonconvex property
KW  - outlier terms
KW  - local error term
KW  - global image registration information
KW  - FlowNorm
KW  - Huber norm
KW  - DSO
KW  - BA-Net
KW  - Optimization
KW  - Convergence
KW  - Cameras
KW  - Robustness
KW  - Optical imaging
KW  - Learning systems
KW  - Nonlinear optics
DO  - 10.1109/ICRA40945.2020.9197118
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many approaches have been proposed to estimate camera poses by directly minimizing photometric error. However, due to the non-convex property of direct alignment, proper initialization is still required for these methods. Many robust norms (e.g. Huber norm) have been proposed to deal with the outlier terms caused by incorrect initializations. These robust norms are solely defined on the magnitude of each error term. In this paper, we propose a novel robust norm, named FlowNorm, that exploits the information from both the local error term and the global image registration information. While the local information is defined on patch alignments, the global information is estimated using a learning-based network. Using both the local and global information, we achieve a large convergence range in which images can be aligned given large view angle changes or small overlaps. We further demonstrate the usability of the proposed robust norm by integrating it into the direct methods DSO and BA-Net, and generate more robust and accurate results in real-time.
ER  - 

TY  - CONF
TI  - Redesigning SLAM for Arbitrary Multi-Camera Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2116
EP  - 2122
AU  - J. Kuo
AU  - M. Muglikar
AU  - Z. Zhang
AU  - D. Scaramuzza
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - sensor-specific modifications
KW  - SLAM systems
KW  - robustness
KW  - camera configurations
KW  - adaptive SLAM system
KW  - multicamera setup
KW  - visual SLAM
KW  - adaptive initialization
KW  - scalable voxel-based map
KW  - sensor-agnostic information-theoretic keyframe selection algorithm
KW  - visual front-end design
KW  - visual-inertial odometry
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Uncertainty
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197553
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information- theoretic keyframe selection algorithm, and a scalable voxel- based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual- inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.
ER  - 

TY  - CONF
TI  - Dynamic SLAM: The Need For Speed
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2123
EP  - 2129
AU  - M. Henein
AU  - J. Zhang
AU  - R. Mahony
AU  - V. Ila
PY  - 2020
KW  - feature extraction
KW  - image motion analysis
KW  - image segmentation
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - rigid moving objects
KW  - static structure
KW  - dynamic structure
KW  - rigid objects
KW  - object-aware dynamic SLAM algorithm
KW  - model-free
KW  - significant motion constraints
KW  - 3D models
KW  - SLAM based approaches
KW  - unstructured dynamic environments
KW  - autonomous systems
KW  - increased deployment
KW  - simultaneous localisation
KW  - static world assumption
KW  - Simultaneous localization and mapping
KW  - Heuristic algorithms
KW  - Dynamics
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Tracking
DO  - 10.1109/ICRA40945.2020.9196895
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The static world assumption is standard in most simultaneous localisation and mapping (SLAM) algorithms. Increased deployment of autonomous systems to unstructured dynamic environments is driving a need to identify moving objects and estimate their velocity in real-time. Most existing SLAM based approaches rely on a database of 3D models of objects or impose significant motion constraints. In this paper, we propose a new feature-based, model-free, object-aware dynamic SLAM algorithm that exploits semantic segmentation to allow estimation of motion of rigid objects in a scene without the need to estimate the object poses or have any prior knowledge of their 3D models. The algorithm generates a map of dynamic and static structure and has the ability to extract velocities of rigid moving objects in the scene. Its performance is demonstrated on simulated, synthetic and real-world datasets.
ER  - 

TY  - CONF
TI  - ∇SLAM: Dense SLAM meets Automatic Differentiation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2130
EP  - 2137
AU  - K. M. Jatavallabhula
AU  - G. Iyer
AU  - L. Paull
PY  - 2020
KW  - gradient methods
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - SLAM (robots)
KW  - automatic differentiation
KW  - dense simultaneous localization
KW  - learning-based approaches
KW  - representation learning approaches
KW  - classical SLAM systems
KW  - differentiable function
KW  - optimize task performance
KW  - typical dense SLAM system
KW  - ∇SLAM
KW  - posing SLAM systems
KW  - differentiable computational graphs
KW  - differentiable trust-region optimizers
KW  - task-based error signals
KW  - Simultaneous localization and mapping
KW  - Optimization
KW  - Three-dimensional displays
KW  - Damping
KW  - Task analysis
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197519
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Learning-based approaches have the potential to leverage data or task performance to directly inform the representation. However, blending representation learning approaches with "classical" SLAM systems has remained an open question, because of their highly modular and complex nature. A SLAM system transforms raw sensor inputs into a distribution over the state(s) of the robot and the environment. If this transformation (SLAM) were expressible as a differentiable function, we could leverage task-based error signals over the outputs of this function to learn representations that optimize task performance. However, this is infeasible as several components of a typical dense SLAM system are non-differentiable. In this work, we propose ∇SLAM (gradSLAM), a methodology for posing SLAM systems as differentiable computational graphs, which unifies gradient-based learning and SLAM. We propose differentiable trust-region optimizers, surface measurement and fusion schemes, and raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM1.
ER  - 

TY  - CONF
TI  - Learning local behavioral sequences to better infer non-local properties in real multi-robot systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2138
EP  - 2144
AU  - T. Choi
AU  - S. Kang
AU  - T. P. Pavlic
PY  - 2020
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - neurocontrollers
KW  - recurrent neural nets
KW  - two-wheeled robotic platform
KW  - local behavioral sequences
KW  - multirobot systems
KW  - multirobot team
KW  - traditional observer-based approach
KW  - machine learning methods
KW  - remote teammate localization modules
KW  - long-short-term-memory
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Training
KW  - Multi-robot systems
KW  - Machine learning
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9196728
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When members of a multi-robot team follow regular motion rules sensitive to robots and other environmental factors within sensing range, the team itself may become an informational fabric for gaining situational awareness without explicit signalling among robots. In our previous work [1], we used machine learning to develop a scalable module, trained only on data from 3-robot teams, that could predict the positions of all robots in larger multi-robot teams based only on observations of the movement of a robot's nearest neighbor. Not only was this approach scalable from 3-to-many robots, but it did not require knowledge of the control laws of the robots under observation, as would a traditional observer-based approach. However, performance was only tested in simulation and could only be a substitute for explicit communication for short periods of time or in cases of very low sensing noise. In this work, we apply more sophisticated machine learning methods to data from a physically realized robotic team to develop Remote Teammate Localization (ReTLo) modules that can be used in realistic environments. To be specific, we adopt Long-Short-Term-Memory (LSTM) [2] to learn the evolution of behaviors in a modular team, which has the effect of greatly reducing errors from regression outcomes. In contrast with our previous work in simulation, all of the experiments conducted in this work were conducted on the Thymio physical, two-wheeled robotic platform.
ER  - 

TY  - CONF
TI  - Unsupervised Geometry-Aware Deep LiDAR Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2145
EP  - 2152
AU  - Y. Cho
AU  - G. Kim
AU  - A. Kim
PY  - 2020
KW  - distance measurement
KW  - geometry
KW  - optical radar
KW  - radar computing
KW  - reliability
KW  - supervised learning
KW  - unsupervised learning
KW  - unsupervised geometry-aware deep LiDAR odometry
KW  - visual perception
KW  - supervised learning-based approaches
KW  - supervised training
KW  - ground-truth pose labels
KW  - trainable LO
KW  - uncertainty-aware loss
KW  - LeGO-LOAM
KW  - unsupervised learning-based approaches
KW  - egomotion estimation approaches
KW  - Stereo-VO datasets
KW  - complex urban datasets
KW  - Oxford RobotCar datasets
KW  - KITTI
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Training
KW  - Estimation
KW  - Two dimensional displays
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197366
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning-based ego-motion estimation approaches have recently drawn strong interest from researchers, mostly focusing on visual perception. A few learning-based approaches using Light Detection and Ranging (LiDAR) have been re-ported; however, they heavily rely on a supervised learning manner. Despite the meaningful performance of these approaches, supervised training requires ground-truth pose labels, which is the bottleneck for real-world applications. Differing from these approaches, we focus on unsupervised learning for LiDAR odometry (LO) without trainable labels. Achieving trainable LO in an unsupervised manner, we introduce the uncertainty-aware loss with geometric confidence, thereby al-lowing the reliability of the proposed pipeline. Evaluation on the KITTI, Complex Urban, and Oxford RobotCar datasets demonstrate the prominent performance of the proposed method compared to conventional model-based methods. The proposed method shows a comparable result against SuMa (in KITTI), LeGO-LOAM (in Complex Urban), and Stereo-VO (in Oxford RobotCar). The video and extra-information of the paper are described in https://sites.google.com/view/deeplo.
ER  - 

TY  - CONF
TI  - SA-Net: Robust State-Action Recognition for Learning from Observations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2153
EP  - 2159
AU  - N. Soans
AU  - E. Asali
AU  - Y. Hong
AU  - P. Doshi
PY  - 2020
KW  - control engineering computing
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - robust state-action recognition
KW  - LfO methods
KW  - SA-Net
KW  - RGB-D data streams
KW  - replicated robotic applications
KW  - mobile ground robots
KW  - robotic manipulator
KW  - physical robot
KW  - deep neural network architecture
KW  - learning from observation
KW  - Image recognition
KW  - Task analysis
KW  - Robot sensing systems
KW  - Feature extraction
KW  - Robot kinematics
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9197393
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning from observation (LfO) offers a new paradigm for transferring task behavior to robots. LfO requires the robot to observe the task being performed and decompose the sensed streaming data into sequences of state-action pairs, which are then input to LfO methods. Thus, recognizing the state-action pairs correctly and quickly in sensed data is a crucial prerequisite. We present SA-Net a deep neural network architecture that recognizes state-action pairs from RGB-D data streams. SA-Net performs well in two replicated robotic applications of LfO - one involving mobile ground robots and another involving a robotic manipulator - which demonstrates that the architecture could generalize well to differing contexts. Comprehensive evaluations including deployment on a physical robot show that SA-Net significantly improves on the accuracy of the previous methods under various conditions.
ER  - 

TY  - CONF
TI  - A Generative Approach for Socially Compliant Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2160
EP  - 2166
AU  - C. -E. Tsai
AU  - J. Oh
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - socially compliant navigation
KW  - robots navigation
KW  - socially compliant behavior
KW  - optimization objectives
KW  - inverse reinforcement learning approaches
KW  - natural behavior
KW  - generative navigation algorithm
KW  - navigation path
KW  - latent social rules
KW  - trained social navigation behavior
KW  - NaviGAN
KW  - Navigation
KW  - Robots
KW  - Force
KW  - Generators
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9197497
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots navigating in human crowds need to optimize their paths not only for their task performance but also for their compliance to social norms. One of the key challenges in this context is the lack of standard metrics for evaluating and optimizing a socially compliant behavior. Existing works in social navigation can be grouped according to the differences in their optimization objectives. For instance, the reinforcement learning approaches tend to optimize on the comfort aspect of the socially compliant navigation, whereas the inverse reinforcement learning approaches are designed to achieve natural behavior. In this paper, we propose NaviGAN, a generative navigation algorithm that jointly optimizes both of the comfort and naturalness aspects. Our approach is designed as an adversarial training framework that can learn to generate a navigation path that is both optimized for achieving a goal and for complying with latent social rules. A set of experiments has been carried out on multiple datasets to demonstrate the strengths of the proposed approach quantitatively. We also perform extensive experiments using a physical robot in a realworld environment to qualitatively evaluate the trained social navigation behavior. The video recordings of the robot experiments can be found in the link: https://youtu.be/61blDymjCpw.
ER  - 

TY  - CONF
TI  - Scalable Multi-Task Imitation Learning with Autonomous Improvement
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2167
EP  - 2173
AU  - A. Singh
AU  - E. Jang
AU  - A. Irpan
AU  - D. Kappler
AU  - M. Dalal
AU  - S. Levinev
AU  - M. Khansari
AU  - C. Finn
PY  - 2020
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - robots
KW  - task analysis
KW  - deploying learning-based systems
KW  - scalable multitask imitation learning
KW  - sparse task-agnostic reward signals
KW  - reinforcement learning algorithms
KW  - continuous improvement
KW  - prior imitation learning approaches
KW  - initial demonstration dataset
KW  - learned latent space
KW  - multitask demonstration data
KW  - multitask setting
KW  - autonomous improvement
KW  - supervised imitation
KW  - autonomous data collection
KW  - imitation learning system
KW  - robot learning
KW  - stable approach
KW  - Task analysis
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Standards
KW  - Trajectory
KW  - Data collection
KW  - Learning systems
DO  - 10.1109/ICRA40945.2020.9197020
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While robot learning has demonstrated promising results for enabling robots to automatically acquire new skills, a critical challenge in deploying learning-based systems is scale: acquiring enough data for the robot to effectively generalize broadly. Imitation learning, in particular, has remained a stable and powerful approach for robot learning, but critically relies on expert operators for data collection. In this work, we target this challenge, aiming to build an imitation learning system that can continuously improve through autonomous data collection, while simultaneously avoiding the explicit use of reinforcement learning, to maintain the stability, simplicity, and scalability of supervised imitation. To accomplish this, we cast the problem of imitation with autonomous improvement into a multi-task setting. We utilize the insight that, in a multi-task setting, a failed attempt at one task might represent a successful attempt at another task. This allows us to leverage the robot's own trials as demonstrations for tasks other than the one that the robot actually attempted. Using an initial dataset of multitask demonstration data, the robot autonomously collects trials which are only sparsely labeled with a binary indication of whether the trial accomplished any useful task or not. We then embed the trials into a learned latent space of tasks, trained using only the initial demonstration dataset, to draw similarities between various trials, enabling the robot to achieve one-shot generalization to new tasks. In contrast to prior imitation learning approaches, our method can autonomously collect data with sparse supervision for continuous improvement, and in contrast to reinforcement learning algorithms, our method can effectively improve from sparse, task-agnostic reward signals.
ER  - 

TY  - CONF
TI  - Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2174
EP  - 2181
AU  - A. K. Tanwani
AU  - P. Sermanet
AU  - A. Yan
AU  - R. Anand
AU  - M. Phielipp
AU  - K. Goldberg
PY  - 2020
KW  - image segmentation
KW  - medical image processing
KW  - recurrent neural nets
KW  - supervised learning
KW  - video signal processing
KW  - visual representations
KW  - embedding space
KW  - downstream tasks
KW  - action segmentation
KW  - motion-centric representation
KW  - surgical video demonstrations
KW  - deep embedding feature space
KW  - video observations
KW  - Siamese network
KW  - action segment
KW  - randomly sampled images
KW  - recurrent neural network
KW  - labeled video segments
KW  - learned model parameters
KW  - surgical suturing kinematic motions
KW  - kinematic pose imitation
KW  - semisupervised representation learning
KW  - JIGSAWS dataset
KW  - Videos
KW  - Motion segmentation
KW  - Image segmentation
KW  - Measurement
KW  - Hidden Markov models
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197324
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning meaningful visual representations in an embedding space can facilitate generalization in downstream tasks such as action segmentation and imitation. In this paper, we learn a motion-centric representation of surgical video demonstrations by grouping them into action segments/subgoals/options in a semi-supervised manner. We present Motion2Vec, an algorithm that learns a deep embedding feature space from video observations by minimizing a metric learning loss in a Siamese network: images from the same action segment are pulled together while pushed away from randomly sampled images of other segments, while respecting the temporal ordering of the images. The embeddings are iteratively segmented with a recurrent neural network for a given parametrization of the embedding space after pre-training the Siamese network. We only use a small set of labeled video segments to semantically align the embedding space and assign pseudo-labels to the remaining unlabeled data by inference on the learned model parameters. We demonstrate the use of this representation to imitate surgical suturing kinematic motions from publicly available videos of the JIGSAWS dataset. Results give 85.5% segmentation accuracy on average suggesting performance improvement over several state-of-the-art baselines, while kinematic pose imitation gives 0.94 centimeter error in position per observation on the test set. Videos, code and data are available at: https://sites.google.com/view/motion2vec.
ER  - 

TY  - CONF
TI  - A New Path Planning Architecture to Consider Motion Uncertainty in Natural Environment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2182
EP  - 2188
AU  - M. Mizuno
AU  - T. Kubota
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - probability
KW  - random processes
KW  - trees (mathematics)
KW  - uncertain systems
KW  - rough environments
KW  - uncertainty propagation
KW  - rapidly-exploring random tree
KW  - position uncertainty
KW  - motion uncertainty
KW  - natural environment
KW  - wheeled robots
KW  - path planning architecture
KW  - path-following
KW  - path replanning
KW  - probability
KW  - collision avoidance
KW  - Uncertainty
KW  - Robot kinematics
KW  - Path planning
KW  - Mobile robots
KW  - Planning
KW  - Global Positioning System
DO  - 10.1109/ICRA40945.2020.9197238
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a new path planning algorithm to consider motion uncertainty for wheeled robots in rough environments. The proposed method uses particles to express the uncertainty propagation in complicated environments constructed with various types of terrain. Also, RRT (Rapidly-exploring Random Tree) is expanded based on the uncertainty of each node in order to prevent increasing the accumulated position uncertainty. As a result, the generated path reduces the times of path-following and re-planning based on inaccurate localization information. The effectiveness of the proposed method is evaluated in simulation using motion uncertainty models obtained by experiments. The results show that the proposed method decreases the position uncertainty while keeping the probability to avoid collisions and to reach the goal area compared with conventional approaches.
ER  - 

TY  - CONF
TI  - Revisiting the Asymptotic Optimality of RRT*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2189
EP  - 2195
AU  - K. Solovey
AU  - L. Janson
AU  - E. Schmerling
AU  - E. Frazzoli
AU  - M. Pavone
PY  - 2020
KW  - computational complexity
KW  - sampling methods
KW  - search problems
KW  - trees (mathematics)
KW  - mathematically-rigorous proof
KW  - asymptotic optimality
KW  - RRT*
KW  - asymptotically-optimal motion planning
KW  - optimality proof
KW  - sampling-based algorithms
KW  - connection radius
KW  - Robustness
KW  - Robots
KW  - Heuristic algorithms
KW  - Manganese
KW  - Planning
KW  - Aerodynamics
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196553
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - RRT* is one of the most widely used sampling-based algorithms for asymptotically-optimal motion planning. RRT* laid the foundations for optimality in motion planning as a whole, and inspired the development of numerous new algorithms in the field, many of which build upon RRT* itself. In this paper, we first identify a logical gap in the optimality proof of RRT*, which was developed by Karaman and Frazzoli (2011). Then, we present an alternative and mathematically-rigorous proof for asymptotic optimality. Our proof suggests that the connection radius used by RRT* should be increased from γ (log n/n)1/d to γ' (log n/n)1/(d+1) in order to account n n for the additional dimension of time that dictates the samples' ordering. Here γ, γ' are constants, and n, d are the number of samples and the dimension of the problem, respectively.
ER  - 

TY  - CONF
TI  - Sample Complexity of Probabilistic Roadmaps via ε-nets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2196
EP  - 2202
AU  - M. Tsao
AU  - K. Solovey
AU  - M. Pavone
PY  - 2020
KW  - computational complexity
KW  - deterministic algorithms
KW  - graph theory
KW  - probability
KW  - shortest δ-clear path
KW  - sample complexity
KW  - probabilistic roadmaps
KW  - ε-nets
KW  - optimality guarantees
KW  - deterministic sampling distribution
KW  - motion planning problem
KW  - parameter completeness
KW  - Planning
KW  - Complexity theory
KW  - Probabilistic logic
KW  - Two dimensional displays
KW  - Robots
KW  - Collision avoidance
KW  - Benchmark testing
DO  - 10.1109/ICRA40945.2020.9196917
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study fundamental theoretical aspects of probabilistic roadmaps (PRM) in the finite time (non-asymptotic) regime. In particular, we investigate how completeness and optimality guarantees of the approach are influenced by the underlying deterministic sampling distribution X and connection radius r > 0. We develop the notion of (δ, ε)-completeness of the parameters X, r, which indicates that for every motion-planning problem of clearance at least δ > 0, PRM using X, r returns a solution no longer than 1+ε times the shortest δ-clear path. Leveraging the concept of e-nets, we characterize in terms of lower and upper bounds the number of samples needed to guarantee (δ, ε)-completeness. This is in contrast with previous work which mostly considered the asymptotic regime in which the number of samples tends to infinity. In practice, we propose a sampling distribution inspired by e-nets that achieves nearly the same coverage as grids while using fewer samples.
ER  - 

TY  - CONF
TI  - Reinforcement Learning Based Manipulation Skill Transferring for Robot-assisted Minimally Invasive Surgery
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2203
EP  - 2208
AU  - H. Su
AU  - Y. Hu
AU  - Z. Li
AU  - A. Knoll
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2020
KW  - end effectors
KW  - Gaussian processes
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulator dynamics
KW  - medical robotics
KW  - motion control
KW  - regression analysis
KW  - surgery
KW  - complex tasks demonstrations
KW  - reinforcement learning algorithm based manipulation skill transferring technique
KW  - robot-assisted minimally invasive surgery
KW  - Gaussian mixture model
KW  - Gaussian mixture regression
KW  - multiple demonstrations
KW  - trial phase performed offline
KW  - practical surgical operation
KW  - KUKA LWR4+ robot
KW  - human manipulation skill
KW  - surgical robots
KW  - Learning (artificial intelligence)
KW  - Robots
KW  - Surgery
KW  - Trajectory
KW  - Task analysis
KW  - Shape
KW  - Education
DO  - 10.1109/ICRA40945.2020.9196588
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The complexity of surgical operation can be released significantly if surgical robots can learn the manipulation skills by imitation from complex tasks demonstrations such as puncture, suturing, and knotting, etc.. This paper proposes a reinforcement learning algorithm based manipulation skill transferring technique for robot-assisted Minimally Invasive Surgery by Teaching by Demonstration. It employed Gaussian mixture model and Gaussian mixture Regression based dynamic movement primitive to model the high-dimensional human-like manipulation skill after multiple demonstrations. Furthermore, this approach fascinates the learning and trial phase performed offline, which reduces the risks and cost for the practical surgical operation. Finally, it is demonstrated by transferring manipulation skills for reaching and puncture using a KUKA LWR4+ robot in a lab setup environment. The results show the effectiveness of the proposed approach for modelling and learning of human manipulation skill.
ER  - 

TY  - CONF
TI  - Safe Mission Planning under Dynamical Uncertainties
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2209
EP  - 2215
AU  - Y. Lu
AU  - M. Kamgarpour
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - Monte Carlo methods
KW  - uncertain systems
KW  - uncertainty model
KW  - path planning
KW  - dynamical uncertainties
KW  - probabilistic model
KW  - safe robot mission planning
KW  - Monte Carlo method
KW  - collision free path
KW  - Uncertainty
KW  - Planning
KW  - Automata
KW  - Computational modeling
KW  - Hazards
DO  - 10.1109/ICRA40945.2020.9196515
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper considers safe robot mission planning in uncertain dynamical environments. This problem arises in applications such as surveillance, emergency rescue, and autonomous driving. It is a challenging problem due to mod-eling and integrating dynamical uncertainties into a safe planning framework, and finding a solution in a computationally tractable way. In this work, we first develop a probabilistic model for dynamical uncertainties. Then, we provide a framework to generate a path that maximizes safety for complex missions by incorporating the uncertainty model. We also devise a Monte Carlo method to obtain a safe path efficiently. Finally, we evaluate the performance of our approach and compare it to potential alternatives in several case studies.
ER  - 

TY  - CONF
TI  - An Iterative Quadratic Method for General-Sum Differential Games with Feedback Linearizable Dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2216
EP  - 2222
AU  - D. Fridovich-Keil
AU  - V. Rubies-Royo
AU  - C. J. Tomlin
PY  - 2020
KW  - control system synthesis
KW  - convergence of numerical methods
KW  - differential games
KW  - feedback
KW  - game theory
KW  - iterative methods
KW  - linear quadratic control
KW  - linearisation techniques
KW  - nonlinear control systems
KW  - path planning
KW  - feedback linearizable dynamics
KW  - nonlinear optimal control community
KW  - multiplayer general-sum differential games
KW  - ILQ methods
KW  - local equilibria
KW  - interactive motion planning problems
KW  - iterative procedures
KW  - initial conditions
KW  - hyperparameter choices
KW  - unsafe trajectories
KW  - dynamical systems
KW  - algorithmic reliability
KW  - feedback linearizable structure
KW  - iterative linear-quadratic method
KW  - Games
KW  - Heuristic algorithms
KW  - Planning
KW  - Feedback linearization
KW  - Iterative methods
KW  - Vehicle dynamics
KW  - Optimal control
DO  - 10.1109/ICRA40945.2020.9196517
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Iterative linear-quadratic (ILQ) methods are widely used in the nonlinear optimal control community. Recent work has applied similar methodology in the setting of multi-player general-sum differential games. Here, ILQ methods are capable of finding local equilibria in interactive motion planning problems in real-time. As in most iterative procedures, however, this approach can be sensitive to initial conditions and hyperparameter choices, which can result in poor computational performance or even unsafe trajectories. In this paper, we focus our attention on a broad class of dynamical systems which are feedback linearizable, and exploit this structure to improve both algorithmic reliability and runtime. We showcase our new algorithm in three distinct traffic scenarios, and observe that in practice our method converges significantly more often and more quickly than was possible without exploiting the feedback linearizable structure.
ER  - 

TY  - CONF
TI  - A Morphable Aerial-Aquatic Quadrotor with Coupled Symmetric Thrust Vectoring
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2223
EP  - 2229
AU  - Y. H. Tan
AU  - B. M. Chen
PY  - 2020
KW  - autonomous aerial vehicles
KW  - autonomous underwater vehicles
KW  - design engineering
KW  - helicopters
KW  - mobile robots
KW  - stability
KW  - single design difficult
KW  - normal aerial vehicles
KW  - rotational acceleration
KW  - quadrotor based vehicle
KW  - vehicle body
KW  - design considerations
KW  - aerial-aquatic quadrotor
KW  - coupled symmetric thrust vectoring
KW  - aerial-aquatic vehicles
KW  - fluid resistance
KW  - energy efficient position
KW  - morphable aerial-aquatic quadrotor
KW  - mechanical actuation
KW  - static stability
KW  - Buoyancy
KW  - Robots
KW  - Force
KW  - Torque
KW  - Prototypes
KW  - Propellers
DO  - 10.1109/ICRA40945.2020.9196687
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hybrid aerial-aquatic vehicles have the unique ability of travelling in both air and water and can benefit from both lower fluid resistance in air and energy efficient position holding in water. However, they have to address the differing requirements which make optimising a single design difficult. While existing examples have shown the possibility of such vehicles, they are mostly structurally identical to normal aerial vehicles with minor adjustments to work underwater. Instead of using rotational acceleration to direct a component of thrust in surge and sway, we propose a quadrotor based vehicle that tilts its rotors about the respective arm so that a larger component of thrust can be directed in the lateral plane or in the opposite direction without rotating the vehicle body. A small-scale prototype of this design is presented here, detailing the design considerations including mechanical actuation, static stability and waterproofing.
ER  - 

TY  - CONF
TI  - An Autonomous Intercept Drone with Image-based Visual Servo
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2230
EP  - 2236
AU  - K. Yang
AU  - Q. Quan
PY  - 2020
KW  - autonomous aerial vehicles
KW  - cameras
KW  - robot vision
KW  - visual servoing
KW  - autonomous intercept drone
KW  - unwanted drone
KW  - radio wave gun
KW  - image-based visual servo algorithm
KW  - Cameras
KW  - Visualization
KW  - Mathematical model
KW  - Drones
KW  - Channel models
KW  - Servomotors
KW  - Angular velocity
DO  - 10.1109/ICRA40945.2020.9197539
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For most people on the ground, facing an unwanted drone buzzing around overhead, there is not a lot that we can do, especially if it is out of gun (radio wave gun or shotgun) range. A solution to this is to use intercept drones that seek out and bring down other drones. In order to make the interception autonomous, an image-based visual servo algorithm is designed with a forward-looking monocular camera. The control command, namely the angular velocity and thrust, is generated for intercept drones to implement accurate and fast interception. The proposed method is demonstrated in both hardware-in-the-loop simulation and demonstrative flight experiments.
ER  - 

TY  - CONF
TI  - On the Human Control of a Multiple Quadcopters with a Cable-suspended Payload System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2253
EP  - 2258
AU  - P. Prajapati
AU  - S. Parekh
AU  - V. Vashista
PY  - 2020
KW  - actuators
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - helicopters
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - cable-suspended payload system
KW  - human control
KW  - multiple quadcopters system
KW  - leader quadcopter
KW  - payload attitude controller
KW  - cable attitude controller
KW  - quadcopter-payload system
KW  - Payloads
KW  - Oscillators
KW  - Angular velocity
KW  - Attitude control
KW  - Vehicle dynamics
KW  - Trajectory
KW  - Quadcopters
KW  - Human control
KW  - Cable-suspended payload
KW  - Collaborative transportation
KW  - Multi-agents
DO  - 10.1109/ICRA40945.2020.9197279
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A quadcopter is an under-actuated system with only four control inputs for six degrees of freedom, and yet the human control of a quadcopter is simple enough to be learned with some practice. In this work, we consider the problem of human control of a multiple quadcopters system to transport a cable-suspended payload. The coupled dynamics of the system, due to the inherent physical constraints, is used to develop a leader-follower architecture where the leader quadcopter is controlled directly by a human operator and the followers are controlled with the proposed Payload Attitude Controller and Cable Attitude Controller. Experiments, where a human operator flew a two quadcopters system to transport a cable-suspended payload, were conducted to study the performance of proposed controller. The results demonstrated successful implementation of human control in these systems. This work presents the possibility of enabling manual control for on-the-go maneuvering of the quadcopter-payload system which motivates aerial transportation in the unknown environments.
ER  - 

TY  - CONF
TI  - A*3D Dataset: Towards Autonomous Driving in Challenging Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2267
EP  - 2273
AU  - Q. -H. Pham
AU  - P. Sevestre
AU  - R. S. Pahwa
AU  - H. Zhan
AU  - C. H. Pang
AU  - Y. Chen
AU  - A. Mustafa
AU  - V. Chandrasekhar
AU  - J. Lin
PY  - 2020
KW  - image annotation
KW  - image colour analysis
KW  - mobile robots
KW  - object detection
KW  - optical radar
KW  - road vehicle radar
KW  - stereo image processing
KW  - traffic engineering computing
KW  - A*3D dataset
KW  - self-driving cars
KW  - 3D object detection
KW  - 3D object annotations
KW  - autonomous driving research
KW  - nuScenes dataset
KW  - KITTI dataset
KW  - high-density images
KW  - LiDAR data
KW  - RGB images
KW  - computer vision tasks
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Autonomous vehicles
KW  - Cameras
KW  - Calibration
KW  - Object detection
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197385
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - With the increasing global popularity of self-driving cars, there is an immediate need for challenging real-world datasets for benchmarking and training various computer vision tasks such as 3D object detection. Existing datasets either represent simple scenarios or provide only day-time data. In this paper, we introduce a new challenging A*3D dataset which consists of RGB images and LiDAR data with a significant diversity of scene, time, and weather. The dataset consists of high-density images (≈ 10 times more than the pioneering KITTI dataset), heavy occlusions, a large number of nighttime frames (≈ 3 times the nuScenes dataset), addressing the gaps in the existing datasets to push the boundaries of tasks in autonomous driving research to more challenging highly diverse environments. The dataset contains 39K frames, 7 classes, and 230K 3D object annotations. An extensive 3D object detection benchmark evaluation on the A*3D dataset for various attributes such as high density, day-time/night-time, gives interesting insights into the advantages and limitations of training and testing 3D object detection in real-world setting.
ER  - 

TY  - CONF
TI  - SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2274
EP  - 2280
AU  - H. Yi
AU  - S. Shi
AU  - M. Ding
AU  - J. Sun
AU  - K. Xu
AU  - H. Zhou
AU  - Z. Wang
AU  - S. Li
AU  - G. Wang
PY  - 2020
KW  - image denoising
KW  - image segmentation
KW  - object detection
KW  - optical radar
KW  - stereo image processing
KW  - traffic engineering computing
KW  - depth-aware features
KW  - 3D vehicle detection
KW  - point cloud distribution
KW  - semantic context information
KW  - ambiguous vehicles
KW  - semantic context encoder
KW  - target detection range
KW  - emantic segmentation masks
KW  - depth-aware head
KW  - SegVoxelNet
KW  - LiDAR
KW  - noisy region suppression
KW  - Three-dimensional displays
KW  - Semantics
KW  - Feature extraction
KW  - Two dimensional displays
KW  - Vehicle detection
KW  - Head
KW  - Convolution
DO  - 10.1109/ICRA40945.2020.9196556
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - 3D vehicle detection based on point cloud is a challenging task in real-world applications such as autonomous driving. Despite significant progress has been made, we observe two aspects to be further improved. First, the semantic context information in LiDAR is seldom explored in previous works, which may help identify ambiguous vehicles. Second, the distribution of point cloud on vehicles varies continuously with increasing depths, which may not be well modeled by a single model. In this work, we propose a unified model SegVoxelNet to address the above two problems. A semantic context encoder is proposed to leverage the free-of-charge semantic segmentation masks in the bird's eye view. Suspicious regions could be highlighted while noisy regions are suppressed by this module. To better deal with vehicles at different depths, a novel depth-aware head is designed to explicitly model the distribution differences and each part of the depth-aware head is made to focus on its own target detection range. Extensive experiments on the KITTI dataset show that the proposed method outperforms the state-of-the-art alternatives in both accuracy and efficiency with point cloud as input only.
ER  - 

TY  - CONF
TI  - Fine-Grained Driving Behavior Prediction via Context-Aware Multi-Task Inverse Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2281
EP  - 2287
AU  - K. Nishi
AU  - M. Shimosaka
PY  - 2020
KW  - behavioural sciences computing
KW  - driver information systems
KW  - learning (artificial intelligence)
KW  - road accidents
KW  - road safety
KW  - road traffic
KW  - unexpected VRU movements
KW  - residential roads
KW  - proficient acceleration
KW  - deceleration
KW  - road width
KW  - traffic direction
KW  - multilinear reward function
KW  - contextual information
KW  - long-term prediction
KW  - defensive driving strategy
KW  - context-aware multitask inverse reinforcement learning
KW  - advanced driver assistance systems
KW  - vulnerable road users
KW  - traffic accident reduction rate
KW  - multitask IRL approach
KW  - fine-grained driving behavior prediction
KW  - inverse reinforcement learning
KW  - Roads
KW  - Context modeling
KW  - Task analysis
KW  - Vehicles
KW  - Hidden Markov models
KW  - Safety
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9197126
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Research on advanced driver assistance systems for reducing risks to vulnerable road users (VRUs) has recently gained popularity because the traffic accident reduction rate for VRUs is still small. Dealing with unexpected VRU movements on residential roads requires proficient acceleration and deceleration. Although fine-grained prediction of driving behavior through inverse reinforcement learning (IRL) has been reported with promising results in recent years, learning of a precise model fails when driving strategies vary with contextual factors, i.e., weather, time of day, road width, and traffic direction. In this work, we propose a novel multi-task IRL approach with a multilinear reward function to incorporate contextual information into the model. This approach can provide precise long-term prediction of fine-grained driving behavior while adjusting to context. Experimental results using actual driving data over 141 km with various contexts and roads confirm the success of this approach in terms of predicting defensive driving strategy even in unknown situations.
ER  - 

TY  - CONF
TI  - How to Keep HD Maps for Automated Driving Up To Date
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2288
EP  - 2294
AU  - D. Pannen
AU  - M. Liebner
AU  - W. Hempel
AU  - W. Burgard
PY  - 2020
KW  - cartography
KW  - data privacy
KW  - road vehicles
KW  - traffic engineering computing
KW  - dedicated mapping vehicles
KW  - low traversal frequencies
KW  - anonymized data
KW  - up-to-dateness
KW  - crowdsourced data
KW  - automatically trigger map update jobs
KW  - map patches
KW  - date HD map
KW  - automated driving functions
KW  - HD maps
KW  - automotive high definition digital map generation
KW  - automated driving up to date
KW  - Roads
KW  - Vehicle dynamics
KW  - Topology
KW  - Robot sensing systems
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197419
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The current state of the art in automotive high definition digital (HD) map generation based on dedicated mapping vehicles cannot reliably keep these maps up to date because of the low traversal frequencies. Anonymized data collected from the fleet of vehicles that is already on the road provides a huge potential to outperform such state of the art solutions in robustness, safety and up-to-dateness of the map while achieving comparable quality. We thus present a solution based on crowdsourced data to (i) detect changes in the map independent of the type of change, (ii) automatically trigger map update jobs for parts of the map, and (iii) create and integrate map patches to keep the map always up to date. The developed solution provides a crowdsourced up to date HD map to make reliable prior information on lane markings and road edges available to automated driving functions.
ER  - 

TY  - CONF
TI  - Binary DAD-Net: Binarized Driveable Area Detection Network for Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2295
EP  - 2301
AU  - A. Frickenstein
AU  - M. -R. Vemparala
AU  - J. Mayr
AU  - N. -S. Nagaraja
AU  - C. Unger
AU  - F. Tombari
AU  - W. Stechele
PY  - 2020
KW  - embedded systems
KW  - field programmable gate arrays
KW  - image segmentation
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - traffic engineering computing
KW  - binarized driveable area detection network
KW  - autonomous driving
KW  - ground-plane detection
KW  - obstacle detection
KW  - maneuver planning
KW  - over-parameterized networks
KW  - slim binary networks
KW  - binary weights
KW  - binary dilated convolutions
KW  - binary DAD-Net
KW  - semantic segmentation networks
KW  - FPGA
KW  - memory size 0.9 MByte
KW  - Convolutional codes
KW  - Task analysis
KW  - Semantics
KW  - Decoding
KW  - Training
KW  - Autonomous vehicles
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9197119
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Driveable area detection is a key component for various applications in the field of autonomous driving (AD), such as ground-plane detection, obstacle detection and maneuver planning. Additionally, bulky and over-parameterized networks can be easily forgone and replaced with smaller networks for faster inference on embedded systems. The driveable area detection, posed as a two class segmentation task, can be efficiently modeled with slim binary networks. This paper proposes a novel binarized driveable area detection network (binary DAD-Net), which uses only binary weights and activations in the encoder, the bottleneck, and the decoder part. The latent space of the bottleneck is efficiently increased (×32→×16 downsampling) through binary dilated convolutions, learning more complex features. Along with automatically generated training data, the binary DAD-Net outperforms state-of-the-art semantic segmentation networks on public datasets. In comparison to a full-precision model, our approach has a ×14.3 reduced compute complexity on an FPGA and it requires only 0.9MB memory resources. Therefore, commodity SIMD-based AD-hardware is capable of accelerating the binary DAD-Net.
ER  - 

TY  - CONF
TI  - UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2310
EP  - 2316
AU  - W. Wen
AU  - Y. Zhou
AU  - G. Zhang
AU  - S. Fahandezh-Saadi
AU  - X. Bai
AU  - W. Zhan
AU  - M. Tomizuka
AU  - L. -T. Hsu
PY  - 2020
KW  - cameras
KW  - image matching
KW  - image registration
KW  - inertial navigation
KW  - optical radar
KW  - satellite navigation
KW  - urban canyon
KW  - urban terrain
KW  - Hong Kong
KW  - San Francisco
KW  - IMU
KW  - GNSS-based solutions
KW  - LIDAR
KW  - global navigation satellite system
KW  - urban scene localization
KW  - urban scene mapping
KW  - inertia measurement units
KW  - camera-based methods
KW  - inertia navigation
KW  - visual feature matching
KW  - point cloud registration
KW  - full sensor suite dataset
KW  - UrbanLoco
KW  - Global navigation satellite system
KW  - Cameras
KW  - Laser radar
KW  - Urban areas
KW  - Robot sensing systems
KW  - Trajectory
KW  - Satellites
DO  - 10.1109/ICRA40945.2020.9196526
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mapping and localization is a critical module of autonomous driving, and significant achievements have been reached in this field. Beyond Global Navigation Satellite System (GNSS), research in point cloud registration, visual feature matching, and inertia navigation has greatly enhanced the accuracy and robustness of mapping and localization in different scenarios. However, highly urbanized scenes are still challenging: LIDAR- and camera-based methods perform poorly with numerous dynamic objects; the GNSS-based solutions experience signal loss and multi-path problems; the inertia measurement units (IMU) suffer from drifting. Unfortunately, current public datasets either do not adequately address this urban challenge or do not provide enough sensor information related to map-ping and localization. Here we present UrbanLoco: a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers. Our dataset includes a wide variety of urban terrains: urban canyons, bridges, tunnels, sharp turns, etc. More importantly, our dataset includes information from LIDAR, cameras, IMU, and GNSS receivers. Now the dataset is publicly available through the link in the footnote 1.
ER  - 

TY  - CONF
TI  - Map As the Hidden Sensor: Fast Odometry-Based Global Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2317
EP  - 2323
AU  - C. Peng
AU  - D. Weikersdorfer
PY  - 2020
KW  - distance measurement
KW  - mobile robots
KW  - path planning
KW  - sensors
KW  - tensors
KW  - odometry-based global localization
KW  - ambiguous observations
KW  - odometry drift
KW  - blind robots
KW  - robot state
KW  - belief tensor
KW  - map-corrected odometry localization
KW  - map traversability
KW  - robotics applications
KW  - hidden sensor
KW  - Robot sensing systems
KW  - Tensile stress
KW  - Trajectory
KW  - Robustness
KW  - Uncertainty
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197225
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.
ER  - 

TY  - CONF
TI  - Joint Human Pose Estimation and Stereo 3D Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2324
EP  - 2330
AU  - W. Deng
AU  - L. Bertoni
AU  - S. Kreiss
AU  - A. Alahi
PY  - 2020
KW  - neural nets
KW  - pose estimation
KW  - stereo image processing
KW  - 3D localization task
KW  - KITTI dataset
KW  - stereo 3D localization
KW  - neural network architecture
KW  - stereo imaging
KW  - human body
KW  - joint human pose estimation
KW  - image stereo pair
KW  - stereo pose dataset
KW  - Three-dimensional displays
KW  - Correlation
KW  - Two dimensional displays
KW  - Pose estimation
KW  - Uncertainty
KW  - Decoding
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197069
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an end-to-end trainable Neural Network architecture for stereo imaging that jointly locates and estimates human body poses in 3D. Our method defines a 2D pose for each human in a stereo pair of images and uses a correlation layer with a composite field to associate each left-right pair of joints. In absence of a stereo pose dataset, we show that we can train our method with synthetic data only and test it on real-world images (i.e., our training stage is domain invariant). Our method is particularly suitable for autonomous vehicles. We achieve state-of-the-art results for the 3D localization task on the challenging real-world KITTI dataset while running four times faster.
ER  - 

TY  - CONF
TI  - Self-Supervised Deep Pose Corrections for Robust Visual Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2331
EP  - 2337
AU  - B. Wagstaff
AU  - V. Peretroukhin
AU  - J. Kelly
PY  - 2020
KW  - distance measurement
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - regression analysis
KW  - stereo image processing
KW  - self-supervised deep pose corrections
KW  - robust visual odometry
KW  - data-driven learning
KW  - six-degrees-of-freedom ground truth
KW  - self-supervised DPC network
KW  - stereo odometry estimators
KW  - pose corrections regression
KW  - monocular odometry estimators
KW  - Image reconstruction
KW  - Cameras
KW  - Training
KW  - Lighting
KW  - Pipelines
KW  - Robustness
KW  - Visual odometry
DO  - 10.1109/ICRA40945.2020.9197562
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a self-supervised deep pose correction (DPC) network that applies pose corrections to a visual odometry estimator to improve its accuracy. Instead of regressing inter-frame pose changes directly, we build on prior work that uses data-driven learning to regress pose corrections that account for systematic errors due to violations of modelling assumptions. Our self-supervised formulation removes any requirement for six-degrees-of-freedom ground truth and, in contrast to expectations, often improves overall navigation accuracy compared to a supervised approach. Through extensive experiments, we show that our self-supervised DPC network can significantly enhance the performance of classical monocular and stereo odometry estimators and substantially out-performs state-of-the-art learning-only approaches.
ER  - 

TY  - CONF
TI  - Ultra-High-Accuracy Visual Marker for Indoor Precise Positioning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2338
EP  - 2343
AU  - H. Tanaka
PY  - 2020
KW  - attitude control
KW  - attitude measurement
KW  - autonomous aerial vehicles
KW  - indoor navigation
KW  - mobile robots
KW  - position control
KW  - robot vision
KW  - local positioning
KW  - marker coordinate system
KW  - high-accuracy global positioning
KW  - ultra-high-accuracy visual marker
KW  - indoor precise positioning
KW  - indoor positioning
KW  - indoor mobile robots
KW  - drones
KW  - general-purpose technology
KW  - attitude measurement
KW  - multiple dynamic moires
KW  - lenticular lens
KW  - attitude estimation error
KW  - marker position error
KW  - reprojection error
KW  - size 10.0 m
KW  - size 1.0 cm
KW  - size 10.0 cm
KW  - attitude accuracy
KW  - Visualization
KW  - Lenses
KW  - Position measurement
KW  - Cameras
KW  - Measurement uncertainty
KW  - Pose estimation
DO  - 10.1109/ICRA40945.2020.9196535
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Indoor positioning technology is essential for indoor mobile robots and drones. However, there has never been a general-purpose technology or infrastructure that enables indoor positioning with an accuracy of less than 10 cm. We have developed an attitude measurement method using multiple dynamic moires with a lenticular lens and developed an ultra- high-accuracy visual marker with an attitude estimation error of less than 0.1°. We also developed a calculation method that minimizes the marker position error by reminimizing reprojection error using its good attitude accuracy. We proved that accurate local positioning with a position error of about 1 cm in a marker coordinate system is possible even when a marker is shot from a distance of 10 m. In addition, a demonstration test was performed in a public space, and it was shown that high-accuracy global positioning with a position error of about 10 cm is possible.
ER  - 

TY  - CONF
TI  - Accurate position tracking with a single UWB anchor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2344
EP  - 2350
AU  - Y. Cao
AU  - C. Yang
AU  - R. Li
AU  - A. Knoll
AU  - G. Beltrame
PY  - 2020
KW  - inertial systems
KW  - Kalman filters
KW  - mobile robots
KW  - nonlinear filters
KW  - object tracking
KW  - observability
KW  - position measurement
KW  - SLAM (robots)
KW  - ultra wideband technology
KW  - velocity measurement
KW  - ultrawideband technology
KW  - UWB anchor
KW  - UWB range
KW  - moving robot tracking
KW  - position tracking
KW  - robotic applications
KW  - localization systems
KW  - optical tracking
KW  - 9 DoF inertial measurement unit
KW  - UWB ranging source
KW  - UWB technology
KW  - robot speed estimation
KW  - orientation estimation
KW  - IMU sensor
KW  - observability
KW  - extended Kalman filter
KW  - EKF
KW  - robot pose estimation
KW  - Robot sensing systems
KW  - Estimation
KW  - Observability
KW  - Velocity measurement
KW  - Distance measurement
KW  - Mobile robots
DO  - 10.1109/ICRA40945.2020.9197345
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate localization and tracking are a fundamental requirement for robotic applications. Localization systems like GPS, optical tracking, simultaneous localization and mapping (SLAM) are used for daily life activities, research, and commercial applications. Ultra-wideband (UWB) technology provides another venue to accurately locate devices both indoors and outdoors. In this paper, we study a localization solution with a single UWB anchor, instead of the traditional multi-anchor setup. Besides the challenge of a single UWB ranging source, the only other sensor we require is a low-cost 9 DoF inertial measurement unit (IMU). Under such a configuration, we propose continuous monitoring of UWB range changes to estimate the robot speed when moving on a line. Combining speed estimation with orientation estimation from the IMU sensor, the system becomes temporally observable. We use an Extended Kalman Filter (EKF) to estimate the pose of a robot. With our solution, we can effectively correct the accumulated error and maintain accurate tracking of a moving robot.
ER  - 

TY  - CONF
TI  - Preference-Based Learning for Exoskeleton Gait Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2351
EP  - 2357
AU  - M. Tucker
AU  - E. Novoseller
AU  - C. Kann
AU  - Y. Sui
AU  - Y. Yue
AU  - J. W. Burdick
AU  - A. D. Ames
PY  - 2020
KW  - gait analysis
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - optimisation
KW  - wearable robots
KW  - personalized gait optimization framework
KW  - lower-body exoskeleton
KW  - numerical objectives
KW  - preference-based interactive learning
KW  - CoSpar algorithm
KW  - pairwise preferences
KW  - exoskeleton walking
KW  - nonintuitive behavior
KW  - numerical feedback
KW  - human walking trajectory features
KW  - user-preferred parameters
KW  - adapting personalizing exoskeletons
KW  - exoskeleton gait optimization
KW  - Exoskeletons
KW  - Legged locomotion
KW  - Optimization
KW  - Bayes methods
KW  - Reliability
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196661
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a personalized gait optimization framework for lower-body exoskeletons. Rather than optimizing numerical objectives such as the mechanical cost of transport, our approach directly learns from user prefer-ences, e.g., for comfort. Building upon work in preference-based interactive learning, we present the CoSpar algorithm. CoSpar prompts the user to give pairwise preferences between trials and suggest improvements; as exoskeleton walking is a non-intuitive behavior, users can provide preferences more easily and reliably than numerical feedback. We show that CoSpar performs competitively in simulation and demonstrate a prototype implementation of CoSpar on a lower-body exoskeleton to optimize human walking trajectory features. In the experiments, CoSpar consistently found user-preferred parameters of the exoskeleton's walking gait, which suggests that it is a promising starting point for adapting and personalizing exoskeletons (or other assistive devices) to individual users.
ER  - 

TY  - CONF
TI  - Adaptive Neural Trajectory Tracking Control for Flexible-Joint Robots with Online Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2358
EP  - 2364
AU  - S. Chen
AU  - J. T. Wen
PY  - 2020
KW  - actuators
KW  - adaptive control
KW  - backpropagation
KW  - control system synthesis
KW  - feedforward
KW  - flexible manipulators
KW  - manipulator dynamics
KW  - neurocontrollers
KW  - nonlinear control systems
KW  - position control
KW  - stability
KW  - online backpropagation
KW  - collaborative robots
KW  - multilayer neural network
KW  - control architecture
KW  - flexible joint dynamics
KW  - control bandwidth
KW  - control design
KW  - space manipulators
KW  - online learning
KW  - flexible-joint robots
KW  - adaptive neural trajectory tracking control
KW  - series-elastic joint actuators
KW  - joint flexibility
KW  - Baxter robot
KW  - commanded joint position
KW  - outer loop control
KW  - nonlinear basis functions
KW  - internal weights
KW  - tracking error
KW  - output layer weights
KW  - linear output layer
KW  - robot dynamics
KW  - linear-in-parameter representation
KW  - feedforward control
KW  - approximate unknown dynamics
KW  - Artificial neural networks
KW  - Trajectory
KW  - Manipulator dynamics
KW  - Aerodynamics
DO  - 10.1109/ICRA40945.2020.9197051
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collaborative robots and space manipulators contain significant joint flexibility. It complicates the control design, compromises the control bandwidth, and limits the tracking accuracy. The imprecise knowledge of the flexible joint dynamics compounds the challenge. In this paper, we present a new control architecture for controlling flexible-joint robots. Our approach uses a multi-layer neural network to approximate unknown dynamics needed for the feedforward control. The network may be viewed as a linear-in-parameter representation of the robot dynamics, with the nonlinear basis of the robot dynamics connected to the linear output layer. The output layer weights are updated based on the tracking error and the nonlinear basis. The internal weights of the nonlinear basis are updated by online backpropagation to further reduce the tracking error. To use time scale separation to reduce the coupling of the two steps - the update of the internal weights is at a lower rate compared to the update of the output layer weights. With the update of the output layer weights, our controller adapts quickly to the unknown dynamics change and disturbances (such as attaching a load). The update of the internal weights would continue to improve the converge of the nonlinear basis functions. We show the stability of the proposed scheme under the "outer loop" control, where the commanded joint position is considered as the control input. Simulation and physical experiments are conducted to demonstrate the performance of the proposed controller on a Baxter robot, which exhibits significant joint flexibility due to the series-elastic joint actuators.
ER  - 

TY  - CONF
TI  - BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2365
EP  - 2371
AU  - F. Lin
AU  - C. Fu
AU  - Y. He
AU  - F. Guo
AU  - Q. Tang
PY  - 2020
KW  - autonomous aerial vehicles
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - remotely operated vehicles
KW  - target tracking
KW  - video signal processing
KW  - unmanned aerial vehicle tracking scenarios
KW  - high computational efficiency
KW  - UAV tracking process
KW  - viewpoint variations
KW  - background appearance
KW  - CF-based trackers
KW  - ideal tracker
KW  - object position
KW  - response-based errors
KW  - forward errors
KW  - backward errors
KW  - current training sample
KW  - historical training samples
KW  - BiCF
KW  - response-based bidirectional incongruity error
KW  - UAV datasets
KW  - UAV123
KW  - bidirectional incongruity-aware correlation filter
KW  - Unmanned aerial vehicles
KW  - Training
KW  - Target tracking
KW  - Correlation
KW  - Robustness
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196530
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correlation filters (CFs) have shown excellent performance in unmanned aerial vehicle (UAV) tracking scenarios due to their high computational efficiency. During the UAV tracking process, viewpoint variations are usually accompanied by changes in the object and background appearance, which poses a unique challenge to CF-based trackers. Since the appearance is gradually changing over time, an ideal tracker can not only forward predict the object position but also backtrack to locate its position in the previous frame. There exist response-based errors in the reversibility of the tracking process containing the information on the changes in appearance. However, some existing methods do not consider the forward and backward errors based on while using only the current training sample to learn the filter. For other ones, the applicants of considerable historical training samples impose a computational burden on the UAV. In this work, a novel bidirectional incongruity-aware correlation filter (BiCF) is proposed. By integrating the response-based bidirectional incongruity error into the CF, BiCF can Efficiently learn the changes in appearance and suppress the inconsistent error. Extensive experiments on 243 challenging sequences from three UAV datasets (UAV123, UAVDT, and DTB70) are conducted to demonstrate that BiCF favorably outperforms other 25 state-of-the-art trackers and achieves a real-time speed of 45.4 FPS on a single CPU, which can be applied in UAV Efficiently.
ER  - 

TY  - CONF
TI  - Adaptive Unknown Object Rearrangement Using Low-Cost Tabletop Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2372
EP  - 2378
AU  - C. -Y. Chai
AU  - W. -H. Peng
AU  - S. -L. Tsao
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - robot vision
KW  - tabletop environment
KW  - object rearrangement
KW  - low-cost tabletop robot
KW  - object rearrangement planning
KW  - learning-based methods
KW  - single-step interaction
KW  - adaptive learning procedure
KW  - size 3.5 cm
KW  - Physics
KW  - Engines
KW  - Task analysis
KW  - Optimization
KW  - Robots
KW  - Adaptation models
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197356
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Studies on object rearrangement planning typically consider known objects. Some learning-based methods can predict the movement of an unknown object after single-step interaction, but require intermediate targets, which are generated manually, to achieve the rearrangement task. In this work, we propose a framework for unknown object rearrangement. Our system first models an object through a small-amount of identification actions and adjust the model parameters during task execution. We implement the proposed framework based on a low-cost tabletop robot (under 180 USD) to demonstrate the advantages of using a physics engine to assist action prediction. Experimental results reveal that after running our adaptive learning procedure, the robot can successfully arrange a novel object using an average of five discrete pushes on our tabletop environment and satisfy a precise 3.5 cm translation and 5° rotation criterion.
ER  - 

TY  - CONF
TI  - Unsupervised Learning and Exploration of Reachable Outcome Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2379
EP  - 2385
AU  - G. Paolo
AU  - A. Laflaquière
AU  - A. Coninx
AU  - S. Doncieux
PY  - 2020
KW  - search problems
KW  - unsupervised learning
KW  - reachable outcome space
KW  - sparse rewards settings
KW  - reinforcement learning
KW  - learning process
KW  - search strategy
KW  - TAXONS
KW  - task agnostic exploration
KW  - population-based divergent-search approach
KW  - diverse policies
KW  - high-dimensional observation
KW  - task-specific information
KW  - low-dimensional outcome space
KW  - learned outcome space
KW  - ground-truth outcome space
KW  - unsupervised learning
KW  - Task analysis
KW  - Robots
KW  - Training
KW  - Space exploration
KW  - Aerospace electronics
KW  - Extraterrestrial measurements
DO  - 10.1109/ICRA40945.2020.9196819
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Performing Reinforcement Learning in sparse rewards settings, with very little prior knowledge, is a challenging problem since there is no signal to properly guide the learning process. In such situations, a good search strategy is fundamental. At the same time, not having to adapt the algorithm to every single problem is very desirable. Here we introduce TAXONS, a Task Agnostic eXploration of Outcome spaces through Novelty and Surprise algorithm. Based on a population-based divergent-search approach, it learns a set of diverse policies directly from high-dimensional observations, without any task-specific information. TAXONS builds a repertoire of policies while training an autoencoder on the high-dimensional observation of the final state of the system to build a low-dimensional outcome space. The learned outcome space, combined with the reconstruction error, is used to drive the search for new policies. Results show that TAXONS can find a diverse set of controllers, covering a good part of the ground-truth outcome space, while having no information about such space.
ER  - 

TY  - CONF
TI  - Context-aware Cost Shaping to Reduce the Impact of Model Error in Receding Horizon Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2386
EP  - 2392
AU  - C. D. McKinnon
AU  - A. P. Schoellig
PY  - 2020
KW  - mobile robots
KW  - predictive control
KW  - probability
KW  - remotely operated vehicles
KW  - robot dynamics
KW  - stochastic systems
KW  - model error
KW  - receding horizon control
KW  - repetitive path-following task
KW  - robot dynamics
KW  - simple learned dynamics model
KW  - MPC horizon
KW  - stochastic MPC
KW  - prediction horizon
KW  - online model learning
KW  - ground robot
KW  - context-aware cost shaping
KW  - stochastic model predictive control
KW  - Robots
KW  - Computational modeling
KW  - Predictive models
KW  - Aerodynamics
KW  - Cost function
KW  - Task analysis
KW  - Stochastic processes
DO  - 10.1109/ICRA40945.2020.9197521
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method to enable a robot using stochastic Model Predictive Control (MPC) to achieve high performance on a repetitive path-following task. In particular, we consider the case where the accuracy of the model for robot dynamics varies significantly over the path-motivated by the fact that the models used in MPC must be computationally efficient, which limits their expressive power. Our approach is based on correcting the cost predicted using a simple learned dynamics model over the MPC horizon. This discourages the controller from taking actions that lead to higher cost than would have been predicted using the dynamics model. In addition, stochastic MPC provides a quantitative measure of safety by limiting the probability of violating state and input constraints over the prediction horizon. Our approach is unique in that it combines both online model learning and cost learning over the prediction horizon and is geared towards operating a robot in changing conditions. We demonstrate our algorithm in simulation and experiment on a ground robot that uses a stereo camera for localization.
ER  - 

TY  - CONF
TI  - Aortic 3D Deformation Reconstruction using 2D X-ray Fluoroscopy and 3D Pre-operative Data for Endovascular Interventions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2393
EP  - 2399
AU  - Y. Zhang
AU  - L. Zhao
AU  - S. Huang
PY  - 2020
KW  - biomedical MRI
KW  - catheters
KW  - computerised tomography
KW  - deformation
KW  - diagnostic radiography
KW  - image reconstruction
KW  - image registration
KW  - medical image processing
KW  - phantoms
KW  - live 3D aortic deformation
KW  - static 3D model
KW  - stereo images
KW  - reconstruction process
KW  - deformation graph approach
KW  - reconstruction accuracy
KW  - aortic 3D deformation reconstruction
KW  - 2D X-ray fluoroscopy
KW  - current clinical endovascular interventions
KW  - catheter manipulation
KW  - aortic 3D surface
KW  - deformation reconstruction frameworks
KW  - 3D intraoperative guidance
KW  - Three-dimensional displays
KW  - Strain
KW  - Solid modeling
KW  - Image reconstruction
KW  - X-ray imaging
KW  - Two dimensional displays
KW  - Deformable models
KW  - aortic deformation reconstruction
KW  - fluoroscopy
KW  - endovascular interventions
DO  - 10.1109/ICRA40945.2020.9197410
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Current clinical endovascular interventions rely on 2D guidance for catheter manipulation. Although an aortic 3D surface is available from the pre-operative CT/MRI imaging, it cannot be used directly as a 3D intra-operative guidance since the vessel will deform during the procedure. This paper aims to reconstruct the live 3D aortic deformation by fusing the static 3D model from the pre-operative data and the 2D live imaging from fluoroscopy. In contrast to some existing deformation reconstruction frameworks which require 3D observations such as RGB-D or stereo images, fluoroscopy only presents 2D information. In the proposed framework, a 2D-3D registration is performed and the reconstruction process is formulated as a non-linear optimization problem based on the deformation graph approach. Detailed simulations and phantom experiments are conducted and the result demonstrates the reconstruction accuracy and robustness, as well as the potential clinical value of this framework.
ER  - 

TY  - CONF
TI  - Design and Kinematic Modeling of a Novel Steerable Needle for Image-Guided Insertion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2400
EP  - 2406
AU  - Y. Chen
AU  - H. Yang
AU  - X. Liu
AU  - K. Xu
PY  - 2020
KW  - biomedical ultrasonics
KW  - medical image processing
KW  - medical robotics
KW  - needles
KW  - path planning
KW  - pose estimation
KW  - surgery
KW  - tumours
KW  - image-guided insertion
KW  - novel steerable needle
KW  - kinematics model
KW  - passive needle tip articulation
KW  - needle path consistency
KW  - articulated tip
KW  - needle design
KW  - tissue mechanics
KW  - needle-tissue interaction
KW  - needle placement
KW  - percutaneous tumor ablation
KW  - biopsy tumor ablation
KW  - Needles
KW  - Kinematics
KW  - Fasteners
KW  - Path planning
KW  - Shape
KW  - Electron tubes
KW  - Laser beam cutting
DO  - 10.1109/ICRA40945.2020.9196960
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Needle-based procedures, such as biopsy and percutaneous tumor ablation, highly depend on the accuracy of needle placement. The accuracy is significantly affected by the needle-tissue interaction no matter what needles (straight or steerable) are used. Due to the unknown tissue mechanics, it is challenging to achieve high accuracy in practice. This paper hence proposes a needle design with an articulated tip for increased steerability and improved needle path consistency. Due to the passive needle tip articulation, tissue mechanics always plays a dominant role such that the needle creates similar paths with approximately piece-wise constant curvature in different tissues. Kinematics model for the proposed needle is presented. The algorithms of path planning and needle tip pose estimation under external imaging modality are developed. Experimental verifications were conducted to demonstrate the needle's steerability as well as the target-reaching capability with obstacles avoidance.
ER  - 

TY  - CONF
TI  - Robotic needle insertion in moving soft tissues using constraint-based inverse Finite Element simulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2407
EP  - 2413
AU  - P. Baksic
AU  - H. Courtecuisse
AU  - C. Duriez
AU  - B. Bayle
PY  - 2020
KW  - biological tissues
KW  - end effectors
KW  - finite element analysis
KW  - inverse problems
KW  - medical image processing
KW  - medical robotics
KW  - needles
KW  - robotic needle insertion
KW  - soft tissues
KW  - robotic steering
KW  - flexible needle
KW  - predefined path
KW  - inverse problem
KW  - robot end effector
KW  - constraint-based formulation
KW  - simulation-guided needle insertion
KW  - direct simulation
KW  - respiratory motion
KW  - numerical simulation
KW  - constraint-based inverse finite element simulation
KW  - Needles
KW  - Robots
KW  - Mathematical model
KW  - Linear programming
KW  - Computational modeling
KW  - Numerical models
KW  - Inverse problems
DO  - 10.1109/ICRA40945.2020.9197515
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a method for robotic steering of a flexible needle inside moving and deformable tissues. The method relies on a set of objective functions allowing to automatically steer the needle along a predefined path. In order to follow the desired trajectory, an inverse problem linking the motion of the robot end effector with the objective functions is solved using a Finite Element simulation. The main contribution of the article is the new constraint-based formulation of the objective functions allowing to: 1) significantly reduce the computation time; 2) increase the accuracy and stability of the simulation-guided needle insertion. The method is illustrated, and its performances are characterized in a realistic framework, using a direct simulation of the respiratory motion generated from in vivo data of a pig. Despite the highly non-linear behavior of the numerical simulation and the significant deformations occurring during the insertion, the obtained performances enable the possibility to follow the trajectory with the desired accuracy for medical purpose.
ER  - 

TY  - CONF
TI  - Collaborative Robot-Assisted Endovascular Catheterization with Generative Adversarial Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2414
EP  - 2420
AU  - W. Chi
AU  - G. Dagnino
AU  - T. M. Y. Kwok
AU  - A. Nguyen
AU  - D. Kundrat
AU  - M. E. M. K. Abdelaziz
AU  - C. Riga
AU  - C. Bicknell
AU  - G. -Z. Yang
PY  - 2020
KW  - blood vessels
KW  - catheters
KW  - diagnostic radiography
KW  - haemodynamics
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - pulsatile flow
KW  - surgery
KW  - reduced procedural duration
KW  - deep reinforcement learning technologies
KW  - complex endovascular tasks
KW  - reduced fatigue
KW  - cognitive workload
KW  - model-based approaches
KW  - model-free generative adversarial imitation learning
KW  - standard arterial catherization task
KW  - automation policies
KW  - catheter motions
KW  - collaborative robot-assisted endovascular catheterization
KW  - master-slave systems
KW  - clinical benefits
KW  - radiation doses
KW  - vascular anatomical model
KW  - Catheters
KW  - Robots
KW  - Task analysis
KW  - Training
KW  - Catheterization
KW  - Instruments
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9196912
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Master-slave systems for endovascular catheterization have brought major clinical benefits including reduced radiation doses to the operators, improved precision and stability of the instruments, as well as reduced procedural duration. Emerging deep reinforcement learning (RL) technologies could potentially automate more complex endovascular tasks with enhanced success rates, more consistent motion and reduced fatigue and cognitive workload of the operators. However, the complexity of the pulsatile flows within the vasculature and non-linear behavior of the instruments hinder the use of model-based approaches for RL. This paper describes model-free generative adversarial imitation learning to automate a standard arterial catherization task. The automation policies have been trained in a pre-clinical setting. Detailed validation results show high success rates after skill transfer to a different vascular anatomical model. The quality of the catheter motions also shows less mean and maximum contact forces compared to manual-based approaches.
ER  - 

TY  - CONF
TI  - GA3C Reinforcement Learning for Surgical Steerable Catheter Path Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2429
EP  - 2435
AU  - A. Segato
AU  - L. Sestini
AU  - A. Castellano
AU  - E. De Momi
PY  - 2020
KW  - biomedical MRI
KW  - catheters
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - path planning
KW  - robot kinematics
KW  - trajectory smoothness
KW  - GA3C Reinforcement Learning
KW  - surgical steerable catheter path planning
KW  - path planning algorithms
KW  - steerable catheters
KW  - anatomical obstacles avoidance
KW  - insertion length
KW  - needle kinematics
KW  - smooth trajectories
KW  - path planning problem
KW  - reinforcement learning problem
KW  - trajectory planning model
KW  - optimal trajectories
KW  - obstacle clearance
KW  - kinematic constraints
KW  - MRI images processing
KW  - path planning model
KW  - curvilinear trajectories
KW  - RRT* algorithms
KW  - obstacle avoidance
KW  - Trajectory
KW  - Needles
KW  - Three-dimensional displays
KW  - Learning (artificial intelligence)
KW  - Catheters
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196954
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path planning algorithms for steerable catheters, must guarantee anatomical obstacles avoidance, reduce the insertion length and ensure the compliance with needle kinematics. The majority of the solutions in literature focuses on graph based or sampling based methods, both limited by the impossibility to directly obtain smooth trajectories. In this work we formulate the path planning problem as a reinforcement learning problem and show that the trajectory planning model, generated from the training, can provide the user with optimal trajectories in terms of obstacle clearance and kinematic constraints. We obtain 2D and 3D environments from MRI images processing and we implement a GA3C algorithm to create a path planning model, able to generalize on different patients anatomies. The curvilinear trajectories obtained from the model in 2D and 3D environments are compared to the ones obtained by A* and RRT* algorithms. Our method achieves state-of-the-art performances in terms of obstacle avoidance, trajectory smoothness and computational time proving this algorithm as valid planning method for complex environments.
ER  - 

TY  - CONF
TI  - MPC-based Controller with Terrain Insight for Dynamic Legged Locomotion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2436
EP  - 2442
AU  - O. Villarreal
AU  - V. Barasuol
AU  - P. M. Wensing
AU  - D. G. Caldwell
AU  - C. Semini
PY  - 2020
KW  - convolutional neural nets
KW  - hydraulic actuators
KW  - legged locomotion
KW  - neurocontrollers
KW  - predictive control
KW  - robot dynamics
KW  - on-board mapping
KW  - contact sequence task
KW  - convolutional neural network
KW  - model predictive controller
KW  - on-board sensing
KW  - MPC-based controller
KW  - terrain insight
KW  - dynamic legged locomotion
KW  - hydraulically actuated quadruped robot HyQReal
KW  - Legged locomotion
KW  - Trajectory
KW  - Task analysis
KW  - Computational modeling
KW  - Dynamics
KW  - Foot
DO  - 10.1109/ICRA40945.2020.9197312
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel control strategy for dynamic legged locomotion in complex scenarios that considers information about the morphology of the terrain in contexts when only on-board mapping and computation are available. The strategy is built on top of two main elements: first a contact sequence task that provides safe foothold locations based on a convolutional neural network to perform fast and continuous evaluation of the terrain in search of safe foothold locations; then a model predictive controller that considers the foothold locations given by the contact sequence task to optimize target ground reaction forces. We assess the performance of our strategy through simulations of the hydraulically actuated quadruped robot HyQReal traversing rough terrain under realistic on-board sensing and computing conditions.
ER  - 

TY  - CONF
TI  - An Adaptive Supervisory Control Approach to Dynamic Locomotion Under Parametric Uncertainty
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2443
EP  - 2449
AU  - P. Chand
AU  - S. Veer
AU  - I. Poulakakis
PY  - 2020
KW  - adaptive control
KW  - control system synthesis
KW  - feedback
KW  - legged locomotion
KW  - parameter estimation
KW  - robot dynamics
KW  - uncertain systems
KW  - feedback control
KW  - estimator design
KW  - dynamic locomotion
KW  - parametric uncertainty
KW  - robotic systems
KW  - parameter estimation
KW  - adaptive supervisory control
KW  - dynamically walking bipedal robot
KW  - Switches
KW  - Uncertainty
KW  - Robots
KW  - Supervisory control
KW  - Adaptive control
KW  - Libraries
DO  - 10.1109/ICRA40945.2020.9197120
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an adaptive control scheme for robotic systems that operate in the face of-potentially large-structured uncertainty. The proposed adaptive controller employs an on-line supervisor that utilizes logic-based switching among a finite set of controllers to identify uncertain parameters, and adapt the behavior of the system based on a current estimate of their value. To achieve this, the adaptive control approach in this paper combines on-line parameter estimation and feedback control while avoiding some of the inherent difficulties of classical adaptive control strategies. Furthermore, the proposed supervisory control architecture is modular as it relies on established "off-the-shelf" feedback control law and estimator design approaches, instead of cus-tomizing the overall design to the specific requirements of an adaptive control algorithm. We demonstrate the efficacy of the method on the problem of a dynamically-walking bipedal robot delivering a payload of unknown mass, and show that, by switching to the controller that is the "best" according to a current estimate of the uncertainty, the system maintains a low energy cost during its operation.
ER  - 

TY  - CONF
TI  - Joint Space Position/Torque Hybrid Control of the Quadruped Robot for Locomotion and Push Reaction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2450
EP  - 2456
AU  - O. Sim
AU  - H. Jeong
AU  - J. Oh
AU  - M. Lee
AU  - K. Kyu Lee
AU  - H. -W. Park
AU  - J. -H. Oh
PY  - 2020
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - position control
KW  - robot dynamics
KW  - torque control
KW  - torque control
KW  - contact-force-control
KW  - joint space hybrid control
KW  - legged robot platform
KW  - hybrid control algorithm
KW  - robot displayed stability
KW  - mammal-type quadruped robot
KW  - dynamic locomotion
KW  - push reaction abilities
KW  - Cartesian space
KW  - external push disturbances
KW  - Force
KW  - Aerospace electronics
KW  - Legged locomotion
KW  - Torque
KW  - Robot kinematics
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197230
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a novel algorithm for joint space position/torque hybrid control of a mammal-type quadruped robot. With this control algorithm, the robot demonstrated both dynamic locomotion and push reaction abilities without the need for torque control in the ab/ad joints. Based on the tipping and slipping condition of the legged robot, we showed that reaction to a typical push in the horizontal direction does not require full contact-force-control in the frontal plane. Furthermore, we showed that position/torque hybrid control in Cartesian space is directly applicable to joint space hybrid control due to the joint configuration of the quadruped robot. We conducted experiments on our legged robot platform to verify the performance of our hybrid control algorithm. With this approach, the robot displayed stability while walking and reacting to external push disturbances.
ER  - 

TY  - CONF
TI  - Improved Performance on Moving-Mass Hopping Robots with Parallel Elasticity
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2457
EP  - 2463
AU  - E. Ambrose
AU  - A. D. Ames
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - legged locomotion
KW  - robot dynamics
KW  - springs (mechanical)
KW  - stability
KW  - mechanical design
KW  - robotic hopping
KW  - moving-mass hopping robots
KW  - hop heights
KW  - single-spring model
KW  - double-spring model
KW  - hopping effort
KW  - parallel spring
KW  - hybrid systems models
KW  - rigorous trajectory optimization method
KW  - one-dimensional hopping robot
KW  - actuator
KW  - single spring
KW  - moving-mass robot
KW  - stable hopping
KW  - ground contact
KW  - Springs
KW  - Actuators
KW  - Force
KW  - Robot kinematics
KW  - Dynamics
KW  - Jacobian matrices
DO  - 10.1109/ICRA40945.2020.9197070
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic Hopping is challenging from the perspective of both modeling the dynamics as well as the mechanical design due to the short period of ground contact in which to actuate on the world. Previous work has demonstrated stable hopping on a moving-mass robot, wherein a single spring was utilized below the body of the robot. This paper finds that the addition of a spring in parallel to the actuator greatly improves the performance of moving mass hopping robots. This is demonstrated through the design of a novel one-dimensional hopping robot. For this robot, a rigorous trajectory optimization method is developed using hybrid systems models with experimentally tuned parameters. Simulation results are used to study the effects of a parallel spring on energetic efficiency, stability, and hopping effort. We find that the double-spring model had 2.5x better energy efficiency than the single-spring model, and was able to hop using 40% less peak force from the actuator. Furthermore, the double-spring model produces stable hopping without the need for stabilizing controllers. These concepts are demonstrated experimentally on a novel hopping robot, wherein hop heights up to 40cm were achieved with comparable efficiency and stability.
ER  - 

TY  - CONF
TI  - Vision Aided Dynamic Exploration of Unstructured Terrain with a Small-Scale Quadruped Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2464
EP  - 2470
AU  - D. Kim
AU  - D. Carballo
AU  - J. Di Carlo
AU  - B. Katz
AU  - G. Bledt
AU  - B. Lim
AU  - S. Kim
PY  - 2020
KW  - collision avoidance
KW  - gait analysis
KW  - legged locomotion
KW  - robot dynamics
KW  - fully sensorized Mini-Cheetah quadruped robot
KW  - unstructured terrain
KW  - dynamic exploration
KW  - dynamic trotting
KW  - highly irregular terrain
KW  - obstacle avoidance
KW  - foothold adjustment
KW  - evaluation algorithms
KW  - effective filtering
KW  - MIT Mini-Cheetah
KW  - Intel RealSense sensors
KW  - jumping
KW  - dynamic locomotion
KW  - effective sensor integration
KW  - walking speed
KW  - obstacle clearance
KW  - narrow paths
KW  - cluttered environments
KW  - rough terrain locomotion capability
KW  - rescue scenarios
KW  - disaster response
KW  - mobile platforms
KW  - legged robots
KW  - small-scale quadruped robot
KW  - size 0.3 m
KW  - mass 9.0 kg
KW  - Cameras
KW  - Robot vision systems
KW  - Robot kinematics
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196777
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Legged robots have been highlighted as promising mobile platforms for disaster response and rescue scenarios because of their rough terrain locomotion capability. In cluttered environments, small robots are desirable as they can maneuver through small gaps, narrow paths, or tunnels. However small robots have their own set of difficulties such as limited space for sensors, limited obstacle clearance, and scaled-down walking speed. In this paper, we extensively address these difficulties via effective sensor integration and exploitation of dynamic locomotion and jumping. We integrate two Intel RealSense sensors into the MIT Mini-Cheetah, a 0.3 m tall, 9 kg quadruped robot. Simple and effective filtering and evaluation algorithms are used for foothold adjustment and obstacle avoidance. We showcase the exploration of highly irregular terrain using dynamic trotting and jumping with the small-scale, fully sensorized Mini-Cheetah quadruped robot.
ER  - 

TY  - CONF
TI  - Distributed Attack-Robust Submodular Maximization for Multi-Robot Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2479
EP  - 2485
AU  - L. Zhou
AU  - V. Tzoumas
AU  - G. J. Pappas
AU  - P. Tokekar
PY  - 2020
KW  - control system security
KW  - distributed algorithms
KW  - divide and conquer methods
KW  - multi-robot systems
KW  - optimisation
KW  - path planning
KW  - target tracking
KW  - distributed attack-robust submodular maximization
KW  - multirobot planning
KW  - swarm-robotics applications
KW  - multirobot motion
KW  - attack-robust algorithms
KW  - robust optimization
KW  - distributed robust maximization
KW  - DRM performance
KW  - multiple robots
KW  - denial-of-service attacks
KW  - DoS attacks
KW  - large-scale robotic applications
KW  - general-purpose distributed algorithm
KW  - divide-and-conquer approach
KW  - robot communication range
KW  - close-to-optimal performance
KW  - active target tracking
KW  - Planning
KW  - Target tracking
KW  - Robot kinematics
KW  - Partitioning algorithms
KW  - Robot sensing systems
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197243
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We aim to guard swarm-robotics applications against denial-of-service (DoS) attacks that result in withdrawals of robots. We focus on applications requiring the selection of actions for each robot, among a set of available ones, e.g., which trajectory to follow. Such applications are central in large-scale robotic applications, e.g., multi-robot motion planning for target tracking. But the current attack-robust algorithms are centralized, and scale quadratically with the problem size (e.g., number of robots). In this paper, we propose a general-purpose distributed algorithm towards robust optimization at scale, with local communications only. We name it distributed robust maximization (DRM). DRM proposes a divide-and-conquer approach that distributively partitions the problem among K cliques of robots. The cliques optimize in parallel, independently of each other. That way, DRM also offers computational speed-ups up to 1/K2 the running time of its centralized counterparts. K depends on the robots' communication range, which is given as input to DRM. DRM also achieves a close-to-optimal performance. We demonstrate DRM's performance in Gazebo and MATLAB simulations, in scenarios of active target tracking with multiple robots. We observe DRM achieves significant computational speed-ups (it is 3 to 4 orders faster) and, yet, nearly matches the tracking performance of its centralized counterparts.
ER  - 

TY  - CONF
TI  - Multirobot Patrolling Against Adaptive Opponents with Limited Information
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2486
EP  - 2492
AU  - C. D. Alvarenga
AU  - N. Basilico
AU  - S. Carpin
PY  - 2020
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - optimisation
KW  - multirobot patrolling
KW  - adaptive opponents
KW  - patrolling problem
KW  - multiple agents
KW  - time consuming
KW  - Robot kinematics
KW  - Task analysis
KW  - Delays
KW  - Optimization
KW  - Games
DO  - 10.1109/ICRA40945.2020.9197287
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study a patrolling problem where multiple agents are tasked with protecting an environment where one or more adversaries are trying to compromise targets of varying value. The objective of the patrollers is to move between targets to quickly spot when an attack is taking place and then diffuse it. Differently from most related literature, we do not assume that attackers have full knowledge of the strategies followed by the patrollers, but rather build a model at run time through repeated observations of how often they visit certain targets. We study three different solutions to this problem. The first two partition the environment using either a fast heuristic or an exact method that is significantly more time consuming. The third method, instead does not partition the environment, but rather lets every patroller roam over the entire environment. After having identified strengths and weaknesses of each method, we contrast their performances against attackers using different algorithms to decide whether to attack or not.
ER  - 

TY  - CONF
TI  - Distributed Optimization of Nonlinear, Non-Gaussian, Communication-Aware Information using Particle Methods
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2493
EP  - 2499
AU  - S. Moon
AU  - E. W. Frew
PY  - 2020
KW  - entropy
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - wireless sensor networks
KW  - mobile robotic sensor networks
KW  - neighbor robots
KW  - conditional mutual information
KW  - communication properties
KW  - specific measurement set
KW  - particle methods
KW  - information computation
KW  - distributed optimization
KW  - local utility design
KW  - communication-aware information gathering
KW  - sampling procedures
KW  - entropy reduction
KW  - Robot sensing systems
KW  - Optimization
KW  - Mutual information
KW  - Planning
KW  - Atmospheric measurements
KW  - Particle measurements
DO  - 10.1109/ICRA40945.2020.9197404
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a distributed optimization framework and its local utility design for communication-aware information gathering by mobile robotic sensor networks. The main idea of the optimization is that each robot decides based on its local utility that considers the decisions of other neighbor robots higher in a given hierarchy. The local utility is designed as conditional mutual information that captures sensing and communication properties. Sampling procedures using a specific measurement set and particle methods are applied to compute the designed utility, which allows nonlinear, non-Gaussian properties of targets, sensing, and communication. Simulation results describe the presented distributed optimization shows more improved estimates and entropy reduction than another approach that does not consider communication properties. Simulation results also verify the presented distributed optimization using the described approach for information computation has better results than using other approaches that simplify the communication-aware information.
ER  - 

TY  - CONF
TI  - Targeted Drug Delivery: Algorithmic Methods for Collecting a Swarm of Particles with Uniform, External Forces
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2508
EP  - 2514
AU  - A. T. Becker
AU  - S. P. Fekete
AU  - L. Huang
AU  - P. Keldenich
AU  - L. Kleist
AU  - D. Krupke
AU  - C. Rieck
AU  - A. Schmidt
PY  - 2020
KW  - computational complexity
KW  - deterministic algorithms
KW  - drug delivery systems
KW  - drugs
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - optimisation
KW  - actuation steps
KW  - deterministic algorithms
KW  - targeted drug delivery
KW  - maze-like environment
KW  - vascular system
KW  - basic scenario
KW  - global external force
KW  - fluidic flow
KW  - deep learning
KW  - Magnetic resonance imaging
KW  - Robots
KW  - Tumors
KW  - Force
KW  - Blood
KW  - Electromagnetics
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9196551
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We investigate algorithmic approaches for targeted drug delivery in a complex, maze-like environment, such as a vascular system. The basic scenario is given by a large swarm of micro-scale particles ("agents") and a particular target region ("tumor") within a system of passageways. Agents are too small to contain on-board power or computation and are instead controlled by a global external force that acts uniformly on all particles, such as an applied fluidic flow or electromagnetic field. The challenge is to deliver all agents to the target region with a minimum number of actuation steps. We provide a number of results for this challenge. We show that the underlying problem is NP-hard, which explains why previous work did not provide provably efficient algorithms. We also develop a number of algorithmic approaches that greatly improve the worst-case guarantees for the number of required actuation steps. We evaluate our algorithmic approaches by a number of simulations, both for deterministic algorithms and searches supported by deep learning, which show that the performance is practically promising.
ER  - 

TY  - CONF
TI  - Enhancing Bilevel Optimization for UAV Time-Optimal Trajectory using a Duality Gap Approach
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2515
EP  - 2521
AU  - G. Tang
AU  - W. Sun
AU  - K. Hauser
PY  - 2020
KW  - autonomous aerial vehicles
KW  - bang-bang control
KW  - duality (mathematics)
KW  - mobile robots
KW  - Newton method
KW  - nonlinear programming
KW  - path planning
KW  - sensitivity analysis
KW  - time optimal control
KW  - sensitivity analysis
KW  - NLP solvers
KW  - parametric optimization problems
KW  - quasiNewton method
KW  - geometric path
KW  - time-optimal velocity profile
KW  - hierarchical optimization
KW  - bang-bang control structure
KW  - nonlinear programming solvers
KW  - dynamic robotic vehicles
KW  - time-optimal trajectories
KW  - duality gap approach
KW  - UAV time-optimal trajectory
KW  - gap-free bilevel optimization
KW  - interior-point method
KW  - Trajectory
KW  - Cost function
KW  - Switches
KW  - Acceleration
KW  - Robustness
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196789
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Time-optimal trajectories for dynamic robotic vehicles are difficult to compute even for state-of-the-art nonlinear programming (NLP) solvers, due to nonlinearity and bang-bang control structure. This paper presents a bilevel optimization framework that addresses these problems by decomposing the spatial and temporal variables into a hierarchical optimization. Specifically, the original problem is divided into an inner layer, which computes a time-optimal velocity profile along a given geometric path, and an outer layer, which refines the geometric path by a Quasi-Newton method. The inner optimization is convex and efficiently solved by interior-point methods. The gradients of the outer layer can be analytically obtained using sensitivity analysis of parametric optimization problems. A novel contribution is to introduce a duality gap in the inner optimization rather than solving it to optimality; this lets the optimizer realize warm-starting of the interior-point method, avoids non-smoothness of the outer cost function caused by active inequality constraint switching. Like prior bilevel frameworks, this method is guaranteed to return a feasible solution at any time, but converges faster than gap-free bilevel optimization. Numerical experiments on a drone model with velocity and acceleration limits show that the proposed method performs faster and more robustly than gap-free bilevel optimization and general NLP solvers.
ER  - 

TY  - CONF
TI  - Constrained Sampling-based Trajectory Optimization using Stochastic Approximation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2522
EP  - 2528
AU  - G. I. Boutselis
AU  - Z. Wang
AU  - E. A. Theodorou
PY  - 2020
KW  - approximation theory
KW  - discrete systems
KW  - gradient methods
KW  - optimal control
KW  - optimisation
KW  - sampling methods
KW  - stochastic processes
KW  - constrained problems
KW  - stochastic search
KW  - box control constraints
KW  - nonlinear state constraints
KW  - discrete dynamical systems
KW  - sampling-based trajectory optimization methodology
KW  - stochastic approximation
KW  - constrained sampling-based trajectory optimization
KW  - nonsmooth penalty functions
KW  - control inputs
KW  - truncated parameterized distributions
KW  - Heuristic algorithms
KW  - Trajectory optimization
KW  - Convergence
KW  - Approximation algorithms
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197284
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a sampling-based trajectory optimization methodology for constrained problems. We extend recent works on stochastic search to deal with box control constraints, as well as nonlinear state constraints for discrete dynamical systems. Regarding the former, our strategy is to optimize over truncated parameterized distributions on control inputs. Furthermore, we show how non-smooth penalty functions can be incorporated into our framework to handle state constraints. Simulations on cartpole and quadcopter show that our approach outperforms previous methods on constrained sampling-based optimization, in terms of quality of solutions and convergence speed.
ER  - 

TY  - CONF
TI  - Learning Control Policies from Optimal Trajectories
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2529
EP  - 2535
AU  - C. Zelch
AU  - J. Peters
AU  - O. von Stryk
PY  - 2020
KW  - feedback
KW  - Gaussian processes
KW  - industrial robots
KW  - learning systems
KW  - mobile robots
KW  - nonlinear control systems
KW  - optimal control
KW  - optimisation
KW  - pendulums
KW  - trajectory control
KW  - time-dependent optimal trajectories
KW  - optimal feedback control policies
KW  - real-world systems
KW  - frequent correction
KW  - model errors
KW  - optimal reference trajectories
KW  - high dimensional state space
KW  - learning control policies
KW  - optimally control robotic systems
KW  - high dimensional nonlinear system dynamic models
KW  - Gaussian processes
KW  - swing-up problem
KW  - underactuated pendulum
KW  - energy-minimal point-to-point movement
KW  - 3-DOF industrial robot
KW  - Trajectory
KW  - Computational modeling
KW  - Task analysis
KW  - Numerical models
KW  - Robots
KW  - Feedback control
KW  - Optimal control
DO  - 10.1109/ICRA40945.2020.9196791
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ability to optimally control robotic systems offers significant advantages for their performance. While time-dependent optimal trajectories can numerically be computed for high dimensional nonlinear system dynamic models, constraints and objectives, finding optimal feedback control policies for such systems is hard. This is unfortunate, as without a policy, the control of real-world systems requires frequent correction or replanning to compensate for disturbances and model errors.In this paper, a feedback control policy is learned from a set of optimal reference trajectories using Gaussian processes. Information from existing trajectories and the current policy is used to find promising start points for the computation of further optimal trajectories. This aspect is important as it avoids exhaustive sampling of the complete state space, which is impractical due to the high dimensional state space, and to focus on the relevant region.The presented method has been applied in simulation to a swing-up problem of an underactuated pendulum and an energy-minimal point-to-point movement of a 3-DOF industrial robot.
ER  - 

TY  - CONF
TI  - Crocoddyl: An Efficient and Versatile Framework for Multi-Contact Optimal Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2536
EP  - 2542
AU  - C. Mastalli
AU  - R. Budhiraja
AU  - W. Merkt
AU  - G. Saurel
AU  - B. Hammoud
AU  - M. Naveau
AU  - J. Carpentier
AU  - L. Righetti
AU  - S. Vijayakumar
AU  - N. Mansard
PY  - 2020
KW  - dynamic programming
KW  - iterative methods
KW  - motion control
KW  - optimal control
KW  - Crocoddyl
KW  - contact robot control by differential dynamic library
KW  - open-source framework
KW  - multicontact optimal control
KW  - state trajectory
KW  - control policy
KW  - sparse analytical derivatives
KW  - differential geometry
KW  - floating-base systems
KW  - FDDP
KW  - computation time
KW  - infeasible state-control trajectories
KW  - highly-dynamic maneuvers
KW  - feasibility-driven differential dynamic programming
KW  - Optimal control
KW  - Trajectory
KW  - Optimization
KW  - Heuristic algorithms
KW  - Dynamic programming
KW  - Robots
KW  - Acceleration
DO  - 10.1109/ICRA40945.2020.9196673
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce Crocoddyl (Contact RObot COntrol by Differential DYnamic Library), an open-source framework tailored for efficient multi-contact optimal control. Crocoddyl efficiently computes the state trajectory and the control policy for a given predefined sequence of contacts. Its efficiency is due to the use of sparse analytical derivatives, exploitation of the problem structure, and data sharing. It employs differential geometry to properly describe the state of any geometrical system, e.g. floating-base systems. Additionally, we propose a novel optimal control algorithm called Feasibility-driven Differential Dynamic Programming (FDDP). Our method does not add extra decision variables which often increases the computation time per iteration due to factorization. FDDP shows a greater globalization strategy compared to classical Differential Dynamic Programming (DDP) algorithms. Concretely, we propose two modifications to the classical DDP algorithm. First, the backward pass accepts infeasible state-control trajectories. Second, the rollout keeps the gaps open during the early "exploratory" iterations (as expected in multipleshooting methods with only equality constraints). We showcase the performance of our framework using different tasks. With our method, we can compute highly-dynamic maneuvers (e.g. jumping, front-flip) within few milliseconds.
ER  - 

TY  - CONF
TI  - Grasp for Stacking via Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2543
EP  - 2549
AU  - J. Zhang
AU  - W. Zhang
AU  - R. Song
AU  - L. Ma
AU  - Y. Li
PY  - 2020
KW  - grippers
KW  - industrial manipulators
KW  - learning systems
KW  - neurocontrollers
KW  - optimal control
KW  - stacking
KW  - deep reinforcement learning
KW  - integrated robotic arm system
KW  - object grasping
KW  - model-free deep Q-learning method
KW  - grasping-stacking task
KW  - GSN
KW  - grasping for stacking network
KW  - industrial environments
KW  - GNet
KW  - optimal location
KW  - long-range planning
KW  - Grasping
KW  - Stacking
KW  - Task analysis
KW  - Feature extraction
KW  - Manipulators
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197508
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Integrated robotic arm system should contain both grasp and place actions. However, most grasping methods focus more on how to grasp objects, while ignoring the placement of the grasped objects, which limits their applications in various industrial environments. In this research, we propose a model-free deep Q-learning method to learn the grasping-stacking strategy end-to-end from scratch. Our method maps the images to the actions of the robotic arm through two deep networks: the grasping network (GNet) using the observation of the desk and the pile to infer the gripper's position and orientation for grasping, and the stacking network (SNet) using the observation of the platform to infer the optimal location when placing the grasped object. To make a long-range planning, the two observations are integrated in the grasping for stacking network (GSN). We evaluate the proposed GSN on a grasping-stacking task in both simulated and real-world scenarios.
ER  - 

TY  - CONF
TI  - CAGE: Context-Aware Grasping Engine
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2550
EP  - 2556
AU  - W. Liu
AU  - A. Daruna
AU  - S. Chernova
PY  - 2020
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - ubiquitous computing
KW  - context-aware grasping engine
KW  - semantic grasping
KW  - stable grasps
KW  - specific object manipulation tasks
KW  - task constraints
KW  - semantic representation
KW  - grasp contexts
KW  - object states
KW  - memorization
KW  - semantic grasps
KW  - CAGE
KW  - statistically significant margins
KW  - memorization balancing
KW  - robot
KW  - Semantics
KW  - Task analysis
KW  - Grasping
KW  - Feature extraction
KW  - Robots
KW  - Cognition
KW  - Context modeling
DO  - 10.1109/ICRA40945.2020.9197289
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide & Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: https://github.com/wliu88/railsemanticgrasping.
ER  - 

TY  - CONF
TI  - Super-Pixel Sampler: a Data-driven Approach for Depth Sampling and Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2588
EP  - 2594
AU  - A. Wolff
AU  - S. Praisler
AU  - I. Tcenov
AU  - G. Gilboa
PY  - 2020
KW  - image colour analysis
KW  - image reconstruction
KW  - image sampling
KW  - random processes
KW  - data-driven approach
KW  - depth sampling
KW  - depth acquisition
KW  - active illumination
KW  - autonomous navigation
KW  - robotic navigation
KW  - mechanical templates
KW  - sampling templates
KW  - autonomous vehicles
KW  - solid-state depth sensors
KW  - adaptive framework
KW  - simple reconstruction algorithm
KW  - generic reconstruction algorithm
KW  - sampling reconstruction algorithm
KW  - random sampling
KW  - depth completion algorithms
KW  - single-pixel prototype sampler
KW  - RGB sampling strategies
KW  - single depth sampling strategies
KW  - piecewise planar depth model
KW  - superpixel sampler
KW  - SPS
KW  - Image reconstruction
KW  - Adaptation models
KW  - Estimation
KW  - Cameras
KW  - Robot sensing systems
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197191
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Depth acquisition, based on active illumination, is essential for autonomous and robotic navigation. LiDARs (Light Detection And Ranging) with mechanical, fixed, sampling templates are commonly used in today's autonomous vehicles. An emerging technology, based on solid-state depth sensors, with no mechanical parts, allows fast and adaptive scans. In this paper, we propose an adaptive, image-driven, fast, sampling and reconstruction strategy. First, we formulate a piece-wise planar depth model and estimate its validity for indoor and outdoor scenes. Our model and experiments predict that, in the optimal case, adaptive sampling strategies with about 20-60 piece-wise planar structures can approximate well a depth map. This translates to requiring a single depth sample for every 1200 RGB samples (less than 0.1%), providing strong motivation to investigate an adaptive framework. Second, we introduce SPS (Super-Pixel Sampler), a simple, generic, sampling and reconstruction algorithm, based on super-pixels. Our sampling improves grid and random sampling, consistently, for a wide variety of reconstruction methods. Third, we propose an extremely simple and fast reconstruction for our sampler. It achieves state-of-the-art results, compared to complex image- guided depth completion algorithms, reducing the required sampling rate by a factor of 3-4. A single-pixel prototype sampler built in our lab illustrates the concept.
ER  - 

TY  - CONF
TI  - Physics-based Simulation of Continuous-Wave LIDAR for Localization, Calibration and Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2595
EP  - 2601
AU  - E. Heiden
AU  - Z. Liu
AU  - R. K. Ramachandran
AU  - G. S. Sukhatme
PY  - 2020
KW  - calibration
KW  - computerised instrumentation
KW  - differentiation
KW  - gradient methods
KW  - optical radar
KW  - optical sensors
KW  - optical tracking
KW  - optimisation
KW  - radar receivers
KW  - parameter estimation
KW  - 2D continuous-wave LIDAR sensors
KW  - light detection and ranging sensors
KW  - depth measurements
KW  - localization pipelines
KW  - autonomous robots
KW  - perception stack
KW  - physics-based simulation
KW  - sensor measurements
KW  - gradient-based optimization
KW  - Hokuyo URG-04LX LIDAR
KW  - surface-light interactions
KW  - physically plausible model
KW  - laser light
KW  - calibration
KW  - time-of-flight cameras
KW  - depth sensors
KW  - Laser radar
KW  - Measurement by laser beam
KW  - Mathematical model
KW  - Sensor phenomena and characterization
KW  - Surface emitting lasers
KW  - Laser modes
DO  - 10.1109/ICRA40945.2020.9197138
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.
ER  - 

TY  - CONF
TI  - A Spatial-temporal Multiplexing Method for Dense 3D Surface Reconstruction of Moving Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2602
EP  - 2608
AU  - C. Sui
AU  - K. He
AU  - Z. Wang
AU  - C. Lyu
AU  - H. Guo
AU  - Y. -H. Liu
PY  - 2020
KW  - image coding
KW  - image filtering
KW  - image matching
KW  - image restoration
KW  - image texture
KW  - multiplexing
KW  - object recognition
KW  - robot vision
KW  - stereo image processing
KW  - surface reconstruction
KW  - high reconstruction accuracy
KW  - fast image acquisition
KW  - spatial-temporal multiplexing method
KW  - moving objects
KW  - three-dimensional reconstruction
KW  - robotic applications
KW  - robotic recognition
KW  - spatial-multiplexing time-multiplexing structured-light techniques
KW  - image acquisition time
KW  - spatial-temporal encoded patterns
KW  - dense 3D surface reconstruction
KW  - texture map
KW  - image blur
KW  - high-frequency phase-shifting fringes
KW  - spatial-coded texture
KW  - Image reconstruction
KW  - Multiplexing
KW  - Three-dimensional displays
KW  - Surface reconstruction
KW  - Robots
KW  - Encoding
KW  - Reliability
DO  - 10.1109/ICRA40945.2020.9197462
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Three-dimensional reconstruction of dynamic objects is important for robotic applications, for example, the robotic recognition and manipulation. In this paper, we present a novel 3D surface reconstruction method for moving objects. The proposed method combines the spatial-multiplexing and time-multiplexing structured-light techniques that have advantages of less image acquisition time and accurate 3D reconstruction, respectively. A set of spatial-temporal encoded patterns are designed, where a spatial-encoded texture map is embedded into the temporal-encoded three-step phase-shifting fringes. The specifically designed spatial-coded texture assigns high-uniqueness codeword to any window on the image which helps to eliminate the phase ambiguity. In addition, the texture is robust to noise and image blur. Combining this texture with high-frequency phase-shifting fringes, high reconstruction accuracy would be ensured. This method only requires 3 patterns to uniquely encode a surface, which facilitates the fast image acquisition for each reconstruction step. A filtering stereo matching algorithm is proposed for the spatial-temporal multiplexing method to improve the matching reliability. Moreover, the reconstruction precision is further enhanced by a correspondence refinement algorithm. Experiments validate the performance of the proposed method including the high accuracy, the robustness to noise and the ability to reconstruct moving objects.
ER  - 

TY  - CONF
TI  - PhaRaO: Direct Radar Odometry using Phase Correlation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2617
EP  - 2623
AU  - Y. S. Park
AU  - Y. -S. Shin
AU  - A. Kim
PY  - 2020
KW  - distance measurement
KW  - Fourier transforms
KW  - image matching
KW  - motion estimation
KW  - radar imaging
KW  - PhaRaO
KW  - direct radar odometry
KW  - scanning radar-based odometry methods
KW  - radar image
KW  - feature-based methods
KW  - radar scans
KW  - log-polar radar images
KW  - large-scale radar data
KW  - odometry estimation
KW  - radar-based navigation
KW  - Fourier Mellin transform
KW  - Correlation
KW  - Radar imaging
KW  - Estimation
KW  - Image resolution
KW  - Feature extraction
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9197231
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recent studies in radar-based navigation present promising navigation performance using scanning radars. These scanning radar-based odometry methods are mostly feature-based; they detect and match salient features within a radar image. Differing from existing feature-based methods, this paper reports on a method using direct radar odometry, PhaRaO, which infers relative motion from a pair of radar scans via phase correlation. Specifically, we apply the Fourier Mellin transform (FMT) for Cartesian and log-polar radar images to sequentially estimate rotation and translation. In doing so, we decouple rotation and translation estimations in a coarse-to-fine manner to achieve real-time performance. The proposed method is evaluated using large-scale radar data obtained from various environments. The inferred trajectory yields a 2.34% (translation) and 2.93° (rotation) Relative Error (RE) over a 4km path length on average for the odometry estimation.
ER  - 

TY  - CONF
TI  - DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR Scans
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2624
EP  - 2630
AU  - A. Dewan
AU  - W. Burgard
PY  - 2020
KW  - Bayes methods
KW  - control engineering computing
KW  - convolutional neural nets
KW  - filtering theory
KW  - image segmentation
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural net architecture
KW  - object detection
KW  - optical radar
KW  - path planning
KW  - recursive estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - DeepTemporalSeg
KW  - temporally consistent semantic segmentation
KW  - 3d LiDAR scans
KW  - semantic characteristics
KW  - autonomous robot operation
KW  - deep convolutional neural network
KW  - DCNN
KW  - LiDAR scan
KW  - pedestrian
KW  - bicyclist
KW  - dense blocks
KW  - depth separable convolutions
KW  - current semantic state
KW  - recursive estimation
KW  - isolated erroneous predictions
KW  - neural network architectures
KW  - Bayes filter approach
KW  - KITTI tracking benchmark
KW  - Semantics
KW  - Laser radar
KW  - Image segmentation
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Neural networks
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9197193
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Understanding the semantic characteristics of the environment is a key enabler for autonomous robot operation. In this paper, we propose a deep convolutional neural network (DCNN) for semantic segmentation of a LiDAR scan into the classes car, pedestrian and bicyclist. This architecture is based on dense blocks and efficiently utilizes depth separable convolutions to limit the number of parameters while still maintaining the state-of-the-art performance. To make the predictions from the DCNN temporally consistent, we propose a Bayes filter based method. This method uses the predictions from the neural network to recursively estimate the current semantic state of a point in a scan. This recursive estimation uses the knowledge gained from previous scans, thereby making the predictions temporally consistent and robust towards isolated erroneous predictions. We compare the performance of our proposed architecture with other state-of-the-art neural network architectures and report substantial improvement. For the proposed Bayes filter approach, we shows results on various sequences in the KITTI tracking benchmark.
ER  - 

TY  - CONF
TI  - Discrete Bimanual Manipulation for Wrench Balancing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2631
EP  - 2637
AU  - S. Cruciani
AU  - D. Almeida
AU  - D. Kragic
AU  - Y. Karayiannidis
PY  - 2020
KW  - manipulators
KW  - motion control
KW  - payload limitations
KW  - single arm
KW  - grasped object
KW  - wrenches
KW  - payload limits
KW  - dual-arm robot
KW  - robot arms changes
KW  - wrench imbalance
KW  - discrete bimanual manipulation
KW  - grasp points
KW  - balanced configuration
KW  - robot experiments
KW  - grasping force
KW  - wrench balancing
KW  - Manipulators
KW  - Force
KW  - Grippers
KW  - Planning
KW  - Grasping
KW  - Payloads
DO  - 10.1109/ICRA40945.2020.9196527
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dual-arm robots can overcome grasping force and payload limitations of a single arm by jointly grasping an object. However, if the distribution of mass of the grasped object is not even, each arm will experience different wrenches that can exceed its payload limits. In this work, we consider the problem of balancing the wrenches experienced by a dual-arm robot grasping a rigid tray. The distribution of wrenches among the robot arms changes due to objects being placed on the tray. We present an approach to reduce the wrench imbalance among arms through discrete bimanual manipulation. Our approach is based on sequential sliding motions of the grasp points on the surface of the object, to attain a more balanced configuration. We validate our modeling approach and system design through a set of robot experiments.
ER  - 

TY  - CONF
TI  - NeuroTac: A Neuromorphic Optical Tactile Sensor applied to Texture Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2654
EP  - 2660
AU  - B. Ward-Cherrier
AU  - N. Pestell
AU  - N. F. Lepora
PY  - 2020
KW  - biomedical equipment
KW  - biomedical optical imaging
KW  - biomimetics
KW  - feedback
KW  - haptic interfaces
KW  - image classification
KW  - image texture
KW  - medical image processing
KW  - prosthetics
KW  - skin
KW  - tactile sensors
KW  - touch (physiological)
KW  - NeuroTac
KW  - texture recognition
KW  - artificial tactile sensing capabilities
KW  - rival human touch
KW  - robotics
KW  - prosthetics
KW  - biomimetic tactile sensors
KW  - grasping
KW  - manipulation tasks
KW  - biomimetic hardware design
KW  - TacTip sensor
KW  - layered papillae structure
KW  - human glabrous skin
KW  - event-based camera
KW  - spike trains
KW  - texture classification task
KW  - spike coding methods
KW  - artificial textures
KW  - spike-based output
KW  - biomimetic tactile perception algorithms
KW  - neuromorphic optical tactile sensor
KW  - Encoding
KW  - Neuromorphics
KW  - Tactile sensors
KW  - Cameras
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197046
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Developing artificial tactile sensing capabilities that rival human touch is a long-term goal in robotics and prosthetics. Gradually more elaborate biomimetic tactile sensors are being developed and applied to grasping and manipulation tasks to help achieve this goal. Here we present the neuroTac, a novel neuromorphic optical tactile sensor. The neuroTac combines the biomimetic hardware design from the TacTip sensor which mimicks the layered papillae structure of human glabrous skin, with an event-based camera (DAVIS240, iniVation) and algorithms which transduce contact information in the form of spike trains. The performance of the sensor is evaluated on a texture classification task, with four spike coding methods being implemented and compared: Intensive, Spatial, Temporal and Spatiotemporal. We found timing-based coding methods performed with the highest accuracy over both artificial and natural textures. The spike-based output of the neuroTac could enable the development of biomimetic tactile perception algorithms in robotics as well as non-invasive and invasive haptic feedback methods in prosthetics.
ER  - 

TY  - CONF
TI  - Reducing Uncertainty in Pose Estimation under Complex Contacts via Force Forecast
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2661
EP  - 2667
AU  - H. Mao
AU  - J. Xiao
PY  - 2020
KW  - haptic interfaces
KW  - manipulators
KW  - pose estimation
KW  - regression analysis
KW  - robotic assembly
KW  - trees (mathematics)
KW  - sphere-tree representation
KW  - least-uncertain estimate
KW  - relative contact
KW  - multiregion complex contacts
KW  - contact types
KW  - contact locations
KW  - object shapes
KW  - object poses
KW  - complex shapes
KW  - pose estimation
KW  - force forecast
KW  - autonomous robotic manipulation
KW  - simulated complex contacts
KW  - force sensing
KW  - constraint-based haptic simulation algorithm
KW  - three-pin peg-in-hole robotic assembly tasks
KW  - contact-rich two-pin peg-in-hole assembly tasks
KW  - calibration
KW  - regression model
KW  - Force
KW  - Uncertainty
KW  - Robot sensing systems
KW  - Task analysis
KW  - Calibration
DO  - 10.1109/ICRA40945.2020.9197190
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - How to reduce uncertainty in object pose estimation under complex contacts is crucial to autonomous robotic manipulation and assembly. In this paper, we introduce an approach through forecasting contact force from simulated complex contacts with calibration based on real force sensing. A constraint-based haptic simulation algorithm is used with sphere-tree representation of contacting objects to compute contact poses and forces, and through matching the computed forces to measured real force data via a regression model, the least-uncertain estimate of the relative contact pose is obtained. Our approach can handle multi-region complex contacts and does not make any assumption about contact types or contact locations. It also does not have restriction on object shapes. We have applied the force forecast approach to reducing uncertainty in estimating object poses in challenging peg-in-hole robotic assembly tasks and demonstrate the effectiveness of the approach by successful completion of contact-rich two-pin and three-pin real peg-in-hole assembly tasks with complex shapes of pins and holes.
ER  - 

TY  - CONF
TI  - Comparison of Constrained and Unconstrained Human Grasp Forces Using Fingernail Imaging and Visual Servoing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2668
EP  - 2674
AU  - N. Fallahinia
AU  - S. A. Mascaro
PY  - 2020
KW  - dexterous manipulators
KW  - force measurement
KW  - grippers
KW  - robot vision
KW  - visual servoing
KW  - fingernail imaging
KW  - force measurement
KW  - visual tracking system
KW  - force collaboration
KW  - constrained human grasp forces
KW  - unconstrained human grasp forces
KW  - visual servoing
KW  - 3D fingertip forces
KW  - robotic arms
KW  - Grasping
KW  - Force
KW  - Estimation
KW  - Cameras
KW  - Mathematical model
KW  - Force measurement
DO  - 10.1109/ICRA40945.2020.9196963
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fingernail imaging has been proven to be effective in prior works [1], [2] for estimating the 3D fingertip forces with a maximum RMS estimation error of 7%. In the current research, fingernail imaging is used to perform unconstrained grasp force measurement on multiple fingers to study human grasping. Moreover, two robotic arms with mounted cameras and a visual tracking system have been devised to keep the human fingers in the camera frame during the experiments. Experimental tests have been conducted for six human subjects under both constrained and unconstrained grasping conditions, and the results indicate a significant difference in force collaboration among the fingers between the two grasping conditions. Another interesting result according to the experiments is that in comparison to constrained grasping, unconstrained grasp forces are more evenly distributed over the fingers and there is less force variation (more steadiness) in each finger force. These results validate the importance of measuring grasp forces in an unconstrained manner in order to study how humans naturally grasp objects.
ER  - 

TY  - CONF
TI  - Robust and Efficient Estimation of Absolute Camera Pose for Monocular Visual Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2675
EP  - 2681
AU  - H. Li
AU  - W. Chen
AU  - J. Zhao
AU  - J. -C. Bazin
AU  - L. Luo
AU  - Z. Liu
AU  - Y. -H. Liu
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - optimisation
KW  - pose estimation
KW  - high generality
KW  - absolute camera pose
KW  - monocular visual odometry
KW  - cost function
KW  - branch-and-bound
KW  - high outlier ratios
KW  - robust estimation
KW  - efficient estimation
KW  - 3D-to-2D point correspondences
KW  - projection constraint
KW  - local optimizer
KW  - effective function bounds
KW  - real-time applications
KW  - synthetic datasets
KW  - real-world datasets
KW  - Robustness
KW  - Pose estimation
KW  - Cameras
KW  - Cost function
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196814
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Given a set of 3D-to-2D point correspondences corrupted by outliers, we aim to robustly estimate the absolute camera pose. Existing methods robust to outliers either fail to guarantee high robustness and efficiency simultaneously, or require an appropriate initial pose and thus lack generality. In contrast, we propose a novel approach based on the robust "L2-minimizing estimate" (L2E) loss. We first define a novel cost function by integrating the projection constraint into the L2E loss. Then to efficiently obtain the global minimum of this function, we propose a hybrid strategy of a local optimizer and branch-and-bound. For branch-and-bound, we derive effective function bounds. Our approach can handle high outlier ratios, leading to high robustness. It can run reliably regardless of whether the initial pose is appropriate, providing high generality. Moreover, given a decent initial pose, it is suitable for real-time applications. Experiments on synthetic and real-world datasets showed that our approach outperforms state-of-the-art methods in terms of robustness and/or efficiency.
ER  - 

TY  - CONF
TI  - Robust Vision-based Obstacle Avoidance for Micro Aerial Vehicles in Dynamic Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2682
EP  - 2688
AU  - J. Lin
AU  - H. Zhu
AU  - J. Alonso-Mora
PY  - 2020
KW  - collision avoidance
KW  - helicopters
KW  - mobile robots
KW  - predictive control
KW  - robot dynamics
KW  - robot vision
KW  - state estimation
KW  - robust vision-based obstacle avoidance
KW  - microaerial vehicle
KW  - dynamic environments
KW  - on-board vision-based approach
KW  - moving obstacle
KW  - efficient obstacle detection
KW  - tracking algorithm
KW  - depth image pairs
KW  - robust collision avoidance
KW  - chance-constrained model predictive controller
KW  - collision probability
KW  - account MAV dynamics
KW  - state estimation
KW  - obstacle sensing uncertainties
KW  - on-line collision avoidance
KW  - Collision avoidance
KW  - Uncertainty
KW  - Cameras
KW  - Robustness
KW  - Sensors
KW  - Predictive models
KW  - State estimation
DO  - 10.1109/ICRA40945.2020.9197481
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an on-board vision-based approach for avoidance of moving obstacles in dynamic environments. Our approach relies on an efficient obstacle detection and tracking algorithm based on depth image pairs, which provides the estimated position, velocity and size of the obstacles. Robust collision avoidance is achieved by formulating a chance-constrained model predictive controller (CC-MPC) to ensure that the collision probability between the micro aerial vehicle (MAV) and each moving obstacle is below a specified threshold. The method takes into account MAV dynamics, state estimation and obstacle sensing uncertainties. The proposed approach is implemented on a quadrotor equipped with a stereo camera and is tested in a variety of environments, showing effective on-line collision avoidance of moving obstacles.
ER  - 

TY  - CONF
TI  - Proximity Estimation Using Vision Features Computed On Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2689
EP  - 2695
AU  - J. Chen
AU  - Y. Liu
AU  - S. J. Carey
AU  - P. Dudek
PY  - 2020
KW  - collision avoidance
KW  - computer vision
KW  - feature extraction
KW  - image sensors
KW  - microcontrollers
KW  - mobile robots
KW  - recurrent neural nets
KW  - robot vision
KW  - sensor arrays
KW  - VLSI
KW  - neural network output
KW  - image capture
KW  - control system
KW  - trained neural network
KW  - fully connected layer-recurrent
KW  - training data
KW  - infrared proximity sensors
KW  - vision output
KW  - sparse feature description data
KW  - feature algorithms
KW  - pixel processor array chip
KW  - embedded 256×256 processor SIMD array
KW  - image sensor
KW  - RC model car
KW  - microcontroller
KW  - SCAMP-5 vision chip
KW  - vision system integrating
KW  - experimental vehicle
KW  - blobs
KW  - corner points
KW  - abstract features
KW  - monocular vision based proximity estimation system
KW  - vision features computed
KW  - velocity 0.64 m/s to 1.8 m/s
KW  - time 4.0 ms
KW  - Hardware
KW  - Estimation
KW  - Neural networks
KW  - Machine vision
KW  - Feature extraction
KW  - Registers
KW  - Image edge detection
DO  - 10.1109/ICRA40945.2020.9197370
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a monocular vision based proximity estimation system using abstract features, such as corner points, blobs and edges, as inputs to a neural network. An experimental vehicle was built using a vision system integrating the SCAMP-5 vision chip, a micro-controller, and an RC model car. The vision chip includes image sensor with embedded 256×256 processor SIMD array. The pixel processor array chip was programmed to capture images and run the feature algorithms directly on the focal plane, and then digest them so that only sparse feature description data were read-out in the form of 40 values. By logging the vision output and the output from three infrared proximity sensors, training data were obtained to train three fully connected layer-recurrent neural networks with fewer than 700 parameters each. The trained neural network was able to estimate the proximity to the level of accuracy sufficient for a reactive collision avoidance behaviour to be achieved. The latency of the control system, from image capture to neural network output, was under 4ms, enabling the vehicles to avoid obstacles while moving at 0.64m/s to 1.8m/s in the experiment.
ER  - 

TY  - CONF
TI  - Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2696
EP  - 2702
AU  - L. Gao
AU  - J. Su
AU  - J. Cui
AU  - X. Zeng
AU  - X. Peng
AU  - L. Kneip
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image registration
KW  - mobile robots
KW  - motion estimation
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - steering systems
KW  - tree searching
KW  - efficient globally-optimal correspondence-less visual odometry
KW  - planar ground vehicles
KW  - 2 DoF Ackermann steering model
KW  - downward facing camera
KW  - simple image registration problem
KW  - 2-parameter planar homography
KW  - ground-plane features
KW  - plane-based Ackermann motion estimation
KW  - correspondence-based hypothesise
KW  - test schemes
KW  - fronto-parallel motion
KW  - branch-and-bound optimisation technique
KW  - low-dimensional parametrisation
KW  - Cameras
KW  - Optimization
KW  - Transmission line matrix methods
KW  - Land vehicles
KW  - Motion estimation
KW  - Image registration
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196595
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The motion of planar ground vehicles is often non-holonomic, and as a result may be modelled by the 2 DoF Ackermann steering model. We analyse the feasibility of estimating such motion with a downward facing camera that exerts fronto-parallel motion with respect to the ground plane. This turns the motion estimation into a simple image registration problem in which we only have to identify a 2-parameter planar homography. However, one difficulty that arises from this setup is that ground-plane features are indistinctive and thus hard to match between successive views. We encountered this difficulty by introducing the first globally-optimal, correspondence-less solution to plane-based Ackermann motion estimation. The solution relies on the branch-and-bound optimisation technique. Through the low-dimensional parametrisation, a derivation of tight bounds, and an efficient implementation, we demonstrate how this technique is eventually amenable to accurate real-time motion estimation. We prove its property of global optimality and analyse the impact of assuming a locally constant centre of rotation. Our results on real data finally demonstrate a significant advantage over the more traditional, correspondence-based hypothesise-and-test schemes.
ER  - 

TY  - CONF
TI  - egoTEB: Egocentric, Perception Space Navigation Using Timed-Elastic-Bands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2703
EP  - 2709
AU  - J. S. Smith
AU  - R. Xu
AU  - P. Vela
PY  - 2020
KW  - aerospace navigation
KW  - aerospace robotics
KW  - collision avoidance
KW  - graph theory
KW  - mobile robots
KW  - Monte Carlo methods
KW  - motion control
KW  - optimisation
KW  - trajectory control
KW  - grid-based representations
KW  - optimization graph
KW  - local planning map
KW  - egoTEB
KW  - timed-elastic-bands
KW  - TEB hierarchical planner
KW  - real-time navigation
KW  - collision avoidance
KW  - goal directed motion
KW  - multitrajectory optimization based synthesis method
KW  - topologically distinct trajectory candidates
KW  - factor graph approach
KW  - egocentric perception space navigation
KW  - egocentric perception space representations
KW  - Monte Carlo evaluations
KW  - autonomous mobile robot
KW  - Trajectory
KW  - Optimization
KW  - Navigation
KW  - Planning
KW  - Robots
KW  - Collision avoidance
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196721
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The TEB hierarchical planner for real-time navigation through unknown environments is highly effective at balancing collision avoidance with goal directed motion. Designed over several years and publications, it implements a multi-trajectory optimization based synthesis method for identifying topologically distinct trajectory candidates through navigable space. Unfortunately, the underlying factor graph approach to the optimization problem induces a mismatch between grid-based representations and the optimization graph, which leads to several time and optimization inefficiencies. This paper explores the impact of using egocentric, perception space representations for the local planning map. Doing so alleviates many of the identified issues related to TEB and leads to a new method called egoTEB. Timing experiments and Monte Carlo evaluations in benchmark worlds quantify the benefits of egoTEB for navigation through uncertain environments.
ER  - 

TY  - CONF
TI  - Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2718
EP  - 2724
AU  - R. Jeong
AU  - Y. Aytar
AU  - D. Khosid
AU  - Y. Zhou
AU  - J. Kay
AU  - T. Lampe
AU  - K. Bousmalis
AU  - F. Nori
PY  - 2020
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - robot vision
KW  - visual robotic manipulation
KW  - robotic visual data
KW  - reinforcement learning algorithms
KW  - robotic learning
KW  - state estimation
KW  - latent state representation
KW  - deep reinforcement learning
KW  - unlabeled real robot data
KW  - robot experience
KW  - time-contrastive techniques
KW  - learned state representation
KW  - vision-based reinforcement learning agent
KW  - standard visual domain adaptation techniques
KW  - self-supervised sim-to-real adaptation
KW  - sequence-based supervised objectives
KW  - contrastive forward dynamics loss
KW  - Robots
KW  - Task analysis
KW  - Adaptation models
KW  - Visualization
KW  - Stacking
KW  - Data models
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197326
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collecting and automatically obtaining reward signals from real robotic visual data for the purposes of training reinforcement learning algorithms can be quite challenging and time-consuming. Methods for utilizing unlabeled data can have a huge potential to further accelerate robotic learning. We consider here the problem of performing manipulation tasks from pixels. In such tasks, choosing an appropriate state representation is crucial for planning and control. This is even more relevant with real images where noise, occlusions and resolution affect the accuracy and reliability of state estimation. In this work, we learn a latent state representation implicitly with deep reinforcement learning in simulation, and then adapt it to the real domain using unlabeled real robot data. We propose to do so by optimizing sequence-based self- supervised objectives. These use the temporal nature of robot experience, and can be common in both the simulated and real domains, without assuming any alignment of underlying states in simulated and unlabeled real images. We further propose a novel such objective, the Contrastive Forward Dynamics loss, which combines dynamics model learning with time-contrastive techniques. The learned state representation that results from our methods can be used to robustly solve a manipulation task in simulation and to successfully transfer the learned skill on a real system. We demonstrate the effectiveness of our approaches by training a vision-based reinforcement learning agent for cube stacking. Agents trained with our method, using only 5 hours of unlabeled real robot data for adaptation, shows a clear improvement over domain randomization, and standard visual domain adaptation techniques for sim-to-real transfer.
ER  - 

TY  - CONF
TI  - Meta Reinforcement Learning for Sim-to-real Domain Adaptation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2725
EP  - 2731
AU  - K. Arndt
AU  - M. Hazara
AU  - A. Ghadirzadeh
AU  - V. Kyrki
PY  - 2020
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - dynamic conditions
KW  - task-specific trajectory generation model
KW  - KUKA LBR 4+ robot
KW  - sim-to-real domain transfer
KW  - robotic policy training
KW  - meta reinforcement learning
KW  - Adaptation models
KW  - Task analysis
KW  - Trajectory
KW  - Robots
KW  - Training
KW  - Learning (artificial intelligence)
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196540
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.
ER  - 

TY  - CONF
TI  - Variational Auto-Regularized Alignment for Sim-to-Real Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2732
EP  - 2738
AU  - M. Hwasser
AU  - D. Kragic
AU  - R. Antonova
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - variational auto-regularized alignment
KW  - sim-to-real control
KW  - general-purpose simulators
KW  - variational autoencoder
KW  - black-box simulation
KW  - latent space
KW  - encoder training
KW  - simulation parameter distribution
KW  - matching parameter distributions
KW  - ABB YuMi robot hardware
KW  - Hardware
KW  - Decoding
KW  - Computational modeling
KW  - Neural networks
KW  - Training
KW  - Trajectory
KW  - Benchmark testing
DO  - 10.1109/ICRA40945.2020.9197130
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - General-purpose simulators can be a valuable data source for flexible learning and control approaches. However, training models or control policies in simulation and then directly applying to hardware can yield brittle control. Instead, we propose a novel way to use simulators as regularizers. Our approach regularizes a decoder of a variational autoencoder to a black-box simulation, with the latent space bound to a subset of simulator parameters. This enables successful encoder training from a small number of real-world trajectories (10 in our experiments), yielding a latent space with simulation parameter distribution that matches the real-world setting. We use a learnable mixture for the latent prior/posterior, which implies a highly flexible class of densities for the posterior fit. Our approach is scalable and does not require restrictive distributional assumptions. We demonstrate ability to recover matching parameter distributions on a range of benchmarks, challenging custom simulation environments and several real-world scenarios. Our experiments using ABB YuMi robot hardware show ability to help reinforcement learning approaches overcome cases of severe sim-to-real mismatch.
ER  - 

TY  - CONF
TI  - Experience Selection Using Dynamics Similarity for Efficient Multi-Source Transfer Learning Between Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2739
EP  - 2745
AU  - M. J. Sorocky
AU  - S. Zhou
AU  - A. P. Schoellig
PY  - 2020
KW  - control engineering computing
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - robot programming
KW  - robust control
KW  - user experience
KW  - robot systems
KW  - multisource inter-robot transfer learning
KW  - quadrotor experiments
KW  - real source quadrotor
KW  - virtual source quadrotor
KW  - experience selection
KW  - dynamics similarity
KW  - robotics literature
KW  - knowledge transfer
KW  - learning process
KW  - robust control theory
KW  - data-efficiency algorithm
KW  - Measurement
KW  - Task analysis
KW  - Heuristic algorithms
KW  - Robot sensing systems
KW  - Robust control
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196744
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the robotics literature, different knowledge transfer approaches have been proposed to leverage the experience from a source task or robot-real or virtual-to accelerate the learning process on a new task or robot. A commonly made but infrequently examined assumption is that incorporating experience from a source task or robot will be beneficial. In practice, inappropriate knowledge transfer can result in negative transfer or unsafe behaviour. In this work, inspired by a system gap metric from robust control theory, the ν-gap, we present a data-efficient algorithm for estimating the similarity between pairs of robot systems. In a multi-source inter-robot transfer learning setup, we show that this similarity metric allows us to predict relative transfer performance and thus informatively select experiences from a source robot before knowledge transfer. We demonstrate our approach with quadrotor experiments, where we transfer an inverse dynamics model from a real or virtual source quadrotor to enhance the tracking performance of a target quadrotor on arbitrary hand-drawn trajectories. We show that selecting experiences based on the proposed similarity metric effectively facilitates the learning of the target quadrotor, improving performance by 62% compared to a poorly selected experience.
ER  - 

TY  - CONF
TI  - DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2746
EP  - 2754
AU  - B. Balaji
AU  - S. Mallya
AU  - S. Genc
AU  - S. Gupta
AU  - L. Dirac
AU  - V. Khare
AU  - G. Roy
AU  - T. Sun
AU  - Y. Tao
AU  - B. Townsend
AU  - E. Calleja
AU  - S. Muralidhara
AU  - D. Karuppasamy
PY  - 2020
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot dynamics
KW  - robot vision
KW  - reality gap
KW  - joint perception
KW  - on-demand compute architecture
KW  - training optimal policies
KW  - robust evaluation
KW  - deep reinforcement learning
KW  - robotic control agent
KW  - raw camera images
KW  - robust path planning
KW  - DeepRacer
KW  - autonomous racing platform
KW  - Sim2Real reinforcement learning
KW  - end-to-end experimentation
KW  - RL
KW  - intelligent control systems
KW  - monocular camera
KW  - physical world
KW  - robust reinforcement learning
KW  - model-free learning
KW  - Training
KW  - Automobiles
KW  - Robots
KW  - Computational modeling
KW  - Cameras
KW  - Robustness
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197465
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub2.
ER  - 

TY  - CONF
TI  - A closed-loop and ergonomic control for prosthetic wrist rotation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2763
EP  - 2769
AU  - M. Legrand
AU  - N. Jarrassé
AU  - F. Richer
AU  - G. Morel
PY  - 2020
KW  - artificial limbs
KW  - biomechanics
KW  - closed loop systems
KW  - electromyography
KW  - ergonomics
KW  - medical signal processing
KW  - open-loop scheme
KW  - ergonomic posture
KW  - control scheme
KW  - prostheses users
KW  - body compensation
KW  - closed-loop control
KW  - prosthetic level
KW  - control loop
KW  - correcting errors
KW  - upper-limb prostheses control
KW  - prosthetic wrist rotation
KW  - ergonomic control
KW  - Prosthetics
KW  - Task analysis
KW  - Wrist
KW  - Ergonomics
KW  - Wires
KW  - Robots
KW  - Muscles
DO  - 10.1109/ICRA40945.2020.9197554
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Beyond the ultimate goal of prosthetics, repairing all the capabilities of amputees, the development line of upper-limb prostheses control mainly relies on three aspects: the robustness, the intuitiveness and the reduction of mental fatigue. Many complex structures and algorithms are proposed but no one question a common open-loop nature, where the user is the one in charge of correcting errors. Yet, closing the control loop at the prosthetic level may help to improve the three main lines of research cited above. One major issue to build a closed-loop control is the definition of a reliable error signal; this paper proposes to use body compensations, naturally exhibited by prostheses users when the motion of their device is inaccurate, as such. The described control scheme measures these compensatory movements and makes the prosthesis move in order to bring back the user into an ergonomic posture. The function of the prosthesis is no longer to perform a given motion but rather to correct the posture of its user while s/he focus on performing an endpoint task. This concept was validated and compared to a standard open-loop scheme, for the control of a prosthetic wrist, with five healthy subjects completing a dedicated task with a customized transradial prosthesis. Results show that the presented closed-loop control allows for more intuitiveness and less mental burden without enhancing body compensation.
ER  - 

TY  - CONF
TI  - Comparison of online algorithms for the tracking of multiple magnetic targets in a myokinetic control interface*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2770
EP  - 2776
AU  - J. Montero
AU  - M. Gherardini
AU  - F. Clemente
AU  - C. Cipriani
PY  - 2020
KW  - Kalman filters
KW  - medical robotics
KW  - optimisation
KW  - prosthetics
KW  - surgery
KW  - telerobotics
KW  - tracking
KW  - localization algorithms
KW  - optimization
KW  - Levenberg-Marquardt algorithm
KW  - trust region reflective algorithm
KW  - robotics applications
KW  - remote tracking
KW  - multiple magnetic targets
KW  - myokinetic control interface
KW  - magnetic tracking algorithms
KW  - biomedical applications
KW  - teleoperated surgical robots
KW  - upper limb prostheses
KW  - Magnetostatics
KW  - Magnetic separation
KW  - Robots
KW  - Magnetic sensors
KW  - Magnetic devices
DO  - 10.1109/ICRA40945.2020.9196804
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Magnetic tracking algorithms can be used to determine the position and orientation of magnetic markers or devices. These techniques are particularly interesting for biomedical applications such as teleoperated surgical robots or the control of upper limb prostheses. The performance of different algorithms used for magnetic tracking was compared in the past. However, in most cases, those algorithms were required to track a single magnet.Here we investigated the performance of three localization algorithms in tracking up to 9 magnets: two optimization-based (Levenberg-Marquardt algorithm, LMA, and Trust Region Reflective algorithm, TRRA) and one recursion-based (Unscented Kalman Filter, UKF). The tracking accuracy of the algorithms and their computation time were investigated through simulations.The accuracy of the three algorithms, when tracking up to six magnets, was similar, leading to estimation errors varying from 0.06 ± 0.02 mm to 2.26 ± 0.07 mm within a 100 mm × 54 mm × 100 mm workspace, at the highest sampling frequency. In all cases, computation times under 300 ms for the UKF and 45 ms for the LMA/TRRA were obtained. The TRRA showed the best tracking performance overall.These outcomes are of interest for a wide range of robotics applications that require remote tracking.
ER  - 

TY  - CONF
TI  - Congestion-aware Evacuation Routing using Augmented Reality Devices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2798
EP  - 2804
AU  - Z. Zhang
AU  - H. Liu
AU  - Z. Jiao
AU  - Y. Zhu
AU  - S. -C. Zhu
PY  - 2020
KW  - augmented reality
KW  - emergency management
KW  - optimisation
KW  - congestion-aware evacuation
KW  - congestion-aware routing solution
KW  - indoor evacuation
KW  - real-time individual-customized evacuation routes
KW  - multiple destinations
KW  - population density map
KW  - obtained on-the-fly
KW  - congestion distribution
KW  - optimal solution
KW  - time-efficient evacuation route
KW  - AR devices
KW  - user-end augmented reality devices
KW  - Sociology
KW  - Statistics
KW  - Routing
KW  - Real-time systems
KW  - Path planning
KW  - Robots
KW  - Headphones
DO  - 10.1109/ICRA40945.2020.9197494
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a congestion-aware routing solution for indoor evacuation, which produces real-time individual-customized evacuation routes among multiple destinations while keeping tracks of all evacuees' locations. A population density map, obtained on-the-fly by aggregating locations of evacuees from user-end Augmented Reality (AR) devices, is used to model the congestion distribution inside a building. To efficiently search the evacuation route among all destinations, a variant of A* algorithm is devised to obtain the optimal solution in a single pass. In a series of simulated studies, we show that the proposed algorithm is more computationally optimized compared to classic path planning algorithms; it generates a more time-efficient evacuation route for each individual that minimizes the overall congestion. A complete system using AR devices is implemented for a pilot study in real-world environments, demonstrating the efficacy of the proposed approach.
ER  - 

TY  - CONF
TI  - Human-robot interaction for robotic manipulator programming in Mixed Reality
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2805
EP  - 2811
AU  - M. Ostanin
AU  - S. Mikhel
AU  - A. Evlampiev
AU  - V. Skvortsova
AU  - A. Klimchik
PY  - 2020
KW  - augmented reality
KW  - human-robot interaction
KW  - manipulators
KW  - robot programming
KW  - human-robot interaction
KW  - robotic manipulator programming
KW  - mixed reality
KW  - interactive programming
KW  - HoloLens glasses
KW  - robotic operation system
KW  - robotic manipulators
KW  - robot location
KW  - point cloud analysis
KW  - virtual markers
KW  - menus
KW  - pick-and-place operation
KW  - contact operations execution
KW  - UR10e robot
KW  - KUKA iiwa robot
KW  - Virtual reality
KW  - Trajectory
KW  - Collision avoidance
KW  - Manipulators
KW  - Programming
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196965
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The paper presents an approach for interactive programming of the robotic manipulator using mixed reality. The developed system is based on the HoloLens glasses connected through Robotic Operation System to Unity engine and robotic manipulators. The system gives a possibility to recognize the real robot location by the point cloud analysis, to use virtual markers and menus for the task creation, to generate a trajectory for execution in the simulator or on the real manipulator. It also provides the possibility of scaling virtual and real worlds for more accurate planning. The proposed framework has been tested on pick-and-place and contact operations execution by UR10e and KUKA iiwa robots.
ER  - 

TY  - CONF
TI  - Heart Rate Sensing with a Robot Mounted mmWave Radar
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2812
EP  - 2818
AU  - P. Zhao
AU  - C. X. Lu
AU  - B. Wang
AU  - C. Chen
AU  - L. Xie
AU  - M. Wang
AU  - N. Trigoni
AU  - A. Markham
PY  - 2020
KW  - cardiology
KW  - convolutional neural nets
KW  - health care
KW  - medical robotics
KW  - medical signal processing
KW  - patient monitoring
KW  - patient rehabilitation
KW  - telemedicine
KW  - mBeats features
KW  - deep neural network predictor
KW  - robust operation
KW  - post-operative rehabilitation
KW  - heart rate sensing
KW  - robot mounted mmWave radar
KW  - post-operative recovery
KW  - noncontact heart rate monitoring
KW  - static wall-mounted device
KW  - millimeter wave radar system
KW  - periodic heart rate measurements
KW  - users daily activities
KW  - mmWave servoing module
KW  - Heart rate
KW  - Robot sensing systems
KW  - Radar
KW  - Monitoring
KW  - Estimation
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197437
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Heart rate monitoring at home is a useful metric for assessing health e.g. of the elderly or patients in post-operative recovery. Although non-contact heart rate monitoring has been widely explored, typically using a static, wall-mounted device, measurements are limited to a single room and sensitive to user orientation and position. In this work, we propose mBeats, a robot mounted millimeter wave (mmWave) radar system that provide periodic heart rate measurements under different user poses, without interfering in a users daily activities. mBeats contains a mmWave servoing module that adaptively adjusts the sensor angle to the best reflection pro le. Furthermore, mBeats features a deep neural network predictor, which can estimate heart rate from the lower leg and additionally provides estimation uncertainty. Through extensive experiments, we demonstrate accurate and robust operation of mBeats in a range of scenarios. We believe by integrating mobility and adaptability, mBeats can empower many down-stream healthcare applications at home, such as palliative care, post-operative rehabilitation and telemedicine.
ER  - 

TY  - CONF
TI  - Prediction of Gait Cycle Percentage Using Instrumented Shoes with Artificial Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2834
EP  - 2840
AU  - A. Prado
AU  - X. Cao
AU  - X. Ding
AU  - S. K. Agrawal
PY  - 2020
KW  - biomedical measurement
KW  - gait analysis
KW  - medical computing
KW  - neural nets
KW  - patient rehabilitation
KW  - encoder-decoder
KW  - RMSE
KW  - root mean square error
KW  - inertial measurement unit
KW  - instrument standard footwear
KW  - overground walking
KW  - continuous gait cycle
KW  - walking over-ground
KW  - gait rehabilitation
KW  - gait parameters
KW  - traditional gait measurement systems
KW  - gait abnormalities
KW  - gait training
KW  - artificial neural networks
KW  - instrumented shoes
KW  - gait cycle percentage
KW  - Training
KW  - Instruments
KW  - Foot
KW  - Legged locomotion
KW  - Prediction algorithms
KW  - Footwear
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196747
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gait training is widely used to treat gait abnormalities. Traditional gait measurement systems are limited to instrumented laboratories. Even though gait measurements can be made in these settings, it is challenging to estimate gait parameters robustly in real-time for gait rehabilitation, especially when walking over-ground. In this paper, we present a novel approach to track the continuous gait cycle during overground walking outside the laboratory. In this approach, we instrument standard footwear with a sensorized insole and an inertial measurement unit. Artificial neural networks are used on the raw data obtained from the insoles and IMUs to compute the continuous percentage of the gait cycle for the entire walking session. We show in this paper that when tested with novel subjects, we can predict the gait cycle with a Root Mean Square Error (RMSE) of 7.2%. The onset of each cycle can be detected within an RMSE time of 41.5 ms with a 99% detection rate. The algorithm was tested with 18840 strides collected from 24 adults. In this paper, we tested a combination of fully-connected layers, an Encoder-Decoder using convolutional layers, and recurrent layers to identify an architecture that provided the best performance.
ER  - 

TY  - CONF
TI  - Flow Compensation for Hydraulic Direct-Drive System with a Single-rod Cylinder Applied to Biped Humanoid Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2857
EP  - 2863
AU  - J. Shimizu
AU  - T. Otani
AU  - H. Mizukami
AU  - K. Hashimoto
AU  - A. Takanishi
PY  - 2020
KW  - flow control
KW  - humanoid robots
KW  - legged locomotion
KW  - pressure control
KW  - robot dynamics
KW  - valves
KW  - hydraulic direct-drive system
KW  - biped humanoid robot
KW  - hydraulic direct drive system
KW  - simple equipment configuration
KW  - single-rod cylinder
KW  - flow rate
KW  - passive flow compensation valve
KW  - valve state
KW  - flow control
KW  - pressure control
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196956
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Biped robots require massive power on each leg while walking, hopping, and running. We have developed a flow-based control system-called hydraulic direct drive system- that can achieve high output while avoiding spatial limitations. To implement the proposed system with simple equipment configuration, a pump and single-rod cylinder are connected in a closed loop. However, because compensation for flow rate is impossible in a completely closed loop, owing to the difference in the pressure receiving area caused by the rod, a passive flow compensation valve is employed. This valve has a simple structure and is easy to implement. Further, an additional sensor is required to detect the open/close state because the valve state will cause an error in flow control. Therefore, we implemented a model in the controller to predict the state of the flow compensation valve and formulated a method of switching from flow control to pressure control according to the predicted state. Experimental results indicate that the error of the joint angle is reduced to less than 1.6 degrees for walking patterns, and stable walking is realized when the system is installed in biped humanoid robots.
ER  - 


