TY  - CONF
TI  - In-Hand Object Pose Tracking via Contact Feedback and GPU-Accelerated Robotic Simulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6203
EP  - 6209
AU  - J. Liang
AU  - A. Handa
AU  - K. V. Wyk
AU  - V. Makoviychuk
AU  - O. Kroemer
AU  - D. Fox
PY  - 2020
KW  - coprocessors
KW  - graphics processing units
KW  - manipulators
KW  - object tracking
KW  - optimisation
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - robot vision
KW  - complex contact dynamics
KW  - GPU-accelerated parallel robot simulations
KW  - sample-based optimizers
KW  - contact feedback
KW  - robot-object interactions
KW  - GPU-accelerated robotic simulation
KW  - robot hand
KW  - vision-based methods
KW  - particle filters
KW  - static grasp setting
KW  - in-hand object pose tracking
KW  - manipulation
KW  - physics simulation
KW  - forward model
KW  - point cloud distance error
KW  - Pose estimation
KW  - Robot sensing systems
KW  - Physics
KW  - Heuristic algorithms
KW  - Cost function
DO  - 10.1109/ICRA40945.2020.9197117
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tracking the pose of an object while it is being held and manipulated by a robot hand is difficult for vision-based methods due to significant occlusions. Prior works have explored using contact feedback and particle filters to localize in-hand objects. However, they have mostly focused on the static grasp setting and not when the object is in motion, as doing so requires modeling of complex contact dynamics. In this work, we propose using GPU-accelerated parallel robot simulations and derivative-free, sample-based optimizers to track in-hand object poses with contact feedback during manipulation. We use physics simulation as the forward model for robot-object interactions, and the algorithm jointly optimizes for the state and the parameters of the simulations, so they better match with those of the real world. Our method runs in real-time (30Hz) on a single GPU, and it achieves an average point cloud distance error of 6mm in simulation experiments and 13mm in the real-world ones.
ER  - 

TY  - CONF
TI  - Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6210
EP  - 6217
AU  - B. Wen
AU  - C. Mitash
AU  - S. Soorian
AU  - A. Kimmel
AU  - A. Sintov
AU  - K. E. Bekris
PY  - 2020
KW  - image registration
KW  - pose estimation
KW  - occlusion-aware pose estimation
KW  - adaptive hands
KW  - manipulation tasks
KW  - within-hand manipulation
KW  - robot hand
KW  - depth-based framework
KW  - robust pose estimation
KW  - adaptive hand
KW  - efficient parallel search
KW  - point cloud
KW  - robust global registration
KW  - object types
KW  - object pose hypotheses
KW  - short response times
KW  - in-hand 6D object pose estimation
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Robot sensing systems
KW  - Robustness
KW  - Computational modeling
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9197350
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many manipulation tasks, such as placement or within-hand manipulation, require the object's pose relative to a robot hand. The task is difficult when the hand significantly occludes the object. It is especially hard for adaptive hands, for which it is not easy to detect the finger's configuration. In addition, RGB-only approaches face issues with texture-less objects or when the hand and the object look similar. This paper presents a depth-based framework, which aims for robust pose estimation and short response times. The approach detects the adaptive hand's state via efficient parallel search given the highest overlap between the hand's model and the point cloud. The hand's point cloud is pruned and robust global registration is performed to generate object pose hypotheses, which are clustered. False hypotheses are pruned via physical reasoning. The remaining poses' quality is evaluated given agreement with observed data. Extensive evaluation on synthetic and real data demonstrates the accuracy and computational efficiency of the framework when applied on challenging, highly-occluded scenarios for different object types. An ablation study identifies how the framework's components help in performance. This work also provides a dataset for in-hand 6D object pose estimation. Code and dataset are available at: https://github.com/wenbowen123/icra20-hand-object-pose.
ER  - 

TY  - CONF
TI  - Robust 6D Object Pose Estimation by Learning RGB-D Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6218
EP  - 6224
AU  - M. Tian
AU  - L. Pan
AU  - M. H. Ang
AU  - G. Hee Lee
PY  - 2020
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object detection
KW  - optimisation
KW  - pose estimation
KW  - regression analysis
KW  - robot vision
KW  - video signal processing
KW  - RGB-D features
KW  - robotic manipulation
KW  - local optimization approach
KW  - distance between closest point pairs
KW  - rotation ambiguity
KW  - symmetric objects
KW  - rotation regression
KW  - local-optimum problem
KW  - object location
KW  - point-wise vectors
KW  - robust 6D object pose estimation
KW  - discrete-continuous formulation
KW  - LINEMOD
KW  - YCB-Video
KW  - Feature extraction
KW  - Pose estimation
KW  - Three-dimensional displays
KW  - Robustness
KW  - Uncertainty
KW  - Image segmentation
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197555
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate 6D object pose estimation is fundamental to robotic manipulation and grasping. Previous methods follow a local optimization approach which minimizes the distance between closest point pairs to handle the rotation ambiguity of symmetric objects. In this work, we propose a novel discrete- continuous formulation for rotation regression to resolve this local-optimum problem. We uniformly sample rotation anchors in SO(3), and predict a constrained deviation from each anchor to the target, as well as uncertainty scores for selecting the best prediction. Additionally, the object location is detected by aggregating point-wise vectors pointing to the 3D center. Experiments on two benchmarks: LINEMOD and YCB-Video, show that the proposed method outperforms state-of-the-art approaches. Our code is available at https://github.com/mentian/object-posenet.
ER  - 

TY  - CONF
TI  - Split Deep Q-Learning for Robust Object Singulation*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6225
EP  - 6231
AU  - I. Sarantopoulos
AU  - M. Kiatos
AU  - Z. Doulgeri
AU  - S. Malassiotis
PY  - 2020
KW  - collision avoidance
KW  - grippers
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - policy learning
KW  - split deep Q-learning
KW  - robust object singulation
KW  - robotic manipulation
KW  - robotic applications
KW  - grasping techniques
KW  - pushing policy
KW  - lateral pushing movements
KW  - reinforcement learning
KW  - optimal push policies
KW  - split DQN
KW  - target object extraction
KW  - Grasping
KW  - Task analysis
KW  - Clutter
KW  - Image segmentation
KW  - Robustness
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196647
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Extracting a known target object from a pile of other objects in a cluttered environment is a challenging robotic manipulation task encountered in many robotic applications. In such conditions, the target object touches or is covered by adjacent obstacle objects, thus rendering traditional grasping techniques ineffective. In this paper, we propose a pushing policy aiming at singulating the target object from its surrounding clutter, by means of lateral pushing movements of both the neighboring objects and the target object until sufficient 'grasping room' has been achieved. To achieve the above goal we employ reinforcement learning and particularly Deep Qlearning (DQN) to learn optimal push policies by trial and error. A novel Split DQN is proposed to improve the learning rate and increase the modularity of the algorithm. Experiments show that although learning is performed in a simulated environment the transfer of learned policies to a real environment is effective thanks to robust feature selection. Finally, we demonstrate that the modularity of the algorithm allows the addition of extra primitives without retraining the model from scratch.
ER  - 

TY  - CONF
TI  - 6-DOF Grasping for Target-driven Object Manipulation in Clutter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6232
EP  - 6238
AU  - A. Murali
AU  - A. Mousavian
AU  - C. Eppner
AU  - C. Paxton
AU  - D. Fox
PY  - 2020
KW  - manipulators
KW  - object detection
KW  - robot vision
KW  - 6DOF grasping
KW  - cluttered environments
KW  - manipulator
KW  - robotic platform
KW  - target-driven object manipulation
KW  - point cloud observations
KW  - collision checking module
KW  - grasp sequence
KW  - Clutter
KW  - Three-dimensional displays
KW  - Grasping
KW  - Grippers
KW  - Robots
KW  - Collision avoidance
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9197318
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Grasping in cluttered environments is a fundamental but challenging robotic skill. It requires both reasoning about unseen object parts and potential collisions with the manipulator. Most existing data-driven approaches avoid this problem by limiting themselves to top-down planar grasps which is insufficient for many real-world scenarios and greatly limits possible grasps. We present a method that plans 6-DOF grasps for any desired object in a cluttered scene from partial point cloud observations. Our method achieves a grasp success of 80.3%, outperforming baseline approaches by 17.6% and clearing 9 cluttered table scenes (which contain 23 unknown objects and 51 picks in total) on a real robotic platform. By using our learned collision checking module, we can even reason about effective grasp sequences to retrieve objects that are not immediately accessible. Supplementary video can be found here.
ER  - 

TY  - CONF
TI  - Single Shot 6D Object Pose Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6239
EP  - 6245
AU  - K. Kleeberger
AU  - M. F. Huber
PY  - 2020
KW  - convolutional neural nets
KW  - object detection
KW  - pose estimation
KW  - regression analysis
KW  - stereo image processing
KW  - rigid objects
KW  - depth images
KW  - convolutional neural network
KW  - 3D input data
KW  - volume elements
KW  - optimized end-to-end
KW  - multiple objects
KW  - single shot 6D object pose estimation
KW  - single shot approach
KW  - object pose network
KW  - regression task
KW  - GPU
KW  - synthetic data
KW  - public benchmark datasets
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Data models
KW  - Task analysis
KW  - Pose estimation
KW  - Image segmentation
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197207
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we introduce a novel single shot approach for 6D object pose estimation of rigid objects based on depth images. For this purpose, a fully convolutional neural network is employed, where the 3D input data is spatially discretized and pose estimation is considered as a regression task that is solved locally on the resulting volume elements. With 65 fps on a GPU, our Object Pose Network (OP-Net) is extremely fast, is optimized end-to-end, and estimates the 6D pose of multiple objects in the image simultaneously. Our approach does not require manually 6D pose-annotated real-world datasets and transfers to the real world, although being entirely trained on synthetic data. The proposed method is evaluated on public benchmark datasets, where we can demonstrate that state-of-the-art methods are significantly outperformed.
ER  - 

TY  - CONF
TI  - MulRan: Multimodal Range Dataset for Urban Place Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6246
EP  - 6253
AU  - G. Kim
AU  - Y. S. Park
AU  - Y. Cho
AU  - J. Jeong
AU  - A. Kim
PY  - 2020
KW  - geophysical image processing
KW  - geophysical techniques
KW  - image recognition
KW  - mobile robots
KW  - object recognition
KW  - optical radar
KW  - radar imaging
KW  - robot vision
KW  - multimodal range dataset
KW  - radio detection and ranging
KW  - light detection and ranging
KW  - urban environment
KW  - range sensor-based place recognition
KW  - 6D baseline trajectories
KW  - place recognition ground truth
KW  - image-format data
KW  - time-stamped 1D intensity arrays
KW  - polar images
KW  - image data
KW  - radar place recognition method
KW  - LiDAR
KW  - longer-range measurements
KW  - urban place recognition
KW  - MulRan
KW  - Laser radar
KW  - Radar imaging
KW  - Three-dimensional displays
KW  - Urban areas
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9197298
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a multimodal range dataset namely for radio detection and ranging (radar) and light detection and ranging (LiDAR) specifically targeting the urban environment. By extending our workshop paper [1] to a larger scale, this dataset focuses on the range sensor-based place recognition and provides 6D baseline trajectories of a vehicle for place recognition ground truth. Provided radar data support both raw-level and image-format data, including a set of time-stamped 1D intensity arrays and 360° polar images, respectively. In doing so, we provide flexibility between raw data and image data depending on the purpose of the research. Unlike existing datasets, our focus is at capturing both temporal and structural diversities for range-based place recognition research. For evaluation, we applied and validated that our previous location descriptor and its search algorithm [2] are highly effective for radar place recognition method. Furthermore, the result shows that radar-based place recognition outperforms LiDAR-based one exploiting its longer-range measurements. The dataset is available from https://sites.google.com/view/mulran-pr.
ER  - 

TY  - CONF
TI  - GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6254
EP  - 6260
AU  - S. Du
AU  - H. Guo
AU  - Y. Chen
AU  - Y. Lin
AU  - X. Meng
AU  - L. Wen
AU  - F. -Y. Wang
PY  - 2020
KW  - cameras
KW  - optimisation
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - GPO
KW  - global plane optimization
KW  - homography estimation
KW  - homography decomposition
KW  - monocular SLAM initialization
KW  - monocular simultaneous localization and mapping problem
KW  - camera poses
KW  - chessboard dataset
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Optimization
KW  - Matrix decomposition
KW  - Transmission line matrix methods
KW  - Estimation
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196970
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Initialization is essential to monocular Simultaneous Localization and Mapping (SLAM) problems. This paper focuses on a novel initialization method for monocular SLAM based on planar features. The algorithm starts by homography estimation in a sliding window. It then proceeds to a global plane optimization (GPO) to obtain camera poses and the plane normal. 3D points can be recovered using planar constraints without triangulation. The proposed method fully exploits the plane information from multiple frames and avoids the ambiguities in homography decomposition. We validate our algorithm on the collected chessboard dataset against baseline implementations and present extensive analysis. Experimental results show that our method outperforms the ne-tuned baselines in both accuracy and real-time.
ER  - 

TY  - CONF
TI  - Large-Scale Volumetric Scene Reconstruction using LiDAR
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6261
EP  - 6267
AU  - T. Kühner
AU  - J. Kümmerle
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image reconstruction
KW  - image representation
KW  - optical radar
KW  - large-scale 3D scene reconstruction
KW  - autonomous driving
KW  - volumetric depth fusion
KW  - indoor applications
KW  - commodity RGB-D cameras
KW  - high reconstruction quality
KW  - LiDAR sensors
KW  - autonomous cars
KW  - large-scale mapping
KW  - urban area
KW  - meshed representation
KW  - real world application
KW  - large-scale volumetric scene reconstruction
KW  - distance 3.7 km
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Image reconstruction
KW  - Graphics processing units
KW  - Sensor fusion
KW  - Weight measurement
DO  - 10.1109/ICRA40945.2020.9197388
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Large-scale 3D scene reconstruction is an important task in autonomous driving and other robotics applications as having an accurate representation of the environment is necessary to safely interact with it. Reconstructions are used for numerous tasks ranging from localization and mapping to planning. In robotics, volumetric depth fusion is the method of choice for indoor applications since the emergence of commodity RGB-D cameras due to its robustness and high reconstruction quality. In this work we present an approach for volumetric depth fusion using LiDAR sensors as they are common on most autonomous cars. We present a framework for large-scale mapping of urban areas considering loop closures. Our method creates a meshed representation of an urban area from recordings over a distance of 3.7km with a high level of detail on consumer graphics hardware in several minutes. The whole process is fully automated and does not need any user interference. We quantitatively evaluate our results from a real world application. Also, we investigate the effects of the sensor model that we assume on reconstruction quality by using synthetic data.
ER  - 

TY  - CONF
TI  - Topological Mapping for Manhattan-like Repetitive Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6268
EP  - 6274
AU  - S. S. Puligilla
AU  - S. Tourani
AU  - T. Vaidya
AU  - U. S. Parihar
AU  - R. Kiran Sarvadevabhatla
AU  - K. M. Krishna
PY  - 2020
KW  - convolutional neural nets
KW  - graph theory
KW  - image representation
KW  - optimisation
KW  - SLAM (robots)
KW  - topology
KW  - Manhattan properties
KW  - topological graph
KW  - unoptimized Pose Graph
KW  - topological Manhattan relations
KW  - ground-truth Pose Graph
KW  - real-world indoor warehouse scenes
KW  - Manhattan-like repetitive environments
KW  - topological mapping framework
KW  - neighbouring nodes
KW  - indoor warehouse setting
KW  - warehouse topological construct
KW  - deep convolutional network
KW  - Siamese-style neural network
KW  - backend pose graph optimization framework
KW  - Manhattan graph aided loop closure relations
KW  - Topology
KW  - Network topology
KW  - Simultaneous localization and mapping
KW  - Neural networks
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197520
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework.
ER  - 

TY  - CONF
TI  - Robust RGB-D Camera Tracking using Optimal Key-frame Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6275
EP  - 6281
AU  - K. M. Han
AU  - Y. J. Kim
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image motion analysis
KW  - image reconstruction
KW  - image sequences
KW  - integer programming
KW  - interpolation
KW  - iterative methods
KW  - motion estimation
KW  - SLAM (robots)
KW  - optimal key-frame selection
KW  - integer programming
KW  - VO method
KW  - camera motion
KW  - elastic-fusion
KW  - discontinuous camera motions
KW  - robust RGB-D camera tracking
KW  - adaptive visual odometry
KW  - TUM benchmark sequences
KW  - camera trajectory errors
KW  - iterative closed point
KW  - Cameras
KW  - Robustness
KW  - Optimization
KW  - Tracking
KW  - Iterative closest point algorithm
KW  - Robot vision systems
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197021
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel RGB-D camera tracking system that robustly reconstructs hand-held RGB-D camera sequences. The robustness of our system is achieved by two independent features of our method: adaptive visual odometry (VO) and integer programming-based key-frame selection. Our VO method adaptively interpolates the camera motion results of the direct VO (DVO) and the iterative closed point (ICP) to yield more optimal results than existing methods such as Elastic-Fusion. Moreover, our key-frame selection method locates globally optimum key-frames using a comprehensive objective function in a deterministic manner rather than heuristic or experience-based rules that prior methods mostly rely on. As a result, our method can complete reconstruction even if the camera fails to be tracked due to discontinuous camera motions, such as kidnap events, when conventional systems need to backtrack the scene. We validated our tracking system on 25 TUM benchmark sequences against state-of-the-art works, such as ORBSLAM2, Elastic-Fusion, and DVO SLAM, and experimentally showed that our method has smaller and more robust camera trajectory errors than these systems.
ER  - 

TY  - CONF
TI  - Aggressive Online Control of a Quadrotor via Deep Network Representations of Optimality Principles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6282
EP  - 6287
AU  - S. Li
AU  - E. Öztürk
AU  - C. De Wagter
AU  - G. C. H. E. de Croon
AU  - D. Izzo
PY  - 2020
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - helicopters
KW  - mobile robots
KW  - neural nets
KW  - optimisation
KW  - time optimal control
KW  - power optimality
KW  - time optimality
KW  - deep neural network
KW  - robotic applications
KW  - optimality principles
KW  - deep network representations
KW  - aggressive online control
KW  - time-optimal maneuvers
KW  - offline optimal control method
KW  - aggressive quadrotor control
KW  - Trajectory
KW  - Optimal control
KW  - Stability analysis
KW  - Neural networks
KW  - Delays
KW  - Training
KW  - Drones
DO  - 10.1109/ICRA40945.2020.9197443
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Optimal control holds great potential to improve a variety of robotic applications. The application of optimal control on-board limited platforms has been severely hindered by the large computational requirements of current state of the art implementations. In this work, we make use of a deep neural network to directly map the robot states to control actions. The network is trained offline to imitate the optimal control computed by a time consuming direct nonlinear method. A mixture of time optimality and power optimality is considered with a continuation parameter used to select the predominance of each objective. We apply our networks (termed G&CNets) to aggressive quadrotor control, first in simulation and then in the real world. We give insight into the factors that influence the `reality gap' between the quadrotor model used by the offline optimal control method and the real quadrotor. Furthermore, we explain how we set up the model and the control structure on-board of the real quadrotor to successfully close this gap and perform time-optimal maneuvers in the real world. Finally, G&CNet's performance is compared to state-of-the-art differential-flatness-based optimal control methods. We show, in the experiments, that G&CNets lead to significantly faster trajectory execution due to, in part, the less restrictive nature of the allowed state-to-input mappings.
ER  - 

TY  - CONF
TI  - Refined Analysis of Asymptotically-Optimal Kinodynamic Planning in the State-Cost Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6344
EP  - 6350
AU  - M. Kleinbort
AU  - E. Granados
AU  - K. Solovey
AU  - R. Bonalli
AU  - K. E. Bekris
AU  - D. Halperin
PY  - 2020
KW  - boundary-value problems
KW  - motion control
KW  - optimal control
KW  - piecewise constant techniques
KW  - robot dynamics
KW  - trees (mathematics)
KW  - asymptotically-optimal kinodynamic planning
KW  - state-cost space
KW  - AO-RRT
KW  - tree-based planner
KW  - motion planning
KW  - kinodynamic constraints
KW  - optimality proof
KW  - piecewise-constant control function
KW  - two-point boundary-value
KW  - Lipschitz-continuity
KW  - Trajectory
KW  - Planning
KW  - Aerospace electronics
KW  - Robots
KW  - Collision avoidance
KW  - Cost function
KW  - Space exploration
DO  - 10.1109/ICRA40945.2020.9197236
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel analysis of AO-RRT: a tree-based planner for motion planning with kinodynamic constraints, originally described by Hauser and Zhou (AO-X, 2016). AO-RRT explores the state-cost space and has been shown to efficiently obtain high-quality solutions in practice without relying on the availability of a computationally-intensive two-point boundary-value solver. Our main contribution is an optimality proof for the single-tree version of the algorithm-a variant that was not analyzed before. Our proof only requires a mild and easily-verifiable set of assumptions on the problem and system: Lipschitz-continuity of the cost function and the dynamics. In particular, we prove that for any system satisfying these assumptions, any trajectory having a piecewise-constant control function and positive clearance from the obstacles can be approximated arbitrarily well by a trajectory found by AORRT. We also discuss practical aspects of AORRT and present experimental comparisons of variants of the algorithm.
ER  - 

TY  - CONF
TI  - Robust quadcopter control with artificial vector fields*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6381
EP  - 6387
AU  - A. M. C. Rezende
AU  - V. M. Gonçalves
AU  - A. H. D. Nunes
AU  - L. C. A. Pimenta
PY  - 2020
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - helicopters
KW  - mobile robots
KW  - multi-robot systems
KW  - nonlinear control systems
KW  - path planning
KW  - position control
KW  - robust control
KW  - time-varying systems
KW  - robust quadcopter control
KW  - artificial vector fields
KW  - path tracking control strategy
KW  - control laws
KW  - vector field
KW  - controlled second order integrator
KW  - quadcopter model
KW  - input-to-state stable
KW  - control inputs
KW  - Robots
KW  - Vehicle dynamics
KW  - Robustness
KW  - Convergence
KW  - Level set
KW  - Mathematical model
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196605
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This article presents a path tracking control strategy for a quadcopter to follow a time varying curve. The control is based on artificial vector fields. The construction of the field is based on a well known technique in the literature. Next, control laws are developed to impose the behavior of the vector field to a second order integrator model. Finally, control laws are developed to impose the dynamics of the controlled second order integrator to a quadcopter model, which assumes the thrust and the angular rates as input commands. Asymptotic convergence of the whole system is proved by showing that the individual systems in cascade connection are input-to-state stable. We also analyze the influence of norm-bounded disturbances in the control inputs to evaluate the robustness of the controller. We show that bounded disturbances originate limited deviations from the target curve. Simulations and a real robot experiment exemplify and validate the developed theory.
ER  - 

TY  - CONF
TI  - Simulation-Based Reinforcement Learning for Real-World Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6411
EP  - 6418
AU  - B. Osiński
AU  - A. Jakubowski
AU  - P. Zięcina
AU  - P. Miłoś
AU  - C. Galias
AU  - S. Homoceanu
AU  - H. Michalewski
PY  - 2020
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - road vehicles
KW  - traffic engineering computing
KW  - simulation-based reinforcement learning
KW  - real-world autonomous driving
KW  - driving system
KW  - real-world vehicle
KW  - driving policy
KW  - RGB images
KW  - single camera
KW  - semantic segmentation
KW  - synthetic data
KW  - real-world data
KW  - segmentation network
KW  - real-world experiments
KW  - sim-to-real policy transfer
KW  - real-world performance
KW  - Training
KW  - Visualization
KW  - Learning (artificial intelligence)
KW  - Semantics
KW  - Robots
KW  - Image segmentation
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9196730
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.
ER  - 

TY  - CONF
TI  - Driving Style Encoder: Situational Reward Adaptation for General-Purpose Planning in Automated Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6419
EP  - 6425
AU  - S. Rosbach
AU  - V. James
AU  - S. Großjohann
AU  - S. Homoceanu
AU  - X. Li
AU  - S. Roth
PY  - 2020
KW  - learning (artificial intelligence)
KW  - path planning
KW  - predictive control
KW  - road traffic control
KW  - situational reward adaptation
KW  - general-purpose planning algorithms
KW  - automated driving
KW  - planning algorithm
KW  - driving kinematics
KW  - linear reward function
KW  - driving situation
KW  - deep learning approach
KW  - situation-dependent reward functions
KW  - sampled driving policies
KW  - driving style
KW  - planning cycle
KW  - Planning
KW  - Neural networks
KW  - Entropy
KW  - Kinematics
KW  - Machine learning
KW  - Tuning
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196778
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - General-purpose planning algorithms for automated driving combine mission, behavior, and local motion planning. Such planning algorithms map features of the environment and driving kinematics into complex reward functions. To achieve this, planning experts often rely on linear reward functions. The specification and tuning of these reward functions is a tedious process and requires significant experience. Moreover, a manually designed linear reward function does not generalize across different driving situations. In this work, we propose a deep learning approach based on inverse reinforcement learning that generates situation-dependent reward functions. Our neural network provides a mapping between features and actions of sampled driving policies of a model-predictive control-based planner and predicts reward functions for upcoming planning cycles. In our evaluation, we compare the driving style of reward functions predicted by our deep network against clustered and linear reward functions. Our proposed deep learning approach outperforms clustered linear reward functions and is at par with linear reward functions with a-priori knowledge about the situation.
ER  - 

TY  - CONF
TI  - Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6426
EP  - 6432
AU  - S. K. Jayaraman
AU  - D. M. Tilbury
AU  - X. Jessie Yang
AU  - A. K. Pradhan
AU  - L. P. Robert
PY  - 2020
KW  - behavioural sciences computing
KW  - human-robot interaction
KW  - navigation
KW  - path planning
KW  - pedestrians
KW  - road traffic control
KW  - road vehicles
KW  - traffic engineering computing
KW  - virtual reality
KW  - automated vehicle interactions
KW  - safe navigation
KW  - automated vehicles
KW  - pedestrian interactions
KW  - human-driven vehicles
KW  - HDV
KW  - hybrid systems model
KW  - constant velocity dynamics
KW  - long-term pedestrian trajectory prediction
KW  - immersive virtual environment
KW  - AV interactions
KW  - pedestrian crosswalk behavior analysis
KW  - pedestrian crosswalk behavior prediction
KW  - gap acceptance behavior
KW  - AV motion planning
KW  - IVE
KW  - Predictive models
KW  - Trajectory
KW  - Legged locomotion
KW  - Vehicle dynamics
KW  - Virtual environments
KW  - Planning
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197347
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For safe navigation around pedestrians, automated vehicles (AVs) need to plan their motion by accurately predicting pedestrians' trajectories over long time horizons. Current approaches to AV motion planning around crosswalks predict only for short time horizons (1-2 s) and are based on data from pedestrian interactions with human-driven vehicles (HDVs). In this paper, we develop a hybrid systems model that uses pedestrians' gap acceptance behavior and constant velocity dynamics for long-term pedestrian trajectory prediction when interacting with AVs. Results demonstrate the applicability of the model for long-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further, we compared measures of pedestrian crossing behaviors in the immersive virtual environment (when interacting with AVs) to that in the real world (results of published studies of pedestrians interacting with HDVs), and found similarities between the two. These similarities demonstrate the applicability of the hybrid model of AV interactions developed from an immersive virtual environment (IVE) for real-world scenarios for both AVs and HDVs.
ER  - 

TY  - CONF
TI  - The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6433
EP  - 6438
AU  - D. Barnes
AU  - M. Gadd
AU  - P. Murcutt
AU  - P. Newman
AU  - I. Posner
PY  - 2020
KW  - CW radar
KW  - distance measurement
KW  - FM radar
KW  - Global Positioning System
KW  - millimetre wave radar
KW  - optical radar
KW  - road vehicle radar
KW  - Oxford Radar RobotCar dataset
KW  - radar extension
KW  - millimetre-wave FMCW scanning radar data
KW  - central Oxford route
KW  - truth optimised radar odometry
KW  - autonomous vehicles
KW  - environmental conditions
KW  - sensor modalities
KW  - AD 2019 01
KW  - urban driving
KW  - weather condition
KW  - traffic condition
KW  - lighting condition
KW  - Navtech CTS350-X radar
KW  - Velodyne HDL-32E 3D LIDAR
KW  - GPS-INS receiver
KW  - ori.ox.ac.uk/datasets/radar-robotear-dataset
KW  - size 280.0 km
KW  - memory size 4.7 TByte
KW  - Robot sensing systems
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Azimuth
KW  - Calibration
DO  - 10.1109/ICRA40945.2020.9196884
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we present The Oxford Radar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.(/P)(P)The data were gathered in January 2019 over thirty-two traversals of a central Oxford route spanning a total of 280 km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7 TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotear-dataset.
ER  - 

TY  - CONF
TI  - Multi-modal Experts Network for Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6439
EP  - 6445
AU  - S. Fang
AU  - A. Choromanska
PY  - 2020
KW  - computational complexity
KW  - control engineering computing
KW  - expert systems
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - road vehicles
KW  - sensory data
KW  - autonomous driving
KW  - autonomous vehicles
KW  - computational complexity
KW  - multistage training procedure
KW  - end-to-end learning
KW  - multimodal experts network architecture
KW  - inference time step
KW  - mixed discrete-continuous policy
KW  - Laser radar
KW  - Feature extraction
KW  - Cameras
KW  - Training
KW  - Autonomous vehicles
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197459
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - End-to-end learning from sensory data has shown promising results in autonomous driving. While employing many sensors enhances world perception and should lead to more robust and reliable behavior of autonomous vehicles, it is challenging to train and deploy such network and at least two problems are encountered in the considered setting. The first one is the increase of computational complexity with the number of sensing devices. The other is the phenomena of network overfitting to the simplest and most informative input. We address both challenges with a novel, carefully tailored multi-modal experts network architecture and propose a multi-stage training procedure. The network contains a gating mechanism, which selects the most relevant input at each inference time step using a mixed discrete-continuous policy. We demonstrate the plausibility of the proposed approach on our 1/6 scale truck equipped with three cameras and one LiDAR.
ER  - 

TY  - CONF
TI  - Localising PMDs through CNN Based Perception of Urban Streets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6454
EP  - 6460
AU  - M. Jayasuriya
AU  - J. Arukgoda
AU  - R. Ranasinghe
AU  - G. Dissanayake
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - Kalman filters
KW  - learning (artificial intelligence)
KW  - nonlinear filters
KW  - object detection
KW  - robot vision
KW  - common environmental landmarks
KW  - point features
KW  - higher level information
KW  - common vision based approaches
KW  - low level hand
KW  - EKF framework
KW  - practical CNN
KW  - typical suburban streets
KW  - localiser
KW  - PMD
KW  - CNN based perception
KW  - urban streets
KW  - localisation scheme
KW  - complementary approaches
KW  - outdoor vision based localisation
KW  - convolutional neural networks
KW  - necessary perceptual information
KW  - camera images
KW  - lane markings
KW  - manhole covers
KW  - vector distance
KW  - binary image
KW  - ground surface boundaries
KW  - CNN based detection
KW  - novel extended Kalman filter
KW  - Feature extraction
KW  - Transforms
KW  - Cameras
KW  - Semantics
KW  - Two dimensional displays
KW  - Data mining
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196639
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The main contribution of this paper is a novel Extended Kalman Filter (EKF) based localisation scheme that fuses two complementary approaches to outdoor vision based localisation. This EKF is aided by a front end consisting of two Convolutional Neural Networks (CNNs) that provide the necessary perceptual information from camera images. The first approach involves a CNN based extraction of information corresponding to artefacts such as curbs, lane markings, and manhole covers to localise on a vector distance transform representation of a binary image of these ground surface boundaries. The second approach involves a CNN based detection of common environmental landmarks such as tree trunks and light poles, which are represented as point features on a sparse map. Utilising CNNs to obtain higher level information about the environment enables this framework to avoid the typical pitfalls of common vision based approaches that use low level hand crafted features for localisation. The EKF framework makes it possible to deal with false positives and missed detections that are inevitable in a practical CNN, to produce a location estimate together with its associated uncertainty. Experiments using a Personal Mobility Device (PMD) driven in typical suburban streets are presented to demonstrate the effectiveness of the proposed localiser.
ER  - 

TY  - CONF
TI  - Hybrid Localization using Model- and Learning-Based Methods: Fusion of Monte Carlo and E2E Localizations via Importance Sampling
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6469
EP  - 6475
AU  - N. Akai
AU  - T. Hirayama
AU  - H. Murase
PY  - 2020
KW  - convolutional neural nets
KW  - importance sampling
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - Monte Carlo methods
KW  - neurocontrollers
KW  - particle filtering (numerical methods)
KW  - path planning
KW  - motion model
KW  - importance sampling
KW  - convolutional neural network
KW  - CNN
KW  - Monte Carlo localization
KW  - particle filter
KW  - hybrid localization method
KW  - learning-based method
KW  - model-based method
KW  - E2E localization
KW  - MCL
KW  - CNN predictions
KW  - posterior distributions
KW  - Atmospheric measurements
KW  - Particle measurements
KW  - Proposals
KW  - Predictive models
KW  - Fuses
KW  - Learning systems
KW  - Monte Carlo methods
DO  - 10.1109/ICRA40945.2020.9196568
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a hybrid localization method that fuses Monte Carlo localization (MCL) and convolutional neural network (CNN)-based end-to-end (E2E) localization. MCL is based on particle filter and requires proposal distributions to sample the particles. The proposal distribution is generally predicted using a motion model. However, because the motion model cannot handle unanticipated errors, the predicted distribution is sometimes inaccurate. The use of other ideal proposal distributions, such as the measurement model, can improve robustness against such unanticipated errors. This technique is called importance sampling (IS). However, it is difficult to sample the particles from such ideal distributions because they are not represented in the closed form. Recent works have proved that CNNs with dropout layers represent the posterior distributions over their outputs conditioned on the inputs and the CNN predictions are equivalent to sampling the outputs from the posterior. Therefore, the proposed method utilizes a CNN to sample the particles and fuses them with MCL via IS. Consequently, the advantages of both MCL and E2E localization can be simultaneously leveraged while preventing their disadvantages. Experiments demonstrate that the proposed method can smoothly estimate the robot pose, similar to the model-based method, and quickly re-localize it from the failures, similar to the learning-based method.
ER  - 

TY  - CONF
TI  - Visual Localization with Google Earth Images for Robust Global Pose Estimation of UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6491
EP  - 6497
AU  - B. Patel
AU  - T. D. Barfoot
AU  - A. P. Schoellig
PY  - 2020
KW  - autonomous aerial vehicles
KW  - distance measurement
KW  - Global Positioning System
KW  - image filtering
KW  - image registration
KW  - image sensors
KW  - mobile robots
KW  - multi-robot systems
KW  - pose estimation
KW  - rendering (computer graphics)
KW  - robot vision
KW  - Google Earth images
KW  - georeferenced rendered images
KW  - dense mutual information technique
KW  - outdoor GPS-denied environments
KW  - image registrations
KW  - gimballed visual odometry pipeline
KW  - visual localization
KW  - robust global pose estimation
KW  - multirotor UAV
KW  - typical feature-based localizer
KW  - Cameras
KW  - Image registration
KW  - Three-dimensional displays
KW  - Visualization
KW  - Earth
KW  - Robustness
KW  - Google
DO  - 10.1109/ICRA40945.2020.9196606
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We estimate the global pose of a multirotor UAV by visually localizing images captured during a flight with Google Earth images pre-rendered from known poses. We metrically localize real images with georeferenced rendered images using a dense mutual information technique to allow accurate global pose estimation in outdoor GPS-denied environments. We show the ability to consistently localize throughout a sunny summer day despite major lighting changes while demonstrating that a typical feature-based localizer struggles under the same conditions. Successful image registrations are used as measurements in a filtering framework to apply corrections to the pose estimated by a gimballed visual odometry pipeline. We achieve less than 1 m and 1° RMSE on a 303 m flight and less than 3 m and 3° RMSE on six 1132 m flights as low as 36 m above ground level conducted at different times of the day from sunrise to sunset.
ER  - 

TY  - CONF
TI  - Adaptive Curriculum Generation from Demonstrations for Sim-to-Real Visuomotor Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6498
EP  - 6505
AU  - L. Hermann
AU  - M. Argus
AU  - A. Eitel
AU  - A. Amiranashvili
AU  - W. Burgard
AU  - T. Brox
PY  - 2020
KW  - computer vision
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - shaped reward functions
KW  - ACGD
KW  - policy transfer
KW  - real-world manipulation tasks
KW  - sim-to-real visuomotor control
KW  - reinforcement learning
KW  - adaptive curriculum generation
KW  - vision-based control policies
KW  - Task analysis
KW  - Training
KW  - Robots
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Adaptation models
KW  - Stacking
DO  - 10.1109/ICRA40945.2020.9197108
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose Adaptive Curriculum Generation from Demonstrations (ACGD) for reinforcement learning in the presence of sparse rewards. Rather than designing shaped reward functions, ACGD adaptively sets the appropriate task difficulty for the learner by controlling where to sample from the demonstration trajectories and which set of simulation parameters to use. We show that training vision-based control policies in simulation while gradually increasing the difficulty of the task via ACGD improves the policy transfer to the real world. The degree of domain randomization is also gradually increased through the task difficulty. We demonstrate zero-shot transfer for two real-world manipulation tasks: pick-and-stow and block stacking. A video showing the results can be found at https://lmb.informatik.uni-freiburg.de/projects/curriculum/.
ER  - 

TY  - CONF
TI  - Accept Synthetic Objects as Real: End-to-End Training of Attentive Deep Visuomotor Policies for Manipulation in Clutter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6506
EP  - 6512
AU  - P. Abolghasemi
AU  - L. Bölöni
PY  - 2020
KW  - clutter
KW  - computer vision
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - data augmentation procedure
KW  - network architectures
KW  - implicit attention ASOR-IA
KW  - explicit attention ASOR-EA
KW  - training data
KW  - uncluttered environment
KW  - cluttered environments
KW  - end-to-end training
KW  - attentive deep visuomotor policies
KW  - end-to-end train multitask deep visuomotor policies
KW  - robotic manipulation
KW  - reinforcement learning
KW  - end-to-end LfD architectures
KW  - Accept Synthetic Objects as Real
KW  - Clutter
KW  - Robots
KW  - Training data
KW  - Task analysis
KW  - Training
KW  - Encoding
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197552
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recent research demonstrated that it is feasible to end-to-end train multi-task deep visuomotor policies for robotic manipulation using variations of learning from demonstration (LfD) and reinforcement learning (RL). In this paper, we extend the capabilities of end-to-end LfD architectures to object manipulation in clutter. We start by introducing a data augmentation procedure called Accept Synthetic Objects as Real (ASOR). Using ASOR we develop two network architectures: implicit attention ASOR-IA and explicit attention ASOR-EA. Both architectures use the same training data (demonstrations in uncluttered environments) as previous approaches. Experimental results show that ASOR-IA and ASOR-EA succeed in a significant fraction of trials in cluttered environments where previous approaches never succeed. In addition, we find that both ASOR-IA and ASOR-EA outperform previous approaches even in uncluttered environments, with ASOR-EA performing better even in clutter compared to the previous best baseline in an uncluttered environment.
ER  - 

TY  - CONF
TI  - Learning of Exception Strategies in Assembly Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6521
EP  - 6527
AU  - B. Nemec
AU  - M. Simonič
AU  - A. Ude
PY  - 2020
KW  - humanoid robots
KW  - mobile robots
KW  - position control
KW  - principal component analysis
KW  - robotic assembly
KW  - humanoid robots
KW  - LfD framework
KW  - exception strategies
KW  - peg-in-hole task
KW  - Franka-Emika Panda robot
KW  - assembly tasks
KW  - assembly policy
KW  - Robot sensing systems
KW  - Task analysis
KW  - Trajectory
KW  - Robot kinematics
KW  - Databases
KW  - Statistical learning
DO  - 10.1109/ICRA40945.2020.9197480
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Assembly tasks performed with a robot often fail due to unforeseen situations, regardless of the fact that we carefully learned and optimized the assembly policy. This problem is even more present in humanoid robots acting in an unstructured environment where it is not possible to anticipate all factors that might lead to the failure of the given task. In this work, we propose a concurrent LfD framework, which associates demonstrated exception strategies to the given context. Whenever a failure occurs, the proposed algorithm generalizes past experience regarding the current context and generates an appropriate policy that solves the assembly issue. For this purpose, we applied PCA on force/torque data, which generates low dimensional descriptor of the current context. The proposed framework was validated in a peg-in-hole (PiH) task using Franka-Emika Panda robot.
ER  - 

TY  - CONF
TI  - An Open-Source Framework for Rapid Development of Interactive Soft-Body Simulations for Real-Time Training
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6544
EP  - 6550
AU  - A. Munawar
AU  - N. Srishankar
AU  - G. S. Fischer
PY  - 2020
KW  - control engineering computing
KW  - force feedback
KW  - haptic interfaces
KW  - manipulators
KW  - medical computing
KW  - medical robotics
KW  - surgery
KW  - telerobotics
KW  - virtual reality
KW  - real-time simulation
KW  - interactive manipulation
KW  - human-readable front-end interface
KW  - commercially available haptic devices
KW  - game controllers
KW  - da Vinci Research Kit
KW  - real-time haptic feedback
KW  - multiuser training
KW  - manipulation problems
KW  - soft-body manipulation
KW  - open-source framework
KW  - interactive soft-body simulations
KW  - real-time training
KW  - master telemanipulators
KW  - Visualization
KW  - Computational modeling
KW  - Real-time systems
KW  - Training
KW  - Robots
KW  - Faces
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197573
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an open-source framework that provides a low barrier to entry for real-time simulation, visualization, and interactive manipulation of user-specifiable soft-bodies, environments, and robots (using a human-readable front-end interface). The simulated soft-bodies can be interacted by a variety of input interface devices including commercially available haptic devices, game controllers, and the Master Tele-Manipulators (MTMs) of the da Vinci Research Kit (dVRK) with real-time haptic feedback. We propose this framework for carrying out multi-user training, user-studies, and improving the control strategies for manipulation problems. In this paper, we present the associated challenges to the development of such a framework and our proposed solutions. We also demonstrate the performance of this framework with examples of soft-body manipulation and interaction with various input devices.
ER  - 

TY  - CONF
TI  - Towards 5-DoF Control of an Untethered Magnetic Millirobot via MRI Gradient Coils
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6551
EP  - 6557
AU  - O. Erin
AU  - D. Antonelli
AU  - M. E. Tiryaki
AU  - M. Sitti
PY  - 2020
KW  - biomedical MRI
KW  - medical image processing
KW  - medical robotics
KW  - microrobots
KW  - path planning
KW  - surgery
KW  - untethered magnetic millirobot
KW  - MRI gradient coils
KW  - electromagnetic field gradients
KW  - magnetic resonance imaging devices
KW  - power untethered magnetic robots
KW  - MRI devices
KW  - magnetic pulling forces
KW  - drug delivery
KW  - MRI-powered untethered magnetic robots
KW  - orientation control
KW  - three-dimensional fluids
KW  - 3-DoF position control
KW  - path-planning-based 5-DoF control algorithm
KW  - optimal controller
KW  - robot manufacturing errors
KW  - pitch angle
KW  - neutral pitching angle
KW  - 3D Bezier curves
KW  - worst-case path-tracking error
KW  - position-tracking error
KW  - orientation-tracking error
KW  - pitch angles
KW  - future MRI-powered active imaging
KW  - laser surgery
KW  - biopsy robots
KW  - Magnetic resonance imaging
KW  - Robots
KW  - Magnetic devices
KW  - Coils
KW  - Three-dimensional displays
KW  - Force
KW  - Medical robotics
KW  - miniature robots
KW  - magnetic actuation
KW  - magnetic resonance imaging
KW  - optimal control
DO  - 10.1109/ICRA40945.2020.9196692
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Electromagnetic field gradients generated by magnetic resonance imaging (MRI) devices pave the way to power untethered magnetic robots remotely. This innovative use of MRI devices allows exerting magnetic pulling forces on untethered magnetic robots, which could be used for navigation, diagnosis, drug delivery and therapeutic procedures inside a human body. So far, MRI-powered untethered magnetic robots lack simultaneous position and orientation control inside three-dimensional (3D) fluids, and therefore, their control has been limited to 3-DoF position control. In this paper, we present a path-planning-based 5-DoF control algorithm to steer and control an MRI-powered untethered robot's position and orientation simultaneously in 3D workspaces in fluids. Eventhough the simulation results show that the proposed optimal controller can successfully control the robot for 5-DoF, in the experiments, we observe a reduced 5-DoF controllability due to the robot manufacturing errors, which result in pitch angle to remain at around the neutral pitching angle at the steady state. The proposed controller was evaluated to track four different paths (linear, planar-horizontal, planar-vertical and 3D paths) generated by 3D Bezier curves. The worst-case path-tracking error was observed for 3D path-following experiments. For this case, the position-tracking error was 2.7±1.8 mm, and the orientation-tracking error was 13.5± 28.7 and 3.7± 10.2 degrees for yaw and pitch angles, respectively. The overall path is completed within 19.6 seconds with 23.6 mm overall displacement and 61.2 and 41.2 degrees of yaw and pitch angle rotation, respectively. Such robots can be used in future MRI-powered active imaging, laser surgery and biopsy robots inside a fluid-filled stomach type of organs.
ER  - 

TY  - CONF
TI  - Balance of Humanoid Robots in a Mix of Fixed and Sliding Multi-Contact Scenarios
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6590
EP  - 6596
AU  - S. Samadi
AU  - S. Caron
AU  - A. Tanguy
AU  - A. Kheddar
PY  - 2020
KW  - approximation theory
KW  - humanoid robots
KW  - legged locomotion
KW  - quadratic programming
KW  - humanoid robots
KW  - multilegged robots
KW  - multicontact setting
KW  - desired sliding-task motions
KW  - center-of-mass
KW  - admissible convex area
KW  - contact positions
KW  - CoM support area
KW  - CSA
KW  - appropriate CoM position
KW  - multiple fixed sliding contacts
KW  - HRP-4 humanoid robot
KW  - quadratic programming
KW  - QP optimization problems
KW  - Humanoid robots
KW  - Mathematical model
KW  - Friction
KW  - Task analysis
KW  - Gravity
KW  - Humanoid and multi-legged robots
KW  - balance
KW  - multi-contacts
KW  - sliding contacts
DO  - 10.1109/ICRA40945.2020.9197253
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study deals with the balance of humanoid or multi-legged robots in a multi-contact setting where a chosen subset of contacts is undergoing desired sliding-task motions. One method to keep balance is to hold the center-of-mass (CoM) within an admissible convex area. This area is calculated based on the contact positions and forces. We introduce a methodology to compute this CoM support area (CSA) for multiple fixed and intentionally sliding contacts. To select the most appropriate CoM position within CSA, we account for (i) constraints of multiple fixed and sliding contacts, (ii) desired wrench distribution for contacts, and (iii) desired CoM position (eventually dictated by other tasks). These are formulated as a quadratic programming (QP) optimization problems. We illustrate our approach with pushing against a wall and wiping, and conducted experiments using the HRP-4 humanoid robot.
ER  - 

TY  - CONF
TI  - Fast Whole-Body Motion Control of Humanoid Robots with Inertia Constraints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6597
EP  - 6603
AU  - G. Ficht
AU  - S. Behnke
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - position control
KW  - predictive control
KW  - robot dynamics
KW  - reduced five mass model
KW  - analytical solution
KW  - mass distribution
KW  - inertial properties
KW  - desired foot positioning
KW  - CRB inertia properties
KW  - model predictive control
KW  - dynamic kicking motion
KW  - humanoid robots
KW  - inertia constraints
KW  - analytical method
KW  - whole-body motions
KW  - fast whole-body motion control
KW  - humanoid open platform robot
KW  - desired composite rigid body inertia
KW  - Legged locomotion
KW  - Foot
KW  - Kinematics
KW  - Humanoid robots
KW  - Hip
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197322
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce a new, analytical method for generating whole-body motions for humanoid robots, which approximate the desired Composite Rigid Body (CRB) inertia. Our approach uses a reduced five mass model, where four of the masses are attributed to the limbs and one is used for the trunk. This compact formulation allows for finding an analytical solution that combines the kinematics with mass distribution and inertial properties of a humanoid robot. The positioning of the masses in Cartesian space is then directly used to obtain joint angles with relations based on simple geometry. Motions are achieved through the time evolution of poses generated through the desired foot positioning and CRB inertia properties. As a result, we achieve short computation times in the order of tens of microseconds. This makes the method suited for applications with limited computation resources, or leaving them to be spent on higher-layer tasks such as model predictive control. The approach is evaluated by performing a dynamic kicking motion with an igus® Humanoid Open Platform robot.
ER  - 

TY  - CONF
TI  - SL1M: Sparse L1-norm Minimization for contact planning on uneven terrain
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6604
EP  - 6610
AU  - S. Tonneau
AU  - D. Song
AU  - P. Fernbach
AU  - N. Mansard
AU  - M. Taïx
AU  - A. Del Prete
PY  - 2020
KW  - integer programming
KW  - legged locomotion
KW  - linear programming
KW  - minimisation
KW  - trajectory control
KW  - kinematic reachability
KW  - contact effectors
KW  - quasistatic COM trajectory
KW  - quasiflat contacts
KW  - contact surfaces
KW  - SL1M
KW  - uneven terrain
KW  - legged locomotion
KW  - combinatorial contact selection problem
KW  - mixed-integer optimization solvers
KW  - sparsity properties
KW  - L1 norm minimization techniques
KW  - online contact replanning
KW  - sparse L1-norm minimization
KW  - Silicon
KW  - Planning
KW  - Foot
KW  - Minimization
KW  - Kinematics
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197371
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - One of the main challenges of planning legged locomotion in complex environments is the combinatorial contact selection problem. Recent contributions propose to use integer variables to represent which contact surface is selected, and then to rely on modern mixed-integer (MI) optimization solvers to handle this combinatorial issue. To reduce the computational cost of MI, we exploit the sparsity properties of L1 norm minimization techniques to relax the contact planning problem into a feasibility linear program. Our approach accounts for kinematic reachability of the center of mass (COM) and of the contact effectors. We ensure the existence of a quasi-static COM trajectory by restricting our plan to quasi-flat contacts. For planning 10 steps with less than 10 potential contact surfaces for each phase, our approach is 50 to 100 times faster that its MI counterpart, which suggests potential applications for online contact re-planning. The method is demonstrated in simulation with the humanoid robots HRP-2 and Talos over various scenarios.
ER  - 

TY  - CONF
TI  - Finding Locomanipulation Plans Quickly in the Locomotion Constrained Manifold
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6611
EP  - 6617
AU  - S. J. Jorgensen
AU  - M. Vedantam
AU  - R. Gupta
AU  - H. Cappel
AU  - L. Sentis
PY  - 2020
KW  - end effectors
KW  - humanoid robots
KW  - legged locomotion
KW  - manipulator dynamics
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - robot programming
KW  - locomanipulation plans
KW  - locomotion constrained manifold
KW  - end-effector trajectory
KW  - injective locomotion constraint manifold
KW  - locomotion scheme
KW  - admissible manipulation trajectories
KW  - weighted-A* graph search
KW  - planner output
KW  - contact transitions
KW  - path progression trajectory
KW  - whole-body kinodynamic locomanipulation plan
KW  - locomanipulability region
KW  - edge transition feasibility
KW  - NASA Valkyrie robot platform
KW  - dynamic locomotion approach
KW  - example locomanipulation scenarios
KW  - divergent-component-of-motion
KW  - Trajectory
KW  - Task analysis
KW  - Foot
KW  - Manifolds
KW  - Pelvis
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197533
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a method that finds locomanipulation plans that perform simultaneous locomotion and manipulation of objects for a desired end-effector trajectory. Key to our approach is to consider an injective locomotion constraint manifold that defines the locomotion scheme of the robot and then using this constraint manifold to search for admissible manipulation trajectories. The problem is formulated as a weighted-A* graph search whose planner output is a sequence of contact transitions and a path progression trajectory to construct the whole-body kinodynamic locomanipulation plan. We also provide a method for computing, visualizing, and learning the locomanipulability region, which is used to efficiently evaluate the edge transition feasibility during the graph search. Numerical simulations are performed with the NASA Valkyrie robot platform that utilizes a dynamic locomotion approach, called the divergent-component-of-motion (DCM), on two example locomanipulation scenarios.
ER  - 

TY  - CONF
TI  - Force-based Control of Bipedal Balancing on Dynamic Terrain with the "Tallahassee Cassie" Robotic Platform
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6618
EP  - 6624
AU  - J. White
AU  - D. Swart
AU  - C. Hubicki
PY  - 2020
KW  - force control
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - PD control
KW  - position control
KW  - springs (mechanical)
KW  - bipedal control
KW  - minimal model information
KW  - dynamic impacts
KW  - walking running controllers
KW  - modeling information
KW  - force-based control
KW  - bipedal balancing
KW  - Tallahassee Cassie robotic platform
KW  - bipedal robots
KW  - force-based double support balancing controller
KW  - dynamic terrain scenarios
KW  - robotic bipedal platform
KW  - minimal information
KW  - robot model
KW  - individual links
KW  - pelvis-centric pelvis positions
KW  - commanding pelvis positions
KW  - model-free PD controller
KW  - Pelvis
KW  - Legged locomotion
KW  - Foot
KW  - Dynamics
KW  - Vehicle dynamics
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196725
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Out in the field, bipedal robots need to travel on terrain that is uneven, non-rigid, and sometimes moving beneath their feet. We present a force-based double support balancing controller for such dynamic terrain scenarios for bipedal robots, and test it on the robotic bipedal platform "Tallahassee Cassie." The presented controller relies on minimal information about the robot model, requiring its kinematics and overall weight, but not inertias of individual links or components. The controller is pelvis-centric, commanding pelvis positions in Cartesian space, which a model-free PD controller converts to motor torques in joint space. By commanding forces, torques, and a frontal center of pressure in this fashion, Tallahassee Cassie is capable of balancing on a variety of scenarios, from a lifting/sliding platform, to soft foam, to a sudden drop. These results show the potential for bipedal control to balance successfully despite minimal model information, the presence of large dynamic impacts-e.g., falling through trap door, and soft series-spring deflections. These results motivate future work for walking and running controllers on dynamic terrain with relatively low reliance on modeling information.
ER  - 

TY  - CONF
TI  - Dense r-robust formations on lattices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6633
EP  - 6639
AU  - L. Guerrero-Bonilla
AU  - D. Saldaña
AU  - V. Kumar
PY  - 2020
KW  - energy consumption
KW  - multi-robot systems
KW  - network theory (graphs)
KW  - cubic lattices
KW  - dense r-robust formations
KW  - robot networks
KW  - malicious robots
KW  - defective robots
KW  - high energy consumption
KW  - communication network
KW  - robot formations
KW  - square lattices
KW  - triangular lattices
KW  - Lattices
KW  - Robot kinematics
KW  - Communication networks
KW  - Robustness
KW  - Robot sensing systems
KW  - Energy consumption
DO  - 10.1109/ICRA40945.2020.9196683
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot networks are susceptible to fail under the presence of malicious or defective robots. Resilient networks in the literature require high connectivity and large communication ranges, leading to high energy consumption in the communication network. This paper presents robot formations with guaranteed resiliency that use smaller communication ranges than previous results in the literature. The formations can be built on triangular and square lattices in the plane, and cubic lattices in the three-dimensional space. We support our theoretical framework with simulations.
ER  - 

TY  - CONF
TI  - Optimizing Topologies for Probabilistically Secure Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6640
EP  - 6646
AU  - R. Wehbe
AU  - R. K. Williams
PY  - 2020
KW  - combinatorial mathematics
KW  - computational complexity
KW  - graph theory
KW  - matrix algebra
KW  - Monte Carlo methods
KW  - multi-robot systems
KW  - optimisation
KW  - set theory
KW  - statistical distributions
KW  - multirobot system
KW  - MRS
KW  - robot interactions
KW  - probability distribution
KW  - optimal solution
KW  - rooted k-connections problem
KW  - graph transformations
KW  - weighted matroid intersection algorithm
KW  - edge set
KW  - interaction graph
KW  - optimal security solution
KW  - secure multirobot systems
KW  - Robots
KW  - Probabilistic logic
KW  - Security
KW  - Optimization
KW  - Topology
KW  - Observers
KW  - Multi-robot systems
DO  - 10.1109/ICRA40945.2020.9197249
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we optimize the interaction graph of a multi-robot system (MRS) by maximizing its probability of security while requiring the MRS to have the fewest edges possible. Edges that represent robot interactions exist according to a probability distribution and security is defined using the control theoretic notion of left invertibility. To compute an optimal solution to our problem, we first start by reducing our problem to a variation of the rooted k-connections problem using three graph transformations. Then, we apply a weighted matroid intersection algorithm (WMIA) on matroids defined on the edge set of the interaction graph. Although the optimal solution can be found in polynomial time, MRSs are dynamic and their topologies may change faster than the rate at which the optimal security solution can be found. To cope with dynamic behavior, we present two heuristics that relax optimality but execute with much lower time complexity. Finally, we validate our results through Monte Carlo simulations.
ER  - 

TY  - CONF
TI  - Efficient Communication in Large Multi-robot Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6647
EP  - 6653
AU  - A. Dutta
AU  - A. Ghosh
AU  - S. Sisley
AU  - O. P. Kreidl
PY  - 2020
KW  - multi-robot systems
KW  - peer-to-peer computing
KW  - radiocommunication
KW  - communication routing
KW  - ground-level communication
KW  - multirobot coordination frameworks
KW  - multirobot system
KW  - multirobot networks
KW  - peer-to-peer radio communication
KW  - Robot kinematics
KW  - Routing
KW  - Multi-robot systems
KW  - Complexity theory
KW  - Relays
KW  - Communication networks
DO  - 10.1109/ICRA40945.2020.9196672
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To achieve coordination in a multi-robot system, the robots typically resort to some form of communication among each other. In most of the multi-robot coordination frameworks, high-level coordination strategies are studied but `how' the ground-level communication takes place, is assumed to be taken care of by another program. In this paper, we study the communication routing problem for large multi-robot systems where the robots have limited communication ranges. The objective is to send a message from a robot to another in the network, routed through a low number of other robots. To this end, we propose a communication model between any pair of robots using peer-to-peer radio communication. Our proposed model is generic to any type of message and guarantees a low hop routing between any pair of robots in this network. These help the robots to exchange large messages (e.g., multi-spectral images) in a short amount of time. Results show that our proposed approach easily scales up to 1000 robots while drastically reducing the space complexity for maintaining the network information.
ER  - 

TY  - CONF
TI  - CyPhyHouse: A programming, simulation, and deployment toolchain for heterogeneous distributed coordination
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6654
EP  - 6660
AU  - R. Ghosh
AU  - J. P. Jansch-Porto
AU  - C. Hsieh
AU  - A. Gosse
AU  - M. Jiang
AU  - H. Taylor
AU  - P. Du
AU  - S. Mitra
AU  - G. Dullerud
PY  - 2020
KW  - control engineering computing
KW  - control system synthesis
KW  - learning (artificial intelligence)
KW  - middleware
KW  - mobile computing
KW  - mobile robots
KW  - multi-threading
KW  - path planning
KW  - program debugging
KW  - specification languages
KW  - heterogeneous distributed coordination
KW  - libraries
KW  - development tools
KW  - application development processes
KW  - mobile computing
KW  - machine learning
KW  - CyPhyHouse
KW  - debugging
KW  - distributed mobile robotic applications
KW  - distributed applications
KW  - Koord programming language
KW  - controller design
KW  - distributed network protocols
KW  - platform-independent middleware
KW  - path planning
KW  - multithreaded simulator
KW  - Koord applications
KW  - application code
KW  - heterogeneous agents
KW  - heterogeneous mobile platforms
KW  - design cycles
KW  - robotic testbed
KW  - distributed task allocation
KW  - deployment toolchain
KW  - hardware-agnostic application
KW  - Robot kinematics
KW  - Task analysis
KW  - Middleware
KW  - Collision avoidance
KW  - Python
DO  - 10.1109/ICRA40945.2020.9196513
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Programming languages, libraries, and development tools have transformed the application development processes for mobile computing and machine learning. This paper introduces CyPhyHouse-a toolchain that aims to provide similar programming, debugging, and deployment benefits for distributed mobile robotic applications. Users can develop hardware-agnostic, distributed applications using the high-level, event driven Koord programming language, without requiring expertise in controller design or distributed network protocols. The modular, platform-independent middleware of CyPhyHouse implements these functionalities using standard algorithms for path planning (RRT), control (MPC), mutual exclusion, etc. A high-fidelity, scalable, multi-threaded simulator for Koord applications is developed to simulate the same application code for dozens of heterogeneous agents. The same compiled code can also be deployed on heterogeneous mobile platforms. The effectiveness of CyPhyHouse in improving the design cycles is explicitly illustrated in a robotic testbed through development, simulation, and deployment of a distributed task allocation application on in-house ground and aerial vehicles.
ER  - 

TY  - CONF
TI  - Chance Constrained Simultaneous Path Planning and Task Assignment for Multiple Robots with Stochastic Path Costs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6661
EP  - 6667
AU  - F. Yang
AU  - N. Chakraborty
PY  - 2020
KW  - distributed algorithms
KW  - graph theory
KW  - multi-robot systems
KW  - path planning
KW  - probability
KW  - stochastic processes
KW  - simultaneous path planning
KW  - multiple robots
KW  - stochastic path costs
KW  - stochastic edge costs
KW  - robot team
KW  - stochastic travel costs
KW  - chance-constrained simultaneous task assignment
KW  - deterministic simultaneous task assignment
KW  - shortest paths
KW  - task locations
KW  - linear assignment problem
KW  - CC-STAP
KW  - Robots
KW  - Task analysis
KW  - Collision avoidance
KW  - Path planning
KW  - Resource management
KW  - Random variables
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197354
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel algorithm for simultaneous task assignment and path planning on a graph (or roadmap) with stochastic edge costs. In this problem, the initially unassigned robots and tasks are located at known positions in a roadmap. We want to assign a unique task to each robot and compute a path for the robot to go to its assigned task location. Given the mean and variance of travel cost of each edge, our goal is to develop algorithms that, with high probability, the total path cost of the robot team is below a minimum value in any realization of the stochastic travel costs. We formulate the problem as a chance-constrained simultaneous task assignment and path planning problem (CC-STAP). We prove that the optimal solution of CC-STAP can be obtained by solving a sequence of deterministic simultaneous task assignment and path planning problems in which the travel cost is a linear combination of mean and variance of the edge cost. We show that the deterministic problem can be solved in two steps. In the first step, robots compute the shortest paths to the task locations and in the second step, the robots solve a linear assignment problem with the costs obtained in the first step. We also propose a distributed algorithm that solves CC-STAP near-optimally. We present simulation results on randomly generated networks and data to demonstrate that our algorithm is scalable with the number of robots (or tasks) and the size of the network.
ER  - 

TY  - CONF
TI  - Optimal Topology Selection for Stable Coordination of Asymmetrically Interacting Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6668
EP  - 6674
AU  - P. Mukherjee
AU  - M. Santilli
AU  - A. Gasparri
AU  - R. K. Williams
PY  - 2020
KW  - integer programming
KW  - mathematical programming
KW  - motion control
KW  - multi-robot systems
KW  - topology
KW  - stable coordinated motion
KW  - robot-to-robot interactions
KW  - asymmetric interaction topologies
KW  - multirobot motion
KW  - mixed integer semidefinite programming
KW  - multirobot systems
KW  - asymmetric interactions
KW  - optimal topology selection
KW  - Robot kinematics
KW  - Topology
KW  - Robot sensing systems
KW  - Multi-robot systems
KW  - Symmetric matrices
KW  - Laplace equations
DO  - 10.1109/ICRA40945.2020.9196822
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we address the problem of optimal topology selection for stable coordination of multi-robot systems with asymmetric interactions. This problem arises naturally for multi-robot systems that interact based on sensing, e.g., with limited field of view (FOV) cameras. From our previous efforts on motion control in such settings, we have shown that not all interaction topologies yield stable coordinated motion when asymmetry exists. At the same time, not all robot-to-robot interactions are of equal quality, and thus we seek to optimize asymmetric interaction topologies subject to the constraint that the topology yields stable multi-robot motion. In this context, we formulate an optimal topology selection problem (OTSP) as a mixed integer semidefinite programming (MISDP) problem to compute optimal topologies that yield stable coordinated motion. Simulation results are provided to corroborate the effectiveness of the proposed OTSP formulation.
ER  - 

TY  - CONF
TI  - Non-Prehensile Manipulation in Clutter with Human-In-The-Loop
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6723
EP  - 6729
AU  - R. Papallas
AU  - M. R. Dogar
PY  - 2020
KW  - manipulators
KW  - mobile robots
KW  - path planning
KW  - human-operator guided planning
KW  - low-level planner
KW  - fully autonomous sampling-based planners
KW  - human-in-the-loop
KW  - high-level plan
KW  - control-based randomized planning
KW  - pushing-based manipulation
KW  - clutter
KW  - nonprehensile manipulation
KW  - Robots
KW  - Planning
KW  - Clutter
KW  - Aerospace electronics
KW  - 1/f noise
KW  - Task analysis
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196689
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a human-operator guided planning approach to pushing-based manipulation in clutter. Most recent approaches to manipulation in clutter employs randomized planning. The problem, however, remains a challenging one where the planning times are still in the order of tens of seconds or minutes, and the success rates are low for difficult instances of the problem. We build on these control-based randomized planning approaches, but we investigate using them in conjunction with human-operator input. In our framework, the human operator supplies a high-level plan, in the form of an ordered sequence of objects and their approximate goal positions. We present experiments in simulation and on a real robotic setup, where we compare the success rate and planning times of our human-in-the-loop approach with fully autonomous sampling-based planners. We show that with a minimal amount of human input, the low-level planner can solve the problem faster and with higher success rates.
ER  - 

TY  - CONF
TI  - PuzzleFlex: kinematic motion of chains with loose joints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6730
EP  - 6737
AU  - S. Lensgraf
AU  - K. Itani
AU  - Y. Zhang
AU  - Z. Sun
AU  - Y. Wu
AU  - A. Q. Li
AU  - B. Zhu
AU  - E. Whiting
AU  - W. Wang
AU  - D. Balkcom
PY  - 2020
KW  - linear programming
KW  - mechanical stability
KW  - mobile robots
KW  - motion control
KW  - robot kinematics
KW  - tolerance analysis
KW  - PuzzleFlex
KW  - loose joints
KW  - free motions
KW  - planar assembly
KW  - rigid bodies
KW  - local distance constraints
KW  - configuration space velocities
KW  - linear programming formulation
KW  - structural stability perturbation analysis
KW  - tolerance analysis
KW  - Robots
KW  - Kinematics
KW  - Analytical models
KW  - Shape
KW  - Jacobian matrices
KW  - Aerospace electronics
KW  - Linear programming
DO  - 10.1109/ICRA40945.2020.9196854
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method of computing free motions of a planar assembly of rigid bodies connected by loose joints. Joints are modeled using local distance constraints, which are then linearized with respect to configuration space velocities, yielding a linear programming formulation that allows analysis of systems with thousands of rigid bodies. Potential applications include analysis of collections of modular robots, structural stability perturbation analysis, tolerance analysis for mechanical systems, and formation control of mobile robots.
ER  - 

TY  - CONF
TI  - Accurate Vision-based Manipulation through Contact Reasoning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6738
EP  - 6744
AU  - A. Kloss
AU  - M. Bauza
AU  - J. Wu
AU  - J. B. Tenenbaum
AU  - A. Rodriguez
AU  - J. Bohg
PY  - 2020
KW  - control engineering computing
KW  - inference mechanisms
KW  - manipulators
KW  - neural nets
KW  - robot vision
KW  - state estimation
KW  - vision-based manipulation
KW  - contact reasoning
KW  - contact interactions
KW  - motion optimization
KW  - state estimation
KW  - state representation
KW  - neural networks
KW  - Shape
KW  - Predictive models
KW  - Planning
KW  - Analytical models
KW  - Robots
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197409
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planning contact interactions is one of the core challenges of many robotic tasks. Optimizing contact locations while taking dynamics into account is computationally costly and, in environments that are only partially observable, executing contact-based tasks often suffers from low accuracy. We present an approach that addresses these two challenges for the problem of vision-based manipulation. First, we propose to disentangle contact from motion optimization. Thereby, we improve planning efficiency by focusing computation on promising contact locations. Second, we use a hybrid approach for perception and state estimation that combines neural networks with a physically meaningful state representation. In simulation and real-world experiments on the task of planar pushing, we show that our method is more efficient and achieves a higher manipulation accuracy than previous vision-based approaches.
ER  - 

TY  - CONF
TI  - A Probabilistic Framework for Constrained Manipulations and Task and Motion Planning under Uncertainty
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6745
EP  - 6751
AU  - J. -S. Ha
AU  - D. Driess
AU  - M. Toussaint
PY  - 2020
KW  - geometric programming
KW  - manipulators
KW  - path planning
KW  - stochastic processes
KW  - trajectory control
KW  - probabilistic framework
KW  - constrained manipulations
KW  - motion planning
KW  - manipulation planning framework
KW  - hierarchical structure
KW  - logic rules
KW  - trajectory optimization
KW  - large-scale sequential manipulation
KW  - tool-use planning problems
KW  - LGP formulation
KW  - stochastic domain
KW  - posterior path distribution
KW  - Gaussian path distribution
KW  - logic-geometric programming
KW  - Robots
KW  - Planning
KW  - Trajectory optimization
KW  - Skeleton
KW  - Programming
DO  - 10.1109/ICRA40945.2020.9196840
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Logic-Geometric Programming (LGP) is a powerful motion and manipulation planning framework, which represents hierarchical structure using logic rules that describe discrete aspects of problems, e.g., touch, grasp, hit, or push, and solves the resulting smooth trajectory optimization. The expressive power of logic allows LGP for handling complex, large-scale sequential manipulation and tool-use planning problems. In this paper, we extend the LGP formulation to stochastic domains. Based on the control-inference duality, we interpret LGP in a stochastic domain as fitting a mixture of Gaussians to the posterior path distribution, where each logic pro le defines a single Gaussian path distribution. The proposed framework enables a robot to prioritize various interaction modes and to acquire interesting behaviors such as contact exploitation for uncertainty reduction, eventually providing a composite control scheme that is reactive to disturbance.
ER  - 

TY  - CONF
TI  - Planning with Selective Physics-based Simulation for Manipulation Among Movable Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6752
EP  - 6758
AU  - M. Suhail Saleem
AU  - M. Likhachev
PY  - 2020
KW  - computer simulation
KW  - manipulators
KW  - path planning
KW  - planning model
KW  - robot manipulator
KW  - planning with selective physics based simulation
KW  - Planning
KW  - Collision avoidance
KW  - Manipulators
KW  - Physics
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197451
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Use of physics-based simulation as a planning model enables a planner to reason and generate plans that involve non-trivial interactions with the world. For example, grasping a milk container out of a cluttered refrigerator may involve moving a robot manipulator in between other objects, pushing away the ones that are moveable and avoiding interactions with certain fragile containers. A physics-based simulator allows a planner to reason about the effects of interactions with these objects and to generate a plan that grasps the milk container successfully. The use of physics-based simulation for planning however is underutilized. One of the reasons for it being that physics-based simulations are typically way too slow for being used within a planning loop that typically requires tens of thousands of actions to be evaluated within a matter of a second or two. In this work, we develop a planning algorithm that tries to address this challenge. In particular, it builds on the observation that only a small number of actions actually need to be simulated using physics, and the remaining set of actions, such as moving an arm around obstacles, can be evaluated using a much simpler internal planning model, e.g., a simple collision-checking model. Motivated by this, we develop an algorithm called Planning with Selective Physics-based Simulation that automatically discovers what should be simulated with physics and what can utilize an internal planning model for pick-and-place tasks.
ER  - 

TY  - CONF
TI  - Hybrid Differential Dynamic Programming for Planar Manipulation Primitives
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6759
EP  - 6765
AU  - N. Doshi
AU  - F. R. Hogan
AU  - A. Rodriguez
PY  - 2020
KW  - closed loop systems
KW  - dynamic programming
KW  - linear systems
KW  - manipulators
KW  - stability
KW  - switching systems (control)
KW  - trajectory control
KW  - hybrid trajectories
KW  - planar manipulation primitives
KW  - hybrid differential dynamic programming
KW  - closed-loop execution
KW  - frictional contact switches
KW  - hybrid DDP
KW  - finite horizon trajectories
KW  - linear stabilizing controllers
KW  - planar pivoting
KW  - hybrid switches
KW  - pose-to-pose closed-loop trajectories
KW  - planar pushing
KW  - Trajectory
KW  - Planning
KW  - Contacts
KW  - Heuristic algorithms
KW  - Force
KW  - Convergence
KW  - Friction
DO  - 10.1109/ICRA40945.2020.9197414
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a hybrid differential dynamic programming (DDP) algorithm for closed-loop execution of manipulation primitives with frictional contact switches. Planning and control of these primitives is challenging as they are hybrid, under-actuated, and stochastic. We address this by developing hybrid DDP both to plan finite horizon trajectories with a few contact switches and to create linear stabilizing controllers. We evaluate the performance and computational cost of our framework in ablations studies for two primitives: planar pushing and planar pivoting. We find that generating pose-to-pose closed-loop trajectories from most configurations requires only a couple (one to two) hybrid switches and can be done in reasonable time (one to five seconds). We further demonstrate that our controller stabilizes these hybrid trajectories on a real pushing system. A video describing our work can be found at https://youtu.be/YGSe4cUfq6Q.
ER  - 

TY  - CONF
TI  - Deep Depth Fusion for Black, Transparent, Reflective and Texture-Less Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6766
EP  - 6772
AU  - C. -Y. Chai
AU  - Y. -P. Wu
AU  - S. -L. Tsao
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image fusion
KW  - iterative methods
KW  - robot vision
KW  - stereo image processing
KW  - black objects
KW  - stereo cameras
KW  - depth values
KW  - structured-light camera
KW  - light path
KW  - depth fusion model
KW  - high-quality point clouds
KW  - short-range robotic applications
KW  - fusion weights
KW  - depth images
KW  - fused depth
KW  - depth prediction
KW  - stereo model
KW  - iterative closest point algorithm
KW  - deep depth fusion
KW  - transparent objects
KW  - reflective objects
KW  - Cameras
KW  - Robot vision systems
KW  - Three-dimensional displays
KW  - Color
KW  - Image color analysis
KW  - Prediction algorithms
DO  - 10.1109/ICRA40945.2020.9196894
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Structured-light and stereo cameras, which are widely used to construct point clouds for robotic applications, have different limitations on estimating depth values. Structured-light cameras fail in black, transparent, and reflective objects, which influence the light path; stereo cameras fail in texture-less objects. In this work, we propose a depth fusion model that complements these two types of methods to generate high-quality point clouds for short-range robotic applications. The model first determines the fusion weights from the two input depth images and then refines the fused depth using color features. We construct a dataset containing the aforementioned challenging objects and report the performance of our proposed model. The results reveal that our method reduces the average L1 distance on depth prediction by 75% and 52% compared with the original depth output of the structured-light camera and the stereo model, respectively. A noticeable improvement on the Iterative Closest Point (ICP) algorithm can be achieved by using the refined depth images output from our method.
ER  - 

TY  - CONF
TI  - LiDAR-enhanced Structure-from-Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6773
EP  - 6779
AU  - W. Zhen
AU  - Y. Hu
AU  - H. Yu
AU  - S. Scherer
PY  - 2020
KW  - cameras
KW  - image enhancement
KW  - image matching
KW  - image texture
KW  - motion estimation
KW  - optical radar
KW  - radar imaging
KW  - stereo image processing
KW  - rotating LiDAR
KW  - stereo camera pair
KW  - sensor motions
KW  - image matching
KW  - maturing technique
KW  - inspection purposes
KW  - LiDAR-enhanced structure-from-motion estimation
KW  - LiDAR-enhanced SfM pipeline algorithms
KW  - pipeline
KW  - Laser radar
KW  - Cameras
KW  - Pipelines
KW  - Three-dimensional displays
KW  - Visualization
KW  - Robustness
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197030
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Although Structure-from-Motion (SfM) as a maturing technique has been widely used in many applications, state-of-the-art SfM algorithms are still not robust enough in certain situations. For example, images for inspection purposes are often taken in close distance to obtain detailed textures, which will result in less overlap between images and thus decrease the accuracy of estimated motion. In this paper, we propose a LiDAR-enhanced SfM pipeline that jointly processes data from a rotating LiDAR and a stereo camera pair to estimate sensor motions. We show that incorporating LiDAR helps to effectively reject falsely matched images and significantly improve the model consistency in large-scale environments. Experiments are conducted in different environments to test the performance of the proposed pipeline and comparison results with the state-of-the-art SfM algorithms are reported.
ER  - 

TY  - CONF
TI  - Low Latency And Low-Level Sensor Fusion For Automotive Use-Cases
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6780
EP  - 6786
AU  - M. Pollach
AU  - F. Schiegg
AU  - A. Knoll
PY  - 2020
KW  - belief networks
KW  - image fusion
KW  - object detection
KW  - sensor synchronization
KW  - multiple sensors
KW  - object detection
KW  - low-level sensor fusion
KW  - automotive use-cases
KW  - probabilistic low level automotive sensor fusion approach
KW  - camera data
KW  - associated data
KW  - sensor modalities
KW  - probabilistic fusion
KW  - association method
KW  - Cameras
KW  - Three-dimensional displays
KW  - Object detection
KW  - Sensor fusion
KW  - Robot sensing systems
KW  - Two dimensional displays
KW  - Radar
KW  - sensor fusion
KW  - object detection
KW  - Bayesian networks
DO  - 10.1109/ICRA40945.2020.9196717
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work proposes a probabilistic low level automotive sensor fusion approach using LiDAR, RADAR and camera data. The method is stateless and directly operates on associated data from all sensor modalities. Tracking is not used, in order to reduce the object detection latency and create existence hypotheses per frame. The probabilistic fusion uses input from 3D and 2D space. An association method using a combination of overlap and distance metrics, avoiding the need for sensor synchronization is proposed. A Bayesian network executes the sensor fusion. The proposed approach is compared with a state of the art fusion system, which is using multiple sensors of the same modality and relies on tracking for object detection. Evaluation was done using low level sensor data recorded in an urban environment. The test results show that the low level sensor fusion reduces the object detection latency.
ER  - 

TY  - CONF
TI  - Robot-Assisted and Wearable Sensor-Mediated Autonomous Gait Analysis§
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6795
EP  - 6802
AU  - H. Zhang
AU  - Z. Chen
AU  - D. Zanotto
AU  - Y. Guo
PY  - 2020
KW  - gait analysis
KW  - kinematics
KW  - medical computing
KW  - mobile robots
KW  - regression analysis
KW  - support vector machines
KW  - autonomous gait analysis system
KW  - mobile robot
KW  - custom-engineered instrumented insoles
KW  - on-board RGB-D sensor
KW  - inertial sensors
KW  - force sensitive resistors
KW  - robot companion
KW  - walking exercises
KW  - support vector regression models
KW  - fundamental kinematic gait parameters
KW  - optical motion capture system
KW  - SVR models
KW  - autonomous mobile robots
KW  - out-of-the-lab gait analysis
KW  - robot-assisted wearable sensor-mediated autonomous gait analysis
KW  - Robot sensing systems
KW  - Legged locomotion
KW  - Task analysis
KW  - Instruments
KW  - Robot kinematics
KW  - Wearable Technology
KW  - Instrumented Footwear
KW  - Gait Analysis
KW  - Assistive Robotics
KW  - SportSole
DO  - 10.1109/ICRA40945.2020.9197571
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose an autonomous gait analysis system consisting of a mobile robot and custom-engineered instrumented insoles. The robot is equipped with an on-board RGB-D sensor, the insoles feature inertial sensors and force sensitive resistors. This system is motivated by the need for a robot companion to engage older adults in walking exercises. Support vector regression (SVR) models were developed to extract accurate estimates of fundamental kinematic gait parameters (i.e., stride length, velocity, foot clearance, and step length), from data collected with the robot's on-board RGB-D sensor and with the instrumented insoles during straight walking and turning tasks. The accuracy of each model was validated against ground-truth data measured by an optical motion capture system with N=10 subjects. Results suggest that the combined use of wearable and robot's sensors yields more accurate gait estimates than either sub-system used independently. Additionally, SVR models are robust to inter-subject variability and type of walking task (i.e., straight walking vs. turning), thereby making it unnecessary to collect subject-specific or task-specific training data for the models. These findings indicate the potential of the synergistic use of autonomous mobile robots and wearable sensors for accurate out-of-the-lab gait analysis.
ER  - 

TY  - CONF
TI  - A Control Framework Definition to Overcome Position/Interaction Dynamics Uncertainties in Force-Controlled Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6819
EP  - 6825
AU  - L. Roveda
AU  - N. Castaman
AU  - P. Franceschi
AU  - S. Ghidoni
AU  - N. Pedrocchi
PY  - 2020
KW  - control engineering computing
KW  - damping
KW  - force control
KW  - image colour analysis
KW  - image sensors
KW  - industrial manipulators
KW  - pose estimation
KW  - position control
KW  - production engineering computing
KW  - control framework definition
KW  - force-controlled tasks
KW  - industrial robots
KW  - increasing autonomy
KW  - manipulator
KW  - working environment
KW  - robust behavior
KW  - industrial interaction tasks
KW  - uncertain working scenes
KW  - 6D pose estimation
KW  - featureless parts
KW  - variable damping impedance controller
KW  - adaptive saturation PI
KW  - outer loop
KW  - high accuracy force control
KW  - force error
KW  - overshoots avoidance
KW  - task uncertainties
KW  - positioning errors
KW  - assembly task
KW  - force-tracking task
KW  - force overshoots
KW  - reference force
KW  - Force
KW  - Pose estimation
KW  - Impedance
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Robots
KW  - Damping
DO  - 10.1109/ICRA40945.2020.9197141
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Within the Industry 4.0 context, industrial robots need to show increasing autonomy. The manipulator has to be able to react to uncertainties/changes in the working environment, displaying a robust behavior. In this paper, a control framework is proposed to perform industrial interaction tasks in uncertain working scenes. The proposed methodology relies on two components: i) a 6D pose estimation algorithm aiming to recognize large and featureless parts; ii) a variable damping impedance controller (inner loop) enhanced by an adaptive saturation PI (outer loop) for high accuracy force control (i.e., zero steady-state force error and force overshoots avoidance). The proposed methodology allows to be robust w.r.t. task uncertainties (i.e. , positioning errors and interaction dynamics). The proposed approach has been evaluated in an assembly task of a side-wall panel to be installed inside the aircraft cabin. As a test platform, the KUKA iiwa 14 R820 has been used together with the Microsoft Kinect 2.0 as RGB-D sensor. Experiments show the reliability in the 6D pose estimation and the high-performance in the force-tracking task, avoiding force overshoots while achieving the tracking of the reference force.
ER  - 

TY  - CONF
TI  - Identification of Compliant Contact Parameters and Admittance Force Modulation on a Non-stationary Compliant Surface
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6826
EP  - 6832
AU  - L. Wijayarathne
AU  - F. L. Hammond
PY  - 2020
KW  - adaptive control
KW  - force control
KW  - manipulators
KW  - medical robotics
KW  - motion control
KW  - parameter estimation
KW  - position control
KW  - surgery
KW  - position-based adaptive force controller
KW  - interaction forces
KW  - nonstationary environments
KW  - fast parameter estimation
KW  - compliant contact parameters
KW  - admittance force modulation
KW  - nonstationary compliant surface
KW  - safety-critical applications
KW  - mechanical probing strategy
KW  - environmental impedance parameters
KW  - compliant environments
KW  - robotic manipulator autonomous control
KW  - manipulator controller design
KW  - surgical tasks
KW  - motion compensation
KW  - Force
KW  - Impedance
KW  - Force control
KW  - Probes
KW  - Surface impedance
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9196897
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Although autonomous control of robotic manipulators has been studied for several decades, they are not commonly used in safety-critical applications due to lack of safety and performance guarantees - many of them concerning the modulation of interaction forces. This paper presents a mechanical probing strategy for estimating the environmental impedance parameters of compliant environments, independent a manipulator's controller design, and configuration. The parameter estimates are used in a position-based adaptive force controller to enable control of interaction forces in compliant, stationary, and non-stationary environments. This approach is targeted for applications where the workspace is constrained and non-stationary, and where force control is critical to task success. These applications include surgical tasks involving manipulation of compliant, delicate, moving tissues. Results show fast parameter estimation and successful force modulation that compensates for motion.
ER  - 

TY  - CONF
TI  - Force Adaptation in Contact Tasks with Dynamical Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6841
EP  - 6847
AU  - W. Amanhoud
AU  - M. Khoramshahi
AU  - M. Bonnesoeur
AU  - A. Billard
PY  - 2020
KW  - adaptive control
KW  - force control
KW  - learning systems
KW  - manipulator dynamics
KW  - motion control
KW  - radial basis function networks
KW  - robots
KW  - uncertain systems
KW  - online adaptation
KW  - state-dependent force correction model
KW  - force error
KW  - collaborative cleaning task
KW  - task adaptation
KW  - adaptive force control
KW  - reactive behaviours
KW  - adaptive behaviours
KW  - force adaptation
KW  - contact tasks
KW  - robot dynamics
KW  - force tracking accuracy
KW  - compensation model
KW  - adaptive framework
KW  - force generation
KW  - time-invariant dynamical system framework
KW  - radial basis functions
KW  - KUKA LWR IV+ robotic arm
KW  - Force
KW  - Robots
KW  - Task analysis
KW  - Surface impedance
KW  - Dynamics
KW  - Impedance
KW  - Tracking
KW  - Force Control
KW  - Compliance and Impedance Control
KW  - Physical Human-Robot Interaction
DO  - 10.1109/ICRA40945.2020.9197509
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many tasks such as finishing operations, achieving accurate force tracking is essential. However, uncertainties in the robot dynamics and the environment limit the force tracking accuracy. Learning a compensation model for these uncertainties to reduce the force error is an effective approach to overcome this limitation. However, this approach requires an adaptive and robust framework for motion and force generation. In this paper, we use the time-invariant Dynamical System (DS) framework for force adaptation in contact tasks. We propose to improve force tracking accuracy through online adaptation of a state-dependent force correction model encoded with Radial Basis Functions (RBFs). We evaluate our method with a KUKA LWR IV+ robotic arm. We show its efficiency to reduce the force error to a negligible amount with different target forces and robot velocities. Furthermore, we study the effect of the hyper-parameters and provide a guideline for their selection. We showcase a collaborative cleaning task with a human by integrating our method to previous works to achieve force, motion, and task adaptation at the same time. Thereby, we highlight the benefits of using adaptive force control in real-world environments where we need reactive and adaptive behaviours in response to interactions with the environment.
ER  - 

TY  - CONF
TI  - Weakly Supervised Silhouette-based Semantic Scene Change Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6861
EP  - 6867
AU  - K. Sakurada
AU  - M. Shibuya
AU  - W. Wang
PY  - 2020
KW  - feature extraction
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - object detection
KW  - weakly supervised silhouette-based semantic scene change detection
KW  - novel semantic scene change detection scheme
KW  - semantic change detection network
KW  - large-scale dataset
KW  - specific dataset
KW  - semantic extraction
KW  - change detection task
KW  - siamese network structure
KW  - publicly available dataset
KW  - Semantics
KW  - Image segmentation
KW  - Cameras
KW  - Training
KW  - Task analysis
KW  - Satellites
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9196985
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel semantic scene change detection scheme with only weak supervision. A straightforward approach for this task is to train a semantic change detection network directly from a large-scale dataset in an end-to-end manner. However, a specific dataset for this task, which is usually labor-intensive and time-consuming, becomes indispensable. To avoid this problem, we propose to train this kind of network from existing datasets by dividing this task into change detection and semantic extraction. On the other hand, the difference in camera viewpoints, for example, images of the same scene captured from a vehicle-mounted camera at different time points, usually brings a challenge to the change detection task. To address this challenge, we propose a new siamese network structure with the introduction of correlation layer. In addition, we create a publicly available dataset for semantic change detection to evaluate the proposed method. The experimental results verified both the robustness to viewpoint difference in change detection task and the effectiveness for semantic change detection of the proposed networks. Our code and dataset are available at https://github.com/xdspacelab/sscdnet.
ER  - 

TY  - CONF
TI  - 3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via Coupled Feature Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6868
EP  - 6875
AU  - L. Du
AU  - J. Tan
AU  - X. Xue
AU  - L. Chen
AU  - H. Wen
AU  - J. Feng
AU  - J. Li
AU  - X. Zhang
PY  - 2020
KW  - feature selection
KW  - image enhancement
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - robust joint 3D semantic-instance segmentation
KW  - human scene perception process
KW  - CFSM
KW  - coupled feature selection module
KW  - reciprocal semantic instance feature selection
KW  - 3DCFS
KW  - robust 3D point cloud segmentation framework
KW  - Semantics
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Logic gates
KW  - Training
KW  - Euclidean distance
DO  - 10.1109/ICRA40945.2020.9197242
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel fast and robust 3D point clouds segmentation framework via coupled feature selection, named 3DCFS, that jointly performs semantic and instance segmentation. Inspired by the human scene perception process, we design a novel coupled feature selection module, named CFSM, that adaptively selects and fuses the reciprocal semantic and instance features from two tasks in a coupled manner. To further boost the performance of the instance segmentation task in our 3DCFS, we investigate a loss function that helps the model learn to balance the magnitudes of the output embedding dimensions during training, which makes calculating the Euclidean distance more reliable and enhances the generalizability of the model. Extensive experiments demonstrate that our 3DCFS outperforms state-of-the-art methods on benchmark datasets in terms of accuracy, speed and computational cost. Codes are available at: https://github.com/Biotan/3DCFS.
ER  - 

TY  - CONF
TI  - Who2com: Collaborative Perception via Learnable Handshake Communication
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6876
EP  - 6883
AU  - Y. -C. Liu
AU  - J. Tian
AU  - C. -Y. Ma
AU  - N. Glaser
AU  - C. -W. Kuo
AU  - Z. Kira
PY  - 2020
KW  - aircraft communication
KW  - autonomous aerial vehicles
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - multi-agent systems
KW  - multi-robot systems
KW  - neural nets
KW  - visual perception
KW  - degraded sensor data
KW  - compressed request
KW  - aerial robots
KW  - semantic segmentation task
KW  - collaborative perception
KW  - learnable handshake communication
KW  - local observations
KW  - neighboring agents
KW  - multiagent reinforcement learning
KW  - bandwidth-sensitive manner
KW  - scene understanding tasks
KW  - communication protocols
KW  - multistage handshake communication mechanism
KW  - neural network
KW  - Who2com
KW  - AirSim simulator
KW  - AirSim-CP dataset
KW  - Task analysis
KW  - Bandwidth
KW  - Semantics
KW  - Training
KW  - Collaboration
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197364
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose the problem of collaborative perception, where robots can combine their local observations with those of neighboring agents in a learnable way to improve accuracy on a perception task. Unlike existing work in robotics and multi-agent reinforcement learning, we formulate the problem as one where learned information must be shared across a set of agents in a bandwidth-sensitive manner to optimize for scene understanding tasks such as semantic segmentation. Inspired by networking communication protocols, we propose a multi-stage handshake communication mechanism where the neural network can learn to compress relevant information needed for each stage. Specifically, a target agent with degraded sensor data sends a compressed request, the other agents respond with matching scores, and the target agent determines who to connect with (i.e., receive information from). We additionally develop the AirSim-CP dataset and metrics based on the AirSim simulator where a group of aerial robots perceive diverse landscapes, such as roads, grasslands, buildings, etc. We show that for the semantic segmentation task, our handshake communication method significantly improves accuracy by approximately 20% over decentralized baselines, and is comparable to centralized ones using a quarter of the bandwidth.
ER  - 

TY  - CONF
TI  - Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6884
EP  - 6890
AU  - Z. Landgraf
AU  - F. Falck
AU  - M. Bloesch
AU  - S. Leutenegger
AU  - A. J. Davison
PY  - 2020
KW  - data visualisation
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - SLAM (robots)
KW  - view-based labelling
KW  - spatial AI systems
KW  - real-time height map fusion
KW  - map-based labelling
KW  - generated scene model
KW  - input view-wise data
KW  - estimate labels
KW  - clear groups
KW  - labelling scenes
KW  - semantic labels
KW  - geometric models
KW  - persistent scene representations
KW  - real-time SLAM
KW  - map-based semantic labelling
KW  - Labeling
KW  - Semantics
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Image reconstruction
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196843
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Generally capable Spatial AI systems must build persistent scene representations where geometric models are combined with meaningful semantic labels. The many approaches to labelling scenes can be divided into two clear groups: view-based which estimate labels from the input view-wise data and then incrementally fuse them into the scene model as it is built; and map-based which label the generated scene model. However, there has so far been no attempt to quantitatively compare view-based and map-based labelling. Here, we present an experimental framework and comparison which uses real-time height map fusion as an accessible platform for a fair comparison, opening up the route to further systematic research in this area.
ER  - 

TY  - CONF
TI  - Generative Modeling of Environments with Scene Grammars and Variational Inference
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6891
EP  - 6897
AU  - G. Izatt
AU  - R. Tedrake
PY  - 2020
KW  - grammars
KW  - inference mechanisms
KW  - mobile robots
KW  - object detection
KW  - probability
KW  - trees (mathematics)
KW  - variational inference algorithm
KW  - observed environments
KW  - nontrivial manipulation-relevant datasets
KW  - scene grammars
KW  - discrete variables
KW  - continuous variables
KW  - probabilistic generative model
KW  - scene trees
KW  - hierarchical relationships
KW  - labeled parse trees
KW  - Grammar
KW  - Robots
KW  - Production
KW  - Testing
KW  - Training
KW  - Random variables
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196910
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In order to understand how a robot will perform in the open world, we aim to establish a quantitative understanding of the distribution of environments that a robot will face when when it is deployed. However, even restricting attention only to the distribution of objects in a scene, these distributions over environments are nontrivial: they describe mixtures of discrete and continuous variables related to the number, type, poses, and attributes of objects in the scene. We describe a probabilistic generative model that uses scene trees to capture hierarchical relationships between collections of objects, as well as a variational inference algorithm for tuning that model to best match a set of observed environments without any need for tediously labeled parse trees. We demonstrate that this model can accurately capture the distribution of a pair of nontrivial manipulation-relevant datasets and be deployed as a density estimator and outlier detector for novel environments.
ER  - 

TY  - CONF
TI  - SHOP-VRB: A Visual Reasoning Benchmark for Object Perception
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6898
EP  - 6904
AU  - M. Nazarczuk
AU  - K. Mikolajczyk
PY  - 2020
KW  - control engineering computing
KW  - image representation
KW  - inference mechanisms
KW  - manipulators
KW  - natural language processing
KW  - query processing
KW  - robot vision
KW  - text analysis
KW  - object perception
KW  - robotics applications
KW  - object grasping
KW  - object properties
KW  - visual text data
KW  - household objects
KW  - natural language descriptions
KW  - question-answer pairs
KW  - visual reasoning queries
KW  - scene semantic representations
KW  - symbolic program execution
KW  - disentangled representation
KW  - visual inputs
KW  - textual inputs
KW  - symbolic programs
KW  - reasoning process
KW  - SHOP-VRB
KW  - visual reasoning benchmark
KW  - object manipulation
KW  - Visualization
KW  - Cognition
KW  - Benchmark testing
KW  - Robots
KW  - Image color analysis
KW  - Task analysis
KW  - Plastics
DO  - 10.1109/ICRA40945.2020.9197332
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we present an approach and a benchmark for visual reasoning in robotics applications, in particular small object grasping and manipulation. The approach and benchmark are focused on inferring object properties from visual and text data. It concerns small household objects with their properties, functionality, natural language descriptions as well as question-answer pairs for visual reasoning queries along with their corresponding scene semantic representations. We also present a method for generating synthetic data which allows to extend the benchmark to other objects or scenes and propose an evaluation protocol that is more challenging than in the existing datasets. We propose a reasoning system based on symbolic program execution. A disentangled representation of the visual and textual inputs is obtained and used to execute symbolic programs that represent a 'reasoning process' of the algorithm. We perform a set of experiments on the proposed benchmark and compare to results from the state of the art methods. These results expose the shortcomings of the existing benchmarks that may lead to misleading conclusions on the actual performance of the visual reasoning systems.
ER  - 

TY  - CONF
TI  - Sensorization of a Continuum Body Gripper for High Force and Delicate Object Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6913
EP  - 6919
AU  - J. Hughes
AU  - S. Li
AU  - D. Rus
PY  - 2020
KW  - closed loop systems
KW  - deformation
KW  - dexterous manipulators
KW  - feedback
KW  - grippers
KW  - tactile sensors
KW  - delicate object grasping
KW  - universal grasping
KW  - heavy bulky items
KW  - lightweight delicate objects
KW  - highly flexible latex bladders
KW  - Magic Ball origami gripper
KW  - tactile sensing
KW  - proprioceptive sensing
KW  - sensor feedback
KW  - closed loop controller
KW  - continuum body gripper sensorization
KW  - high force object grasping
KW  - Grippers
KW  - Force
KW  - Robot sensing systems
KW  - Strain
KW  - Bladder
KW  - Mechanical sensors
DO  - 10.1109/ICRA40945.2020.9196603
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The goal of achieving `universal grasping' where many objects can be handled with minimal control input is the focus of much research due to potential high impact applications ranging from grocery packing to recycling. However, many of the grippers developed suffer from limited sensing capabilities which can prevent handing of both heavy bulky items and also lightweight delicate objects which require fine control when grasping. Sensorizing such grippers is often challenging due to the highly deformable surfaces. We propose a novel sensing approach which uses highly flexible latex bladders. By measuring changes in the air pressure of the bladders, normal force and longitudinal strain can be measured. These sensors have been integrated into a `Magic Ball' origami gripper to provide both tactile and proprioceptive sensing. The sensors show reasonable sensitivity and repeatability, are durable and low-cost, and can be easily integrated into the gripper without affecting performance. When the sensors are used for classification, they enabled identification of 10 objects with over 90% accuracy, and also allow failure to be detected through slippage detection. A control algorithm has been developed which uses the sensor feedback to extend the capabilities of the gripper to include both delicate and strong grasping. It is shown that this closed loop controller enables delicate grasping of potato chips; 80% of those tested were grasped without damage.
ER  - 

TY  - CONF
TI  - A Soft Gripper with Retractable Nails for Advanced Grasping and Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6928
EP  - 6934
AU  - S. Jain
AU  - T. Stalin
AU  - V. Subramaniam
AU  - J. Agarwal
AU  - P. V. y Alvarado
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - manipulator dynamics
KW  - path planning
KW  - pneumatic actuators
KW  - manipulation tasks
KW  - normal grasping forces
KW  - delicate pinch grasps
KW  - robotic grasping tasks
KW  - soft gripper
KW  - advanced grasping
KW  - retractable finger nails
KW  - reconfigurable palm
KW  - finger nail mechanism
KW  - Nails
KW  - Grippers
KW  - Grasping
KW  - Three-dimensional displays
KW  - Actuators
KW  - Payloads
KW  - Fingers
DO  - 10.1109/ICRA40945.2020.9197259
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study describes the enhancement of a vacuum actuated soft gripper's grasping capabilities using retractable finger nails and an active re-configurable palm. The finger nail mechanism is pneumatically actuated and enables the gripper to perform complex grasping and manipulation tasks with high repeatability. The retracted nails can exert normal grasping forces of up to 1.8N and enable grasping of objects up to 200μm thick from flat surfaces, while allowing the gripper to execute delicate pinch grasps without complex trajectory or grasp planning. A wide array of robotic grasping tasks that were not possible without nails are also described.
ER  - 

TY  - CONF
TI  - Real-time Continuous Hand Motion Myoelectric Decoding by Automated Data Labeling*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6951
EP  - 6957
AU  - X. Hu
AU  - H. Zeng
AU  - D. Chen
AU  - J. Zhu
AU  - A. Song
PY  - 2020
KW  - biomechanics
KW  - electromyography
KW  - gesture recognition
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - medical signal processing
KW  - prosthetics
KW  - sEMG data
KW  - hand motion measurement
KW  - automated data labeling neural network
KW  - hand motion myoelectric decoding
KW  - hand motion labels
KW  - unsupervised neural network
KW  - unlabeled sEMG
KW  - hand motion signals
KW  - bio-signals
KW  - surface electromyography array
KW  - dataset collecting
KW  - Muscles
KW  - Neurons
KW  - Wrist
KW  - Training
KW  - Feature extraction
KW  - Neural networks
KW  - Principal component analysis
DO  - 10.1109/ICRA40945.2020.9197286
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper an automated data labeling (ADL) neural network is proposed to streamline dataset collecting for real-time predicting the continuous motion of hand and wrist, these gestures are only decoded from a surface electromyography (sEMG) array of eight channels. Unlike collecting both the bio-signals and hand motion signals as samples and labels in supervised learning, this algorithm only collects unlabeled sEMG into an unsupervised neural network, in which the hand motion labels are auto-generated. The coefficient of determination (R2) for three DOFs, i.e. wrist flex/extension, wrist pro/supination, hand open/close, was 0.86, 0.89 and 0.87 respectively. The comparison between real motion labels and auto-generated labels shows that the latter has earlier response than former. The results of Fitts' law test indicate that ADL has capability of controlling multi-DOFs simultaneously even though the training set only contains sEMG data from single DOF gesture. Moreover, no more hand motion measurement needed which greatly helps upper limb amputee imagine the gesture of residual limb to control a dexterous prosthesis.
ER  - 

TY  - CONF
TI  - Towards Proactive Navigation: A Pedestrian-Vehicle Cooperation Based Behavioral Model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6958
EP  - 6964
AU  - M. Kabtoul
AU  - A. Spalanzani
AU  - P. Martinet
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - remotely operated vehicles
KW  - road traffic control
KW  - road vehicles
KW  - trajectory control
KW  - pedestrian-vehicle cooperation
KW  - autonomous vehicle
KW  - intelligent transportation
KW  - autonomous navigation research
KW  - safe proactive navigation
KW  - pedestrian-vehicle interaction behavioral model
KW  - interaction scenario
KW  - quantitative time-varying function
KW  - cooperation estimation
KW  - cooperation-based trajectory planning model
KW  - Navigation
KW  - Space vehicles
KW  - Predictive models
KW  - Strain
KW  - Task analysis
KW  - Trajectory
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9196669
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Developing autonomous vehicles capable of navigating safely and socially around pedestrians is a major challenge in intelligent transportation. This challenge cannot be met without understanding pedestrians' behavioral response to an autonomous vehicle, and the task of building a clear and quantitative description of the pedestrian to vehicle interaction remains a key milestone in autonomous navigation research. As a step towards safe proactive navigation in a space shared with pedestrians, this work introduces a pedestrian-vehicle interaction behavioral model. The model estimates the pedestrian's cooperation with the vehicle in an interaction scenario by a quantitative time-varying function. Using this cooperation estimation the pedestrian's trajectory is predicted by a cooperation-based trajectory planning model. Both parts of the model are tested and validated using real-life recorded scenarios of pedestrian-vehicle interaction. The model is capable of describing and predicting agents' behaviors when interacting with a vehicle in both lateral and frontal crossing scenarios.
ER  - 

TY  - CONF
TI  - Studying Navigation as a Form of Interaction: a Design Approach for Social Robot Navigation Methods*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6965
EP  - 6972
AU  - P. Scales
AU  - O. Aycard
AU  - V. Aubergé
PY  - 2020
KW  - human-robot interaction
KW  - mobile robots
KW  - motion control
KW  - navigation
KW  - social robot navigation methods
KW  - social navigation methods
KW  - human sciences fields
KW  - mobile robot navigation
KW  - robot behavior
KW  - social hierarchy
KW  - socio-physical context
KW  - robot motion
KW  - human-robot interaction
KW  - human behavior
KW  - Navigation
KW  - Biological system modeling
KW  - Robot sensing systems
KW  - Mobile robots
KW  - Design methodology
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197037
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Social Navigation methods attempt to integrate knowledge from Human Sciences fields such as the notion of Proxemics into mobile robot navigation. They are often evaluated in simulations, or lab conditions with informed participants, and studies of the impact of the robot behavior on humans are rare. Humans communicate and interact through many vectors, among which are motion and positioning, which can be related to social hierarchy and the socio-physical context. If a robot is to be deployed among humans, the methods it uses should be designed with this in mind. This work acts as the first step in an ongoing project in which we explore how to design navigation methods for mobile robots destined to be deployed among humans. We aim to consider navigation as more than just a functionality of the robot, and to study the impact of robot motion on humans. In this paper, we focus on the person-following task. We selected a state of the art person-following method as the basis for our method, which we modified and extended in order for it to be more general and adaptable. We conducted pilot experiments using this method on a real mobile robot in ecological contexts. We used results from the experiments to study the Human-Robot Interaction as a whole by analysing both the person-following method and the human behavior. Our preliminary results show that the way in which the robot followed a person had an impact on the interaction that emerged between them.
ER  - 

TY  - CONF
TI  - Robot Plan Model Generation and Execution with Natural Language Interface*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6973
EP  - 6978
AU  - K. -M. Yang
AU  - K. -H. Seo
AU  - S. H. Kang
AU  - Y. Lim
PY  - 2020
KW  - human computer interaction
KW  - mobile robots
KW  - natural language interfaces
KW  - path planning
KW  - human instructions
KW  - service environments
KW  - robot plan model generation
KW  - natural language interface
KW  - interaction inconvenient
KW  - verbal interaction-based method
KW  - human involvement
KW  - human user
KW  - unclear instructions
KW  - reactive plan model
KW  - Task analysis
KW  - Service robots
KW  - Natural languages
KW  - Human-robot interaction
KW  - Robot sensing systems
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196987
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Verbal interaction between a human and a robot may play a key role in conveying suitable directions for a robot to achieve the goal of a user's request. However, a robot may need to correct task plans or make new decisions with human help, which would make the interaction inconvenient and also increase the interaction time. In this paper, we propose a new verbal interaction-based method that can generate plan models and execute proper actions without human involvement in the middle of performing a task by a robot. To understand the verbal behaviors of humans when giving instructions to a robot, we first conducted a brief user study and found that a human user does not explicitly express the required task. To handle such unclear instructions by a human, we propose two different algorithms that can generate a component of new plan models based on intents and entities parsed from natural language and can resolve the unclear entities existed in human instructions. An experimental scenario with a robot, Cozmo, was tried in the lab environment to test whether or not the proposed method could generate an appropriate plan model. As a result, we found that the robot could successfully accomplish the task following human instructions and also found that the number of interactions and components in the plan model could be reduced as opposed to the general reactive plan model. In the future, we are going to improve the automated process of generating plan models and apply various scenarios under different service environments and robots.
ER  - 

TY  - CONF
TI  - Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6979
EP  - 6985
AU  - J. Jin
AU  - N. M. Nguyen
AU  - N. Sakib
AU  - D. Graves
AU  - H. Yao
AU  - M. Jagersand
PY  - 2020
KW  - collision avoidance
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - robot dynamics
KW  - robot programming
KW  - time-efficient path planning behavior
KW  - dynamic crowds
KW  - social-safety-awareness
KW  - reinforcement learning
KW  - 2D laser scans
KW  - mapless collision-avoidance navigation
KW  - ego-safety
KW  - pedestrians
KW  - robot tests
KW  - Collision avoidance
KW  - Navigation
KW  - Training
KW  - Robot sensing systems
KW  - Lasers
KW  - Path planning
DO  - 10.1109/ICRA40945.2020.9197148
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of mapless collision-avoidance navigation where humans are present using 2D laser scans. Our proposed method uses ego-safety to measure collision from the robot's perspective and social-safety to measure the impact of robot's actions on surrounding pedestrians. Specifically, the social-safety part predicts the intrusion impact of the robot's action into the interaction area with surrounding humans. We train the policy using reinforcement learning on a simple simulator and directly evaluate the learned policy in Gazebo and real robot tests. Experiments show the learned policy smoothly transferred to different scenarios without any fine tuning. We observe that our method demonstrates time-efficient path planning behavior with high success rate in the mapless navigation task. Furthermore, we test our method in a navigation task among dynamic crowds, considering both low and high volume traffic. Our learned policy demonstrates cooperative behavior that actively drives our robot into traffic flows while showing respect to nearby pedestrians. Evaluation videos are at https://sites.google.com/view/ssw-batman.
ER  - 

TY  - CONF
TI  - Steering Control of Magnetic Helical Swimmers in Swirling Flows due to Confinement
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6994
EP  - 7000
AU  - H. O. Caldag
AU  - S. Yesilyurt
PY  - 2020
KW  - biomechanics
KW  - cell motility
KW  - computational fluid dynamics
KW  - flow simulation
KW  - hydrodynamics
KW  - magnetic actuators
KW  - microrobots
KW  - mobile robots
KW  - Navier-Stokes equations
KW  - position control
KW  - propulsion
KW  - swirling flow
KW  - trajectory control
KW  - vortices
KW  - prospective robotic agents
KW  - rotating magnetic field
KW  - magnetized swimmer
KW  - helical tail
KW  - helical paths
KW  - pusher-mode swimmers
KW  - rotating magnetic head
KW  - microswimmers
KW  - swimmer orientation
KW  - render orientation-based methods
KW  - confined swimmer
KW  - control law
KW  - swimmer position
KW  - swirling flow
KW  - helical pusher-mode trajectories
KW  - steering control
KW  - magnetic helical swimmers
KW  - Magnetic fields
KW  - Magnetosphere
KW  - Magnetic confinement
KW  - Magnetohydrodynamics
KW  - Propulsion
KW  - Navigation
KW  - microswimmers
KW  - helical swimming
KW  - low Reynolds number
KW  - steering
KW  - control
KW  - stability
DO  - 10.1109/ICRA40945.2020.9196521
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Artificial microswimmers are prospective robotic agents especially in biomedical applications. A rotating magnetic field can actuate a magnetized swimmer with a helical tail and enable propulsion. Such swimmers exhibit several modes of instability. Inside conduits, for example, hydrodynamic interactions with the boundaries lead to helical paths for pusher-mode swimmers; in this mode the helical tail pushes a rotating magnetic head. State-of-the-art in controlled navigation of micro-swimmers is based on aligning the swimmer orientation according to a reference path, thereby requiring both swimmer orientation and position to be known. Object-orientation is hard to track especially in in vivo scenarios which render orientation-based methods practically unfeasible. Here, we show that the kinematics for a confined swimmer can be linearized by assuming a low wobbling angle. This allows for a control law solely based on the swimmer position. The approach is demonstrated through experiments and two different numerical models: the first is based on the resistive force theory for a swimmer inside a swirling flow represented by a forced vortex and the second is a computational fluid dynamics model, which solves Stokes equations for a swimmer inside a circular channel. Helical pusher-mode trajectories are suppressed significantly for the straight path following problem. The error in real-life experiments remains comparable to those in the state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Sim2real gap is non-monotonic with robot complexity for morphology-in-the-loop flapping wing design
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7001
EP  - 7007
AU  - K. Rosser
AU  - J. Kok
AU  - J. Chahl
AU  - J. Bongard
PY  - 2020
KW  - aerospace components
KW  - aerospace control
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - biological exemplars
KW  - robot design
KW  - morphology-in-the-loop flapping wing design
KW  - robot complexity
KW  - sim2real gap
KW  - design complexity
KW  - sim2real transfer
KW  - high performance robot morphologies
KW  - parameterised morphology design space
KW  - flapping wing flight
KW  - machine learning
KW  - Morphology
KW  - Robots
KW  - Shape
KW  - Finite element analysis
KW  - Complexity theory
KW  - Computational modeling
KW  - Machine learning
KW  - morphology
KW  - simulation to reality
KW  - evolution
KW  - bio-inspired
DO  - 10.1109/ICRA40945.2020.9196539
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Morphology of a robot design is important to its ability to achieve a stated goal and therefore applying machine learning approaches that incorporate morphology in the design space can provide scope for significant advantage. Our study is set in a domain known to be reliant on morphology: flapping wing flight. We developed a parameterised morphology design space that draws features from biological exemplars and apply automated design to produce a set of high performance robot morphologies in simulation. By performing sim2real transfer on a selection, for the first time we measured the shape of the reality gap for variations in design complexity. We found for the flapping wing that the reality gap changes non-monotonically with complexity, suggesting that certain morphology details narrow the gap more than others, and that such details could be identified and further optimised in a future end-to-end automated morphology design process.
ER  - 

TY  - CONF
TI  - A Linearized Model for an Ornithopter in Gliding Flight: Experiments and Simulations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7008
EP  - 7014
AU  - R. Lopez-Lopez
AU  - V. Perez-Sanchez
AU  - P. Ramon-Soria
AU  - A. Martín-Alcántara
AU  - R. Fernandez-Feria
AU  - B. C. Arrue
AU  - A. Ollero
PY  - 2020
KW  - aerodynamics
KW  - aerospace components
KW  - autonomous aerial vehicles
KW  - linearisation techniques
KW  - position control
KW  - velocity control
KW  - longitudinal gliding flight configuration
KW  - aerodynamic forces
KW  - linearized potential theory
KW  - flat plate
KW  - flapping-wing episodes
KW  - linear potential theory
KW  - steady-state descent
KW  - terminal velocity
KW  - pitching
KW  - gliding angles
KW  - tail position
KW  - flapping-wing configuration
KW  - flight velocity
KW  - climbing episodes
KW  - realistic simulation tool
KW  - flapping frequencies
KW  - ornithopter flight
KW  - linearized model
KW  - flapping-wings UAV
KW  - Unreal Engine 4
KW  - Numerical models
KW  - Force
KW  - Mathematical model
KW  - Hardware
KW  - Steady-state
KW  - Computed tomography
KW  - Aerodynamics
DO  - 10.1109/ICRA40945.2020.9196929
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work studies the accuracy of a simple but effective analytical model for a flapping-wings UAV in longitudinal gliding flight configuration comparing it with experimental results of a real ornithopter. The aerodynamic forces are modeled following the linearized potential theory for a flat plate in gliding configuration, extended to flapping-wing episodes modeled also by the (now unsteady) linear potential theory, which are studied numerically. In the gliding configuration, the model reaches a steady-state descent at given terminal velocity and pitching and gliding angles, governed by the wings and tail position. In the flapping-wing configuration, it is noticed that the vehicle can increase its flight velocity and perform climbing episodes. A realistic simulation tool based on Unreal Engine 4 was developed to visualize the effect of the tail position and flapping frequencies and amplitudes on the ornithopter flight in real time. The paper also includes the experimental validation of the gliding flight and the data has been released for the community.
ER  - 

TY  - CONF
TI  - Towards biomimicry of a bat-style perching maneuver on structures: the manipulation of inertial dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7015
EP  - 7021
AU  - A. Ramezani
PY  - 2020
KW  - aerospace control
KW  - angular momentum
KW  - biomimetics
KW  - closed loop systems
KW  - gears
KW  - geometry
KW  - mobile robots
KW  - motion control
KW  - remotely operated vehicles
KW  - robot dynamics
KW  - bat-style perching maneuver
KW  - aerial drone designs
KW  - aerial flip turns
KW  - landing surface
KW  - zero-angular-momentum turns
KW  - detachable landing gear
KW  - closed-loop manipulations
KW  - biomimicry
KW  - inertial dynamics manipulation
KW  - bat flight characteristics
KW  - dynamical system
KW  - geometric conservation properties
KW  - Aerodynamics
KW  - Manipulator dynamics
KW  - Mathematical model
KW  - Robot sensing systems
KW  - Birds
DO  - 10.1109/ICRA40945.2020.9197376
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The flight characteristics of bats remarkably have been overlooked in aerial drone designs. Unlike other animals, bats leverage the manipulation of inertial dynamics to exhibit aerial flip turns when they perch. Inspired by this unique maneuver, this work develops and uses a tiny robot called Harpoon to demonstrate that the preparation for upside-down landing is possible through: 1) reorientation towards the landing surface through zero-angular-momentum turns and 2) reaching to the surface through shooting a detachable landing gear. The closed-loop manipulations of inertial dynamics takes place based on a symplectic description of the dynamical system (body and appendage), which is known to exhibit an excellent geometric conservation properties.
ER  - 

TY  - CONF
TI  - Bioinspired object motion filters as the basis of obstacle negotiation in micro aerial systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7022
EP  - 7028
AU  - R. Zhou
AU  - H. -T. Lin
PY  - 2020
KW  - aerospace robotics
KW  - collision avoidance
KW  - feedback
KW  - image filtering
KW  - image motion analysis
KW  - image sequences
KW  - microrobots
KW  - object detection
KW  - robot vision
KW  - object motion filters
KW  - obstacle negotiation
KW  - microaerial systems
KW  - biological visual guidance
KW  - machine vision system
KW  - motion vision
KW  - dense optic flow map
KW  - insect vision inspired object motion filter model
KW  - microracing drone
KW  - proximaldistal object separation
KW  - early-stage motion detection
KW  - feedback control loop
KW  - Visualization
KW  - Kernel
KW  - Optical filters
KW  - Drones
KW  - Band-pass filters
KW  - Biological system modeling
KW  - Insects
KW  - motion vision
KW  - obstacle avoidance
KW  - visual guidance
DO  - 10.1109/ICRA40945.2020.9196752
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - All animals and robots that move in the world must navigate to a goal while clearing obstacles. Using vision to accomplish such task has several advantages in cost and payload, which explains the prevalence of biological visual guidance. However, the computational overhead has been an obvious concern when increasing number of pixels and frames that need to be analyzed in real-time for a machine vision system. The use of motion vision and optic flow has been a popular bio-inspired solution for this problem. However, many early-stage motion detection approaches rely on special hardware (e.g. event-cameras) or extensive computation (e.g. dense optic flow map). Here we demonstrate a method to combine an insect vision inspired object motion filter model with simple visual guidance rules to fly through a cluttered environment. We have implemented a complete feedback control loop in a micro racing drone and achieved proximaldistal object separation through only two object motion filters. We discuss the key constraints and the scalability of this approach for future development.
ER  - 

TY  - CONF
TI  - ARCSnake: An Archimedes’ Screw-Propelled, Reconfigurable Serpentine Robot for Complex Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7029
EP  - 7034
AU  - D. A. Schreiber
AU  - F. Richter
AU  - A. Bilan
AU  - P. V. Gavrilov
AU  - H. Man Lam
AU  - C. H. Price
AU  - K. C. Carpenter
AU  - M. C. Yip
PY  - 2020
KW  - mobile robots
KW  - propulsion
KW  - robot dynamics
KW  - orientation control
KW  - versatile serpentine robot platform
KW  - screw threads
KW  - mechanical design
KW  - electrical design
KW  - reconfigurable serpentine robot
KW  - serpentine robots
KW  - screw propulsion
KW  - ARCSnake robot
KW  - omni-wheel drive-like motions
KW  - NASA-JPL EELS program
KW  - NASA-JPL Exobiology Extant Life Surveyor program
KW  - Robots
KW  - Fasteners
KW  - Brushless motors
KW  - Skin
KW  - Propulsion
KW  - Torque
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9196968
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the design and performance of a new locomotion strategy for serpentine robots using screw propulsion. The ARCSnake robot comprises serially linked, identical modules, each incorporating an Archimedes' screw for propulsion and a universal joint (U-Joint) for orientation control. When serially chained, these modules form a versatile serpentine robot platform which enables the robot to reshape its body configuration for varying environments, typical of a snake. Furthermore, the Archimedes' screws allow for novel omni-wheel drive-like motions by speed controlling their screw threads. This paper considers the mechanical and electrical design, as well as the software architecture for realizing a fully integrated system. The system includes 3N actuators for N segments, each controlled using a BeagleBone Black with a customized power-electronics cape, a 9 Degrees of Freedom (DoF) Inertial Measurement Unit (IMU), and a scalable communication channel over ROS. This robot serves as the first proof-of-concept demonstration of the NASA-JPL Exobiology Extant Life Surveyor (EELS) program that aims to deliver scientific instrumentation deep within the plume vents, caves, and ice sheets of Enceladus and Europa in search for extant lifeforms*.
ER  - 

TY  - CONF
TI  - GPR-based Subsurface Object Detection and Reconstruction Using Random Motion and DepthNet
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7035
EP  - 7041
AU  - J. Feng
AU  - L. Yang
AU  - H. Wang
AU  - Y. Song
AU  - J. Xiao
PY  - 2020
KW  - feature extraction
KW  - geophysical image processing
KW  - geophysical techniques
KW  - ground penetrating radar
KW  - image reconstruction
KW  - inspection
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - radar detection
KW  - radar imaging
KW  - stereo image processing
KW  - GPR-based subsurface object detection
KW  - DepthNet
KW  - Ground Penetrating Radar
KW  - nondestructive evaluation devices
KW  - underground scene
KW  - GPR based inspection
KW  - underground targets
KW  - pose information
KW  - GPR device
KW  - GPR image
KW  - B-scan data
KW  - GPR scan data
KW  - B-scan image
KW  - Geophysical Survey System Inc.
KW  - synthetic GPR data
KW  - B-scan feature detection
KW  - underground target depth prediction
KW  - GSSI
KW  - visual inertial fusion module
KW  - VIF module
KW  - gprMax3.0 simulator
KW  - NDE devices
KW  - 3D GPR migration
KW  - dielectric prediction system
KW  - deep neural network module
KW  - Ground penetrating radar
KW  - Three-dimensional displays
KW  - Image reconstruction
KW  - Dielectrics
KW  - Feature extraction
KW  - Object detection
KW  - Inspection
DO  - 10.1109/ICRA40945.2020.9197043
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Ground Penetrating Radar (GPR) is one of the most important non-destructive evaluation (NDE) devices to detect the subsurface objects (i.e. rebars, utility pipes) and reveal the underground scene. One of the biggest challenges in GPR based inspection is the subsurface targets reconstruction. In order to address this issue, this paper presents a 3D GPR migration and dielectric prediction system to detect and reconstruct underground targets. This system is composed of three modules: 1) visual inertial fusion (VIF) module to generate the pose information of GPR device, 2) deep neural network module (i.e., DepthNet) which detects B-scan of GPR image, extracts hyperbola features to remove the noise in B-scan data and predicts dielectric to determine the depth of the objects, 3) 3D GPR migration module which synchronizes the pose information with GPR scan data processed by DepthNet to reconstruct and visualize the 3D underground targets. Our proposed DepthNet processes the GPR data by removing the noise in B-scan image as well as predicting depth of subsurface objects. For DepthNet model training and testing, we collect the real GPR data in the concrete test pit at Geophysical Survey System Inc. (GSSI) and create the synthetic GPR data by using gprMax3.0 simulator. The dataset we create includes 350 labeled GPR images. The DepthNet achieves an average accuracy of 92.64% for B-scan feature detection and an 0.112 average error for underground target depth prediction. In addition, the experimental results verify that our proposed method improve the migration accuracy and performance in generating 3D GPR image compared with the traditional migration methods.
ER  - 

TY  - CONF
TI  - Real-time Stereo Visual Servoing for Rose Pruning with Robotic Arm
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7050
EP  - 7056
AU  - H. Cuevas-Velasquez
AU  - A. -J. Gallego
AU  - R. Tylecek
AU  - J. Hemming
AU  - B. van Tuijl
AU  - A. Mencarelli
AU  - R. B. Fisher
PY  - 2020
KW  - cutting
KW  - end effectors
KW  - gardening
KW  - real-time systems
KW  - robot vision
KW  - service robots
KW  - stereo image processing
KW  - visual servoing
KW  - multiple cameras
KW  - single stereo camera
KW  - end effector
KW  - robotic arm
KW  - real time stereo visual servoing
KW  - automated robotic rose cutter
KW  - rose bush pruning
KW  - gardening
KW  - rose pruning robots
KW  - Cameras
KW  - Robot vision systems
KW  - Manipulators
KW  - Real-time systems
KW  - Pipelines
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197272
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The paper presents a working pipeline which integrates hardware and software in an automated robotic rose cutter. To the best of our knowledge, this is the first robot able to prune rose bushes in a natural environment. Unlike similar approaches like tree stem cutting, the proposed method does not require to scan the full plant, have multiple cameras around the bush, or assume that a stem does not move. It relies on a single stereo camera mounted on the end-effector of the robot and real-time visual servoing to navigate to the desired cutting location on the stem. The evaluation of the whole pipeline shows a good performance in a garden with unconstrained conditions, where finding and approaching a specific location on a stem is challenging due to occlusions caused by other stems and dynamic changes caused by the wind.
ER  - 

TY  - CONF
TI  - Slip-Limiting Controller for Redundant Line-Suspended Robots: Application to Line Ranger
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7081
EP  - 7087
AU  - P. Hamelin
AU  - P. -L. Richard
AU  - M. Lepage
AU  - M. Lagacé
AU  - A. Sartor
AU  - G. Lambert
AU  - C. Hébert
AU  - N. Pouliot
PY  - 2020
KW  - angular velocity control
KW  - angular velocity measurement
KW  - centralised control
KW  - mobile robots
KW  - motion control
KW  - power transmission lines
KW  - service robots
KW  - wheels
KW  - wheel slippage
KW  - slip-limiting controller
KW  - redundant line-suspended robots
KW  - v-shaped wheels
KW  - wheel radius
KW  - wheel angular velocity measurements
KW  - slip limitation
KW  - control allocation algorithm
KW  - high-level velocity controller
KW  - centralized control
KW  - line ranger
KW  - Wheels
KW  - Mobile robots
KW  - Angular velocity
KW  - Robot sensing systems
KW  - Resource management
KW  - Velocity measurement
DO  - 10.1109/ICRA40945.2020.9196832
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a slip-limiting controller for redundant line-suspended robots is presented. This kind of robot is usually equipped with v-shaped wheels, which brings uncertainty about the effective wheel radius, particularly when crossing obstacles. The proposed algorithm is able to estimate and limit wheel slippage in the presence of such uncertainty, relying only on wheel angular velocity measurements. Slip limitation occurs in the control allocation algorithm and hence is decoupled from the high-level velocity controller, allowing a broad applicability in centralized control approaches. Experimental results on Line Ranger show that it effectively reduces wheel slippage compared to traditional centralized control while being more energy efficient than traditional decentralized control approaches.
ER  - 

TY  - CONF
TI  - Interval Search Genetic Algorithm Based on Trajectory to Solve Inverse Kinematics of Redundant Manipulators and Its Application
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7088
EP  - 7094
AU  - D. Wu
AU  - W. Zhang
AU  - M. Qin
AU  - B. Xie
PY  - 2020
KW  - end effectors
KW  - genetic algorithms
KW  - kinematics
KW  - redundant manipulators
KW  - search problems
KW  - trajectory control
KW  - tunnels
KW  - interval search strategy
KW  - reference point strategy
KW  - redundant manipulators
KW  - continuous motion
KW  - interval search genetic algorithm
KW  - inverse kinematics problem
KW  - parametric joint angle method
KW  - population continuity strategy
KW  - trajectory control
KW  - evolutionary generation
KW  - fitness function
KW  - tunnel shotcrete robot
KW  - end effector
KW  - Manipulators
KW  - Kinematics
KW  - Sociology
KW  - Statistics
KW  - Trajectory
KW  - Genetic algorithms
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196890
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a new method is proposed to solve the inverse kinematics problem of redundant manipulators. This method demonstrates superior performance on continuous motion by combining interval search genetic algorithm based on trajectory which we propose with parametric joint angle method. In this method, population continuity strategy is utilized to improve search speed and reduce evolutionary generation, interval search strategy is introduced to enhance the search ability and overcome the influence of singularity, and reference point strategy is used to avoid sudden changes of joint variables. By introducing those three strategies, this method is especially suitable for redundant manipulators that perform continuous motion. It can not only obtain solutions of inverse kinematics quickly, but also ensure the motion continuity of manipulator and accuracy of the end effector. Moreover, this algorithm can also perform multi-objective tasks by adjusting the fitness function. Finally, this algorithm is applied to an 8 degree of freedom tunnel shotcrete robot. Field experiments and data analysis show that the algorithm can solve the problem quickly in industrial field, and ensure the motion continuity and accuracy.
ER  - 

TY  - CONF
TI  - Analytical Expressions of Serial Manipulator Jacobians and their High-Order Derivatives based on Lie Theory*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7095
EP  - 7100
AU  - Z. Fu
AU  - E. Spyrakos-Papastavridis
AU  - Y. -h. Lin
AU  - J. S. Dai
PY  - 2020
KW  - approximation theory
KW  - end effectors
KW  - iterative methods
KW  - Jacobian matrices
KW  - Lie algebras
KW  - manipulator kinematics
KW  - manipulator Jacobian
KW  - high-order derivatives
KW  - Lie theory
KW  - higher-order derivatives
KW  - higher-order Jacobian derivatives
KW  - serial manipulator kinematics
KW  - joint variables
KW  - joint-space coordinates
KW  - serial manipulator Jacobians
KW  - task-space Cartesian coordinates
KW  - inertial-fixed frames
KW  - body-fixed frames
KW  - KUKA LRB iiwa7 R800 manipulator
KW  - Jacobian matrices
KW  - Manipulators
KW  - Kinematics
KW  - Acceleration
KW  - Fasteners
DO  - 10.1109/ICRA40945.2020.9197131
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Serial manipulator kinematics provide a mapping between joint variables in joint-space coordinates, and end-effector configurations in task-space Cartesian coordinates. Velocity mappings are represented via the manipulator Jacobian produced by direct differentiation of the forward kinematics. Acquisition of acceleration, jerk, and snap expressions, typically utilized for accurate trajectory-tracking, requires the computation of high-order Jacobian derivatives. As compared to conventional numerical/D-H approaches, this paper proposes a novel methodology to derive the Jacobians and their high-order derivatives symbolically, based on Lie theory, which requires that the derivatives are calculated with respect to each joint variable and time. Additionally, the technique described herein yields a mathematically sound solution to the high-order Jacobian derivatives, which distinguishes it from other relevant works. Performing computations with respect to the two inertial-fixed and body-fixed frames, the analytical form of the spatial and body Jacobians are derived, as well as their higher-order derivatives, without resorting to any approximations, whose expressions would depend explicitly on the joint state and the choice of reference frames. The proposed method provides more tractable computation of higher-order Jacobian derivatives, while its effectiveness has been verified by conducting a comparative analysis based on experimental data extracted from a KUKA LRB iiwa7 R800 manipulator.
ER  - 

TY  - CONF
TI  - Inverse Kinematics for Serial Kinematic Chains via Sum of Squares Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7101
EP  - 7107
AU  - F. Marić
AU  - M. Giamou
AU  - S. Khoubyarian
AU  - I. Petrović
AU  - J. Kelly
PY  - 2020
KW  - convex programming
KW  - end effectors
KW  - manipulator kinematics
KW  - mobile robots
KW  - polynomials
KW  - position control
KW  - degrees of freedom
KW  - articulated robots
KW  - globally optimal solution
KW  - serial manipulators
KW  - kinematic constraints
KW  - highly redundant serial kinematic chains
KW  - joint limit constraints
KW  - inverse kinematics problem
KW  - convex optimization techniques
KW  - numerical methods
KW  - nonlinear problem
KW  - feasible joint configurations
KW  - task-related workspace constraints
KW  - sum of squares optimization
KW  - Kinematics
KW  - Optimization
KW  - Conferences
KW  - Automation
KW  - Robots
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196704
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Inverse kinematics is a fundamental challenge for articulated robots: fast and accurate algorithms are needed for translating task-related workspace constraints and goals into feasible joint configurations. In general, inverse kinematics for serial kinematic chains is a difficult nonlinear problem, for which closed form solutions cannot easily be obtained. Therefore, computationally efficient numerical methods that can be adapted to a general class of manipulators are of great importance. In this paper, we use convex optimization techniques to solve the inverse kinematics problem with joint limit constraints for highly redundant serial kinematic chains with spherical joints in two and three dimensions. This is accomplished through a novel formulation of inverse kinematics as a nearest point problem, and with a fast sum of squares solver that exploits the sparsity of kinematic constraints for serial manipulators. Our method has the advantages of post-hoc certification of global optimality and a runtime that scales polynomially with the number of degrees of freedom. Additionally, we prove that our convex relaxation leads to a globally optimal solution when certain conditions are met, and demonstrate empirically that these conditions are common and represent many practical instances. Finally, we provide an open source implementation of our algorithm.
ER  - 

TY  - CONF
TI  - Multi-task closed-loop inverse kinematics stability through semidefinite programming
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7108
EP  - 7114
AU  - J. Marti-Saumell
AU  - A. Santamaria-Navarro
AU  - C. Ocampo-Martinez
AU  - J. Andrade-Cetto
PY  - 2020
KW  - closed loop systems
KW  - control system synthesis
KW  - discrete time systems
KW  - humanoid robots
KW  - linear matrix inequalities
KW  - Lyapunov methods
KW  - mathematical programming
KW  - mobile robots
KW  - robot kinematics
KW  - stability
KW  - multitask closed-loop inverse kinematics stability
KW  - multiobjective task resolution
KW  - humanoid robots
KW  - local stability problem
KW  - closed-loop inverse kinematics algorithm
KW  - highly redundant robots
KW  - system stability
KW  - closed-loop control gains
KW  - semidefinite programming problem
KW  - discrete-time Lyapunov stability condition
KW  - SDP optimization problem
KW  - stability conditions
KW  - Task analysis
KW  - Stability analysis
KW  - Robots
KW  - Kinematics
KW  - Thermal stability
KW  - Numerical stability
KW  - Asymptotic stability
DO  - 10.1109/ICRA40945.2020.9196750
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Today's complex robotic designs comprise in some cases a large number of degrees of freedom, enabling for multi-objective task resolution (e.g., humanoid robots or aerial manipulators). This paper tackles the local stability problem of a hierarchical closed-loop inverse kinematics algorithm for such highly redundant robots. We present a method to guarantee this system stability by performing an online tuning of the closed-loop control gains. We define a semi-definite programming problem (SDP) with these gains as decision variables and a discrete-time Lyapunov stability condition as a linear matrix inequality, constraining the SDP optimization problem and guaranteeing the local stability of the prioritized tasks. To the best of authors' knowledge, this work represents the first mathematical development of an SDP formulation that introduces these stability conditions for a multi-objective closed-loop inverse kinematic problem for highly redundant robots. The validity of the proposed approach is demonstrated through simulation case studies, including didactic examples and a Matlab toolbox for the benefit of the community.
ER  - 

TY  - CONF
TI  - Securing Industrial Operators with Collaborative Robots: Simulation and Experimental Validation for a Carpentry task
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7128
EP  - 7134
AU  - N. Benhabib
AU  - V. Padois
AU  - D. Daney
PY  - 2020
KW  - industrial robots
KW  - machine tools
KW  - milling
KW  - mobile robots
KW  - safety
KW  - wood
KW  - industrial operators
KW  - collaborative robot
KW  - carpentry task
KW  - robotic assistance strategy
KW  - machine-tool
KW  - wood milling
KW  - accidentogenic aspect
KW  - physical model
KW  - tooling process
KW  - safety
KW  - Task analysis
KW  - Cutting tools
KW  - Milling
KW  - Force
KW  - Service robots
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9197161
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, a robotic assistance strategy is developed to improve the safety in an artisanal task that involves a strong interaction between a machine-tool and an operator. Wood milling is chosen as a pilot task due to its importance in carpentry and its accidentogenic aspect. A physical model of the tooling process including a human is proposed and a simulator is thereafter developed to better understand situations that are dangerous for the craftsman. This simulator is validated with experiments on three subjects using an harmless mock-up. This validation shows the pertinence of the proposed control approach for the collaborative robot used to increase the safety of the task.
ER  - 

TY  - CONF
TI  - Learning Shape-based Representation for Visual Localization in Extremely Changing Conditions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7135
EP  - 7141
AU  - H. -G. Jeon
AU  - S. Im
AU  - J. Oh
AU  - M. Hebert
PY  - 2020
KW  - convolutional neural nets
KW  - disasters
KW  - image representation
KW  - image texture
KW  - learning (artificial intelligence)
KW  - natural scenes
KW  - pose estimation
KW  - shape recognition
KW  - shape-based representation
KW  - visual localization
KW  - extremely changing conditions
KW  - convolutional neural network
KW  - layout changes
KW  - approximate scene coordinates
KW  - scene layout
KW  - CNN
KW  - stylized images
KW  - estimated dominant planes
KW  - query images
KW  - simulated disaster dataset
KW  - reliable camera pose predictions
KW  - Visualization
KW  - Shape
KW  - Cameras
KW  - Semantics
KW  - Robustness
KW  - Geometry
KW  - Buildings
DO  - 10.1109/ICRA40945.2020.9196842
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual localization is an important task for applications such as navigation and augmented reality, but is a challenging problem when there are changes in scene appearances through day, seasons, or environments. In this paper, we present a convolutional neural network (CNN)-based approach for visual localization across normal to drastic appearance variations such as pre- and post-disaster cases. Our approach aims to address two key challenges: (1) to reduce the biases based on scene textures as in traditional CNNs, our model learns a shape-based representation by training on stylized images; (2) to make the model robust against layout changes, our approach uses the estimated dominant planes of query images as approximate scene coordinates. Our method is evaluated on various scenes including a simulated disaster dataset to demonstrate the effectiveness of our method in significant changes of scene layout. Experimental results show that our method provides reliable camera pose predictions in various changing conditions.
ER  - 

TY  - CONF
TI  - Trajectory Planning with Safety Guaranty for a Multirotor based on the Forward and Backward Reachability Analysis
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7142
EP  - 7148
AU  - H. Seo
AU  - C. Youngdong Son
AU  - D. Lee
AU  - H. Jin Kim
PY  - 2020
KW  - aircraft control
KW  - collision avoidance
KW  - helicopters
KW  - reachability analysis
KW  - robust control
KW  - set theory
KW  - trajectory control
KW  - obstacle avoidance
KW  - risk free flight
KW  - backward reachable sets
KW  - forward reachable sets
KW  - robust trajectory planning algorithm
KW  - safety guarantee
KW  - multirotor
KW  - Hamilton-Jacobi reachability analysis
KW  - Trajectory
KW  - Planning
KW  - Safety
KW  - Reachability analysis
KW  - Robustness
KW  - Optimization
KW  - Aerospace engineering
DO  - 10.1109/ICRA40945.2020.9196760
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planning a trajectory with guaranteed safety is a core part for a risk-free flight of a multirotor. If a trajectory planner only aims to ensure safety, it may generate trajectories which overly bypass risky regions and prevent the system from achieving specific missions. This work presents a robust trajectory planning algorithm which simultaneously guarantees the safety and reachability to the target state in the presence of unknown disturbances. We first characterize how the forward and backward reachable sets (FRSs and BRSs) are constructed by using Hamilton-Jacobi reachability analysis. Based on the analysis, we present analytic expressions for the reachable sets and then propose minimal ellipsoids which closely approximate the reachable sets. In the planning process, we optimize the reference trajectory to connect the FRSs and BRSs, while avoiding obstacles. By combining the FRSs and BRSs, we can guarantee that any state inside of the initial set reaches the target set. We validate the proposed algorithm through a simulation of traversing a narrow gap.
ER  - 

TY  - CONF
TI  - A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7149
EP  - 7155
AU  - S. Bansal
AU  - A. Bajcsy
AU  - E. Ratner
AU  - A. D. Dragan
AU  - C. J. Tomlin
PY  - 2020
KW  - Bayes methods
KW  - belief networks
KW  - continuous time systems
KW  - motion control
KW  - path planning
KW  - predictive control
KW  - probability
KW  - reachability analysis
KW  - robots
KW  - stochastic processes
KW  - probabilistic predictive models
KW  - human behavior
KW  - future motion
KW  - observation models
KW  - state predictions
KW  - robot motion plan
KW  - human behavioral data
KW  - human motion prediction
KW  - Hamilton-Jacobi reachability problem
KW  - continuous-time dynamical system
KW  - model parameters
KW  - worst-case forward reachable set
KW  - future state distributions
KW  - robust planning
KW  - Hamilton-Jacobi reachability-based framework
KW  - human motion analysis
KW  - safe planning
KW  - real-world autonomous systems
KW  - Predictive models
KW  - Robots
KW  - Stochastic processes
KW  - Planning
KW  - Data models
KW  - Computational modeling
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197257
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Real-world autonomous systems often employ probabilistic predictive models of human behavior during planning to reason about their future motion. Since accurately modeling human behavior a priori is challenging, such models are often parameterized, enabling the robot to adapt predictions based on observations by maintaining a distribution over the model parameters. Although this enables data and priors to improve the human model, observation models are difficult to specify and priors may be incorrect, leading to erroneous state predictions that can degrade the safety of the robot motion plan. In this work, we seek to design a predictor which is more robust to misspecified models and priors, but can still leverage human behavioral data online to reduce conservatism in a safe way. To do this, we cast human motion prediction as a Hamilton-Jacobi reachability problem in the joint state space of the human and the belief over the model parameters. We construct a new continuous-time dynamical system, where the inputs are the observations of human behavior, and the dynamics include how the belief over the model parameters change. The results of this reachability computation enable us to both analyze the effect of incorrect priors on future predictions in continuous state and time, as well as to make predictions of the human state in the future. We compare our approach to the worst-case forward reachable set and a stochastic predictor which uses Bayesian inference and produces full future state distributions. Our comparisons in simulation and in hardware demonstrate how our framework can enable robust planning while not being overly conservative, even when the human model is inaccurate. Videos of our experiments can be found at the project website1.
ER  - 

TY  - CONF
TI  - Enhancing Privacy in Robotics via Judicious Sensor Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7156
EP  - 7165
AU  - S. Eick
AU  - A. I. Antón
PY  - 2020
KW  - data privacy
KW  - design
KW  - robots
KW  - sensors
KW  - judicious sensor selection
KW  - roboticists
KW  - robot design
KW  - robotics journals
KW  - privacy preservation
KW  - robot lifecycle
KW  - privacy impact assessments
KW  - privacy enhancement
KW  - Robot sensing systems
KW  - Privacy
KW  - Data privacy
KW  - Cameras
KW  - Task analysis
KW  - Law
KW  - privacy
KW  - privacy by design
KW  - robotics
KW  - robot design
KW  - sensor selection
KW  - compliance
KW  - privacy impact assessments
DO  - 10.1109/ICRA40945.2020.9196983
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Roboticists are grappling with how to address privacy in robot design at a time when regulatory frameworks around the world increasingly require systems to be engineered to preserve and protect privacy. This paper surveys the top robotics journals and conferences over the past four decades to identify contributions with respect to privacy in robot design. Our survey revealed that less than half of one percent of the ~89,120 papers in our study even mention the word privacy. Herein, we propose privacy preserving approaches for roboticists to employ in robot design, including, assessing a robot's purpose and environment; ensuring privacy by design by selecting sensors that do not collect information that is not essential to the core objectives of that robot; embracing both privacy and performance as fundamental design challenges to be addressed early in the robot lifecycle; and performing privacy impact assessments.
ER  - 

TY  - CONF
TI  - Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7166
EP  - 7172
AU  - S. Li
AU  - O. Bastani
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - nonlinear control systems
KW  - nonlinear dynamical systems
KW  - predictive control
KW  - probability
KW  - robust control
KW  - stochastic processes
KW  - stochastic systems
KW  - backup policy
KW  - learned policy
KW  - control policy
KW  - additive stochastic disturbances
KW  - nominal dynamics
KW  - stochastic nonlinear dynamical systems
KW  - stochastic dynamics
KW  - safe reinforcement learning
KW  - robust model predictive shielding
KW  - stochastic systems
KW  - statistical learning theory
KW  - backup controller
KW  - tube-based robust nonlinear model predictive controller
KW  - Safety
KW  - Robustness
KW  - Stochastic processes
KW  - Robots
KW  - Trajectory
KW  - Nonlinear dynamical systems
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196867
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a framework for safe reinforcement learning that can handle stochastic nonlinear dynamical systems. We focus on the setting where the nominal dynamics are known, and are subject to additive stochastic disturbances with known distribution. Our goal is to ensure the safety of a control policy trained using reinforcement learning, e.g., in a simulated environment. We build on the idea of model predictive shielding (MPS), where a backup controller is used to override the learned policy as needed to ensure safety. The key challenge is how to compute a backup policy in the context of stochastic dynamics. We propose to use a tube-based robust nonlinear model predictive controller (NMPC) as the backup controller. We estimate the tubes using sampled trajectories, leveraging ideas from statistical learning theory to obtain high-probability guarantees. We empirically demonstrate that our approach can ensure safety in stochastic systems, including cart-pole and a non-holonomic particle with random obstacles.
ER  - 

TY  - CONF
TI  - Segregation of Heterogeneous Swarms of Robots in Curves
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7173
EP  - 7179
AU  - E. B. Ferreira Filho
AU  - L. C. A. Pimenta
PY  - 2020
KW  - collision avoidance
KW  - decentralised control
KW  - mobile robots
KW  - multi-robot systems
KW  - topology
KW  - decentralized control strategy
KW  - heterogeneous robot swarms
KW  - formation control
KW  - collision avoidance strategy
KW  - multiple heterogeneous robots
KW  - heterogeneous swarm segregation
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Heuristic algorithms
KW  - Topology
KW  - Convergence
KW  - Damping
DO  - 10.1109/ICRA40945.2020.9196851
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a decentralized control strategy to reach segregation in heterogeneous robot swarms distributed in curves. The approach is based on a formation control algorithm applied to each robot and a heuristics to compute the distance between the groups, i.e. the distance from the beginning of the curve. We consider that robots can communicate through a fixed underlying topology and also when they are within a certain distance. A convergence proof with a collision avoidance strategy is presented. Simulations and experimental results show that our approach allows a swarm of multiple heterogeneous robots to segregate into groups.
ER  - 

TY  - CONF
TI  - A Fast, Accurate, and Scalable Probabilistic Sample-Based Approach for Counting Swarm Size
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7180
EP  - 7185
AU  - H. Wang
AU  - M. Rubenstein
PY  - 2020
KW  - control engineering computing
KW  - distributed algorithms
KW  - multi-robot systems
KW  - particle swarm optimisation
KW  - path planning
KW  - counting swarm
KW  - distributed algorithm
KW  - neighboring robots
KW  - robot swarm
KW  - Robots
KW  - Estimation
KW  - Shape
KW  - Task analysis
KW  - Heuristic algorithms
KW  - Random variables
KW  - Clocks
DO  - 10.1109/ICRA40945.2020.9196529
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper describes a distributed algorithm for computing the number of robots in a swarm, only requiring communication with neighboring robots. The algorithm can adjust the estimated count when the number of robots in the swarm changes, such as the addition or removal of robots. Probabilistic guarantees are given, which show the accuracy of this method, and the trade-off between accuracy, speed, and adaptability to changing numbers. The proposed approach is demonstrated in simulation as well as a real swarm of robots.
ER  - 

TY  - CONF
TI  - Bayes Bots: Collective Bayesian Decision-Making in Decentralized Robot Swarms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7186
EP  - 7192
AU  - J. T. Ebert
AU  - M. Gauci
AU  - F. Mallmann-Trenn
AU  - R. Nagpal
PY  - 2020
KW  - Bayes methods
KW  - decision making
KW  - multi-robot systems
KW  - collective Bayesian decision-making
KW  - decentralized robot swarms
KW  - distributed Bayesian algorithm
KW  - spatially distributed feature
KW  - farm field
KW  - robotics
KW  - decentralized Bayesian algorithms
KW  - sparsely distributed robots
KW  - decision-making accuracy
KW  - bio-inspired positive feedback
KW  - fixed-time benchmark algorithm
KW  - Bayes bots
KW  - bio-inspired approaches
KW  - Robot sensing systems
KW  - Bayes methods
KW  - Decision making
KW  - Classification algorithms
KW  - Task analysis
KW  - Color
DO  - 10.1109/ICRA40945.2020.9196584
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a distributed Bayesian algorithm for robot swarms to classify a spatially distributed feature of an environment. This type of "go/no-go" decision appears in applications where a group of robots must collectively choose whether to take action, such as determining if a farm field should be treated for pests. Previous bio-inspired approaches to decentralized decision-making in robotics lack a statistical foundation, while decentralized Bayesian algorithms typically require a strongly connected network of robots. In contrast, our algorithm allows simple, sparsely distributed robots to quickly reach accurate decisions about a binary feature of their environment. We investigate the speed vs. accuracy tradeoff in decision-making by varying the algorithm's parameters. We show that making fewer, less-correlated observations can improve decision-making accuracy, and that a well-chosen combination of prior and decision threshold allows for fast decisions with a small accuracy cost. Both speed and accuracy also improved with the addition of bio-inspired positive feedback. This algorithm is also adaptable to the difficulty of the environment. Compared to a fixed-time benchmark algorithm with accuracy guarantees, our Bayesian approach resulted in equally accurate decisions, while adapting its decision time to the difficulty of the environment.
ER  - 

TY  - CONF
TI  - Supervisory Control of Robot Swarms Using Public Events
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7193
EP  - 7199
AU  - Y. K. Lopes
AU  - S. M. Trenkwalder
AU  - A. B. Leal
AU  - T. J. Dodd
AU  - R. Groß
PY  - 2020
KW  - discrete event systems
KW  - mobile robots
KW  - multi-robot systems
KW  - robot swarms
KW  - public events
KW  - supervisory control theory
KW  - formal framework
KW  - discrete event systems
KW  - correct-by-construction controllers
KW  - swarm robotics systems
KW  - extended SCT framework
KW  - mobile robots
KW  - e-puck robots
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Mobile robots
KW  - Generators
KW  - Supervisory control
DO  - 10.1109/ICRA40945.2020.9197418
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Supervisory Control Theory (SCT) provides a formal framework for controlling discrete event systems. It has recently been used to generate correct-by-construction controllers for swarm robotics systems. Current SCT frameworks are limited, as they support only (private) events that are observable within the same robot. In this paper, we propose an extended SCT framework that incorporates (public) events that are shared among robots. The extended framework allows to model formally the interactions among the robots. It is evaluated using a case study, where a group of mobile robots need to synchronise their movements in space and time-a requirement that is specified at the formal level. We validate our approach through experiments with groups of e-puck robots.
ER  - 

TY  - CONF
TI  - Automatic tool for Gazebo world construction: from a grayscale image to a 3D solid model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7226
EP  - 7232
AU  - B. Abbyasov
AU  - R. Lavrenov
AU  - A. Zakiev
AU  - K. Yakovlev
AU  - M. Svinin
AU  - E. Magid
PY  - 2020
KW  - control engineering computing
KW  - laser ranging
KW  - mobile robots
KW  - SLAM (robots)
KW  - solid modelling
KW  - Gazebo world construction
KW  - grayscale image
KW  - 3D solid model
KW  - robot simulators
KW  - simulated physical environment
KW  - 2D image
KW  - 2D laser range finder data
KW  - Gazebo simulator
KW  - 3D Collada
KW  - simultaneous localization and mapping
KW  - real-time factor
KW  - SLAM missions
KW  - RTF
KW  - Tools
KW  - Solid modeling
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Gray-scale
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196621
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot simulators provide an easy way for evaluation of new concepts and algorithms in a simulated physical environment reducing development time and cost. Therefore it is convenient to have a tool that quickly creates a 3D landscape from an arbitrary 2D image or 2D laser range finder data. This paper presents a new tool that automatically constructs such landscapes for Gazebo simulator. The tool converts a grayscale image into a 3D Collada format model, which could be directly imported into Gazebo. We run three different simultaneous localization and mapping (SLAM) algorithms within three varying complexity environments that were constructed with our tool. A real-time factor (RTF) was used as an efficiency benchmark. Successfully completed SLAM missions with acceptable RTF levels demonstrated the efficiency of the tool. The source code is available for free academic use.
ER  - 

TY  - CONF
TI  - A ROS Gazebo plugin to simulate ARVA sensors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7233
EP  - 7239
AU  - J. Cacace
AU  - N. Mimmo
AU  - L. Marconi
PY  - 2020
KW  - aerospace communication
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - operating systems (computers)
KW  - radio transceivers
KW  - rescue robots
KW  - robot programming
KW  - sensors
KW  - ROS Gazebo plugin
KW  - forefront technology
KW  - Search & Rescue operations
KW  - ARVA sensor simulation
KW  - transceiver sensor
KW  - Appareil de Recherche de Victims en Avalanche
KW  - Unmanned Aerial Vehicle
KW  - Receivers
KW  - Transmitters
KW  - Sensors
KW  - Electromagnetics
KW  - Antennas
KW  - Robots
KW  - Unmanned aerial vehicles
DO  - 10.1109/ICRA40945.2020.9196914
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the problem to simulate ARVA sensors using ROS and Gazebo. ARVA is a French acronym which stands for Appareil de Recherche de Victims en Avalanche and represents the forefront technology adopted in Search & Rescue operations to localize victims of avalanches buried under the snow. The aim of this paper is to describe the mathematical and theoretical background of the transceiver, discussing its implementation and integration with ROS allowing researchers to develop faster and smarter Search &Rescue strategies based on ARVA receiver data. To assess the effectiveness of the proposed sensor model, We present a simulation scenario in which an Unmanned Aerial Vehicle equipped with the transceiver sensor performs a basic S&R pattern using the output of ARVA system.
ER  - 

TY  - CONF
TI  - Is That a Chair? Imagining Affordances Using Simulations of an Articulated Human Body
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7240
EP  - 7246
AU  - H. Wu
AU  - D. Misra
AU  - G. S. Chirikjian
PY  - 2020
KW  - CAD
KW  - cameras
KW  - image classification
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - pose estimation
KW  - articulated human body
KW  - object affordances
KW  - physical interactions
KW  - physical simulations
KW  - arbitrarily oriented object
KW  - physical sitting interaction
KW  - object affordance reasoning
KW  - object classification
KW  - chair classification
KW  - appearance-based deep learning methods
KW  - affordances imagining
KW  - synthetic 3D CAD models
KW  - training data
KW  - functional pose predictions
KW  - depth camera
KW  - Robots
KW  - Solid modeling
KW  - Cognition
KW  - Physics
KW  - Three-dimensional displays
KW  - Data models
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9197384
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For robots to exhibit a high level of intelligence in the real world, they must be able to assess objects for which they have no prior knowledge. Therefore, it is crucial for robots to perceive object affordances by reasoning about physical interactions with the object. In this paper, we propose a novel method to provide robots with an ability to imagine object affordances using physical simulations. The class of chair is chosen here as an initial category of objects to illustrate a more general paradigm. In our method, the robot "imagines" the affordance of an arbitrarily oriented object as a chair by simulating a physical sitting interaction between an articulated human body and the object. This object affordance reasoning is used as a cue for object classification (chair vs non-chair). Moreover, if an object is classified as a chair, the affordance reasoning can also predict the upright pose of the object which allows the sitting interaction to take place. We call this type of poses the functional pose. We demonstrate our method in chair classification on synthetic 3D CAD models. Although our method uses only 30 models for training, it outperforms appearance-based deep learning methods, which require a large amount of training data, when the upright orientation is not assumed to be known a priori. In addition, we showcase that the functional pose predictions of our method align well with human judgments on both synthetic models and real objects scanned by a depth camera.
ER  - 

TY  - CONF
TI  - Toward Sim-to-Real Directional Semantic Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7247
EP  - 7253
AU  - S. Iqbal
AU  - J. Tremblay
AU  - A. Campbell
AU  - K. Leung
AU  - T. To
AU  - J. Cheng
AU  - E. Leitch
AU  - D. McKay
AU  - S. Birchfield
PY  - 2020
KW  - control engineering computing
KW  - end effectors
KW  - grippers
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - rendering (computer graphics)
KW  - robot vision
KW  - directional semantic grasping
KW  - deep reinforcement learning
KW  - double deep Q-network
KW  - robot simulator
KW  - rendering
KW  - monocular RGB images
KW  - wrist mounted camera
KW  - cartesian robot control
KW  - crossentropy method
KW  - domain randomization
KW  - end effector
KW  - Grippers
KW  - Grasping
KW  - Cameras
KW  - Training
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197310
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the problem of directional semantic grasping, that is, grasping a specific object from a specific direction. We approach the problem using deep reinforcement learning via a double deep Q-network (DDQN) that learns to map downsampled RGB input images from a wrist-mounted camera to Q-values, which are then translated into Cartesian robot control commands via the cross-entropy method (CEM). The network is learned entirely on simulated data generated by a custom robot simulator that models both physical reality (contacts) and perceptual quality (high-quality rendering). The reality gap is bridged using domain randomization. The system is an example of end-to-end (mapping input monocular RGB images to output Cartesian motor commands) grasping of objects from multiple pre-defined object-centric orientations, such as from the side or top. We show promising results in both simulation and the real world, along with some challenges faced and the need for future research in this area.
ER  - 

TY  - CONF
TI  - Inferring the Material Properties of Granular Media for Robotic Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2770
EP  - 2777
AU  - C. Matl
AU  - Y. Narang
AU  - R. Bajcsy
AU  - F. Ramos
AU  - D. Fox
PY  - 2020
KW  - Bayes methods
KW  - calibration
KW  - granular flow
KW  - granular materials
KW  - industrial robots
KW  - rolling friction
KW  - sliding friction
KW  - material properties
KW  - granular media
KW  - robotic tasks
KW  - cereal grains
KW  - plastic resin pellets
KW  - robotics-integrated industries
KW  - pharmaceutical development
KW  - accurate simulation
KW  - hardware framework
KW  - fast physics simulator
KW  - granular materials
KW  - real-world depth images
KW  - grain formations
KW  - likelihood-free Bayesian inference
KW  - calibrated simulator
KW  - unseen granular formations
KW  - simulator predictions
KW  - Robots
KW  - Friction
KW  - Numerical models
KW  - Bayes methods
KW  - Material properties
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197063
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Granular media (e.g., cereal grains, plastic resin pellets, and pills) are ubiquitous in robotics-integrated industries, such as agriculture, manufacturing, and pharmaceutical development. This prevalence mandates the accurate and efficient simulation of these materials. This work presents a software and hardware framework that automatically calibrates a fast physics simulator to accurately simulate granular materials by inferring material properties from real-world depth images of granular formations (i.e., piles and rings). Specifically, coefficients of sliding friction, rolling friction, and restitution of grains are estimated from summary statistics of grain formations using likelihood-free Bayesian inference. The calibrated simulator accurately predicts unseen granular formations in both simulation and experiment; furthermore, simulator predictions are shown to generalize to more complex tasks, including using a robot to pour grains into a bowl, as well as to create a desired pattern of piles and rings.
ER  - 

TY  - CONF
TI  - KETO: Learning Keypoint Representations for Tool Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7278
EP  - 7285
AU  - Z. Qin
AU  - K. Fang
AU  - Y. Zhu
AU  - L. Fei-Fei
AU  - S. Savarese
PY  - 2020
KW  - control engineering computing
KW  - image representation
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - KETO
KW  - tool manipulation
KW  - informative representation
KW  - task-specific keypoints
KW  - 3D point clouds
KW  - tool object
KW  - deep neural network
KW  - informative description
KW  - self-supervised robot interactions
KW  - task environment
KW  - manipulation tasks
KW  - task success rates
KW  - keypoint prediction
KW  - tool generation
KW  - learned representations
KW  - keypoint representation learning
KW  - Tools
KW  - Task analysis
KW  - Robots
KW  - Visualization
KW  - Force
KW  - Three-dimensional displays
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196971
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We aim to develop an algorithm for robots to manipulate novel objects as tools for completing different task goals. An efficient and informative representation would facilitate the effectiveness and generalization of such algorithms. For this purpose, we present KETO, a framework of learning keypoint representations of tool-based manipulation. For each task, a set of task-specific keypoints is jointly predicted from 3D point clouds of the tool object by a deep neural network. These keypoints offer a concise and informative description of the object to determine grasps and subsequent manipulation actions. The model is learned from self-supervised robot interactions in the task environment without the need for explicit human annotations. We evaluate our framework in three manipulation tasks with tool use. Our model consistently outperforms state-of-the-art methods in terms of task success rates. Qualitative results of keypoint prediction and tool generation are shown to visualize the learned representations.
ER  - 

TY  - CONF
TI  - Learning to See before Learning to Act: Visual Pre-training for Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7286
EP  - 7293
AU  - L. Yen-Chen
AU  - A. Zeng
AU  - S. Song
AU  - P. Isola
AU  - T. -Y. Lin
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object detection
KW  - robot vision
KW  - visual priors
KW  - vision-based manipulation
KW  - transfer learning
KW  - passive vision task
KW  - data distribution
KW  - active manipulation task
KW  - affordance maps
KW  - vision networks
KW  - zero-shot adaptation
KW  - zero robotic experience
KW  - visual pre-training
KW  - object detection
KW  - object manipulation
KW  - affordance prediction networks
KW  - Task analysis
KW  - Robots
KW  - Visualization
KW  - Predictive models
KW  - Grasping
KW  - Head
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9197331
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Does having visual priors (e.g. the ability to detect objects) facilitate learning to perform vision-based manipulation (e.g. picking up objects)? We study this problem under the framework of transfer learning, where the model is first trained on a passive vision task (i.e., the data distribution does not depend on the agent's decisions), then adapted to perform an active manipulation task (i.e., the data distribution does depend on the agent's decisions). We find that pre-training on vision tasks significantly improves generalization and sample efficiency for learning to manipulate objects. However, realizing these gains requires careful selection of which parts of the model to transfer. Our key insight is that outputs of standard vision models highly correlate with affordance maps commonly used in manipulation. Therefore, we explore directly transferring model parameters from vision networks to affordance prediction networks, and show that this can result in successful zero-shot adaptation, where a robot can pick up certain objects with zero robotic experience. With just a small amount of robotic experience, we can further fine-tune the affordance model to achieve better results. With just 10 minutes of suction experience or 1 hour of grasping experience, our method achieves ~ 80% success rate at picking up novel objects.
ER  - 

TY  - CONF
TI  - Contact-based in-hand pose estimation using Bayesian state estimation and particle filtering
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7294
EP  - 7299
AU  - F. von Drigalski
AU  - S. Taniguchi
AU  - R. Lee
AU  - T. Matsubara
AU  - M. Hamaya
AU  - K. Tanaka
AU  - Y. Ijiri
PY  - 2020
KW  - assembling
KW  - Bayes methods
KW  - calibration
KW  - force sensors
KW  - grippers
KW  - industrial manipulators
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - robot vision
KW  - robotic assembly
KW  - state estimation
KW  - Bayesian state estimation
KW  - particle filtering
KW  - industrial assembly tasks
KW  - force sensor
KW  - robotic gripper
KW  - rigid object
KW  - contact based inhand pose estimation
KW  - Pose estimation
KW  - Grippers
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Tactile sensors
DO  - 10.1109/ICRA40945.2020.9196640
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In industrial assembly tasks, the position of an object grasped by the robot has to be known with high precision in order to insert or place it. In real applications, this problem is commonly solved by jigs that are specially produced for each part. However, they significantly limit flexibility and are prohibitive when the target parts change often, so a flexible method to localize parts with high accuracy after grasping is desired. To solve this problem, we propose a method that can estimate the position of an object in the robot's hand to sub-millimeter precision, and can improve its estimate incrementally, using only minimal calibration and a force sensor. Our method is applicable to any robotic gripper and any rigid object that the gripper can hold, and requires only a force sensor. We demonstrate that the method can determine the position of an object to a precision of under 1 mm without using any part-specific jigs or equipment.
ER  - 

TY  - CONF
TI  - A Single Multi-Task Deep Neural Network with Post-Processing for Object Detection with Reasoning and Robotic Grasp Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7300
EP  - 7306
AU  - D. Park
AU  - Y. Seo
AU  - D. Shin
AU  - J. Choi
AU  - S. Y. Chun
PY  - 2020
KW  - humanoid robots
KW  - inference mechanisms
KW  - manipulators
KW  - neural nets
KW  - object detection
KW  - robot vision
KW  - robotic grasp detection
KW  - object detection
KW  - separate networks
KW  - target objects
KW  - single RGB-D camera
KW  - multitask DNN
KW  - accurate detections
KW  - relationship reasoning
KW  - state-of-the-art performance
KW  - object grasping tasks
KW  - humanoid robot
KW  - single multitask deep neural network
KW  - deep neural network based object
KW  - network output
KW  - high-level reasoning
KW  - VMRD
KW  - Cornell datasets
KW  - Robots
KW  - Grasping
KW  - Cognition
KW  - Task analysis
KW  - Neural networks
KW  - Grippers
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9197179
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Applications of deep neural network (DNN) based object and grasp detections could be expanded significantly when the network output is processed by a high-level reasoning over relationship of objects. Recently, robotic grasp detection and object detection with reasoning have been investigated using DNNs. There have been efforts to combine these multitasks using separate networks so that robots can deal with situations of grasping specific target objects in the cluttered, stacked, complex piles of novel objects from a single RGB-D camera. We propose a single multi-task DNN that yields accurate detections of objects, grasp position and relationship reasoning among objects. Our proposed methods yield state-of-the-art performance with the accuracy of 98.6% and 74.2% with the computation speed of 33 and 62 frame per second on VMRD and Cornell datasets, respectively. Our methods also yielded 95.3% grasp success rate for novel object grasping tasks with a 4-axis robot arm and 86.7% grasp success rate in cluttered novel objects with a humanoid robot.
ER  - 

TY  - CONF
TI  - Practical Persistence Reasoning in Visual SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7307
EP  - 7313
AU  - Z. Hashemifar
AU  - K. Dantu
PY  - 2020
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - static environments
KW  - dynamic environments
KW  - persistence filters
KW  - ORB-SLAM
KW  - visual SLAM algorithm
KW  - persistence filtering
KW  - persistence reasoning
KW  - semistatic environments
KW  - Simultaneous localization and mapping
KW  - Visualization
KW  - Estimation
KW  - Cognition
KW  - Probability
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196913
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many existing SLAM approaches rely on the assumption of static environments for accurate performance. However, several robot applications require them to traverse repeatedly in semi-static or dynamic environments. There has been some recent research interest in designing persistence filters to reason about persistence in such scenarios. Our goal in this work is to incorporate such persistence reasoning in visual SLAM. To this end, we incorporate persistence filters [1] into ORB-SLAM, a well-known visual SLAM algorithm. We observe that the simple integration of their proposal results in inefficient persistence reasoning. Through a series of modifications and using two locally collected datasets, we demonstrate the utility of such persistence filtering as well as our customizations in ORB-SLAM. Overall, incorporating persistence filtering could result in a significant reduction in map size (about 30% in the best case) and a corresponding reduction in run-time while retaining similar accuracy to methods that use much larger maps.
ER  - 

TY  - CONF
TI  - FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7322
EP  - 7328
AU  - T. Zhang
AU  - H. Zhang
AU  - Y. Li
AU  - Y. Nakamura
AU  - L. Zhang
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image motion analysis
KW  - image reconstruction
KW  - image segmentation
KW  - image sequences
KW  - mobile robots
KW  - motion estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - dynamic environments
KW  - visual SLAM
KW  - moving objects
KW  - static environment features
KW  - lead
KW  - wrong camera motion estimation
KW  - dense RGB-D SLAM solution
KW  - camera ego-motion estimation
KW  - static background reconstructions
KW  - optical flow residuals
KW  - dynamic semantics
KW  - RGB-D point clouds
KW  - camera tracking
KW  - background reconstruction
KW  - dense reconstruction results
KW  - dynamic scenes
KW  - static environments
KW  - dynamic dense RGB-D SLAM
KW  - Cameras
KW  - Dynamics
KW  - Optical imaging
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197349
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dynamic environments are challenging for visual SLAM since the moving objects occlude the static environment features and lead to wrong camera motion estimation. In this paper, we present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions. Our novelty is using optical flow residuals to highlight the dynamic semantics in the RGB-D point clouds and provide more accurate and efficient dynamic/static segmentation for camera tracking and background reconstruction. The dense reconstruction results on public datasets and real dynamic scenes indicate that the proposed approach achieved accurate and efficient performances in both dynamic and static environments compared to state-of-the-art approaches.
ER  - 

TY  - CONF
TI  - Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7344
EP  - 7350
AU  - R. Michelmore
AU  - M. Wicker
AU  - L. Laurenti
AU  - L. Cardelli
AU  - Y. Gal
AU  - M. Kwiatkowska
PY  - 2020
KW  - Bayes methods
KW  - belief networks
KW  - collision avoidance
KW  - decision making
KW  - inference mechanisms
KW  - neurocontrollers
KW  - probability
KW  - road safety
KW  - Bayesian inference methods
KW  - uncertainty computation
KW  - pointwise uncertainty measures
KW  - end-to-end Bayesian controllers
KW  - autonomous driving scenarios
KW  - Bayesian neural networks
KW  - sensor noise
KW  - controller behaviour
KW  - safety guarantees
KW  - neural network controllers
KW  - end-to-end autonomous driving control
KW  - statistical guarantees
KW  - uncertainty quantification
KW  - Uncertainty
KW  - Safety
KW  - Autonomous vehicles
KW  - Neural networks
KW  - Automobiles
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196844
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep neural network controllers for autonomous driving have recently benefited from significant performance improvements, and have begun deployment in the real world. Prior to their widespread adoption, safety guarantees are needed on the controller behaviour that properly take account of the uncertainty within the model as well as sensor noise. Bayesian neural networks, which assume a prior over the weights, have been shown capable of producing such uncertainty measures, but properties surrounding their safety have not yet been quantified for use in autonomous driving scenarios. In this paper, we develop a framework based on a state-of-the-art simulator for evaluating end-to-end Bayesian controllers. In addition to computing pointwise uncertainty measures that can be computed in real time and with statistical guarantees, we also provide a method for estimating the probability that, given a scenario, the controller keeps the car safe within a finite horizon. We experimentally evaluate the quality of uncertainty computation by three Bayesian inference methods in different scenarios and show how the uncertainty measures can be combined and calibrated for use in collision avoidance. Our results suggest that uncertainty estimates can greatly aid decision making in autonomous driving.
ER  - 

TY  - CONF
TI  - Autonomously Navigating a Surgical Tool Inside the Eye by Learning from Demonstration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7351
EP  - 7357
AU  - J. W. Kim
AU  - C. He
AU  - M. Urias
AU  - P. Gehlbach
AU  - G. D. Hager
AU  - I. Iordachita
AU  - M. Kobilarov
PY  - 2020
KW  - end effectors
KW  - eye
KW  - learning by example
KW  - medical robotics
KW  - navigation
KW  - position control
KW  - robot programming
KW  - surgery
KW  - visual servoing
KW  - retinal surgery
KW  - auditory feedback
KW  - autonomous navigation system
KW  - needle surgical tool navigation
KW  - learning from demonstration
KW  - haptic feedback
KW  - deep network training
KW  - visual servoing
KW  - steady hand eye robot
KW  - SHER surgical robot
KW  - end effector
KW  - Tools
KW  - Retina
KW  - Surgery
KW  - Navigation
KW  - Task analysis
KW  - Trajectory
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196537
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A fundamental challenge in retinal surgery is safely navigating a surgical tool to a desired goal position on the retinal surface while avoiding damage to surrounding tissues, a procedure that typically requires tens-of-microns accuracy. In practice, the surgeon relies on depth-estimation skills to localize the tool-tip with respect to the retina in order to perform the tool-navigation task, which can be prone to human error. To alleviate such uncertainty, prior work has introduced ways to assist the surgeon by estimating the tooltip distance to the retina and providing haptic or auditory feedback. However, automating the tool-navigation task itself remains unsolved and largely unexplored. Such a capability, if reliably automated, could serve as a building block to streamline complex procedures and reduce the chance for tissue damage. Towards this end, we propose to automate the tool-navigation task by learning to mimic expert demonstrations of the task. Specifically, a deep network is trained to imitate expert trajectories toward various locations on the retina based on recorded visual servoing to a given goal specified by the user. The proposed autonomous navigation system is evaluated in simulation and in physical experiments using a silicone eye phantom. We show that the network can reliably navigate a needle surgical tool to various desired locations within 137 μm accuracy in physical experiments and 94 μm in simulation on average, and generalizes well to unseen situations such as in the presence of auxiliary surgical tools, variable eye backgrounds, and brightness conditions.
ER  - 


