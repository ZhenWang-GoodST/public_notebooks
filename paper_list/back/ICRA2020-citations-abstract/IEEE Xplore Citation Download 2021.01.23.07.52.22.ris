TY  - CONF
TI  - Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4051
EP  - 4058
AU  - R. Li
AU  - A. Jabri
AU  - T. Darrell
AU  - P. Agrawal
PY  - 2020
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - multiobject manipulation
KW  - relational reinforcement learning
KW  - learning robotic manipulation tasks
KW  - outrageous data requirements
KW  - task curriculum
KW  - graph-based relational architectures
KW  - simulated block stacking task
KW  - step-wise sparse rewards
KW  - zero-shot generalization
KW  - Task analysis
KW  - Stacking
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Poles and towers
KW  - Training
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197468
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning robotic manipulation tasks using reinforcement learning with sparse rewards is currently impractical due to the outrageous data requirements. Many practical tasks require manipulation of multiple objects, and the complexity of such tasks increases with the number of objects. Learning from a curriculum of increasingly complex tasks appears to be a natural solution, but unfortunately, does not work for many scenarios. We hypothesize that the inability of the state- of-the-art algorithms to effectively utilize a task curriculum stems from the absence of inductive biases for transferring knowledge from simpler to complex tasks. We show that graph-based relational architectures overcome this limitation and enable learning of complex tasks when provided with a simple curriculum of tasks with increasing numbers of objects. We demonstrate the utility of our framework on a simulated block stacking task. Starting from scratch, our agent learns to stack six blocks into a tower. Despite using step-wise sparse rewards, our method is orders of magnitude more data- efficient and outperforms the existing state-of-the-art method that utilizes human demonstrations. Furthermore, the learned policy exhibits zero-shot generalization, successfully stacking blocks into taller towers and previously unseen configurations such as pyramids, without any further training.
ER  - 

TY  - CONF
TI  - SwarmMesh: A Distributed Data Structure for Cooperative Multi-Robot Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4059
EP  - 4065
AU  - N. Majcherczyk
AU  - C. Pinciroli
PY  - 2020
KW  - control engineering computing
KW  - data handling
KW  - data structures
KW  - distributed processing
KW  - mobile robots
KW  - multi-robot systems
KW  - storage management
KW  - topology
KW  - data item position
KW  - near-perfect data retention
KW  - distributed data structure
KW  - cooperative multirobot applications
KW  - distributed storage
KW  - mobile robots
KW  - shared global memory
KW  - external storage infrastructure
KW  - swarm topology
KW  - data storage
KW  - SwarmMesh
KW  - data type
KW  - Robots
KW  - Data structures
KW  - Peer-to-peer computing
KW  - Overlay networks
KW  - Routing
KW  - Distributed databases
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9197403
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an approach to the distributed storage of data across a swarm of mobile robots that forms a shared global memory. We assume that external storage infrastructure is absent, and that each robot is capable of devoting a quota of memory and bandwidth to distributed storage. Our approach is motivated by the insight that in many applications data is collected at the periphery of a swarm topology, but the periphery also happens to be the most dangerous location for storing data, especially in exploration missions. Our approach is designed to promote data storage in the locations in the swarm that best suit a specific feature of interest in the data, while accounting for the constantly changing topology due to individual motion. We analyze two possible features of interest: the data type and the data item position in the environment. We assess the performance of our approach in a large set of simulated experiments. The evaluation shows that our approach is capable of storing quantities of data that exceed the memory of individual robots, while maintaining near-perfect data retention in high-load conditions.
ER  - 

TY  - CONF
TI  - Avalanche victim search via robust observers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4066
EP  - 4072
AU  - N. Mimmo
AU  - P. Bernard
AU  - L. Marconi
PY  - 2020
KW  - adaptive control
KW  - autonomous aerial vehicles
KW  - emergency management
KW  - multi-robot systems
KW  - observers
KW  - rescue robots
KW  - robust control
KW  - sensor fusion
KW  - avalanche victim search
KW  - robust observers
KW  - victim localization
KW  - ARVA sensor
KW  - adaptive control
KW  - UAVs
KW  - least square identifier
KW  - Receivers
KW  - Transmitters
KW  - Drones
KW  - Observers
KW  - Trajectory
KW  - Electromagnetics
KW  - Adaptive control
KW  - Search and Rescue
KW  - Robust Control
DO  - 10.1109/ICRA40945.2020.9196646
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a new approach for victim localization in avalanches that will be exploited by UAVs using the ARVA sensor. We show that the nominal ARVA measurement can be linearly related to a quantity that is sufficient to reconstruct the victim position. We explicitly deal with a robust scenario in which the measurement is actually perturbed by a noise that grows with the distance to the victim and we propose an adaptive control scheme made of a least-square identifier and a trajectory generator whose role is both to guarantee the persistence of excitation for the identifier and to steer the ARVA receiver towards the victim. We show that the system succeeds in localizing the victim in a domain where the ARVA output is sufficiently informative and illustrate its performance in simulation. This new approach could significantly reduce the searching time by providing an exploitable estimate before having reached the victim. The work is framed within the EU project AirBorne whose goals is to develop at TRL8 a drone for quick localization of victims in avalanche scenarios.
ER  - 

TY  - CONF
TI  - Reactive Control and Metric-Topological Planning for Exploration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4073
EP  - 4079
AU  - M. T. Ohradzansky
AU  - A. B. Mills
AU  - E. R. Rush
AU  - D. G. Riley
AU  - E. W. Frew
AU  - J. Sean Humbert
PY  - 2020
KW  - collision avoidance
KW  - graph theory
KW  - image sequences
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - optic flow
KW  - insect visuomotor system
KW  - obstacle detection
KW  - topological graph
KW  - metric-topological planning
KW  - autonomous navigation
KW  - robotic platforms
KW  - bio-inspired reactive control
KW  - spatial decomposition
KW  - obstacle avoidance
KW  - Fourier residual analysis
KW  - image processing
KW  - continuous occupancy grid
KW  - graph edge
KW  - Optical feedback
KW  - Optical sensors
KW  - Optical imaging
KW  - Planning
KW  - Navigation
KW  - Harmonic analysis
KW  - Mathematical model
KW  - exploration
KW  - centering
KW  - control
KW  - mapping
DO  - 10.1109/ICRA40945.2020.9197381
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous navigation in unknown environments with the intent of exploring all traversable areas is a significant challenge for robotic platforms. In this paper, a simple yet reliable method for exploring unknown environments is presented based on bio-inspired reactive control and metric-topological planning. The reactive control algorithm is modeled after the spatial decomposition of wide and small-field patterns of optic flow in the insect visuomotor system. Centering behaviour and small obstacle detection and avoidance are achieved through wide-field integration and Fourier residual analysis of instantaneous measured nearness respectively. A topological graph is estimated using image processing techniques on a continuous occupancy grid. Node paths are rapidly generated to navigate to the nearest unexplored edge in the graph. It is shown through rigorous field-testing that the proposed control and planning method is robust, reliable, and computationally efficient.
ER  - 

TY  - CONF
TI  - Information Theoretic Active Exploration in Signed Distance Fields
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4080
EP  - 4085
AU  - K. Saulnier
AU  - N. Atanasov
AU  - G. J. Pappas
AU  - V. Kumar
PY  - 2020
KW  - deterministic algorithms
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - sensors
KW  - tree searching
KW  - robot sensing trajectories
KW  - autonomous TSDF mapping
KW  - TSDF uncertainty
KW  - sensor measurements
KW  - efficient uncertainty prediction
KW  - long-horizon optimization
KW  - deterministic tree-search algorithm
KW  - information gain
KW  - TSDF distribution
KW  - efficient planning
KW  - uninformative sensing trajectories
KW  - active TSDF mapping approach
KW  - simulated environments
KW  - information theoretic active exploration
KW  - occupancy mapping
KW  - mobile robot
KW  - truncated signed distance field
KW  - robot motion primitive sequences
KW  - branch-and-bound pruning
KW  - complex visibility constraints
KW  - Robot sensing systems
KW  - Trajectory
KW  - Measurement uncertainty
KW  - Standards
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9196882
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on exploration and occupancy mapping of unknown environments using a mobile robot. While a truncated signed distance field (TSDF) is a popular, efficient, and highly accurate representation of occupancy, few works have considered optimizing robot sensing trajectories for autonomous TSDF mapping. We propose an efficient approach for maintaining TSDF uncertainty and predicting its evolution from potential future sensor measurements without actually receiving them. Efficient uncertainty prediction is critical for long-horizon optimization of potential sensing trajectories. We develop a deterministic tree-search algorithm that evaluates the information gain between the TSDF distribution and potential observations along sequences of robot motion primitives. Efficient planning is achieved by branch-and-bound pruning of uninformative sensing trajectories. The effectiveness of our active TSDF mapping approach is evaluated in several simulated environments with complex visibility constraints.
ER  - 

TY  - CONF
TI  - Bayesian Learning-Based Adaptive Control for Safety Critical Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4093
EP  - 4099
AU  - D. D. Fan
AU  - J. Nguyen
AU  - R. Thakker
AU  - N. Alatur
AU  - A. -a. Agha-mohammadi
AU  - E. A. Theodorou
PY  - 2020
KW  - adaptive control
KW  - Bayes methods
KW  - belief networks
KW  - control engineering computing
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - Lyapunov methods
KW  - Markov processes
KW  - neural nets
KW  - probability
KW  - safety-critical software
KW  - stability
KW  - real-time performance
KW  - deep neural networks
KW  - model uncertainties
KW  - Bayesian model learning
KW  - adaptive control framework
KW  - control Lyapunov functions
KW  - control barrier functions
KW  - tractable Bayesian model
KW  - safety-critical high-speed Mars rover missions
KW  - Bayesian learning-based adaptive control
KW  - deep learning
KW  - safety-critical systems
KW  - high-speed terrestrial mobility
KW  - Safety
KW  - Adaptation models
KW  - Bayes methods
KW  - Stochastic processes
KW  - Uncertainty
KW  - Computational modeling
KW  - Switches
KW  - Robust/Adaptive Control of Robotic Systems
KW  - Robot Safety
KW  - Probability and Statistical Methods
KW  - Bayesian Adaptive Control
KW  - Deep Learning
KW  - Mars Rover
DO  - 10.1109/ICRA40945.2020.9196709
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep learning has enjoyed much recent success, and applying state-of-the-art model learning methods to controls is an exciting prospect. However, there is a strong reluctance to use these methods on safety-critical systems, which have constraints on safety, stability, and real-time performance. We propose a framework which satisfies these constraints while allowing the use of deep neural networks for learning model uncertainties. Central to our method is the use of Bayesian model learning, which provides an avenue for maintaining appropriate degrees of caution in the face of the unknown. In the proposed approach, we develop an adaptive control framework leveraging the theory of stochastic CLFs (Control Lyapunov Functions) and stochastic CBFs (Control Barrier Functions) along with tractable Bayesian model learning via Gaussian Processes or Bayesian neural networks. Under reasonable assumptions, we guarantee stability and safety while adapting to unknown dynamics with probability 1. We demonstrate this architecture for high-speed terrestrial mobility targeting potential applications in safety-critical high-speed Mars rover missions.
ER  - 

TY  - CONF
TI  - Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4158
EP  - 4164
AU  - M. Ramezani
AU  - G. Tinchev
AU  - E. Iuganov
AU  - M. Fallon
PY  - 2020
KW  - feature extraction
KW  - graph theory
KW  - image matching
KW  - image registration
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - neural nets
KW  - optical radar
KW  - optimisation
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - online LiDAR-SLAM
KW  - legged robot
KW  - robust registration
KW  - deep-learned loop closure
KW  - 3D factor-graph LiDAR-SLAM system
KW  - industrial environments
KW  - point clouds
KW  - inertial-kinematic state estimator
KW  - ICP registration
KW  - loop proposal mechanism
KW  - deep learning method
KW  - odometry
KW  - loop closure factors
KW  - pose graph optimization
KW  - SLAM map
KW  - risk alignment prediction method
KW  - deeply learned feature-based loop closure detector
KW  - Laser radar
KW  - Legged locomotion
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Iterative closest point algorithm
DO  - 10.1109/ICRA40945.2020.9196769
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.
ER  - 

TY  - CONF
TI  - Voxel Map for Visual SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4181
EP  - 4187
AU  - M. Muglikar
AU  - Z. Zhang
AU  - D. Scaramuzza
PY  - 2020
KW  - computer vision
KW  - feature extraction
KW  - image representation
KW  - image retrieval
KW  - SLAM (robots)
KW  - visual SLAM systems
KW  - camera field-of-view
KW  - voxel map representation
KW  - keyframe map
KW  - map points retrieval
KW  - simultaneous localization and mapping
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Cameras
KW  - Visualization
KW  - Cognition
KW  - Feature extraction
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197357
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multi-camera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable.
ER  - 

TY  - CONF
TI  - Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4188
EP  - 4194
AU  - O. Mees
AU  - M. Merklinger
AU  - G. Kalweit
AU  - W. Burgard
PY  - 2020
KW  - interactive video
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - video signal processing
KW  - adversarial skill-transfer loss
KW  - task domain
KW  - learned skill embeddings
KW  - entropy-regularized adversarial skill-transfer loss
KW  - temporal video coherence
KW  - metric learning loss
KW  - adversarial loss
KW  - task context
KW  - unlabeled multiview videos
KW  - task-agnostic skill embedding space
KW  - reinforcement learning agents
KW  - unsupervised robot skill learning
KW  - adversarial skill networks
KW  - learned embedding
KW  - Task analysis
KW  - Entropy
KW  - Measurement
KW  - Training
KW  - Robots
KW  - Interpolation
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9196582
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.
ER  - 

TY  - CONF
TI  - Event-Based Angular Velocity Regression with Spiking Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4195
EP  - 4202
AU  - M. Gehrig
AU  - S. B. Shrestha
AU  - D. Mouritzen
AU  - D. Scaramuzza
PY  - 2020
KW  - angular velocity
KW  - image resolution
KW  - image sensors
KW  - neural nets
KW  - regression analysis
KW  - temporal regression problem
KW  - rotating event-camera
KW  - SNN
KW  - irregular event-based input
KW  - asynchronous event-based input
KW  - high-temporal resolution
KW  - synthetic event-camera
KW  - event-based angular velocity regression
KW  - spiking networks
KW  - temporal spikes
KW  - bio-inspired networks
KW  - brightness change
KW  - spiking neural networks
KW  - spike-based computational model
KW  - asynchronous sensors
KW  - artificial neural networks
KW  - 3-DOF angular velocity
KW  - Neurons
KW  - Biological neural networks
KW  - Angular velocity
KW  - Kernel
KW  - Task analysis
KW  - Computer architecture
KW  - Neuromorphics
DO  - 10.1109/ICRA40945.2020.9197133
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Spiking Neural Networks (SNNs) are bio-inspired networks that process information conveyed as temporal spikes rather than numeric values. An example of a sensor providing such data is the event-camera. It only produces an event when a pixel reports a significant brightness change. Similarly, the spiking neuron of an SNN only produces a spike whenever a significant number of spikes occur within a short period of time. Due to their spike-based computational model, SNNs can process output from event-based, asynchronous sensors without any pre-processing at extremely lower power unlike standard artificial neural networks. This is possible due to specialized neuromorphic hardware that implements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have not enjoyed the same rise of popularity as artificial neural networks. This not only stems from the fact that their input format is rather unconventional but also due to the challenges in training spiking networks. Despite their temporal nature and recent algorithmic advances, they have been mostly evaluated on classification problems. We propose, for the first time, a temporal regression problem of numerical values given events from an event-camera. We specifically investigate the prediction of the 3- DOF angular velocity of a rotating event-camera with an SNN. The difficulty of this problem arises from the prediction of angular velocities continuously in time directly from irregular, asynchronous event-based input. Directly utilising the output of event-cameras without any pre-processing ensures that we inherit all the benefits that they provide over conventional cameras. That is high-temporal resolution, high-dynamic range and no motion blur. To assess the performance of SNNs on this task, we introduce a synthetic event-camera dataset generated from real-world panoramic images and show that we can successfully train an SNN to perform angular velocity regression.
ER  - 

TY  - CONF
TI  - Visual Odometry Revisited: What Should Be Learnt?
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4203
EP  - 4210
AU  - H. Zhan
AU  - C. S. Weerasekera
AU  - J. -W. Bian
AU  - I. Reid
PY  - 2020
KW  - convolutional neural nets
KW  - distance measurement
KW  - geometry
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - SLAM (robots)
KW  - monocular visual odometry algorithm
KW  - geometry-based methods
KW  - monocular systems
KW  - scale-drift issue
KW  - deep learning
KW  - epipolar geometry
KW  - perspective-n-point method
KW  - frame-to-frame VO algorithm
KW  - DF-VO
KW  - scale consistent single-view depth CNN
KW  - Cameras
KW  - Adaptive optics
KW  - Geometry
KW  - Optical imaging
KW  - Machine learning
KW  - Estimation
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197374
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue. Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system. Code is available at here: DF-VO.
ER  - 

TY  - CONF
TI  - 3D Scene Geometry-Aware Constraint for Camera Localization with Deep Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4211
EP  - 4217
AU  - M. Tian
AU  - Q. Nie
AU  - H. Shen
PY  - 2020
KW  - cameras
KW  - convolutional neural nets
KW  - feature extraction
KW  - image classification
KW  - image motion analysis
KW  - image reconstruction
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - pose estimation
KW  - 3D scene geometry-aware constraint
KW  - camera localization
KW  - deep learning
KW  - fundamental component
KW  - autonomous driving vehicles
KW  - path planning
KW  - motion control
KW  - end-to-end approaches
KW  - convolutional neural network
KW  - 3D-geometry based traditional methods
KW  - compact network
KW  - absolute camera
KW  - image contents
KW  - image-level structural similarity loss
KW  - challenging scenes
KW  - Cameras
KW  - Three-dimensional displays
KW  - Training
KW  - Geometry
KW  - Robot vision systems
KW  - Transforms
DO  - 10.1109/ICRA40945.2020.9196940
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Camera localization is a fundamental and key component of autonomous driving vehicles and mobile robots to localize themselves globally for further environment perception, path planning and motion control. Recently end-to-end approaches based on convolutional neural network have been much studied to achieve or even exceed 3D-geometry based traditional methods. In this work, we propose a compact network for absolute camera pose regression. Inspired from those traditional methods, a 3D scene geometry-aware constraint is also introduced by exploiting all available information including motion, depth and image contents. We add this constraint as a regularization term to our proposed network by defining a pixel-level photometric loss and an image-level structural similarity loss. To benchmark our method, different challenging scenes including indoor and outdoor environment are tested with our proposed approach and state-of-the-arts. And the experimental results demonstrate significant performance improvement of our method on both prediction accuracy and convergence efficiency.
ER  - 

TY  - CONF
TI  - ACDER: Augmented Curiosity-Driven Experience Replay
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4218
EP  - 4224
AU  - B. Li
AU  - T. Lu
AU  - J. Li
AU  - N. Lu
AU  - Y. Cai
AU  - S. Wang
PY  - 2020
KW  - augmented reality
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - manipulators
KW  - augmented curiosity-driven experience replay
KW  - hindsight experience replay
KW  - sample-efficiency
KW  - automatic exploratory curriculum
KW  - dynamic initial states selection
KW  - task-relevant states
KW  - goal-oriented curiosity-driven exploration
KW  - action space
KW  - high dimensional continuous state
KW  - low exploration efficiency
KW  - RL agent
KW  - reinforcement learning
KW  - sparse feed-back
KW  - ACDER
KW  - multistep robotic task learning
KW  - basic tasks
KW  - challenging robotic manipulation tasks
KW  - valuable states
KW  - Task analysis
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Training
KW  - Incentive schemes
KW  - Buffer storage
KW  - Games
DO  - 10.1109/ICRA40945.2020.9197421
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exploration in environments with sparse feed-back remains a challenging research problem in reinforcement learning (RL). When the RL agent explores the environment randomly, it results in low exploration efficiency, especially in robotic manipulation tasks with high dimensional continuous state and action space. In this paper, we propose a novel method, called Augmented Curiosity-Driven Experience Replay (ACDER), which leverages (i) a new goal-oriented curiosity-driven exploration to encourage the agent to pursue novel and task-relevant states more purposefully and (ii) the dynamic initial states selection as an automatic exploratory curriculum to further improve the sample-efficiency. Our approach complements Hindsight Experience Replay (HER) by introducing a new way to pursue valuable states. Experiments conducted on four challenging robotic manipulation tasks with binary rewards, including Reach, Push, Pick&Place and Multi-step Push. The empirical results show that our proposed method significantly outperforms existing methods in the first three basic tasks and also achieves satisfactory performance in multi-step robotic task learning.
ER  - 

TY  - CONF
TI  - TrueRMA: Learning Fast and Smooth Robot Trajectories with Recursive Midpoint Adaptations in Cartesian Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4225
EP  - 4231
AU  - J. C. Kiemel
AU  - P. Meißner
AU  - T. Kröger
PY  - 2020
KW  - learning (artificial intelligence)
KW  - motion control
KW  - neurocontrollers
KW  - robot kinematics
KW  - time optimal control
KW  - trajectory control
KW  - TrueRMA
KW  - robot trajectory learning
KW  - recursive midpoint adaptations
KW  - Cartesian space
KW  - differentiable path
KW  - inverse kinematics
KW  - time optimal parameterization
KW  - reinforcement learning
KW  - robot movement
KW  - neural network training
KW  - KUKA iiwa robot
KW  - Trajectory
KW  - Robots
KW  - Kinematics
KW  - Task analysis
KW  - Training
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196711
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present TrueRMA, a data-efficient, model-free method to learn cost-optimized robot trajectories over a wide range of starting points and endpoints. The key idea is to calculate trajectory waypoints in Cartesian space by recursively predicting orthogonal adaptations relative to the midpoints of straight lines. We generate a differentiable path by adding circular blends around the waypoints, calculate the corresponding joint positions with an inverse kinematics solver and calculate a time-optimal parameterization considering velocity and acceleration limits. During training, the trajectory is executed in a physics simulator and costs are assigned according to a user-specified cost function which is not required to be differentiable. Given a starting point and an endpoint as input, a neural network is trained to predict midpoint adaptations that minimize the cost of the resulting trajectory via reinforcement learning. We successfully train a KUKA iiwa robot to keep a ball on a plate while moving between specified points and compare the performance of TrueRMA against two baselines. The results show that our method requires less training data to learn the task while generating shorter and faster trajectories.
ER  - 

TY  - CONF
TI  - Fog Robotics Algorithms for Distributed Motion Planning Using Lambda Serverless Computing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4232
EP  - 4238
AU  - J. Ichnowski
AU  - W. Lee
AU  - V. Murta
AU  - S. Paradis
AU  - R. Alterovitz
AU  - J. E. Gonzalez
AU  - I. Stoica
AU  - K. Goldberg
PY  - 2020
KW  - computational complexity
KW  - control engineering computing
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - parallel processing
KW  - path planning
KW  - distributed motion planning
KW  - Lambda serverless computing
KW  - motion planning algorithms
KW  - RRT
KW  - computational load
KW  - local environment changes
KW  - cloud-based serverless lambda computing
KW  - parallel computation
KW  - serverless computers
KW  - learned parallel allocation
KW  - Amazon Lambda
KW  - sporadically computationally intensive motion planning tasks
KW  - fog robotics algorithms
KW  - Planning
KW  - Parallel processing
KW  - FAA
KW  - Robot kinematics
KW  - Cloud computing
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196651
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For robots using motion planning algorithms such as RRT and RRT*, the computational load can vary by orders of magnitude as the complexity of the local environment changes. To adaptively provide such computation, we propose Fog Robotics algorithms in which cloud-based serverless lambda computing provides parallel computation on demand. To use this parallelism, we propose novel motion planning algorithms that scale effectively with an increasing number of serverless computers. However, given that the allocation of computing is typically bounded by both monetary and time constraints, we show how prior learning can be used to efficiently allocate resources at runtime. We demonstrate the algorithms and application of learned parallel allocation in both simulation and with the Fetch commercial mobile manipulator using Amazon Lambda to complete a sequence of sporadically computationally intensive motion planning tasks.
ER  - 

TY  - CONF
TI  - Exploration of 3D terrains using potential fields with elevation-based local distortions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4239
EP  - 4244
AU  - R. Maffei
AU  - M. P. Souza
AU  - M. Mantelli
AU  - D. Pittol
AU  - M. Kolberg
AU  - V. A. M. Jorge
PY  - 2020
KW  - mobile robots
KW  - optical radar
KW  - radar receivers
KW  - stereo image processing
KW  - terrain mapping
KW  - exploration approach
KW  - potential fields
KW  - uneven terrains
KW  - high declivity regions
KW  - ground robot
KW  - elevation-based local distortions
KW  - mobile robots
KW  - numerous outdoor tasks
KW  - military applications
KW  - 3D terrain exploration
KW  - patrolling application
KW  - delivery application
KW  - 2D LIDAR sensors
KW  - Distortion
KW  - Robot sensing systems
KW  - Three-dimensional displays
KW  - Boundary conditions
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197577
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mobile robots can be used in numerous outdoor tasks such as patrolling, delivery and military applications. In order to deploy mobile robots in this kind of environment, where there are different challenges like slopes, elevations, or even holes, they should be able to detect such challenges and determine the best path to accomplish their tasks. In this paper, we are proposing an exploration approach based on potential fields with local distortions, in which we define preferences in uneven terrains to avoid high declivity regions without compromising the best path. The approach was implemented and tested in simulated environments, considering a ground robot embedded with two 2D LIDAR sensors, and the experiments demonstrated the efficiency of our method.
ER  - 

TY  - CONF
TI  - R3T: Rapidly-exploring Random Reachable Set Tree for Optimal Kinodynamic Planning of Nonlinear Hybrid Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4245
EP  - 4251
AU  - A. Wu
AU  - S. Sadraddini
AU  - R. Tedrake
PY  - 2020
KW  - control engineering computing
KW  - formal verification
KW  - nonlinear control systems
KW  - reachability analysis
KW  - robot dynamics
KW  - sampling methods
KW  - trees (mathematics)
KW  - R3T
KW  - random reachable set tree
KW  - optimal kinodynamic planning
KW  - nonlinear hybrid systems
KW  - reachability-based variant
KW  - rapidly-exploring random tree
KW  - multiple polytopes
KW  - reachability analysis
KW  - nonlinear systems
KW  - contact-rich robotic systems
KW  - formal verification
KW  - Planning
KW  - Approximation algorithms
KW  - Trajectory
KW  - Heuristic algorithms
KW  - Measurement
KW  - Robots
KW  - Silicon
DO  - 10.1109/ICRA40945.2020.9196802
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce R3T, a reachability-based variant of the rapidly-exploring random tree (RRT) algorithm that is suitable for (optimal) kinodynamic planning in nonlinear and hybrid systems. We developed tools to approximate reachable sets using polytopes and perform sampling-based planning with them. This method has a unique advantage in hybrid systems: different dynamic modes in the reachable set can be explicitly represented using multiple polytopes. We prove that under mild assumptions, R3T is probabilistically complete in kinodynamic systems, and asymptotically optimal through rewiring. Moreover, R3T provides a formal verification method for reachability analysis of nonlinear systems. The advantages of R3T are demonstrated with case studies on nonlinear, hybrid, and contact-rich robotic systems.
ER  - 

TY  - CONF
TI  - DeepSemanticHPPC: Hypothesis-based Planning over Uncertain Semantic Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4252
EP  - 4258
AU  - Y. Han
AU  - H. Lin
AU  - J. Banfi
AU  - K. Bala
AU  - M. Campbell
PY  - 2020
KW  - belief networks
KW  - computational geometry
KW  - image reconstruction
KW  - mobile robots
KW  - neural nets
KW  - path planning
KW  - robot vision
KW  - DeepSemanticHPPC
KW  - hypothesis-based planning
KW  - uncertain semantic point clouds
KW  - deep Bayesian neural network
KW  - flexible point cloud scene representation
KW  - sparse visual measurements
KW  - hypothesis-based path planner
KW  - uncertainty-aware hypothesis-based planner
KW  - Uncertainty
KW  - Three-dimensional displays
KW  - Trajectory
KW  - Semantics
KW  - Robots
KW  - Planning
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196828
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planning in unstructured environments is challenging - it relies on sensing, perception, scene reconstruction, and reasoning about various uncertainties. We propose DeepSemanticHPPC, a novel uncertainty-aware hypothesis-based planner for unstructured environments. Our algorithmic pipeline consists of: a deep Bayesian neural network which segments surfaces with uncertainty estimates; a flexible point cloud scene representation; a next-best-view planner which minimizes the uncertainty of scene semantics using sparse visual measurements; and a hypothesis-based path planner that proposes multiple kinematically feasible paths with evolving safety confidences given next-best-view measurements. Our pipeline iteratively decreases semantic uncertainty along planned paths, filtering out unsafe paths with high confidence. We show that our framework plans safe paths in real-world environments where existing path planners typically fail.
ER  - 

TY  - CONF
TI  - Balancing Actuation and Computing Energy in Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4259
EP  - 4265
AU  - S. Sudhakar
AU  - S. Karaman
AU  - V. Sze
PY  - 2020
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - balancing actuation
KW  - motion planning problems
KW  - low-energy robotic vehicles
KW  - high-endurance autonomous blimps
KW  - computing hardware
KW  - actuation hardware
KW  - CEIMP
KW  - anytime planning algorithm
KW  - actuation energy
KW  - asymptotic computational complexity
KW  - sampling-based motion planning algorithms
KW  - compute energy included motion planning algorithm
KW  - Planning
KW  - Robots
KW  - Meters
KW  - Task analysis
KW  - Hardware
KW  - Buildings
KW  - Space exploration
DO  - 10.1109/ICRA40945.2020.9197164
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study a novel class of motion planning problems, inspired by emerging low-energy robotic vehicles, such as insect-size flyers, chip-size satellites, and high-endurance autonomous blimps, for which the energy consumed by computing hardware during planning a path can be as large as the energy consumed by actuation hardware during the execution of the same path. We propose a new algorithm, called Compute Energy Included Motion Planning (CEIMP). CEIMP operates similarly to any other anytime planning algorithm, except it stops when it estimates further computing will require more computing energy than potential savings in actuation energy. We show that CEIMP has the same asymptotic computational complexity as existing sampling-based motion planning algorithms, such as PRM*. We also show that CEIMP outperforms the average baseline of using maximum computing resources in realistic computational experiments involving 10 floor plans from MIT buildings. In one representative experiment, CEIMP outperforms the average baseline 90.6% of the time when energy to compute one more second is equal to the energy to move one more meter, and 99.7% of the time when energy to compute one more second is equal to or greater than the energy to move 3 more meters.
ER  - 

TY  - CONF
TI  - Posterior Sampling for Anytime Motion Planning on Graphs with Expensive-to-Evaluate Edges
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4266
EP  - 4272
AU  - B. Hou
AU  - S. Choudhury
AU  - G. Lee
AU  - A. Mandalika
AU  - S. S. Srinivasa
PY  - 2020
KW  - collision avoidance
KW  - graph theory
KW  - minimisation
KW  - mobile robots
KW  - 7D planning problems
KW  - posterior sampling
KW  - anytime motion planning
KW  - collision checking
KW  - computational bottleneck
KW  - lazy algorithms
KW  - collision uncertainty
KW  - shortest path
KW  - real-time applications
KW  - anytime performance
KW  - anytime lazy motion planning algorithm
KW  - edge collisions
KW  - initial feasible path
KW  - shorter paths
KW  - Planning
KW  - Bayes methods
KW  - History
KW  - Uncertainty
KW  - Geometry
KW  - Learning (artificial intelligence)
KW  - Search problems
DO  - 10.1109/ICRA40945.2020.9197014
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collision checking is a computational bottleneck in motion planning, requiring lazy algorithms that explicitly reason about when to perform this computation. Optimism in the face of collision uncertainty minimizes the number of checks before finding the shortest path. However, this may take a prohibitively long time to compute, with no other feasible paths discovered during this period. For many real-time applications, we instead demand strong anytime performance, defined as minimizing the cumulative lengths of the feasible paths yielded over time. We introduce Posterior Sampling for Motion Planning (PSMP), an anytime lazy motion planning algorithm that leverages learned posteriors on edge collisions to quickly discover an initial feasible path and progressively yield shorter paths. PSMP obtains an expected regret bound of Õ(√(SAT)) and outperforms comparative baselines on a set of 2D and 7D planning problems.
ER  - 

TY  - CONF
TI  - Upset Recovery Control for Quadrotors Subjected to a Complete Rotor Failure from Large Initial Disturbances
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4273
EP  - 4279
AU  - S. Sun
AU  - M. Baert
AU  - B. S. van Schijndel
AU  - C. de Visser
PY  - 2020
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - cascade control
KW  - control system synthesis
KW  - helicopters
KW  - Monte Carlo methods
KW  - quadratic programming
KW  - upset recovery control
KW  - fault-tolerant controller
KW  - arbitrary initial orientations
KW  - angular velocities
KW  - control method
KW  - post-failure quadrotor
KW  - Monte-Carlo simulation
KW  - control allocator
KW  - quadratic programming
KW  - control allocation method
KW  - almost-global convergence attitude controller
KW  - Rotors
KW  - Attitude control
KW  - Angular velocity
KW  - Fault tolerance
KW  - Fault tolerant systems
KW  - Resource management
KW  - Drones
DO  - 10.1109/ICRA40945.2020.9197239
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study has developed a fault-tolerant controller that is able to recover a quadrotor from arbitrary initial orientations and angular velocities, despite the complete failure of a rotor. This cascaded control method includes a position/altitude controller, an almost-global convergence attitude controller, and a control allocation method based on quadratic programming. As a major novelty, a constraint of undesirable angular velocity is derived and fused into the control allocator, which significantly improves the recovery performance. For validation, we have conducted a set of Monte-Carlo simulation to test the reliability of the proposed method of recovering the quadrotor from arbitrary initial attitude/rate conditions. In addition, real-life flight tests have been performed. The results demonstrate that the post-failure quadrotor can recover after being casually tossed into the air.
ER  - 

TY  - CONF
TI  - Identification and evaluation of a force model for multirotor UAVs*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4280
EP  - 4286
AU  - A. Letalenet
AU  - P. Morin
PY  - 2020
KW  - aerodynamics
KW  - autonomous aerial vehicles
KW  - blades
KW  - momentum
KW  - propellers
KW  - rotors (mechanical)
KW  - vehicle dynamics
KW  - force model
KW  - multirotor UAV
KW  - model identification method
KW  - propellers
KW  - blade element theories
KW  - aerodynamics
KW  - momentum theory
KW  - actuation dynamics
KW  - Propellers
KW  - Aerodynamics
KW  - Atmospheric modeling
KW  - Predictive models
KW  - Blades
KW  - Acceleration
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9197317
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a model identification method and evaluation of a force model for multirotor UAVs. The model incorporates propellers' aerodynamics derived from momentum and blade element theories, as well as aerodynamics of the UAV's structure and actuation dynamics. A two-steps identification approach of the model parameters is proposed. The model is identified and evaluated from outdoor experiments with flight speeds exceeding 10m/s.
ER  - 

TY  - CONF
TI  - Preliminary Study of an Aerial Manipulator with Elastic Suspension
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4287
EP  - 4293
AU  - A. Yiğit
AU  - G. Grappe
AU  - L. Cuvillon
AU  - S. Durand
AU  - J. Gangloff
PY  - 2020
KW  - actuators
KW  - aerospace robotics
KW  - feedback
KW  - manipulator dynamics
KW  - propellers
KW  - vibration control
KW  - elastic suspension
KW  - aerial manipulator
KW  - contra-rotating propellers
KW  - computed torque control strategy
KW  - active vibration canceling
KW  - feedback linearization control strategy
KW  - robotic carrier
KW  - Springs
KW  - Manipulators
KW  - Propellers
KW  - Task analysis
KW  - Grippers
DO  - 10.1109/ICRA40945.2020.9196942
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a preliminary study of an Aerial Manipulator suspended by a spring to a robotic carrier. The suspended aerial manipulator is actuated by six pairs of contra-rotating propellers generating a 6-DoF wrench. Simulations show path following results using a computed torque (feedback linearization) control strategy. Active vibration canceling is validated experimentally on a first prototype.
ER  - 

TY  - CONF
TI  - Towards Low-Latency High-Bandwidth Control of Quadrotors using Event Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4294
EP  - 4300
AU  - R. S. Dimitrova
AU  - M. Gehrig
AU  - D. Brescianini
AU  - D. Scaramuzza
PY  - 2020
KW  - aircraft control
KW  - angular velocity control
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - cameras
KW  - closed loop systems
KW  - feedback
KW  - helicopters
KW  - Hough transforms
KW  - image resolution
KW  - image sensors
KW  - Kalman filters
KW  - PD control
KW  - robot vision
KW  - state estimation
KW  - attitude tracking
KW  - state estimator
KW  - rotor thrusts
KW  - black-and-white disk
KW  - angular velocity
KW  - roll angle
KW  - Kalman filter
KW  - Hough transform
KW  - dualcopter platform
KW  - one-dimensional attitude tracking
KW  - drones
KW  - quadrotors
KW  - low-latency high-bandwidth control
KW  - event-based feedback
KW  - event-camera-driven closed loop control
KW  - proportional-derivative attitude control law
KW  - event-based state estimation
KW  - temporal resolution
KW  - sensor latency
KW  - high speed vision-based control
KW  - frequency 1.0 kHz
KW  - time 12.0 ms
KW  - Cameras
KW  - Robot vision systems
KW  - Transforms
KW  - Angular velocity
KW  - State estimation
KW  - Attitude control
DO  - 10.1109/ICRA40945.2020.9197530
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Event cameras are a promising candidate to enable high speed vision-based control due to their low sensor latency and high temporal resolution. However, purely event-based feedback has yet to be used in the control of drones. In this work, a first step towards implementing low-latency high-bandwidth control of quadrotors using event cameras is taken. In particular, this paper addresses the problem of one-dimensional attitude tracking using a dualcopter platform equipped with an event camera. The event-based state estimation consists of a modified Hough transform algorithm combined with a Kalman filter that outputs the roll angle and angular velocity of the dualcopter relative to a horizon marked by a black-and-white disk. The estimated state is processed by a proportional-derivative attitude control law that computes the rotor thrusts required to track the desired attitude. The proposed attitude tracking scheme shows promising results of event-camera-driven closed loop control: the state estimator performs with an update rate of 1 kHz and a latency determined to be 12 ms, enabling attitude tracking at speeds of over 1600°/s.
ER  - 

TY  - CONF
TI  - Perception-constrained and Motor-level Nonlinear MPC for both Underactuated and Tilted-propeller UAVS
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4301
EP  - 4306
AU  - M. Jacquet
AU  - G. Corsini
AU  - D. Bicego
AU  - A. Franchi
PY  - 2020
KW  - actuators
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - helicopters
KW  - motion control
KW  - nonlinear control systems
KW  - predictive control
KW  - propellers
KW  - rotors (mechanical)
KW  - vehicle dynamics
KW  - motor-level Nonlinear MPC
KW  - tilted-propeller
KW  - Perception-constrained Nonlinear Model Predictive Control framework
KW  - real-time control
KW  - multirotor aerial vehicles
KW  - perceptive sensor
KW  - realistic actuator limitations
KW  - rotor minimum
KW  - maximum speeds
KW  - multirotor platforms
KW  - underactuated quadrotors
KW  - tilted-propellers hexarotors
KW  - motor-torque level
KW  - Propellers
KW  - Task analysis
KW  - Robot sensing systems
KW  - Real-time systems
KW  - Actuators
KW  - Vehicle dynamics
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197281
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a Perception-constrained Nonlinear Model Predictive Control (NMPC) framework for the real-time control of multi-rotor aerial vehicles. Our formulation considers both constraints from a perceptive sensor and realistic actuator limitations that are the rotor minimum and maximum speeds and accelerations. The formulation is meant to be generic and considers a large range of multi-rotor platforms (such as underactuated quadrotors or tilted-propellers hexarotors) since it does not rely on differential flatness for the dynamical equations, and a broad range of sensors, such as cameras, lidars, etc.... The perceptive constraints are expressed to maintain visibility of a feature point in the sensor's field of view, while performing a reference maneuver. We demonstrate both in simulation and real experiments that our framework is able to exploit the full capabilities of the multi-rotor, to achieve the motion under the aforementioned constraints, and control in real-time the platform at a motor-torque level, avoiding the use of an intermediate unconstrained trajectory tracker.
ER  - 

TY  - CONF
TI  - Coordinate-Free Dynamics and Differential Flatness of a Class of 6DOF Aerial Manipulators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4307
EP  - 4313
AU  - J. Welde
AU  - V. Kumar
PY  - 2020
KW  - control system synthesis
KW  - end effectors
KW  - manipulator dynamics
KW  - manipulator kinematics
KW  - path planning
KW  - position control
KW  - simulated aerial videography task
KW  - differential flatness
KW  - 6DOF aerial manipulators
KW  - coordinate-free formulation
KW  - coupled dynamics
KW  - 2DOF articulated manipulator
KW  - end effector frame
KW  - Manipulator dynamics
KW  - End effectors
KW  - Trajectory
KW  - Task analysis
KW  - Planning
KW  - Vehicle dynamics
DO  - 10.1109/ICRA40945.2020.9196705
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we derive a coordinate-free formulation of the coupled dynamics of a class of 6DOF aerial manipulators consisting of an underactuated quadrotor equipped with a 2DOF articulated manipulator, and demonstrate that the system is differentially flat with respect to the end effector pose. In particular, we require the center of mass of the entire system to be fixed in the end effector frame, suggesting a reasonable mechanical design criterion. We make use of an inertial decoupling transformation to demonstrate differential flatness, allowing us to plan dynamically feasible trajectories for the system in the space of the 6DOF pose of the end effector, which is ideal for achieving precise manipulator tasks. Simulation results validate the flatness-based planning methodology for our dynamic model, and its usefulness is demonstrated in a simulated aerial videography task.
ER  - 

TY  - CONF
TI  - CMTS: A Conditional Multiple Trajectory Synthesizer for Generating Safety-Critical Driving Scenarios
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4314
EP  - 4321
AU  - W. Ding
AU  - M. Xu
AU  - D. Zhao
PY  - 2020
KW  - Bayes methods
KW  - data analysis
KW  - mobile robots
KW  - road safety
KW  - road vehicles
KW  - safety-critical software
KW  - trajectory control
KW  - CMTS
KW  - conditional multiple trajectory synthesizer
KW  - safety-critical driving scenarios
KW  - naturalistic driving trajectory generation
KW  - autonomous driving algorithms
KW  - collision-free scenarios
KW  - safety-critical cases
KW  - near-miss scenarios
KW  - off-the-shelf datasets
KW  - generative model
KW  - conditional probability
KW  - trajectory predictions
KW  - autonomous vehicle safety
KW  - safety-critical data synthesizing framework
KW  - variational Bayesian methods
KW  - Trajectory
KW  - Interpolation
KW  - Roads
KW  - Training
KW  - Aerospace electronics
KW  - Data models
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9197145
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Naturalistic driving trajectory generation is crucial for the development of autonomous driving algorithms. However, most of the data is collected in collision-free scenarios leading to the sparsity of the safety-critical cases. When considering safety, testing algorithms in near-miss scenarios that rarely show up in off-the-shelf datasets and are costly to accumulate is a vital part of the evaluation. As a remedy, we propose a safety-critical data synthesizing framework based on variational Bayesian methods and term it as Conditional Multiple Trajectory Synthesizer (CMTS). We extend a generative model to connect safe and collision driving data by representing their distribution in the latent space and use conditional probability to adapt to different maps. Sampling from the mixed distribution enables us to synthesize the safety-critical data not shown in the safe or collision datasets. Experimental results demonstrate that the generated dataset covers many different realistic scenarios, especially the near-misses. We conclude that the use of data generated by CMTS can improve the accuracy of trajectory predictions and autonomous vehicle safety.
ER  - 

TY  - CONF
TI  - LiDAR Inertial Odometry Aided Robust LiDAR Localization System in Changing City Scenes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4322
EP  - 4328
AU  - W. Ding
AU  - S. Hou
AU  - H. Gao
AU  - G. Wan
AU  - S. Song
PY  - 2020
KW  - distance measurement
KW  - graph theory
KW  - inertial systems
KW  - maximum likelihood estimation
KW  - motion estimation
KW  - optical radar
KW  - localization estimation
KW  - inertial LiDAR intensity
KW  - matching estimation
KW  - LiDAR localization system
KW  - environmental change detection method
KW  - kinematic estimation
KW  - frame-to-frame motion estimation
KW  - multiresolution occupancy grid based LiDAR inertial odometry
KW  - pose graph fusion framework
KW  - Apollo-SouthBay dataset
KW  - MAP estimation problem
KW  - Laser radar
KW  - Estimation
KW  - Robustness
KW  - Roads
KW  - Windows
KW  - Optimization
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9196698
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Environmental fluctuations pose crucial challenges to a localization system in autonomous driving. We present a robust LiDAR localization system that maintains its kinematic estimation in changing urban scenarios by using a dead reckoning solution implemented through a LiDAR inertial odometry. Our localization framework jointly uses information from complementary modalities such as global matching and LiDAR inertial odometry to achieve accurate and smooth localization estimation. To improve the performance of the LiDAR odometry, we incorporate inertial and LiDAR intensity cues into an occupancy grid based LiDAR odometry to enhance frame-to-frame motion and matching estimation. Multi-resolution occupancy grid is implemented yielding a coarse-to-fine approach to balance the odometry's precision and computational requirement. To fuse both the odometry and global matching results, we formulate a MAP estimation problem in a pose graph fusion framework that can be efficiently solved. An effective environmental change detection method is proposed that allows us to know exactly when and what portion of the map requires an update. We comprehensively validate the effectiveness of the proposed approaches using both the Apollo-SouthBay dataset and our internal dataset. The results confirm that our efforts lead to a more robust and accurate localization system, especially in dynamically changing urban scenarios.
ER  - 

TY  - CONF
TI  - Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning in Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4329
EP  - 4335
AU  - M. Huegle
AU  - G. Kalweit
AU  - M. Werling
AU  - J. Boedecker
PY  - 2020
KW  - convolutional neural nets
KW  - decision making
KW  - graph theory
KW  - image representation
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - traffic engineering computing
KW  - DeepScene-Q off-policy reinforcement learning algorithms
KW  - graph-Q
KW  - graph convolutional networks
KW  - multiple variable-length sequences
KW  - novel deep scene architecture
KW  - complex interaction-aware scene representations
KW  - traffic participants
KW  - traffic signs
KW  - object types
KW  - high-level decision making
KW  - deep reinforcement learning
KW  - high-level decision component
KW  - perception component
KW  - autonomous driving systems
KW  - dynamic interaction-aware scene understanding
KW  - traffic simulator SUMO
KW  - Computer architecture
KW  - Autonomous vehicles
KW  - Lenses
KW  - Learning (artificial intelligence)
KW  - Decision making
KW  - Predictive models
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197086
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The common pipeline in autonomous driving systems is highly modular and includes a perception component which extracts lists of surrounding objects and passes these lists to a high-level decision component. In this case, leveraging the benefits of deep reinforcement learning for high-level decision making requires special architectures to deal with multiple variable-length sequences of different object types, such as vehicles, lanes or traffic signs. At the same time, the architecture has to be able to cover interactions between traffic participants in order to find the optimal action to be taken. In this work, we propose the novel Deep Scenes architecture, that can learn complex interaction-aware scene representations based on extensions of either 1) Deep Sets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q off-policy reinforcement learning algorithms, both outperforming state-ofthe-art methods in evaluations with the publicly available traffic simulator SUMO.
ER  - 

TY  - CONF
TI  - Interacting Vehicle Trajectory Prediction with Convolutional Recurrent Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4336
EP  - 4342
AU  - S. Mukherjee
AU  - S. Wang
AU  - A. Wallace
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - feedback
KW  - learning (artificial intelligence)
KW  - path planning
KW  - recurrent neural nets
KW  - traffic engineering computing
KW  - car
KW  - convolutional recurrent neural networks
KW  - convolutional long short term memory
KW  - Conv-LSTM
KW  - interacting vehicle trajectory prediction
KW  - novel feedback scheme
KW  - path planning
KW  - interaction learning
KW  - temporal learning
KW  - motion learning
KW  - Automobiles
KW  - Trajectory
KW  - Feature extraction
KW  - Hidden Markov models
KW  - Predictive models
KW  - Computer architecture
KW  - Road transportation
DO  - 10.1109/ICRA40945.2020.9196807
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Anticipating the future trajectories of surrounding vehicles is a crucial and challenging task in path planning for autonomy. We propose a novel Convolutional Long Short Term Memory (Conv-LSTM) based neural network architecture to predict the future positions of cars using several seconds of historical driving observations. This consists of three modules: 1) Interaction Learning to capture the effect of surrounding cars, 2) Temporal Learning to identify the dependency on past movements and 3) Motion Learning to convert the extracted features from these two modules into future positions. To continuously achieve accurate prediction, we introduce a novel feedback scheme where the current predicted positions of each car are leveraged to update future motion, encapsulating the effect of the surrounding cars. Experiments on two public datasets demonstrate that the proposed methodology can match or outperform the state-of-the-art methods for long-term trajectory prediction.
ER  - 

TY  - CONF
TI  - Navigation Command Matching for Vision-based Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4343
EP  - 4349
AU  - Y. Pan
AU  - J. Xue
AU  - P. Zhang
AU  - W. Ouyang
AU  - J. Fang
AU  - X. Chen
PY  - 2020
KW  - control engineering computing
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - navigation
KW  - path planning
KW  - road traffic control
KW  - robot vision
KW  - robust control
KW  - traffic engineering computing
KW  - suboptimal policy
KW  - CARLA driving benchmark
KW  - vision-based autonomous driving
KW  - imitative reinforcement learning
KW  - robust driving policy
KW  - nonsmooth rewards
KW  - state-action pairs
KW  - smooth rewards
KW  - matching measurer
KW  - navigation rewards
KW  - navigation command matching
KW  - attention-guided agent
KW  - salient regions
KW  - RGB images
KW  - Navigation
KW  - Task analysis
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Autonomous vehicles
KW  - Smoothing methods
KW  - Current measurement
DO  - 10.1109/ICRA40945.2020.9196609
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning an optimal policy for autonomous driving task to confront with complex environment is a long- studied challenge. Imitative reinforcement learning is accepted as a promising approach to learn a robust driving policy through expert demonstrations and interactions with environments. However, this model utilizes non-smooth rewards, which have a negative impact on matching between navigation commands and trajectory (state-action pairs), and degrade the generalizability of an agent. Smooth rewards are crucial to discriminate actions generated from sub-optimal policy. In this paper, we propose a navigation command matching (NCM) model to address this issue. There are two key components in NCM, 1) a matching measurer produces smooth navigation rewards that measure matching between navigation commands and trajectory; 2) attention-guided agent performs actions given states where salient regions in RGB images (i.e. roadsides, lane markings and dynamic obstacles) are highlighted to amplify their influence on the final model. We obtain navigation rewards and store transitions to replay buffer after an episode, so NCM is able to discriminate actions generated from suboptimal policy. Experiments on CARLA driving benchmark show our proposed NCM outperforms previous state-of-the- art models on various tasks in terms of the percentage of successfully completed episodes. Moreover, our model improves generalizability of the agent and obtains good performance even in unseen scenarios.
ER  - 

TY  - CONF
TI  - GraphRQI: Classifying Driver Behaviors Using Graph Spectrums
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4350
EP  - 4357
AU  - R. Chandra
AU  - U. Bhattacharya
AU  - T. Mittal
AU  - X. Li
AU  - A. Bera
AU  - D. Manocha
PY  - 2020
KW  - computational complexity
KW  - driver information systems
KW  - eigenvalues and eigenfunctions
KW  - graph theory
KW  - multi-agent systems
KW  - pattern classification
KW  - supervised learning
KW  - GraphRQI
KW  - graph spectrums
KW  - road-agent trajectories
KW  - driving traits
KW  - aggressive driving
KW  - conservative driving
KW  - nearby road-agents
KW  - interagent interactions
KW  - unweighted traffic graphs
KW  - undirected traffic graphs
KW  - supervised learning algorithm
KW  - traffic graph
KW  - eigenvalue algorithm
KW  - autonomous driving datasets
KW  - prior driver behavior classification algorithms
KW  - Vehicles
KW  - Trajectory
KW  - Heuristic algorithms
KW  - Eigenvalues and eigenfunctions
KW  - Laplace equations
KW  - Classification algorithms
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196751
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel algorithm (GraphRQI) to identify driver behaviors from road-agent trajectories. Our approach assumes that the road-agents exhibit a range of driving traits, such as aggressive or conservative driving. Moreover, these traits affect the trajectories of nearby road-agents as well as the interactions between road-agents. We represent these inter-agent interactions using unweighted and undirected traffic graphs. Our algorithm classifies the driver behavior using a supervised learning algorithm by reducing the computation to the spectral analysis of the traffic graph. Moreover, we present a novel eigenvalue algorithm to compute the spectrum efficiently. We provide theoretical guarantees for the running time complexity of our eigenvalue algorithm and show that it is faster than previous methods by 2 times. We evaluate the classification accuracy of our approach on traffic videos and autonomous driving datasets corresponding to urban traffic. In practice, GraphRQI achieves an accuracy improvement of up to 25% over prior driver behavior classification algorithms. We also use our classification algorithm to predict the future trajectories of road-agents.
ER  - 

TY  - CONF
TI  - Kidnapped Radar: Topological Radar Localisation using Rotationally-Invariant Metric Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4358
EP  - 4364
AU  - Ş. Săftescu
AU  - M. Gadd
AU  - D. De Martini
AU  - D. Barnes
AU  - P. Newman
PY  - 2020
KW  - convolutional neural nets
KW  - CW radar
KW  - feature extraction
KW  - FM radar
KW  - learning (artificial intelligence)
KW  - nearest neighbour methods
KW  - radar imaging
KW  - radar tracking
KW  - polar nature
KW  - radar scan formation
KW  - cylindrical convolutions
KW  - anti-aliasing blurring
KW  - azimuth-wise max-pooling
KW  - rotational invariance
KW  - enforced metric space
KW  - topological localisation system
KW  - random rotational perturbation
KW  - kidnapped radar
KW  - topological radar localisation
KW  - rotationally-invariant metric learning
KW  - large-scale topological localisation
KW  - frequency-modulated continuous-wave scanning radar
KW  - efficient learning-based approach
KW  - radar data
KW  - polar radar scans
KW  - NetVLAD architectures
KW  - visual domain
KW  - feature extraction
KW  - radar-focused mobile autonomy dataset
KW  - CNN architectures
KW  - reference trajectory
KW  - nearest neighbour
KW  - place recognition
KW  - root architecture
KW  - convolutional neural network
KW  - distance 280.0 km
KW  - Measurement
KW  - Radar imaging
KW  - Robot sensing systems
KW  - Azimuth
KW  - Feature extraction
KW  - Trajectory
KW  - radar
KW  - localisation
KW  - place recognition
KW  - deep learning
KW  - metric learning
DO  - 10.1109/ICRA40945.2020.9196682
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a system for robust, large-scale topological localisation using Frequency-Modulated Continuous-Wave scanning radar which extends the state-of-the-art by an efficient, learning-based approach to handle radar data for localisation. We learn a metric space for embedding polar radar scans using CNN and NetVLAD architectures traditionally applied to the visual domain. However, we tailor the feature extraction for more suitability to the polar nature of radar scan formation using cylindrical convolutions, anti-aliasing blurring, and azimuth-wise max-pooling; all in order to bolster the rotational invariance. The enforced metric space is then used to encode a reference trajectory, serving as a map, which is queried for nearest neighbour for recognition of places at run-time. We demonstrate the performance of our topological localisation system over the course of many repeat forays using the largest radar-focused mobile autonomy dataset released to date, totalling 280 km of urban driving, a small portion of which we also use to learn the weights of the modified architecture. As this work represents a novel application for radar, we analyse the utility of the proposed method via a comprehensive set of metrics which provide insight into the efficacy when used in a realistic system, showing improved performance over the root architecture even in the face of random rotational perturbation.
ER  - 

TY  - CONF
TI  - Global visual localization in LiDAR-maps through shared 2D-3D embedding space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4365
EP  - 4371
AU  - D. Cattaneo
AU  - M. Vaghi
AU  - S. Fontana
AU  - A. L. Ballardini
AU  - D. G. Sorrenti
PY  - 2020
KW  - image recognition
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - optical radar
KW  - robot vision
KW  - SLAM (robots)
KW  - global visual localization
KW  - LiDAR-maps
KW  - place recognition
KW  - autonomous driving field
KW  - vision-based approaches
KW  - image database
KW  - high definition 3D maps
KW  - deep neural network
KW  - shared embedding space
KW  - 3D-LiDAR place recognition
KW  - 3D DNN
KW  - 2D-3D embedding space
KW  - Oxford Robotcar Dataset
KW  - image w.r.t.
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Laser radar
KW  - Feature extraction
KW  - Visualization
KW  - Robots
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9196859
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Global localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using the Oxford Robotcar Dataset, which encompasses a wide range of weather and light conditions.
ER  - 

TY  - CONF
TI  - Unsupervised Learning Methods for Visual Place Recognition in Discretely and Continuously Changing Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4372
EP  - 4378
AU  - S. Schubert
AU  - P. Neubert
AU  - P. Protzel
PY  - 2020
KW  - convolutional neural nets
KW  - image matching
KW  - object recognition
KW  - principal component analysis
KW  - unsupervised learning
KW  - place recognition performance
KW  - in-sequence condition changes
KW  - unsupervised learning methods
KW  - visual place recognition
KW  - query set
KW  - reference set
KW  - single distinctive condition
KW  - query sequence
KW  - traversal daytime-dusk-night-dawn
KW  - CNN-based descriptors
KW  - in-sequence changes
KW  - continuous changes
KW  - statistical normalization
KW  - PCA
KW  - Standardization
KW  - Visualization
KW  - Lighting
KW  - Unsupervised learning
KW  - Principal component analysis
KW  - Dimensionality reduction
KW  - Clouds
DO  - 10.1109/ICRA40945.2020.9197044
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual place recognition in changing environments is the problem of finding matchings between two sets of observations, a query set and a reference set, despite severe appearance changes. Recently, image comparison using CNNbased descriptors showed very promising results. However, the experiments in the literature typically assume a single distinctive condition within each set (e.g., reference images are captured at daytime and the query sequence is at night). In this paper, we will demonstrate that as soon as the conditions change within one set (e.g., reference is daytime and now the query is a traversal daytime-dusk-night-dawn), different places under the same condition can suddenly look more similar than same places under different conditions. As a consequence, state-of-the-art approaches like CNN-based descriptors fail. This paper discusses this practically very important problem of in-sequence condition changes and defines a hierarchy of problem setups from (1) no in-sequence changes, (2) discrete in-sequence changes, to (3) continuous in-sequence changes. We will experimentally evaluate the effect of in-sequence condition changes on two state-of-the-art CNN-descriptors and investigate unsupervised methods to improve their performance. This includes an evaluation of the importance of statistical normalization (standardization) of descriptors, which is often omitted in existing approaches but can considerably improve results for problems up to discrete in-sequence changes. To address the practical most relevant setup of continuous changes, we investigate the application of unsupervised learning methods using two PCA-based approaches from the literature and propose a novel clustering-based extension of the statistical normalization. We experimentally demonstrate that these approaches can significantly improve place recognition performance in case of continuous in-sequence condition changes. Matlab implementations of the presented approaches are available online: www.tu-chemnitz.de/etit/proaut/cont_changing_envs.
ER  - 

TY  - CONF
TI  - LOL: Lidar-only Odometry and Localization in 3D point cloud maps*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4379
EP  - 4385
AU  - D. Rozenberszki
AU  - A. L. Majdik
PY  - 2020
KW  - distance measurement
KW  - image enhancement
KW  - image matching
KW  - image segmentation
KW  - object detection
KW  - object recognition
KW  - optical radar
KW  - LOL system
KW  - 3D point cloud maps
KW  - Lidar-equipped vehicles
KW  - 3D point segment matching method
KW  - Lidar-only odometry and localization algorithm
KW  - Kitti datasets
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Sensors
KW  - Iterative closest point algorithm
KW  - Image color analysis
KW  - Trajectory
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197450
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we deal with the problem of odometry and localization for Lidar-equipped vehicles driving in urban environments, where a premade target map exists to localize against. In our problem formulation, to correct the accumulated drift of the Lidar-only odometry we apply a place recognition method to detect geometrically similar locations between the online 3D point cloud and the a priori offline map. In the proposed system, we integrate a state-of-the-art Lidaronly odometry algorithm with a recently proposed 3D point segment matching method by complementing their advantages. Also, we propose additional enhancements in order to reduce the number of false matches between the online point cloud and the target map, and to refine the position estimation error whenever a good match is detected. We demonstrate the utility of the proposed LOL system on several Kitti datasets of different lengths and environments, where the relocalization accuracy and the precision of the vehicle's trajectory were significantly improved in every case, while still being able to maintain real-time performance.
ER  - 

TY  - CONF
TI  - Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4386
EP  - 4392
AU  - L. Sun
AU  - D. Adolfsson
AU  - M. Magnusson
AU  - H. Andreasson
AU  - I. Posner
AU  - T. Duckett
PY  - 2020
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - Monte Carlo methods
KW  - neural nets
KW  - optical radar
KW  - path planning
KW  - recursive estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - precise lidar-based robot localisation
KW  - large-scale environments
KW  - global localisation
KW  - mobile robots
KW  - Monte Carlo Localisation
KW  - MCL
KW  - fast localisation system
KW  - deep-probabilistic model
KW  - Gaussian process regression
KW  - deep kernel
KW  - precise recursive estimator
KW  - Gaussian method
KW  - deep probabilistic localisation
KW  - large-scale localisation
KW  - largescale environment
KW  - time 0.8 s
KW  - size 0.75 m
KW  - Robots
KW  - Neural networks
KW  - Three-dimensional displays
KW  - Gaussian processes
KW  - Laser radar
KW  - Monte Carlo methods
KW  - Kernel
DO  - 10.1109/ICRA40945.2020.9196708
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deeplearned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and nonGaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75 m in a largescale environment of approximately 0.5 km2.
ER  - 

TY  - CONF
TI  - Set-membership state estimation by solving data association
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4393
EP  - 4399
AU  - S. Rohou
AU  - B. Desrochers
AU  - L. Jaulin
PY  - 2020
KW  - mobile robots
KW  - position control
KW  - robot vision
KW  - sensor fusion
KW  - SLAM (robots)
KW  - state estimation
KW  - deterministic approach
KW  - data association
KW  - underwater robot
KW  - sonar data
KW  - membership state estimation
KW  - localization problem
KW  - indistinguishable landmarks
KW  - diving phase
KW  - unknown initial position
KW  - Sonar
KW  - State estimation
KW  - Rocks
KW  - Trajectory
KW  - Robot sensing systems
KW  - Reliability
DO  - 10.1109/ICRA40945.2020.9197039
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper deals with the localization problem of a robot in an environment made of indistinguishable landmarks, and assuming the initial position of the vehicle is unknown. This scenario is typically encountered in underwater applications for which landmarks such as rocks all look alike. Furthermore, the position of the robot may be lost during a diving phase, which obliges us to consider unknown initial position. We propose a deterministic approach to solve simultaneously the problems of data association and state estimation, without combinatorial explosion. The efficiency of the method is shown on an actual experiment involving an underwater robot and sonar data.
ER  - 

TY  - CONF
TI  - A Linearly Constrained Nonparametric Framework for Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4400
EP  - 4406
AU  - Y. Huang
AU  - D. G. Caldwell
PY  - 2020
KW  - Bayes methods
KW  - end effectors
KW  - learning systems
KW  - predictive control
KW  - trajectory control
KW  - imitation learning
KW  - constrained skills
KW  - linearly constrained optimization problem
KW  - nonparametric solution
KW  - linearly constrained nonparametric framework
KW  - human skills learning
KW  - constrained motor skills learning
KW  - robotic systems
KW  - end-effector trajectory
KW  - linearly constrained kernelized movement primitives
KW  - LC-KMP
KW  - probabilistic properties
KW  - predictive control
KW  - locomotion tasks
KW  - grasping tasks
KW  - human robot collaborations
KW  - Trajectory
KW  - Probabilistic logic
KW  - Robots
KW  - Task analysis
KW  - Optimization
KW  - Kernel
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196821
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In recent years, a myriad of advanced results have been reported in the community of imitation learning, ranging from parametric to non-parametric, probabilistic to non-probabilistic and Bayesian to frequentist approaches. Meanwhile, ample applications (e.g., grasping tasks and humanrobot collaborations) further show the applicability of imitation learning in a wide range of domains. While numerous literature is dedicated to the learning of human skills in unconstrained environments, the problem of learning constrained motor skills, however, has not received equal attention. In fact, constrained skills exist widely in robotic systems. For instance, when a robot is demanded to write letters on a board, its end-effector trajectory must comply with the plane constraint from the board. In this paper, we propose linearly constrained kernelized movement primitives (LC-KMP) to tackle the problem of imitation learning with linear constraints. Specifically, we propose to exploit the probabilistic properties of multiple demonstrations, and subsequently incorporate them into a linearly constrained optimization problem, which finally leads to a non-parametric solution. In addition, a connection between our framework and the classical model predictive control is provided. Several examples including simulated writing and locomotion tasks are presented to show the effectiveness of our framework.
ER  - 

TY  - CONF
TI  - An Energy-based Approach to Ensure the Stability of Learned Dynamical Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4407
EP  - 4413
AU  - M. Saveriano
PY  - 2020
KW  - learning (artificial intelligence)
KW  - motion control
KW  - nonlinear dynamical systems
KW  - regression analysis
KW  - stability
KW  - energy-based approach
KW  - learned dynamical systems
KW  - nonlinear dynamical systems
KW  - reactive motion generation
KW  - stable motions
KW  - accurate motions
KW  - learning problems
KW  - training time
KW  - single-step learning
KW  - regression technique
KW  - single-step approach
KW  - energy considerations
KW  - learned dynamics
KW  - demonstrated motion
KW  - Lyapunov methods
KW  - Training data
KW  - Robots
KW  - Electrostatic discharges
KW  - Stability analysis
KW  - Trajectory
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196978
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Non-linear dynamical systems represent a compact, flexible, and robust tool for reactive motion generation. The effectiveness of dynamical systems relies on their ability to accurately represent stable motions. Several approaches have been proposed to learn stable and accurate motions from demonstration. Some approaches work by separating accuracy and stability into two learning problems, which increases the number of open parameters and the overall training time. Alternative solutions exploit single-step learning but restrict the applicability to one regression technique. This paper presents a single-step approach to learn stable and accurate motions that work with any regression technique. The approach makes energy considerations on the learned dynamics to stabilize the system at run-time while introducing small deviations from the demonstrated motion. Since the initial value of the energy injected into the system affects the reproduction accuracy, it is estimated from training data using an efficient procedure. Experiments on a real robot and a comparison on a public benchmark shows the effectiveness of the proposed approach.
ER  - 

TY  - CONF
TI  - IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4414
EP  - 4420
AU  - A. Mandlekar
AU  - F. Ramos
AU  - B. Boots
AU  - S. Savarese
AU  - L. Fei-Fei
AU  - A. Garg
AU  - D. Fox
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - RoboTurk Cans dataset
KW  - offline learning
KW  - IRIS
KW  - offline robot manipulation data
KW  - offline task demonstrations
KW  - robotics
KW  - goal-conditioned low-level controller
KW  - high-level goal selection mechanism
KW  - learning control
KW  - learning from large-scale demonstration datasets
KW  - Implicit Reinforcement without Interaction at Scale
KW  - crowdsourcing
KW  - Task analysis
KW  - Robots
KW  - Trajectory
KW  - Iris
KW  - Learning (artificial intelligence)
KW  - Iris recognition
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196935
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning from offline task demonstrations is a problem of great interest in robotics. For simple short-horizon manipulation tasks with modest variation in task instances, offline learning from a small set of demonstrations can produce controllers that successfully solve the task. However, leveraging a fixed batch of data can be problematic for larger datasets and longer-horizon tasks with greater variations. The data can exhibit substantial diversity and consist of suboptimal solution approaches. In this paper, we propose Implicit Reinforcement without Interaction at Scale (IRIS), a novel framework for learning from large-scale demonstration datasets. IRIS factorizes the control problem into a goal-conditioned low-level controller that imitates short demonstration sequences and a high-level goal selection mechanism that sets goals for the low-level and selectively combines parts of suboptimal solutions leading to more successful task completions. We evaluate IRIS across three datasets, including the RoboTurk Cans dataset collected by humans via crowdsourcing, and show that performant policies can be learned from purely offline learning. Additional results at https://sites.google.com/stanford.edu/iris/.
ER  - 

TY  - CONF
TI  - Geometry-aware Dynamic Movement Primitives
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4421
EP  - 4426
AU  - F. J. Abu-Dakka
AU  - V. Kyrki
PY  - 2020
KW  - differential geometry
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - matrix algebra
KW  - geometry-aware dynamic movement primitives
KW  - robot control problems
KW  - manipulability ellipsoids
KW  - symmetric positive definite matrices
KW  - DMPs
KW  - SPD matrices
KW  - Euclidean space
KW  - mathematically principled framework
KW  - SPD manifold
KW  - Riemannian metrics
KW  - Manifolds
KW  - Robots
KW  - Symmetric matrices
KW  - Standards
KW  - Ellipsoids
KW  - Switches
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9196952
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many robot control problems, factors such as stiffness and damping matrices and manipulability ellipsoids are naturally represented as symmetric positive definite (SPD) matrices, which capture the specific geometric characteristics of those factors. Typical learned skill models such as dynamic movement primitives (DMPs) can not, however, be directly employed with quantities expressed as SPD matrices as they are limited to data in Euclidean space. In this paper, we propose a novel and mathematically principled framework that uses Riemannian metrics to reformulate DMPs such that the resulting formulation can operate with SPD data in the SPD manifold. Evaluation of the approach demonstrates that beneficial properties of DMPs such as change of the goal during operation apply also to the proposed formulation.
ER  - 

TY  - CONF
TI  - Learning a Pile Loading Controller from Demonstrations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4427
EP  - 4433
AU  - W. Yang
AU  - N. Strokina
AU  - N. Serbenyuk
AU  - R. Ghabcheloo
AU  - J. Kämäräinen
PY  - 2020
KW  - control engineering computing
KW  - earthmoving equipment
KW  - foundations
KW  - image representation
KW  - mobile robots
KW  - neural net architecture
KW  - random forests
KW  - video signal processing
KW  - hydrostatic driving pressure
KW  - control signals
KW  - application specific deep visual features
KW  - Siamese network architecture
KW  - random forest regressor
KW  - loading distance
KW  - autonomous robotic wheel loader
KW  - learning-based pile loading controller
KW  - controller parameters
KW  - low level sensor
KW  - boom angle
KW  - bucket angle
KW  - cross-entropy
KW  - contrastive loss
KW  - soil type
KW  - Visualization
KW  - Loading
KW  - Robot sensing systems
KW  - Task analysis
KW  - Feature extraction
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196907
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work introduces a learning-based pile loading controller for autonomous robotic wheel loaders. Controller parameters are learnt from a small number of demonstrations for which low level sensor (boom angle, bucket angle and hydrostatic driving pressure), egocentric video frames and control signals are recorded. Application specific deep visual features are learnt from demonstrations using a Siamese network architecture and a combination of cross-entropy and contrastive loss. The controller is based on a Random Forest (RF) regressor that provides robustness against changes in field conditions (loading distance, soil type, weather and illumination). The controller is deployed to a real autonomous robotic wheel loader and it outperforms prior art with a clear margin.
ER  - 

TY  - CONF
TI  - Learning Navigation Costs from Demonstration in Partially Observable Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4434
EP  - 4440
AU  - T. Wang
AU  - V. Dhiman
AU  - N. Atanasov
PY  - 2020
KW  - cost optimal control
KW  - dynamic programming
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mobile robots
KW  - navigation
KW  - observability
KW  - path planning
KW  - probability
KW  - state-space methods
KW  - trajectory control
KW  - inverse reinforcement learning
KW  - safe navigation
KW  - autonomous navigation
KW  - unknown partially observable environments
KW  - navigation behavior
KW  - state-control trajectory
KW  - cost function representation
KW  - probabilistic occupancy encoder
KW  - observation sequence
KW  - cost encoder
KW  - occupancy features
KW  - representation parameters
KW  - control policy
KW  - value function
KW  - state space
KW  - motion-planning algorithm
KW  - robot navigation
KW  - navigation cost learning
KW  - dynamic
KW  - Robots
KW  - Cost function
KW  - Navigation
KW  - Planning
KW  - Feature extraction
KW  - Heuristic algorithms
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197199
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on inverse reinforcement learning (IRL) to enable safe and efficient autonomous navigation in unknown partially observable environments. The objective is to infer a cost function that explains expert-demonstrated navigation behavior while relying only on the observations and state-control trajectory used by the expert. We develop a cost function representation composed of two parts: a probabilistic occupancy encoder, with recurrent dependence on the observation sequence, and a cost encoder, defined over the occupancy features. The representation parameters are optimized by differentiating the error between demonstrated controls and a control policy computed from the cost encoder. Such differentiation is typically computed by dynamic programming through the value function over the whole state space. We observe that this is inefficient in large partially observable environments because most states are unexplored. Instead, we rely on a closed-form subgradient of the cost-to-go obtained only over a subset of promising states via an efficient motion-planning algorithm such as A* or RRT. Our experiments show that our model exceeds the accuracy of baseline IRL algorithms in robot navigation tasks, while substantially improving the efficiency of training and test-time inference.
ER  - 

TY  - CONF
TI  - Towards Bimanual Vein Cannulation: Preliminary Study of a Bimanual Robotic System With a Dual Force Constraint Controller
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4441
EP  - 4447
AU  - C. He
AU  - A. Ebrahimi
AU  - E. Yang
AU  - M. Urias
AU  - Y. Yang
AU  - P. Gehlbach
AU  - I. Iordachita
PY  - 2020
KW  - biological tissues
KW  - biomechanics
KW  - blood vessels
KW  - eye
KW  - manipulators
KW  - medical robotics
KW  - phantoms
KW  - surgery
KW  - surgical tools
KW  - dual force constraint controller
KW  - robot-assisted retinal surgery
KW  - tool-to-sclera forces
KW  - cannulation tool
KW  - dual force-sensing capability
KW  - force information
KW  - robot controller
KW  - retinal vein cannulation
KW  - target vessel
KW  - robotic manipulators
KW  - tool-to-tissue forces
KW  - retinal tissue injury
KW  - bimanual cannulation
KW  - bimanual robotic system
KW  - retinal vein occlusion
KW  - occluded vessel
KW  - interaction forces
KW  - bimanual vein cannulation
KW  - tool-to-sclera force
KW  - tool-to-vessel force
KW  - steady hand eye robot platforms
KW  - Tools
KW  - Force
KW  - Robot sensing systems
KW  - Retina
KW  - Surgery
KW  - Veins
DO  - 10.1109/ICRA40945.2020.9196889
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Retinal vein cannulation is a promising approach for treating retinal vein occlusion that involves injecting medicine into the occluded vessel to dissolve the clot. The approach remains largely unexploited clinically due to surgeon limitations in detecting interaction forces between surgical tools and retinal tissue. In this paper, a dual force constraint controller for robot-assisted retinal surgery was presented to keep the tool-to-vessel forces and tool-to-sclera forces below prescribed thresholds. A cannulation tool and forceps with dual force-sensing capability were developed and used to measure force information fed into the robot controller, which was implemented on existing Steady Hand Eye Robot platforms. The robotic system facilitates retinal vein cannulation by allowing a user to grasp the target vessel with the forceps and then enter the vessel with the cannula. The system was evaluated on an eye phantom. The results showed that, while the eyeball was subjected to rotational disturbances, the proposed controller actuates the robotic manipulators to maintain the average tool-to-vessel force at 10.9 mN and 13.1 mN and the average tool-to-sclera force at 38.1 mN and 41.2 mN for the cannula and the forcpes, respectively. Such small tool-to-tissue forces are acceptable to avoid retinal tissue injury. Additionally, two clinicians participated in a preliminary user study of the bimanual cannulation demonstrating that the operation time and tool-to-tissue forces are significantly decreased when using the bimanual robotic system as compared to freehand performance.
ER  - 

TY  - CONF
TI  - Evaluation of a combined grip of pinch and power grips in manipulating a master manipulator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4448
EP  - 4454
AU  - S. Jeong
AU  - K. Tadano
PY  - 2020
KW  - biomechanics
KW  - dexterous manipulators
KW  - grippers
KW  - medical robotics
KW  - surgery
KW  - master manipulator
KW  - combined-grip-handle scheme
KW  - pinch grip motion
KW  - power grip motion
KW  - robotic surgery
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196547
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In conventional robotic surgery, the manipulating methods exhibit limitations that are strongly related to the advantages and disadvantages of a pinch grip and power grip. The context of this study is focused on the introduction of a combined grip to compensate for such restraints. In particular, this study proposed the combined-grip-handle scheme on a master manipulator. In this framework, the position of fingertips was designed to be adjustable in distance and direction to allow for a pinch grip motion around the holding axis of a power grip motion. A ring-bar experiment applying the master-slave scheme was conducted with the master manipulator under several manipulating conditions of the combined grip and the conventional gripping types. Results for using the combined grip demonstrated that the combined grip showed better performance on the positioning operation, compared with the conventional gripping types.
ER  - 

TY  - CONF
TI  - Contact Stability Analysis of Magnetically-Actuated Robotic Catheter Under Surface Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4455
EP  - 4462
AU  - R. Hao
AU  - T. Greigarn
AU  - M. C. Çavuşoğlu
PY  - 2020
KW  - biological tissues
KW  - biomedical MRI
KW  - cardiology
KW  - catheters
KW  - medical image processing
KW  - medical robotics
KW  - contact stability analysis
KW  - magnetically-actuated robotic catheter
KW  - contact force quality
KW  - lesion formation
KW  - lesion size
KW  - gap-free lesion
KW  - tissue surface motion
KW  - contact model
KW  - contact force control schemes
KW  - heart surface motions
KW  - magnetic resonance imaging-actuated robotic catheter
KW  - Catheters
KW  - Force
KW  - Force control
KW  - Robots
KW  - Stability analysis
KW  - Magnetic resonance imaging
KW  - Friction
DO  - 10.1109/ICRA40945.2020.9196951
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Contact force quality is one of the most critical factors for safe and effective lesion formation during cardiac ablation. The contact force and contact stability plays important roles in determining the lesion size and creating a gap-free lesion. In this paper, the contact stability of a novel magnetic resonance imaging (MRI)-actuated robotic catheter under tissue surface motion is studied. The robotic catheter is modeled using a pseudo-rigid-body model, and the contact model under surface constraint is provided. Two contact force control schemes to improve the contact stability of the catheter under heart surface motions are proposed and their performance are evaluated in simulation.
ER  - 

TY  - CONF
TI  - Fast and accurate intracorporeal targeting through an anatomical orifice exhibiting unknown behavior.
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4463
EP  - 4469
AU  - R. Chalard
AU  - D. Reversat
AU  - G. Morel
AU  - M. -A. Vitrani
PY  - 2020
KW  - adaptive control
KW  - biomechanics
KW  - biomedical equipment
KW  - force control
KW  - manipulators
KW  - medical robotics
KW  - motion control
KW  - position control
KW  - surgery
KW  - anatomical orifice
KW  - minimally invasive surgery
KW  - interaction forces
KW  - patient anatomy
KW  - orifice behavior
KW  - adaptive control scheme
KW  - wrist velocity
KW  - tip velocity
KW  - intracorporeal targeting
KW  - instrument tip positioning
KW  - 3DOF wrist center positioning problem
KW  - Robots
KW  - Instruments
KW  - Wrist
KW  - Surgery
KW  - Force
KW  - Adaptation models
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196950
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgery may involve precise instrument tip positioning in a minimally invasive way. During these operations, the instrument is inserted in the body through an orifice. The movements of the instrument are constrained by interaction forces arising at the orifice level. The physical constraints may drastically vary depending on the patient's anatomy. This introduces uncertainties that challenge the positioning task for a robot. Indeed, it raises an antagonism: On one side, the required precision appeals for a rigid behavior. On the other side, forces applied at the entry point should be limited, which requires softness. In this paper we choose to minimize forces at the orifice by using a passive ball joint wrist to manipulate the instrument. From a control perspective, this leads to consider the task as a 3 DOF wrist center positioning problem, whose softness can be achieved through conventional low impedance control. However, positioning the wrist center, even with a high static precision, does not allow to achieve a high precision of the instrument tip positioning when the orifice behavior is not known. To cope with this problem, we implement a controller that servos the tip position by commanding the wrist position. In order to deal with uncertainties, we exploit an adaptive control scheme that identifies in real-time the unknown mapping between the wrist velocity and the tip velocity. Both simulations and in vitro experimental results show the efficiency of the control law.
ER  - 

TY  - CONF
TI  - Robotic Swarm Control for Precise and On-Demand Embolization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4470
EP  - 4476
AU  - M. Luo
AU  - J. Law
AU  - X. Wang
AU  - L. Xin
AU  - G. Shan
AU  - M. Badiwala
AU  - X. Huang
AU  - Y. Sun
PY  - 2020
KW  - biomedical materials
KW  - bioMEMS
KW  - blood vessels
KW  - haemodynamics
KW  - magnetic particles
KW  - medical robotics
KW  - microfluidics
KW  - targeted embolization
KW  - swarm control technique
KW  - magnetic particles
KW  - mean absolute error
KW  - aggregation control approach
KW  - fluidic shear
KW  - magnetic forces
KW  - magnetic field
KW  - magnetic swarm control strategy
KW  - blood vessels
KW  - clinical embolization
KW  - swarm control capability
KW  - fluidic flow environment
KW  - magnetic aggregates
KW  - magnetic swarms
KW  - robotic control
KW  - robotic swarm control
KW  - Aggregates
KW  - Magnetic tunneling
KW  - Coils
KW  - Junctions
KW  - Magnetic particles
KW  - Blood vessels
KW  - Magnetic separation
DO  - 10.1109/ICRA40945.2020.9197009
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Existing approaches for robotic control of magnetic swarms are not capable of generating magnetic aggregates precisely in an arbitrarily specified target region in a fluidic flow environment. Such a swarm control capability is demanded by medical applications such as clinical embolization (i.e., localized clogging of blood vessels). This paper presents a new magnetic swarm control strategy to generate aggregates only in a specified target region under fluidic flow. Within the target region, the magnetic field generates sufficiently large magnetic forces among magnetic particles to maintain the aggregates' integrity at the junctions of blood vessels. In contrast, unintended aggregates outside the target region are disassembled by fluidic shear. The aggregation control approach achieved a mean absolute error of 0.15 mm in positioning a target region and a mean absolute error of 0.30 mm in controlling the target region's radius. With thrombin coating, 1 μm magnetic particles were controlled to perform embolization both in vitro (using microfluidic channel networks) and ex vivo (using porcine tissue). Experiments proved the effectiveness of the swarm control technique for on-demand, targeted embolization.
ER  - 

TY  - CONF
TI  - Bilateral Teleoperation Control of a Redundant Manipulator with an RCM Kinematic Constraint
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4477
EP  - 4482
AU  - H. Su
AU  - Y. Schmirander
AU  - Z. Li
AU  - X. Zhou
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2020
KW  - augmented reality
KW  - end effectors
KW  - haptic interfaces
KW  - human-robot interaction
KW  - medical robotics
KW  - motion control
KW  - position control
KW  - redundant manipulators
KW  - stability
KW  - surgery
KW  - telerobotics
KW  - energy tank model
KW  - haptic feedback
KW  - KUKA LWR4+ serial robot
KW  - Sigma 7 haptic manipulator
KW  - redundant manipulator
KW  - RCM kinematic constraint
KW  - serial robot manipulator
KW  - remote center of motion constraint
KW  - decoupled cartesian admittance control
KW  - end effector
KW  - bilateral teleoperation control stability
KW  - augmented reality
KW  - teleoperated surgery
KW  - Manipulators
KW  - Surgery
KW  - Force
KW  - Frequency modulation
KW  - Kinematics
KW  - Haptic interfaces
KW  - Switches
DO  - 10.1109/ICRA40945.2020.9197267
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a bilateral teleoperation control of a serial robot manipulator, which guarantees a Remote Center of Motion (RCM) constraint in its kinematic level, is developed. A two-layered approach based on the energy tank model is proposed to achieve haptic feedback on the end effector with a pedal switch. The redundancy of the manipulator is exploited to maintain the RCM constraint using the decoupled Cartesian Admittance Control. Transparency and stability of the proposed bilateral teleoperation are demonstrated using a KUKA LWR4+ serial robot and a Sigma 7 haptic manipulator with an RCM constraint in augmented reality. The results prove that the control can achieve not only the bilateral teleoperation but also maintain the RCM constraint.
ER  - 

TY  - CONF
TI  - From Bipedal Walking to Quadrupedal Locomotion: Full-Body Dynamics Decomposition for Rapid Gait Generation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4491
EP  - 4497
AU  - W. -L. Ma
AU  - A. D. Ames
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - robot dynamics
KW  - trajectory control
KW  - full-order dynamics
KW  - rapid generation
KW  - stepping-in-place gaits
KW  - diagonally symmetric ambling gait
KW  - dynamic bipedal walking
KW  - quadrupedal locomotion
KW  - rapid gait generation
KW  - hybrid dynamics
KW  - three-dimensional quadrupedal robot
KW  - hybrid zero dynamics framework
KW  - bipedal robots
KW  - bipedal walking gaits
KW  - quadrupedal trajectory
KW  - Legged locomotion
KW  - Robot kinematics
KW  - Dynamics
KW  - Nonlinear dynamical systems
KW  - Jacobian matrices
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196841
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper systematically decomposes a quadrupedal robot into bipeds to rapidly generate walking gaits and then recomposes these gaits to obtain quadrupedal locomotion. We begin by decomposing the full-order, nonlinear and hybrid dynamics of a three-dimensional quadrupedal robot, including its continuous and discrete dynamics, into two bipedal systems that are subject to external forces. Using the hybrid zero dynamics (HZD) framework, gaits for these bipedal robots can be rapidly generated (on the order of seconds) along with corresponding controllers. The decomposition is achieved in such a way that the bipedal walking gaits and controllers can be composed to yield dynamic walking gaits for the original quadrupedal robot - the result is the rapid generation of dynamic quadruped gaits utilizing the full-order dynamics. This methodology is demonstrated through the rapid generation (3.96 seconds on average) of four stepping-in-place gaits and one diagonally symmetric ambling gait at 0.35 m/s on a quadrupedal robot - the Vision 60, with 36 state variables and 12 control inputs - both in simulation and through outdoor experiments. This suggested a new approach for fast quadrupedal trajectory planning using full-body dynamics, without the need for empirical model simplification, wherein methods from dynamic bipedal walking can be directly applied to quadrupeds.
ER  - 

TY  - CONF
TI  - Posture Control for a Low-Cost Commercially-Available Hexapod Robot*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4498
EP  - 4504
AU  - M. Tikam
AU  - D. Withey
AU  - N. J. Theron
PY  - 2020
KW  - compliance control
KW  - force control
KW  - gait analysis
KW  - image motion analysis
KW  - legged locomotion
KW  - position control
KW  - robot dynamics
KW  - torque control
KW  - direct force control
KW  - Vicon motion capture system
KW  - custom-designed platforms
KW  - low-cost commercially-available hexapod robot
KW  - legged robots
KW  - custom-designed robotic platforms
KW  - commercially-available robots
KW  - low-cost research platforms
KW  - low-cost joint actuators
KW  - torque control capabilities
KW  - hierarchical control system
KW  - virtual model control
KW  - simple foot force distribution
KW  - walking posture control system
KW  - Legged locomotion
KW  - Foot
KW  - Force
KW  - Robot sensing systems
KW  - Control systems
KW  - Engines
DO  - 10.1109/ICRA40945.2020.9197147
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Posture control for legged robots has been widely developed on custom-designed robotic platforms, with little work being done on commercially-available robots despite their potential as low-cost research platforms. This paper presents the implementation of a Walking Posture Control system on a commercially-available hexapod robot which utilizes low-cost joint actuators without torque control capabilities. The hierarchical control system employs Virtual Model Control with simple foot force distribution and a novel, position-based Foot Force Controller that enables direct force control during the leg's stance phase and active compliance control during the swing phase. Ground truth measurements in experimental tests, obtained with a Vicon motion capture system, demonstrate the improvement to posture made by the control system on uneven terrain, with the results comparing favorably to those obtained in similar tests on more sophisticated, custom-designed platforms.
ER  - 

TY  - CONF
TI  - Collaborative Multi-Robot Localization in Natural Terrain*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4529
EP  - 4535
AU  - A. Wiktor
AU  - S. Rock
PY  - 2020
KW  - autonomous underwater vehicles
KW  - filtering theory
KW  - mobile robots
KW  - Monte Carlo methods
KW  - multi-robot systems
KW  - path planning
KW  - sensor fusion
KW  - Monterey Bay
KW  - terrain relative navigation
KW  - filter architecture
KW  - collaborative multirobot localization
KW  - standard TRN
KW  - Monte Carlo simulation
KW  - inter-vehicle range measurements
KW  - autonomous underwater vehicle
KW  - multirobot information
KW  - TRN techniques
KW  - covariance intersection
KW  - Robots
KW  - Correlation
KW  - Atmospheric measurements
KW  - Particle measurements
KW  - Extraterrestrial measurements
KW  - Collaboration
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197576
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel filter architecture that allows a team of vehicles to collaboratively localize using Terrain Relative Navigation (TRN). The work explores several causes of measurement correlation that preclude the use of traditional estimators, and proposes an estimator structure that eliminates one source of measurement correlation while properly incorporating others through the use of Covariance Intersection. The result is a consistent estimator that is able to augment proven TRN techniques with multi-robot information, significantly improving localization for vehicles in uninformative terrain. The approach is demonstrated using field data from an Autonomous Underwater Vehicle (AUV) navigating with TRN in Monterey Bay and simulated inter-vehicle range measurements. In addition, a Monte Carlo simulation was used to quantify the algorithm's performance on one example mission. Monte Carlo results show that a vehicle operating in uninformative terrain has 62% lower localization error when fusing range measurements to two converged AUVs than it would using standard TRN.
ER  - 

TY  - CONF
TI  - Multi-Robot Control Using Coverage Over Time-Varying Non-Convex Domains
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4536
EP  - 4542
AU  - X. Xu
AU  - Y. Diaz-Mercado
PY  - 2020
KW  - computational geometry
KW  - linearisation techniques
KW  - multi-robot systems
KW  - time-varying systems
KW  - time-varying domains
KW  - nonconvex shape
KW  - nonconvex coverage problem
KW  - control law
KW  - time-varying density
KW  - time-varying diffeomorphism
KW  - multirobot control
KW  - time-varying nonconvex domains
KW  - coverage control
KW  - Robot kinematics
KW  - Multi-robot systems
KW  - Time-varying systems
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Transforms
DO  - 10.1109/ICRA40945.2020.9196630
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the problem of a domain becoming non-convex while using coverage control of a multirobot system over time-varying domains. When the domain moves around in the workspace, its motion and the presence of obstacles might cause it to deform into some non-convex shape, and the robot team should act in a coordinating manner to maintain coverage. The proposed solution is based on a framework for constructing a diffeomorphism to transform a non-convex coverage problem into a convex one. A control law is developed to capture the effects of time variations (e.g., from a time-varying density, time-varying convex hull of the domain and time-varying diffeomorphism) in the system. Analytic expressions of each term in the control law are found for uniform density case. A simulation and robotic implementation are used to validate the proposed algorithm.
ER  - 

TY  - CONF
TI  - Efficient Large-Scale Multi-Drone Delivery Using Transit Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4543
EP  - 4550
AU  - S. Choudhury
AU  - K. Solovey
AU  - M. J. Kochenderfer
AU  - M. Pavone
PY  - 2020
KW  - autonomous aerial vehicles
KW  - computational complexity
KW  - graph theory
KW  - multi-robot systems
KW  - optimisation
KW  - near-optimal polynomial-time task allocation algorithm
KW  - delivery sequences
KW  - two-layer approach
KW  - multifaceted complexity
KW  - maximum time
KW  - comprehensive algorithmic framework
KW  - public transit vehicles
KW  - efficient large-scale multidrone delivery
KW  - transit network
KW  - bounded-suboptimal multiagent pathfinding techniques
KW  - Drones
KW  - Task analysis
KW  - Resource management
KW  - Routing
KW  - Urban areas
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197313
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of controlling a large fleet of drones to deliver packages simultaneously across broad urban areas. To conserve energy, drones hop between public transit vehicles (e.g., buses and trams). We design a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery. We address the multifaceted complexity of the problem through a two-layer approach. First, the upper layer assigns drones to package delivery sequences with a near-optimal polynomial-time task allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network while employing efficient bounded-suboptimal multi-agent pathfinding techniques tailored to our setting. Experiments demonstrate the efficiency of our approach on settings with up to 200 drones, 5000 packages, and transit networks with up to 8000 stops in San Francisco and Washington DC. Our results show that the framework computes solutions within a few seconds (up to 2 minutes at most) on commodity hardware, and that drones travel up to 450% of their flight range with public transit.
ER  - 

TY  - CONF
TI  - Resilience in multi-robot target tracking through reconfiguration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4551
EP  - 4557
AU  - R. K. Ramachandran
AU  - N. Fronda
AU  - G. S. Sukhatme
PY  - 2020
KW  - convex programming
KW  - covariance matrices
KW  - integer programming
KW  - Kalman filters
KW  - mobile robots
KW  - multi-robot systems
KW  - target tracking
KW  - multirobot target
KW  - resource availability
KW  - networked multirobot system
KW  - target tracking
KW  - sensing resources
KW  - computational resources
KW  - distributed Kalman filter
KW  - sensor measurement noise covariance matrix
KW  - sensing quality deteriorates
KW  - systems communication graph
KW  - sensor quality
KW  - active communication links
KW  - mixed integer semidefinite programming formulations
KW  - agent-centric strategy
KW  - team-centric strategy
KW  - greedy strategy
KW  - Robot sensing systems
KW  - Target tracking
KW  - Covariance matrices
KW  - Kalman filters
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196961
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the problem of maintaining resource availability in a networked multi-robot system performing distributed target tracking. In our model, robots are equipped with sensing and computational resources enabling them to track a target's position using a Distributed Kalman Filter (DKF). We use the trace of each robot's sensor measurement noise covariance matrix as a measure of sensing quality. When a robot's sensing quality deteriorates, the systems communication graph is modified by adding edges such that the robot with deteriorating sensor quality may share information with other robots to improve the team's target tracking ability. This computation is performed centrally and is designed to work without a large change in the number of active communication links. We propose two mixed integer semi-definite programming formulations (an `agent-centric' strategy and a `team-centric' strategy) to achieve this goal. We implement both formulations and a greedy strategy in simulation and show that the team-centric strategy outperforms the agent-centric and greedy strategies.
ER  - 

TY  - CONF
TI  - Teleoperation of Multi-Robot Systems to Relax Topological Constraints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4558
EP  - 4564
AU  - L. Sabattini
AU  - B. Capelli
AU  - C. Fantuzzi
AU  - C. Secchi
PY  - 2020
KW  - graph theory
KW  - mobile robots
KW  - multi-robot systems
KW  - telerobotics
KW  - motion pattern
KW  - graph
KW  - multirobot teleoperation
KW  - mobile robots
KW  - topological constraints
KW  - Multi-robot systems
KW  - Force
KW  - Mobile robots
KW  - Force feedback
KW  - Collision avoidance
KW  - Damping
DO  - 10.1109/ICRA40945.2020.9197254
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multi-robot systems are able to achieve common objectives exchanging information among each other. This is possible exploiting a communication structure, usually modeled as a graph, whose topological properties (such as connectivity) are very relevant in the overall performance of the multirobot system. When considering mobile robots, such properties can change over time: robots are then controlled to preserve them, thus guaranteeing the possibility, for the overall system, to achieve its goals. This, however, implies limitations on the possible motion patterns of the robots, thus reducing the flexibility of the overall multi-robot system. In this paper we introduce teleoperation as a means to reduce these limitations, allowing temporary violations of topological properties, with the aim of increasing the flexibility of the multi-robot system.
ER  - 

TY  - CONF
TI  - Eciton robotica: Design and Algorithms for an Adaptive Self-Assembling Soft Robot Collective
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4565
EP  - 4571
AU  - M. Malley
AU  - B. Haghighat
AU  - L. Houe
AU  - R. Nagpal
PY  - 2020
KW  - mobile robots
KW  - multi-robot systems
KW  - search problems
KW  - self-adjusting systems
KW  - Eciton robotica
KW  - self-assembling soft robot collective
KW  - social insects
KW  - centralized control system
KW  - army ants build bridges
KW  - flexible materials
KW  - robotic collectives
KW  - flexible robots
KW  - self-assembling robotic systems
KW  - lattice-based structures
KW  - soft robots
KW  - amorphous structures
KW  - Robot sensing systems
KW  - Bridges
KW  - Grippers
KW  - Vibrations
KW  - Self-assembly
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9196565
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Social insects successfully create bridges, rafts, nests and other structures out of their own bodies and do so with no centralized control system, simply by following local rules. For example, while traversing rough terrain, army ants (genus Eciton) build bridges which grow and dissolve in response to local traffic. Because these self-assembled structures incorporate smart, flexible materials (i.e. ant bodies) and emerge from local behavior, the bridges are adaptive and dynamic. With the goal of realizing robotic collectives with similar features, we designed a hardware system, Eciton robotica, consisting of flexible robots that can climb over each other to assemble compliant structures and communicate locally using vibration. In simulation, we demonstrate self-assembly of structures: using only local rules and information, robots build and dissolve bridges in response to local traffic and varying terrain. Unlike previous self-assembling robotic systems that focused on lattice-based structures and predetermined shapes, our system takes a new approach where soft robots attach to create amorphous structures whose final self-assembled shape can adapt to the needs of the group.
ER  - 

TY  - CONF
TI  - Stable Tool-Use with Flexible Musculoskeletal Hands by Learning the Predictive Model of Sensor State Transition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4572
EP  - 4578
AU  - K. Kawaharazuka
AU  - K. Tsuzuki
AU  - M. Onitsuka
AU  - Y. Asano
AU  - K. Okada
AU  - K. Kawasaki
AU  - M. Inaba
PY  - 2020
KW  - feedback
KW  - feedforward
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - materials handling
KW  - sensors
KW  - feedforward controls
KW  - initial contact state
KW  - predictive network
KW  - sensor state transition
KW  - actual robot sensor information
KW  - feedback control
KW  - stable tool-use
KW  - flexible musculoskeletal hands
KW  - predictive model
KW  - adaptability
KW  - impact resistance
KW  - sensors
KW  - actuators
KW  - Grasping
KW  - Muscles
KW  - Robot sensing systems
KW  - Gold
KW  - Tools
DO  - 10.1109/ICRA40945.2020.9197188
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The flexible under-actuated musculoskeletal hand is superior in its adaptability and impact resistance. On the other hand, since the relationship between sensors and actuators cannot be uniquely determined, almost all its controls are based on feedforward controls. When grasping and using a tool, the contact state of the hand gradually changes due to the inertia of the tool or impact of action, and the initial contact state is hardly kept. In this study, we propose a system that trains the predictive network of sensor state transition using the actual robot sensor information, and keeps the initial contact state by a feedback control using the network. We conduct experiments of hammer hitting, vacuuming, and brooming, and verify the effectiveness of this study.
ER  - 

TY  - CONF
TI  - Learning to Transfer Dynamic Models of Underactuated Soft Robotic Hands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4579
EP  - 4585
AU  - L. Schramm
AU  - A. Sintov
AU  - A. Boularias
PY  - 2020
KW  - dexterous manipulators
KW  - learning (artificial intelligence)
KW  - Lyapunov methods
KW  - neural nets
KW  - underactuated soft robotic hands
KW  - transfer learning
KW  - data limitations
KW  - data collection
KW  - physical robots
KW  - neural networks
KW  - transferred model
KW  - trained transition model
KW  - dynamic model
KW  - chaotic behavior
KW  - divergent behavior
KW  - upper bound
KW  - Lyapunov exponent
KW  - Adaptation models
KW  - Robots
KW  - Data models
KW  - Neural networks
KW  - Analytical models
KW  - Friction
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9197300
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Transfer learning is a popular approach to bypassing data limitations in one domain by leveraging data from another domain. This is especially useful in robotics, as it allows practitioners to reduce data collection with physical robots, which can be time-consuming and cause wear and tear. The most common way of doing this with neural networks is to take an existing neural network, and simply train it more with new data. However, we show that in some situations this can lead to significantly worse performance than simply using the transferred model without adaptation. We find that a major cause of these problems is that models trained on small amounts of data can have chaotic or divergent behavior in some regions. We derive an upper bound on the Lyapunov exponent of a trained transition model, and demonstrate two approaches that make use of this insight. Both show significant improvement over traditional fine-tuning. Experiments performed on real underactuated soft robotic hands clearly demonstrate the capability to transfer a dynamic model from one hand to another.
ER  - 

TY  - CONF
TI  - Periodic movement learning in a soft-robotic arm*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4586
EP  - 4592
AU  - P. Oikonomou
AU  - M. Khamassi
AU  - C. S. Tzafestas
PY  - 2020
KW  - end effectors
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - motion control
KW  - trajectory control
KW  - cyclic rhythmic patterns
KW  - oscillatory signals
KW  - actuator
KW  - central pattern generator
KW  - periodic motion
KW  - end-effector
KW  - model-free neurodynamic scheme
KW  - CPG model
KW  - simulation model
KW  - learning architecture
KW  - periodic movement learning
KW  - modular bio-inspired soft-robotic arm
KW  - Oscillators
KW  - Trajectory
KW  - Manipulators
KW  - Biological system modeling
KW  - Soft robotics
KW  - Mathematical model
KW  - Reinforcement learning
KW  - Central pattern generators
KW  - Soft Robotics
KW  - Rhythmic movements
DO  - 10.1109/ICRA40945.2020.9197035
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we introduce a novel technique that aims to dynamically control a modular bio-inspired soft-robotic arm in order to perform cyclic rhythmic patterns. Oscillatory signals are produced at the actuator's level by a central pattern generator (CPG), resulting in the generation of a periodic motion by the robot's end-effector. The proposed controller is based on a model-free neurodynamic scheme and is assigned with the task of training a policy that computes the parameters of the CPG model which generates a trajectory with desired features. The proposed methodology is first evaluated with a simulation model, which successfully reproduces the trained targets. Then experiments are also conducted using the real robot. Both procedures validate the efficiency of the learning architecture to successfully complete these tasks.
ER  - 

TY  - CONF
TI  - Mechanism and Model of a Soft Robot for Head Stabilization in Cancer Radiation Therapy
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4609
EP  - 4615
AU  - O. Ogunmolu
AU  - X. Liu
AU  - N. Gans
AU  - R. D. Wiersma
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - cancer
KW  - deformation
KW  - elastic deformation
KW  - elasticity
KW  - electric actuators
KW  - finite element analysis
KW  - kinematics
KW  - medical image processing
KW  - medical robotics
KW  - radiation therapy
KW  - robot kinematics
KW  - stress-strain relations
KW  - soft robot
KW  - head stabilization
KW  - cancer radiation
KW  - parallel robot mechanism
KW  - constituent soft actuators
KW  - real-time motion-correction
KW  - treatment machine
KW  - stress-strain constitutive laws
KW  - inverse kinematics
KW  - radially symmetric displacement formulation
KW  - finite elastic deformation framework
KW  - Strain
KW  - Actuators
KW  - Adaptation models
KW  - Stress
KW  - Real-time systems
KW  - Robots
KW  - Cancer
DO  - 10.1109/ICRA40945.2020.9197007
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a parallel robot mechanism and the constitutive laws that govern the deformation of its constituent soft actuators. Our ultimate goal is the real-time motion-correction of a patient's head deviation from a target pose where the soft actuators control the position of the patient's cranial region on a treatment machine. We describe the mechanism, derive the stress-strain constitutive laws for the individual actuators and the inverse kinematics that prescribes a given deformation, and then present simulation results that validate our mathematical formulation. Our results demonstrate deformations consistent with our radially symmetric displacement formulation under a finite elastic deformation framework.
ER  - 

TY  - CONF
TI  - Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4616
EP  - 4622
AU  - I. Akinola
AU  - J. Varley
AU  - D. Kalashnikov
PY  - 2020
KW  - calibration
KW  - cameras
KW  - image colour analysis
KW  - image representation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - multiple depth sensors
KW  - imperfect camera calibration
KW  - uncalibrated cameras
KW  - camera-views
KW  - single view robotic agents
KW  - voxel grid
KW  - relative pose estimation
KW  - 3D scene representations
KW  - registered output
KW  - explicit 3D representations
KW  - sensor dropout
KW  - insertion tasks
KW  - task performance
KW  - multicamera approach
KW  - uncalibrated RGB camera
KW  - precise manipulation tasks
KW  - closed-loop end-to-end learning
KW  - multiview approach
KW  - multiple uncalibrated cameras
KW  - precise 3D manipulation
KW  - Task analysis
KW  - Cameras
KW  - Robot vision systems
KW  - Three-dimensional displays
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197181
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we present an effective multi-view approach to closed-loop end-to-end learning of precise manipulation tasks that are 3D in nature. Our method learns to accomplish these tasks using multiple statically placed but uncalibrated RGB camera views without building an explicit 3D representation such as a pointcloud or voxel grid. This multi-camera approach achieves superior task performance on difficult stacking and insertion tasks compared to single-view baselines. Single view robotic agents struggle from occlusion and challenges in estimating relative poses between points of interest. While full 3D scene representations (voxels or pointclouds) are obtainable from registered output of multiple depth sensors, several challenges complicate operating off such explicit 3D representations. These challenges include imperfect camera calibration, poor depth maps due to object properties such as reflective surfaces, and slower inference speeds over 3D representations compared to 2D images. Our use of static but uncalibrated cameras does not require camera-robot or camera-camera calibration making the proposed approach easy to setup and our use of sensor dropout during training makes it resilient to the loss of camera-views after deployment.
ER  - 

TY  - CONF
TI  - Surfing on an uncertain edge: Precision cutting of soft tissue using torque-based medium classification
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4623
EP  - 4629
AU  - A. Straižys
AU  - M. Burke
AU  - S. Ramamoorthy
PY  - 2020
KW  - biological tissues
KW  - closed loop systems
KW  - image classification
KW  - manipulator dynamics
KW  - medical robotics
KW  - robot vision
KW  - surgery
KW  - torque measurement
KW  - joint torque measurements
KW  - closed loop control law
KW  - grapefruit cutting task
KW  - grapefruit pulp
KW  - uncertain edge
KW  - precision cutting
KW  - soft tissue
KW  - torque-based medium classification
KW  - visibility constraints
KW  - cutting trajectory
KW  - binary medium classifier
KW  - robotics
KW  - Task analysis
KW  - Trajectory
KW  - Pipelines
KW  - Torque
KW  - Robot sensing systems
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9196623
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Precision cutting of soft-tissue remains a challenging problem in robotics, due to the complex and unpredictable mechanical behaviour of tissue under manipulation. Here, we consider the challenge of cutting along the boundary between two soft mediums, a problem that is made extremely difficult due to visibility constraints, which means that the precise location of the cutting trajectory is typically unknown. This paper introduces a novel strategy to address this task, using a binary medium classifier trained using joint torque measurements, and a closed loop control law that relies on an error signal compactly encoded in the decision boundary of the classifier. We illustrate this on a grapefruit cutting task, successfully modulating a nominal trajectory t using dynamic movement primitives to follow the boundary between grapefruit pulp and peel using torque based medium classification. Results show that this control strategy is successful in 72 % of attempts in contrast to control using a nominal trajectory, which only succeeds in 50 % of attempts.
ER  - 

TY  - CONF
TI  - Dynamic Cloth Manipulation with Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4630
EP  - 4636
AU  - R. Jangir
AU  - G. Alenyà
AU  - C. Torras
PY  - 2020
KW  - clothing
KW  - learning (artificial intelligence)
KW  - sparse reward learning techniques
KW  - deep reinforcement learning approach
KW  - dynamic cloth manipulation tasks
KW  - rigid objects
KW  - followed trajectory
KW  - grasped points
KW  - goal positions
KW  - nongrasped points
KW  - adequate trajectories
KW  - control policy learning
KW  - sparse reward approach
KW  - engineering complex reward functions
KW  - state representations
KW  - control policy encodings
KW  - Task analysis
KW  - Manipulator dynamics
KW  - Trajectory
KW  - Textiles
KW  - Deformable models
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196659
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we present a Deep Reinforcement Learning approach to solve dynamic cloth manipulation tasks. Differing from the case of rigid objects, we stress that the followed trajectory (including speed and acceleration) has a decisive influence on the final state of cloth, which can greatly vary even if the positions reached by the grasped points are the same. We explore how goal positions for non-grasped points can be attained through learning adequate trajectories for the grasped points. Our approach uses few demonstrations to improve control policy learning, and a sparse reward approach to avoid engineering complex reward functions. Since perception of textiles is challenging, we also study different state representations to assess the minimum observation space required for learning to succeed. Finally, we compare different combinations of control policy encodings, demonstrations, and sparse reward learning techniques, and show that our proposed approach can learn dynamic cloth manipulation in an efficient way, i.e., using a reduced observation space, a few demonstrations, and a sparse reward.
ER  - 

TY  - CONF
TI  - Learning to combine primitive skills: A step towards versatile robotic manipulation §
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4637
EP  - 4643
AU  - R. Strudel
AU  - A. Pashevich
AU  - I. Kalevatykh
AU  - I. Laptev
AU  - J. Sivic
AU  - C. Schmid
PY  - 2020
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - path planning
KW  - robot vision
KW  - dynamic scene changes
KW  - visual inputs
KW  - task-specific reward engineering
KW  - previous limitations
KW  - reinforcement learning approach
KW  - primitive skills
KW  - learning methods
KW  - intermediate rewards
KW  - complete task demonstrations
KW  - vision-based task planning
KW  - basic skills
KW  - synthetic demonstrations
KW  - data augmentation
KW  - manipulation tasks
KW  - UR5 robotic arm
KW  - versatile robotic manipulation
KW  - robotics
KW  - traditional task
KW  - motion planning methods
KW  - state observability
KW  - Task analysis
KW  - Robots
KW  - Planning
KW  - Learning (artificial intelligence)
KW  - Training
KW  - Trajectory
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196619
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Manipulation tasks such as preparing a meal or assembling furniture remain highly challenging for robotics and vision. Traditional task and motion planning (TAMP) methods can solve complex tasks but require full state observability and are not adapted to dynamic scene changes. Recent learning methods can operate directly on visual inputs but typically require many demonstrations and/or task-specific reward engineering. In this work we aim to overcome previous limitations and propose a reinforcement learning (RL) approach to task planning that learns to combine primitive skills. First, compared to previous learning methods, our approach requires neither intermediate rewards nor complete task demonstrations during training. Second, we demonstrate the versatility of our vision-based task planning in challenging settings with temporary occlusions and dynamic scene changes. Third, we propose an efficient training of basic skills from few synthetic demonstrations by exploring recent CNN architectures and data augmentation. Notably, while all of our policies are learned on visual inputs in simulated environments, we demonstrate the successful transfer and high success rates when applying such policies to manipulation tasks on a real UR5 robotic arm.
ER  - 

TY  - CONF
TI  - Learning Affordance Space in Physical World for Vision-based Robotic Object Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4652
EP  - 4658
AU  - H. Wu
AU  - Z. Zhang
AU  - H. Cheng
AU  - K. Yang
AU  - J. Liu
AU  - Z. Guo
PY  - 2020
KW  - image texture
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - probability
KW  - robot vision
KW  - pixel-wise probability affordance map
KW  - image space
KW  - world space
KW  - viewpoints
KW  - multiple-object pushing
KW  - multiple-object grasping
KW  - physical world
KW  - vision-based robotic object manipulation
KW  - Affordance Space Perception Network
KW  - deep neural network
KW  - 3D affordance space
KW  - training strategy
KW  - task-agnostic framework
KW  - singular-object pushing
KW  - singular-object grasping
KW  - Robots
KW  - Task analysis
KW  - Robustness
KW  - Grasping
KW  - Data models
KW  - Calibration
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9196783
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - What is a proper representation for objects in manipulation? What would human try to perceive when manipulating a new object in a new environment? In fact, instead of focusing on the texture and illumination, human can infer the "affordance" [36] of the objects from vision. Here "affordance" describes the object's intrinsic property that affords a particular type of manipulation. In this work, we investigate whether such affordance can be learned by a deep neural network. In particular, we propose an Affordance Space Perception Network (ASPN) that takes an image as input and outputs an affordance map. Different from existing works that infer the pixel-wise probability affordance map in image space, our affordance is defined in the real world space, thus eliminates the need of hand-eye calibration. In addition, we extend the representation ability of affordance by defining it in a 3D affordance space and propose a novel training strategy to improve the performance. Trained purely with simulation data, ASPN can achieve significant performance in the real world. It is a task-agnostic framework and can handle different objects, scenes and viewpoints. Extensive real-world experiments demonstrate the accuracy and robustness of our approach. We achieve the success rates of 94.2% for singular-object pushing and 92.4% for multiple-object pushing. We also achieve the success rates of 97.2% for singular-object grasping and 95.4% for multiple-object grasping, which outperform current state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Observability Analysis of Flight State Estimation for UAVs and Experimental Validation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4659
EP  - 4665
AU  - P. Huang
AU  - H. Meyr
AU  - M. Dörpinghaus
AU  - G. Fettweis
PY  - 2020
KW  - inertial systems
KW  - Kalman filters
KW  - magnetic sensors
KW  - nonlinear filters
KW  - observability
KW  - pressure sensors
KW  - remotely operated vehicles
KW  - singular value decomposition
KW  - state estimation
KW  - UAV
KW  - cost-efficient onboard flight state estimation
KW  - robustness
KW  - MEMS-based inertial system
KW  - static pressure sensors
KW  - dynamic pressure sensors
KW  - magnetic sensor
KW  - weak magnetic field
KW  - necessary condition
KW  - system state
KW  - in-depth observability analysis
KW  - sensor data
KW  - test flights
KW  - EKF
KW  - undisturbed estimates
KW  - wind state variable
KW  - observable spaces
KW  - multisensor extended Kalman filter
KW  - singular value decomposition
KW  - SVD
KW  - glider
KW  - Observability
KW  - Mathematical model
KW  - Aerodynamics
KW  - State estimation
KW  - Global Positioning System
KW  - Magnetometers
KW  - Pressure measurement
DO  - 10.1109/ICRA40945.2020.9196635
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - UAVs require reliable, cost-efficient onboard flight state estimation that achieves high accuracy and robustness to perturbation. We analyze a multi-sensor extended Kalman filter (EKF) based on the work by Leutenegger. The EKF uses measurements from a MEMS-based inertial system, static and dynamic pressure sensors as well as GPS. As opposed to other implementations we do not use a magnetic sensor because the weak magnetic field of the earth is subject to disturbances. Observability of the state is a necessary condition for the EKF to work. In this paper, we demonstrate that the system state is observable - which is in contrast to statements in the literature - if the random nature of the air mass is taken into account. Therefore, we carry out an in-depth observability analysis based on a singular value decomposition (SVD). The numerical SVD delivers a wealth of information regarding the observable (sub)spaces. We validated the theoretical findings based on sensor data recorded in test flights on a glider. Most importantly, we demonstrate that the EKF works. It is capable of absorbing large perturbations in the wind state variable converging to the undisturbed estimates.
ER  - 

TY  - CONF
TI  - OpenVINS: A Research Platform for Visual-Inertial Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4666
EP  - 4672
AU  - P. Geneva
AU  - K. Eckenhoff
AU  - W. Lee
AU  - Y. Yang
AU  - G. Huang
PY  - 2020
KW  - calibration
KW  - cameras
KW  - estimation theory
KW  - image filtering
KW  - Kalman filters
KW  - robot vision
KW  - SLAM (robots)
KW  - research platform
KW  - visual-inertial estimation research
KW  - open sourced codebase
KW  - visual-inertial systems
KW  - visual-inertial estimation features
KW  - on-manifold sliding window Kalman filter
KW  - consistent First-Estimates Jacobian treatments
KW  - modular type system
KW  - extendable visual-inertial system simulator
KW  - competing estimation performance
KW  - OpenVINS
KW  - online camera intrinsic calibration
KW  - open sourced algorithms
KW  - online camera extrinsic calibration
KW  - inertial sensor time offset calibration
KW  - SLAM landmarks
KW  - state management
KW  - Cameras
KW  - Current measurement
KW  - Jacobian matrices
KW  - Calibration
KW  - Documentation
KW  - Estimation
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9196524
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an open platform, termed OpenVINS, for visual-inertial estimation research for both the academic community and practitioners from industry. The open sourced codebase provides a foundation for researchers and engineers to quickly start developing new capabilities for their visual-inertial systems. This codebase has out of the box support for commonly desired visual-inertial estimation features, which include: (i) on-manifold sliding window Kalman filter, (ii) online camera intrinsic and extrinsic calibration, (iii) camera to inertial sensor time offset calibration, (iv) SLAM landmarks with different representations and consistent First-Estimates Jacobian (FEJ) treatments, (v) modular type system for state management, (vi) extendable visual-inertial system simulator, and (vii) extensive toolbox for algorithm evaluation. Moreover, we have also focused on detailed documentation and theoretical derivations to support rapid development and research, which are greatly lacked in the current open sourced algorithms. Finally, we perform comprehensive validation of the proposed OpenVINS against state-of-the-art open sourced algorithms, showing its competing estimation performance.
ER  - 

TY  - CONF
TI  - Decentralized Collaborative State Estimation for Aided Inertial Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4673
EP  - 4679
AU  - R. Jung
AU  - C. Brommer
AU  - S. Weiss
PY  - 2020
KW  - communication complexity
KW  - inertial navigation
KW  - Kalman filters
KW  - nonlinear filters
KW  - position measurement
KW  - state estimation
KW  - communication links
KW  - versatile filter formulation
KW  - independent state estimation
KW  - relative position measurements
KW  - aided inertial navigation
KW  - Q-ESEKF
KW  - IMU propagation
KW  - communication complexity
KW  - decentralized collaborative state estimation
KW  - quaternion-based error-state extended Kalman filter
KW  - CSE concept
KW  - probabilistic reinitialization
KW  - prominent benchmark datasets
KW  - Cameras
KW  - Sensors
KW  - Collaboration
KW  - State estimation
KW  - Three-dimensional displays
KW  - Calibration
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197178
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a Quaternion-based Error-State Extended Kalman Filter (Q-ESEKF) based on IMU propagation with an extension for Collaborative State Estimation (CSE) and a communication complexity of O(1) (in terms of required communication links). Our approach combines a versatile filter formulation with the concept of CSE, allowing independent state estimation on each of the agents and at the same time leveraging and statistically maintaining interdependencies between agents, after joint measurements and communication (i.e. relative position measurements) occur. We discuss the development of the overall framework and the probabilistic (re-)initialization of the agent's states upon initial or recurring joint observations. Our approach is evaluated in a simulation framework on two prominent benchmark datasets in 3D.
ER  - 

TY  - CONF
TI  - Analytic Combined IMU Integration (ACI2) For Visual Inertial Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4680
EP  - 4686
AU  - Y. Yang
AU  - B. P. Wisely Babu
AU  - C. Chen
AU  - G. Huang
AU  - L. Ren
PY  - 2020
KW  - calibration
KW  - inertial navigation
KW  - maximum likelihood estimation
KW  - Monte Carlo methods
KW  - optimisation
KW  - robot vision
KW  - sensor fusion
KW  - SLAM (robots)
KW  - analytic combined IMU integration
KW  - visual inertial navigation
KW  - batch optimization
KW  - visual sensor fusion
KW  - robotic tasks
KW  - maximum likelihood estimation
KW  - partial-fixed estimates
KW  - ACI2
KW  - inertial measurement unit
KW  - Monte-Carlo simulations
KW  - Optimization
KW  - Jacobian matrices
KW  - Maximum likelihood estimation
KW  - Time measurement
KW  - Cameras
KW  - Visualization
KW  - Calibration
DO  - 10.1109/ICRA40945.2020.9197280
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Batch optimization based inertial measurement unit (IMU) and visual sensor fusion enables high rate localization for many robotic tasks. However, it remains a challenge to ensure that the batch optimization is computationally efficient while being consistent for high rate IMU measurements without marginalization. In this paper, we derive inspiration from maximum likelihood estimation with partial-fixed estimates to provide a unified approach for handing both IMU preintegration and time-offset calibration. We present a modularized analytic combined IMU integrator (ACI2) with elegant derivations for IMU integrations, bias Jabcobians and related covariances. To simplify our derivation, we also prove that the right Jacobians for Hamilton quaterions and SO(3) are equivalent. Finally, we present a time offset calibrator that operates by fixing the linearization point for a given time offset. This reduces re-integration of the IMU measurements and thus improve efficiency. The proposed ACI2 and time-offset calibration is verified by intensive Monte-Carlo simulations generated from real world datasets. A proof-of-concept real world experiment is also conducted to verify the proposed ACI2 estimator.
ER  - 

TY  - CONF
TI  - Second-order Kinematics for Floating-base Robots using the Redundant Acceleration Feedback of an Artificial Sensory Skin
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4687
EP  - 4694
AU  - Q. Leboutet
AU  - J. R. Guadarrama-Olvera
AU  - F. Bergner
AU  - G. Cheng
PY  - 2020
KW  - calibration
KW  - estimation theory
KW  - feedback
KW  - humanoid robots
KW  - Kalman filters
KW  - manipulator kinematics
KW  - motion control
KW  - redundant manipulators
KW  - floating-base robots
KW  - redundant acceleration feedback
KW  - artificial sensory skin
KW  - estimation method
KW  - second-order kinematics
KW  - highly redundant distributed inertial feedback
KW  - linear acceleration
KW  - robot link
KW  - skin acceleration data
KW  - link level
KW  - state dimensionality reduction
KW  - main inertial measurement unit
KW  - Sigma-point Kalman filter
KW  - joint velocities
KW  - REEM-C humanoid robot
KW  - Acceleration
KW  - Robot sensing systems
KW  - Skin
KW  - Gyroscopes
KW  - Accelerometers
KW  - Acceleration Feedback
KW  - Artificial Robot Skin
KW  - Sigma-point Kalman Filter
DO  - 10.1109/ICRA40945.2020.9197169
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we propose a new estimation method for second-order kinematics for floating-base robots, based on highly redundant distributed inertial feedback. The linear acceleration of each robot link is measured at multiple points using a multimodal, self-configuring and self-calibrating artificial skin. The proposed algorithm is two-fold: i) the skin acceleration data is fused at the link level for state dimensionality reduction; ii) the estimated values are then fused limb-wise with data from the joint encoders and the main inertial measurement unit (IMU), using a Sigma-point Kalman filter. In this manner, it is possible to estimate the joint velocities and accelerations while avoiding the lag and noise amplification phenomena associated with conventional numerical derivation approaches. Experiments performed on the right arm and torso of a REEM-C humanoid robot, demonstrate the consistency of the proposed estimation method.
ER  - 

TY  - CONF
TI  - Clock-based time sync hronization for an event-based camera dataset acquisition platform *
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4695
EP  - 4701
AU  - V. Osadcuks
AU  - M. Pudzs
AU  - A. Zujevs
AU  - A. Pecka
AU  - A. Ardavs
PY  - 2020
KW  - cameras
KW  - data acquisition
KW  - image sensors
KW  - microcontrollers
KW  - mobile robots
KW  - optical radar
KW  - robot vision
KW  - synchronisation
KW  - monocular camera
KW  - mobile robotic platform
KW  - time synchronization architecture
KW  - time synchronization approach
KW  - accurate time synchronization
KW  - event-based camera dataset acquisition platform
KW  - dynamic visual sensor
KW  - next-generation vision sensor
KW  - event-based vision
KW  - dataset creation
KW  - temporal accuracy
KW  - high temporal resolution
KW  - evaluation task
KW  - visual data
KW  - event camera
KW  - ambient environment sensors
KW  - data acquisition
KW  - clock-based time synchronization
KW  - LIDAR
KW  - PIC32 microcontroller
KW  - Synchronization
KW  - Robot sensing systems
KW  - Cameras
KW  - Laser radar
KW  - Clocks
KW  - Voltage control
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9197303
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The Dynamic Visual Sensor is considered to be a next-generation vision sensor. Since event-based vision is in its early stage of development, a small number of datasets has been created during the last decade. Dataset creation is motivated by the need for real data from one or many sensors. Temporal accuracy of data in such datasets is crucially important since the events have high temporal resolution measured in microseconds and, during an algorithm evaluation task, such type of visual data is usually fused with data from other types of sensors. The main aim of our research is to achieve the most accurate possible time synchronization between an event camera, LIDAR, and ambient environment sensors during a session of data acquisition. All the mentioned sensors as well as a stereo and a monocular camera were installed on a mobile robotic platform. In this work, a time synchronization architecture and algorithm are proposed for time synchronization with an implementation example on a PIC32 microcontroller. The overall time synchronization approach is scalable for other sensors where there is a need for accurate time synchronization between many nodes. The evaluation results of the proposed solution are reported and discussed in the paper.
ER  - 

TY  - CONF
TI  - Model Predictive Impedance Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4702
EP  - 4708
AU  - M. Bednarczyk
AU  - H. Omran
AU  - B. Bayle
PY  - 2020
KW  - human-robot interaction
KW  - predictive control
KW  - collaborative robotics
KW  - high performance control
KW  - model predictive impedance control
KW  - human robot compliant interactions
KW  - Robots
KW  - Integrated circuit modeling
KW  - Impedance
KW  - Mathematical model
KW  - Task analysis
KW  - Collaboration
KW  - Impedance control
KW  - collaborative robotics
KW  - physical human-robot interaction
DO  - 10.1109/ICRA40945.2020.9196969
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots are more and more often designed in order to perform tasks in synergy with human operators. In this context, a current research focus for collaborative robotics lies in the design of high-performance control solutions, which ensure security in spite of unmodeled external forces. The present work provides a method based on Model Predictive Control (MPC) to allow compliant behavior when interacting with an environment, while respecting practical robotic constraints. The study shows in particular how to define the impedance control problem as a MPC problem. The approach is validated with an experimental setup including a collaborative robot. The obtained results emphasize the ability of this control strategy to solve constraints like speed, energy or jerk limits, which have a direct impact on the operator's security during human-robot compliant interactions.
ER  - 

TY  - CONF
TI  - Kinematic Modeling and Compliance Modulation of Redundant Manipulators Under Bracing Constraints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4709
EP  - 4716
AU  - G. L. H. Johnston
AU  - A. L. Orekhov
AU  - N. Simaan
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - design engineering
KW  - dexterous manipulators
KW  - end effectors
KW  - manipulator kinematics
KW  - motion control
KW  - redundant manipulators
KW  - kinematic modeling
KW  - compliance modulation
KW  - redundant manipulators
KW  - bracing constraints
KW  - low torque actuators
KW  - passive safety reasons
KW  - human operator
KW  - in-situ collaborative robots
KW  - conflicting demands
KW  - low torque actuation
KW  - deep confined spaces
KW  - constrained kinematics
KW  - endeffector compliance
KW  - redundancy resolution framework
KW  - directional compliance
KW  - end-effector dexterity
KW  - kinematic simulation results
KW  - redundancy resolution strategy
KW  - kinematic conditioning
KW  - bracing task
KW  - admittance control framework
KW  - collaborative control
KW  - ISCR
KW  - Kinematics
KW  - Robots
KW  - Redundancy
KW  - Task analysis
KW  - Collaboration
KW  - Torque
KW  - Safety
KW  - Bracing
KW  - redundancy resolution
KW  - stiffness modulation
KW  - compliance
KW  - collaborative robots
DO  - 10.1109/ICRA40945.2020.9197387
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collaborative robots should ideally use low torque actuators for passive safety reasons. However, some applications require these collaborative robots to reach deep into confined spaces while assisting a human operator in physically demanding tasks. In this paper, we consider the use of in-situ collaborative robots (ISCRs) that balance the conflicting demands of passive safety dictating low torque actuation and the need to reach into deep confined spaces. We consider the judicious use of bracing as a possible solution to these conflicting demands and present a modeling framework that takes into account the constrained kinematics and the effect of bracing on the endeffector compliance. We then define a redundancy resolution framework that minimizes the directional compliance of the end-effector while maximizing end-effector dexterity. Kinematic simulation results show that the redundancy resolution strategy successfully decreases compliance and improves kinematic conditioning while satisfying the constraints imposed by the bracing task. Applications of this modeling framework can support future research on the choice of bracing locations and support the formation of an admittance control framework for collaborative control of ISCRs under bracing constraints. Such robots can benefit workers in the future by reducing the physiological burdens that contribute to musculoskeletal injury.
ER  - 

TY  - CONF
TI  - Successive Stiffness Increment and Time Domain Passivity Approach for Stable and High Bandwidth Control of Series Elastic Actuator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4717
EP  - 4723
AU  - C. Lee
AU  - D. -H. Kim
AU  - H. Singh
AU  - J. -H. Ryu
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - flexible manipulators
KW  - force control
KW  - haptic interfaces
KW  - human-robot interaction
KW  - impact (mechanical)
KW  - stability
KW  - telerobotics
KW  - time domain passivity approach
KW  - bandwidth control
KW  - human-robot interaction
KW  - flexible manipulators
KW  - series elastic actuator based manipulators
KW  - elastic element
KW  - system durability
KW  - actuators
KW  - SEA manipulator
KW  - system bandwidth
KW  - impedance control
KW  - system stability
KW  - successive stiffness increment approach
KW  - haptic domain
KW  - TDPA
KW  - system passivity
KW  - impact force
KW  - teleoperation
KW  - two-port electrical circuit network
KW  - size 350.0 m
KW  - size 120.0 m
KW  - Impedance
KW  - Bandwidth
KW  - Force
KW  - Actuators
KW  - Torque
KW  - Manipulators
KW  - Springs
DO  - 10.1109/ICRA40945.2020.9196995
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For safe human-robot interaction, various type of flexible manipulators have been developed. Especially series elastic actuator (SEA) based manipulators have been getting huge attention since the elastic element of SEA prevents people from injury when undesirable collision happens. Moreover, it improves system durability by absorbing impact force, which could damage actuators. However, the elastic element inside SEA manipulator causes low system bandwidth which limits the speed performance of conventional impedance control approaches. To alleviate the low bandwidth issue of impedance controlled SEA while guaranteeing system stability, we implement Time Domain Passivity Approach (TDPA) and Successive Stiffness Increment (SSI) approach, which was invented in haptic and teleoperation domain. Impedance controlled SEA is reformulated as a two-port electrical circuit network for implementing TDPA. In addition, a pair of input and output power conjugate variable, dominating the system passivity is identified for implementing SSI approach. Experimental results showed that TDPA and SSI approach can render the stiffness of the impedance controller, which decides the bandwidth, upto 350 kN/m without any stability issue, while normal impedance controller only render upto 120 kN/m. Although both of the approaches significantly increased the bandwidth of the impedance controlled SEA, TDPA slightly outperformed in stability, and SSI outperformed in tracking.
ER  - 

TY  - CONF
TI  - Arm-hand motion-force coordination for physical interactions with non-flat surfaces using dynamical systems: Toward compliant robotic massage
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4724
EP  - 4730
AU  - M. Khoramshahi
AU  - G. Henriks
AU  - A. Naef
AU  - S. S. M. Salehian
AU  - J. Kim
AU  - A. Billard
PY  - 2020
KW  - biomechanics
KW  - dexterous manipulators
KW  - force control
KW  - motion control
KW  - path planning
KW  - regression analysis
KW  - support vector machines
KW  - unified motion-force control approach
KW  - human limb
KW  - compliant robotic massage
KW  - dynamical system approach
KW  - skin surface
KW  - robot fingers
KW  - complexity increases
KW  - manipulation tasks
KW  - dynamical systems
KW  - nonflat surface
KW  - physical interactions
KW  - arm-hand motion-force coordination
KW  - desired motion patterns
KW  - unknown surface
KW  - mannequin arm
KW  - Allegro robotic hand
KW  - KUKA IIWA robotic arm
KW  - robotic fingers
KW  - DS-based impedance control
KW  - desired motions
KW  - distance-to-surface mapping
KW  - Surface impedance
KW  - Robot kinematics
KW  - Task analysis
KW  - Force
KW  - Manipulators
KW  - Thumb
DO  - 10.1109/ICRA40945.2020.9196593
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many manipulation tasks require coordinated motions for arm and fingers. Complexity increases when the task requires to control for the force at contact against a non-flat surface; This becomes even more challenging when this contact is done on a human. All these challenges are regrouped when one, for instance, massages a human limb. When massaging, the robotic arm is required to continuously adapt its orientation and distance to the limb while the robot fingers exert desired patterns of forces and motion on the skin surface. To address these challenges, we adopt a Dynamical System (DS) approach that offers a unified motion-force control approach and enables to easily coordinate multiple degrees of freedom. As each human limb may slightly differ, we learn a model of the surface using support vector regression (SVR) which enable us to obtain a distance-to-surface mapping. The gradient of this mapping, along with the DS, generates the desired motions for the interaction with the surface. A DS-based impedance control for the robotic fingers allows to control separately for force along the normal direction of the surface while moving in the tangential plane. We validate our approach using the KUKA IIWA robotic arm and Allegro robotic hand for massaging a mannequin arm covered with a skin-like material. We show that our approach allows for 1) reactive motion planning to reach for an unknown surface, 2) following desired motion patterns on the surface, and 3) exerting desired interaction forces profiles. Our results show the effectiveness of our approach; especially the robustness toward uncertainties for shape and the given location of the surface.
ER  - 

TY  - CONF
TI  - A Bio-Signal Enhanced Adaptive Impedance Controller for Lower Limb Exoskeleton
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4739
EP  - 4744
AU  - L. Xia
AU  - Y. Feng
AU  - F. Chen
AU  - X. Wu
PY  - 2020
KW  - adaptive control
KW  - biomechanics
KW  - electromyography
KW  - gait analysis
KW  - medical robotics
KW  - medical signal processing
KW  - motion control
KW  - neurocontrollers
KW  - patient rehabilitation
KW  - radial basis function networks
KW  - robot dynamics
KW  - torque control
KW  - unpredictable human body movements
KW  - complex body movements
KW  - elaborate control strategy design
KW  - uncertain dynamical parameters
KW  - human-exoskeleton interaction
KW  - lower limb exoskeleton
KW  - bio-signal enhanced adaptive impedance controller
KW  - rehabilitation lower-limb exoskeleton
KW  - exoskeleton track desired motion trajectory
KW  - radial basis function neural network enhanced adaptive impedance controller
KW  - surface electromyogram signals
KW  - neural network-based torque estimation method
KW  - joint torque
KW  - human lower extremity dynamics
KW  - human operator walking
KW  - Exoskeletons
KW  - Torque
KW  - Estimation
KW  - Muscles
KW  - Impedance
KW  - Artificial neural networks
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196774
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The problem of human-exoskeleton interaction with uncertain dynamical parameters remains an open-ended research area. It requires an elaborate control strategy design of the exoskeleton to accommodate complex and unpredictable human body movements. In this paper, we proposed a novel control approach for the lower limb exoskeleton to realize its task of assisting the human operator walking. The main challenge of this study was to determine the human lower extremity dynamics, such as the joint torque. For this purpose, we developed a neural network-based torque estimation method. It can predict the joint torques of humans with surface electromyogram signals (sEMG). Then an radial basis function neural network (RBF NN) enhanced adaptive impedance controller is employed to ensure exoskeleton track desired motion trajectory of a human operator. Algorithm performance is evaluated with two healthy subjects and the rehabilitation lower-limb exoskeleton developed by Shenzhen Institutes of Advanced Technology (SIAT).
ER  - 

TY  - CONF
TI  - Differentiable Mapping Networks: Learning Structured Map Representations for Sparse Visual Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4753
EP  - 4759
AU  - P. Karkus
AU  - A. Angelova
AU  - V. Vanhoucke
AU  - R. Jonschkowski
PY  - 2020
KW  - gradient methods
KW  - image representation
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - particle filtering (numerical methods)
KW  - robot vision
KW  - DMN architecture
KW  - end-to-end differentiable
KW  - sparse visual localization
KW  - end-to-end learning
KW  - differentiable mapping network
KW  - spatially structured view-embedding map
KW  - subsequent visual localization
KW  - learning structured map representations
KW  - Street View dataset
KW  - particle filter
KW  - gradient descent
KW  - robotics
KW  - neural network architecture
KW  - Task analysis
KW  - Visualization
KW  - Feature extraction
KW  - Neural networks
KW  - Robot kinematics
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197452
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mapping and localization, preferably from a small number of observations, are fundamental tasks in robotics. We address these tasks by combining spatial structure (differentiable mapping) and end-to-end learning in a novel neural network architecture: the Differentiable Mapping Network (DMN). The DMN constructs a spatially structured view-embedding map and uses it for subsequent visual localization with a particle filter. Since the DMN architecture is end-to-end differentiable, we can jointly learn the map representation and localization using gradient descent. We apply the DMN to sparse visual localization, where a robot needs to localize in a new environment with respect to a small number of images from known viewpoints. We evaluate the DMN using simulated environments and a challenging real-world Street View dataset. We find that the DMN learns effective map representations for visual localization. The benefit of spatial structure increases with larger environments, more viewpoints for mapping, and when training data is scarce. Project website: https://sites.google.com/view/differentiable-mapping.
ER  - 

TY  - CONF
TI  - Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using Video Demonstration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4760
EP  - 4766
AU  - K. Ramachandruni
AU  - M. Babu V.
AU  - A. Majumder
AU  - S. Dutta
AU  - S. Kumar
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - image representation
KW  - learning (artificial intelligence)
KW  - video signal processing
KW  - task-specific objects
KW  - intended task
KW  - imitation learning
KW  - video demonstration
KW  - end-to-end self-supervised feature representation network
KW  - video-based task imitation
KW  - multilevel spatial attention module
KW  - spatial features
KW  - weighted combination
KW  - multiple intermediate feature maps
KW  - respective feature maps
KW  - metric learning loss
KW  - multiple view points
KW  - AT-Net features
KW  - reinforcement learning problem
KW  - attentive task-net
KW  - self supervised task-attention network
KW  - neural connections
KW  - learning task-specific feature embeddings
KW  - temporally consecutive frames
KW  - publicly available multiview pouring dataset
KW  - RL agent
KW  - Gazebo simulator
KW  - CNN pipeline
KW  - Task analysis
KW  - Measurement
KW  - Robots
KW  - Feature extraction
KW  - Training
KW  - Visualization
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197544
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes an end-to-end self-supervised feature representation network named Attentive Task-Net or AT-Net for video-based task imitation. The proposed AT-Net incorporates a novel multi-level spatial attention module to highlight spatial features corresponding to the intended task demonstrated by the expert. The neural connections in AT-Net ensure the relevant information in the demonstration is amplified and the irrelevant information is suppressed while learning task-specific feature embeddings. This is achieved by a weighted combination of multiple intermediate feature maps of the input image at different stages of the CNN pipeline. The weights of the combination are given by the compatibility scores, predicted by the attention module for respective feature maps. The AT-Net is trained using a metric learning loss which aims to decrease the distance between the feature representations of concurrent frames from multiple view points and increase the distance between temporally consecutive frames. The AT-Net features are then used to formulate a reinforcement learning problem for task imitation. Through experiments on the publicly available Multi-view pouring dataset, it is demonstrated that the output of the attention module highlights the task-specific objects while suppressing the rest of the background. The efficacy of the proposed method is further validated by qualitative and quantitative comparison with a state-of-the-art technique along with intensive ablation studies. The proposed method is implemented to imitate a pouring task where an RL agent is learned with the AT-Net in Gazebo simulator. Our findings show that the AT-Net achieves 6.5% decrease in alignment error along with a reduction in the number of training iterations by almost 155k over the state-of-the-art while satisfactorily imitating the intended task.
ER  - 

TY  - CONF
TI  - OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong Deep Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4767
EP  - 4773
AU  - Q. She
AU  - F. Feng
AU  - X. Hao
AU  - Q. Yang
AU  - C. Lan
AU  - V. Lomonaco
AU  - X. Shi
AU  - Z. Wang
AU  - Y. Guo
AU  - Y. Zhang
AU  - F. Qiao
AU  - R. H. M. Chan
PY  - 2020
KW  - control engineering computing
KW  - data visualisation
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - robot vision
KW  - service robots
KW  - lifelong deep learning
KW  - visual algorithms
KW  - standard computer vision datasets
KW  - adaptive visual perceptual systems
KW  - lifelong robotic vision dataset
KW  - lifelong object recognition algorithms
KW  - lifelong learning algorithms
KW  - OpenLORIS-Object dataset
KW  - object recognition task
KW  - robotic vision
KW  - ImageNet dataset
KW  - COCO dataset
KW  - Task analysis
KW  - Lighting
KW  - Object recognition
KW  - Cameras
KW  - Robot vision systems
KW  - Clutter
DO  - 10.1109/ICRA40945.2020.9196887
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The recent breakthroughs in computer vision have benefited from the availability of large representative datasets (e.g. ImageNet and COCO) for training. Yet, robotic vision poses unique challenges for applying visual algorithms developed from these standard computer vision datasets due to their implicit assumption over non-varying distributions for a fixed set of tasks. Fully retraining models each time a new task becomes available is infeasible due to computational, storage and sometimes privacy issues, while naïve incremental strategies have been shown to suffer from catastrophic forgetting. It is crucial for the robots to operate continuously under open-set and detrimental conditions with adaptive visual perceptual systems, where lifelong learning is a fundamental capability. However, very few datasets and benchmarks are available to evaluate and compare emerging techniques. To fill this gap, we provide a new lifelong robotic vision dataset ("OpenLORIS-Object") collected via RGB-D cameras. The dataset embeds the challenges faced by a robot in the real-life application and provides new benchmarks for validating lifelong object recognition algorithms. Moreover, we have provided a testbed of 9 state-of-the-art lifelong learning algorithms. Each of them involves 48 tasks with 4 evaluation metrics over the OpenLORIS-Object dataset. The results demonstrate that the object recognition task in the ever-changing difficulty environments is far from being solved and the bottlenecks are at the forward/backward transfer designs. Our dataset and benchmark are publicly available at https://lifelong-robotic-vision.github.io/dataset/object.
ER  - 

TY  - CONF
TI  - Geometric Pretraining for Monocular Depth Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4782
EP  - 4788
AU  - K. Wang
AU  - Y. Chen
AU  - H. Guo
AU  - L. Wen
AU  - S. Shen
PY  - 2020
KW  - geometry
KW  - image classification
KW  - learning (artificial intelligence)
KW  - video signal processing
KW  - monocular depth estimation
KW  - ImageNet-pretrained networks
KW  - semantic information
KW  - spatial information
KW  - per-pixel depth estimation
KW  - geometric-pretrained networks
KW  - geometric-transferred networks
KW  - self-supervised geometric pretraining task
KW  - conditional autoencoder-decoder structure
KW  - Task analysis
KW  - Estimation
KW  - Videos
KW  - Optical imaging
KW  - Adaptive optics
KW  - Training
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196847
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - ImageNet-pretrained networks have been widely used in transfer learning for monocular depth estimation. These pretrained networks are trained with classification losses for which only semantic information is exploited while spatial information is ignored. However, both semantic and spatial information is important for per-pixel depth estimation. In this paper, we design a novel self-supervised geometric pretraining task that is tailored for monocular depth estimation using uncalibrated videos. The designed task decouples the structure information from input videos by a simple yet effective conditional autoencoder-decoder structure. Using almost unlimited videos from the internet, networks are pretrained to capture a variety of structures of the scene and can be easily transferred to depth estimation tasks using calibrated images. Extensive experiments are used to demonstrate that the proposed geometric-pretrained networks perform better than ImageNet-pretrained networks in terms of accuracy, few-shot learning and generalization ability. Using existing learning methods, geometric-transferred networks achieve new state-of-the-art results by a large margin. The pretrained networks will be open source soon1 .
ER  - 

TY  - CONF
TI  - Joint Rotation Angle Sensing of Flexible Endoscopic Surgical Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4789
EP  - 4795
AU  - W. Lai
AU  - L. Cao
AU  - P. T. Phan
AU  - I. -W. Wu
AU  - S. C. Tjin
AU  - S. Jay Phee
PY  - 2020
KW  - Bragg gratings
KW  - closed loop systems
KW  - endoscopes
KW  - fibre optic sensors
KW  - manipulators
KW  - medical robotics
KW  - motion control
KW  - surgery
KW  - closed-loop motion control
KW  - robotic endoscopic grasper
KW  - robotic joint
KW  - autonomous robotic surgery
KW  - motion hysteresis
KW  - endoscopic surgical robot
KW  - joint rotation angle sensing
KW  - motion control
KW  - tendon-sheath mechanisms
KW  - Fiber Bragg Grating
KW  - Robot sensing systems
KW  - Optical fiber sensors
KW  - Substrates
KW  - Optical fiber theory
KW  - Medical robotics
KW  - Surgery
KW  - Flexible Robots
KW  - Surgical Robotics
KW  - Fiber Optics Sensor
KW  - Tendon Sheath Mechanism.
DO  - 10.1109/ICRA40945.2020.9196549
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate motion control of surgical robots is critical for the efficiency and safety of both state-of-the-art teleoperated robotic surgery and the ultimate autonomous robotic surgery. However, fine motion control for a flexible endoscopic surgical robot is highly challenging because of the shape-dependent and speed-dependent motion hysteresis of tendon-sheath mechanisms (TSMs) in the long, tortuous, and dynamically shape-changing robot body. Aiming to achieve precise closed-loop motion control, we propose a small and flexible sensor to directly sense the large and sharp rotations of the articulated joints of a flexible endoscopic surgical robot. The sensor-a Fiber Bragg Grating (FBG) eccentrically embedded in a thin and flexible epoxy substrate-can be significantly bent with a large bending angle range of [-62.9°, 75.5°] and small bending radius of 6.9 mm. Mounted in-between the two pivot-connected links of a joint, the sensor will bend once the joint is actuated, resulting in the wavelength shift of the FBG. In this study, the relationship between the wavelength shift and the rotation angle of the joint was theoretically modeled and then experimentally verified before and after the installation of the sensor in a robotic endoscopic grasper. The sensor, with the calibrated model, can track the rotation of the robotic joint with an RMSE of 3.34°. This small and flexible sensor has good repeatability, high sensitivity (around 147.5 pm/degree), and low hysteresis (7.72%). It is suitable for surgical robots and manipulators whose articulated joints have a large rotation angle and small bending radius.
ER  - 

TY  - CONF
TI  - Soft, Round, High Resolution Tactile Fingertip Sensors for Dexterous Robotic Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4796
EP  - 4802
AU  - B. Romero
AU  - F. Veiga
AU  - E. Adelson
PY  - 2020
KW  - dexterous manipulators
KW  - geometry
KW  - tactile sensors
KW  - tactile fingertip sensors
KW  - dexterous robotic manipulation
KW  - dexterous multifingered hands
KW  - illumination geometry
KW  - dexterous manipulation tasks
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Plastics
KW  - Geometry
KW  - Light emitting diodes
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196909
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - High resolution tactile sensors are often bulky and have shape profiles that make them awkward for use in manipulation. This becomes important when using such sensors as fingertips for dexterous multi-fingered hands, where boxy or planar fingertips limit the available set of smooth manipulation strategies. High resolution optical based sensors such as GelSight have until now been constrained to relatively flat geometries due to constraints on illumination geometry. Here, we show how to construct a rounded fingertip that utilizes a form of light piping for directional illumination. Our sensors can replace the standard rounded fingertips of the Allegro hand. They can capture high resolution maps of the contact surfaces, and can be used to support various dexterous manipulation tasks.
ER  - 

TY  - CONF
TI  - FootTile: a Rugged Foot Sensor for Force and Center of Pressure Sensing in Soft Terrain
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4810
EP  - 4816
AU  - F. Ruppert
AU  - A. Badri-Spröwitz
PY  - 2020
KW  - biomechanics
KW  - force sensors
KW  - gait analysis
KW  - legged locomotion
KW  - pressure sensors
KW  - FootTile
KW  - rugged foot sensor
KW  - pressure sensing
KW  - soft terrain
KW  - sensor design
KW  - standard biomechanical devices
KW  - pressure plates
KW  - pressure distribution
KW  - ground reaction force estimation
KW  - sensing capabilities
KW  - waterproof sensor
KW  - reaction force
KW  - force plates
KW  - rough terrain
KW  - legged locomotion
KW  - granular substrate
KW  - liquid mud
KW  - mass 0.9 g
KW  - frequency 330.0 Hz
KW  - Robot sensing systems
KW  - Force
KW  - Legged locomotion
KW  - Foot
KW  - Sensor arrays
KW  - Force sensors
DO  - 10.1109/ICRA40945.2020.9197466
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present FootTile, a foot sensor for reaction force and center of pressure sensing in challenging terrain. We compare our sensor design to standard biomechanical devices, force plates and pressure plates. We show that FootTile can accurately estimate force and pressure distribution during legged locomotion. FootTile weighs 0.9 g, has a sampling rate of 330 Hz, a footprint of 10×10 mm and can easily be adapted in sensor range to the required load case. In three experiments, we validate: first, the performance of the individual sensor, second an array of FootTiles for center of pressure sensing and third the ground reaction force estimation during locomotion in granular substrate. We then go on to show the accurate sensing capabilities of the waterproof sensor in liquid mud, as a showcase for real world rough terrain use.
ER  - 

TY  - CONF
TI  - Learning a Control Policy for Fall Prevention on an Assistive Walking Device
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4833
EP  - 4840
AU  - V. C. V. Kumar
AU  - S. Ha
AU  - G. Sawicki
AU  - C. K. Liu
PY  - 2020
KW  - biomechanics
KW  - gait analysis
KW  - handicapped aids
KW  - humanoid robots
KW  - legged locomotion
KW  - assistive walking device
KW  - fall prevention
KW  - control policy
KW  - augmented assistive device
KW  - models realistic human gait
KW  - robust human walking policy
KW  - actuators
KW  - onboard sensors
KW  - recovery policy
KW  - fall predictor
KW  - Legged locomotion
KW  - Perturbation methods
KW  - Assistive devices
KW  - Sensors
KW  - Adaptation models
KW  - Biological system modeling
DO  - 10.1109/ICRA40945.2020.9196798
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fall prevention is one of the most important components in senior care. We present a technique to augment an assistive walking device with the ability to prevent falls. Given an existing walking device, our method develops a fall predictor and a recovery policy by utilizing the onboard sensors and actuators. The key component of our method is a robust human walking policy that models realistic human gait under a moderate level of perturbations. We use this human walking policy to provide training data for the fall predictor, as well as to teach the recovery policy how to best modify the person's gait when a fall is imminent. Our evaluation shows that the human walking policy generates walking sequences similar to those reported in biomechanics literature. Our experiments in simulation show that the augmented assistive device can indeed help recover balance from a variety of external perturbations. We also provide a quantitative method to evaluate the design choices for an assistive device.
ER  - 

TY  - CONF
TI  - Assistive Force of a Belt-type Hip Assist Suit for Lifting the Swing Leg during Walking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4841
EP  - 4847
AU  - S. Guo
AU  - Q. Xiang
AU  - K. Hashimoto
AU  - S. Jin
PY  - 2020
KW  - belts
KW  - gait analysis
KW  - medical control systems
KW  - muscle
KW  - torque
KW  - assistive force
KW  - phase shift factor
KW  - force function
KW  - force magnitude
KW  - assistive torque
KW  - muscle force
KW  - belt-type hip assist suit
KW  - swing leg
KW  - lifting
KW  - walking
KW  - rectus femoris
KW  - ankle joints
KW  - mid-swing phase
KW  - walk ratio
KW  - Force
KW  - Hip
KW  - Legged locomotion
KW  - Muscles
KW  - Belts
KW  - Torque
KW  - Windings
DO  - 10.1109/ICRA40945.2020.9196788
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a relatively simple function of assistive force for a belt-type hip assist suit developed by the authors' group. The function, which is inspired by the muscle force of the rectus femoris, contains only two parameters, the magnitude and a phase shift factor. Thus, it can reduce the amount of calculation in generating the desired assistive force during walking. Tests were performed on three healthy subjects to confirm its effect and to investigate its influence on the motions of hip, knee and ankle joints. It was demonstrated that the effect of the assist depended greatly on the phase shift factor, i.e., the location of the peak of the assistive force in a swing period. A large effect was observed when the peak of the assistive force came at mid-swing phase. The results of the tests showed that the proposed force function could help to increase walk ratio (the ratio of step length to the number of steps per minute) by an average value of 11.2% at a force magnitude of 35 N, which could produce an assistive torque of the same order as the magnitude of the muscle force of the rectus femoris around the hip joint.
ER  - 

TY  - CONF
TI  - Soft Pneumatic System for Interface Pressure Regulation and Automated Hands-Free Donning in Robotic Prostheses
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4848
EP  - 4854
AU  - A. B. Ambrose
AU  - F. L. Hammond
PY  - 2020
KW  - human-robot interaction
KW  - medical robotics
KW  - pneumatic systems
KW  - pressure control
KW  - prosthetics
KW  - real-time systems
KW  - wearable robots
KW  - human-socket interface
KW  - wearable device
KW  - synthetic forearm model
KW  - real time pressure regulation
KW  - automated hands free donning
KW  - automated underactuated donning mechanism
KW  - soft pneumatic socket
KW  - robotic prostheses
KW  - interface pressure regulation
KW  - Sockets
KW  - Bladder
KW  - Valves
KW  - Fingers
KW  - Prosthetics
KW  - Laser beam cutting
DO  - 10.1109/ICRA40945.2020.9197405
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper discusses the design and preliminary evaluation of a soft pneumatic socket (SPS) with real-time pressure regulation and an automated underactuated donning mechanism (UDM). The ability to modulate the pressure at the human-socket interface of a prosthesis or wearable device to accommodate user's activities has the potential to make the user more comfortable. Furthermore, a hands-free, underactuated donning mechanism designed to reliably and safely don the socket onto the user may increase the convenience of prostheses and wearable devices. The pneumatic socket and donning mechanism are evaluated on synthetic forearm model designed to closely match the mechanical properties of the human forearm. The pneumatic socket was tested to determine the maximum loads it can withstand before slipping and the displacement of the socket after loading. The donning mechanism was able to successfully don the socket on to the replica forearm with a 100% success rate for the 30 trials that were tested. Both devices were also tested to determine the pressures they impart on the user. The highest pressures the socket can impart on the user is 4psi and the maximum pressure the donning mechanism imparts on the user is 0.83psi. These pressures were found to be lower than the reported pressures that cause pain and tissue damage.
ER  - 

TY  - CONF
TI  - Automated detection of soleus concentric contraction in variable gait conditions for improved exosuit control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4855
EP  - 4862
AU  - R. W. Nuckols
AU  - K. Swaminathan
AU  - S. Lee
AU  - L. Awad
AU  - C. J. Walsh
AU  - R. D. Howe
PY  - 2020
KW  - gait analysis
KW  - image segmentation
KW  - image sequences
KW  - medical robotics
KW  - muscle
KW  - patient rehabilitation
KW  - automated routine
KW  - normalized gait cycle
KW  - real-time rates
KW  - gait cycle
KW  - manual estimation
KW  - healthy individuals
KW  - persons post-stroke walking
KW  - comfortable walking speed
KW  - onset timing
KW  - automated detection
KW  - soleus concentric contraction
KW  - variable gait conditions
KW  - individualized assistance
KW  - changing gait
KW  - exosuit control strategy
KW  - muscle power
KW  - soleus muscle
KW  - positive power
KW  - low-profile ultrasound system
KW  - walking individuals
KW  - biological mechanisms
KW  - frequency 130.0 Hz
KW  - Muscles
KW  - Legged locomotion
KW  - Kinematics
KW  - Tendons
KW  - Real-time systems
KW  - Electromyography
DO  - 10.1109/ICRA40945.2020.9197428
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exosuits can reduce metabolic demand and improve gait. Controllers explicitly derived from biological mechanisms that reflect the user's joint or muscle dynamics should in theory allow for individualized assistance and enable adaptation to changing gait. With the goal of developing an exosuit control strategy based on muscle power, we present an approach for estimating, at real time rates, when the soleus muscle begins to generate positive power. A low-profile ultrasound system recorded B-mode images of the soleus in walking individuals. An automated routine using optical flow segmented the data to a normalized gait cycle and estimated the onset of concentric contraction at real-time rates (~130Hz). Segmentation error was within 1% of the gait cycle compared to using ground reaction forces. Estimation of onset of concentric contraction had a high correlation (R2=0.92) and an RMSE of 2.6% gait cycle relative to manual estimation. We demonstrated the ability to estimate the onset of concentric contraction during fixed speed walking in healthy individuals that ranged from 39.3% to 45.8% of the gait cycle and feasibility in two persons post-stroke walking at comfortable walking speed. We also showed the ability to measure a shift in onset timing to 7% earlier when the biological system adapts from level to incline walking. Finally, we provided an initial evaluation for how the onset of concentric contraction might be used to inform exosuit control in level and incline walking.
ER  - 

TY  - CONF
TI  - Soft Sensing Shirt for Shoulder Kinematics Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4863
EP  - 4869
AU  - Y. Jin
AU  - C. M. Glover
AU  - H. Cho
AU  - O. A. Araromi
AU  - M. A. Graule
AU  - N. Li
AU  - R. J. Wood
AU  - C. J. Walsh
PY  - 2020
KW  - biomechanics
KW  - biomedical measurement
KW  - capacitive sensors
KW  - coaxial cables
KW  - inertial systems
KW  - kinematics
KW  - mean square error methods
KW  - motion measurement
KW  - patient monitoring
KW  - patient rehabilitation
KW  - readout electronics
KW  - regression analysis
KW  - strain sensors
KW  - wearable motion tracking
KW  - ground truth optical motion capture system
KW  - strain sensor data
KW  - joint angle estimation
KW  - normalized root mean square errors
KW  - joint velocity estimation
KW  - recursive feature elimination-based sensor selection analysis
KW  - shoulder kinematics estimation
KW  - soft strain sensors
KW  - unobtrusive approach
KW  - noncyclic joint movements
KW  - cyclic arm movements
KW  - random arm movements
KW  - shoulder joint
KW  - customized readout electronics board
KW  - sewn microcoaxial cables
KW  - textile-based capacitive strain sensors
KW  - multidegree-of-freedom noncyclic joint movements
KW  - Tracking
KW  - Shoulder
KW  - Electrodes
KW  - Capacitive sensors
KW  - Robot sensing systems
KW  - Strain
DO  - 10.1109/ICRA40945.2020.9196586
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft strain sensors have been explored as an unobtrusive approach for wearable motion tracking. However, accurate tracking of multi degree-of-freedom (DOF) noncyclic joint movements remains a challenge. This paper presents a soft sensing shirt for tracking shoulder kinematics of both cyclic and random arm movements in 3 DOFs: adduction/abduction, horizontal flexion/extension, and internal/external rotation. The sensing shirt consists of 8 textile-based capacitive strain sensors sewn around the shoulder joint that communicate to a customized readout electronics board through sewn micro-coaxial cables. An optimized sensor design includes passive shielding and demonstrates high linearity and low hysteresis, making it suitable for wearable motion tracking. In a study with a single human subject, we evaluated the tracking capability of the integrated shirt in comparison with a ground truth optical motion capture system. An ensemble-based regression algorithm was implemented in post-processing to estimate joint angles and angular velocities from the strain sensor data. Results demonstrated root mean square errors (RMSEs) less than 4.5° for joint angle estimation and normalized root mean square errors (NRMSEs) less than 4% for joint velocity estimation. Furthermore, we applied a recursive feature elimination (RFE)-based sensor selection analysis to down select the number of sensors for future shirt designs. This sensor selection analysis found that 5 sensors out of 8 were sufficient to generate comparable accuracies.
ER  - 

TY  - CONF
TI  - Motion Reasoning for Goal-Based Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4878
EP  - 4884
AU  - D. -A. Huang
AU  - Y. -W. Chao
AU  - C. Paxton
AU  - X. Deng
AU  - L. Fei-Fei
AU  - J. C. Niebles
AU  - A. Garg
AU  - D. Fox
PY  - 2020
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - video signal processing
KW  - goal-based imitation learning
KW  - third-person video demonstration
KW  - human demonstrators
KW  - motion reasoning
KW  - motion planning
KW  - Task analysis
KW  - Trajectory
KW  - Cognition
KW  - Planning
KW  - Motion segmentation
KW  - Human-robot interaction
DO  - 10.1109/ICRA40945.2020.9197172
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to recognize the goals that cannot be disambiguated by previous action-based approaches. We evaluate our approach on a new dataset of 96 video demonstrations in a mockup kitchen environment. We show that our motion reasoning plays an important role in recognizing the actual goal of the demonstrator and improves the success rate by over 20%. We further show that by using the automatically inferred goal from the video demonstration, our robot is able to reproduce the same task in a real kitchen environment.
ER  - 

TY  - CONF
TI  - Flexible online adaptation of learning strategy using EEG-based reinforcement signals in real-world robotic applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4885
EP  - 4891
AU  - S. K. Kim
AU  - E. Andrea Kirchner
AU  - F. Kirchner
PY  - 2020
KW  - electroencephalography
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - medical signal processing
KW  - learned policy
KW  - learning phases
KW  - current control strategy
KW  - robot learning
KW  - intrinsic human feedback
KW  - human-robot interaction
KW  - intrinsic interactive reinforcement learning approach
KW  - human-robot collaboration
KW  - flexible adaptation
KW  - real-world robotic applications
KW  - reinforcement signals
KW  - flexible online adaptation
KW  - learning progress
KW  - Electroencephalography
KW  - Decoding
KW  - Electronic learning
KW  - Task analysis
KW  - Human-robot interaction
KW  - Robot learning
DO  - 10.1109/ICRA40945.2020.9197538
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Flexible adaptation of learning strategy depending on online changes of the user's current intents have a high relevance in human-robot collaboration. In our previous study, we proposed an intrinsic interactive reinforcement learning approach for human-robot interaction, in which a robot learns his/her action strategy based on intrinsic human feedback that is generated in the human's brain as neural signature of the human's implicit evaluation of the robot's actions. Our approach has an inherent property that allows robots to adapt their behavior depending on online changes of the human's current intents. Such flexible adaptation is possible, since robot learning is updated in real time by human's online feedback. In this paper, the adaptivity of robot learning is tested on eight subjects who change their current control strategy by adding a new gesture to the previous used gestures. This paper evaluates the learning progress by analyzing learning phases (before and after adding a new gesture for control). The results show that the robot can adapt the previously learned policy depending on online changes of the user's intents. Especially, learning progress is interrelated with the classification performance of electroencephalograms (EEGs), which are used to measure the human's implicit evaluation of the robot's actions.
ER  - 

TY  - CONF
TI  - Object-oriented Semantic Graph Based Natural Question Generation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4892
EP  - 4898
AU  - J. Moon
AU  - B. -H. Lee
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - natural language processing
KW  - object detection
KW  - recurrent neural nets
KW  - robot vision
KW  - sequential scenes
KW  - recurrent neural network
KW  - feature extraction
KW  - autonomous robots
KW  - graph convolutional network
KW  - object-oriented semantic graphs
KW  - semantic graph mapping
KW  - natural question generation
KW  - Semantics
KW  - Feature extraction
KW  - Object oriented modeling
KW  - Neural networks
KW  - Convolution
KW  - Autonomous robots
DO  - 10.1109/ICRA40945.2020.9196563
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Generating a natural question can enable autonomous robots to propose problems according to their surroundings. However, recent studies on question generation rarely consider semantic graph mapping, which is widely used to understand environments. In this paper, we introduce a method to generate natural questions using object-oriented semantic graphs. First, a graph convolutional network extracts features from the graph. Then, a recurrent neural network generates the natural question from the extracted features. Using graphs, we can generate natural questions for both single and sequential scenes. The proposed method outperforms conventional methods on a publicly available dataset for single scenes and can generate questions for sequential scenes.
ER  - 

TY  - CONF
TI  - Towards Safe Human-Robot Collaboration Using Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4899
EP  - 4905
AU  - M. El-Shamouty
AU  - X. Wu
AU  - S. Yang
AU  - M. Albus
AU  - M. F. Huber
PY  - 2020
KW  - hazards
KW  - human-robot interaction
KW  - industrial robots
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - occupational safety
KW  - risk management
KW  - hazard source
KW  - safety engineers
KW  - risk assessment processes
KW  - deep RL agents
KW  - human-robot collaboration
KW  - HRC-productivity
KW  - deep reinforcement learning
KW  - systematic methodology
KW  - core components
KW  - Task analysis
KW  - Training
KW  - Hazards
KW  - Robot sensing systems
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196924
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Safety in Human-Robot Collaboration (HRC) is a bottleneck to HRC-productivity in industry. With robots being the main source of hazards, safety engineers use over-emphasized safety measures, and carry out lengthy and expensive risk assessment processes on each HRC-layout reconfiguration. Recent advances in deep Reinforcement Learning (RL) offer solutions to add intelligence and comprehensibility of the environment to robots. In this paper, we propose a framework that uses deep RL as an enabling technology to enhance intelligence and safety of the robots in HRC scenarios and, thus, reduce hazards incurred by the robots. The framework offers a systematic methodology to encode the task and safety requirements and context of applicability into RL settings. The framework also considers core components, such as behavior explainer and verifier, which aim for transferring learned behaviors from research labs to industry. In the evaluations, the proposed framework shows the capability of deep RL agents learning collision-free point-to-point motion on different robots inside simulation, as shown in the supplementary video.
ER  - 

TY  - CONF
TI  - Deep compositional robotic planners that follow natural language commands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4906
EP  - 4912
AU  - Y. -L. Kuo
AU  - B. Katz
AU  - A. Barbu
PY  - 2020
KW  - convolutional neural nets
KW  - mobile robots
KW  - natural language processing
KW  - path planning
KW  - deep compositional robotic planners
KW  - natural language commands
KW  - sampling-based robotic planner
KW  - continuous configuration space
KW  - complex command
KW  - sampling-based planner
KW  - recurrent hierarchical deep network
KW  - Robots
KW  - Task analysis
KW  - Planning
KW  - Natural languages
KW  - Aerospace electronics
KW  - Cognition
KW  - Space exploration
DO  - 10.1109/ICRA40945.2020.9197464
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments.
ER  - 

TY  - CONF
TI  - Learning User Preferences from Corrections on State Lattices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4913
EP  - 4919
AU  - N. Wilde
AU  - D. Kulić
AU  - S. L. Smith
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mobile robots
KW  - path planning
KW  - robot programming
KW  - state lattices
KW  - autonomous mobile robots
KW  - motion planning problem
KW  - robot traffic
KW  - motion features
KW  - learned user preferences
KW  - learning from corrections
KW  - algorithm completeness proving
KW  - human robot interaction
KW  - Task analysis
KW  - Lattices
KW  - Cost function
KW  - Mobile robots
KW  - Robot motion
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9197040
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Enabling a broader range of users to efficiently deploy autonomous mobile robots requires intuitive frameworks for specifying a robot's task and behaviour. We present a novel approach using learning from corrections (LfC), where a user is iteratively presented with a solution to a motion planning problem. Users might have preferences about parts of a robot's environment that are suitable for robot traffic or that should be avoided as well as preferences on the control actions a robot can take. The robot is initially unaware of these preferences; thus, we ask the user to provide a correction to the presented path. We assume that the user evaluates paths based on environment and motion features. From a sequence of corrections we learn weights for these features, which are then considered by the motion planner, resulting in future paths that better fit the user's preferences. We prove completeness of our algorithm and demonstrate its performance in simulations. Thereby, we show that the learned preferences yield good results not only for a set of training tasks but also for test tasks, as well as for different types of user behaviour.
ER  - 

TY  - CONF
TI  - Visual Servoing-based Navigation for Monitoring Row-Crop Fields
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4920
EP  - 4926
AU  - A. Ahmadi
AU  - L. Nardi
AU  - N. Chebrolu
AU  - C. Stachniss
PY  - 2020
KW  - agricultural robots
KW  - agriculture
KW  - agrochemicals
KW  - crops
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - visual servoing
KW  - visual servoing-based navigation
KW  - autonomous navigation
KW  - field robots
KW  - precision agriculture tasks
KW  - agrochemicals
KW  - visual-based navigation framework
KW  - crop-row structure
KW  - row-crop fields monitoring
KW  - Agriculture
KW  - Navigation
KW  - Cameras
KW  - Robot vision systems
KW  - Visualization
KW  - Monitoring
DO  - 10.1109/ICRA40945.2020.9197114
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous navigation is a pre-requisite for field robots to carry out precision agriculture tasks. Typically, a robot has to navigate along a crop field multiple times during a season for monitoring the plants, for applying agrochemicals, or for performing targeted interventions. In this paper, we propose a visual-based navigation framework tailored to row-crop fields that exploits the regular crop-row structure present in fields. Our approach uses only the images from on-board cameras without the need for performing explicit localization or maintaining a map of the field. Thus, it can operate without expensive RTK-GPS solutions often used in agricultural automation systems. Our navigation approach allows the robot to follow the crop rows accurately and handles the switch to the next row seamlessly within the same framework. We implemented our approach using C++ and ROS and thoroughly tested it in several simulated fields with different shapes and sizes. We also demonstrated the system running at frame-rate on an actual robot operating on a test row-crop field. The code and data have been published.
ER  - 

TY  - CONF
TI  - Optimal Routing Schedules for Robots Operating in Aisle-Structures
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4927
EP  - 4933
AU  - F. Betti Sorbelli
AU  - S. Carpin
AU  - F. Corò
AU  - A. Navarra
AU  - C. M. Pinotti
PY  - 2020
KW  - computational complexity
KW  - graph theory
KW  - mobile robots
KW  - optimisation
KW  - scheduling
KW  - COP-FR
KW  - computational complexity
KW  - optimal solutions
KW  - highly unbalanced rewards
KW  - optimal routing schedules
KW  - aisle-structures
KW  - constant-cost orienteering problem
KW  - travel budget
KW  - aisle-graph
KW  - loosely connected rows
KW  - Robots
KW  - Optimized production technology
KW  - Routing
KW  - Heuristic algorithms
KW  - Task analysis
KW  - Irrigation
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9197579
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we consider the Constant-cost Orienteering Problem (COP) where a robot, constrained by a limited travel budget, aims at selecting a path with the largest reward in an aisle-graph. The aisle-graph consists of a set of loosely connected rows where the robot can change lane only at either end, but not in the middle. Even when considering this special type of graphs, the orienteering problem is known to be intractable. We optimally solve in polynomial time two special cases, COP-FR where the robot can only traverse full rows, and COP-SC where the robot can access the rows only from one side. To solve the general COP, we then apply our special case algorithms as well as a new heuristic that suitably combines them. Despite its light computational complexity and being confined into a very limited class of paths, the optimal solutions for COP-FR turn out to be competitive in terms of achieved rewards even for COP. This is shown by means of extended simulations performed on both real and synthetic scenarios. Furthermore, our new heuristic for the general case outperforms state-of-art algorithms, especially for input with highly unbalanced rewards.
ER  - 

TY  - CONF
TI  - Time Optimal Motion Planning with ZMP Stability Constraint for Timber Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4934
EP  - 4940
AU  - J. Song
AU  - I. Sharf
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - mobile robots
KW  - motion control
KW  - optimisation
KW  - path planning
KW  - robot kinematics
KW  - stability
KW  - timber
KW  - time optimal motion planning
KW  - ZMP stability constraint
KW  - timber manipulation
KW  - dynamic stability-constrained optimal motion
KW  - timber harvesting machine
KW  - rough terrain
KW  - kinematics model
KW  - optimization problem
KW  - computation time
KW  - motion plan
KW  - dynamic stability constraint
KW  - zero moment point stability measure
KW  - Planning
KW  - Kinematics
KW  - Manipulator dynamics
KW  - Acceleration
KW  - Stability analysis
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9196836
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a dynamic stability-constrained optimal motion planning algorithm developed for a timber harvesting machine working on rough terrain. First, the kinematics model of the machine, and the Zero Moment Point (ZMP) stability measure is presented. Then, an approach to simplify the model to gain insight and achieve a fast solution of the optimization problem is introduced. The performance and computation time of the motion plan obtained with the simplified model is compared against that obtained with the full kinematics model of the machine with the help of MATLAB simulations. The results demonstrate feasibility of fast motion planning while satisfying the dynamic stability constraint.
ER  - 

TY  - CONF
TI  - Push and Drag: An Active Obstacle Separation Method for Fruit Harvesting Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4957
EP  - 4962
AU  - Y. Xiong
AU  - Y. Ge
AU  - P. J. From
PY  - 2020
KW  - agricultural robots
KW  - collision avoidance
KW  - control engineering computing
KW  - grippers
KW  - image colour analysis
KW  - image segmentation
KW  - industrial robots
KW  - mobile robots
KW  - object detection
KW  - robot vision
KW  - point cloud operation
KW  - deep learning
KW  - object detection
KW  - color thresholding
KW  - image processing
KW  - active obstacle separation
KW  - linear motions
KW  - zig-zag push
KW  - trajectory
KW  - separation motion
KW  - drag motions
KW  - obstacle avoidance
KW  - target fruit
KW  - fruit harvesting robots
KW  - Grippers
KW  - Robots
KW  - Three-dimensional displays
KW  - Drag
KW  - Trajectory
KW  - Force
KW  - Radio frequency
DO  - 10.1109/ICRA40945.2020.9197469
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Selectively picking a target fruit surrounded by obstacles is one of the major challenges for fruit harvesting robots. Different from traditional obstacle avoidance methods, this paper presents an active obstacle separation strategy that combines push and drag motions. The separation motion and trajectory are generated based on the 3D visual perception of the obstacle information around the target. A linear push is used to clear the obstacles from the area below the target, while a zig-zag push that contains several linear motions is proposed to push aside more dense obstacles. The zig-zag push can generate multi-directional pushes and the side-to-side motion can break the static contact force between the target and obstacles, thus helping the gripper to receive a target in more complex situations. Moreover, we propose a novel drag operation to address the issue of mis-capturing obstacles located above the target, in which the gripper drags the target to a place with fewer obstacles and then pushes back to move the obstacles aside for further detachment. Furthermore, an image processing pipeline consisting of color thresholding, object detection using deep learning and point cloud operation, is developed to implement the proposed method on a harvesting robot. Field tests show that the proposed method can improve the picking performance substantially. This method helps to enable complex clusters of fruits to be harvested with a higher success rate than conventional methods.
ER  - 


