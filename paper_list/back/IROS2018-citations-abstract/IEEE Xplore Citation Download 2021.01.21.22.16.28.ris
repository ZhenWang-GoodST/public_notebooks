TY  - CONF
TI  - A Neurorobotic Experiment for Crossmodal Conflict Resolution in Complex Environments *
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2330
EP  - 2335
AU  - G. I. Parisi
AU  - P. Barros
AU  - D. Fu
AU  - S. Magg
AU  - H. Wu
AU  - X. Liu
AU  - S. Wermter
PY  - 2018
KW  - audio signal processing
KW  - audio-visual systems
KW  - avatars
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - motion control
KW  - crossmodal conflict resolution
KW  - robot sensorimotor coupling
KW  - swift behaviour
KW  - robust behaviour
KW  - neurorobotic experiment
KW  - iCub robot exhibits
KW  - complex crossmodal environment
KW  - multisensory conflicts
KW  - behavioural study
KW  - audio-visual cues
KW  - visual bias
KW  - discrete behavioural response
KW  - complex environments
KW  - incongruent dynamic audio-visual cues
KW  - human-like responses
KW  - environmental statistics
KW  - stereophonic sound processing
KW  - facial features
KW  - body motion
KW  - deep learning model
KW  - animated avatars
KW  - Avatars
KW  - Visualization
KW  - Robot sensing systems
KW  - Lips
KW  - Task analysis
KW  - Spatial resolution
DO  - 10.1109/IROS.2018.8594036
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Crossmodal conflict resolution is crucial for robot sensorimotor coupling through the interaction with the environment, yielding swift and robust behaviour also in noisy conditions. In this paper, we propose a neurorobotic experiment in which an iCub robot exhibits human-like responses in a complex crossmodal environment. To better understand how humans deal with multisensory conflicts, we conducted a behavioural study exposing 33 subjects to congruent and incongruent dynamic audio-visual cues. In contrast to previous studies using simplified stimuli, we designed a scenario with four animated avatars and observed that the magnitude and extension of the visual bias are related to the semantics embedded in the scene, i.e., visual cues that are congruent with environmental statistics (moving lips and vocalization) induce the strongest bias. We implement a deep learning model that processes stereophonic sound, facial features, and body motion to trigger a discrete behavioural response. After training the model, we exposed the iCub to the same experimental conditions as the human subjects, showing that the robot can replicate similar responses in real time. Our interdisciplinary work provides important insights into how crossmodal conflict resolution can be modelled in robots and introduces future research directions for the efficient combination of sensory observations with internally generated knowledge and expectations.
ER  - 

TY  - CONF
TI  - Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2336
EP  - 2341
AU  - J. Cartucho
AU  - R. Ventura
AU  - M. Veloso
PY  - 2018
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - object detection
KW  - object recognition
KW  - service robots
KW  - robust object recognition
KW  - symbiotic deep learning
KW  - mobile service robot
KW  - human environments
KW  - symbiotic autonomy approach
KW  - HHELP
KW  - RGB camera
KW  - onboard tablet
KW  - object detection
KW  - deep neural network
KW  - domestic environment
KW  - YOLOv2 neural network
KW  - bootstrap YOLOv2
KW  - CMU
KW  - Monarch Mbot
KW  - ISR-Lisbon
KW  - Neural networks
KW  - Labeling
KW  - Training
KW  - Symbiosis
KW  - Service robots
KW  - Object recognition
DO  - 10.1109/IROS.2018.8594067
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Despite the recent success of state-of-the-art deep learning algorithms in object recognition, when these are deployed as-is on a mobile service robot, we observed that they failed to recognize many objects in real human environments. In this paper, we introduce a learning algorithm in which robots address this flaw by asking humans for help, also known as a symbiotic autonomy approach. In particular, we bootstrap YOLOv2, a state-of-the-art deep neural network and train a new neural network, that we call HHELP, using only data collected from human help. Using an RGB camera and an onboard tablet, the robot proactively seeks human input to assist it in labeling surrounding objects. Pepper, located at CMU, and Monarch Mbot, located at ISR-Lisbon, were the service robots that we used to validate the proposed approach. We conducted a study in a realistic domestic environment over the course of 20 days with 6 research participants. To improve object detection, we used the two neural networks, YOLOv2 + HHELP, in parallel. Following this methodology, the robot was able to detect twice the number of objects compared to the initial YOLOv2 neural network, and achieved a higher mAP (mean Average Precision) score. Using the learning algorithm the robot also collected data about where an object was located and to whom it belonged to by asking humans. This enabled us to explore a future use case where robots can search for a specific person's object. We view the contribution of this work to be relevant for service robots in general, in addition to Pepper, and Mbot.
ER  - 

TY  - CONF
TI  - People as Sensors: Imputing Maps from Human Actions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2342
EP  - 2348
AU  - O. Afolabi
AU  - K. Driggs–Campbell
AU  - R. Dong
AU  - M. J. Kochenderfer
AU  - S. S. Sastry
PY  - 2018
KW  - collision avoidance
KW  - driver information systems
KW  - mobile robots
KW  - pedestrians
KW  - road vehicles
KW  - human actions
KW  - autonomous vehicles
KW  - pedestrian detection
KW  - collision avoidance
KW  - map estimation
KW  - human driving experiments
KW  - landmark-based mapping approaches
KW  - agents actions
KW  - Random variables
KW  - Estimation
KW  - Automobiles
KW  - Computational modeling
KW  - Intelligent sensors
DO  - 10.1109/IROS.2018.8594511
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Despite growing attention in autonomy, there are still many open problems, including how autonomous vehicles will interact and communicate with other agents, such as human drivers and pedestrians. Unlike most approaches that focus on pedestrian detection and planning for collision avoidance, this paper considers modeling the interaction between human drivers and pedestrians and how it might influence map estimation, as a proxy for detection. We take a mapping inspired approach and incorporate people as sensors into mapping frameworks. By taking advantage of other agents' actions, we demonstrate how we can impute portions of the map that would otherwise be occluded. We evaluate our framework in human driving experiments and on real-world data, using occupancy grids and landmark-based mapping approaches. Our approach significantly improves overall environment awareness and outperforms standard mapping techniques.
ER  - 

TY  - CONF
TI  - How do humans read robotics? The matter of the lexical ambiguity resolution
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2349
EP  - 2354
AU  - C. Pieters
AU  - E. Danblon
AU  - J. P. Laumond
AU  - L. ULB
PY  - 2018
KW  - human-robot interaction
KW  - robotic actions
KW  - lexical ambiguity resolution
KW  - Robots
KW  - Linguistics
KW  - Rhetoric
KW  - Semantics
KW  - Task analysis
KW  - Psychology
DO  - 10.1109/IROS.2018.8594138
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The words used to describe robotic performances include a degree of ambiguity that the human brain should solve without difficulty. However, the language used in-and about-robotics seems to escape from the ordinary processing of lexical ambiguity resolution. In this paper, we argue that there is no lack of an adequate language for robotics but that the lexicon at hand is forced by our representations. We investigate the main representational issues of the notions that express robotic actions and dispositions (i.e. behaviors).
ER  - 

TY  - CONF
TI  - Free-View, 3D Gaze-Guided, Assistive Robotic System for Activities of Daily Living
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2355
EP  - 2361
AU  - M. Wang
AU  - A. A. Kogkas
AU  - A. Darzi
AU  - G. P. Mylonas
PY  - 2018
KW  - assisted living
KW  - end effectors
KW  - gaze tracking
KW  - medical robotics
KW  - object recognition
KW  - trajectory control
KW  - user interfaces
KW  - gaze control
KW  - assistive robotic system
KW  - daily living
KW  - free-view gaze interface
KW  - object recognition
KW  - trajectory planning
KW  - quadriplegia patient
KW  - end-effector position
KW  - Three-dimensional displays
KW  - Cameras
KW  - Task analysis
KW  - Robot kinematics
KW  - Object recognition
KW  - Planning
DO  - 10.1109/IROS.2018.8594045
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Patients suffering from quadriplegia have limited body motion which prevents them from performing daily activities. We have developed an assistive robotic system with an intuitive free-view gaze interface. The user's point of regard is estimated in 3D space while allowing free head movement and is combined with object recognition and trajectory planning. This framework allows the user to interact with objects using fixations. Two operational modes have been implemented to cater for different eventualities. The automatic mode performs a pre-defined task associated with a gaze-selected object, while the manual mode allows gaze control of the robot's end-effector position on the user's frame of reference. User studies reported effortless operation in automatic mode. A manual pick and place task achieved a success rate of 100% on the users' first attempt.
ER  - 

TY  - CONF
TI  - The Future of Legal and Ethical Regulations for Autonomous Robotics
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2362
EP  - 2366
AU  - H. Xu
AU  - J. E. Borson
PY  - 2018
KW  - consumer electronics
KW  - mobile robots
KW  - autonomous robotics
KW  - autonomous systems
KW  - novel autonomy framework
KW  - device safety
KW  - regulatory frameworks
KW  - specific framework those devices
KW  - future autonomy regulations
KW  - consumer electronics vis-á-vis medical devices
KW  - regulatory landscape
KW  - autonomous elements
KW  - future regulation
KW  - Autonomous systems
KW  - Robots
KW  - Law
KW  - Standards
KW  - Ethics
KW  - Complex systems
DO  - 10.1109/IROS.2018.8593915
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - “Autonomous robotics” promise significant improvements across a host of different complex systems, which will need to be managed within regulatory frameworks to promote, at a minimum, device safety. Contrary to how they are often portrayed, however, these systems do not necessarily require fundamentally new approaches to engineering or regulatory challenges, i.e., the development of a novel “autonomy framework” applicable to different types of devices. Rather, because autonomous systems generally represent a progressive improvement of existing complex systems, preexisting regulatory scheme offer the best guidance for considering future regulation of autonomous elements. Moreover, the regulatory landscape differs considerably based on the type of device at issue (e.g., consumer electronics vis-á-vis medical devices). This paper argues that users and regulators must consider future autonomy regulations within the specific framework those devices currently inhabit, rather than focusing on a novel set of rules divorced from the preexisting context.
ER  - 

TY  - CONF
TI  - Uncertainty-based Online Mapping and Motion Planning for Marine Robotics Guidance
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2367
EP  - 2374
AU  - È. Pairet
AU  - J. D. Hernández
AU  - M. Lahijanian
AU  - M. Carreras
PY  - 2018
KW  - autonomous underwater vehicles
KW  - path planning
KW  - probability
KW  - robot dynamics
KW  - vehicle dynamics
KW  - uncertainty-based framework
KW  - online computation constraints
KW  - motion planning
KW  - marine robotics guidance
KW  - robotic systems
KW  - safe path
KW  - underwater environments
KW  - autonomous vehicles
KW  - probabilistic safety
KW  - online mapping
KW  - Uncertainty
KW  - Safety
KW  - Planning
KW  - Probabilistic logic
KW  - Robot sensing systems
KW  - Vehicle dynamics
DO  - 10.1109/IROS.2018.8593394
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In real-world robotics, motion planning remains to be an open challenge. Not only robotic systems are required to move through unexplored environments, but also their manoeuvrability is constrained by their dynamics and often suffer from uncertainty. One approach to overcome this problem is to incrementally map the surroundings while, simultaneously, planning a safe and feasible path to a desired goal. This is especially critical in underwater environments, where autonomous vehicles must deal with both motion and environment uncertainties. In order to cope with these constraints, this work proposes an uncertainty-based framework for mapping and planning3 feasible motions online with probabilistic safety-guarantees. The proposed approach deals with the motion, probabilistic safety, and online computation constraints by (i) incrementally representing the environment as a collection of local maps, and (ii) iteratively (re)planning kinodynamically-feasible and probabilistically-safe paths to goal. The proposed framework is evaluated on the Sparus II, a nonholonomic torpedo-shaped AUV, by conducting simulated and real-world trials, thus proving the efficacy of the method and its suitability even for systems with limited on-board computational power.
ER  - 

TY  - CONF
TI  - Heterogeneous Vehicles Routing for Water Canal Damage Assessment
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2375
EP  - 2382
AU  - D. Deng
AU  - P. Palli
AU  - F. Shu
AU  - K. Shimada
AU  - T. Pang
PY  - 2018
KW  - autonomous aerial vehicles
KW  - canals
KW  - graph theory
KW  - inspection
KW  - integer programming
KW  - irrigation
KW  - path planning
KW  - quadratic programming
KW  - vehicle routing
KW  - water canal damage assessment
KW  - irrigation water canals
KW  - manual inspection
KW  - shortened inspection time
KW  - reduced labor cost
KW  - automated inspection
KW  - road networks
KW  - path planning
KW  - UAV
KW  - unmanned aerial vehicles
KW  - ground vehicles
KW  - integer quadratic program
KW  - IQP
KW  - heterogeneous vehicle routing
KW  - Irrigation
KW  - Automobiles
KW  - Inspection
KW  - Roads
KW  - Planning
KW  - Batteries
KW  - Routing
DO  - 10.1109/IROS.2018.8593365
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In Japan, inspection of irrigation water canals has been mostly conducted manually. However, the huge demand for more regular inspections as infrastructure ages, coupled with the limited time window available for inspection, has rendered manual inspection increasingly insufficient. With shortened inspection time and reduced labor cost, automated inspection using a combination of unmanned aerial vehicles (UAVs) and ground vehicles (cars) has emerged as an attractive alternative to manual inspection. In this paper, we propose a path planning framework that generates optimal plans for UAVs and cars to inspect water canals in a large agricultural area (tens of square kilometers). In addition to optimality, the paths need to satisfy several constraints, in order to guarantee UAV navigation safety and to abide by local traffic regulations. In the proposed framework, the canal and road networks are first modeled as two graphs, which are then partitioned into smaller subgraphs that can be covered by a given fleet of UAVs within one battery charge. The problem of finding optimal paths for both UAVs and cars on the graphs, subject to the constraints, is formulated as a integer quadratic program (IQP). The proposed framework can also quickly generate new plans when a current plan is interrupted. The effectiveness of the proposed framework is validated by simulation results showing the successful generation of plans covering all given canal segments, and the ability to quickly revise the plan when conditions change.
ER  - 

TY  - CONF
TI  - Passive acoustic tracking for behavior mode classification between surface and underwater vehicles
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2383
EP  - 2388
AU  - E. M. Fischell
AU  - O. Viquez
AU  - H. Schmidt
PY  - 2018
KW  - autonomous underwater vehicles
KW  - hydrophones
KW  - mobile robots
KW  - acoustic modems
KW  - vehicle state
KW  - communication line
KW  - submerged vehicles
KW  - hydrophone arrays
KW  - AUV mode estimates
KW  - dynamic time
KW  - simulation data
KW  - simulation-based classifier
KW  - bearing tracking data
KW  - passive tracking
KW  - TTI data
KW  - field array data
KW  - experiment data
KW  - surface vessels
KW  - AUV behavior
KW  - passive acoustic tracking
KW  - behavior mode classification
KW  - autonomous underwater vehicles
KW  - speed-of-light communication
KW  - AUV platforms
KW  - surface vehicle behavior
KW  - K-nearest-neighbor
KW  - Boats
KW  - Acoustics
KW  - Sea surface
KW  - Trajectory
KW  - Arrays
KW  - Data models
KW  - Sonar equipment
DO  - 10.1109/IROS.2018.8593981
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Autonomous underwater vehicles (AUVs) pose significant communication challenges: vehicles are submerged for periods of time in which speed-of-light communication is impossible. This is a particular problem on low-cost AUV platforms, on which acoustic modems are not available to get vehicle state or provide re-deploy commands. We investigate one possible method of providing operators with a communication line to these vehicles by using noise underwater to both classify behavior of submerged vehicles and to command them. In this scheme, processing of data from hydrophone arrays provide operators with AUV mode estimates and AUVs with surface vehicle behavior updates. Simulation studies were used to characterize trajectories for simple transect versus loiter behaviors based on the bearing and time to intercept (TTI). A classifier based on K-nearest-neighbor with dynamic time warping as a distance metric was used to classify simulation data. The simulation-based classifier was then applied to classify bearing tracking data from passive tracking of a loitering AUV and bearing and TTI data from passive tracking of a transecting boat based on field array data. Experiment data was classified with 76 % accuracy using bearing-only data, 96% accuracy for TTI -only data and 99 % accuracy for combined classification. The techniques developed here could be used for AUV cuing by surface vessels and monitoring of AUV behavior.
ER  - 

TY  - CONF
TI  - A Rationale-Driven Team Plan Representation for Autonomous Intra-Robot Replanning*
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2389
EP  - 2394
AU  - P. Cooksey
AU  - M. Veloso
PY  - 2018
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - rationale-driven team plan representation
KW  - autonomous multirobot teams
KW  - autonomous intrarobot replanning
KW  - team planner
KW  - intrarobot replanning algorithm
KW  - Robots
KW  - Oceans
KW  - Task analysis
KW  - Prediction algorithms
KW  - Planning
KW  - Satellites
KW  - Global Positioning System
DO  - 10.1109/IROS.2018.8593765
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For autonomous multi-robot teams, the individual team members are tasked with completing their assigned tasks as defined by a team plan provided by a centralized team planner. However in complex dynamic domains, the team plans are generated by the team planner with assumptions due to the complexity of modeling the domain. Failures in execution are therefore inevitable for the team members, and as such, replanning will occur for the team. In this paper, we introduce a rationale-driven team plan representation that provides rationales on why actions were chosen by the team planner. During a failure, the individual team members autonomously use our described intra-robot replanning algorithm to select all applicable replan policies for a given rationale. We then describe a method to learn the predicted cost of each replan policy, given a state of the environment, in order for the individual robots to select the lowest costing replan policy to improve team performance.
ER  - 

TY  - CONF
TI  - Stochastic Optimization for Autonomous Vehicles with Limited Control Authority
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2395
EP  - 2401
AU  - D. Jones
AU  - G. A. Hollinger
AU  - M. J. Kuhlman
AU  - D. A. Sofge
AU  - S. K. Gupta
PY  - 2018
KW  - gradient methods
KW  - greedy algorithms
KW  - mobile robots
KW  - optimisation
KW  - state-space methods
KW  - stochastic processes
KW  - SGA
KW  - multivehicle information gathering
KW  - action space representation
KW  - stochastic optimization scheme
KW  - perturbed action sequences
KW  - state space information function
KW  - sequential greedy allocation
KW  - autonomous vehicles
KW  - stochastic gradient ascent algorithm
KW  - vehicle control authority
KW  - navy coastal ocean model
KW  - NCOM
KW  - Gulf of Mexico
KW  - GoM
KW  - Monte Carlo tree search method
KW  - MCTS
KW  - Oceans
KW  - Optimization
KW  - Stochastic processes
KW  - Approximation algorithms
KW  - Trajectory
KW  - Aerospace electronics
KW  - Robots
DO  - 10.1109/IROS.2018.8594020
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this work, we present a Stochastic Gradient Ascent (SGA) algorithm for multi-vehicle information gathering that accounts for limitations on a vehicle's control authority caused by external forces. By representing vehicle paths using a novel action space representation, rather than a state space representation, we remove the need to perform feasibility calculations on the vehicle's path. Our algorithm uses a stochastic optimization scheme by sampling perturbed action sequences around the current best known sequence to estimate the gradient of a state space information function with respect to the action sequence. Additionally, we use sequential greedy allocation to plan for multiple vehicles. Results are shown using a Navy Coastal Ocean Model (NCOM) for the Gulf of Mexico (GoM). SGA shows improvement in the amount of information gained over a greedy baseline. Additionally, we compare to Monte Carlo Tree Search (MCTS) Method, which is able to gather competitive amounts of information but is more computationally intensive than our approach.
ER  - 

TY  - CONF
TI  - Proactive Collision Avoidance for ASVs using A Dynamic Reciprocal Velocity Obstacles Method
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2402
EP  - 2409
AU  - D. K. M. Kufoalor
AU  - E. F. Brekke
AU  - T. A. Johansen
PY  - 2018
KW  - collision avoidance
KW  - marine vehicles
KW  - mobile robots
KW  - velocity control
KW  - reciprocal velocity obstacles framework
KW  - dynamic obstacles
KW  - collision avoidance decisions
KW  - international regulations
KW  - autonomous surface vessel
KW  - future behavior
KW  - interactive behavior
KW  - collision avoidance method
KW  - dynamic reciprocal velocity obstacles method
KW  - proactive collision avoidance
KW  - COLREGs
KW  - ASV behavior
KW  - complex dynamic models
KW  - RVO framework
KW  - predictive approach
KW  - Collision avoidance
KW  - Decision making
KW  - Propulsion
KW  - Uncertainty
KW  - Sensor systems
KW  - Computational modeling
DO  - 10.1109/IROS.2018.8594382
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose a collision avoidance method that incorporates the interactive behavior of agents and is proactive in dealing with the uncertainty of the future behavior of obstacles. The proposed method considers interactions that will be experienced by an autonomous surface vessel (ASV) in an environment governed by the international regulations for preventing collisions at sea (COLREGs). Our approach aims at encouraging dynamic obstacles to cooperate according to COLREGs. Therefore, we propose a strategy for assessing the cooperative behavior of obstacles, and the result of the assessment is used to adapt collision avoidance decisions within the Reciprocal Velocity Obstacles (RVO) framework. Moreover, we propose a predictive approach to solving known limitations of the RVO framework, and we present computationally feasible extensions that enable the use of complex dynamic models and objectives suitable for ASVs. We demonstrate the performance and potentials of our method through a simulation study, and the results show that the proposed method leads to proactive and more predictable ASV behavior compared with both Velocity Obstacles (VO) and RVO, especially when obstacles cooperate by following COLREGs.
ER  - 

TY  - CONF
TI  - A Multi-Task Priority Framework for Redundant Robots with Multiple Kinematic Chains under Hard Joint and Cartesian Constraints
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2410
EP  - 2417
AU  - A. Peñalver
AU  - J. J. Fernández
AU  - A. Soriano
AU  - P. J. Sanz
PY  - 2018
KW  - redundant manipulators
KW  - multiple kinematic chains
KW  - hard joint
KW  - reverse priority framework
KW  - kinematic control
KW  - redundant robots
KW  - reverse priority method
KW  - robotic systems
KW  - bilateral constraints
KW  - unilateral constraints
KW  - multitask priority framework
KW  - joint priorities
KW  - Cartesian constraints
KW  - Task analysis
KW  - Kinematics
KW  - Jacobian matrices
KW  - Redundancy
KW  - End effectors
DO  - 10.1109/IROS.2018.8593967
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper introduces an extension of the reverse priority framework for the kinematic control of redundant robots. It integrates, in a unified framework, the treatment of multiple tasks, multiple kinematic chains, different joint priorities and hard constraints. The management of multiple tasks is based on the reverse priority method, that has been modified so that it makes possible the assignment of different priorities to each joint in order to accomplish the tasks. This framework is also suitable for robotic systems with multiple kinematic chains, which could share several joints. Moreover, it can deal with bilateral and unilateral constraints, that can be defined either at joint or cartesian space. Hard constraints are considered at each priority level, instead of treating them separately at the highest priority level. The proposed framework has been evaluated in simulation and in real experiments with a redundant underwater vehicle-manipulator system at sea.
ER  - 

TY  - CONF
TI  - Vision-based Target Tracking for a Skid-steer Vehicle using Guided Policy Search with Field-of-view Constraint
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2418
EP  - 2425
AU  - T. Kim
AU  - C. Lee
AU  - H. Seo
AU  - S. Choi
AU  - W. Kim
AU  - H. J. Kim
PY  - 2018
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - remotely operated vehicles
KW  - robot dynamics
KW  - robot kinematics
KW  - robot vision
KW  - search problems
KW  - steering systems
KW  - target tracking
KW  - skid-steer vehicle
KW  - guided policy search
KW  - skid-type robot
KW  - local policy optimization
KW  - FOV constraint
KW  - vision-based tracking policy
KW  - skid-steer mobile robot
KW  - field-of-view constraint
KW  - vision-based target tracking method
KW  - end-to-end policy
KW  - pixel image data
KW  - deep reinforcement learning
KW  - kinematic slip model
KW  - Mobile robots
KW  - Training
KW  - Kinematics
KW  - Cameras
KW  - Target tracking
KW  - Wheels
DO  - 10.1109/IROS.2018.8593843
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper describes a vision-based target tracking method for a skid-steer vehicle. With the development of deep reinforcement learning, many researchers have tried to generate an end-to-end policy to control the mobile robot from a raw pixel image data. However, the action in most research only concerns high-level decisions such as go straight, turn left and right. High-level decisions alone are not sufficient to precisely control platforms such as a skid-steer vehicle due to the lack of steering mechanism. Thus, unlike existing work, we aim to control the motor command for the wheels directly. To this end, we employ guided policy search (GPS) based on the general kinematic slip model for the skid-type robot. Furthermore, to prohibit the target from getting out of the camera field of view (FOV) in the training phase, we update local policy optimization with a FOV constraint and perform a pre-training to make the initial policy more efficient. Our method allows the skid-type robot to automatically acquire the vision-based tracking policy while local policies satisfy the FOV constraint during the training phase. We evaluate our method through both simulation and experiment with a skid-steer mobile robot. Finally, we test the performance of learned policy with a moving target in a new environment.
ER  - 

TY  - CONF
TI  - On the Kinematics of Wheeled Motion Control of a Hybrid Wheeled-Legged CENTAURO robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2426
EP  - 2433
AU  - M. Kamedula
AU  - N. Kashiri
AU  - N. G. Tsagarakis
PY  - 2018
KW  - legged locomotion
KW  - motion control
KW  - robot kinematics
KW  - stability
KW  - wheels
KW  - wheeled-legged CENTAURO robot
KW  - real-world terrains
KW  - mobile platforms
KW  - versatile mobility
KW  - first-order inverse kinematics scheme
KW  - wheeled mobility
KW  - legged-wheeled motion kinematics control
KW  - wheels camber angles
KW  - legged-wheeled system stability
KW  - floating base model
KW  - legged-wheeled centaur-like robot
KW  - Legged locomotion
KW  - Wheels
KW  - Kinematics
KW  - Robot kinematics
KW  - Solid modeling
DO  - 10.1109/IROS.2018.8594222
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Legged-wheeled robots combine the advantages of efficient wheeled mobility with the adaptability to real-world terrains through the legged locomotion. Due to this hybrid mobility skill, they can excel in many application scenarios where other mobile platforms are not suitable for. However, their versatile mobility increases the number of constraints in their motion control where both the properties of legged and wheeled systems need to be considered. Relevant schemes for legged-wheeled platforms so far have been developed exploiting separate motion control of the wheeled and legged functionalities. This paper discusses the legged-wheeled motion kinematics without constraining the camber angles of the wheels, and it proposes a first-order inverse kinematics scheme that stabilizes the legged-wheeled system in the wheeled motion. Furthermore, the work adopts a floating base model that allows to easily incorporate the legged motion to the scheme. The developed controller is tested in simulation and experiments on a legged-wheeled centaur-like robot - CENTAURO.
ER  - 

TY  - CONF
TI  - Development of Stone Throwing Robot and High Precision Driving Control for Curling
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2434
EP  - 2440
AU  - J. H. Choi
AU  - C. Song
AU  - K. Kim
AU  - S. Oh
PY  - 2018
KW  - artificial intelligence
KW  - control engineering computing
KW  - control system synthesis
KW  - feedforward
KW  - mobile robots
KW  - motion control
KW  - observers
KW  - position control
KW  - three-term control
KW  - velocity control
KW  - wheels
KW  - AI system
KW  - stone throwing robot
KW  - curling sports
KW  - throwing-curling
KW  - artificial intelligence system
KW  - precise driving control
KW  - STR driving
KW  - robust heading angle control
KW  - model-based feedforward control
KW  - conventional PID controller
KW  - anti-slip control
KW  - dimensional drive control
KW  - curling sport
KW  - robot component
KW  - developed robot
KW  - novel mobile robot
KW  - high precision driving Control
KW  - Mobile robots
KW  - Wheels
KW  - Cameras
KW  - Artificial intelligence
KW  - Ice
KW  - Servers
DO  - 10.1109/IROS.2018.8594026
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, a novel mobile robot developed to perform Curling sports is introduced. The developed robot is a Stone Throwing Robot (STR) for Curling that can travel on the ice with wheels and throw a stone as well as make curls of the stone. The STR is developed as a robot component of an Artificial Intelligence(AI) system that can autonomously play the curling sport. The proposed STR can throw a stone at any desired speed and in any desired direction, which are determined by the AI system. To achieve this precise driving of the STR and throwing of the stone, two dimensional drive control is developed for the STR, which consists of 1) anti-slip control for high traction, 2) precise velocity control and 3) high accuracy heading angle control. In addition to the conventional PID controller, model-based feedforward control, Model Following Control (MFC) for the anti-slip control of the wheel on the ice and Yaw Moment Observer (YMO) for the robust heading angle control are applied as key technologies for the STR driving. The design configurations of the STR to achieve the detection of its own location and throwing/curling of the stone is proposed in this paper as well as the detail of the precise driving control.
ER  - 

TY  - CONF
TI  - MAP - A Mobile Agile Printer Robot for on-site Construction
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2441
EP  - 2448
AU  - J. Sustarevas
AU  - D. Butters
AU  - M. Hammid
AU  - G. Dwyer
AU  - R. Stuart-Smith
AU  - V. M. Pawar
PY  - 2018
KW  - construction
KW  - legged locomotion
KW  - mobile robots
KW  - robot kinematics
KW  - service robots
KW  - three-dimensional printing
KW  - wheels
KW  - outdoors construction site
KW  - 3D printing large structures
KW  - omnidirectional robot capable
KW  - Mobile Agile Printer construction robot
KW  - on-site construction
KW  - concurrent on-site operations
KW  - low 3D printing trajectory deviations
KW  - construction robots
KW  - mobile platform
KW  - high-DoF 3D printing system
KW  - MAP
KW  - Three-dimensional printing
KW  - Wheels
KW  - Legged locomotion
KW  - Printers
DO  - 10.1109/IROS.2018.8593815
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we present a Mobile Agile Printer (MAP) construction robot; a highly agile, 4-legged, omnidirectional robot capable of 3D printing large structures. To overcome dynamic challenges when operating within an outdoors construction site, MAP incorporates a high-DoF 3D printing system connected to a mobile platform with novel features designed to enable disturbance rejection and live adaption to the robot's pose. In doing so, we demonstrate the benefits of designing construction robots with a focus on agility, a compact working volume and ability to operate within a potentially unlimited workspace. Performance tests were conducted showing smooth omni-directional motion as a key requirement for maintaining low 3D printing trajectory deviations over a large volume. In doing so, we show that MAP has the ability to construct in new ways more sensitive to its environment, context and concurrent on-site operations.
ER  - 

TY  - CONF
TI  - Slip Modeling and Estimation for a Planetary Exploration Rover: Experimental Results from Mt. Etna
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2449
EP  - 2456
AU  - K. Bussmann
AU  - L. Meyer
AU  - F. Steidle
AU  - A. Wedler
PY  - 2018
KW  - aerospace robotics
KW  - mobile robots
KW  - planetary rovers
KW  - position control
KW  - wheels
KW  - wheel-soil interaction properties
KW  - inherent errors
KW  - wheel slippage
KW  - parameter-based approach
KW  - whole-body slip modeling
KW  - lightweight rover system
KW  - slip parameter calibration
KW  - system-specific implementation
KW  - experimental results
KW  - Mt. Etna
KW  - resulting wheel odometry measurements
KW  - space exploration scenario
KW  - planetary exploration rover
KW  - wheeled mobile systems
KW  - planetary rovers
KW  - planetary exploration missions
KW  - Wheels
KW  - Jacobian matrices
KW  - Trajectory
KW  - Current measurement
KW  - Extraterrestrial measurements
KW  - Soil
KW  - Mathematical model
DO  - 10.1109/IROS.2018.8594294
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For wheeled mobile systems, the wheel odometry is an important source of information about the current motion of the vehicle. It is used e.g. in the context of pose estimation and self-localization of planetary rovers, which is a crucial part of the success of planetary exploration missions. Depending on the wheel-soil interaction properties, wheel odometry measurements are subject to inherent errors such as wheel slippage. In this paper, a parameter-based approach for whole-body slip modeling and calibration is applied to a four-wheeled lightweight rover system. Details on the method for slip parameter calibration as well as the system-specific implementation are given. Experimental results from a test campaign on Mt. Etna are presented, showing significant improvements of the resulting wheel odometry measurements. The results are validated during a long range drive of approx. 900 m and discussed w. r. t. the advantages but also limitations of the method within a space exploration scenario.
ER  - 

TY  - CONF
TI  - User-specific Gaussian Process Model of Wheelchair Drivers with a Haptic Joystick Interface
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2457
EP  - 2463
AU  - A. Hünternann
AU  - E. Demeester
AU  - E. V. Poorten
PY  - 2018
KW  - Gaussian processes
KW  - handicapped aids
KW  - haptic interfaces
KW  - human-robot interaction
KW  - interactive devices
KW  - man-machine systems
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - user modelling
KW  - wheelchairs
KW  - Gaussian process
KW  - spastic wheelchair user
KW  - navigation assistance frequency
KW  - achievable user model evaluation frequency
KW  - haptic joysticks
KW  - probabilistic user-specific driver model
KW  - mental navigation plan
KW  - particular user
KW  - personalised driver model
KW  - navigation plans
KW  - probabilistic framework
KW  - navigation task
KW  - inherent uncertainty
KW  - heterogeneous driving styles
KW  - mobile robot
KW  - intuitive control
KW  - driving semiautonomous
KW  - collaborative human-robot navigation
KW  - haptic joystick interface
KW  - wheelchair drivers
KW  - user-specific Gaussian process model
KW  - Wheelchairs
KW  - Navigation
KW  - Mobile robots
KW  - Probabilistic logic
KW  - Gaussian processes
KW  - Hidden Markov models
DO  - 10.1109/IROS.2018.8593931
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In collaborative human-robot navigation such as when driving semi-autonomous robotic wheelchairs, intuitive control of the mobile robot is only possible if the robot understands its user. This becomes especially important as users present varying levels of abilities and heterogeneous driving styles. Furthermore, the robot needs to consider the inherent uncertainty on its navigation task because the user may not be able to communicate his or her plans explicitly. In order to address these requirements, we have adopted a probabilistic framework to recognise navigation plans. A key component in this framework is a personalised driver model, which captures how a particular user transforms his or her mental navigation plan into inputs to the robot. In this work, we evaluate the use of Gaussian Processes to implement and calibrate this probabilistic, user-specific driver model, and this for use with haptic joysticks. Furthermore, special care was taken to obtain fast online evaluation of this user model through sparse approximation and parallel computation on a GPU. This resulted in an achievable user model evaluation frequency of 40 Hz, which is far above the navigation assistance frequency we aimed for, i.e. 5 Hz. We illustrate the validity of the approach by recognising the navigation plans of a spastic wheelchair user.
ER  - 

TY  - CONF
TI  - A minimalist Stair Climbing Robot (SCR) formed as a leg balancing & climbing Mobile Inverted Pendulum (MIP)
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2464
EP  - 2469
AU  - D. Yang
AU  - T. Bewley
PY  - 2018
KW  - feedback
KW  - legged locomotion
KW  - motion control
KW  - pendulums
KW  - robot kinematics
KW  - service robots
KW  - stability
KW  - wheels
KW  - SCR
KW  - leg balancing
KW  - patent-pending
KW  - minimal-complexity Stair Climbing Robot
KW  - vehicle design
KW  - stairs
KW  - leveraging feedback control
KW  - foot
KW  - MIP drive wheels
KW  - reaction wheels
KW  - stair-climbing throwbot
KW  - mobile inverted pendulum
KW  - left-right stability
KW  - fore-aft stabilization
KW  - chassis-wheel assembly
KW  - minimalist stair climbing robot
KW  - Wheels
KW  - Legged locomotion
KW  - Robot kinematics
KW  - Gears
KW  - Torque
DO  - 10.1109/IROS.2018.8593988
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a (patent-pending) small, quasi-static, minimal-complexity Stair Climbing Robot (SCR). The vehicle design is given simply by adding a third motor to a (Segway-like) Mobile Inverted Pendulum (MIP), enabling it to maneuver up stairs, leveraging feedback control, by planting it's “foot” onto the ground in front of the next step, lifting the chassis/wheel assembly up it's own “leg”, leaning over onto the top of the next step, self uprighting, and repeating for the following step(s). Fore/aft stabilization during leg balancing is given by using the MIP drive wheels as reaction wheels, while left/right stability is given by the width of the foot itself. The design is small and simple enough to potentially be ruggedized as a stair-climbing throwbot, akin to the Recon Scout (but able to climb up stairs) for reconnaissance in military and homeland security applications.
ER  - 

TY  - CONF
TI  - Tire Force Estimation of Dynamic Wheeled Mobile Robots using Tire-Model Based Constrained Kalman Filtering
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2470
EP  - 2477
AU  - S. Jeon
AU  - R. Chung
AU  - D. Lee
PY  - 2018
KW  - automobiles
KW  - feedback
KW  - friction
KW  - gears
KW  - Kalman filters
KW  - mobile robots
KW  - robot dynamics
KW  - tyres
KW  - wheels
KW  - wheel encoders
KW  - tire-road interaction
KW  - estimation algorithm
KW  - three-dimensional individual tire forces
KW  - dynamic wheeled mobile robots
KW  - tire-model based constrained Kalman filtering
KW  - tire force real-time estimation
KW  - tire force estimation techniques
KW  - car-like rearwheel-driven four wheel wheeled mobile robots
KW  - onboard navigation sensors
KW  - feedback
KW  - tire-road friction coefficient
KW  - torque inputs
KW  - differential gear
KW  - CarSim
KW  - Tires
KW  - Wheels
KW  - Force
KW  - Estimation
KW  - Dynamics
KW  - Sensors
KW  - Friction
DO  - 10.1109/IROS.2018.8593708
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose a novel real-time algorithm to estimate the full three-dimensional individual tire forces (i.e., vertical, longitudinal as well as lateral) of a car-like rearwheel-driven four wheel wheeled mobile robots equipped with onboard navigation sensors and wheel encoders. The key enabling idea for this is to utilize the tire model (i.e., the magic formula) in a feedback manner on the framework of the constrained Kalman filtering to render the tire force estimation: 1) more accurate as compared to the typical tire force estimation techniques neglecting the tire-road interaction; and 2) more robust as compared to the results adopting the tire model, yet, only in an open-loop manner. Our proposed algorithm, while performing this full tire force onboard/real-time estimation, also provides the estimation of: 1) tire-road friction coefficient; and 2) torque inputs of the rear left and right wheels, which are connected via differential gear. Simulations with CarSim and outdoor experiments are performed to validate the proposed estimation algorithm.
ER  - 

TY  - CONF
TI  - Online Spatial Sound Perception Using Microphone Array on Mobile Robot*
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2478
EP  - 2484
AU  - Y. Sasaki
AU  - R. Tanabe
AU  - H. Takernura
PY  - 2018
KW  - acoustic generators
KW  - acoustic signal detection
KW  - acoustic signal processing
KW  - convolutional neural nets
KW  - microphone arrays
KW  - mobile robots
KW  - probability
KW  - probabilistic regions
KW  - sound sources
KW  - microphone array
KW  - autonomous mobile robot
KW  - three-dimensional position localization
KW  - sound signals detection
KW  - online spatial sound perception system
KW  - convolutional neural network
KW  - CNN
KW  - three-dimensional position recognition
KW  - sound positions estimation
KW  - Mobile robots
KW  - Robot sensing systems
KW  - Microphone arrays
KW  - Estimation
KW  - Probabilistic logic
DO  - 10.1109/IROS.2018.8593777
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The paper proposes a spatial sound perception system for an autonomous mobile robot. The system performs three-dimensional position localization and recognition as online processing from a robot in motion. For online processing, the sound positions are estimated as probabilistic regions in three dimensional space, because the robot could observe only arrival direction of the sound at a moment. The detected sound signals are recognized using Convolutional Neural Network (CNN), for the adjustment to short input signals. The experimental results show our mobile robot could observe surrounding sound sources online and continuously update its position and sound label.
ER  - 

TY  - CONF
TI  - Extracting the Relationship between the Spatial Distribution and Types of Bird Vocalizations Using Robot Audition System HARK
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2485
EP  - 2490
AU  - S. Sumitani
AU  - R. Suzuki
AU  - S. Matsubayashi
AU  - T. Arita
AU  - K. Nakadai
AU  - H. G. Okuno
PY  - 2018
KW  - acoustic signal processing
KW  - biocommunications
KW  - microphone arrays
KW  - robots
KW  - zoology
KW  - HARK robot audition system
KW  - song-behavior relationships
KW  - HARKBird
KW  - wild birds
KW  - 2D localize vocalizations
KW  - vocalizations characteristics
KW  - microphone arrays
KW  - portable observation system
KW  - wild bird vocalizations
KW  - ecological functions
KW  - spatial distribution
KW  - Birds
KW  - Microphone arrays
KW  - Two dimensional displays
KW  - Robots
KW  - Real-time systems
KW  - Arrays
DO  - 10.1109/IROS.2018.8594130
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For a deeper understanding of ecological functions and semantics of wild bird vocalizations (i.e., songs and calls), it is important to clarify the fine-scaled and detailed relationships among their characteristics of vocalizations and their behavioral contexts. However, it takes a lot of time and effort to obtain such data using conventional recordings or by human observation. Bringing out a robot to a field is our approach to solve this problem. We are developing a portable observation system called HARKBird using a robot audition HARK and microphone arrays to understand temporal patterns of vocalizations characteristics and their behavioral contexts. In this paper, we introduce a prototype system to 2D localize vocalizations of wild birds in real-time, and to classify their song types after recording. We show that the system can estimate the position of songs of a target individual and classify their songs with a reasonable quality to discuss their song - behavior relationships.
ER  - 

TY  - CONF
TI  - Failure Detection Using Proprioceptive, Auditory and Visual Modalities
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2491
EP  - 2496
AU  - A. Inceoglu
AU  - G. Ince
AU  - Y. Yaslan
AU  - S. Sariel
PY  - 2018
KW  - computerised monitoring
KW  - humanoid robots
KW  - manipulators
KW  - robot vision
KW  - sensor fusion
KW  - failure detection
KW  - continuous execution monitoring
KW  - multimodal failure monitoring
KW  - single sensor modality
KW  - high level proprioceptive
KW  - auditory predicates
KW  - visual predicates
KW  - humanoid robot
KW  - tabletop manipulation scenarios
KW  - safety handling
KW  - multimodal fusion
KW  - Robot sensing systems
KW  - Hidden Markov models
KW  - Monitoring
KW  - Visualization
KW  - Task analysis
KW  - Grasping
DO  - 10.1109/IROS.2018.8594169
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Handling safety is crucial to achieve lifelong autonomy for robots. Unsafe situations might arise during manipulation in unstructured environments due to noises in sensory feedback, improper action parameters, hardware limitations or external factors. In order to assure safety, continuous execution monitoring and failure detection procedures are mandatory. To this end, we present a multimodal failure monitoring and detection system to detect manipulation failures. Rather than relying only on a single sensor modality, we consider integration of different modalities to get better detection performance in different failure cases. In our system, high level proprioceptive, auditory and visual predicates are extracted by processing each modality separately. Then, the extracted predicates are fused. Experiments on a humanoid robot for tabletop manipulation scenarios indicate that the contribution of each modality is different depending on the action in execution and multimodal fusion results in an overall performance increase in detecting failures compared to the performance attained by unimodal processing.
ER  - 

TY  - CONF
TI  - HARK-Bird-Box: A Portable Real-time Bird Song Scene Analysis System
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2497
EP  - 2502
AU  - R. Kojima
AU  - O. Sugiyama
AU  - K. Hoshiba
AU  - R. Suzuki
AU  - K. Nakadai
PY  - 2018
KW  - acoustic signal processing
KW  - feature extraction
KW  - learning (artificial intelligence)
KW  - microphone arrays
KW  - neural nets
KW  - public domain software
KW  - robots
KW  - zoology
KW  - wild birds
KW  - portable device
KW  - sound sources
KW  - real-time requirement
KW  - sound source detection
KW  - bird song analysis
KW  - open source software
KW  - bird song classifier
KW  - portability
KW  - computational time
KW  - bird song dataset
KW  - classification accuracy
KW  - HARK-bird-box
KW  - real-time bird song scene analysis system
KW  - Birds
KW  - Real-time systems
KW  - Microphone arrays
KW  - Source separation
KW  - Feature extraction
KW  - Image analysis
KW  - bird song scene analysis
KW  - robot audition
KW  - scene understanding
KW  - real-time system
DO  - 10.1109/IROS.2018.8594070
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper addresses real-time bird song scene analysis. Observation of animal behavior such as communication of wild birds would be aided by a portable device implementing a real-time system that can localize sound sources, measure their timing, classify their sources, and visualize these factors of sources. The difficulty of such a system is an integration of these functions considering the real-time requirement. To realize such a system, we propose a cascaded approach, cascading sound source detection, localization, separation, feature extraction, classification, and visualization for bird song analysis. Our system is constructed by combining an open source software for robot audition called HARK and a deep learning library to implement a bird song classifier based on a convolutional neural network (CNN). Considering portability, we implemented this system on a single-board computer, Jetson TX2, with a microphone array and developed a prototype device for bird song scene analysis. A preliminary experiment confirms a computational time for the whole system to realize a real-time system. Also, an additional experiment with a bird song dataset revealed a trade-off relationship between classification accuracy and time consuming and the effectiveness of our classifier.
ER  - 

TY  - CONF
TI  - Multi-timescale Feature-extraction Architecture of Deep Neural Networks for Acoustic Model Training from Raw Speech Signal
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2503
EP  - 2510
AU  - R. Takeda
AU  - K. Nakadai
AU  - K. Komatani
PY  - 2018
KW  - acoustic signal processing
KW  - feature extraction
KW  - neural nets
KW  - speech recognition
KW  - robot audition
KW  - normalization-free processing
KW  - speech features
KW  - multitimescale architecture
KW  - speech signals
KW  - low-latency speech recognition
KW  - utterance-wise mean subtraction
KW  - acoustic models
KW  - raw speech signal
KW  - acoustic model training
KW  - deep neural networks
KW  - multitimescale feature-extraction architecture
KW  - Feature extraction
KW  - Acoustics
KW  - Artificial neural networks
KW  - Splicing
KW  - Robots
KW  - Filter banks
KW  - Training
DO  - 10.1109/IROS.2018.8593925
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper describes a new architecture of deep neural networks (DNNs) for acoustic models. Training DNNs from raw speech signals will provide 1) novel features of signals, 2) normalization-free processing such as utterance-wise mean subtraction, and 3) low-latency speech recognition for robot audition. Exploiting the longer context of raw speech signals seems useful in improving recognition accuracy. However, naive use of longer contexts results in the loss of short-term patterns; thus, recognition accuracy degrades. We propose a multi-timescale feature-extraction architecture of DNNs with blocks of different time scales, which enable capturing long- and short-term patterns of speech signals. Each block consists of complex-valued networks that correspond to Fourier and filterbank transformations for analysis. Experiments showed that the proposed multi-timescale architecture reduced the word error rate by about 3% compared with those only with the longterm context. Analysis of the extracted features revealed that our architecture efficiently captured the slow and fast changes of speech features.
ER  - 

TY  - CONF
TI  - Tracking a moving sound source from a multi-rotor drone
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2511
EP  - 2516
AU  - L. Wang
AU  - R. Sanchez-Matilla
AU  - A. Cavallaro
PY  - 2018
KW  - acoustic signal processing
KW  - audio signal processing
KW  - autonomous aerial vehicles
KW  - cameras
KW  - feature extraction
KW  - helicopters
KW  - humanoid robots
KW  - particle filtering (numerical methods)
KW  - signal denoising
KW  - spatial filters
KW  - time-frequency analysis
KW  - ground-truth trajectory
KW  - noisy estimations
KW  - direction of arrival
KW  - ego-noise
KW  - human speaker
KW  - multirotor drone
KW  - moving sound source
KW  - moving source
KW  - short audio segments
KW  - time-frequency spatial filter
KW  - specific drone
KW  - propellers
KW  - motors
KW  - emergency whistle
KW  - Drones
KW  - Time-frequency analysis
KW  - Direction-of-arrival estimation
KW  - Microphone arrays
KW  - Loudspeakers
KW  - Propellers
DO  - 10.1109/IROS.2018.8594483
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose a method to track from a multi-rotor drone a moving source, such as a human speaker or an emergency whistle, whose sound is mixed with the strong ego-noise generated by rotating motors and propellers. The proposed method is independent of the specific drone and does not need pre-training nor reference signals. We first employ a time-frequency spatial filter to estimate, on short audio segments, the direction of arrival of the moving source and then we track these noisy estimations with a particle filter. We quantitatively evaluate the results using a ground-truth trajectory of the sound source obtained with an on-board camera and compare the performance of the proposed method with baseline solutions.
ER  - 

TY  - CONF
TI  - Kinematic Morphing Networks for Manipulation Skill Transfer
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2517
EP  - 2523
AU  - P. Englert
AU  - M. Toussaint
PY  - 2018
KW  - affine transforms
KW  - image morphing
KW  - iterative methods
KW  - manipulators
KW  - motion control
KW  - neural nets
KW  - robot vision
KW  - kinematic model
KW  - robot motions
KW  - robot skill
KW  - manipulation skill transfer
KW  - kinematic morphing networks
KW  - affine transformations
KW  - map depth image observations
KW  - deep neural network
KW  - Kinematics
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Prototypes
KW  - Neural networks
KW  - Solid modeling
DO  - 10.1109/IROS.2018.8593832
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The transfer of a robot skill between different geometric environments is non-trivial since a wide variety of environments exists, sensor observations as well as robot motions are high-dimensional, and the environment might only be partially observed. We consider the problem of extracting a low-dimensional description of the manipulated environment in form of a kinematic model. This allows us to transfer a skill by defining a policy on a prototype model and morphing the observed environment to this prototype. A deep neural network is used to map depth image observations of the environment to morphing parameter, which include transformations and configurations of the prototype model. Using the concatenation property of affine transformations and the ability to convert point clouds to depth images allows to apply the network in an iterative manner. The network is trained on data generated in a simulator and on augmented data that is created with its own predictions. The algorithm is evaluated on different tasks, where it is shown that iterative predictions lead to a higher accuracy than one-step predictions.
ER  - 

TY  - CONF
TI  - Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correction
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2524
EP  - 2531
AU  - E. J. Shamwell
AU  - S. Leung
AU  - W. D. Nothwang
PY  - 2018
KW  - accelerometers
KW  - calibration
KW  - cameras
KW  - distance measurement
KW  - gyroscopes
KW  - inertial navigation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - vision-aided absolute trajectory estimation
KW  - unsupervised deep network
KW  - online error correction
KW  - unsupervised deep neural network approach
KW  - RGB-D imagery
KW  - inertial measurements
KW  - Visual-Inertial-Odometry Learner
KW  - inertial measurement unit intrinsic parameters
KW  - white noise
KW  - extrinsic calibration
KW  - camera
KW  - IMU measurements
KW  - hypothesis trajectories
KW  - scaled image projection errors
KW  - visual odometry
KW  - visual simultaneous localization
KW  - KITTI Odometry dataset
KW  - competitive odometry performance
KW  - visual-inertial odometry
KW  - Cameras
KW  - Jacobian matrices
KW  - Image reconstruction
KW  - Trajectory
KW  - Simultaneous localization and mapping
KW  - Training
DO  - 10.1109/IROS.2018.8593573
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Adstract- We present an unsupervised deep neural network approach to the fusion of RGB-D imagery with inertial measurements for absolute trajectory estimation. Our network, dubbed the Visual-Inertial-Odometry Learner (VIOLearner), learns to perform visual-inertial odometry (VIO) without inertial measurement unit (IMU) intrinsic parameters (corresponding to gyroscope and accelerometer bias or white noise) or the extrinsic calibration between an IMU and camera. The network learns to integrate IMU measurements and generate hypothesis trajectories which are then corrected online according to the Jacobians of scaled image projection errors with respect to a spatial grid of pixel coordinates. We evaluate our network against state-of-the-art (SOA) visual-inertial odometry, visual odometry, and visual simultaneous localization and mapping (VSLAM) approaches on the KITTI Odometry dataset [1] and demonstrate competitive odometry performance.
ER  - 

TY  - CONF
TI  - Distributed Deep Reinforcement Learning based Indoor Visual Navigation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2532
EP  - 2537
AU  - S. Hsu
AU  - S. Chan
AU  - P. Wu
AU  - K. Xiao
AU  - L. Fu
PY  - 2018
KW  - indoor environment
KW  - indoor navigation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - path planning
KW  - robot vision
KW  - complicated environment scene
KW  - motor control command
KW  - navigation task
KW  - large-scale indoor complex environment
KW  - pre-constructed map
KW  - indoor environment
KW  - complex spatial perception possible
KW  - indoor space
KW  - complex navigation path
KW  - aforementioned large-scale environment
KW  - real environments
KW  - distributed deep reinforcement learning based indoor visual navigation
KW  - Navigation
KW  - Visualization
KW  - Task analysis
KW  - Training
KW  - Reinforcement learning
KW  - Robots
KW  - Indoor environments
KW  - deep reinforcement learning
KW  - visual navigation
DO  - 10.1109/IROS.2018.8594352
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Recently, as the rise of deep reinforcement learning, it not only can help the robot to convert the complicated environment scene to motor control command directly but also can accomplish the navigation task properly. In this paper, we propose a novel structure, where the objective is to achieve navigation in large-scale indoor complex environment without pre-constructed map. Generally, it requires good understanding of such indoor environment to make complex spatial perception possible, especially when the indoor space consists of many walls and doors which might block the view of robot leading to complex navigation path. By the proposed distributed deep reinforcement learning in different local regions, our method can achieve indoor visual navigation in the aforementioned large-scale environment without extra map information and human instruction. In the experiments, we validate our proposed method by conducting highly promising navigation tasks both in simulation and real environments.
ER  - 

TY  - CONF
TI  - Synthesizing Neural Network Controllers with Probabilistic Model-Based Reinforcement Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2538
EP  - 2544
AU  - J. C. Gamboa Higuera
AU  - D. Meger
AU  - G. Dudek
PY  - 2018
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neurocontrollers
KW  - underwater vehicles
KW  - complex neural network controllers
KW  - motor controllers
KW  - probabilistic model-based reinforcement learning
KW  - robotics systems
KW  - sample-based version
KW  - Deep-PILeO
KW  - model-based algorithm
KW  - random numbers
KW  - clips gradients
KW  - neural network dynamics model
KW  - data-efficient synthesis
KW  - complex neural network policies
KW  - data-efficiency
KW  - truncated log-normal noise
KW  - Robots
KW  - Optimization
KW  - Vehicle dynamics
KW  - Task analysis
KW  - Heuristic algorithms
KW  - Neural networks
KW  - Stochastic processes
DO  - 10.1109/IROS.2018.8594018
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present an algorithm for rapidly learning neural network policies for robotics systems. The algorithm follows the model-based reinforcement learning paradigm and improves upon existing algorithms: PILeO and a sample-based version of PILeo with neural network dynamics (Deep-PILeO). To improve convergence, we propose a model-based algorithm that uses fixed random numbers and clips gradients during optimization. We propose training a neural network dynamics model using variational dropout with truncated Log-Normal noise. These improvements enable data-efficient synthesis of complex neural network policies. We test our approach on a variety of benchmark tasks, demonstrating data-efficiency that is competitive with that of PILeO, while being able to optimize complex neural network controllers. Finally, we assess the performance of the algorithm for learning motor controllers for a six legged autonomous underwater vehicle. This demonstrates the potential of the algorithm for scaling up the dimensionality and dataset sizes, in more complex tasks.
ER  - 

TY  - CONF
TI  - Composite Reinforcement Learning for Social Robot Navigation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2553
EP  - 2558
AU  - P. Ciou
AU  - Y. Hsiao
AU  - Z. Wu
AU  - S. Tseng
AU  - L. Fu
PY  - 2018
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - service robots
KW  - minimum distance path
KW  - deep reinforcement learning technique
KW  - navigational movement
KW  - service robot
KW  - social robot navigation
KW  - CRL system
KW  - human robot interaction
KW  - human feedback
KW  - composite reinforcement learning framework
KW  - high dimension complex problem
KW  - Navigation
KW  - Reinforcement learning
KW  - Legged locomotion
KW  - Task analysis
KW  - Collision avoidance
KW  - Robot kinematics
DO  - 10.1109/IROS.2018.8593410
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For a service robot, it is not adequate to let its navigational movement be based only on a single metric, such as minimum distance path. In the environment where the robot and humans are coexisting, the robot should always perform social navigation whenever it is moving. However, to perform social navigation, the robot needs to follow certain “social norms” of the environment. Recently, deep reinforcement learning (DRL) technique is popularly applied to the robotics field; yet, it is rarely used to solve the mentioned social navigation problem, generally deemed as a high dimension complex problem. In this paper, we propose the composite reinforcement learning (CRL) framework under which the robot learns appropriate social navigation with sensor input and reward update based on human feedback. For learning the aspect of human robot interaction (HRI), we provide a method to facilitate the training of DRL in real environment by incorporating prior knowledge to the system. It turns out that our CRL system not only can incrementally learn how to set its velocity and to perform HRI but also keep collecting human feedback to synchronize the reward functions to the current social norms. The experiments show that the proposed CRL system can safely learn how to navigate in the environment and show that our system is able to perform HRI for social navigation.
ER  - 

TY  - CONF
TI  - Apple Counting using Convolutional Neural Networks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2559
EP  - 2565
AU  - N. Häni
AU  - P. Roy
AU  - V. Isler
PY  - 2018
KW  - agricultural products
KW  - convolutional neural nets
KW  - Gaussian processes
KW  - horticulture
KW  - image classification
KW  - object detection
KW  - horticultural studies
KW  - logistics planning
KW  - Gaussian mixture model
KW  - convolutional neural network
KW  - yield estimate
KW  - per-image accuracy
KW  - fruit counting
KW  - fruit detection
KW  - vegetable counts
KW  - apple counting
KW  - Image color analysis
KW  - Yield estimation
KW  - Agriculture
KW  - Image segmentation
KW  - Clustering algorithms
KW  - Task analysis
KW  - Vegetation
DO  - 10.1109/IROS.2018.8594304
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Estimating accurate and reliable fruit and vegetable counts from images in real-world settings, such as orchards, is a challenging problem that has received significant recent attention. Estimating fruit counts before harvest provides useful information for logistics planning. While considerable progress has been made toward fruit detection, estimating the actual counts remains challenging. In practice, fruits are often clustered together. Therefore, methods that only detect fruits fail to offer general solutions to estimate accurate fruit counts. Furthermore, in horticultural studies, rather than a single yield estimate, finer information such as the distribution of the number of apples per cluster is desirable. In this work, we formulate fruit counting from images as a multi-class classification problem and solve it by training a Convolutional Neural Network. We first evaluate the per-image accuracy of our method and compare it with a state of the art method based on Gaussian Mixture Models over four test datasets. Even though the parameters of the Gaussian Mixture Model based method are specifically tuned for each dataset, our network outperforms it in three out of four datasets with a maximum of 94% accuracy. Next, we use the method to estimate the yield for two datasets for which we have ground truth. Our method achieved 96-97% accuracies. For additional details please see our video here: https://www.youtube.com/watch?v=Le0mb5P-SYc.
ER  - 

TY  - CONF
TI  - Target Localization with Drones using Mobile CNNs
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2566
EP  - 2573
AU  - Y. Lu
AU  - Z. Wang
AU  - Z. Tang
AU  - T. Javidi
PY  - 2018
KW  - aerospace robotics
KW  - decision theory
KW  - learning (artificial intelligence)
KW  - Markov processes
KW  - search problems
KW  - target localization
KW  - accurate visual search
KW  - effective search strategies
KW  - observation models
KW  - latest developments
KW  - mobile platforms
KW  - policy search
KW  - point based methods
KW  - POMDP framework
KW  - single basketball
KW  - perception modules
KW  - error characteristics
KW  - control algorithm
KW  - realistic parameters
KW  - fast search strategy
KW  - longer search time
KW  - real data
KW  - error rates
KW  - false positive rates
KW  - false negative rates
KW  - visual search strategies
KW  - drone platform
KW  - robust algorithm
KW  - mobile CNN
KW  - Drones
KW  - Sensors
KW  - Search problems
KW  - Data collection
KW  - Computational modeling
KW  - Delays
KW  - Visualization
DO  - 10.1109/IROS.2018.8594163
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Fast and accurate visual search is an enabler for many applications of drones. Prior works use POMDPs to produce effective search strategies. As the observation models are from heuristics, the robustness of these approaches on the field is unclear. This work builds a testbed that combines latest developments in related areas, including mobile CNNs for inference on mobile platforms and policy search with point based methods, in a POMDP framework. A dataset for a simple but realistic application, search for a single basketball, is collected to train the perception modules, investigate their error characteristics and validate the control algorithm. From simulation using realistic parameters, we found the significant role persistent factors in the environment can play in designing a fast search strategy. Failure to taking these factors into account results in up to 60% longer search time at the same success rate. Our empirical tests using mobile CNN and real data reveals that prior assumptions on error rates as functions of heights are wrong. The errors grows non-linearly, and there is significant between false positive and false negative rates. Our findings shed new lights on what to consider in designing visual search strategies in a drone platform and is one step towards a fast and robust algorithm.
ER  - 

TY  - CONF
TI  - An Adjustable Force Sensitive Sensor with an Electromagnet for a Soft, Distributed, Digital 3-axis Skin Sensor
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2582
EP  - 2588
AU  - A. C. Holgado
AU  - J. A. Alvarez Lopez
AU  - A. Schmitz
AU  - T. P. Tomo
AU  - S. Somlor
AU  - L. Jamone
AU  - S. Sugano
PY  - 2018
KW  - distributed sensors
KW  - electromagnets
KW  - foams
KW  - force measurement
KW  - force sensors
KW  - magnetic field measurement
KW  - magnetic sensors
KW  - microcontrollers
KW  - microsensors
KW  - skin
KW  - 3-axis force sensor
KW  - soft distributed digital 3-axis skin sensor
KW  - integrated microcontroller
KW  - adjustable force sensitive sensor
KW  - magnetic field strength
KW  - soft foam
KW  - 3-axis magnetic sensor
KW  - planar electromagnet
KW  - Robot sensing systems
KW  - Sensitivity
KW  - Magnetometers
KW  - Magnetic separation
KW  - Coils
KW  - Force
DO  - 10.1109/IROS.2018.8593757
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Typically, the range and sensitivity of force sensors are determined during production. However, to be able to do both delicate and high-force demanding work, adjustable force sensitivity would be beneficial. The current paper proposes such a sensor by implementing a planar electromagnet above a 3-axis magnetic sensor, separated by soft foam. Furthermore, the sensor has digital output with an integrated microcontroller. The magnetic field strength with varying currents is examined in simulation, and the field changes according to displacements are investigated both in simulation and with the actual sensor. A prototype 3-axis force sensor is implemented and the relationship between the magnetic field change and the corresponding applied force is also investigated. It could be shown that the sensitivity of the sensor to displacements, as well as force, can indeed be adjusted.
ER  - 

TY  - CONF
TI  - Object Recognition Through Active Sensing Using a Multi-Fingered Robot Hand with 3D Tactile Sensors
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2589
EP  - 2595
AU  - S. Funabashi
AU  - S. Morikuni
AU  - A. Geier
AU  - A. Schmitz
AU  - S. Ogasa
AU  - T. P. Torno
AU  - S. Somlor
AU  - S. Sugano
PY  - 2018
KW  - control engineering computing
KW  - convolutional neural nets
KW  - dexterous manipulators
KW  - object recognition
KW  - recurrent neural nets
KW  - tactile sensors
KW  - time series
KW  - multifingered robot hand
KW  - triaxial force vector measurements
KW  - 3D tactile sensors
KW  - distributed force vector measurements
KW  - feedforward neural network
KW  - recurrent neural network
KW  - time-series training
KW  - active object sensing
KW  - Allegro Hand
KW  - uSkin tactile sensors
KW  - tactile object recognition
KW  - time series data
KW  - Force
KW  - Force measurement
KW  - Object recognition
KW  - Tactile sensors
DO  - 10.1109/IROS.2018.8594159
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper investigates tactile object recognition with relatively densely distributed force vector measurements and evaluates what kind of tactile information is beneficial for object recognition. The uSkin tactile sensors are embedded in an Allegro Hand, and provide 240 triaxial force vector measurements in total in all fingers. Active object sensing is used to gather time-series training and testing data. A simple feedforward, a recurrent, and a convolutional neural network are used for recognizing objects. Evaluations with different number of employed measurements, static vs. time series data and force vector vs. only normal force vector measurements show that the high-dimensional information provided by the sensors is indeed beneficial. An object recognition rate of up to 95% for 20 objects was achieved.
ER  - 

TY  - CONF
TI  - Sensory-motor augmentation of the robot with shared human perception
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2596
EP  - 2603
AU  - R. Ishida
AU  - L. Meli
AU  - Y. Tanaka
AU  - K. Minamizawa
AU  - D. Prattichizzo
PY  - 2018
KW  - control engineering computing
KW  - dexterous manipulators
KW  - haptic interfaces
KW  - human-robot interaction
KW  - manipulators
KW  - man-machine systems
KW  - mobile robots
KW  - robot vision
KW  - robots
KW  - tactile sensors
KW  - operator actions
KW  - co-manipulated object
KW  - human hand
KW  - tiny vibration sensor
KW  - low-level robot intelligence
KW  - human operator
KW  - human-robot collaboration
KW  - human-robot cooperation
KW  - dexterous manipulation operations
KW  - manufacturing production lines
KW  - shared human perception
KW  - sensory-motor augmentation
KW  - Robot sensing systems
KW  - Task analysis
KW  - Vibrations
KW  - Force
KW  - Mechanical sensors
DO  - 10.1109/IROS.2018.8594496
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robots have replaced people in many manufacturing production lines but the information they gather from sensors might not be sufficient to autonomously accomplish dexterous manipulation operations. Symbiotic human-robot cooperation appears to be a more realistic near future in industrial scenarios. In this paper we present a configuration of human-robot collaboration in which the robot is sensory-augmented by means of a set of tactile signals coming from the human operator. The incorporation of low-level robot “intelligence” permits the cooperative manipulation of an object while enabling the human operator to stay focused on task itself and carry it out in the most natural way. The effectiveness of this approach is demonstrated in a use case in which a robot helps a human operator to successfully accomplish a writing task. System performance has been evaluated, considering several positions of the tiny vibration sensor in charge of gathering the human perception, by testing it on both the human hand and the co-manipulated object. Results suggest that the sensor provides valuable information for recognizing operator actions when it is placed either on the human hand or on the co-manipulated object. However, the sensor on the finger directly represents the operator's perception, while the output of the sensor attached to the object changes according to the distance between the interaction point and the sensor itself. In addition, in wearing the sensor, neither the object nor the robot need to be instrumented: the operator is free to interact with a large set of objects and collaborate with any existing robot without requiring supplemental equipment.
ER  - 

TY  - CONF
TI  - HTC Vive: Analysis and Accuracy Improvement
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2610
EP  - 2615
AU  - M. Borges
AU  - A. Symington
AU  - B. Coltin
AU  - T. Smith
AU  - R. Ventura
PY  - 2018
KW  - calibration
KW  - object tracking
KW  - pose estimation
KW  - tracking
KW  - virtual reality
KW  - estimation repeatability
KW  - calibration procedure
KW  - open-source tracking algorithm
KW  - robotics applications
KW  - shelf algorithm
KW  - virtual reality applications
KW  - inertial measurements
KW  - millimeter magnitude
KW  - controlled experiments
KW  - ground truth
KW  - off-the-shelf tracking system
KW  - cost-effective
KW  - HTC Vive
KW  - Robots
KW  - Tracking
KW  - Photodiodes
KW  - Extraterrestrial measurements
KW  - Pose estimation
KW  - Transforms
DO  - 10.1109/IROS.2018.8593707
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - HTC Vive has been gaining attention as a cost-effective, off-the-shelf tracking system for collecting ground truth pose data. We assess this system's pose estimation through a series of controlled experiments where we show its precision to be in the millimeter magnitude and accuracy to range from millimeter to meter. We also show that Vive gives greater weight to inertial measurements in order to produce a smooth trajectory for virtual reality applications. Hence, the Vive's off the shelf algorithm is poorly suited for robotics applications such as measuring ground truth poses, where accuracy and repeatability are key. Therefore we introduce a new open-source tracking algorithm and calibration procedure for Vive which address these problems. We also show that our approach improves the pose estimation repeatability and accuracy by up to two orders of magnitude.
ER  - 

TY  - CONF
TI  - Improving indoor robots localisation by fusing different sensors
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2616
EP  - 2623
AU  - B. P. Alvarado
AU  - F. Matía
AU  - R. Galán
PY  - 2018
KW  - cameras
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - pose estimation
KW  - laser LMS-200
KW  - omnidirectional camera Mobotix C2S
KW  - tour guide robot
KW  - external landmarks
KW  - indoor mobile robots navigation
KW  - odometry
KW  - external sensors
KW  - indoor robots localisation
KW  - Cameras
KW  - Measurement by laser beam
KW  - Lasers
KW  - Robot vision systems
DO  - 10.1109/IROS.2018.8593667
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Indoor mobile robots navigation must use external sensors to complement odometry. This paper analyses two different external sensors such as a laser LMS-200 and an omnidirectional camera Mobotix C2S. Experiments with only one of these sensors and with both integrated are carried out on a tour guide robot in order to obtain conclusions about their contribution to robot pose estimation, and how to locate external landmarks in the environment.
ER  - 

TY  - CONF
TI  - Robust Camera Pose Estimation via Consensus on Ray Bundle and Vector Field
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2624
EP  - 2631
AU  - H. Li
AU  - J. Zhao
AU  - J. Bazin
AU  - L. Luo
AU  - J. Wu
AU  - J. Yao
PY  - 2018
KW  - Bayes methods
KW  - expectation-maximisation algorithm
KW  - image matching
KW  - pose estimation
KW  - probability
KW  - vectors
KW  - robust camera pose estimation
KW  - point correspondences
KW  - general outlier removal strategy
KW  - 3D ray bundle consensus
KW  - 2D vector field consensus
KW  - expectation-maximization algorithm
KW  - inlier probability
KW  - outlier rejection methods
KW  - Cameras
KW  - Pose estimation
KW  - Two dimensional displays
KW  - Three-dimensional displays
KW  - Robot vision systems
KW  - Robustness
KW  - Probabilistic logic
DO  - 10.1109/IROS.2018.8594486
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Estimating the camera pose requires point correspondences. However, in practice, correspondences are inevitably corrupted by outliers, which affects the pose estimation. We propose a general and accurate outlier removal strategy for robust camera pose estimation. The proposed strategy can detect outliers by leveraging the fact that only inliers comply with two effective consensuses, i.e., 3D ray bundle consensus and 2D vector field consensus. Our strategy has a nested structure. First, the outer module utilizes the 3D ray bundle consensus. We define the likelihood based on the probabilistic mixture model and maximize it by the expectation-maximization (EM) algorithm. The inlier probability of each correspondence and the camera pose are determined alternately. Second, the inner module exploits the 2D vector field consensus to refine the probabilities obtained by the outer module. The refinement based on the Bayesian rule facilitates the convergence of the outer module and improves the accuracy of the entire framework. Our strategy can be integrated into various existing camera pose estimation methods which are originally vulnerable to outliers. Experiments on both synthesized data and real images have shown that our approach outperforms state-of-the-art outlier rejection methods in terms of accuracy and robustness.
ER  - 

TY  - CONF
TI  - Efficient Map Representations for Multi-Dimensional Normal Distributions Transforms
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2679
EP  - 2686
AU  - C. Schulz
AU  - R. Hanten
AU  - A. Zell
PY  - 2018
KW  - mobile robots
KW  - Monte Carlo methods
KW  - normal distribution
KW  - probability
KW  - robot vision
KW  - stereo image processing
KW  - transforms
KW  - indoor environments
KW  - outdoor environments
KW  - driving flying robots
KW  - fast approach
KW  - accurate approach
KW  - indexed kd-trees
KW  - free space
KW  - occupancy probabilities
KW  - map consistency
KW  - large-scale environments
KW  - mapping efficiency
KW  - efficient map representations
KW  - 3D map representations
KW  - static environments
KW  - dynamic environments
KW  - multidimensional normal distributions transforms
KW  - 3D normal distributions transform mapping
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Gaussian distribution
KW  - Two dimensional displays
KW  - Task analysis
KW  - Transforms
DO  - 10.1109/IROS.2018.8593602
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Efficient 2D and 3D map representations of both static and dynamic, indoor and outdoor environments are crucial for navigation of driving and flying robots. In this paper, we propose a fast and accurate approach for 2D and 3D Normal Distributions Transform (NDT) mapping based on indexed kd-trees. Similar to other approaches, we also model free space, which allows us to obtain occupancy probabilities. Additionally, we provide optional visibility based updates to enhance map consistency in case of noisy data, e.g. from stereo cameras. Unlike other available implementations, our approach is natively applicable to large-scale environments and in real-time, because our maps are able to grow dynamically. This also offers applicability to exploration tasks. To evaluate our approach, we present experimental results on publicly available datasets and discuss the mapping efficiency in terms of accuracy, runtime and memory management. As an exemplary use case, we apply our maps to Monte Carlo Localization on a well-known large-scale dataset.
ER  - 

TY  - CONF
TI  - Modeling and Control of an Articulated Tail for Maneuvering a Reduced Degree of Freedom Legged Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2695
EP  - 2700
AU  - W. Saab
AU  - J. Yang
AU  - P. Ben-Tzvi
PY  - 2018
KW  - actuators
KW  - feedback
KW  - hardware-in-the loop simulation
KW  - legged locomotion
KW  - linearisation techniques
KW  - motion control
KW  - robot dynamics
KW  - leg mechanisms
KW  - quadruped robot
KW  - dynamic tail motions
KW  - robotic system design
KW  - outer loop controller
KW  - articulated tail mechanism
KW  - inner loop controller
KW  - tail prototype
KW  - dynamic modeling control
KW  - articulated robotic tail
KW  - maneuvering
KW  - legged robotic systems
KW  - reduced degree of freedom legged robot
KW  - hardware-in-the-loop experiments
KW  - quadruped platform simulation
KW  - feedback linearization maps
KW  - Legged locomotion
KW  - Robot kinematics
KW  - Foot
KW  - Dynamics
KW  - Task analysis
KW  - Manipulators
DO  - 10.1109/IROS.2018.8593945
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents dynamic modeling and control of an articulated robotic tail to maneuver and stabilize a reduced degree-of-freedom (DOF) quadruped robot. Conventional legged robotic systems consist of leg mechanisms that provide simultaneous propulsion, maneuvering and stabilization. However, in nature animals have been observed to utilize their tails to assist the legs in multiple tasks. Similarly, by incorporating an articulated tail onboard a quadruped robot, dynamic tail motions can be used to aid maneuvering. Therefore, tail implementation can potentially lead to simplifications in design and control of the legged robot since the legs will be responsible for only propulsion tasks. In this paper, a robotic system design consisting of an articulated tail and quadruped robot system is presented. Dynamic models are derived to analyze an optimal tail mass and length ratio to enhance inertial adjustment applications and develop an outer loop controller to plan tail trajectories for desired maneuvering applications. Results of analytical optimization are corroborated with measured data from biological animals. To decouple the dynamics of the articulated tail mechanism an inner loop controller using feedback linearization maps the desired behavior to the actuator inputs. This approach is validated using hardware-in-the-loop experiments with tail prototype in conjunction with simulated quadruped platform. Results demonstrate the capabilities of the articulated tail in enabling precise left and right turning (maneuvering).
ER  - 

TY  - CONF
TI  - Modeling and Fuzzy Control of One-legged Somersaulting Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2701
EP  - 2706
AU  - M. Zabihi
AU  - A. Alasty
PY  - 2018
KW  - fuzzy control
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - robot kinematics
KW  - springs (mechanical)
KW  - flight phases
KW  - hybrid dynamic model
KW  - challenging control issue
KW  - one-legged hopping robots
KW  - springy leg
KW  - fuzzy logic control
KW  - one-legged somersaulting robot
KW  - multilegged ones
KW  - hopping motion
KW  - SLIP robots
KW  - Legged locomotion
KW  - Actuators
KW  - Mathematical model
KW  - Torque
KW  - Wheels
DO  - 10.1109/IROS.2018.8593897
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Research on legged robots has developed rapidly in the recent decades. One-legged robots, unlike multi-legged ones, have only one type of motion, called hopping. Hopping motion is generally divided into stance and flight phases. Switching between these two phases represents a hybrid dynamic model. Dynamic stabilization of hopping motion is a challenging control issue. Most of one-legged hopping robots studied in the past are able to hop with their one springy leg. In this paper, a novel one-legged robot is introduced and studied with two springs on the two sides. The one-legged somersaulting robot is able to hop with both springy sides. This ability causes lower energy consumption in passing obstacles and a longer step length in comparison with well-known SLIP robots with hopping motion stemming from the fact that it only has one rotary actuator. Fuzzy logic control is applied to achieve a stable limit cycle in the robot's somersaulting motion.
ER  - 

TY  - CONF
TI  - Towards a Passive Adaptive Planar Foot with Ground Orientation and Contact Force Sensing for Legged Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2707
EP  - 2714
AU  - R. Käslin
AU  - H. Kolvenbach
AU  - L. Paez
AU  - K. Lika
AU  - M. Hutter
PY  - 2018
KW  - force sensors
KW  - gait analysis
KW  - legged locomotion
KW  - robot dynamics
KW  - soil
KW  - stability
KW  - ground orientation
KW  - drift-free relative foot sole pose
KW  - passive adaptive planar foot
KW  - ground contact
KW  - highly dynamic legged robots
KW  - point foot design
KW  - legged locomotion
KW  - contact force sensor
KW  - stability
KW  - inertial measurement units
KW  - IMUs
KW  - quadrupedal robot ANYmal
KW  - compressible soils
KW  - built-in 6-axis force-torque transducer
KW  - Foot
KW  - Sensors
KW  - Legged locomotion
KW  - Force
KW  - Force measurement
KW  - Microcontrollers
DO  - 10.1109/IROS.2018.8593875
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Adapting to the ground enables stable footholds in legged locomotion by exploiting the structure of the terrain. On that account, we present a passive adaptive planar foot with three rotational degrees of freedom that is lightweight and thus suited for highly dynamic legged robots. Its low laying pivot joint provides high stability towards kinking. Information about the relative foot sole pose, and accordingly, the ground orientation is gathered by inertial measurement units (IMUs) placed on the foot sole and the shank. A complementary filter is presented that fuses these orientation estimates with an angular encoder to obtain a drift-free relative foot sole pose. The passive adaptive planar foot has been tested and compared to the classical point foot design on a variety of terrains and shows superior traction performance, especially on compressible soils. Being mounted on the quadrupedal robot ANYmal, the foot provides a reliable contact detection based on the fusion of the built-in 6-axis force/torque transducer and the IMUs. This allows to walk and trot on uneven terrain, loose soils, as well as climbing up a ramp and stairs while keeping the entire foot sole in ground contact all the time.
ER  - 

TY  - CONF
TI  - SLIP-Model-Based Dynamic Motion Transition Between Different Fixed Points in One Stride in a Leg-Wheel Transformable Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2715
EP  - 2720
AU  - H. Lin
AU  - Y. Lin
AU  - P. Lin
PY  - 2018
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - nonlinear control systems
KW  - pendulums
KW  - stable running motion
KW  - stable fixed-point trajectories
KW  - ordinary SLIP model
KW  - passive spring
KW  - leg-spring stiffness
KW  - fixed-point trajectory
KW  - multistride transition
KW  - leg-wheel transformable robot
KW  - SLIP-model-based dynamic motion transition
KW  - motion generation strategy
KW  - force control
KW  - TurboQuad
KW  - Legged locomotion
KW  - Springs
KW  - Robot kinematics
KW  - Mathematical model
KW  - Dynamics
KW  - DC motors
DO  - 10.1109/IROS.2018.8594364
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We report on the development of a motion generation strategy that allows the robot to transit from one stable running motion to another in one stride by actively changing leg stiffness in real time. Stable motion of the robot is generated based on the stable fixed-point trajectories of the spring-loaded inverted pendulum (SLIP) model. While the transition of the ordinary SLIP model with fixed parameters gradually converges if stable, a robot that uses force control to simulate the passive spring of the SLIP can actively modulate leg-spring stiffness. This enables the robot to switch instantly to another fixed-point trajectory of the SLIP model without going through multi-stride transition. The proposed method is implemented on a leg-wheel transformable robot, TurboQuad, and is evaluated experimentally. The results confirm that the robot can successfully transit between different fixed-point trajectories.
ER  - 

TY  - CONF
TI  - Continuous Shape Changing Locomotion of 32-legged Spherical Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2721
EP  - 2726
AU  - H. Nozaki
AU  - Y. Kujirai
AU  - R. Niiyama
AU  - Y. Kawahara
AU  - T. Yonezawa
AU  - J. Nakazawa
PY  - 2018
KW  - legged locomotion
KW  - trajectory control
KW  - amoeba movement
KW  - Mochibot
KW  - trajectory control
KW  - deformable robots
KW  - omni directional continuous crawling
KW  - free form locomotion
KW  - 32-legged spherical robot
KW  - continuous shape changing locomotion
KW  - Shape
KW  - Legged locomotion
KW  - Actuators
KW  - Skeleton
KW  - Rails
DO  - 10.1109/IROS.2018.8593791
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Shape changing robot is an approach towards locomotion on uncertain terrain due to its omni-directional features. However, the current locomotion method for such robots rely on discontinuous rolling. We propose a free form locomotion: an omni directional continuous crawling for deformable robots. This method introduce continuous shifting of contact surface similar to amoeba movement. A Mochibot that has thirty two telescopic legs is developed to verify the proposed locomotion method. Through the experiments, we have confirmed that the robot can track smooth paths: straight, smooth, and hand written curves. We also evaluate errors between desired and measured trajectories of the robot.
ER  - 

TY  - CONF
TI  - End-effector with a Hook and Two Fingers for the Locomotion and Simple Work of a Four-limbed Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2727
EP  - 2732
AU  - T. Matsuzawa
AU  - A. Imai
AU  - K. Hashimoto
AU  - T. Teramachi
AU  - X. Sun
AU  - S. Kimura
AU  - N. Sakai
AU  - Y. Yoshida
AU  - K. Kumaaai
AU  - T. Matsubara
AU  - K. Yamaguchi
AU  - A. Takanishi
PY  - 2018
KW  - end effectors
KW  - legged locomotion
KW  - motion control
KW  - manipulation tasks
KW  - four-limb robot WAREC-1
KW  - vertical ladder
KW  - fingers
KW  - locomotion modes
KW  - legged robot
KW  - hook shape
KW  - grasping working
KW  - end-effector
KW  - End effectors
KW  - Thumb
KW  - Grasping
KW  - Force
KW  - Shape
DO  - 10.1109/IROS.2018.8593422
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose an end-effector for realizing various locomotion modes and simple work of a legged robot. The locomotion modes include climbing a vertical ladder, crawling, and walking. The simple work includes grasping and switching motions required at a disaster site. The developed end-effector has a two-pronged hook shape and two fingers for grasping and working and can be used to perform the locomotion and manipulation tasks described above. The experimental results confirmed that the four-limb robot WAREC-1 (WAseda REsCuer-No. 1) equipped with our proposed end-effector was able to climb a vertical ladder and perform the crawling motion. We also confirmed that the end-effector could grasp and switch five types of objects: a cylinder, cylinder with trigger, T-shaped, disk, and thin plate.
ER  - 

TY  - CONF
TI  - A Framework for Modeling Closed Kinematic Chains with a Focus on Legged Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2733
EP  - 2738
AU  - V. R. Kamidi
AU  - A. Williams
AU  - P. Ben–Tzvi
PY  - 2018
KW  - legged locomotion
KW  - robot kinematics
KW  - singularly perturbed systems
KW  - legged robots
KW  - MATLAB framework
KW  - dynamic modeling simulation
KW  - legged locomotive mechanisms
KW  - fixed-base systems
KW  - singular perturbation theory
KW  - CKC mechanisms
KW  - dynamic monopedal gait
KW  - closed kinematic chains
KW  - floating-base systems
KW  - functional API
KW  - Mathematical model
KW  - Legged locomotion
KW  - Kinematics
KW  - Software
KW  - Computational modeling
KW  - Couplings
DO  - 10.1109/IROS.2018.8593909
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents the foundations of a MATLAB framework for dynamic modeling and simulation of closed kinematic chain (CKC) mechanisms, with a particular focus on implementation with legged locomotive mechanisms. As such, the framework supports both floating-base and fixed-base systems. Through the use of singular perturbation theory, various CKC mechanisms can be modeled so that constraint errors asymptotically converge to zero, thus avoiding the numerical drift that plagues commonly used methods. A functional API and the relevant core commands necessary to construct a model are presented. Two robotic legs incorporating CKC mechanisms are utilized as case studies, and simulations of each leg performing a dynamic monopedal gait are illustrated.
ER  - 

TY  - CONF
TI  - Steering of an Underactuated Legged Robot through Terrain Contact with an Active Tail
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2739
EP  - 2746
AU  - C. S. Casarez
AU  - R. S. Fearing
PY  - 2018
KW  - closed loop systems
KW  - drag
KW  - gait analysis
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - robot kinematics
KW  - steering systems
KW  - underactuated legged robot
KW  - terrain contact
KW  - rapid point turn
KW  - active tail payload
KW  - steady-state turning model
KW  - differential drive turning gaits
KW  - tail impact turning
KW  - tail drag
KW  - palm-sized legged robot
KW  - LoadRoACH
KW  - tail contact turning strategies
KW  - closed-loop corner steering maneuver
KW  - mass 55.0 g
KW  - Turning
KW  - Legged locomotion
KW  - Force
KW  - Steady-state
KW  - Drag
KW  - Aerodynamics
DO  - 10.1109/IROS.2018.8594384
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper analyzes and implements two novel turning strategies for underactuated legged robots that leverage contact of an active tail against terrain. The first strategy produces a sustained turn with a tail dragging against the ground during forward locomotion. The second strategy produces a rapid point turn by impacting the tail against the ground. LoadRoACH, a 55 g palm-sized legged robot, is developed to carry the active tail payload used in turning experiments. A steady-state turning model predicts the achievable turn speed of the robot on carpet, and open-loop turning experiments characterize the performance of the two tail contact turning strategies. Tail drag turning provides comparable turning maneuverability to differential drive turning gaits on carpet and gravel surfaces. Tail impact turning can produce rapid point turns on carpet, tarp, and gravel, but has a large variability in turn angle and time to recover from the turn. Finally, tail drag and tail impact turning control methods are implemented in an aggressive closed-loop corner steering maneuver.
ER  - 

TY  - CONF
TI  - Characterization of Active/Passive Pneumatic Actuators for Assistive Devices
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2747
EP  - 2754
AU  - D. Kaneishi
AU  - M. Tomizuka
AU  - R. P. Matthew
PY  - 2018
KW  - handicapped aids
KW  - nonlinear control systems
KW  - pneumatic actuators
KW  - springs (mechanical)
KW  - stability
KW  - passive nonlinear spring
KW  - stability
KW  - assistive device
KW  - active/passive pneumatic actuators
KW  - adjustable passive nonlinear spring
KW  - Force
KW  - Valves
KW  - Springs
KW  - Task analysis
KW  - Assistive devices
KW  - Pneumatic actuators
DO  - 10.1109/IROS.2018.8594143
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Assistive devices have been developed for power augmentation and task-oriented assistance such as loaded walking. The effective joint dynamics of the user can be altered using a wearable system, providing assistance when a task is performed. The authors have investigated an Active/Passive Pneumatic Actuator (AP2A) for an assistive device, which has a simple structure and responds as a passive nonlinear spring with controllable stiffness. This paper introduces a novel controller for the AP2 A and validates the performance through experiments. The developed controller is found to stabilize at the desired stiffness response within 1 second, confirming the ability of the AP2 A to act as an adjustable passive nonlinear spring.
ER  - 

TY  - CONF
TI  - Unpowered Lower-Body Exoskeleton with Torso Lifting Mechanism for Supporting Sit-to-Stand Transitions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2755
EP  - 2761
AU  - D. F. P. Granados
AU  - H. Kadone
AU  - K. Suzuki
PY  - 2018
KW  - biomechanics
KW  - bone
KW  - injuries
KW  - muscle
KW  - patient rehabilitation
KW  - pulleys
KW  - unpowered lower-body exoskeleton
KW  - torso lifting mechanism
KW  - knee joint
KW  - lower torso
KW  - spinal cord injury
KW  - lower-body impairments
KW  - power transfer mechanism
KW  - lumbar motion
KW  - cable-driven pulley system
KW  - human body dynamics
KW  - rigid link model
KW  - impedance model
KW  - passive system
KW  - natural motions
KW  - upper body
KW  - body residual capabilities
KW  - external power source
KW  - upper-body
KW  - passive energy storage
KW  - muscle activity
KW  - exoskeleton support
KW  - passive exoskeleton
KW  - STS training
KW  - STS posture transitions
KW  - sit-to-stand posture transitions
KW  - STS transition support
KW  - upright locomotion
KW  - patient rehabilitation
KW  - Exoskeletons
KW  - Biological system modeling
KW  - Torso
KW  - Torque
KW  - Robots
KW  - Wheelchairs
KW  - Dynamics
KW  - Passive exoskeleton
KW  - human dynamics modeling
KW  - physical human-robot interaction
KW  - rehabilitation robotics
DO  - 10.1109/IROS.2018.8594199
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose the design of an exoskeleton with support at the knee joint and lower torso for sit-to-stand and stand-to-sit (STS) posture transitions; devised for users with spinal cord injury and other complete lower-body impairments. The STS transitions assistance is achieved through a power transfer mechanism that synchronizes knees and lumbar motion through a cable-driven pulley system. We analyze the human body dynamics in the posture transition with a rigid link model and the interaction interface with the exoskeleton through an impedance model for producing a passive system voluntarily controlled by natural motions of the upper body. Therefore, allowing the potential users to achieve STS transitions with their body residual capabilities without an external power source. Instead, transferring power from their upper-body to lower-body, herewith, controlling a passive energy storage. A prototype was constructed and evaluated with seven healthy subjects observing the proposed motion and muscle activity during the STS transitions. The results show a significant reduction in the muscle activity evaluated, at the erector spinae, gluteus maximus and rectus femoris, with reductions between 30% to 50% at the p <; 0.01 level comparing STS transitions with and without the exoskeleton support. Concluding that the STS transitions support is feasible with the passive exoskeleton envisioned for applications in upright locomotion, STS training, and rehabilitation.
ER  - 

TY  - CONF
TI  - Development of Master-slave Type Lower Limb Motion Teaching System
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2762
EP  - 2767
AU  - T. Tagami
AU  - T. Kawase
AU  - D. Morisaki
AU  - R. Miyazaki
AU  - T. Miyazaki
AU  - T. Kanno
AU  - K. Kawashima
PY  - 2018
KW  - control engineering computing
KW  - electromyography
KW  - medical robotics
KW  - motion control
KW  - pneumatic actuators
KW  - recurrent neural nets
KW  - robot vision
KW  - teaching
KW  - pneumatic artificial rubber muscle
KW  - PARM
KW  - assistive force
KW  - hip joint motion
KW  - master-slave type lower limb motion teaching system
KW  - motor skill learning
KW  - physical activities
KW  - teachers motion
KW  - recurrent neural network
KW  - electromyogram signals
KW  - learners motion
KW  - Hip
KW  - Education
KW  - Visualization
KW  - Force
KW  - Neural networks
KW  - Delays
KW  - Belts
DO  - 10.1109/IROS.2018.8593737
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Motor skill learning is fundamental in many physical activities of human. In the processes of learning of motor skills, learners often receive visual or physical information about postures from teachers. However, the information about postures usually cannot be transmitted precisely. In this paper, we propose a motion teaching system to transmit teachers' motion to learners directly by using a motion capture and an assist suit. The assist suit, which has a pneumatic artificial rubber muscle (PARM) as an actuator, was designed to move a learner's hip joint with less loss of assistive force and less constraint of motion. Hip joint motion of a teacher can be transmitted to the assist suit by master-slave control. In addition, to compensate the delay of the PARM, posture of the teacher is predicted before the occurence by a recurrent neural network by using electromyogram signals and the past joint angle. We confirmed the system can transmit a teacher's motion to a learner in real time, and with the neural network, the delay of the learner's motion could be suppressed to approximately 0.1s, which is enough to feel visual and physical information synchronous. Therefore, the proposed motion teaching system would have the ability to transmit teachers' motion to learners visually and physically with precision sufficient to facilitate skill transmission.
ER  - 

TY  - CONF
TI  - Design and Experimental Characterisation of a Hydrostatic Transmission for Upper Limb Exoskeletons
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2768
EP  - 2773
AU  - M. Bolignari
AU  - G. Moretti
AU  - M. Fontana
PY  - 2018
KW  - controllability
KW  - force control
KW  - friction
KW  - hydrostatics
KW  - medical robotics
KW  - power transmission (mechanical)
KW  - torque control
KW  - wearable robots
KW  - high performance fluid power transmission
KW  - leakage-free operation
KW  - virtually zero stick-friction
KW  - intrinsic backdrivable operation
KW  - fluid transmission system
KW  - design parameters
KW  - upper limb exoskeleton
KW  - hydrostatic transmission
KW  - remote electrical actuation
KW  - hydrostatic air-liquid torque transmission system
KW  - rolling membrane cylinders
KW  - controllability
KW  - Exoskeletons
KW  - Torque
KW  - Robots
KW  - Hydraulic systems
KW  - Layout
KW  - Pulleys
KW  - Actuators
DO  - 10.1109/IROS.2018.8593639
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper introduces a novel hydrostatic air-liquid torque transmission system for an upper limb exoskeleton. The proposed design is based on remote electrical actuation, with grounded motors, combined with high performance fluid power transmission employed to deliver the power to the joints of the exoskeleton. The fluid transmission is based on rolling membrane cylinders that guarantee leakage-free operation, no backlash, and virtually zero stick-friction. This solution makes it possible to obtain easy controllability, good efficiency, intrinsic backdrivable operation, and reduced mass/inertia of the links of the robot. Additionally, the proposed system can be potentially implemented at relatively low-costs thanks to the employment of standard components and an architecture based on a modular approach. A test bench of the fluid transmission system is developed and a campaign of experiments is conducted to characterize its static/dynamic response for different choice of design parameters. In addition, we present a preliminary complete integrated arrangement of an upper limb exoskeleton equipped with the proposed transmission system. Results confirm the feasibility of the proposed actuation approach for the envisaged application.
ER  - 

TY  - CONF
TI  - Development of Tendon Driven Under-Actuated Mechanism Applied in an EMG Prosthetic Hand with Three Major Grasps for Daily Life
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2774
EP  - 2779
AU  - X. Jing
AU  - X. Yong
AU  - L. Tian
AU  - S. Togo
AU  - Y. Jiang
AU  - H. Yokoi
AU  - G. Li
PY  - 2018
KW  - actuators
KW  - biomechanics
KW  - biomimetics
KW  - electromyography
KW  - medical control systems
KW  - prosthetics
KW  - three-dimensional printing
KW  - EMG prosthetic hand
KW  - grasps
KW  - daily life
KW  - actuators
KW  - grasping tasks
KW  - weight saving
KW  - short driven distance
KW  - 3D printing technology
KW  - flexion-extension
KW  - artificial hand
KW  - tendon driven under-actuated mechanism
KW  - biomimetic prosthetic hand
KW  - thumb adduction-abduction
KW  - compact structure
KW  - motion verification
KW  - transradial amputee
KW  - Thumb
KW  - Tendons
KW  - Indexes
KW  - Grasping
KW  - Prosthetic hand
KW  - Actuators
DO  - 10.1109/IROS.2018.8593939
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a lightweight (<;250 g) and low-cost (<;350 USD) biomimetic prosthetic hand with two actuators embedded in the palm. One of them is employed for flexion/extension of the five digits, and the other one is used for the adduction/abduction of thumb. Thus, the hand can achieve major grasping tasks that account for about 85% of activities in daily life. The unique transmission provides various advantages such as a compact structure, weight saving, and short driven distance. Furthermore, by using 3D printing technology, most parts of the prosthetic hand were made to be much lighter and have a humanlike appearance, compared with conventionally manufactured artificial hand. Finally, the performance and practical applicability of the proposed design was verified experimentally through both of a motion verification and an intuitive control test by a healthy subject and a transradial amputee.
ER  - 

TY  - CONF
TI  - Muscle Activation Source Model-based sEMG Signal Decomposition and Recognition of Interface Rotation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2780
EP  - 2786
AU  - M. Kim
AU  - W. K. Chung
PY  - 2018
KW  - biomechanics
KW  - electromyography
KW  - medical signal processing
KW  - motion estimation
KW  - skin
KW  - muscle structures
KW  - muscle activation source model-based sEMG signal decomposition
KW  - sEMG interface rotation
KW  - muscle activation signals
KW  - surface electromyography signals
KW  - muscle activation extraction
KW  - hand motion estimation
KW  - rotation recognition
KW  - inertial measurement unit
KW  - Electrodes
KW  - Muscles
KW  - Mathematical model
KW  - Signal resolution
KW  - Electromyography
KW  - Conductivity
KW  - Motion estimation
DO  - 10.1109/IROS.2018.8593448
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Muscle activation signals are measured from the skin surface as surface electromyography (EMG) signals that contain information on human intentions; therefore, they are widely used in various robotics applications owing to their usability. However, selective muscle activation extraction is difficult because of the complexity of muscle structures. This study investigated muscle activation source model-based sEMG signal decomposition that considers the anatomical factors of muscle structures. The main advantage of the proposed model-based signal decomposition is that sEMG interface rotation can be recognized by comparing source parameters identified before and after rotation. To assess the performance of the proposed model-based decomposition method, hand motion estimation and rotation recognition were conducted. Additionally, two-dimensional simultaneous control was conducted with an inertial measurement unit to verify the usability of the proposed model. The results indicate that the proposed model decomposes an sEMG signal based on motion with good performance and demonstrate feasibility of motion estimation independent of sEMG interface rotation.
ER  - 

TY  - CONF
TI  - Design, Control and Preliminary Test of Robotic Ankle Prosthesis
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2787
EP  - 2793
AU  - X. Sun
AU  - F. Sugai
AU  - K. Okada
AU  - M. Inaba
PY  - 2018
KW  - actuators
KW  - artificial limbs
KW  - biomechanics
KW  - elasticity
KW  - gait analysis
KW  - medical control systems
KW  - medical robotics
KW  - prosthetics
KW  - springs (mechanical)
KW  - torque
KW  - torque control
KW  - variable transmission series elastic actuator
KW  - commercially available ankle foot prosthesis
KW  - robotic ankle prosthesis
KW  - preliminary test
KW  - variable transmission mechanism
KW  - powered plantar flexion
KW  - robotic ankle foot prosthesis
KW  - ankle joint torque-angle
KW  - ankle angle varies
KW  - variable transmission ratio
KW  - ankle foot joint
KW  - Prosthetics
KW  - Foot
KW  - Legged locomotion
KW  - Torque
KW  - Springs
DO  - 10.1109/IROS.2018.8594498
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Currently, most of commercially available ankle foot prosthesis are passive, which don't exhibit appropriate biomechanics during walking and could not adapt to dynamic property of able-bodied walking. In this paper, we present a novel robotic ankle foot prosthesis with variable transmission series elastic actuator (SEA). Slider crank mechanism is applied to transform linear motion of series elastic actuator to rotary motion of ankle foot joint. And this could contribute to variable transmission ratio while ankle angle varies. Because of variable transmission ratio, ankle joint torque is increasing while ankle angle is flexed from plantar flexion to dorsiflexion, whose feature has similar increase trend with human's ankle joint torque-angle relationship, and exhibits an appropriate characteristic for developing robotic ankle foot prosthesis. Larger torque could be obtained in powered plantar flexion, and this indicates that variable transmission mechanism would help reduce required motor torque compared with traditional mechanism. Energy stored in springs of series elastic actuator contribute a torque to powered plantar flexion. Preliminary experiments with a transtibial amputee and a transferomal amputee have been performed to test the prototype.
ER  - 

TY  - CONF
TI  - A Method for Robot Motor Fatigue Management in Physical Interaction and Human-Robot Collaboration Tasks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2850
EP  - 2856
AU  - L. Peternel
AU  - N. Tsagarakis
AU  - A. Ajoudani
PY  - 2018
KW  - collision avoidance
KW  - fatigue
KW  - human-robot interaction
KW  - industrial accidents
KW  - industrial robots
KW  - mobile robots
KW  - motion control
KW  - robot kinematics
KW  - human co-worker
KW  - KUKA lightweight robot
KW  - human-robot collaboration tasks
KW  - software frameworks
KW  - robot motor fatigue management
KW  - robot kinematic redundancy
KW  - collaborative human-robot surface polishing
KW  - autonomous surface wiping
KW  - Cartesian task production
KW  - two-stage reaction process
KW  - robot productivity
KW  - hardware solutions
KW  - accidental collisions
KW  - human safety
KW  - Robots
KW  - Task analysis
KW  - Fatigue
KW  - Temperature measurement
KW  - Torque
KW  - Force
KW  - Collision avoidance
DO  - 10.1109/IROS.2018.8594196
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Collaborative robots are often designed with limited power and force capacity, with the aim to provide affordable solutions and ensure human safety in case of accidental collisions and impacts. If a task requires a power beyond this capacity, or is performed repeatedly over long periods, such limits may be exceeded, which can cause inevitable robot damage and contribute to the lost productivity. In such cases, where hardware solutions and improvements are not applicable, effective software frameworks can prolong robot productivity and lifetime. To this end, in this paper we propose a novel technique for the monitoring and management of robot fatigue in repetitive or high-effort task execution scenarios. The robot fatigue is estimated by the measured temperature of motors in the joints. The proposed fatigue management system is composed of two-stage reaction process that is triggered by different levels of the estimated fatigue. The first stage exploits the kinematic redundancy of robot structure in attempt to minimise the load in the specific joints that under fatigue by reconfiguration in the joint space through the null space of the Cartesian task production. If the first stage is not successful in reducing the fatigue, the second stage is activated that gradually reduces the forces of hybrid controller. At that point, the human co-worker can temporarily take over the task execution until the robot will be recovered from the excessive fatigue. To validate the proposed approach we conducted experiments on KUKA Lightweight Robot performing two interaction tasks: autonomous surface wiping and collaborative human-robot surface polishing.
ER  - 

TY  - CONF
TI  - Adaptive Task Planner for Performing Home Service Tasks in Cooperation with a Human
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2857
EP  - 2864
AU  - S. Lee
AU  - J. Park
AU  - D. Kim
AU  - J. Kim
PY  - 2018
KW  - humanoid robots
KW  - human-robot interaction
KW  - image colour analysis
KW  - image sensors
KW  - path planning
KW  - robot vision
KW  - sequence network
KW  - episodic memory
KW  - user behaviors
KW  - task scheduler schedules
KW  - executable behavior
KW  - alternative behavior sequence
KW  - failed behavior problem
KW  - wheel-based humanoid robot
KW  - adaptive task planner
KW  - home service task
KW  - temporal sequence
KW  - fast forward planner
KW  - sequence to sequence network
KW  - Task analysis
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Planning
KW  - Generators
KW  - Thermal sensors
DO  - 10.1109/IROS.2018.8594040
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - To perform a home service task through cooperation with a human in a real environment, a robot needs to deal with the environmental changes and accordingly plan appropriate behavior sequence. For this purpose, in this paper, we propose an adaptive task planner which is based on memory and reasoning. A robot perceives user behaviors and objects using an RGB-depth and thermal sensor. The robot stores a temporal sequence of behaviors for performing a task in its episodic memory that is realized by a sequence to sequence network. When the user command is given, the episodic memory is used to retrieve the behavior sequence to carry out the command. On the other hand, when the robot perceives user behaviors, the robot postpones its behavior till his/her behavior is stopped. Once stopped, the episodic memory retrieves the behavior sequence to conduct a task that the user has intended. A task scheduler schedules the behavior sequence from the memory and sends it to an internal simulator. The internal simulator confirms the behavior sequence to be executable and then if executable, it sends the next executable behavior to the execution module. If a behavior fails in the internal simulation test, fast forward planner generates an alternative behavior sequence to resolve the failed behavior problem. The effectiveness and applicability of the proposed planner is demonstrated by a wheel-based humanoid robot.
ER  - 

TY  - CONF
TI  - Design of SUPERball v2, a Compliant Tensegrity Robot for Absorbing Large Impacts
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2865
EP  - 2871
AU  - M. Vespignani
AU  - J. M. Friesen
AU  - V. SunSpiral
AU  - J. Bruce
PY  - 2018
KW  - actuators
KW  - aerospace robotics
KW  - cables (mechanical)
KW  - design engineering
KW  - impact (mechanical)
KW  - mobile robots
KW  - planetary rovers
KW  - torque control
KW  - velocity control
KW  - SUPERball v2
KW  - spherical six-bar tensegrity robot
KW  - fully actuated six-bar design
KW  - compliant nylon cables
KW  - torque-control enabled motor
KW  - robust mechanical structure
KW  - system design
KW  - compliant tensegrity robot
KW  - impact velocities
KW  - 24 actuators
KW  - actuation
KW  - high-speed landings
KW  - six-bar tensegrity robot
KW  - Robot kinematics
KW  - Springs
KW  - Meters
KW  - NASA
KW  - Mechanical cables
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8594374
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we present the system design and initial testing of SUPERball v2, a completely re-designed 2-meter spherical six-bar tensegrity robot designed to survive high-speed landings as well as locomote to desired locations. SUPERball v2 was designed to enable a host of new actuation and experimentation. The prototype features a fully actuated six-bar design (24 actuators), compliant nylon cables (up to 15% stretch), torque-control enabled motors, and a robust mechanical structure capable of surviving impact velocities upwards of 8 m/s.
ER  - 

TY  - CONF
TI  - Slip Avoidance in Dual-Arm Manipulation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2872
EP  - 2879
AU  - D. S. Carabis
AU  - J. T. Wen
PY  - 2018
KW  - aerospace robotics
KW  - force control
KW  - friction
KW  - manipulator dynamics
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - position control
KW  - stability
KW  - slip avoidance
KW  - dual-arm manipulation
KW  - multifinger
KW  - multiarm grasping
KW  - friction contacts
KW  - contact slippage
KW  - space robotics
KW  - stable grasp
KW  - static conditions
KW  - grasp stability
KW  - safe force closure condition
KW  - specified motion trajectory
KW  - estimated inertial force
KW  - required squeeze force
KW  - inertial force component
KW  - motion-induced disturbance force
KW  - slip prevention strategy
KW  - dual-arm transportation
KW  - dual-arm robot
KW  - dynamic squeeze adjustment
KW  - robot-load motion
KW  - Force
KW  - Manipulators
KW  - Satellites
KW  - Grasping
KW  - Collision avoidance
KW  - Friction
DO  - 10.1109/IROS.2018.8593377
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In multi-finger or multi-arm grasping with friction contacts, maintaining force closure during motion is critical. Violation of this condition would cause contact slippage and possibly loss of grasp. This issue is of particular importance in space robotics, where the loss of grasp could lead to catastrophic consequences. There has been ample literature on stable grasp and force closure under static conditions. This paper investigates multi-arm grasping during motion, where the inertial force from the load could adversely affect grasp stability. Our approach dynamically adjusts the squeeze force and commanded robot/load motion to maintain a safe force closure condition. For a specified motion trajectory, the squeeze force is updated to prevent slippage based on the estimated inertial force. When the required squeeze force is beyond what the manipulators can safely apply, the trajectory will be scaled to reduce the inertial force component. In addition to motion-induced disturbance force, contact between the load and other objects in the environment can also cause slippage. The slip prevention strategy is extended to this case as well. The application scenario is based on the dual-arm transportation and berthing of a load in a micro-gravity environment. For laboratory testing, we use a fixed-base dual-arm robot to grasp, transport, and berth an object on a planar air bearing table. We also extend the transportation tests to a more general spatial setting, and use the dynamic squeeze adjustment to grasp, lift, and transport an object. Experimental results show the proposed method is effective at avoiding contact slippage during motion and when the object is in contact with the environment.
ER  - 

TY  - CONF
TI  - Relative and inertial attitude determination in three-vehicle long formations
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2880
EP  - 2885
AU  - P. Cruz
AU  - P. Batista
PY  - 2018
KW  - attitude control
KW  - attitude measurement
KW  - inertial navigation
KW  - mobile robots
KW  - multi-robot systems
KW  - inertial attitude
KW  - three-vehicle long formations
KW  - attitude determination problem
KW  - three-vehicle formation
KW  - independent inertial measurement
KW  - attitude relations
KW  - relative attitude
KW  - inertial candidates
KW  - constrained formations
KW  - sensor noise
KW  - Position measurement
KW  - Sensors
KW  - Extraterrestrial measurements
KW  - Visualization
KW  - Estimation
KW  - Space vehicles
KW  - Navigation
DO  - 10.1109/IROS.2018.8593763
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper addresses a new attitude determination problem for formations. It considers a three-vehicle formation with relative and inertial measurements from sensors, where Constraints limit the relative measurements, which are not available between two of the vehicles, also known as deputies. The other vehicle is called the chief and does not have any limitation. Furthermore, each of the vehicles has an independent inertial measurement, whose references are known. The goal is to determine all attitude relations, both inertial and relative. The solution for this problem is divided into different stages. First, the relative attitude between the chief and the deputies is assessed, which results in two candidates for each of these relations. Then, each candidate yields a candidate for the inertial attitude of the chief. Next, comparing the four inertial candidates gives the solution for their respective relations and consequently for the relative relations as well. The remaining relations derive directly from those already known. The paper also provides some early insights about degeneracies, possible particular cases of the solution, and the effect of sensor noise. Finally, the solution is validated with a simulation, whose results are similar to attitude determination problems in constrained formations.
ER  - 

TY  - CONF
TI  - Steerable Locomotion Controller for Six-strut Icosahedral Tensegrity Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2886
EP  - 2892
AU  - M. Vespignani
AU  - C. Ercolani
AU  - J. M. Friesen
AU  - J. Bruce
PY  - 2018
KW  - accelerometers
KW  - mobile robots
KW  - motion control
KW  - nonlinear dynamical systems
KW  - robust control
KW  - steerable locomotion controller
KW  - nonlinear dynamics
KW  - six-strut icosahedral tensegrity robots
KW  - step-wise locomotion
KW  - SUPERball v2 robot
KW  - preexisting step-wise controller
KW  - tensegrity structure
KW  - locomotion problem
KW  - step-wise motion controllers
KW  - Robot kinematics
KW  - NASA
KW  - Trajectory
KW  - Face
KW  - Robot sensing systems
KW  - Navigation
DO  - 10.1109/IROS.2018.8593676
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper proposes a novel steerable locomotion controller for six-strut tensegrity robots. Tensegrity robots are lightweight and have many promising features such as robustness, shape-shifting capabilities, and deployability, making them good candidates for exploration and scouting of remote areas. Despite these advantages, tensegrity robots are challenging to control due to their large number of degrees of freedom, nonlinear dynamics, and intrinsic compliance. Recently, many step-wise motion controllers have been employed to simplify the locomotion problem, thanks to the discrete nature of the tensegrity structure. In this paper we present a novel locomotion controller which will steer the direction of motion of a six-strut tensegrity robot when used in conjunction with any preexisting step-wise controller. We validated our controller on the SUPERball v2 robot, showing straight and curved trajectories, and an example of navigation around obstacles. Our method is computationally inexpensive, only requires knowledge about the current base triangle (e.g, via accelerometer data), and can be generalized to any six-strut tensegrity robot which can perform step-wise locomotion.
ER  - 

TY  - CONF
TI  - Series Elastic Tether Management for Rappelling Rovers
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2893
EP  - 2900
AU  - T. Brown
AU  - A. Stefanini
AU  - J. Sawoniewicz
AU  - I. Nesnas
AU  - N. Georgiev
PY  - 2018
KW  - actuators
KW  - closed loop systems
KW  - design engineering
KW  - elasticity
KW  - force control
KW  - mobile robots
KW  - position control
KW  - robot kinematics
KW  - wheels
KW  - series elastic tether management
KW  - Axel rappelling rover
KW  - intriguing science sites
KW  - important science sites
KW  - difficult terrains
KW  - conventional rovers
KW  - extended autonomous rappelling
KW  - tether spooling
KW  - shock tolerance
KW  - first-generation tether management system
KW  - double bull-wheel capstan
KW  - low-stiffness series elastic actuator
KW  - SEA
KW  - decouple internal spooling tension
KW  - external tether tension
KW  - closed-loop tether tension control
KW  - rappelling system
KW  - constant spooling tension
KW  - measured output tension
KW  - tension contribution
KW  - shock-drop tolerance
KW  - Springs
KW  - Bandwidth
KW  - Sea measurements
KW  - Actuators
KW  - Electric shock
KW  - Robots
KW  - Friction
DO  - 10.1109/IROS.2018.8594134
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The Axel rappelling rover was designed to enable access to intriguing and important science sites that lie in difficult terrains that are inaccessible to conventional rovers. Extended autonomous rappelling calls for careful control of tether tension, precise management of tether spooling, and some measure of shock tolerance. This paper covers the design and testing of a first-generation tether management system (TMS) for Axel. The system uses a double bull-wheel capstan driven by a low-stiffness series elastic actuator (SEA) to provide tension control and decouple internal spooling tension from external tether tension. A series elastic actuator was chosen for this application to permit closed-loop tether tension control and to provide shock/drop tolerance of the rappelling system both while moving and when the system is inactive with the motors locked. Experiments on the new TMS show that this design performs well in keeping nearly constant spooling tension while rejecting large dynamic disturbances at the output. While the SEA is very effective at maintaining a given tension contribution, the additional effects of friction and the unique mechanical properties of the tether result in substantial errors in the measured output tension. Upcoming field trials will be used to evaluate the effectiveness and sufficiency of this system when integrated in Axel.
ER  - 

TY  - CONF
TI  - Image Based Visual Servoing for Tumbling Objects
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2901
EP  - 2908
AU  - P. Mithun
AU  - H. Pandya
AU  - A. Gaud
AU  - S. V. Shah
AU  - K. M. Krishna
PY  - 2018
KW  - feature extraction
KW  - image reconstruction
KW  - mobile robots
KW  - pose estimation
KW  - robot vision
KW  - visual servoing
KW  - image based visual servoing
KW  - image plane
KW  - elliptical track
KW  - feature points
KW  - image space
KW  - feature error
KW  - explicit reconstruction
KW  - uncooperative tumbling object
KW  - robotic system
KW  - inertial axis
KW  - tumbling motion
KW  - Visual servoing
KW  - Cameras
KW  - Feature extraction
KW  - Estimation
KW  - Solid modeling
KW  - Satellites
DO  - 10.1109/IROS.2018.8594176
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Objects in space often exhibit a tumbling motion around the major inertial axis. In this paper, we address the image based visual servoing of a robotic system towards an uncooperative tumbling object. In contrast to previous approaches that require explicit reconstruction of the object and an estimation of its velocity, we propose a novel controller that is able to minimize the feature error directly in image space. This is achieved by observing that the feature points on the tumbling object follow a circular path around the axis of rotation and their projection creates an elliptical track in the image plane. Our controller minimizes the error between this elliptical track and the desired features, such that at the desired pose the features lie on the circumference of the ellipse. The effectiveness of our framework is exhibited by implementing the algorithm in simulation as well on a mobile robot.
ER  - 

TY  - CONF
TI  - Online Path Planning and Compliance Control of Space Robot for Capturing Tumbling Large Object
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2909
EP  - 2916
AU  - D. Hirano
AU  - H. Kato
AU  - T. Saito
PY  - 2018
KW  - aerospace control
KW  - aerospace robotics
KW  - compliance control
KW  - end effectors
KW  - force feedback
KW  - image capture
KW  - manipulator dynamics
KW  - mobile robots
KW  - motion control
KW  - object recognition
KW  - path planning
KW  - position control
KW  - robot vision
KW  - robust control
KW  - compliance control
KW  - planned trajectory
KW  - moving grasping point
KW  - end-effector position error
KW  - end-effector motion
KW  - coordinated control
KW  - spacecraft base
KW  - robotic arm
KW  - online path planning
KW  - space robot
KW  - coordinated motion control
KW  - robust control scheme
KW  - end-effector trajectory
KW  - tumbling large object capturing
KW  - Grasping
KW  - End effectors
KW  - Robot kinematics
KW  - Trajectory
DO  - 10.1109/IROS.2018.8594099
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents the path planning and coordinated control of a space robot with a manipulator for capturing a rotating large object. As the grasping point on a rotating large object is translationally moving fast, an appropriate strategy and coordinated motion control of the spacecraft base and robotic arm must be employed for approaching and tracking such a grasping point. In this paper, we propose a robust control scheme including the online path planning and compliance control for grasping such a target. The path planning is derived in a simple form that allows the desired end-effector trajectory to be easily modified in real-time using the newly updated states without complex numerical calculation. In addition, the compliance control allows the end-effector to track the planned trajectory or the moving grasping point, while using contact force feedback to reduce the end-effector position error from the grasping point when capturing the target. This end-effector motion is implemented by coordinated control on the spacecraft base and robotic arm, which can suitably alter their distribution of motion according to scenes using a weighted pseudoinverse matrix. Experiments are conducted to demonstrate the validity of the proposed path planning and compliance control.
ER  - 

TY  - CONF
TI  - Workspace Aware Online Grasp Planning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2917
EP  - 2924
AU  - I. Akinola
AU  - J. Varley
AU  - B. Chen
AU  - P. K. Allen
PY  - 2018
KW  - end effectors
KW  - path planning
KW  - reachable end-effector configurations
KW  - unique end-effector poses
KW  - workspace aware online grasp planning
KW  - reachable grasps
KW  - Planning
KW  - Robots
KW  - Trajectory
KW  - Measurement
KW  - Kinematics
KW  - Grasping
KW  - Databases
DO  - 10.1109/IROS.2018.8593644
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This work provides a framework for a workspace aware online grasp planner. This framework greatly improves the performance of standard online grasp planning algorithms by incorporating a notion of reachability into the online grasp planning process. Offline, a database of hundreds of thousands of unique end-effector poses were queried for feasibility. At runtime, our grasp planner uses this database to bias the hand towards reachable end-effector configurations. The bias keeps the grasp planner in accessible regions of the planning scene so that the resulting grasps are tailored to the situation at hand. This results in a higher percentage of reachable grasps, a higher percentage of successful grasp executions, and a reduced planning time. We also present experimental results using simulated and real environments.
ER  - 

TY  - CONF
TI  - Robotic Grasping Using Proximity Sensors for Detecting both Target Object and Support Surface
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2925
EP  - 2932
AU  - K. Sasaki
AU  - K. Koyama
AU  - A. Ming
AU  - M. Shimojo
AU  - R. Plateaux
AU  - J. Choley
PY  - 2018
KW  - biomechanics
KW  - dexterous manipulators
KW  - force control
KW  - grippers
KW  - manipulators
KW  - position control
KW  - robot vision
KW  - tactile sensors
KW  - support surface
KW  - target object
KW  - positioning
KW  - posturing
KW  - robotic grasping
KW  - proximity sensors
KW  - adequate relative posture
KW  - Grasping
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Visualization
KW  - Optical sensors
DO  - 10.1109/IROS.2018.8594430
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The robustness of the positioning and posturing of robot hands relative to target object and support surface is an important issue for autonomous grasping. For example, to perform a grasping action such as picking up thin objects from a table top, the position and posture of the hand must be controlled to keep adequate relative posture and distance to the support surface besides those between the hand and the target object. Because slight errors in the posture and position are enough to cause grasping failure, the positioning and posturing of the hand must be precise enough, specially when the hand is close to the target object and support surface. To improve the robustness of robotic grasping, in this paper we present a method by grasping control based on the relative posture and position between hand and support surface besides those between hand and target object, using proximity sensors. Proximity sensors are newly installed on fingernails besides on the fingertips. As the fingernail sensor, an integration of Time-of-Flight (TOF) sensor and photo-reflector is designed to realize long range detection, as well as with precise and high-speed detection regardless of the reflectance of support surfaces when approaching the support surface. By the sensors, the hand can approach the object and support surface coarsely first, and then can be controlled fast and precisely to realize adequate grasping motion along the support surface but without contact with the support face. The method has been implemented to a manipulator system, and successful grasping experiments have demonstrated the effectiveness of the proposed method.
ER  - 

TY  - CONF
TI  - Model-free and learning-free grasping by Local Contact Moment matching
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2933
EP  - 2940
AU  - M. Adjigble
AU  - N. Marturi
AU  - V. Ortenzi
AU  - V. Rajasekaran
AU  - P. Corke
AU  - R. Stolkin
PY  - 2018
KW  - dexterous manipulators
KW  - grippers
KW  - image matching
KW  - learning (artificial intelligence)
KW  - path planning
KW  - robot vision
KW  - local contact moment matching
KW  - LoCoMo metric
KW  - grasp planners
KW  - learning-based approaches
KW  - prototype grasp configurations
KW  - robust contacts
KW  - fingertip contacts
KW  - physical parameters
KW  - force-closure analysis
KW  - object surface patches
KW  - zero-moment shift features
KW  - learning-free grasping
KW  - Grasping
KW  - Robots
KW  - Measurement
KW  - Grippers
KW  - Shape
KW  - Three-dimensional displays
KW  - Training data
DO  - 10.1109/IROS.2018.8594226
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper addresses the problem of grasping arbitrarily shaped objects, observed as partial point-clouds, without requiring: models of the objects, physics parameters, training data, or other a-priori knowledge. A grasp metric is proposed based on Local Contact Moment (LoCoMo). LoCoMo combines zero-moment shift features, of both hand and object surface patches, to determine local similarity. This metric is then used to search for a set of feasible grasp poses with associated grasp likelihoods. LoCoMo overcomes some limitations of both classical grasp planners and learning-based approaches. Unlike force-closure analysis, LoCoMo does not require knowledge of physical parameters such as friction coefficients, and avoids assumptions about fingertip contacts, instead enabling robust contacts of large areas of hand and object surface. Unlike more recent learning-based approaches, LoCoMo does not require training data, and does not need any prototype grasp configurations to be taught by kinesthetic demonstration. We present results of real-robot experiments grasping 21 different objects, observed by a wrist-mounted depth camera. All objects are grasped successfully when presented to the robot individually. The robot also successfully clears cluttered heaps of objects by sequentially grasping and lifting objects until none remain.
ER  - 

TY  - CONF
TI  - A Framework for Robot Grasp Transferring with Non-rigid Transformation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2941
EP  - 2948
AU  - H. Lin
AU  - T. Tang
AU  - Y. Fan
AU  - M. Tomizuka
PY  - 2018
KW  - collision avoidance
KW  - control engineering computing
KW  - dexterous manipulators
KW  - grippers
KW  - learning (artificial intelligence)
KW  - optimisation
KW  - path planning
KW  - orientation search
KW  - collision avoidance
KW  - grasp generation
KW  - dexterous tasks execution
KW  - online planning
KW  - grasp planning
KW  - robot grasp transferring
KW  - task requirements
KW  - robot reachability
KW  - grasp robustness
KW  - nonrigid transformation
KW  - human demonstration
KW  - analytic approach
KW  - Task analysis
KW  - Robots
KW  - Grasping
KW  - Planning
KW  - Collision avoidance
KW  - Grippers
KW  - Databases
DO  - 10.1109/IROS.2018.8593668
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Grasp planning is essential for robots to execute dexterous tasks. Solving the optimal grasps for various objects online, however, is challenging due to the heavy computation load during exhaustive sampling, and the difficulties to consider task requirements. This paper proposes a framework to combine analytic approach with learning for efficient grasp generation. The example grasps are taught by human demonstration and mapped to similar objects by a non-rigid transformation. The mapped grasps are evaluated analytically and refined by an orientation search to improve the grasp robustness and robot reachability. The proposed approach is able to plan high-quality grasps, avoid collision, satisfy task requirements, and achieve efficient online planning. The effectiveness of the proposed method is verified by a series of experiments.
ER  - 

TY  - CONF
TI  - Using human studies to analyze capabilities of underactuated and compliant hands in manipulation tasks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2949
EP  - 2954
AU  - J. Morrow
AU  - A. Kothari
AU  - Y. H. Ong
AU  - N. Harlan
AU  - R. Balasubramanian
AU  - C. Grimm
PY  - 2018
KW  - actuators
KW  - manipulators
KW  - human studies
KW  - underactuated hands
KW  - compliant hands
KW  - manipulation tasks
KW  - human-subjects
KW  - manipulation performance
KW  - robotic hands
KW  - compliance
KW  - compliant distal joints
KW  - task completion
KW  - different poses
KW  - superior task performance
KW  - fully-actuated techniques
KW  - actuation
KW  - robotic systems
KW  - Task analysis
KW  - Grasping
KW  - Robots
KW  - Spraying
KW  - Shape
KW  - Potentiometers
KW  - Rubber
DO  - 10.1109/IROS.2018.8594344
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present a human-subjects study approach that supports the analysis of the manipulation performance of robotic hands that have the same morphology but different actuation and compliance. Specifically, we use this approach to analyze three different types of hands (one underactuated, one fully actuated, one fully actuated with compliant distal joints) as they are used to perform two manipulation tasks. The first task uses a power grasp (spraying with a spray bottle), the second a precision grasp (tracing a line on a bowl with a pen). We show that compliance in the distal joints significantly improves performance and task completion. We also show that humans choose significantly different poses for the same task when using a fully-actuated versus underactuated hand, which also results in superior task performance. Our results suggest that humans use a combination of under-actuated and fully-actuated techniques, which when used on robotic systems would also improve their performance on manipulation tasks.
ER  - 

TY  - CONF
TI  - Affordance Wayfields for Task and Motion Planning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2955
EP  - 2962
AU  - T. McMahon
AU  - O. C. Jenkins
AU  - N. Amato
PY  - 2018
KW  - gradient methods
KW  - manipulators
KW  - path planning
KW  - manipulation affordances
KW  - affordance wayfields
KW  - motion planning
KW  - gradient descent
KW  - Michigan Progress Fetch mobile manipulator
KW  - Planning
KW  - Task analysis
KW  - End effectors
KW  - Trajectory
KW  - Cost function
DO  - 10.1109/IROS.2018.8594492
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Affordances provide a natural means for a robot to describe its agency as actions it can perform on objects. Further, affordances can enable robots to reason complicated, multi-step tasks that involve proper use of a diversity of objects. This paper proposes the concept of affordance wayfields for representing manipulation affordances as objective functions in configuration space. Affordance wayfields quantify how well a path, or sequence of motions, will accomplish an afforded action on an object. Paths that enact affordances can be located by performing a randomized form of gradient descent over affordance wayfields. Incorporating obstacles, or other constraints into wayfields allows our method to adaptively generate valid motions for executing afforded actions. We demonstrate that affordance wayfields can enable robots, such as the Michigan Progress Fetch mobile manipulator, to solve complex real-world tasks such as assembling a table, or loading and unloading objects from a storage chest.
ER  - 

TY  - CONF
TI  - Tactile Regrasp: Grasp Adjustments via Simulated Tactile Transformations
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2963
EP  - 2970
AU  - F. R. Hogan
AU  - M. Bauza
AU  - O. Canal
AU  - E. Donlon
AU  - A. Rodriguez
PY  - 2018
KW  - convolutional neural nets
KW  - grippers
KW  - manipulators
KW  - motion control
KW  - path planning
KW  - robot vision
KW  - tactile regrasp
KW  - simulated tactile transformations
KW  - tactile sensing
KW  - regrasp action
KW  - local transformations
KW  - grasp quality metric
KW  - deep convolutional neural network
KW  - rigid-body transformations
KW  - grasp quality network
KW  - grasp actions
KW  - tactile measurements
KW  - grasp adjustments
KW  - regrasp control policy
KW  - tactile imprints
KW  - robot motions
KW  - Grasping
KW  - Measurement
KW  - Tactile sensors
KW  - Grippers
DO  - 10.1109/IROS.2018.8593528
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a novel regrasp control policy that makes use of tactile sensing to plan local grasp adjustments. Our approach determines regrasp actions by virtually searching for local transformations of tactile measurements that improve the quality of the grasp. First, we construct a tactile-based grasp quality metric using a deep convolutional neural network trained on over 2800 grasps. The quality of each grasp, a continuous value between 0 and 1, is determined experimentally by measuring its resistance to external perturbations. Second, we simulate the tactile imprints associated with robot motions relative to the initial grasp by performing rigid-body transformations of the given tactile measurements. The newly generated tactile imprints are evaluated with the learned grasp quality network and the regrasp action is chosen to maximize the grasp quality. Results show that the grasp quality network can predict the outcome of grasps with an average accuracy of 85% on known objects and 75% on novel objects. The regrasp control policy improves the success rate of grasp actions by an average relative increase of 70% on a test set of 8 objects. We provide a video summarizing our approach at https://youtu.be/gjn7DmfpwDk.
ER  - 

TY  - CONF
TI  - Adaptive Autonomous Grasp Selection via Pairwise Ranking
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2971
EP  - 2976
AU  - D. Kent
AU  - R. Toris
PY  - 2018
KW  - feature selection
KW  - manipulators
KW  - mobile robots
KW  - robot vision
KW  - object subsets
KW  - grasp selection algorithm
KW  - grasp metrics
KW  - object features
KW  - user-specified grasp preferences
KW  - pairwise ranking problem
KW  - pointwise ranking formulation
KW  - adaptive autonomous grasp selection
KW  - object databases
KW  - grasping strategies
KW  - robot pick-and-place applications
KW  - Measurement
KW  - Solid modeling
KW  - Training data
KW  - Three-dimensional displays
KW  - Databases
KW  - Training
KW  - Data models
DO  - 10.1109/IROS.2018.8594105
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Autonomous grasp selection for robot pick-and-place applications makes use of either empirical methods leveraging object databases, which generate grasps for specific objects at the initial cost of modeling effort, or analytical methods, which generalize to novel objects but fail on object subsets that require specific grasping strategies not captured by the algorithm. We introduce a grasp selection algorithm that ranks grasp candidates with a set of grasp metrics augmented with object features, creating an approach that adapts its strategies based on user-specified grasp preferences. We formulate grasp selection as a pairwise ranking problem, which significantly reduces data collection compared to traditional grasp ranking methods and generalizes to novel objects. Our approach outperforms a state-of-the-art grasp calculation baseline and a pointwise ranking formulation of the same problem.
ER  - 

TY  - CONF
TI  - Experience-Based Model Selection to Enable Long-Term, Safe Control for Repetitive Tasks Under Changing Conditions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2977
EP  - 2984
AU  - C. D. McKinnon
AU  - A. P. Schoellig
PY  - 2018
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mobile robots
KW  - regression analysis
KW  - robot dynamics
KW  - learning approaches
KW  - significant performance improvements
KW  - robotic control
KW  - realistic scenarios
KW  - rapid changes
KW  - existing single-mode safe learning controller
KW  - increasing number
KW  - nonlinear models
KW  - robot dynamics
KW  - visited operating conditions
KW  - new operating condition
KW  - distinct operating condition
KW  - control loop
KW  - physical changes
KW  - artificial changes
KW  - experience-based model selection
KW  - enable long-term
KW  - safe control
KW  - repetitive tasks
KW  - Gaussian process regression
KW  - Robots
KW  - Vehicle dynamics
KW  - Safety
KW  - Heuristic algorithms
KW  - Data models
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/IROS.2018.8593882
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Learning approaches have enabled significant performance improvements in robotic control allowing robots to execute motions that were previously impossible. The majority of the work to date, however, assumes that the parts to be learned are static or slowly changing, which limits their applicability in realistic scenarios with rapid changes in the conditions. This paper presents a method to extend an existing single-mode safe learning controller based on Gaussian Process Regression to learn an increasing number of non-linear models for the robot dynamics. We show that this approach enables a robot to re-use past experiences from a large number of previously visited operating conditions, and to safely adapt when a new and distinct operating condition is encountered. This allows the robot to achieve safety and high performance in a large number of operating conditions that do not have to be specified ahead of time. Our approach runs independently from the controller, imposing no additional computation time on the control loop regardless of the number of previous operating conditions considered. We demonstrate the effectiveness of our approach in experiment on a 900 kg ground robot with both physical and artificial changes to its dynamics. All of our experiments are conducted using vision for localization.
ER  - 

TY  - CONF
TI  - Efficient Model Identification for Tensegrity Locomotion
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2985
EP  - 2990
AU  - S. Zhu
AU  - D. Surovik
AU  - K. Bekris
AU  - A. Boularias
PY  - 2018
KW  - Bayes methods
KW  - legged locomotion
KW  - optimisation
KW  - robot dynamics
KW  - Tensegrity locomotion
KW  - mechanical models
KW  - actuated robot links
KW  - dynamical robotic tasks
KW  - Bayesian optimization framework
KW  - high-dimensional Tensegrity robot
KW  - compliant Tensegrity robot
KW  - precise locomotion control
KW  - model identification
KW  - unknown physical parameters
KW  - physics engine
KW  - Robots
KW  - Engines
KW  - Optimization
KW  - Physics
KW  - Predictive models
KW  - Dimensionality reduction
KW  - Task analysis
DO  - 10.1109/IROS.2018.8594425
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper aims to identify in a practical manner unknown physical parameters, such as mechanical models of actuated robot links, which are critical in dynamical robotic tasks. Key features include the use of an off-the-shelf physics engine and the Bayesian optimization framework. The task being considered is locomotion with a high-dimensional, compliant Tensegrity robot. A key insight, in this case, is the need to project the space of models into an appropriate lower dimensional space for time efficiency. Comparisons with alternatives indicate that the proposed method can identify the parameters more accurately within the given time budget, which also results in more precise locomotion control.
ER  - 

TY  - CONF
TI  - Robot-driven Trajectory Improvement for Feeding Tasks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2991
EP  - 2996
AU  - T. Rhodes
AU  - M. Veloso
PY  - 2018
KW  - assisted living
KW  - handicapped aids
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - mobile robots
KW  - path planning
KW  - search problems
KW  - trajectory control
KW  - robotic joints
KW  - kinesthetic learning
KW  - active learning
KW  - robot-driven trajectory improvement
KW  - assistive robotics
KW  - parameterized similar path search algorithm
KW  - PSPS
KW  - feeding tasks
KW  - computer programming
KW  - Trajectory
KW  - Task analysis
KW  - Robot kinematics
KW  - Cost function
KW  - Training
KW  - Manipulators
DO  - 10.1109/IROS.2018.8593525
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Kinesthetic learning is a type of learning from demonstration in which the teacher manually moves the robot through the demonstrated trajectory. It shows great promise in the area of assistive robotics since it enables a caretaker who is not an expert in computer programming to communicate a novel task to an assistive robot. However, the trajectory the caretaker demonstrates to solve the task may be a high-cost trajectory for the robot. The demonstrated trajectory could be high-cost because the teacher does not know what trajectories are easy or hard for the robot to perform, which would be due to a limitation of the teacher's knowledge, or because the teacher has difficulty moving all the robotic joints precisely along the desired trajectories, which would be due to a limitation of the teacher's coordination. We propose the Parameterized Similar Path Search (PSPS) algorithm to extend kinesthetic learning so that a robot can improve the learned trajectory over a known cost function. This algorithm is based on active learning from the robot through collaboration between the robot's knowledge of the cost function and the caretaker's knowledge of the constraints of the assigned task.
ER  - 

TY  - CONF
TI  - Accelerating Learning in Constructive Predictive Frameworks with the Successor Representation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 2997
EP  - 3003
AU  - C. Sherstan
AU  - M. C. Machado
AU  - P. M. Pilarski
PY  - 2018
KW  - computer aided instruction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot programming
KW  - unstructured environments
KW  - dynamic environments
KW  - reinforcement learning
KW  - predictive questions
KW  - massive network
KW  - interconnected GVFs
KW  - interdependent GVFs
KW  - SR
KW  - continual learning
KW  - physical robot arm
KW  - constructive predictive frameworks
KW  - constructive knowledge system
KW  - general value functions
KW  - successor representation
KW  - accelerated learning
KW  - Robots
KW  - Prediction algorithms
KW  - Function approximation
KW  - Acceleration
KW  - Approximation algorithms
KW  - Standards
KW  - Adaptation models
DO  - 10.1109/IROS.2018.8594242
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose using the Successor Representation (SR) to accelerate learning in a constructive knowledge system based on General Value Functions (GVFs). In real-world settings, like robotics for unstructured and dynamic environments, it is impossible to model all meaningful aspects of a system and its environment by hand. Instead, robots must learn and adapt to changes in their environment and task, incrementally constructing models from their own experience. GVFs, taken from the field of reinforcement learning (RL), are a way of modeling the world as predictive questions. One approach to such models proposes a massive network of interconnected and interdependent GVFs, which are incrementally added over time. It is reasonable to expect that new, incrementally added predictions can be learned more swiftly if the learning process leverages knowledge gained from past experience. The SR provides a means of capturing regularities that can be reused across multiple GVFs by separating the dynamics of the world from the prediction targets. As a primary contribution of this work, we show that using the SR can improve sample efficiency and learning speed of GVFs in a continual learning setting where new predictions are incrementally added and learned over time. We analyze our approach in a grid-world and then demonstrate its potential on data from a physical robot arm.
ER  - 

TY  - CONF
TI  - Reinforcement Learning with Symbolic Input-Output Models
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3004
EP  - 3009
AU  - E. Derner
AU  - J. Kubalík
AU  - R. Babuška
PY  - 2018
KW  - autoregressive processes
KW  - learning systems
KW  - optimal control
KW  - regression analysis
KW  - state-space methods
KW  - reinforcement learning
KW  - symbolic input-output models
KW  - dynamic prediction model
KW  - RL algorithms
KW  - nonlinear autoregressive with exogenous input
KW  - symbolic regression
KW  - parsimonious models
KW  - symbolic input-output process model
KW  - state-space models
KW  - Robots
KW  - Springs
KW  - Data models
KW  - Computational modeling
KW  - Optimal control
KW  - Reinforcement learning
KW  - Process control
KW  - Model learning
KW  - symbolic regression
KW  - reinforcement learning
KW  - optimal control
DO  - 10.1109/IROS.2018.8593881
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - It is well known that reinforcement learning (RL) can benefit from the use of a dynamic prediction model which is learned on data samples collected online from the process to be controlled. Most RL algorithms are formulated in the state-space domain and use state-space models. However, learning state-space models is difficult, mainly because in the vast majority of problems the full state cannot be measured on the system or reconstructed from the measurements. To circumvent this limitation, we propose to use input-output models of the NARX (nonlinear autoregressive with exogenous input) type. Symbolic regression is employed to construct parsimonious models and the corresponding value functions. Thanks to this approach, we can learn accurate models and compute optimal policies even from small amounts of training data. We demonstrate the approach on two simulated examples, a hopping robot and a 1-DOF robot arm, and on a real inverted pendulum system. Results show that our proposed method can reliably determine a good control policy based on a symbolic input-output process model and value function.
ER  - 

TY  - CONF
TI  - A Framework for Teaching Impedance Behaviours by Combining Human and Robot ‘Best Practice’
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3010
EP  - 3015
AU  - Y. Zhao
AU  - A. Sena
AU  - F. Wu
AU  - M. J. Howard
PY  - 2018
KW  - control engineering education
KW  - educational robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - muscle
KW  - robot dynamics
KW  - robot programming
KW  - human robot best practice
KW  - physical robot
KW  - teaching impedance
KW  - impedance modulation
KW  - human demonstrations
KW  - human stiffness
KW  - damping
KW  - muscle level
KW  - task demands
KW  - robotic systems
KW  - task critical component
KW  - robot-specific controller
KW  - variable impedance profile
KW  - Impedance
KW  - Task analysis
KW  - Damping
KW  - Modulation
KW  - Robot kinematics
KW  - Covariance matrices
DO  - 10.1109/IROS.2018.8593502
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a programming by demonstration framework for teaching impedance modulation using human demonstrations. Physiologically, human stiffness and damping are coupled at the muscle level, restricting the ability to modulate impedance according to task demands. Robotic systems often do not have this restriction (stiffness and damping can be varied independently), but the challenge is to devise an appropriate variable impedance profile for a given task. In this paper, the task critical component is first learned for imitation and a robot-specific controller is then blended into the control using the null space. In doing so, the control cheme takes advantage of both human and robot `best practice'. Experimental results on a physical robot suggest an order of magnitude better mean performance, with lower variance, can be achieved using the blended scheme.
ER  - 

TY  - CONF
TI  - Automated Tuning of Nonlinear Model Predictive Controller by Reinforcement Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3016
EP  - 3021
AU  - M. Mehndiratta
AU  - E. Camci
AU  - E. Kayacan
PY  - 2018
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - nonlinear control systems
KW  - predictive control
KW  - trajectory control
KW  - nonlinear MPC
KW  - trajectory tracking control
KW  - aerial robots
KW  - NMPC weights
KW  - automated tuning
KW  - nonlinear model predictive controller
KW  - reinforcement learning
KW  - nontrivial weight tuning process
KW  - generic user-independent framework
KW  - trial-and-error method
KW  - iterative Gazebo simulations
KW  - standard desktop computer
KW  - Tuning
KW  - Rotors
KW  - Computational modeling
KW  - Optimization
KW  - Aerodynamics
KW  - Iron
KW  - Reinforcement learning
DO  - 10.1109/IROS.2018.8594350
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - One of the major challenges of model predictive control (MPC) for robotic applications is the non-trivial weight tuning process while crafting the objective function. This process is often executed using the trial-and-error method by the user. Consequently, the optimality of the weights and the time required for the process become highly dependent on the skill set and experience of the user. In this study, we present a generic and user-independent framework which automates the tuning process by reinforcement learning. The proposed method shows competency in tuning a nonlinear MPC (NMPC) which is employed for trajectory tracking control of aerial robots. It explores the desirable weights within less than an hour in iterative Gazebo simulations running on a standard desktop computer. The real world experiments illustrate that the NMPC weights explored by the proposed method result in a satisfactory trajectory tracking performance.
ER  - 

TY  - CONF
TI  - Soft-obstacle Avoidance for Redundant Manipulators with Recurrent Neural Network
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3022
EP  - 3027
AU  - Y. Li
AU  - B. Hannaford
PY  - 2018
KW  - biological tissues
KW  - collision avoidance
KW  - medical robotics
KW  - motion control
KW  - optimisation
KW  - recurrent neural nets
KW  - redundant manipulators
KW  - surgery
KW  - telerobotics
KW  - surgical trauma
KW  - safety motion constraints
KW  - soft-obstacle avoidance problem
KW  - redundant manipulators
KW  - recurrent neural network
KW  - human beings
KW  - teleoperated robots
KW  - robotic autonomy
KW  - soft tissues
KW  - surgical safety
KW  - robotic surgery
KW  - optimization problem
KW  - minimally invasive surgeries
KW  - Raven-II surgical robot
KW  - RNNs
KW  - Surgery
KW  - Manipulators
KW  - Task analysis
KW  - Collision avoidance
KW  - Recurrent neural networks
KW  - Optimization
KW  - Soft Obstacle Avoidance
KW  - Autonomous Robotic Surgery
KW  - Robot Arm
KW  - Recurrent Neural Network
DO  - 10.1109/IROS.2018.8594346
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Compressing soft-obstacles secondary to a controlled motion task is common for human beings. While these tasks are nearly trivial for teleoperated robots, they remain a challenging problem in robotic autonomy. Addressing the problem is significant. For example, in Minimally Invasive Surgeries (MISs), safely compressing soft tissues ensures the surgical safety and decreases tissue removal, thus dramatically decreases surgical trauma and operating room time, and leads to improved surgical outcomes. In this work, we define the problem of soft-obstacle avoidance and project the safety motion constraints into the task space and the velocity space. We illustrate the significance of addressing this problem in the robotic surgery scenario. We present a Recurrent Neural Networks (RNNs) based solution, which formulates the problem as an inequality constrained optimization problem and solves it in its dual space. The application of the proposed method was demonstrated in the Raven II surgical robot. Experimental results demonstrated that the proposed method is effective in addressing the soft-obstacle avoidance problem.
ER  - 

TY  - CONF
TI  - GONet: A Semi-Supervised Deep Learning Approach For Traversability Estimation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3044
EP  - 3051
AU  - N. Hirose
AU  - A. Sadeghian
AU  - M. Vázquez
AU  - P. Goebel
AU  - S. Savarese
PY  - 2018
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - stereo image processing
KW  - traversable places
KW  - traversable spaces
KW  - traversability estimation approaches
KW  - GONet
KW  - semisupervised deep learning approach
KW  - fisheye images
KW  - generative adversarial networks
KW  - GANs
KW  - stereo fisheye cameras
KW  - time 24.0 hour
KW  - Cameras
KW  - Estimation
KW  - Robot vision systems
KW  - Generators
KW  - Training
KW  - Gallium nitride
DO  - 10.1109/IROS.2018.8594031
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present semi-supervised deep learning approaches for traversability estimation from fisheye images. Our method, GONet, and the proposed extensions leverage Generative Adversarial Networks (GANs) to effectively predict whether the area seen in the input image(s) is safe for a robot to traverse. These methods are trained with many positive images of traversable places, but just a small set of negative images depicting blocked and unsafe areas. This makes the proposed methods practical. Positive examples can be collected easily by simply operating a robot through traversable spaces, while obtaining negative examples is time consuming, costly, and potentially dangerous. Through extensive experiments and several demonstrations, we show that the proposed traversability estimation approaches are robust and can generalize to unseen scenarios. Further, we demonstrate that our methods are memory efficient and fast, allowing for real-time operation on a mobile robot with single or stereo fisheye cameras. As part of our contributions, we open-source two new datasets for traversability estimation. These datasets are composed of approximately 24h of videos from more than 25 indoor environments. Our methods outperform baseline approaches for traversability estimation on these new datasets.
ER  - 

TY  - CONF
TI  - Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3052
EP  - 3059
AU  - M. Everett
AU  - Y. F. Chen
AU  - J. P. How
PY  - 2018
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - safe operation
KW  - deep reinforcement learning
KW  - complex interactions
KW  - environment increases
KW  - dynamic agents
KW  - particular behavior rules
KW  - arbitrary number
KW  - motion planning
KW  - decision-making agents
KW  - collision avoidance algorithms
KW  - Collision avoidance
KW  - Robots
KW  - Training
KW  - Decision making
KW  - Heuristic algorithms
KW  - Sensors
KW  - Navigation
DO  - 10.1109/IROS.2018.8593871
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efficient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents' behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a fixed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed.
ER  - 

TY  - CONF
TI  - Real-Time Workload Classification during Driving using HyperNetworks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3060
EP  - 3065
AU  - R. Wang
AU  - P. V. Amadori
AU  - Y. Demiris
PY  - 2018
KW  - cognition
KW  - medical signal processing
KW  - pattern classification
KW  - recurrent neural nets
KW  - signal classification
KW  - traffic engineering computing
KW  - m-HyperLSTM
KW  - mixture hyper long short term memory networks
KW  - cognitive demands
KW  - data variability
KW  - robotics
KW  - physiological signals
KW  - behavioral signals
KW  - human cognitive states
KW  - eye-gaze pattern dataset
KW  - HyperNetworks
KW  - real-time cognitive workload classification
KW  - sensor artefacts
KW  - Adaptation models
KW  - Data models
KW  - Physiology
KW  - Task analysis
KW  - Real-time systems
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8594305
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Classifying human cognitive states from behavioral and physiological signals is a challenging problem with important applications in robotics. The problem is challenging due to the data variability among individual users, and sensor artefacts. In this work, we propose an end-to-end framework for real-time cognitive workload classification with mixture Hyper Long Short Term Memory Networks (m-HyperLSTM), a novel variant of HyperNetworks. Evaluating the proposed approach on an eye-gaze pattern dataset collected from simulated driving scenarios of different cognitive demands, we show that the proposed framework outperforms previous baseline methods and achieves 83.9% precision and 87.8% recall during test. We also demonstrate the merit of our proposed architecture by showing improved performance over other LSTM-based methods.
ER  - 

TY  - CONF
TI  - Augmenting Physical Simulators with Stochastic Neural Networks: Case Study of Planar Pushing and Bouncing
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3066
EP  - 3073
AU  - A. Ajay
AU  - J. Wu
AU  - N. Fazeli
AU  - M. Bauza
AU  - L. P. Kaelbling
AU  - J. B. Tenenbaum
AU  - A. Rodriguez
PY  - 2018
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - state estimation
KW  - robot state estimation
KW  - planar pushing
KW  - ball bouncing
KW  - analytical rigid-body simulator
KW  - model uncertainty
KW  - symbolic simulators
KW  - stochastic neural networks
KW  - generalizable physical simulator
KW  - universal uncertainty estimates
KW  - analytical learned simulators
KW  - Gaussian processes
KW  - object trajectories
KW  - Analytical models
KW  - Predictive models
KW  - Physics
KW  - Data models
KW  - Uncertainty
KW  - Engines
KW  - Neural networks
DO  - 10.1109/IROS.2018.8593995
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - An efficient, generalizable physical simulator with universal uncertainty estimates has wide applications in robot state estimation, planning, and control. In this paper, we build such a simulator for two scenarios, planar pushing and ball bouncing, by augmenting an analytical rigid-body simulator with a neural network that learns to model uncertainty as residuals. Combining symbolic, deterministic simulators with learnable, stochastic neural nets provides us with expressiveness, efficiency, and generalizability simultaneously. Our model outperforms both purely analytical and purely learned simulators consistently on real, standard benchmarks. Compared with methods that model uncertainty using Gaussian processes, our model runs much faster, generalizes better to new object shapes, and is able to characterize the complex distribution of object trajectories.
ER  - 

TY  - CONF
TI  - Learning to Pour using Deep Deterministic Policy Gradients
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3074
EP  - 3079
AU  - C. Do
AU  - C. Gordillo
AU  - W. Burgard
PY  - 2018
KW  - learning (artificial intelligence)
KW  - robots
KW  - domestic environments
KW  - industrial environments
KW  - pre-defined heights
KW  - liquid dynamics
KW  - PR2 robot
KW  - learned policy
KW  - fundamental skill
KW  - deep deterministic policy gradients
KW  - liquid simulator
KW  - Liquids
KW  - Training
KW  - Task analysis
KW  - Reinforcement learning
KW  - Service robots
KW  - Trajectory
DO  - 10.1109/IROS.2018.8593654
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Pouring is a fundamental skill for robots in both domestic and industrial environments. Ideally, a robot should be able to pour with high accuracy to specific, pre-defined heights and without spilling. However, due to the complex dynamics of liquids, it is difficult to learn how to pour to achieve these goals. In this paper we present an approach to learn a policy for pouring using Deep Deterministic Policy Gradients (DDPG). We remove the need for collecting training experiences on a real robot, by using a state-of-the-art liquid simulator, which allows for learning the liquid dynamics. We show through our experiments, performed with a PR2 robot, that it is possible to successfully transfer the learned policy to a real robot and even apply it to different liquids.
ER  - 

TY  - CONF
TI  - Learning Sample-Efficient Target Reaching for Mobile Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3080
EP  - 3087
AU  - A. Khan
AU  - V. Kumar
AU  - A. Ribeiro
PY  - 2018
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - self-supervised policy gradient algorithm
KW  - unsupervised auxiliary tasks
KW  - sparse range-finder measurements
KW  - convolutional networks
KW  - network architecture
KW  - sparse reward problem
KW  - robots uncertainty
KW  - unsupervised tasks
KW  - mobile robots
KW  - planning problem
KW  - sample-efficient target reaching learning
KW  - Task analysis
KW  - Robot sensing systems
KW  - Planning
KW  - Encoding
KW  - Uncertainty
KW  - Training
DO  - 10.1109/IROS.2018.8594168
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose a novel architecture and a self-supervised policy gradient algorithm, which employs unsupervised auxiliary tasks to enable a mobile robot to learn how to navigate to a given goal. The dependency on the global information is eliminated by providing only sparse range-finder measurements to the robot. The partially observable planning problem is addressed by splitting it into a hierarchical process. We use convolutional networks to plan locally, and a differentiable memory to provide information about past time steps in the trajectory. These modules, combined in our network architecture, produce globally consistent plans. The sparse reward problem is mitigated by our modified policy gradient algorithm. We model the robots uncertainty with unsupervised tasks to force exploration. The novel architecture we propose with the modified version of the policy gradient algorithm allows our robot to reach the goal in a sample efficient manner, which is orders of magnitude faster than the current state of the art policy gradient algorithm. Simulation and experimental results are provided to validate the proposed approach.
ER  - 

TY  - CONF
TI  - Generative Modeling of Multimodal Multi-Human Behavior
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3088
EP  - 3095
AU  - B. Ivanovic
AU  - E. Schmerling
AU  - K. Leung
AU  - M. Pavone
PY  - 2018
KW  - approximation theory
KW  - behavioural sciences computing
KW  - bin packing
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - statistical distributions
KW  - deep learning approximations
KW  - probabilistic graphical models
KW  - candidate future agent behavior
KW  - crowded environments
KW  - human-driven vehicles
KW  - human-robot collaborative bin packing
KW  - multimodal probability distribution
KW  - multihuman interactions
KW  - basketball player trajectories
KW  - multimodal multihuman behavior
KW  - self-driving cars
KW  - warehouse
KW  - autoencoders
KW  - response dynamics
KW  - robotic applications
KW  - proxy
KW  - Trajectory
KW  - Predictive models
KW  - Analytical models
KW  - Deep learning
KW  - Ground penetrating radar
KW  - Data models
KW  - Robots
DO  - 10.1109/IROS.2018.8594393
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This work presents a methodology for modeling and predicting human behavior in settings with N humans interacting in highly multimodal scenarios (i.e. where there are many possible highly-distinct futures). A motivating example includes robots interacting with humans in crowded environments, such as self-driving cars operating alongside human-driven vehicles or human-robot collaborative bin packing in a warehouse. Our approach to model human behavior in such uncertain environments is to model humans in the scene as nodes in a graphical model, with edges encoding relationships between them. For each human, we learn a multimodal probability distribution over future actions from a dataset of multi-human interactions. Learning such distributions is made possible by recent advances in the theory of conditional variational autoencoders and deep learning approximations of probabilistic graphical models. Specifically, we learn action distributions conditioned on interaction history, neighboring human behavior, and candidate future agent behavior in order to take into account response dynamics. We demonstrate the performance of such a modeling approach in modeling basketball player trajectories, a highly multimodal, multi-human scenario which serves as a proxy for many robotic applications.
ER  - 

TY  - CONF
TI  - Predicting Part Affordances of Objects Using Two-Stream Fully Convolutional Network with Multimodal Inputs
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3096
EP  - 3101
AU  - K. Chaudhary
AU  - K. Okada
AU  - M. Inaba
AU  - X. Chen
PY  - 2018
KW  - convolutional neural nets
KW  - image fusion
KW  - learning (artificial intelligence)
KW  - convolutional network
KW  - part affordances
KW  - object affordances
KW  - affordance detection network
KW  - physical properties
KW  - geometrical structures
KW  - two-stream fully convolutional network
KW  - potential affordances
KW  - multimodal encoding
KW  - geometrical properties
KW  - abstract rich photometrical properties
KW  - depth images
KW  - powerful discriminative features
KW  - decoding stream
KW  - encoding streams
KW  - RGB-D data
KW  - Feature extraction
KW  - Encoding
KW  - Task analysis
KW  - Robots
KW  - Streaming media
KW  - Decoding
KW  - Fuses
KW  - Affordance detection network (ADNet)
KW  - fully convolutional network (FCN)
DO  - 10.1109/IROS.2018.8593617
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For a robot to manipulate an object, it has to understand the functions and the actions that can be subjected to the object. This set of information is known as affordance of the object. Affordances are generally defined by the geometrical structures and physical properties of the objects. In this paper, we present an affordance detection network (ADNet) for detecting object affordances using multimodal input i.e., RGB-D data. The method is based on the state-of-the-art fully convolutional network with two encoding streams and one decoding stream. In the presented formulation, the network learns powerful discriminative features independently from the RGB and depth images, which enables it to abstract rich photometrical and geometrical properties of the objects. The multimodal encoding is combined at multiple stages of the network using the late-fusion strategy and used is for predicting the potential affordances of the objects.
ER  - 

TY  - CONF
TI  - Deep Multi-Sensor Lane Detection
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3102
EP  - 3109
AU  - M. Bai
AU  - G. Mattyus
AU  - N. Homayounfar
AU  - S. Wang
AU  - S. K. Lakshmikanth
AU  - R. Urtasun
PY  - 2018
KW  - driver information systems
KW  - image sensors
KW  - neural nets
KW  - object detection
KW  - optical radar
KW  - road traffic
KW  - multisensor lane detection
KW  - reliable lane detection
KW  - accurate lane detection
KW  - long-standing problem
KW  - autonomous driving
KW  - image space
KW  - accurate image estimates
KW  - precise 3D lane boundaries
KW  - modern motion planning algorithms
KW  - deep neural network
KW  - camera sensors
KW  - accurate estimates
KW  - LiDAR
KW  - Cameras
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Sensors
KW  - Roads
KW  - Task analysis
KW  - Reliability
DO  - 10.1109/IROS.2018.8594388
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Reliable and accurate lane detection has been a long-standing problem in the field of autonomous driving. In recent years, many approaches have been developed that use images (or videos) as input and reason in image space. In this paper we argue that accurate image estimates do not translate to precise 3D lane boundaries, which are the input required by modern motion planning algorithms. To address this issue, we propose a novel deep neural network that takes advantage of both LiDAR and camera sensors and produces very accurate estimates directly in 3D space. We demonstrate the performance of our approach on both highways and in cities, and show very accurate estimates in complex scenarios such as heavy traffic (which produces occlusion), fork, merges and intersections.
ER  - 

TY  - CONF
TI  - Deep Reinforcement Learning to Acquire Navigation Skills for Wheel-Legged Robots in Complex Environments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3110
EP  - 3116
AU  - X. Chen
AU  - A. Ghadirzadeh
AU  - J. Folkesson
AU  - M. Björkman
AU  - P. Jensfelt
PY  - 2018
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - path planning
KW  - robot vision
KW  - wheels
KW  - navigation skills
KW  - navigation behaviors
KW  - action policies training
KW  - height-map image observations
KW  - motor commands
KW  - dynamic environments
KW  - mobile robot navigation
KW  - complex environments
KW  - deep reinforcement learning
KW  - wheel-legged robots
KW  - Training
KW  - Task analysis
KW  - Navigation
KW  - Mobile robots
KW  - Trajectory
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8593702
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Mobile robot navigation in complex and dynamic environments is a challenging but important problem. Reinforcement learning approaches fail to solve these tasks efficiently due to reward sparsities, temporal complexities and high-dimensionality of sensorimotor spaces which are inherent in such problems. We present a novel approach to train action policies to acquire navigation skills for wheel-legged robots using deep reinforcement learning. The policy maps height-map image observations to motor commands to navigate to a target position while avoiding obstacles. We propose to acquire the multifaceted navigation skill by learning and exploiting a number of manageable navigation behaviors. We also introduce a domain randomization technique to improve the versatility of the training samples. We demonstrate experimentally a significant improvement in terms of data-efficiency, success rate, robustness against irrelevant sensory data, and also the quality of the maneuver skills.
ER  - 

TY  - CONF
TI  - Learning and Generalization of Dynamic Movement Primitives by Hierarchical Deep Reinforcement Learning from Demonstration
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3117
EP  - 3123
AU  - W. Kim
AU  - C. Lee
AU  - H. J. Kim
PY  - 2018
KW  - grippers
KW  - learning (artificial intelligence)
KW  - nonlinear differential equations
KW  - meta-controller
KW  - learning generalization
KW  - dynamic movement primitives
KW  - hierarchical deep reinforcement learning
KW  - nonlinear differential equation
KW  - observed movement
KW  - hierarchical strategy
KW  - hierarchical deep RL
KW  - DMP framework
KW  - 6-degree-of-freedom arm
KW  - deterministic actor-critic algorithm
KW  - robotic skill learning
KW  - Task analysis
KW  - Robots
KW  - Reinforcement learning
KW  - Mathematical model
KW  - Differential equations
KW  - Dynamics
KW  - Deep learning
DO  - 10.1109/IROS.2018.8594476
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents an approach to learn and generalize robotic skills from a demonstration using deep reinforcement learning (deep RL). Dynamic Movement Primitives (DMPs) formulate a nonlinear differential equation and produce the observed movement from a demonstration. However, it is hard to generate new behaviors from using DMPs. Thus, we apply DMPs framework into deep RL as an initial setting for learning the robotic skills. First, we build a network to represent this differential equation, and learn and generalize the movements by optimizing the shape of DMPs with respect to the rewards up to the end of each sequence of movement primitives. In order to do this, we consider a deterministic actor-critic algorithm for deep RL and we also apply a hierarchical strategy. This drastically reduces the search space for a robot by decomposing the task, which allows to solve the sparse reward problem from a complex task. In order to integrate DMPs with hierarchical deep RL, the differential equation is considered as temporal abstraction of option. The overall structure is mainly composed of two controllers: meta-controller and sub-controller. The meta-controller learns a policy over intrinsic goals and a sub-controller learns a policy over actions to accomplish the given goals. We demonstrate our approach on a 6 degree-of-freedom (DOF) arm with a I-DOF gripper and evaluate our approach through a pick-and-place task.
ER  - 

TY  - CONF
TI  - Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3124
EP  - 3129
AU  - S. Hosseinzadeh
AU  - M. Shakeri
AU  - H. Zhang
PY  - 2018
KW  - convolutional neural nets
KW  - learning (artificial intelligence)
KW  - object detection
KW  - robot vision
KW  - statistical analysis
KW  - support vector machines
KW  - patched convolutional neural network
KW  - semantic-aware patch-level convolutional neural network
KW  - statistical features
KW  - multiclass support vector machine
KW  - deep learning framework
KW  - robotic applications
KW  - vision systems
KW  - shadow detection methods
KW  - Image color analysis
KW  - Image edge detection
KW  - Support vector machines
KW  - Robots
KW  - Image segmentation
KW  - Training
KW  - Time complexity
DO  - 10.1109/IROS.2018.8594050
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In recent years, various shadow detection methods from a single image have been proposed and used in vision systems; however, most of them are not appropriate for the robotic applications due to the expensive time complexity. This paper introduces a fast shadow detection method using a deep learning framework, with a time cost that is appropriate for robotic applications. In our solution, we first obtain a shadow prior map with the help of multi-class support vector machine using statistical features. Then, we use a semantic-aware patch-level Convolutional Neural Network that efficiently trains on shadow examples by combining the original image and the shadow prior map. Experiments on benchmark datasets demonstrate the proposed method significantly decreases the time complexity of shadow detection, by one or two orders of magnitude compared with state-of-the-art methods, without losing accuracy.
ER  - 

TY  - CONF
TI  - Robust Decentralized Context-Aware Sensor Fault Detection with In-Place Self-Calibration
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3130
EP  - 3136
AU  - J. L. Paneque
AU  - J. R. Martinez-Dedios
AU  - A. Ollero
PY  - 2018
KW  - Bayes methods
KW  - calibration
KW  - fault diagnosis
KW  - hidden Markov models
KW  - sensors
KW  - statistical analysis
KW  - complex context information
KW  - decentralized RANSAC
KW  - Bayesian networks
KW  - uncalibrated sensor
KW  - hidden Markov models
KW  - robust decentralized context-aware sensor fault detection methods
KW  - industrial plants
KW  - in-place sensor self-recalibration capability
KW  - consensus-based modeling step
KW  - statistical analysis
KW  - network topology
KW  - complex dynamic systems
KW  - Robot sensing systems
KW  - Hidden Markov models
KW  - Computational modeling
KW  - Monitoring
KW  - Temperature measurement
KW  - Bayes methods
KW  - Analytical models
DO  - 10.1109/IROS.2018.8593680
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - There is a high demand in advanced fault detection methods suitable for sensor networks monitoring complex dynamic systems such as industrial plants or large infrastructure units. This paper proposes a robust and efficient decentralized sensor fault detection method with in-place sensor self-recalibration capability that extracts and uses complex context information referred to the full monitored process. The method includes three main components, all decentralized and sharing the same statistical framework: 1) a consensus-based modeling step based on decentralized RANSAC; 2) a statistical analysis based on Bayesian networks and Hidden Markov Models in which each sensor identifies inconsistencies with the consensus model and determines if it is correctly calibrated, uncalibrated or faulty and; 3) a final step in which each uncalibrated sensor self-recalibrates using the consensus model. The proposed method is efficient in the use of computational and communicational resources, it is scalable and robust against outliers, transmission errors, sensor failures and network topology changes. It has been extensively validated in an experimental industrial setting.
ER  - 

TY  - CONF
TI  - Heterogeneous Sensor-Robot Team Positioning and Mixed Strategy Scheduling
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3137
EP  - 3144
AU  - B. T. Hartman
AU  - R. D. Tatum
AU  - M. J. Bays
PY  - 2018
KW  - game theory
KW  - multi-robot systems
KW  - scheduling
KW  - sensor placement
KW  - simulated annealing
KW  - heterogeneous sensor-robot team positioning
KW  - mixed strategy scheduling
KW  - effector robots
KW  - anticipated arrival traffic
KW  - adversarial game
KW  - sensor positions
KW  - anticipated potential arrival paths
KW  - uniform power schedule
KW  - adaptive simulated annealing
KW  - Robot sensing systems
KW  - Schedules
KW  - Robot kinematics
KW  - Games
KW  - Linear programming
DO  - 10.1109/IROS.2018.8594263
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We are faced with the problem of optimally placing a heterogeneous team of sensors and effector robots in an area while taking into account the environment, anticipated arrival traffic, and desired power consumption of the team. We stage the problems of anticipating arrival traffic and determining a proper power schedule as an adversarial game, incorporating our analysis of the game in the objective function which evaluates sensor positions. We obtain the set of sensor positions which performs best at the desired power consumption, evaluating the mixed strategy of sensor activity that best counters the anticipated potential arrival paths. To determine an approximate global optima for a large number of heterogeneous nodes, we employ Adaptive Simulated Annealing (ASA) to ensure our algorithm is flexible over a varied range of scenarios. We compare the proposed algorithm to a gradient-based greedy placement algorithm with a uniform power schedule within simulation.
ER  - 

TY  - CONF
TI  - Robotic Subsurface Pipeline Mapping with a Ground-penetrating Radar and a Camera
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3145
EP  - 3150
AU  - H. Li
AU  - C. Chou
AU  - L. Fan
AU  - B. Li
AU  - D. Wang
AU  - D. Song
PY  - 2018
KW  - buried object detection
KW  - geophysical image processing
KW  - geophysical techniques
KW  - ground penetrating radar
KW  - image reconstruction
KW  - maximum likelihood estimation
KW  - pipelines
KW  - radar detection
KW  - radar imaging
KW  - robot vision
KW  - pipeline groups
KW  - hyperbola response
KW  - GPR sensing process
KW  - Ground Penetrating Radar scans
KW  - subsurface pipeline mapping method
KW  - robotic subsurface pipeline mapping
KW  - subsurface pipes
KW  - representative pipeline configurations
KW  - maximum likelihood estimation
KW  - J-Linkage method
KW  - hyperbolas
KW  - GPR scans
KW  - mapping outputs
KW  - visual simultaneous localization
KW  - nonperpendicular angles
KW  - general scanning
KW  - size 4.69 cm
KW  - Ground penetrating radar
KW  - Pipelines
KW  - Cameras
KW  - Three-dimensional displays
KW  - Trajectory
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8594006
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose a novel subsurface pipeline mapping method by fusing Ground Penetrating Radar (GPR) scans and camera images. To facilitate the simultaneous detection of multiple pipelines, we model the GPR sensing process and prove hyperbola response for general scanning with non-perpendicular angles. Furthermore, we fuse visual simultaneous localization and mapping outputs, encoder readings with GPR scans to classify hyperbolas into different pipeline groups. We extensively apply the J-Linkage method and maximum likelihood estimation to improve algorithm robustness and accuracy. As the result, we optimally estimate the radii and locations of all pipelines. We have implemented our method and tested it in physical experiments with representative pipeline configurations. The results show that our method successfully reconstructs all subsurface pipes. Moreover, the average localization error is 4.69cm.
ER  - 

TY  - CONF
TI  - UAV Based Wireless Charging of Sensor Networks Without Prior Knowledge
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3151
EP  - 3158
AU  - N. W. Najeeb
AU  - C. Detweiler
PY  - 2018
KW  - autonomous aerial vehicles
KW  - wireless sensor networks
KW  - Power Transfer Efficiency Compensation
KW  - efficiency drops
KW  - real-world power transfer scenarios
KW  - knowledge algorithm
KW  - maximum power transfer efficiency
KW  - constant maximum efficiency CPTEC
KW  - UAV based Wireless Charging
KW  - Unmanned Aerial Vehicles
KW  - Wireless Rechargeable Sensor Networks
KW  - charging efficiency
KW  - wireless transmitter
KW  - charged node
KW  - sensor nodes
KW  - power information
KW  - limits scalability
KW  - wireless receiver
KW  - power level increase
KW  - power level increase
KW  - Robot sensing systems
KW  - Unmanned aerial vehicles
KW  - Wireless sensor networks
KW  - Wireless communication
KW  - Receivers
KW  - Wireless power transfer
DO  - 10.1109/IROS.2018.8594255
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Unmanned Aerial Vehicles (UAVs) can charge Wireless Rechargeable Sensor Networks (WRSNs) in remote or hard to access locations. However, the charging efficiency is heavily affected by the distance between the wireless transmitter and receiver. This efficiency impacts the possible power level increase of each charged node. Most charging algorithms require full knowledge of sensor nodes' power levels to identify the nodes to charge. Collecting this power information adds overhead to the network and limits scalability. We propose and implement Charging with Power Transfer Efficiency Compensation (CPTEC), an algorithm that charges a WRSN without the need for a priori knowledge of the nodes' power levels. We show that CPTEC compensates for efficiency drops, due to landing alignments, making it practical for real-world power transfer scenarios. Our results show that CPTEC is able to perform with a median at ≈ 72% of the optimal performance of a full knowledge algorithm that assumes maximum power transfer efficiency, while other work drops to ≈ 22%. Under constant maximum efficiency CPTEC performs ≈ 90% of the optimal full knowledge case.
ER  - 

TY  - CONF
TI  - Mobile Robot Localization Considering Class of Sensor Observations
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3159
EP  - 3166
AU  - N. Akai
AU  - L. Y. Morales
AU  - H. Murase
PY  - 2018
KW  - collision avoidance
KW  - mobile robots
KW  - localization robustness
KW  - environment dynamics
KW  - robots
KW  - sensor observations
KW  - mapped obstacles
KW  - observation model
KW  - unmapped obstacles
KW  - real-world mobile robot navigation competition
KW  - mobile robot localization
KW  - Robot sensing systems
KW  - Mathematical model
KW  - Robustness
KW  - Hidden Markov models
KW  - Mobile robots
KW  - Collision avoidance
DO  - 10.1109/IROS.2018.8594146
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Localization robustness against environment dynamics is significant for robots to achieve autonomous navigation in unmodified environments. A basic method of improving the robustness of a robot is considering the sensor observations obtained from mapped obstacles and using them for localizing the robot's pose. This study proposes an observation model that considers the class of sensor observations, where “class” categorizes the sensor observations as those obtained from mapped and unmapped obstacles. In the proposed approach, the robot's pose and the class are estimated simultaneously. As a result, the robot's pose can be localized using the sensor observations obtained only from mapped obstacles. First, we evaluated the performance of the proposed approach using simulations. Further, we tested the proposed approach in a real-world mobile robot navigation competition, called “Tsukuba Challenge,” held in Japan. The robustness and effectiveness of the proposed approach against environment dynamics were verified from the experimental results.
ER  - 

TY  - CONF
TI  - Robust Odometry using Sensor Consensus Analysis
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3167
EP  - 3173
AU  - A. W. Palmer
AU  - N. Nourani-Vatani
PY  - 2018
KW  - calibration
KW  - distance measurement
KW  - Kalman filters
KW  - measurement uncertainty
KW  - nonlinear filters
KW  - sensors
KW  - statistical testing
KW  - odometry system
KW  - measurement pre-processing stage
KW  - sensor consensus analysis
KW  - German Intercity-Express highspeed trains
KW  - wheel slip
KW  - autonomous systems
KW  - rail industry
KW  - extended Kalman filter
KW  - automatic train protection systems
KW  - incorrect velocity estimation
KW  - robust odometry systems
KW  - wheel encoder miscalibration
KW  - wheel slippage
KW  - wheel encoder calibration
KW  - SCA
KW  - statistical z-testing
KW  - measurement uncertainty
KW  - Wheels
KW  - Robot sensing systems
KW  - Acceleration
KW  - Measurement uncertainty
KW  - Global Positioning System
KW  - Extraterrestrial measurements
KW  - Length measurement
DO  - 10.1109/IROS.2018.8594473
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Odometry forms an important component of many manned and autonomous systems. In the rail industry in particular, having precise and robust odometry is crucial for the correct operation of the Automatic Train Protection systems that ensure the safety of high-speed trains in operation around the world. Two problems commonly encountered in such odometry systems are miscalibration of the wheel encoders and slippage of the wheels under acceleration and braking, resulting in incorrect velocity estimates. This paper introduces an odometry system that addresses these problems. It comprises of an Extended Kalman Filter that tracks the calibration of the wheel encoders as state variables, and a measurement pre-processing stage called Sensor Consensus Analysis (SCA) that scales the uncertainty of a measurement based on how consistent it is with the measurements from the other sensors. SCA uses the statistical z-test to determine when an individual measurement is inconsistent with the other measurements, and scales the uncertainty until the z-test passes. This system is demonstrated on data from German Intercity-Express highspeed trains and it is shown to successfully deal with errors due to miscalibration and wheel slip.
ER  - 

TY  - CONF
TI  - Octree map based on sparse point cloud and heuristic probability distribution for labeled images
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 3174
EP  - 3181
AU  - J. S. Berrio
AU  - W. Zhou
AU  - J. Ward
AU  - S. Worrall
AU  - E. Nebot
PY  - 2018
KW  - calibration
KW  - cameras
KW  - convolutional neural nets
KW  - image recognition
KW  - object recognition
KW  - octrees
KW  - probability
KW  - stereo image processing
KW  - semantic octree maps
KW  - probabilistic octree framework
KW  - single lidar scans
KW  - octree map building algorithm
KW  - labeled lidar scan
KW  - camera-lidar calibration parameters
KW  - convolutional neural network
KW  - accurate driving maneuvers
KW  - automated vehicle
KW  - urban roads
KW  - labeled images
KW  - heuristic probability distribution
KW  - sparse point cloud
KW  - Three-dimensional displays
KW  - Semantics
KW  - Laser radar
KW  - Uncertainty
KW  - Octrees
KW  - Cameras
KW  - Buildings
DO  - 10.1109/IROS.2018.8594024
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - To navigate through urban roads, an automated vehicle must be able to perceive and recognize objects in a three-dimensional environment. A high level contextual understanding of the surroundings is necessary to execute accurate driving maneuvers. This paper presents a novel approach to build three dimensional semantic octree maps from lidar scans and the output of a convolutional neural network (CNN) to obtain the labels of the environment. We present a heuristic method to associate uncertainties to the labels from the images based on a combination of the labels themselves, score maps retrieved by the CNN and the raw images. These uncertainties and the camera-lidar calibration parameters for multiple cameras are considered in the projection of the labels and their uncertainties into the point cloud. Every labeled lidar scan works as an input to an octree map building algorithm that calculates and updates the label probabilities of the voxels in the map. This paper also presents a qualitative and quantitative evaluation of accuracy, analyzing projection in single lidar scans and complete maps built with our probabilistic octree framework.
ER  - 


