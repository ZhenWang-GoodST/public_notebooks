TY  - CONF
TI  - Estimation of Interaction Forces in Robotic Surgery using a Semi-Supervised Deep Neural Network Model
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 761
EP  - 768
AU  - A. Marban
AU  - V. Srinivasan
AU  - W. Samek
AU  - J. Fern√°ndez
AU  - A. Casals
PY  - 2018
KW  - force feedback
KW  - image reconstruction
KW  - image representation
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - neural nets
KW  - surgery
KW  - unsupervised learning
KW  - video signal processing
KW  - interaction forces
KW  - force estimation task
KW  - LSTM network
KW  - RGB frame
KW  - CAE
KW  - Convolutional Auto-Encoder
KW  - Long-Short Term Memory network
KW  - encoder network
KW  - SemiSupervised Learning framework
KW  - SL techniques
KW  - UL
KW  - Unsupervised Learning method
KW  - video frame
KW  - compact representation
KW  - unlabeled video sequences
KW  - video sequence
KW  - Supervised Learning setting
KW  - Vision-Based Force Sensing
KW  - current Robot-Assisted Minimally Invasive Surgery systems
KW  - force feedback
KW  - semisupervised deep neural network model
KW  - robotic surgery
KW  - Video sequences
KW  - Force
KW  - Tools
KW  - Robot sensing systems
KW  - Surgery
KW  - Estimation
KW  - Vision Based Force Sensing
KW  - Robotic Surgery
KW  - Deep Neural Networks
KW  - Semi-Supervised Learning
DO  - 10.1109/IROS.2018.8593701
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Providing force feedback as a feature in current Robot-Assisted Minimally Invasive Surgery systems still remains a challenge. In recent years, Vision-Based Force Sensing (VBFS) has emerged as a promising approach to address this problem. Existing methods have been developed in a Supervised Learning (SL) setting. Nonetheless, most of the video sequences related to robotic surgery are not provided with ground-truth force data, which can be easily acquired in a controlled environment. A powerful approach to process unlabeled video sequences and find a compact representation for each video frame relies on using an Unsupervised Learning (UL) method. Afterward, a model trained in an SL setting can take advantage of the available ground-truth force data. In the present work, UL and SL techniques are used to investigate a model in a Semi-Supervised Learning (SSL) framework, consisting of an encoder network and a Long-Short Term Memory (LSTM) network. First, a Convolutional Auto-Encoder (CAE) is trained to learn a compact representation for each RGB frame in a video sequence. To facilitate the reconstruction of high and low frequencies found in images, this CAE is optimized using an adversarial framework and a L1-loss, respectively. Thereafter, the encoder network of the CAE is serially connected with an LSTM network and trained jointly to minimize the difference between ground-truth and estimated force data. Datasets addressing the force estimation task are scarce. Therefore, the experiments have been validated in a custom dataset. The results suggest that the proposed approach is promising.
ER  - 

TY  - CONF
TI  - Cross-Scene Suture Thread Parsing for Robot Assisted Anastomosis based on Joint Feature Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 769
EP  - 776
AU  - Y. Gu
AU  - Y. Hu
AU  - L. Zhang
AU  - J. Yang
AU  - G. Yang
PY  - 2018
KW  - feature extraction
KW  - manipulators
KW  - medical computing
KW  - medical robotics
KW  - object detection
KW  - surgery
KW  - unsupervised learning
KW  - joint feature learning framework
KW  - background adaptation
KW  - surgical suture thread detection
KW  - unsupervised domain adaptation
KW  - labelled training data
KW  - partially labelled target domain
KW  - organs
KW  - adversarial learning
KW  - cross-scene suture thread parsing
KW  - task autonomy
KW  - robot-assisted anastomosis
KW  - automatic thread detection
KW  - surgical robots
KW  - robot manipulation
KW  - surgical settings
KW  - foreground adaptation
KW  - semisupervised domain adaptation
KW  - detection model learning
KW  - semantic identity
KW  - Yarn
KW  - Task analysis
KW  - Instruction sets
KW  - Adaptation models
KW  - Image segmentation
KW  - Surgery
KW  - Splines (mathematics)
DO  - 10.1109/IROS.2018.8593622
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Task autonomy is an important consideration for the development of future surgical robots. For robot-assisted anastomosis, suture thread detection is a prerequisite for subsequent robot manipulation. Previous works on automatic thread detection are focused on the learning of the models with specific surgical settings that are poorly generalisable to generic settings. In this paper, we propose a joint feature learning framework that caters for the foreground and background adaptation for surgical suture thread detection. The proposed method is developed in the context of semi-supervised and unsupervised domain adaptation, leveraging the labelled training data from the source domain to learn the detection model for unlabelled or partially labelled target domain, which can also be from different types of threads or organs. Based on adversarial learning, we further preserve the semantic identity and introduce curriculum adaptation to generate synthetic data. Experiments on four domain adaptation tasks for suture thread detection demonstrate the strength of the proposed method being able to generate good quality synthetic data and transfer between specific domains with limited or even no labelled data of the target domain.
ER  - 

TY  - CONF
TI  - Unsupervised Trajectory Segmentation and Promoting of Multi-Modal Surgical Demonstrations
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 777
EP  - 782
AU  - Z. Shao
AU  - H. Zhao
AU  - J. Xie
AU  - Y. Qu
AU  - Y. Guan
AU  - J. Tan
PY  - 2018
KW  - feature extraction
KW  - image segmentation
KW  - medical robotics
KW  - robot kinematics
KW  - surgery
KW  - unsupervised learning
KW  - video signal processing
KW  - wavelet transforms
KW  - multimodal surgical demonstrations
KW  - surgical trajectory segmentation
KW  - robot learning
KW  - robot-assisted minimally invasive surgery
KW  - kinematic data
KW  - over-segmentation issue
KW  - unsupervised deep learning network
KW  - convolutional auto-encoder
KW  - videos
KW  - unsupervised trajectory segmentation method
KW  - JIGSAWS dataset
KW  - wavelet transform
KW  - feature extraction
KW  - Kinematics
KW  - Feature extraction
KW  - Surgery
KW  - Trajectory
KW  - Convolution
KW  - Visualization
KW  - Wavelet transforms
DO  - 10.1109/IROS.2018.8593379
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - To improve the efficiency of surgical trajectory segmentation for robot learning in robot-assisted minimally invasive surgery, this paper presents a fast unsupervised method using video and kinematic data, followed by a promoting procedure to address the over-segmentation issue. Unsupervised deep learning network, stacking convolutional auto-encoder, is employed to extract more discriminative features from videos in an effective way. To further improve the accuracy of segmentation, on one hand, wavelet transform is used to filter out the noises existed in the features from video and kinematic data. On the other hand, the segmentation result is promoted by identifying the adjacent segments with no state transition based on the predefined similarity measurements. Extensive experiments on a public dataset JIGSAWS show that our method achieves much higher accuracy of segmentation than state-of-the-art methods in the shorter time.
ER  - 

TY  - CONF
TI  - Autonomous Localization, Navigation and Haustral Fold Detection for Robotic Endoscopy
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 783
EP  - 790
AU  - J. M. Prendergast
AU  - G. A. Formosa
AU  - C. R. Heckman
AU  - M. E. Rentschler
PY  - 2018
KW  - biological organs
KW  - biomedical optical imaging
KW  - cancer
KW  - endoscopes
KW  - medical image processing
KW  - medical robotics
KW  - surgery
KW  - autonomous localization
KW  - Haustral fold detection
KW  - robotic endoscopy
KW  - capsule endoscopes
KW  - minimally invasive devices
KW  - gastrointestinal abnormalities
KW  - colorectal cancer
KW  - real-time navigation system
KW  - observational devices
KW  - autonomous navigation
KW  - single minimally invasive device
KW  - vision system
KW  - autonomous lumen center tracking
KW  - haustral fold identification
KW  - multiple haustral folds
KW  - robotic endoscope platform
KW  - active simulator
KW  - real-time localization
KW  - center tracking algorithm
KW  - colonoscopy
KW  - in vivo video
KW  - surgical tools
KW  - mobility system
KW  - Endoscopes
KW  - Robot sensing systems
KW  - Colon
KW  - Navigation
KW  - Wheels
KW  - In vivo
DO  - 10.1109/IROS.2018.8594106
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Capsule endoscopes have gained popularity over the last decade as minimally invasive devices for diagnosing gastrointestinal abnormalities such as colorectal cancer. While this technology offers a less invasive and more convenient alternative to traditional scopes, these capsules are only able to provide observational capabilities due to their passive nature. With the addition of a reliable mobility system and a real-time navigation system, capsule endoscopes could transform from observational devices into active surgical tools, offering biopsy and therapeutic capabilities and even autonomous navigation in a single minimally invasive device. In this work, a vision system is developed to allow for autonomous lumen center tracking and haustral fold identification and tracking during colonoscopy. This system is tested for its ability to accurately identify and track multiple haustral folds across many frames in both simulated and in vivo video, and the lumen center tracking is tested onboard a robotic endoscope platform (REP) within an active simulator to demonstrate autonomous navigation. In addition, real-time localization is demonstrated using open source ORB-SLAM2. The vision system successfully identified 95.6% of Haustral folds in simulator frames and 70.6% in in vivo frames and false positives occurred in less than 1% of frames. The center tracking algorithm showed in vivo center estimates within a mean error of 6.6% of physician estimates and allowed for the REP to traverse 2 m of the active simulator in 6 minutes without intervention.
ER  - 

TY  - CONF
TI  - Towards to a Robotic Assisted System for Percutaneous Nephrolithotomy
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 791
EP  - 797
AU  - H. Li
AU  - I. Paranawithana
AU  - Z. H. Chau
AU  - L. Yang
AU  - T. S. K. Lim
AU  - S. Foong
AU  - F. C. Ng
AU  - U. Tan
PY  - 2018
KW  - biomedical ultrasonics
KW  - kidney
KW  - medical robotics
KW  - needles
KW  - skin
KW  - surgery
KW  - ultrasonic therapy
KW  - target kidney stone
KW  - surgeon
KW  - robotic assisted system
KW  - percutaneous nephrolithotomy
KW  - recommended treatment method
KW  - kidney stone removal
KW  - percutaneous access
KW  - targeted calyx
KW  - flank skin
KW  - surgical performance
KW  - ultrasound probe
KW  - respiratory motion
KW  - percutaneous target
KW  - Surgery
KW  - Needles
KW  - Probes
KW  - Robot sensing systems
KW  - Force
KW  - Robot kinematics
DO  - 10.1109/IROS.2018.8593689
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Percutaneous Nephrolithotomy is a recommended treatment method for large kidney stone removal. However, the first and most important step, i.e., getting the percutaneous access to create the tract between the targeted calyx and the flank skin, is challenging as the surgeon is often occupied by several tasks at a given time. Therefore, in this paper, we propose a robotic assisted system that collaborates with the surgeon and provides assistance in order for the surgeons to focus on more critical jobs resulting in better surgical performance. A procedure for this robot including three working stages is described. This procedure allows the surgeon to choose a suitable percutaneous target using an ultrasound probe based on his or her experience and the robot will track the respiratory motion of the target kidney stone and insert the needle automatically after the surgeon releases the probe. Experiments are conducted to demonstrate the procedure with the proposed assisted robot for PCNL.
ER  - 

TY  - CONF
TI  - On Muscle Activation for Improving Robotic Rehabilitation after Spinal Cord Injury
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 798
EP  - 805
AU  - R. Cheng
AU  - Y. Sui
AU  - D. Sayenko
AU  - J. W. Burdick
PY  - 2018
KW  - biomechanics
KW  - electromyography
KW  - injuries
KW  - medical robotics
KW  - neuromuscular stimulation
KW  - neurophysiology
KW  - patient rehabilitation
KW  - patient treatment
KW  - motor activation patterns
KW  - improved standing ability
KW  - SCI patients
KW  - healthy activity
KW  - improving robotic rehabilitation
KW  - spinal cord stimulation
KW  - motor complete spinal cord injury
KW  - recovered motor activity
KW  - motor training
KW  - spinal stimulation
KW  - bipedal standing
KW  - spinal rehabilitation therapies
KW  - healthy subjects
KW  - muscle activation patterns
KW  - SCI patient motor activity
KW  - healthy motor activity
KW  - healthy standing muscle activity
KW  - patient stand training
KW  - Electromyography
KW  - Muscles
KW  - Training
KW  - Electrical stimulation
KW  - Robot sensing systems
KW  - Feature extraction
DO  - 10.1109/IROS.2018.8593973
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Spinal cord stimulation (SCS) has recently enabled humans with motor complete spinal cord injury (SCI) to independently stand and recover some lost autonomic function. However, the nature of the recovered motor activity and the interplay between SCS and motor training are not well understood. Understanding the effect of stand training and spinal stimulation on motor activity during bipedal standing is important for designing spinal rehabilitation therapies that seek to combine spinal stimulation and rehabilitative robots. In this study, we examined electromyography (EMG) data gathered from two SCI patients and six healthy subjects as they attempted standing. We analyzed the muscle activation patterns and EMG waveform shape to quantify both the changes in SCI patient motor activity with training, and the differences between healthy motor activity and SCI patient motor activity under stimulation. We also looked for correlations between the similarity in SCI patients' motor activity to healthy subjects and their overall standing ability. We found that good standing in SCI patients does not emulate healthy standing muscle activity. Furthermore, patient stand training heavily influenced motor activation patterns, but not in ways that improved standing ability. These results indicate that current training techniques do not optimally influence motor activity, and robotic rehabilitation strategies for SCI patients should target essential features of motor activity to optimize functional performance, rather than emulate healthy activity.
ER  - 

TY  - CONF
TI  - Printing Strain Gauges on Intuitive Surgical da Vinci Robot End Effectors
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 806
EP  - 812
AU  - R. Pe√±a
AU  - M. J. Smith
AU  - N. P. Ontiveros
AU  - F. L. Hammond
AU  - R. J. Wood
PY  - 2018
KW  - biomedical equipment
KW  - biomedical measurement
KW  - end effectors
KW  - force feedback
KW  - medical robotics
KW  - needles
KW  - strain gauges
KW  - surgery
KW  - printing strain gauges
KW  - robotic surgery
KW  - strain gauge printing method
KW  - da Vinci surgical robot end effectors
KW  - additive deposition-based sensor fabrication method
KW  - vapor-deposition-based sensor fabrication method
KW  - sensor performance
KW  - minimally invasive procedures
KW  - Sensors
KW  - Surface treatment
KW  - End effectors
KW  - Shafts
KW  - Strain
KW  - Strain measurement
KW  - Surgery
DO  - 10.1109/IROS.2018.8594517
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Force feedback during robotic surgery is critical in order to minimize potential injury to the patient and decrease recovery time from surgical procedures. Here we describe the use of a novel strain gauge printing method to apply low profile, low cost sensors directly to the surface of da Vinci surgical robot end effectors (Intuitive Surgical, Inc.) to sense deflection and provide force feedback. This additive, vapor-deposition-based sensor fabrication method is used to deposit strain gauges directly onto the surfaces of the end effectors with minimal disruption to the device and without the need for adhesives or machining operations. Initial experiments characterize sensor performance and indicate the applicability of the proposed approach for force feedback during minimally invasive procedures.
ER  - 

TY  - CONF
TI  - Group emotion recognition strategies for entertainment robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 813
EP  - 818
AU  - S. Cosentino
AU  - E. I. S. Randria
AU  - J. Lin
AU  - T. Pellegrini
AU  - S. Sessa
AU  - A. Takanishi
PY  - 2018
KW  - affective computing
KW  - cloud computing
KW  - emotion recognition
KW  - face recognition
KW  - humanoid robots
KW  - mobile robots
KW  - face API
KW  - human perceptions
KW  - assistive robotics
KW  - emotion API
KW  - Microsoft Azure cognitive services
KW  - Waseda entertainment robots
KW  - computer science
KW  - Ekman's extended Big Six emotional model
KW  - group emotion recognition strategies
KW  - affective computing
KW  - cloud-computing based solution
KW  - facial expression analysis
KW  - Face
KW  - Emotion recognition
KW  - Entertainment industry
KW  - Cameras
KW  - Humanoid robots
KW  - Mood
KW  - humanoid robot
KW  - entertainment robot
KW  - assistive robotics
KW  - emotion recognition
DO  - 10.1109/IROS.2018.8593503
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, a system to determine the emotion of a group of people via facial expression analysis is proposed for the Waseda Entertainment Robots. General models and standard methods for emotion definition and recognition are briefly described, as well as strategies for computing the group global emotion, knowing the individual emotions of group members. This work is based on Ekman's extended ‚ÄúBig Six‚Äù emotional model, popular in Computer Science and Affective Computing. Emotion recognition via facial expression analysis is performed with a cloud-computing based solution, using Microsoft Azure Cognitive services. First, the performances of both the Face API to detect faces, and Emotion API, to compute emotion via face expression analysis, are tested. After that, a solution to compute the emotion of a group of people has been implemented and its performances compared to human perceptions. This work presents concepts and strategies which can be generalized for applications within the scope of assistive robotics and, more broadly, affective computing, wherever it will be necessary to determine the emotion of a group of people.
ER  - 

TY  - CONF
TI  - Learning How Pedestrians Navigate: A Deep Inverse Reinforcement Learning Approach
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 819
EP  - 826
AU  - M. Fahad
AU  - Z. Chen
AU  - Y. Guo
PY  - 2018
KW  - collision avoidance
KW  - feature extraction
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - neural nets
KW  - trajectory control
KW  - mobile robots
KW  - human robot interaction
KW  - robot navigation algorithms
KW  - human navigation behaviors
KW  - maximum entropy deep inverse reinforcement learning
KW  - nonlinear reward function
KW  - deep neural network approximation
KW  - social affinity map
KW  - human motion trajectories
KW  - learned reward function
KW  - natural social navigation behaviors
KW  - deep inverse reinforcement learning approach
KW  - pedestrian trajectories
KW  - MEDIRL algorithm
KW  - feature extraction
KW  - collision avoidance
KW  - pedestrians navigation
KW  - Navigation
KW  - Robots
KW  - Trajectory
KW  - Reinforcement learning
KW  - Collision avoidance
KW  - Neural networks
KW  - Entropy
DO  - 10.1109/IROS.2018.8593438
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Humans and mobile robots will be increasingly cohabiting in the same environments, which has lead to an increase in studies on human robot interaction (HRI). One important topic in these studies is the development of robot navigation algorithms that are socially compliant to humans navigating in the same space. In this paper, we present a method to learn human navigation behaviors using maximum entropy deep inverse reinforcement learning (MEDIRL). We use a large open dataset of pedestrian trajectories collected in an uncontrolled environment as the expert demonstrations. Human navigation behaviors are captured by a nonlinear reward function through deep neural network (DNN) approximation. The developed MEDIRL algorithm takes feature inputs including social affinity map (SAM) that are extracted from human motion trajectories. We perform simulation experiments using the learned reward function, and the performance is evaluated comparing it with the real measured pedestrian trajectories in the dataset. The evaluation results show that the proposed method has acceptable prediction accuracy compared to other state-of-the-art methods, and it can generate pedestrian trajectories similar to real human trajectories with natural social navigation behaviors such as collision avoidance, leader-follower, and split-and-rejoin.
ER  - 

TY  - CONF
TI  - Situated Human‚ÄìRobot Collaboration: predicting intent from grounded natural language
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 827
EP  - 833
AU  - J. Brawer
AU  - O. Mangin
AU  - A. Roncone
AU  - S. Widder
AU  - B. Scassellati
PY  - 2018
KW  - human-robot interaction
KW  - interactive systems
KW  - mobile robots
KW  - natural language processing
KW  - context models
KW  - collaborator
KW  - collaborative construction task
KW  - autonomous robot
KW  - task representations
KW  - naturalistic data sets
KW  - human-robot collaboration
KW  - grounded natural language
KW  - human teamwork
KW  - fluent interactions
KW  - nonverbal cues
KW  - robotic platforms
KW  - explicit commands
KW  - unequivocal representations
KW  - human partners
KW  - naturalistic speech
KW  - action selection
KW  - human-robot collaborative activities
KW  - separate speech
KW  - Task analysis
KW  - Collaboration
KW  - Context modeling
KW  - Natural languages
KW  - Tools
KW  - Robot kinematics
DO  - 10.1109/IROS.2018.8593942
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Research in human teamwork shows that a key element of fluid and fluent interactions is the interpretation of implicit verbal and non-verbal cues in context. This poses an issue to robotic platforms, however, as they have historically worked best when controlled through explicit commands that have employed structured, unequivocal representations of the external world and their human partners. In this work, we present a framework for effectively grounding situated and naturalistic speech to action selection during human-robot collaborative activities. This is accomplished by maintaining and incrementally updating separate ‚Äúspeech‚Äù and ‚Äúcontext‚Äù models that jointly classify a collaborator's utterance. We evaluate the efficacy of the system on a collaborative construction task with an autonomous robot and human participants. We first demonstrate that our system is capable of acquiring and deploying new task representations from limited and naturalistic data sets, and without any prior domain knowledge of language or the task itself. Finally, we show that our system is capable of significantly improving performance on an unfamiliar task after a one-shot exposure.
ER  - 

TY  - CONF
TI  - Social Coordination for Looking-Together Situations
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 834
EP  - 841
AU  - S. Akita
AU  - S. Satake
AU  - M. Shiomi
AU  - M. Imai
AU  - T. Kanda
PY  - 2018
KW  - telerobotics
KW  - social coordination
KW  - looking-together situations
KW  - utility-maximizing behavior
KW  - joint utility
KW  - joint-utility computation
KW  - utility-yielding behavior
KW  - utility model
KW  - teleoperated robot
KW  - Robot kinematics
KW  - Wheelchairs
KW  - Legged locomotion
KW  - Computational modeling
KW  - Human-robot interaction
DO  - 10.1109/IROS.2018.8594141
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - People engage in social coordination without explicitly communicating when they are conflicting over spatial resources, e.g., a shop clerk who yields to customers the best place to view products. In this study, we proposed a method that achieves such social coordination with a robot. Our idea is that the social coordination between two agents can be represented as utility-maximizing behavior for joint utility rather than just by a single agent utility. That is, given that each agent's reasonable behavior can be represented as utility-maximizing behavior for single agent utility, we model each agent's plans for himself as well as for the partner agent. Moreover, superiority relationships exist in this joint-utility computation. Since each agent knows such superiority relationships, social coordination can be modeled as utility-yielding behavior based on informed superiority. We specifically focus on looking-together situations for which we developed a utility model. With simulations, we investigate whether the above joint-utility-based modeling successfully reproduces social coordination in looking-together situations. We conducted an experiment in a situation where a tele-operated robot and a customer together look at products in a shop environment. Our experimental results show that our proposed method enables the robot to socially coordinate spatial resources, yielding significantly more thoughtful, less-self-centered, and appropriate impressions than the alternate robot.
ER  - 

TY  - CONF
TI  - Policy Shaping with Supervisory Attention Driven Exploration
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 842
EP  - 847
AU  - T. K. Faulkner
AU  - E. S. Short
AU  - A. L. Thomaz
PY  - 2018
KW  - interactive systems
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - policy shaping
KW  - robots
KW  - human supervision
KW  - human teacher
KW  - information-gathering actions
KW  - interactive reinforcement learning
KW  - interactive RL
KW  - Reinforcement learning
KW  - Task analysis
KW  - Negative feedback
KW  - Markov processes
KW  - Prediction algorithms
KW  - Intelligent robots
DO  - 10.1109/IROS.2018.8594312
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robots deployed for long periods of time need to be able to explore and learn from their environment. One approach to this problem has been reinforcement learning (RL), in which robots receive rewards from the environment that allow them to choose optimal actions. To speed learning when human supervision is available, interactive reinforcement learning solicits feedback from a human teacher. However, this approach typically assumes that learning takes place under continuous supervision, which is unlikely to hold in long-term scenarios. We propose an extension to a method of interactive reinforcement learning, policy shaping, that takes into account human attention. Our approach enables better performance while unattended by favoring information-gathering actions when attended and actions that have received positive feedback when unattended. We test our approach in both simulation and on a robot, finding that our method learns faster than policy shaping and performs more safely than policy shaping while no one is paying attention to the robot.
ER  - 

TY  - CONF
TI  - Friendly Motion Learning towards Sustainable Human Robot Interaction
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 848
EP  - 853
AU  - S. Sato
AU  - H. Kamide
AU  - Y. Mae
AU  - M. Kojima
AU  - T. Arai
PY  - 2018
KW  - convolutional neural nets
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - sustainable human robot interaction
KW  - interaction motion features
KW  - machine learning technique
KW  - convolution neural network
KW  - interaction behavior
KW  - human impression
KW  - friendly motion learning
KW  - Robots
KW  - Convolution
KW  - Neural networks
KW  - Decoding
KW  - Feature extraction
KW  - Mathematical model
KW  - Human-robot interaction
DO  - 10.1109/IROS.2018.8593432
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For generating interactive behavior of robot to build a long-term relationship between humans and robots, we focus on the difference in familiarity of the human behaviors during conversation. It is difficult to extract interaction motion features correlated to such familiarity as a model in manual. Therefore, we use a machine learning technique: convolution neural network to learn and generate interaction behavior with different familiarity. In the evaluation experiment, we generated interaction behavior using a convolution neural network, which learned from the behaviors of friendship and unknown relationship, who have high and low familiarity respectively. We evaluated how much such interaction behavior affect the human impression by questionnaire survey.
ER  - 

TY  - CONF
TI  - On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 854
EP  - 860
AU  - E. Lakomkin
AU  - M. A. Zamani
AU  - C. Weber
AU  - S. Magg
AU  - S. Wermter
PY  - 2018
KW  - emotion recognition
KW  - humanoid robots
KW  - human-robot interaction
KW  - neural nets
KW  - speech recognition
KW  - acoustic events
KW  - iCub robot platform
KW  - neural approaches
KW  - speech emotion recognition
KW  - deep neural networks
KW  - human-robot collaboration
KW  - research community
KW  - neural network-based architectures
KW  - neural SER models
KW  - in-domain data
KW  - noisy conditions
KW  - state-of-the-art neural acoustic emotion recognition models
KW  - human-robot interaction scenarios
KW  - room conditions
KW  - Robots
KW  - Training
KW  - Emotion recognition
KW  - Data models
KW  - Speech recognition
KW  - Acoustics
KW  - Feature extraction
DO  - 10.1109/IROS.2018.8593571
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Speech emotion recognition (SER) is an important aspect of effective human-robot collaboration and received a lot of attention from the research community. For example, many neural network-based architectures were proposed recently and pushed the performance to a new level. However, the applicability of such neural SER models trained only on in-domain data to noisy conditions is currently under-researched. In this work, we evaluate the robustness of state-of-the-art neural acoustic emotion recognition models in human-robot interaction scenarios. We hypothesize that a robot's ego noise, room conditions, and various acoustic events that can occur in a home environment can significantly affect the performance of a model. We conduct several experiments on the iCub robot platform and propose several novel ways to reduce the gap between the model's performance during training and testing in real-world conditions. Furthermore, we observe large improvements in the model performance on the robot and demonstrate the necessity of introducing several data augmentation techniques like overlaying background noise and loudness variations to improve the robustness of the neural approaches.
ER  - 

TY  - CONF
TI  - Modeling Supervisor Safe Sets for Improving Collaboration in Human-Robot Teams
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 861
EP  - 868
AU  - D. L. McPherson
AU  - D. R. R. Scobee
AU  - J. Menke
AU  - A. Y. Yang
AU  - S. S. Sastry
PY  - 2018
KW  - cognition
KW  - human-robot interaction
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - reachability analysis
KW  - human-robot teams
KW  - human supervisor collaborates
KW  - optimization
KW  - reachability theory
KW  - robots dynamic
KW  - robot behavior
KW  - human behavior
KW  - cognitive resources
KW  - Robots
KW  - Safety
KW  - Trajectory
KW  - Level set
KW  - Noise measurement
KW  - Optimal control
KW  - Optimization
DO  - 10.1109/IROS.2018.8593865
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - When a human supervisor collaborates with a team of robots, the human's attention is divided, and cognitive resources are at a premium. We aim to optimize the distribution of these resources and the flow of attention. To this end, we propose the model of an idealized supervisor to describe human behavior. Such a supervisor employs a potentially inaccurate internal model of the the robots' dynamics to judge safety. We represent these safety judgements by constructing a safe set from this internal model using reachability theory. When a robot leaves this safe set, the idealized supervisor will intervene to assist, regardless of whether or not the robot remains objectively safe. False positives, where a human supervisor incorrectly judges a robot to be in danger, needlessly consume supervisor attention. In this work, we propose a method that decreases false positives by learning the supervisor's safe set and using that information to govern robot behavior. We prove that robots behaving according to our approach will reduce the occurrence of false positives for our idealized supervisor model. Furthermore, we empirically validate our approach with a user study that demonstrates a significant (p = 0.0328) reduction in false positives for our method compared to a baseline safety controller.
ER  - 

TY  - CONF
TI  - Deep Semantic Lane Segmentation for Mapless Driving
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 869
EP  - 875
AU  - A. Meyer
AU  - N. O. Salscheider
AU  - P. F. Orzechowski
AU  - C. Stiller
PY  - 2018
KW  - automobiles
KW  - feature extraction
KW  - image colour analysis
KW  - image segmentation
KW  - mobile robots
KW  - neural nets
KW  - object detection
KW  - path planning
KW  - road traffic
KW  - robot vision
KW  - deep semantic lane segmentation
KW  - autonomous driving systems
KW  - automated cars
KW  - sensor system
KW  - urban scenarios
KW  - deep neural network
KW  - lane semantics
KW  - road scene
KW  - mapless autonomous driving
KW  - street scenes
KW  - RGB images
KW  - lane detection
KW  - cityscapes dataset
KW  - Roads
KW  - Semantics
KW  - Neural networks
KW  - Image segmentation
KW  - Autonomous vehicles
KW  - Three-dimensional displays
KW  - Pipelines
DO  - 10.1109/IROS.2018.8594450
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In autonomous driving systems a strong relation to highly accurate maps is taken to be inevitable, although street scenes change frequently. However, a preferable system would be to equip the automated cars with a sensor system that is able to navigate urban scenarios without an accurate map. We present a novel pipeline using a deep neural network to detect lane semantics and topology given RGB images. On the basis of this classification, the information about the road scene can be extracted just from the sensor setup supporting mapless autonomous driving. In addition to superseding the huge effort of creating and maintaining highly accurate maps, our system reduces the need for precise localization. Using an extended Cityscapes dataset, we show accurate ego lane detection including lane semantics on challenging scenarios for autonomous driving.
ER  - 

TY  - CONF
TI  - Closed-Loop Robot Task Planning Based on Referring Expressions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 876
EP  - 881
AU  - D. Kuhner
AU  - J. Aldinger
AU  - F. Burget
AU  - M. G√∂belbecker
AU  - W. Burgard
AU  - B. Nebel
PY  - 2018
KW  - adaptive control
KW  - closed loop systems
KW  - mobile robots
KW  - path planning
KW  - planning (artificial intelligence)
KW  - user interfaces
KW  - fetch-and-carry tasks
KW  - autonomous robots accessibility
KW  - user friendly
KW  - closed-loop robot task planning
KW  - complex task
KW  - automated planning
KW  - robotic systems
KW  - domain-independent planning system
KW  - goal formulation
KW  - referring expressions
KW  - adaptive control interface
KW  - manipulable objects
KW  - dynamic environments
KW  - Task analysis
KW  - Planning
KW  - Robots
KW  - Natural languages
KW  - Graphical user interfaces
KW  - Glass
DO  - 10.1109/IROS.2018.8593371
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Increasing the accessibility of autonomous robots also for inexperienced users requires user-friendly and high-level control opportunities of robotic systems. While automated planning is able to decompose a complex task into a sequence of steps which reaches an intended goal, it is difficult to formulate such a goal without knowing the internals of the planning system and the exact capabilities of the robot. This becomes even more important in dynamic environments in which manipulable objects are subject to change. In this paper, we present an adaptive control interface which allows users to specify goals based on an internal world model by incrementally building referring expressions to the objects in the world. We consider fetch-and-carry tasks and automatically deduce potential high-level goals from the world model to make them available to the user. Based on its perceptions our system can react to changes in the environment by adapting the goal formulation within the domain-independent planning system.
ER  - 

TY  - CONF
TI  - Learning Robotic Grasping Strategy Based on Natural-Language Object Descriptions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 882
EP  - 887
AU  - A. B. Rao
AU  - K. Krishnan
AU  - H. He
PY  - 2018
KW  - control engineering computing
KW  - dexterous manipulators
KW  - learning (artificial intelligence)
KW  - natural language processing
KW  - robotic grasping strategy
KW  - natural-language object descriptions
KW  - anthropomorphic robotic hand
KW  - natural-language descriptions
KW  - learning-based approach
KW  - natural language description
KW  - object features
KW  - natural-language processing technique
KW  - grasp type
KW  - human grasping taxonomy
KW  - AR10 robotic hand
KW  - Robots
KW  - Grasping
KW  - Taxonomy
KW  - Shape
KW  - Natural language processing
KW  - Kinematics
KW  - Task analysis
DO  - 10.1109/IROS.2018.8593886
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Given the description of an object, s physical attributes, humans can determine a proper strategy and grasp an object. This paper proposes an approach to determine grasping strategy for an anthropomorphic robotic hand simply based on natural-language descriptions of an object. A learning-based approach is proposed to help a robotic hand learn suitable grasp poses starting from the natural language description of the object. Object features are parsed from natural-language descriptions by using a customized natural-language processing technique. The most likely grasp type for the given object is learned from the human grasping taxonomy based on the parsed features. The grasping strategy generated by the proposed approach is evaluated both by simulation study and execution of the grasps on an AR10 robotic hand.
ER  - 

TY  - CONF
TI  - Semantic Grid Estimation with a Hybrid Bayesian and Deep Neural Network Approach
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 888
EP  - 895
AU  - √ñ. Erkent
AU  - C. Wolf
AU  - C. Laugier
AU  - D. S. Gonzalez
AU  - V. R. Cano
PY  - 2018
KW  - Bayes methods
KW  - belief networks
KW  - image colour analysis
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - optical radar
KW  - particle filtering (numerical methods)
KW  - semantic grid estimation
KW  - hybrid Bayesian
KW  - deep neural network
KW  - autonomous vehicle setting
KW  - high-level semantic information
KW  - grid cell
KW  - semantic label
KW  - hybrid approach
KW  - semantic segmentation
KW  - monocular RGB images
KW  - supervised learning
KW  - labeled groundtruth data
KW  - occupancy grids
KW  - LIDAR data
KW  - generative Bayesian particle filter
KW  - geometric information
KW  - RGB data
KW  - Semantics
KW  - Image segmentation
KW  - Bayes methods
KW  - Laser radar
KW  - Neural networks
KW  - Three-dimensional displays
KW  - Sensors
DO  - 10.1109/IROS.2018.8593434
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In an autonomous vehicle setting, we propose a method for the estimation of a semantic grid, i.e. a bird's eye grid centered on the car's position and aligned with its driving direction, which contains high-level semantic information about the environment and its actors. Each grid cell contains a semantic label with divers classes, as for instance {Road, Vegetation, Building, Pedestrian, Car...}. We propose a hybrid approach, which combines the advantages of two different methodologies: we use Deep Learning to perform semantic segmentation on monocular RGB images with supervised learning from labeled groundtruth data. We combine these segmentations with occupancy grids calculated from LIDAR data using a generative Bayesian particle filter. The fusion itself is carried out with a deep neural network, which learns to integrate geometric information from the LIDAR with semantic information from the RGB data. We tested our method on two datasets, namely the KITTI dataset, which is publicly available and widely used, and our own dataset obtained with our own platform, equipped with a LIDAR and various sensors. We largely outperform baselines which calculate the semantic grid either from the RGB image alone or from LIDAR output alone, showing the interest of this hybrid approach.
ER  - 

TY  - CONF
TI  - PRISM: Pose Registration for Integrated Semantic Mapping
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 896
EP  - 902
AU  - J. W. Hart
AU  - R. Shah
AU  - S. Kirmani
AU  - N. Walker
AU  - K. Baldauf
AU  - N. John
AU  - P. Stone
PY  - 2018
KW  - mobile robots
KW  - multi-robot systems
KW  - navigation
KW  - pose estimation
KW  - service robots
KW  - SLAM (robots)
KW  - computer science department
KW  - modern SLAM algorithms
KW  - map data
KW  - tedious manual process
KW  - automatically generated maps
KW  - PRISM
KW  - semantic markup
KW  - pose registration
KW  - integrated semantic
KW  - robotics applications
KW  - hotel
KW  - room service
KW  - hospital
KW  - medication
KW  - patient
KW  - UT Austin
KW  - autonomous mobile robots
KW  - BWIBots
KW  - building-wide intelligence project
KW  - Robots
KW  - Semantics
KW  - Three-dimensional displays
KW  - Cameras
KW  - Two dimensional displays
KW  - Computational modeling
KW  - Navigation
DO  - 10.1109/IROS.2018.8593681
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Many robotics applications involve navigating to positions specified in terms of their semantic significance. A robot operating in a hotel may need to deliver room service to a named room. In a hospital, it may need to deliver medication to a patient's room. The Building-Wide Intelligence Project at UT Austin has been developing a fleet of autonomous mobile robots, called BWIBots, which perform tasks in the computer science department. Tasks include guiding a person, delivering a message, or bringing an object to a location such as an office, lecture hall, or classroom. The process of constructing a map that a robot can use for navigation has been simplified by modern SLAM algorithms. The attachment of semantics to map data, however, remains a tedious manual process of labeling locations in otherwise automatically generated maps. This paper introduces a system called PRISM to automate a step in this process by enabling a robot to localize door signs - a semantic markup intended to aid the human occupants of a building - and to annotate these locations in its map.
ER  - 

TY  - CONF
TI  - 3D Deep Object Recognition and Semantic Understanding for Visually-Guided Robotic Service
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 903
EP  - 910
AU  - S. Lee
AU  - A. M. Naguib
AU  - N. U. Islam
PY  - 2018
KW  - Bayes methods
KW  - convolutional neural nets
KW  - feature extraction
KW  - image reconstruction
KW  - learning (artificial intelligence)
KW  - neurocontrollers
KW  - object recognition
KW  - ontologies (artificial intelligence)
KW  - robot vision
KW  - service robots
KW  - semantic understanding
KW  - visually-guided robotic service
KW  - visually-guided robotic errand service
KW  - visual environments
KW  - deep learning architecture
KW  - FER-CNN
KW  - layer-wise independent feedback connections
KW  - reconstructed features
KW  - object categories
KW  - 3D daily-life objects
KW  - recognition rate
KW  - ontology
KW  - feature extraction
KW  - 3D deep object recognition
KW  - adaptive Bayesian recognition framework
KW  - Three-dimensional displays
KW  - Bayes methods
KW  - Robots
KW  - Object recognition
KW  - Deep learning
KW  - Feature extraction
KW  - Two dimensional displays
DO  - 10.1109/IROS.2018.8593985
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For the success of visually-guided robotic errand service, it is critical to ensure dependability under various ill-conditioned visual environments. To this end, we have developed Adaptive Bayesian Recognition Framework in which in-situ selection of multiple sets of optimal features or evidences as well as proactive collection of sufficient evidences are proposed to implement the principle of dependability. The framework has shown excellent performance with a limited number of objects in a scene. However, there arises a need to extend the framework for handling a larger number of objects without performance degradation, while avoiding difficulty in feature engineering. To this end, a novel deep learning architecture, referred to here as FER-CNN, is introduced and integrated into the Adaptive Bayesian Recognition Framework. FER-CNN has capability of not only extracting but also reconstructing a hierarchy of features with the layer-wise independent feedback connections that can be trained. Reconstructed features representing parts of 3D objects then allow them to be semantically linked to ontology for exploring object categories and properties. Experiments are conducted in a home environment with real 3D daily-life objects as well as with the standard ModelNet dataset. In particular, it is shown that FER-CNN allows the number of objects and their categories to be extended by 10 and 5 times, respectively, while registering the recognition rate for ModelNet10 and ModelNet40 by 97% and 89.5%, respectively.
ER  - 

TY  - CONF
TI  - Semantic Mapping with Simultaneous Object Detection and Localization
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 911
EP  - 918
AU  - Z. Zeng
AU  - Y. Zhou
AU  - O. C. Jenkins
AU  - K. Desingh
PY  - 2018
KW  - image sensors
KW  - mobile robots
KW  - object detection
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - semantic mapping problem
KW  - CT-Map method
KW  - six degree-of-freedom pose
KW  - pose estimation
KW  - RGB-D sensor
KW  - Michigan progress fetch robot
KW  - particle filtering algorithm
KW  - CRF
KW  - conditional random field
KW  - contextual temporal mapping
KW  - object localization
KW  - object detection
KW  - Semantics
KW  - Object detection
KW  - Context modeling
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Simultaneous localization and mapping
DO  - 10.1109/IROS.2018.8594205
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present a filtering-based method for semantic mapping to simultaneously detect objects and localize their 6 degree-of-freedom pose. For our method, called Contextual Temporal Mapping (or CT-Map), we represent the semantic map as a belief over object classes and poses across an observed scene. Inference for the semantic mapping problem is then modeled in the form of a Conditional Random Field (CRF). CT-Map is a CRF that considers two forms of relationship potentials to account for contextual relations between objects and temporal consistency of object poses, as well as a measurement potential on observations. A particle filtering algorithm is then proposed to perform inference in the CT-Map model. We demonstrate the efficacy of the CT-Map method with a Michigan Progress Fetch robot equipped with a RGB-D sensor. Our results demonstrate that the particle filtering based inference of CT-Map provides improved object detection and pose estimation with respect to baseline methods that treat observations as independent samples of a scene.
ER  - 

TY  - CONF
TI  - Optimization-based Design and Analysis of Planar Rotary Springs
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 927
EP  - 934
AU  - N. Georgiev
AU  - J. Burdick
PY  - 2018
KW  - actuators
KW  - finite element analysis
KW  - optimisation
KW  - robots
KW  - springs (mechanical)
KW  - torque
KW  - torsion
KW  - rotary series elastic actuator springs
KW  - rapid torsional loading
KW  - FEA
KW  - mechanical testing
KW  - planar rotary springs
KW  - optimization-based design method
KW  - robotics applications
KW  - Springs
KW  - Strain
KW  - Stress
KW  - Mathematical model
KW  - Robots
KW  - Actuators
KW  - Optimization
DO  - 10.1109/IROS.2018.8594186
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper develops new methods to design high performance rotary series elastic actuator springs for robotics applications. The approach is based on a spring arm mathematical model that was previously introduced by the authors. The key contribution is the development of an optimization-based design method which maximizes the springs' overall torque density through optimization of the arm profile. An improved analysis algorithm allows for rapid torsional loading response simulation with possible internal contacts between the spring arms. The proposed design and analysis algorithms are validated through FEA and prototype mechanical testing.
ER  - 

TY  - CONF
TI  - Quaternion Joint: Dexterous 3-DOF Joint Representing Quaternion Motion for High-Speed Safe Interaction
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 935
EP  - 942
AU  - Y. Kim
AU  - J. Kim
AU  - W. Jang
PY  - 2018
KW  - actuators
KW  - dexterous manipulators
KW  - manipulator dynamics
KW  - manipulator kinematics
KW  - quaternion joint
KW  - wrist mechanism
KW  - tendon-driven mechanism
KW  - dexterous 3-DOF joint
KW  - quaternion motion
KW  - forward kinematics
KW  - inverse kinematics
KW  - lightweight manipulators
KW  - Wires
KW  - Wrist
KW  - Manipulators
KW  - Quaternions
KW  - Pulleys
KW  - Kinematics
DO  - 10.1109/IROS.2018.8594301
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a dexterous three degree-of-freedom (3-DOF) wrist mechanism with a large range of motion and uniform manipulability without singular points throughout the entire range of motion. It has a 2-DOF spherical pure rolling joint surrounded by two pairs of actuating wires, the motions of which directly represent the quaternion values of the joint; this joint is therefore named the quaternion joint. Based on this property, it has simple and clear forward and inverse kinematics and high manipulability. By adding a 1-DOF rotation joint at the distal end of the quaternion joint, it can be extended to a 3-DOF joint mechanism. To precisely approximate the spherical pure rolling motion in a confined central space, a novel parallel mechanism composed of three identical supporting linkages was introduced. Unlike conventional parallel mechanisms, it has a compact and simple structure with no interference among the supporting linkages. Because the wrist mechanism is a tendon-driven mechanism, and is thus suitable for lightweight manipulators, it is mounted to a low-inertia manipulator with high stiffness and strength, namely, LIMS2-AMBIDEX, which is an improved version of the authors' previous research. The basic concept and thorough theoretical analysis of the wrist mechanism are described herein, and the simulations and experiments conducted for a quantitative validation are presented.
ER  - 

TY  - CONF
TI  - Design of a 2 Motor 2 Degrees-of-Freedom Coupled Tendon-driven Joint Module
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 943
EP  - 948
AU  - W. Li
AU  - P. Chen
AU  - D. Bai
AU  - X. Zhu
AU  - S. Togo
AU  - H. Yokoi
AU  - Y. Jiang
PY  - 2018
KW  - actuators
KW  - control system synthesis
KW  - design engineering
KW  - mobile robots
KW  - position control
KW  - hybrid-actuated structure
KW  - internally-separately-actuated structure
KW  - internally-coaxially-actuated structure
KW  - externally-actuated structure
KW  - tendon coupling
KW  - torque reallocation
KW  - 2 motor 2 degrees-of-freedom coupled tendon-driven joint module
KW  - anthropomorphic robot arm
KW  - 2M2D coupled tendon-driven joint module
KW  - motor position
KW  - Pulleys
KW  - Tendons
KW  - Torque
KW  - Manipulators
KW  - Couplings
KW  - Routing
DO  - 10.1109/IROS.2018.8594080
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - A 2 motor 2 degrees-of-freedom (2M2D) coupled tendon driven joint module is proposed as a basic component for robot arms. Torque reallocation via tendon coupling can enhance the output torque of one single joint. According to the motor position, the joint module is classified into four types: the externally-actuated structure, the internally-coaxially-actuated structure, the internally-separately-actuated structure, and the hybrid-actuated structure. The four structures are analyzed and compared, and their implementation design examples are given. Experiments comparing the proposed joint module with directly-actuated traditional joint suggested that the 2M2D coupled tendon-driven joint module can obtain high control accuracy, and the torque reallocation via tendon coupling is effective to improve output torque. Additionally, an anthropomorphic robot arm with low weight and high payload was developed to show the utility of the proposed joint module.
ER  - 

TY  - CONF
TI  - A Differential Elastic Joint for Multi-linked Pipeline Inspection Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 949
EP  - 954
AU  - A. Kakogawa
AU  - S. Ma
PY  - 2018
KW  - actuators
KW  - elasticity
KW  - inspection
KW  - mobile robots
KW  - pipelines
KW  - pipes
KW  - rubber
KW  - service robots
KW  - springs (mechanical)
KW  - bi-directional series elasticity
KW  - series elastic actuators
KW  - slippery inner surfaces
KW  - vertical pipes
KW  - pipe wall
KW  - multilinked pipeline inspection robots
KW  - differential elastic joint
KW  - differential elastic actuator
KW  - active joint
KW  - in-pipe inspections
KW  - Springs
KW  - Gears
KW  - Torque
KW  - Robots
KW  - Wheels
KW  - Inspection
KW  - Actuators
DO  - 10.1109/IROS.2018.8593872
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This study presents a differential elastic joint for use in multi-linked pipeline inspection robots. Active joints to stretch against the pipe wall are essential for adapting robots to use in vertical pipes and slippery inner surfaces where a large traction force is required. Series elastic actuators with a high reduction system have typically been used to sense force/torque in such applications. However, compactness, power, and bi-directional series elasticity are required to conduct in-pipe inspections. In this study, we propose an active joint using a differential elastic actuator with a rubber spring for decreasing the size and increasing the stiffness of the joint. After describing the configuration of the differential elastic actuator that is suitable for our robot and the design theory of the rubber spring cross-section, we conducted experiments to verify its torque property.
ER  - 

TY  - CONF
TI  - A Novel Design of Extended Coaxial Spherical Joint Module for a New Modular Type-Multiple DOFs Robotic Platform
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 955
EP  - 960
AU  - J. Lee
AU  - J. Noh
AU  - J. Yang
AU  - W. Yang
PY  - 2018
KW  - actuators
KW  - biomechanics
KW  - motion control
KW  - robot dynamics
KW  - robot kinematics
KW  - torque
KW  - design constraints
KW  - mechanical impedance reduction effect
KW  - E-CoSMo
KW  - extended coaxial spherical joint module
KW  - robot platform
KW  - coaxial spherical parallel mechanism
KW  - universal joint mechanism
KW  - mechanical performance
KW  - modular type-multiple DOFs robotic platform
KW  - single actuator
KW  - Conferences
KW  - Intelligent robots
DO  - 10.1109/IROS.2018.8593687
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this study, we propose an extended coaxial spherical joint module (E-CoSMo) with three to four degrees of freedom (DOFs) for a multi-DOF robot platform. The E-CoSMo consists of a coaxial spherical parallel mechanism (CSPM) with three DOFs and one extended DOF based on a universal joint mechanism (UJM) coaxially connected to the CSPM. This structure enables the application of serial link configuration (such as shoulder-elbow) with wide and universal ROMs while allowing all four actuators to be placed in the base. This makes the inertia of the moving link part to be dramatically reduced and thus contributes to decreasing the mechanical impedance of the multi-DOF robot system. In addition, through the effective design of the coaxial spherical joint module, the output rotational torque in a specific axial direction reaches approximately three times then the torque of a single actuator. To optimally implement this, we applied an optimal design approach that considers the mechanical performance and design constraints. The mechanical impedance reduction effect through the proposed module is discussed. The feasibility of the E-CoSMo is also verified through a dynamic simulation. Finally, the proposed mechanism is verified using a fabricated prototype.
ER  - 

TY  - CONF
TI  - A Novel Cable Actuation Mechanism for 2-DOF Hyper-redundant Bending Robot Composed of Pulleyless Rolling Joints
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 961
EP  - 966
AU  - J. Suh
PY  - 2018
KW  - actuators
KW  - buckling
KW  - cables (mechanical)
KW  - catheters
KW  - endoscopes
KW  - fixtures
KW  - medical robotics
KW  - motion control
KW  - production engineering computing
KW  - pulleys
KW  - redundant manipulators
KW  - surgery
KW  - three-dimensional printing
KW  - hyper-redundant bending robot composed
KW  - pulleyless rolling joint
KW  - surgical robots
KW  - wire cables
KW  - robot joints
KW  - miniature joint structure
KW  - cable driver design
KW  - 3D printing
KW  - steerable endoscopes
KW  - Joints
KW  - Mechanical cables
KW  - Pulleys
KW  - Fasteners
KW  - Robots
KW  - Muscles
KW  - Hysteresis
DO  - 10.1109/IROS.2018.8593890
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Many surgical robots are remotely actuated by means of wire cables. In the past, the cables wound around circular pulleys at the robot joints did not constitute a problem of the cable driver structure. However, the pulleys inside the joints are removed recently in order to miniaturize the joints, so a specially designed cable driver suitable for the miniature joint structure is required for stable driving. In this paper, we propose a novel cable driver design for driving a pulleyless rolling joint and extend it to 2-DOF structure. Then, the proposed cable driver is manufactured using 3D printing with the 2-DOF bending joint, and an experiment is performed to evaluate them using the prototype. The cable driver proposed in this paper can drive pulleyless rolling joints stably with low cable tension. In addition, it can decouple yaw and pitch motion of the joints completely, therefore it can be applied to a variety of thin robots and instruments including steerable endoscopes and surgical robots.
ER  - 

TY  - CONF
TI  - Design of Robotic Gripper with Constant Transmission Ratio Based on Twisted String Actuator: Concept and Evaluation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 967
EP  - 972
AU  - S. Nedelchev
AU  - I. Gaponov
AU  - J. Ryu
PY  - 2018
KW  - actuators
KW  - grippers
KW  - industrial robots
KW  - materials handling
KW  - mobile robots
KW  - robots
KW  - twisted string actuator
KW  - robotic systems
KW  - object handling
KW  - manipulation
KW  - modern engineering
KW  - robustness
KW  - gripper design
KW  - exhibits nearly-constant transmission ratio
KW  - efficient robotic gripper
KW  - practical gripper
KW  - designed device
KW  - Grippers
KW  - Force
KW  - Actuators
KW  - Kinematics
KW  - Mathematical model
KW  - Service robots
DO  - 10.1109/IROS.2018.8593794
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robotic systems for object handling and manipulation are hugely important for modern engineering and industry, with their efficiency, agility and robustness often depending on gripper design and performance. In this work, we investigate a gripper design that, when driven by a twisted string actuator, exhibits nearly-constant transmission ratio throughout its motion range. This allows for design of a highly-compact, modular and efficient robotic gripper driven by a low-power motor. We investigate kinematics of the device, experimentally verify developed models with a practical gripper testbed, and analyze transmission ratio and efficiency of the designed device. The resulting system has a nearly-constant transmission ratio of 550, with the constancy coefficient of 0.985.
ER  - 

TY  - CONF
TI  - Stopper Angle Design for a Multi-link Articulated Wheeled In-pipe Robot with Underactuated Twisting Joints
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 973
EP  - 978
AU  - Y. Oka
AU  - A. Kakogawa
AU  - S. Ma
PY  - 2018
KW  - actuators
KW  - design engineering
KW  - drives
KW  - gears
KW  - mobile robots
KW  - motion control
KW  - pipes
KW  - robot kinematics
KW  - wheels
KW  - roll joint
KW  - single actuator
KW  - drive wheel
KW  - miter-geared differential mechanism
KW  - rear wheels
KW  - joint movement
KW  - helical movement
KW  - kinematic model
KW  - multilink articulated wheeled in-pipe robot
KW  - underactuated twisting joints
KW  - stopper angle design
KW  - roll angle
KW  - Mobile robots
KW  - Wheels
KW  - Robot kinematics
KW  - Kinematics
KW  - Pipelines
KW  - Actuators
DO  - 10.1109/IROS.2018.8594208
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we present a multi-link articulated wheeled in-pipe robot that can drive the wheel and roll joint by using only a single actuator installed in each link. The proposed mechanism enables the robot to move forward or backward and helically in pipes owing to rotation of the drive wheel and twisting of the body. These two movements are generated by a miter-geared differential mechanism installed in each joint, and the magnitudes of these movements depend on the load applied to the wheels and roll joints. However, controlling of two outputs independently and aligning the rotation of the roll joints as desired are extremely challenging. Therefore, in this study, we switch those two movements by driving the rear wheels and the front wheels of the robot alternately. In addition, a stopper is used to constrain the roll joint movement. By calculating the angle of elevation of the robot's helical movement in the pipe by using a kinematic model, we can design a stopper to precisely adjust the roll angle. We verified that the robot can twist using the differential mechanism, and we validated experimentally the effectiveness of the stopper.
ER  - 

TY  - CONF
TI  - Image-Based Visual Servoing Controller for Multirotor Aerial Robots Using Deep Reinforcement Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 979
EP  - 986
AU  - C. Sampedro
AU  - A. Rodriguez-Ramos
AU  - I. Gil
AU  - L. Mejias
AU  - P. Campoy
PY  - 2018
KW  - aerospace computing
KW  - aerospace robotics
KW  - aircraft control
KW  - control engineering computing
KW  - gradient methods
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - visual servoing
KW  - deep reinforcement learning algorithm
KW  - deep deterministic policy gradients
KW  - image-based visual servoing controller
KW  - IBVS policy
KW  - linear velocity commands
KW  - multirotor aerial robots
KW  - simulated flight scenarios
KW  - Gazebo-based simulation scenario
KW  - RL-IBVS controller
KW  - Visual servoing
KW  - Reinforcement learning
KW  - Unmanned aerial vehicles
KW  - Task analysis
KW  - Detectors
KW  - Cameras
DO  - 10.1109/IROS.2018.8594249
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose a novel Image-Based Visual Servoing (IBVS) controller for multirotor aerial robots based on a recent deep reinforcement learning algorithm named Deep Deterministic Policy Gradients (DDPG). The proposed RL-IBVS controller is successfully trained in a Gazebo-based simulation scenario in order to learn the appropriate IBVS policy for directly mapping a state, based on errors in the image, to the linear velocity commands of the aerial robot. A thorough validation of the proposed controller has been conducted in simulated and real flight scenarios, demonstrating outstanding capabilities in object following applications. Moreover, we conduct a detailed comparison of the RL-IBVS controller with respect to classic and partitioned IBVS approaches.
ER  - 

TY  - CONF
TI  - Perspective Correcting Visual Odometry for Agile MAVs using a Pixel Processor Array
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 987
EP  - 994
AU  - C. Greatwood
AU  - L. Bose
AU  - T. Richardson
AU  - W. Mayol-Cuevas
AU  - J. Chen
AU  - S. J. Carey
AU  - P. Dudek
PY  - 2018
KW  - cameras
KW  - computer vision
KW  - distance measurement
KW  - image motion analysis
KW  - image sensors
KW  - sensor arrays
KW  - SCAMP-5 vision chip
KW  - Pixel Processor Array camera
KW  - visual odometry approach
KW  - agile MAVs
KW  - traditional image sensors
KW  - low frame rates
KW  - significant motion blur
KW  - motion capture system
KW  - direct comparison
KW  - PPA based approach
KW  - MAV
KW  - image alignment based odometry
KW  - perspective correction
KW  - HDR edge detection
KW  - computer vision tasks
KW  - Visual odometry
KW  - Cameras
KW  - Robot sensing systems
KW  - Arrays
KW  - Parallel processing
KW  - Visualization
KW  - Performance evaluation
DO  - 10.1109/IROS.2018.8594500
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a visual odometry approach using a Pixel Processor Array (PPA) camera, specifically, the SCAMP-5 vision chip. In this device, each pixel is capable of storing data and performing computation, enabling a variety of computer vision tasks to be carried out directly upon the sensor itself. In this work the PPA performs HDR edge detection, perspective correction and image alignment based odometry, allowing the position and heading of a MAV to be tracked at several hundred frames per second. We evaluate our PPA based approach by direct comparison with a motion capture system for a variety of trajectories. These include rapid accelerations that would incur significant motion blur at low frame rates, and lighting conditions that would typically lead to under or over exposure of image detail. Such challenging conditions would often lead to unusable images when relying on traditional image sensors.
ER  - 

TY  - CONF
TI  - C-blox: A Scalable and Consistent TSDF-based Dense Mapping Approach
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 995
EP  - 1002
AU  - A. Millane
AU  - Z. Taylor
AU  - H. Oleynikova
AU  - J. Nieto
AU  - R. Siegwart
AU  - C. Cadena
PY  - 2018
KW  - autonomous aerial vehicles
KW  - image reconstruction
KW  - image sensors
KW  - robot vision
KW  - SLAM (robots)
KW  - truncated signed distance field
KW  - TSDF subvolumes
KW  - lightweight micro aerial vehicle
KW  - scalable maps
KW  - map growth
KW  - bundle adjustment
KW  - feature-based camera tracking
KW  - dense 3D mapping
KW  - map consistency
KW  - delayed loop closure
KW  - accumulated camera tracking error
KW  - precise dense 3D maps
KW  - higher level decision making
KW  - robotic platforms
KW  - consistent dense map
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Image reconstruction
KW  - Three-dimensional displays
KW  - Robot vision systems
DO  - 10.1109/IROS.2018.8593427
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In many applications, maintaining a consistent dense map of the environment is key to enabling robotic platforms to perform higher level decision making. Several works have addressed the challenge of creating precise dense 3D maps from visual sensors providing depth information. However, during operation over longer missions, reconstructions can easily become inconsistent due to accumulated camera tracking error and delayed loop closure. Without explicitly addressing the problem of map consistency, recovery from such distortions tends to be difficult. We present a novel system for dense 3D mapping which addresses the challenge of building consistent maps while dealing with scalability. Central to our approach is the representation of the environment as a collection of overlapping Truncated Signed Distance Field (TSDF) subvolumes. These subvolumes are localized through feature-based camera tracking and bundle adjustment. Our main contribution is a pipeline for identifying stable regions in the map, and to fuse the contributing subvolumes. This approach allows us to reduce map growth while still maintaining consistency. We demonstrate the proposed system on a publicly available dataset and simulation engine, and demonstrate the efficacy of the proposed approach for building consistent and scalable maps. Finally we demonstrate our approach running in real-time onboard a lightweight Micro Aerial Vehicle (MAV).
ER  - 

TY  - CONF
TI  - Challenges of Autonomous Flight in Indoor Environments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1003
EP  - 1009
AU  - G. De Croon
AU  - C. De Wagter
PY  - 2018
KW  - aircraft navigation
KW  - autonomous aerial vehicles
KW  - Global Positioning System
KW  - indoor navigation
KW  - sensors
KW  - indoor environments
KW  - GPS
KW  - velocity estimates
KW  - global navigation systems
KW  - drone research
KW  - indoor navigation
KW  - autonomous flight
KW  - onboard sensors
KW  - Indoor environments
KW  - Drones
KW  - Robots
KW  - Measurement
KW  - Global Positioning System
KW  - Collision avoidance
KW  - Cameras
DO  - 10.1109/IROS.2018.8593704
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Indoor navigation has been a major focus of drone research over the last few decades. The main reason for the term ‚Äúindoor‚Äù came from the fact that in outdoor environments, drones could rely on global navigation systems such as GPS for their position and velocity estimates. By focusing on unknown indoor environments, the research had to focus on solutions using onboard sensors and processing. In this article, we present an overview of the state of the art and remaining challenges in this area, with a focus on small drones.
ER  - 

TY  - CONF
TI  - A Deep Reinforcement Learning Technique for Vision-Based Autonomous Multirotor Landing on a Moving Platform
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1010
EP  - 1017
AU  - A. Rodriguez-Ramos
AU  - C. Sampedro
AU  - H. Bavle
AU  - I. G. Moreno
AU  - P. Campoy
PY  - 2018
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - continuous systems
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mobile robots
KW  - motion control
KW  - neurocontrollers
KW  - robot vision
KW  - state-space methods
KW  - deep learning techniques
KW  - deep deterministic policy gradients algorithm
KW  - motion control
KW  - deep Q- learning
KW  - active domain
KW  - robotics-related tasks
KW  - multirotor control
KW  - attitude control
KW  - state space
KW  - continuous action space
KW  - deep reinforcement learning technique
KW  - vision-based autonomous multirotor landing maneuver
KW  - continuous state
KW  - continuous action domain
KW  - moving platform
KW  - Reinforcement learning
KW  - Unmanned aerial vehicles
KW  - Robots
KW  - Cameras
KW  - Aerospace electronics
KW  - Neural networks
KW  - Task analysis
DO  - 10.1109/IROS.2018.8594472
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Deep learning techniques for motion control have recently been qualitatively improved, since the successful application of Deep Q- Learning to the continuous action domain in Atari-like games. Based on these ideas, Deep Deterministic Policy Gradients (DDPG) algorithm was able to provide impressive results in continuous state and action domains, which are closely linked to most of the robotics-related tasks. In this paper, a vision-based autonomous multirotor landing maneuver on top of a moving platform is presented. The behaviour has been completely learned in simulation without prior human knowledge and by means of deep reinforcement learning techniques. Since the multirotor is controlled in attitude, no high level state estimation is required. The complete behaviour has been trained with continuous action and state spaces, and has provided proper results (landing at a maximum velocity of 2 m/s), Furthermore, it has been validated in a wide variety of conditions, for both simulated and real-flight scenarios, using a low-cost, lightweight and out-of-the-box consumer multirotor.
ER  - 

TY  - CONF
TI  - Stereo Visual Odometry and Semantics based Localization of Aerial Robots in Indoor Environments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1018
EP  - 1023
AU  - H. Bavle
AU  - S. Manthe
AU  - P. de la Puente
AU  - A. Rodriguez-Ramos
AU  - C. Sampedro
AU  - P. Campoy
PY  - 2018
KW  - distance measurement
KW  - image colour analysis
KW  - image segmentation
KW  - indoor environment
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - object detection
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - indoor environments
KW  - particle filter localization approach
KW  - semantic information
KW  - mini-aerial robots
KW  - stereo VO algorithm
KW  - semantic measurements
KW  - pre-trained deep learning based object detector
KW  - 3D point clouds
KW  - visual SLAM approach
KW  - stereo visual odometry
KW  - semantics based localization
KW  - DL
KW  - RGB spectrum
KW  - drift free pose estimation
KW  - Semantics
KW  - Three-dimensional displays
KW  - Unmanned aerial vehicles
KW  - Robots
KW  - Atmospheric measurements
KW  - Particle measurements
KW  - Prediction algorithms
DO  - 10.1109/IROS.2018.8593426
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper we propose a particle filter localization approach, based on stereo visual odometry (VO) and semantic information from indoor environments, for mini-aerial robots. The prediction stage of the particle filter is performed using the 3D pose of the aerial robot estimated by the stereo VO algorithm. This predicted 3D pose is updated using inertial as well as semantic measurements. The algorithm processes semantic measurements in two phases; firstly, a pre-trained deep learning (DL) based object detector is used for real time object detections in the RGB spectrum. Secondly, from the corresponding 3D point clouds of the detected objects, we segment their dominant horizontal plane and estimate their relative position, also augmenting a prior map with new detections. The augmented map is then used in order to obtain a drift free pose estimate of the aerial robot. We validate our approach in several real flight experiments where we compare it against ground truth and a state of the art visual SLAM approach.
ER  - 

TY  - CONF
TI  - Laser-Based Reactive Navigation for Multirotor Aerial Robots using Deep Reinforcement Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1024
EP  - 1031
AU  - C. Sampedro
AU  - H. Bavle
AU  - A. Rodriguez-Ramos
AU  - P. de la Puente
AU  - P. Campoy
PY  - 2018
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - traditional motion planning algorithms
KW  - precise maps
KW  - fast reactive navigation algorithm
KW  - multirotor aerial robots
KW  - 2D-laser range measurements
KW  - Gazebo-based simulation scenario
KW  - artificial potential field formulation
KW  - laser-based reactive navigation
KW  - collision avoidance capabilities
KW  - reactive navigation behavior
KW  - deep reinforcement learning
KW  - dynamic obstacles
KW  - static obstacles
KW  - Navigation
KW  - Robots
KW  - Unmanned aerial vehicles
KW  - Lasers
KW  - Heuristic algorithms
KW  - Reinforcement learning
KW  - Planning
DO  - 10.1109/IROS.2018.8593706
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Navigation in unknown indoor environments with fast collision avoidance capabilities is an ongoing research topic. Traditional motion planning algorithms rely on precise maps of the environment, where re-adapting a generated path can be highly demanding in terms of computational cost. In this paper, we present a fast reactive navigation algorithm using Deep Reinforcement Learning applied to multi rotor aerial robots. Taking as input the 2D-laser range measurements and the relative position of the aerial robot with respect to the desired goal, the proposed algorithm is successfully trained in a Gazebo-based simulation scenario by adopting an artificial potential field formulation. A thorough evaluation of the trained agent has been carried out both in simulated and real indoor scenarios, showing the appropriate reactive navigation behavior of the agent in the presence of static and dynamic obstacles.
ER  - 

TY  - CONF
TI  - Drone Detection Using Depth Maps
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1034
EP  - 1037
AU  - A. Carrio
AU  - S. Vemprala
AU  - A. Ripoll
AU  - S. Saripalli
AU  - P. Campoy
PY  - 2018
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - static obstacle avoidance
KW  - dynamic objects
KW  - field-of-view requirements
KW  - on-board small UAVs
KW  - relative altitude
KW  - azimuth
KW  - depth map-based approach
KW  - collision avoidance
KW  - depth map sequences
KW  - unmanned aerial vehicle navigation
KW  - collision-free path planning
KW  - FOV
KW  - deep learning-based drone detection model
KW  - sensing technologies
KW  - 3D localization
KW  - Drones
KW  - Cameras
KW  - Three-dimensional displays
KW  - Atmospheric modeling
KW  - Sensors
KW  - Neural networks
KW  - Two dimensional displays
DO  - 10.1109/IROS.2018.8593405
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Obstacle avoidance is a key feature for safe Unmanned Aerial Vehicle (UAV) navigation. While solutions have been proposed for static obstacle avoidance, systems enabling avoidance of dynamic objects, such as drones, are hard to implement due to the detection range and field-of-view (FOV) requirements, as well as the constraints for integrating such systems on-board small UAVs. In this work, a dataset of 6k synthetic depth maps of drones has been generated and used to train a state-of-the-art deep learning-based drone detection model. While many sensing technologies can only provide relative altitude and azimuth of an obstacle, our depth map-based approach enables full 3D localization of the obstacle. This is extremely useful for collision avoidance, as 3D localization of detected drones is key to perform efficient collision-free path planning. The proposed detection technique has been validated in several real depth map sequences, with multiple types of drones flying at up to 2 m/s, achieving an average precision of 98.7 %, an average recall of 74.7 % and a record detection range of 9.5 meters.
ER  - 

TY  - CONF
TI  - Real-Time Dance Generation to Music for a Legged Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1038
EP  - 1044
AU  - T. Bi
AU  - P. Fankhauser
AU  - D. Bellicoso
AU  - M. Hutter
PY  - 2018
KW  - feedback
KW  - feedforward
KW  - humanoid robots
KW  - image motion analysis
KW  - legged locomotion
KW  - Markov processes
KW  - motion control
KW  - music
KW  - robot vision
KW  - synchronisation
KW  - music tempo
KW  - dance generation
KW  - feedforward delay controller
KW  - Markov chain
KW  - quadrupedal robot
KW  - robot whole-body controller reference input
KW  - feedback delay controller
KW  - time-shifting
KW  - delays
KW  - picked dance motion
KW  - base motions
KW  - stepping motions
KW  - dance motions
KW  - user-generated dance motion library
KW  - dance choreography
KW  - onboard microphone
KW  - live music
KW  - external stimuli
KW  - legged robot
KW  - Robot kinematics
KW  - Legged locomotion
KW  - Music
KW  - Delays
KW  - Trajectory
KW  - Real-time systems
DO  - 10.1109/IROS.2018.8593983
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The development of robots that can dance has received considerable attention. However, they are often either limited to a pre-defined set of movements and music or demonstrate little variance when reacting to external stimuli, such as microphone or camera input. In this paper, we contribute with a novel approach allowing a legged robot to listen to live music while dancing in synchronization with the music in a diverse fashion. This is achieved by extracting the beat from an onboard microphone in real-time, and subsequently creating a dance choreography by picking from a user-generated dance motion library at every new beat. Dance motions include various stepping and base motions. The process of picking from the library is defined by a probabilistic model, namely a Markov chain, that depends on the previously picked dance motion and the current music tempo. Finally, delays are determined online by time-shifting a measured signal and a reference signal, and minimizing the least squares error with the time-shift as parameter. Delays are then compensated for by using a combined feedforward and feedback delay controller which shifts the robot whole-body controller reference input in time. Results from experiments on a quadrupedal robot demonstrate the fast convergence and synchrony to the perceived music.
ER  - 

TY  - CONF
TI  - Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure from Motion
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1045
EP  - 1052
AU  - X. Liu
AU  - S. W. Chen
AU  - S. Aditya
AU  - N. Sivakumar
AU  - S. Dcunha
AU  - C. Qu
AU  - C. J. Taylor
AU  - J. Das
AU  - V. Kumar
PY  - 2018
KW  - cameras
KW  - computer vision
KW  - feature extraction
KW  - image classification
KW  - image colour analysis
KW  - image motion analysis
KW  - image segmentation
KW  - image sequences
KW  - Kalman filters
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - video signal processing
KW  - Motion algorithm
KW  - double counted fruit tracks
KW  - ground-truth human-annotated visual counts
KW  - fruit counting pipeline
KW  - fruit counting pipeline
KW  - tracking process
KW  - Kanade-Lucas-Tomasi Tracker
KW  - Hungarian Algorithm
KW  - nonfruit pixels
KW  - segment video frame images
KW  - image streams
KW  - pipeline works
KW  - visible fruits
KW  - frame tracking
KW  - deep segmentation
KW  - deep learning
KW  - robust fruit counting
KW  - counting accuracy
KW  - image sequences
KW  - Image segmentation
KW  - Tracking
KW  - Three-dimensional displays
KW  - Pipelines
KW  - Image sequences
KW  - Deep learning
KW  - Cameras
DO  - 10.1109/IROS.2018.8594239
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present a novel fruit counting pipeline that combines deep segmentation, frame to frame tracking, and 3D localization to accurately count visible fruits across a sequence of images. Our pipeline works on image streams from a monocular camera, both in natural light, as well as with controlled illumination at night. We first train a Fully Convolutional Network (FCN) and segment video frame images into fruit and non-fruit pixels. We then track fruits across frames using the Hungarian Algorithm where the objective cost is determined from a Kalman Filter corrected Kanade-Lucas-Tomasi (KLT) Tracker. In order to correct the estimated count from tracking process, we combine tracking results with a Structure from Motion (SfM) algorithm to calculate relative 3D locations and size estimates to reject outliers and double counted fruit tracks. We evaluate our algorithm by comparing with ground-truth human-annotated visual counts. Our results demonstrate that our pipeline is able to accurately and reliably count fruits across image sequences, and the correction step can significantly improve the counting accuracy and robustness. Although discussed in the context of fruit counting, our work can extend to detection, tracking, and counting of a variety of other stationary features of interest such as leaf-spots, wilt, and blossom.
ER  - 

TY  - CONF
TI  - Towards View-Invariant Intersection Recognition from Videos using Deep Network Ensembles
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1053
EP  - 1060
AU  - A. Kumar
AU  - G. Gupta
AU  - A. Sharma
AU  - K. M. Krishna
PY  - 2018
KW  - data mining
KW  - image recognition
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object recognition
KW  - video signal processing
KW  - recognition accuracy
KW  - road segments
KW  - LSTM based Siamese style deep network
KW  - meeting point
KW  - deep network ensembles
KW  - videos
KW  - view-invariant intersection recognition
KW  - video recognition
KW  - Videos
KW  - Trajectory
KW  - Visualization
KW  - Task analysis
KW  - Roads
KW  - Image recognition
KW  - Training
DO  - 10.1109/IROS.2018.8594449
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper strives to answer the following question: Is it possible to recognize an intersection when seen from different road segments that constitute the intersection? An intersection or a junction typically is a meeting point of three or four road segments. Its recognition from a road segment that is transverse to or 180 degrees apart from its previous sighting is an extremely challenging and yet a very relevant problem to be addressed from the point of view of both autonomous driving as well as loop detection. This paper formulates this as a problem of video recognition and proposes a novel LSTM based Siamese style deep network for video recognition. For what is indeed a challenging problem and the limited annotated dataset available we show competitive results of recognizing intersections when approached from diverse viewpoints or road segments. Specifically, we tabulate effective recognition accuracy even as the approaches to the intersection being compared are disparate both in terms of viewpoints and weather/illumination conditions. We show competitive results on both synthetic yet highly realistic data mined from the gaming platform GTA as well as on real world data made available through Mapillary.
ER  - 

TY  - CONF
TI  - Semantically Meaningful View Selection
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1061
EP  - 1066
AU  - J. Gu√©rin
AU  - O. Gibaru
AU  - E. Nyiri
AU  - S. Thieryl
AU  - B. Boots
PY  - 2018
KW  - feature extraction
KW  - image classification
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - object recognition
KW  - pattern clustering
KW  - pose estimation
KW  - robot vision
KW  - meaningful view selection
KW  - high-level abstract tasks
KW  - lower-level concrete tasks
KW  - deep learning
KW  - image understanding
KW  - object recognition
KW  - robot sorting tasks
KW  - fixed top-down view
KW  - viewing angle
KW  - semantically informative view
KW  - semantic view selection
KW  - semantic knowledge
KW  - observed object
KW  - image dataset
KW  - semantic score
KW  - view image
KW  - camera
KW  - Cameras
KW  - Semantics
KW  - Robot vision systems
KW  - Task analysis
KW  - Feature extraction
KW  - Measurement
DO  - 10.1109/IROS.2018.8593524
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - An understanding of the nature of objects could help robots to solve both high-level abstract tasks and improve performance at lower-level concrete tasks. Although deep learning has facilitated progress in image understanding, a robot's performance in problems like object recognition often depends on the angle from which the object is observed. Traditionally, robot sorting tasks rely on a fixed top-down view of an object. By changing its viewing angle, a robot can select a more semantically informative view leading to better performance for object recognition. In this paper, we introduce the problem of semantic view selection, which seeks to find good camera poses to gain semantic knowledge about an observed object. We propose a conceptual formulation of the problem, together with a solvable relaxation based on clustering. We then present a new image dataset consisting of around 10k images representing various views of 144 objects under different poses. Finally we use this dataset to propose a first solution to the problem by training a neural network to predict a ‚Äúsemantic score‚Äù from a top view image and camera pose. The views predicted to have higher scores are then shown to provide better clustering results than fixed top-down views.
ER  - 

TY  - CONF
TI  - Distributed Deep Reinforcement Learning for Fighting Forest Fires with a Network of Aerial Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1067
EP  - 1074
AU  - R. N. Haksar
AU  - M. Schwager
PY  - 2018
KW  - aerospace control
KW  - autonomous aerial vehicles
KW  - dynamic programming
KW  - fires
KW  - learning (artificial intelligence)
KW  - Markov processes
KW  - Monte Carlo methods
KW  - optimal control
KW  - rescue robots
KW  - distributed deep reinforcement learning based strategy
KW  - UAVs
KW  - Markov decision process
KW  - deep RL approach
KW  - deep RL policy
KW  - forest sizes
KW  - simulated forest fire
KW  - unmanned aerial vehicles
KW  - aerial robots
KW  - Vegetation
KW  - Forestry
KW  - Sensors
KW  - Retardants
KW  - Monitoring
KW  - Lattices
KW  - Unmanned aerial vehicles
DO  - 10.1109/IROS.2018.8593539
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper proposes a distributed deep reinforcement learning (RL) based strategy for a team of Unmanned Aerial Vehicles (UAVs) to autonomously fight forest fires. We first model the forest fire as a Markov decision process (MDP) with a factored structure. We consider optimally controlling the forest fire without agents using dynamic programming, and show any exact solution and many approximate solutions are computationally intractable. Given the problem complexity, we consider a deep RL approach in which each agent learns a policy requiring only local information. We show with Monte Carlo simulations that the deep RL policy outperforms a hand-tuned heuristic, and scales well for various forest sizes and different numbers of UAVs as well as variations in model parameters. Experimental demonstrations with mobile robots fighting a simulated forest fire in the Robotarium at the Georgia Institute of Technology are also presented.
ER  - 

TY  - CONF
TI  - Tree Species Identification from Bark Images Using Convolutional Neural Networks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1075
EP  - 1081
AU  - M. Carpentier
AU  - P. Gigu√®re
AU  - J. Gaudreault
PY  - 2018
KW  - feature extraction
KW  - forestry
KW  - geophysical image processing
KW  - image classification
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - vegetation mapping
KW  - bark images
KW  - tree individual number
KW  - high-resolution bark images
KW  - species recognition
KW  - tree diameters
KW  - tree bark species classification
KW  - standard vision problems
KW  - deep learning
KW  - forestry related tasks
KW  - convolutional neural networks
KW  - tree species identification
KW  - Vegetation
KW  - Forestry
KW  - Deep learning
KW  - Feature extraction
KW  - Training
KW  - Cameras
KW  - Task analysis
DO  - 10.1109/IROS.2018.8593514
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Tree species identification using bark images is a challenging problem that could prove useful for many forestry related tasks. However, while the recent progress in deep learning showed impressive results on standard vision problems, a lack of datasets prevented its use on tree bark species classification. In this work, we present, and make publicly available, a novel dataset called BarkNet 1.0 containing more than 23,000 high-resolution bark images from 23 different tree species over a wide range of tree diameters. With it, we demonstrate the feasibility of species recognition through bark images, using deep learning. More specifically, we obtain an accuracy of 93.88% on single crop, and an accuracy of 97.81% using a majority voting approach on all of the images of a tree. We also empirically demonstrate that, for a fixed number of images, it is better to maximize the number of tree individuals in the training database, thus directing future data collection efforts.
ER  - 

TY  - CONF
TI  - UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1082
EP  - 1088
AU  - V. Madhu Babu
AU  - K. Das
AU  - A. Majumdar
AU  - S. Kumar
PY  - 2018
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image reconstruction
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - motion estimation
KW  - pose estimation
KW  - stereo image processing
KW  - unsupervised learning
KW  - unsupervised deep network
KW  - ego-motion estimation
KW  - unsupervised visual odometry system
KW  - monocular view
KW  - objective function
KW  - temporally alligned sequences
KW  - monocular images
KW  - disparity-based depth estimation network
KW  - dense depth map
KW  - UnDEMoN
KW  - binocular stereo image pairs
KW  - temporal reconstruction losses
KW  - pose estimation network
KW  - 6DoF camera pose estimation
KW  - Image reconstruction
KW  - Cameras
KW  - Pose estimation
KW  - Training
KW  - Meters
KW  - Linear programming
DO  - 10.1109/IROS.2018.8593864
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a deep network based unsupervised visual odometry system for 6-DoF camera pose estimation and finding dense depth map for its monocular view. The proposed network is trained using unlabeled binocular stereo image pairs and is shown to provide superior performance in depth and ego-motion estimation compared to the existing state-of-the-art. This is achieved by introducing a novel objective function and training the network using temporally alligned sequences of monocular images. The objective function is based on the Charbonnier penalty applied to spatial and bi-directional temporal reconstruction losses. The overall novelty of the approach lies in the fact that the proposed deep framework combines a disparity-based depth estimation network with a pose estimation network to obtain absolute scale-aware 6-DoF camera pose and superior depth map. According to our knowledge, such a framework with complete unsupervised end-to-end learning has not been tried so far, making it a novel contribution in the field. The effectiveness of the approach is demonstrated through performance comparison with the state-of-the-art methods on KITTI driving dataset.
ER  - 

TY  - CONF
TI  - Leveraging Convolutional Pose Machines for Fast and Accurate Head Pose Estimation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1089
EP  - 1094
AU  - Y. Cao
AU  - O. Can√©vet
AU  - J. Odobez
PY  - 2018
KW  - face recognition
KW  - feature extraction
KW  - feedforward neural nets
KW  - learning (artificial intelligence)
KW  - multilayer perceptrons
KW  - object detection
KW  - pose estimation
KW  - appearance information
KW  - keypoint relationships
KW  - convolutional neural networks
KW  - multilayer perceptrons
KW  - keypoint detection model
KW  - CPM
KW  - head pose estimation
KW  - facial keypoint features
KW  - estimation framework
KW  - convolutional pose machines
KW  - Magnetic heads
KW  - Pose estimation
KW  - Face
KW  - Feature extraction
KW  - Nose
KW  - Ear
DO  - 10.1109/IROS.2018.8594223
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose a head pose estimation framework that leverages on a recent keypoint detection model. More specifically, we apply the convolutional pose machines (CPMs) to input images, extract different types of facial keypoint features capturing appearance information and keypoint relationships, and train multilayer perceptrons (MLPs) and convolutional neural networks (CNNs) for head pose estimation. The benefit of leveraging on the CPMs (which we apply anyway for other purposes like tracking) is that we can design highly efficient models for practical usage. We evaluate our approach on the Annotated Facial Landmarks in the Wild (AFLW) dataset and achieve competitive results with the state-of-the-art.
ER  - 

TY  - CONF
TI  - Conceptualization of Object Compositions Using Persistent Homology
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1095
EP  - 1102
AU  - C. A. Mueller
AU  - A. Birk
PY  - 2018
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - shape recognition
KW  - topology
KW  - topological shape analysis
KW  - shape commonalities
KW  - spatial topology analysis
KW  - point cloud segment constellations
KW  - description space
KW  - object segment decompositions
KW  - persistent homology
KW  - Shape
KW  - Three-dimensional displays
KW  - Visualization
KW  - Topology
KW  - Dictionaries
KW  - Prototypes
KW  - Training
DO  - 10.1109/IROS.2018.8594516
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - A topological shape analysis is proposed and utilized to learn concepts that reflect shape commonalities. Our approach is two-fold: i) a spatial topology analysis of point cloud segment constellations within objects. Therein constellations are decomposed and described in an hierarchical manner - from single segments to segment groups until a single group reflects an entire object. ii) a topology analysis of the description space in which segment decompositions are exposed in. Inspired by Persistent Homology, hidden groups of shape commonalities are revealed from object segment decompositions. Experiments show that extracted persistent groups of commonalities can represent semantically meaningful shape concepts. We also show the generalization capability of the proposed approach considering samples of external datasets.
ER  - 

TY  - CONF
TI  - Kitting in the Wild through Online Domain Adaptation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1103
EP  - 1109
AU  - M. Mancini
AU  - H. Karaoguz
AU  - E. Ricci
AU  - P. Jensfelt
AU  - B. Caputo
PY  - 2018
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - robot vision
KW  - visual perception
KW  - online adaptation algorithm
KW  - standard domain adaptation algorithms
KW  - batch-normalization layers
KW  - deep models
KW  - robot visual recognition algorithms
KW  - standard object recognition datasets
KW  - visual dataset
KW  - robotic kitting
KW  - vision systems
KW  - online domain adaptation
KW  - Robots
KW  - Adaptation models
KW  - Task analysis
KW  - Training
KW  - Data models
KW  - Visualization
KW  - Standards
DO  - 10.1109/IROS.2018.8593862
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Technological developments call for increasing perception and action capabilities of robots. Among other skills, vision systems that can adapt to any possible change in the working conditions are needed. Since these conditions are unpredictable, we need benchmarks which allow to assess the generalization and robustness capabilities of our visual recognition algorithms. In this work we focus on robotic kitting in unconstrained scenarios. As a first contribution, we present a new visual dataset for the kitting task. Differently from standard object recognition datasets, we provide images of the same objects acquired under various conditions where camera, illumination and background are changed. This novel dataset allows for testing the robustness of robot visual recognition algorithms to a series of different domain shifts both in isolation and unified. Our second contribution is a novel online adaptation algorithm for deep models, based on batch-normalization layers, which allows to continuously adapt a model to the current working conditions. Differently from standard domain adaptation algorithms, it does not require any image from the target domain at training time. We benchmark the performance of the algorithm on the proposed dataset, showing its capability to fill the gap between the performances of a standard architecture and its counterpart adapted offline to the given target domain.
ER  - 

TY  - CONF
TI  - CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1110
EP  - 1117
AU  - G. Iyer
AU  - R. K. Ram
AU  - J. K. Murthy
AU  - K. M. Krishna
PY  - 2018
KW  - calibration
KW  - cameras
KW  - image processing
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - optical radar
KW  - extrinsic calibration parameters
KW  - underlying geometric problem
KW  - photometric consistency
KW  - geometric consistency
KW  - camera calibration matrix K
KW  - LiDAR point cloud
KW  - calibration efforts
KW  - rigid body transformation
KW  - geometrically supervised deep network capable
KW  - calibration targets
KW  - calibration techniques
KW  - meaningful data
KW  - sensor rig
KW  - 3D LiDAR
KW  - 3D spatial transformer networks
KW  - geometrically supervised extrinsic calibration
KW  - Calibration
KW  - Three-dimensional displays
KW  - Cameras
KW  - Laser radar
KW  - Robot sensing systems
KW  - Training
KW  - Two dimensional displays
DO  - 10.1109/IROS.2018.8593693
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - 3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a geometrically supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet.
ER  - 

TY  - CONF
TI  - Compact & Comprehensive Canonical Appearances Discovered Autonomously
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1118
EP  - 1123
AU  - K. T√ºrksoy
AU  - H. I·π£ll Bozma
PY  - 2018
KW  - decision making
KW  - image representation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - exploration approach
KW  - autonomous ground robot
KW  - depth sensor
KW  - bubble space representation
KW  - exploration path length
KW  - topological mapping
KW  - canonical appearances
KW  - appearance-based learning
KW  - Robot sensing systems
KW  - Decision making
KW  - Robot kinematics
KW  - Cognition
KW  - Lasers
KW  - Measurement
DO  - 10.1109/IROS.2018.8593544
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents an exploration approach for discovering canonical appearances in unknown environments using an autonomous ground robot equipped with a depth sensor. This approach is based on the previously proposed two-stage algorithm that alternates between local and global decision making for efficient topological mapping based on bubble space representation. Differing from it, the approach aims to identify vantage viewpoints with characterizing views for subsequent appearance-based learning as well as achieving complete coverage. This is demonstrated by a series of experiments using an outdoor benchmark data set including a comparative study with evaluation metrics including the exploration path length and number of canonical appearances discovered.
ER  - 

TY  - CONF
TI  - Deep Learning for Exploration and Recovery of Uncharted and Dynamic Targets from UAV-like Vision
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1124
EP  - 1131
AU  - W. Andrew
AU  - C. Greatwood
AU  - T. Burghardt
PY  - 2018
KW  - autonomous aerial vehicles
KW  - convolutional neural nets
KW  - image classification
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - probability
KW  - random processes
KW  - robot vision
KW  - target tracking
KW  - online search tasks
KW  - multitarget environments
KW  - dynamic targets
KW  - UAV-like vision
KW  - deep learning
KW  - dynamic search
KW  - strategic explorational agency
KW  - single deep network
KW  - navigational actions
KW  - dual-stream classification paradigm
KW  - sensory processing
KW  - agent location
KW  - static evolutions
KW  - dynamic evolutions
KW  - probabilistic placement
KW  - fully random target walks
KW  - herd-inspired behaviours
KW  - dual-stream architecture
KW  - unmanned aerial vehicle
KW  - convolutional neural network
KW  - multitarget behaviour classes
KW  - optimal navigational decision samples
KW  - long term map memory
KW  - Navigation
KW  - Robot sensing systems
KW  - Task analysis
KW  - History
KW  - Visualization
KW  - Vehicle dynamics
KW  - Reinforcement learning
DO  - 10.1109/IROS.2018.8593751
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper discusses deep learning for solving static and dynamic search and recovery tasks - such as the retrieval of all instances of actively moving targets - based on partial-view Unmanned Aerial Vehicle (UAV)-like sensing. In particular, we demonstrate that abstracted tactic and strategic explorational agency can be implemented effectively via a single deep network that optimises in unity: the mapping of sensory inputs and positional history towards navigational actions. We propose a dual-stream classification paradigm that integrates one Convolutional Neural Network (CNN) for sensory processing with a second one for interpreting an evolving longterm map memory. In order to learn effective search behaviours given agent location and agent-centric sensory inputs, we train this design against 400k+ optimal navigational decision samples from each set of static and dynamic evolutions for different multi-target behaviour classes. We quantify recovery performance across an extensive range of scenarios; including probabilistic placement and dynamics, as well as fully random target walks and herd-inspired behaviours. Detailed results comparisons show that our design can outperform naive, independent stream and off-the-shelf DRQN solutions. We conclude that the proposed dual-stream architecture can provide a unified, rationally motivated and effective architecture for solving online search tasks in dynamic, multi-target environments. With this paper we publish3 key source code and associated models.
ER  - 

TY  - CONF
TI  - Hybrid Multi-camera Visual Servoing to Moving Target
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1132
EP  - 1137
AU  - H. Cuevas-Velasquez
AU  - N. Li
AU  - R. Tylecek
AU  - M. Saval-Calvo
AU  - R. B. Fisher
PY  - 2018
KW  - cameras
KW  - image sensors
KW  - position control
KW  - robot vision
KW  - stereo image processing
KW  - tracking
KW  - visual servoing
KW  - hybrid multicamera visual servoing
KW  - moving target
KW  - robotics
KW  - multiple visual sources
KW  - visual servoing approach
KW  - hybrid multicamera input data
KW  - robot arm
KW  - RGBD sensors
KW  - arm-mounted stereo camera
KW  - Eye-in-Hand
KW  - EtoH cameras
KW  - EinH sensor
KW  - EtoH sensors
KW  - adaptive visual input data
KW  - eye-to-hand visual input
KW  - Three-dimensional displays
KW  - Cameras
KW  - Visualization
KW  - Robot vision systems
DO  - 10.1109/IROS.2018.8593652
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Visual servoing is a well-known task in robotics. However, there are still challenges when multiple visual sources are combined to accurately guide the robot or occlusions appear. In this paper we present a novel visual servoing approach using hybrid multi-camera input data to lead a robot arm accurately to dynamically moving target points in the presence of partial occlusions. The approach uses four RGBD sensors as Eye-to-Hand (EtoH) visual input, and an arm-mounted stereo camera as Eye-in-Hand (EinH). A Master supervisor task selects between using the EtoH or the EinH, depending on the distance between the robot and target. The Master also selects the subset of EtoH cameras that best perceive the target. When the EinH sensor is used, if the target becomes occluded or goes out of the sensor's view-frustum, the Master switches back to the EtoH sensors to re-track the object. Using this adaptive visual input data, the robot is then controlled using an iterative planner that uses position, orientation and joint configuration to estimate the trajectory. Since the target is dynamic, this trajectory is updated every time-step. Experiments show good performance in four different situations: tracking a ball, targeting a bulls-eye, guiding a straw to a mouth and delivering an item to a moving hand. The experiments cover both simple situations such as a ball that is mostly visible from all cameras, and more complex situations such as the mouth which is partially occluded from some of the sensors.
ER  - 

TY  - CONF
TI  - Detecting and Picking of Folded Objects with a Multiple Sensor Integrated Robot Hand
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1138
EP  - 1145
AU  - S. Hasegawa
AU  - K. Wada
AU  - K. Okada
AU  - M. Inaba
PY  - 2018
KW  - control engineering computing
KW  - dexterous manipulators
KW  - image recognition
KW  - object detection
KW  - pressure sensors
KW  - robot vision
KW  - folded object
KW  - robotic picking
KW  - Suction Pinching Hand
KW  - proximity sensors
KW  - multiple sensor integrated robot hand
KW  - trial-and-error picking system
KW  - suction grasp
KW  - flex sensors
KW  - air pressure sensor
KW  - image recognition
KW  - Robot sensing systems
KW  - Uncertainty
KW  - Image recognition
KW  - Grippers
KW  - Hardware
DO  - 10.1109/IROS.2018.8593398
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robotic picking of folded objects such as books is required for picking various objects. As a folded object is easily unfolded, it is difficult to carry it stably and place it in a desired pose due to its dangling part. For overcoming this difficulty, we propose a trial-and-error picking system using our Suction Pinching Hand, which can push the dangling part up with pinch grasp until the object lifted with suction grasp is folded. That system utilizes proximity sensors on the hand to predict whether folding will succeed with a current hand pose and decide whether to retry with another pose. Also, proximity sensors, flex sensors and an air pressure sensor are used to deal with uncertainty of the image recognition, the hand hardware and suction grasp. We evaluate our proposed system with experiments of picking and placing folded objects. It is confirmed that our proposed system realizes picking with the ability of our Suction Pinching Hand to carry folded objects stably and place them in desired poses. It is also proved that our proposed system is robust against the uncertainty.
ER  - 

TY  - CONF
TI  - Information Sparsification in Visual-Inertial Odometry
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1146
EP  - 1153
AU  - J. Hsiung
AU  - M. Hsiao
AU  - E. Westman
AU  - R. Valencia
AU  - M. Kaess
PY  - 2018
KW  - computational complexity
KW  - distance measurement
KW  - graph theory
KW  - mobile robots
KW  - SLAM (robots)
KW  - information sparsification
KW  - tightly couple visual measurements
KW  - inertial measurements
KW  - fixed-lag visual-inertial odometry framework
KW  - bound computational complexity
KW  - fixed-lag smoothers
KW  - densely connected linear
KW  - information-theoretic perspective
KW  - dense marginalization step
KW  - information content
KW  - nonlinear factor graph
KW  - information loss
KW  - information sparsity
KW  - VIO methods
KW  - EuRoC visual-inertial dataset
KW  - structural similarity
KW  - nonlinearity
KW  - computational complexity
KW  - Optimization
KW  - Markov processes
KW  - Microsoft Windows
KW  - Computational complexity
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Visualization
DO  - 10.1109/IROS.2018.8594007
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we present a novel approach to tightly couple visual and inertial measurements in a fixed-lag visual-inertial odometry (VIO) framework using information sparsification. To bound computational complexity, fixed-lag smoothers typically marginalize out variables, but consequently introduce a densely connected linear prior which significantly deteriorates accuracy and efficiency. Current state-of-the-art approaches account for the issue by selectively discarding measurements and marginalizing additional variables. However, such strategies are sub-optimal from an information-theoretic perspective. Instead, our approach performs a dense marginalization step and preserves the information content of the dense prior. Our method sparsifies the dense prior with a nonlinear factor graph by minimizing the information loss. The resulting factor graph maintains information sparsity, structural similarity, and nonlinearity. To validate our approach, we conduct real-time drone tests and perform comparisons to current state-of-the-art fixed-lag VIO methods in the EuRoC visual-inertial dataset. The experimental results show that the proposed method achieves competitive and superior accuracy in almost all trials. We include a detailed run-time analysis to demonstrate that the proposed algorithm is suitable for real-time applications.
ER  - 

TY  - CONF
TI  - Towards Robust Visual Odometry with a Multi-Camera System
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1154
EP  - 1161
AU  - P. Liu
AU  - M. Geppert
AU  - L. Heng
AU  - T. Sattler
AU  - A. Geiger
AU  - M. Pollefeys
PY  - 2018
KW  - cameras
KW  - distance measurement
KW  - image sampling
KW  - minimisation
KW  - photometry
KW  - pose estimation
KW  - position measurement
KW  - stereo image processing
KW  - robust visual odometry algorithm
KW  - robust VO algorithm
KW  - current pose tracker estimation
KW  - photometric error minimisation
KW  - plane-sweeping stereo cameras
KW  - near-infrared illumination
KW  - NIR illumination
KW  - single stereo configuration
KW  - multicamera setup
KW  - sliding window optimizer
KW  - sampled feature points
KW  - local mapper
KW  - multicamera system
KW  - Cameras
KW  - Tracking
KW  - Lighting
KW  - Visual odometry
KW  - Robot vision systems
KW  - Robustness
KW  - Simultaneous localization and mapping
DO  - 10.1109/IROS.2018.8593561
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present a visual odometry (VO) algorithm for a multi-camera system and robust operation in challenging environments. Our algorithm consists of a pose tracker and a local mapper. The tracker estimates the current pose by minimizing photometric errors between the most recent keyframe and the current frame. The mapper initializes the depths of all sampled feature points using plane-sweeping stereo. To reduce pose drift, a sliding window optimizer is used to refine poses and structure jointly. Our formulation is flexible enough to support an arbitrary number of stereo cameras. We evaluate our algorithm thoroughly on five datasets. The datasets were captured in different conditions: daytime, night-time with near-infrared (NIR) illumination and nighttime without NIR illumination. Experimental results show that a multi-camera setup makes the VO more robust to challenging environments, especially night-time conditions, in which a single stereo configuration fails easily due to the lack of features.
ER  - 

TY  - CONF
TI  - Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1162
EP  - 1167
AU  - P. Yin
AU  - L. Xu
AU  - Z. Liu
AU  - L. Li
AU  - H. Salman
AU  - Y. He
AU  - W. Xu
AU  - H. Wang
AU  - H. Choset
PY  - 2018
KW  - entropy
KW  - feature extraction
KW  - geometry
KW  - image matching
KW  - image recognition
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - octrees
KW  - optical radar
KW  - robot vision
KW  - unsupervised learning
KW  - Generative Adversarial Network
KW  - adversarial feature
KW  - place recognition
KW  - global geometry map
KW  - Conditional Entropy Reduction module
KW  - unsupervised place feature
KW  - local 2D maps
KW  - dynamic octree mapping module
KW  - core modules
KW  - LiDAR inputs
KW  - end-to-end feature
KW  - geometry matching
KW  - traditional methods
KW  - LiDAR-based place recognition
KW  - unsupervised feature learning
KW  - feature size
KW  - place recognition task
KW  - North Campus Long-Term LiDAR dataset
KW  - feature learning process
KW  - place feature learning
KW  - Octrees
KW  - Laser radar
KW  - Task analysis
KW  - Decoding
KW  - Simultaneous localization and mapping
KW  - Generative adversarial networks
DO  - 10.1109/IROS.2018.8593562
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.
ER  - 

TY  - CONF
TI  - DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1168
EP  - 1174
AU  - C. Yu
AU  - Z. Liu
AU  - X. Liu
AU  - F. Xie
AU  - Y. Yang
AU  - Q. Wei
AU  - Q. Fei
PY  - 2018
KW  - mobile robots
KW  - object detection
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - high-dynamic environments
KW  - ORB-SLAM2
KW  - dense semantic octo-tree map
KW  - dynamic objects
KW  - DS-SLAM combines semantic segmentation network
KW  - dense semantic map creation
KW  - local mapping
KW  - robust semantic visual SLAM
KW  - impressed SLAM systems
KW  - Semantics
KW  - Simultaneous localization and mapping
KW  - Image segmentation
KW  - Feature extraction
KW  - Heuristic algorithms
KW  - Three-dimensional displays
KW  - Optical flow
DO  - 10.1109/IROS.2018.8593691
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Simultaneous Localization and Mapping (SLAM) is considered to be a fundamental capability for intelligent mobile robots. Over the past decades, many impressed SLAM systems have been developed and achieved good performance under certain circumstances. However, some problems are still not well solved, for example, how to tackle the moving objects in the dynamic environments, how to make the robots truly understand the surroundings and accomplish advanced tasks. In this paper, a robust semantic visual SLAM towards dynamic environments named DS-SLAM is proposed. Five threads run in parallel in DS-SLAM: tracking, semantic segmentation, local mapping, loop closing and dense semantic map creation. DS-SLAM combines semantic segmentation network with moving consistency check method to reduce the impact of dynamic objects, and thus the localization accuracy is highly improved in dynamic environments. Meanwhile, a dense semantic octo-tree map is produced, which could be employed for high-level tasks. We conduct experiments both on TUM RGB-D dataset and in real-world environment. The results demonstrate the absolute trajectory accuracy in DS-SLAM can be improved one order of magnitude compared with ORB-SLAM2. It is one of the state-of-the-art SLAM systems in high-dynamic environments.
ER  - 

TY  - CONF
TI  - A robust pose graph approach for city scale LiDAR mapping
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1175
EP  - 1182
AU  - S. Yang
AU  - X. Zhu
AU  - X. Nian
AU  - L. Feng
AU  - X. Qu
AU  - T. Ma
PY  - 2018
KW  - graph theory
KW  - image filtering
KW  - image reconstruction
KW  - Kalman filters
KW  - mobile robots
KW  - nonlinear filters
KW  - optical radar
KW  - optimisation
KW  - pose estimation
KW  - radar imaging
KW  - robot vision
KW  - SLAM (robots)
KW  - map quality
KW  - quantitative experimental results
KW  - robust optimization strategy
KW  - systematical initialization bias
KW  - factor graph
KW  - refined structure
KW  - urban environments
KW  - multitask acquisitions
KW  - scan-matching factors
KW  - graph optimization
KW  - cumulative drift
KW  - city scale LiDAR mapping
KW  - robust pose graph approach
KW  - Optimization
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Global Positioning System
KW  - Feature extraction
KW  - Sensors
KW  - Urban areas
DO  - 10.1109/IROS.2018.8593754
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a method for reconstructing globally consistent 3D High-Definition (HD) maps at city scale. Current approaches for eliminating cumulative drift are mainly based on the pose graph optimization under the constraint of scan-matching factors. The misaligned edges in the graph may have negative impacts on the results. To address this problem and further handle inconsistency caused by multi-task acquisitions in urban environments, we introduce a refined structure of the factor graph considering systematical initialization bias, where the scan-matching factors are twice validated through a novel classifier and a robust optimization strategy. In addition, we incorporate a multi-hypothesis extended Kalman filter (MH-EKF) to remove dynamic objects. Quantitative experimental results demonstrate that the proposed method outperforms state-of-the-art techniques in terms of map quality.
ER  - 

TY  - CONF
TI  - Good Feature Selection for Least Squares Pose Optimization in VO/VSLAM
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1183
EP  - 1189
AU  - Y. Zhao
AU  - P. A. Vela
PY  - 2018
KW  - computational complexity
KW  - control engineering computing
KW  - feature extraction
KW  - least squares approximations
KW  - optimisation
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - least squares pose optimization
KW  - pose estimation
KW  - pose tracking
KW  - NP-hard Max-logDet problem
KW  - feature selection
KW  - VO-VSLAM
KW  - integrating Max-logDet feature selection
KW  - Feature extraction
KW  - Optimization
KW  - Pose estimation
KW  - Simultaneous localization and mapping
KW  - Measurement uncertainty
KW  - Approximation algorithms
DO  - 10.1109/IROS.2018.8593641
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper aims to select features that contribute most to the pose estimation in VO/VSLAM. Unlike existing feature selection works that are focused on efficiency only, our method significantly improves the accuracy of pose tracking, while introducing little overhead. By studying the impact of feature selection towards least squares pose optimization, we demonstrate the applicability of improving accuracy via good feature selection. To that end, we introduce the Max-logDet metric to guide the feature selection, which is connected to the conditioning of least squares pose optimization problem. We then describe an efficient algorithm for approximately solving the NP-hard Max-logDet problem. Integrating Max-logDet feature selection into a state-of-the-art visual SLAM system leads to accuracy improvements with low overhead, as demonstrated via evaluation on a public benchmark.
ER  - 

TY  - CONF
TI  - Dynamic Scaling Factors of Covariances for Accurate 3D Normal Distributions Transform Registration
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1190
EP  - 1196
AU  - H. Hong
AU  - B. H. Lee
PY  - 2018
KW  - covariance analysis
KW  - image registration
KW  - iterative methods
KW  - normal distribution
KW  - stereo image processing
KW  - transforms
KW  - NDT-D2D
KW  - 3D normal distributions transform registration
KW  - distribution-to-distribution normal distributions transform
KW  - PNDT-D2D
KW  - distribution-to-distribution probabilistic NDT
KW  - objective function
KW  - fast point set registrations
KW  - dynamic scaling factors
KW  - Linear programming
KW  - Gaussian distribution
KW  - Correlation
KW  - Probabilistic logic
KW  - Three-dimensional displays
KW  - Transforms
KW  - Robots
DO  - 10.1109/IROS.2018.8593839
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Distribution-to-distribution normal distributions transform (NDT-D2D) is one of the fast point set registrations. Since the normal distributions transform (NDT) is a set of normal distributions generated by discrete and regular cells, local minima of the objective function is an issue of NDT-D2D. Also, we found that the objective function based on L2 distance between distributions has a negative correlation with rotational alignment. To overcome the problems, we present a method using dynamic scaling factors of covariances to improve the accuracy of NDT-D2D. Two scaling factors are defined for the preceding and current NDTs respectively, and they are dynamically varied in each iteration of NDT-D2D. We implemented the proposed method based on conventional NDT-D2D and probabilistic NDT-D2D and compared to the NDT-D2D with fixed scaling factors using KITTI benchmark data set. Also, we experimented estimating odometry with an initial guess as an application of distribution-to-distribution probabilistic NDT (PNDT-D2D) with the proposed method. As a result, the proposed method improves both translational and rotational accuracy of the NDT-D2D and PNDT-D2D.
ER  - 

TY  - CONF
TI  - HMAPs - Hybrid Height- Voxel Maps for Environment Representation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1197
EP  - 1203
AU  - L. Garrote
AU  - C. Premebida
AU  - D. Silva
AU  - U. J. Nunes
PY  - 2018
KW  - mobile robots
KW  - optical radar
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - 2.5D representation
KW  - Microsoft Kinect One
KW  - SLAM approach
KW  - complex elements
KW  - Velodyne VLP-16 LiDAR
KW  - updated grid representation
KW  - complex environments
KW  - reliable method
KW  - occupied space
KW  - free space
KW  - HVoxel
KW  - height-voxel elements
KW  - 3D point-clouds
KW  - mobile robot
KW  - grid-based mapping approach
KW  - environment representation
KW  - hybrid height- voxel maps
KW  - HMAP
KW  - Two dimensional displays
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Pipelines
KW  - Ray tracing
KW  - Planning
KW  - Indexing
DO  - 10.1109/IROS.2018.8594113
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a hybrid 3D-like grid-based mapping approach, that we called HMAP, used as a reliable and efficient 3D representation of the environment surrounding a mobile robot. Considering 3D point-clouds as input data, the proposed mapping approach addresses the representation of height-voxel (HVoxel) elements inside the HMAP, where free and occupied space is modeled through HVoxels, resulting in a reliable method for 3D representation. The proposed method corrects some of the problems inherent to the representation of complex environments based on 2D and 2.5D representations, while keeping an updated grid representation. Additionally, we also propose a complete pipeline for SLAM based on HMAPs. Indoor and outdoor experiments were carried out to validate the proposed representation using data from a Microsoft Kinect One (indoor) and a Velodyne VLP-16 LiDAR (outdoor). The obtained results show that HMAPs can provide a more detailed view of complex elements in a scene when compared to a classic 2.5D representation. Moreover, validation of the proposed SLAM approach was carried out in an outdoor dataset with promising results, which lay a foundation for further research in the topic.
ER  - 

TY  - CONF
TI  - Kalman Filter Based Observer for an External Force Applied to Medium-sized Humanoid Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1204
EP  - 1211
AU  - L. Hawley
AU  - R. Rahem
AU  - W. Suleiman
PY  - 2018
KW  - force sensors
KW  - humanoid robots
KW  - Kalman filters
KW  - legged locomotion
KW  - medium-sized humanoid robot
KW  - external force observer
KW  - force/torque sensors
KW  - small robots
KW  - medium-sized humanoid robots
KW  - robot structure
KW  - Kalman filter formulation
KW  - force components
KW  - robot hardware
KW  - robot inertial measurement unit
KW  - Nao humanoid robot
KW  - external force
KW  - force-sensing resistors
KW  - Force
KW  - Humanoid robots
KW  - Robot sensing systems
KW  - Observers
KW  - Force measurement
DO  - 10.1109/IROS.2018.8593610
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - External force observer for humanoid robots has been widely studied in the literature. However, most of the proposed approaches generally rely on information from six-axis force/torque sensors, which the small or medium-sized humanoid robots usually do not have. As a result, those approaches cannot be applied to this category of humanoid robots, which is widely used nowadays in education or research. In this paper, we improve the external force observer in [1] to handle the case of an external force applied in any direction and at an arbitrary point of the robot structure. The new observer is based on Kalman filter formulation and it allows the estimation of the three force components. The observer is simple to implement and can easily run in real time using the embedded processor of a medium-sized humanoid robot such as Nao or Darwin-OP. Moreover, the observer does not require any change to the robot hardware as it only uses measurements from the available force-sensing resistors (FSR) inserted under the feet of the humanoid robot and from the robot inertial measurement unit (IMU). The proposed observer was extensively validated on a Nao humanoid robot. In all conducted experiments, the observer successfully estimated the external force within a reasonable margin of error.
ER  - 

TY  - CONF
TI  - CPG-based Controllers can Generate Both Discrete and Rhythmic Movements
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1212
EP  - 1217
AU  - M. Jouaiti
AU  - P. Henaff
PY  - 2018
KW  - human-robot interaction
KW  - motion control
KW  - neural net architecture
KW  - neurocontrollers
KW  - three-term control
KW  - CPG-based controllers
KW  - discrete movements
KW  - rhythmic movements
KW  - bio-inspired robot controller
KW  - oscillating neurons
KW  - PID controller
KW  - handshaking
KW  - Task analysis
KW  - Neurons
KW  - Oscillators
KW  - Manipulators
KW  - Grippers
KW  - Intelligent robots
DO  - 10.1109/IROS.2018.8593889
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Complex tasks require the combination of both discrete and rhythmic movements. Though scientists do not yet agree on the neural architecture involved in both types and in the transition from one to the other, the importance of having robot controllers able to behave rhythmically and discretely is universally recoanized. In this paper, a bio-inspired robot controller based on oscillating neurons is proposed to realize both discrete and rhythmic movements and easily transition from one to the other. It is shown that, under certain parameter conditions, the CPG controller behaves like a PID controller. In order to demonstrate the feasibility of controlling both discrete and rhythmic movements, the CPG is applied to the initiation of handshaking, namely, reach towards the human hand and start to shake it. Results show that this architecture is suitable for both discrete and rhythmic movements and can easily transition from one to the other.
ER  - 

TY  - CONF
TI  - A 3D Template Model for Healthy and Impaired Walking
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1218
EP  - 1225
AU  - M. A. Sharbafi
AU  - M. Zadravec
AU  - Z. Matjaƒçiƒá
AU  - A. Seyfarth
PY  - 2018
KW  - biomechanics
KW  - elasticity
KW  - gait analysis
KW  - legged locomotion
KW  - motion control
KW  - muscle
KW  - nonlinear control systems
KW  - pendulums
KW  - robot dynamics
KW  - springs (mechanical)
KW  - 3D template model
KW  - modeling studies
KW  - neuromuscular control
KW  - impaired unperturbed gaits
KW  - human strategies
KW  - lateral asymmetries
KW  - experimental studies
KW  - stance time relations
KW  - stroke patients
KW  - bipedal SLIP
KW  - spring-loaded inverted pendulum
KW  - pathologic gaits
KW  - modulated compliant hip
KW  - VBLA model
KW  - velocity based leg adjustment
KW  - asymmetric leg
KW  - control parameters
KW  - similar gait patterns
KW  - hip stiffness
KW  - rest angles
KW  - FMCH models
KW  - Legged locomotion
KW  - Three-dimensional displays
KW  - Mathematical model
KW  - Solid modeling
KW  - Hip
KW  - Biological system modeling
KW  - Springs
DO  - 10.1109/IROS.2018.8594013
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Several modeling studies, which address neuromuscular control in impaired unperturbed gaits, were performed to predict human strategies to cope with lateral asymmetries in the body. Experimental studies show different step length and stance time relations between limbs in walking of stroke patients. By extension of a bipedal SLIP (spring-loaded inverted pendulum) based model and the corresponding controllers to 3D space, we focus on different features of the pathologic gaits. The introduced model is based on an extension of the FMCH (force modulated compliant hip) and VBLA (velocity based leg adjustment) model to 3D space. With the proposed model, asymmetric leg and control parameters can result in similar gait patterns as observed in experiments. These parameters comprise hip stiffness and rest angles in FMCH models and the tuning parameter of VBLA for foot placement. It is shown that asymmetries in muscle properties (e.g. stiffness) and leg adjustment can play an important role in generating pathologic gaits.
ER  - 

TY  - CONF
TI  - Exploiting Friction in Torque Controlled Humanoid Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1226
EP  - 1232
AU  - G. Nava
AU  - D. Ferigo
AU  - D. Pucci
PY  - 2018
KW  - friction
KW  - humanoid robots
KW  - motion control
KW  - robot dynamics
KW  - stability
KW  - torque control
KW  - torque controlled humanoid robots
KW  - common architecture
KW  - nested loops
KW  - joint/motor torques
KW  - joint friction phenomena
KW  - high level control objectives
KW  - joint task space control
KW  - humanoid robot iCub
KW  - stabilizing property
KW  - Friction
KW  - Brushless motors
KW  - Humanoid robots
KW  - Task analysis
KW  - Robot sensing systems
KW  - Robot kinematics
DO  - 10.1109/IROS.2018.8594505
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - A common architecture for torque controlled humanoid robots consists in two nested loops. The outer loop generates desired joint/motor torques, and the inner loop stabilizes these desired values. In doing so, the inner loop usually compensates for joint friction phenomena, thus removing their inherent stabilizing property that may be also beneficial for high level control objectives. This paper shows how to exploit friction for joint and task space control of humanoid robots. Experiments are carried out on the humanoid robot iCub.
ER  - 

TY  - CONF
TI  - Structure preserving Multi-Contact Balance Control for Series-Elastic and Visco-Elastic Humanoid Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1233
EP  - 1240
AU  - A. Werner
AU  - B. Henze
AU  - M. Keppler
AU  - F. Loeffl
AU  - S. Leyendecker
AU  - C. Ott
PY  - 2018
KW  - actuators
KW  - elasticity
KW  - humanoid robots
KW  - legged locomotion
KW  - predictive control
KW  - robot dynamics
KW  - visco-elastic humanoid robots
KW  - actuator control
KW  - multicontact balancing
KW  - force distribution problem
KW  - actuator dynamics
KW  - dynamically consistent force distribution
KW  - model predictive controller
KW  - contact force
KW  - actuator constraints
KW  - multicontact balance control
KW  - series-elastic humanoid robos
KW  - structure preservation control concept
KW  - locomotion
KW  - Force
KW  - Actuators
KW  - Robot kinematics
KW  - Dynamics
KW  - Task analysis
KW  - Humanoid robots
DO  - 10.1109/IROS.2018.8593596
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper proposes an integration of multi-body and actuator control for multi-contact balancing for robots with highly elastic joints. Inspired by the structure preserving control concept for series-elastic fixed-base robots, the presented approach aims to minimize the control effort by keeping the system structure intact. Balancing on multiple contacts requires to solve the force distribution problem. In locomotion, contacts change quickly, requiring a swift redistribution of contact forces. This is a challenge for elastic robots as the actuator dynamics and limits prevent instantaneous changes of contact forces. The proposed dynamically consistent force distribution is implemented as a model predictive controller which resolves redundancy while complying with contact force and actuator constraints.
ER  - 

TY  - CONF
TI  - Feedback Control For Cassie With Deep Reinforcement Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1241
EP  - 1246
AU  - Z. Xie
AU  - G. Berseth
AU  - P. Clary
AU  - J. Hurst
AU  - M. van de Panne
PY  - 2018
KW  - feedback
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - Markov processes
KW  - motion control
KW  - robot dynamics
KW  - velocity control
KW  - Cassie
KW  - deep reinforcement learning
KW  - bipedal locomotion skills
KW  - local linearization
KW  - reduced-order abstractions
KW  - tractable solutions
KW  - model-based control strategies
KW  - torque limits
KW  - joint limits
KW  - nonlinearities
KW  - control computations
KW  - DRL
KW  - machine learning literature
KW  - ad-hoc simulation models
KW  - realizable bipedal robots
KW  - feedback control problem
KW  - robust walking controllers
KW  - controller robustness
KW  - model-free approach
KW  - Legged locomotion
KW  - Reinforcement learning
KW  - Computational modeling
KW  - Aerospace electronics
KW  - Feedback control
KW  - Trajectory
DO  - 10.1109/IROS.2018.8593722
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Bipedal locomotion skills are challenging to develop. Control strategies often use local linearization of the dynamics in conjunction with reduced-order abstractions to yield tractable solutions. In these model-based control strategies, the controller is often not fully aware of many details, including torque limits, joint limits, and other non-linearities that are necessarily excluded from the control computations for simplicity. Deep reinforcement learning (DRL) offers a promising model-free approach for controlling bipedal locomotion which can more fully exploit the dynamics. However, current results in the machine learning literature are often based on ad-hoc simulation models that are not based on corresponding hardware. Thus it remains unclear how well DRL will succeed on realizable bipedal robots. In this paper, we demonstrate the effectiveness of DRL using a realistic model of Cassie, a bipedal robot. By formulating a feedback control problem as finding the optimal policy for a Markov Decision Process, we are able to learn robust walking controllers that imitate a reference motion with DRL. Controllers for different walking speeds are learned by imitating simple time-scaled versions of the original reference motion. Controller robustness is demonstrated through several challenging tests, including sensory delay, walking blindly on irregular terrain and unexpected pushes at the pelvis. We also show we can interpolate between individual policies and that robustness can be improved with an interpolated policy.
ER  - 

TY  - CONF
TI  - Robust and Stretched-Knee Biped Walking Using Joint-Space Motion Control
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1247
EP  - 1254
AU  - K. Nguyen
AU  - S. Noda
AU  - Y. Kojio
AU  - F. Sugai
AU  - S. Nozawa
AU  - Y. Kakiuchi
AU  - K. Okada
AU  - M. Inaba
PY  - 2018
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - robot kinematics
KW  - robust control
KW  - IK based motion control
KW  - kinematics singularity problem
KW  - motion optimization method
KW  - human-like walking motion
KW  - SIMBICON
KW  - inverse kinematics
KW  - joint-space motion control
KW  - stretched-knee biped walking
KW  - walking robustness
KW  - simple biped locomotion control
KW  - Legged locomotion
KW  - Foot
KW  - Torque
KW  - Optimization
KW  - Robustness
KW  - Knee
DO  - 10.1109/IROS.2018.8594440
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Comparing to IK (Inverse Kinematics) based motion control, joint-space motion control is more advantageous in terms of not being restricted by kinematics singularity problem. In this paper, we start with SIMBICON (Simple Biped Locomotion Control) based controller, a joint-space motion control method, extend it for enhancing walking's robustness and versatility. We propose a motion optimization method considering walking robustness, desired walking velocity and energy efficient minimization for walking motion generation. This method enables us to achieve human-like walking motion, which has stretched-knee posture and robust to large push disturbances. We also apply our proposed method to a life-sized biped robot and validate its effectiveness with push recovery and walking on unknown debris experiments.
ER  - 

TY  - CONF
TI  - Public perception of android robots: Indications from an analysis of YouTube comments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1255
EP  - 1260
AU  - E. Vlachos
AU  - Z. Tan
PY  - 2018
KW  - control engineering computing
KW  - data mining
KW  - humanoid robots
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - public administration
KW  - social networking (online)
KW  - text analysis
KW  - end-users
KW  - Youtube comments
KW  - social perception indication
KW  - machine learning
KW  - text mining
KW  - rendering interactions
KW  - textual reactions
KW  - video stimuli
KW  - technical specification
KW  - science fiction valley
KW  - public perception
KW  - human-robot relationships
KW  - robotic society
KW  - quantitative content analysis
KW  - android robots
KW  - YouTube
KW  - Videos
KW  - Androids
KW  - Humanoid robots
KW  - Clustering algorithms
KW  - Text mining
DO  - 10.1109/IROS.2018.8594058
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The public perception of android robots is a field of growing applied relevance. Currently, most androids are confined within controlled environments rendering interactions between potential end-users, and robots challenging. Even more challenging is for researchers to investigate end-users' perception of androids. We exploit pre-existing YouTube comments as artifacts for quantitative content analysis to gain an indication of social perception on androids. We perform a content analysis of 10301 YouTube comments from four different videos, and reflect on the textual reactions to video stimuli of four extremely human-like android robots. We use text mining and machine learning techniques to process and analyze our corpus. Our findings reveal three equally important topics that should be considered for paving the way towards a robotic society: human-robot relationships, technical specifications, and the science fiction valley. Considering people's attitudes, fears and wishes towards androids, researchers can increase citizen awareness, and engagement.
ER  - 

TY  - CONF
TI  - Towards Automatic 3D Shape Instantiation for Deployed Stent Grafts: 2D Multiple-class and Class-imbalance Marker Segmentation with Equally-weighted Focal U-Net
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1261
EP  - 1267
AU  - X. Zhou
AU  - C. Riga
AU  - S. Lee
AU  - G. Yang
PY  - 2018
KW  - blood vessels
KW  - cardiovascular system
KW  - image registration
KW  - image segmentation
KW  - medical image processing
KW  - mobile robots
KW  - path planning
KW  - stents
KW  - focal loss function
KW  - fluoroscopy projection
KW  - robot-assisted fenestrated endovascular aortic repair
KW  - automatic 3D shape instantiation
KW  - focal u-net
KW  - multiple class marker segmentation
KW  - multiple class marker center determination
KW  - robust perspective-S-point method
KW  - tensorflow codes
KW  - mean intersection over union
KW  - weighted u-net
KW  - network architecture
KW  - graft gap interpolation
KW  - stent graft
KW  - semiautomatic 3D shape instantiation method
KW  - FEVAR
KW  - class-imbalance marker segmentation
KW  - initial marker segmentation
KW  - fluoroscopy projections
KW  - Image segmentation
KW  - Shape
KW  - Three-dimensional displays
KW  - Aneurysm
KW  - Training
KW  - Two dimensional displays
KW  - Testing
DO  - 10.1109/IROS.2018.8594178
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robot-assisted Fenestrated Endovascular Aortic Repair (FEVAR) is currently navigated by 2D fluoroscopy which is insufficiently informative. Previously, a semi-automatic 3D shape instantiation method was developed to instantiate the 3D shape of a main, deployed, and fenestrated stent graft from a single fluoroscopy projection in real-time, which could help 3D FEVAR navigation and robotic path planning. This proposed semi-automatic method was based on the Robust Perspective-S-Point (RP5P) method, graft gap interpolation and semiautomatic multiple-class marker center determination. In this paper, an automatic 3D shape instantiation could be achieved by automatic multiple-class marker segmentation and hence automatic multiple-class marker center determination. Firstly, the markers were designed into five different shapes. Then, Equally-weighted Focal U-Net was proposed to segment the fluoroscopy projections of customized markers into five classes and hence to determine the marker centers. The proposed Equally-weighted Focal U-Net utilized U-Net as the network architecture, equally-weighted loss function for initial marker segmentation, and then equally-weighted focal loss function for improving the initial marker segmentation. This proposed network outperformed traditional Weighted U-Net on the class-imbalance segmentation in this paper with reducing one hyperparameter - the weight. An overall mean Intersection over Union (mIoU) of 0.6943 was achieved on 78 testing images, where 81.01 % markers were segmented with a center position error <; 1.6mm. Comparable accuracy of 3D shape instantiation was also achieved and stated. The data, trained models and TensorFlow codes are available on-line.
ER  - 

TY  - CONF
TI  - A Confidence-Based Shared Control Strategy for the Smart Tissue Autonomous Robot (STAR)
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1268
EP  - 1275
AU  - H. Saeidi
AU  - J. D. Opfermann
AU  - M. Kam
AU  - S. Raghunathan
AU  - S. Leonard
AU  - A. Krieger
PY  - 2018
KW  - biological tissues
KW  - blood
KW  - medical robotics
KW  - mobile robots
KW  - surgery
KW  - confidence-based shared control strategy
KW  - STAR
KW  - surgery systems
KW  - robotic accuracy
KW  - surgical procedures
KW  - complex surgical environments
KW  - surgical scenarios
KW  - cutting pattern
KW  - robotic electrocautery tool
KW  - surgical task
KW  - confidence models
KW  - confidence-based control allocation function
KW  - autonomous robot controller
KW  - smart tissue autonomous robot
KW  - autonomous robotic assisted surgery
KW  - 2D pattern cutting
KW  - Robots
KW  - Trajectory
KW  - Task analysis
KW  - Surgery
KW  - Human factors
KW  - Cameras
KW  - Blood
DO  - 10.1109/IROS.2018.8594290
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Autonomous robotic assisted surgery (RAS) systems aim to reduce human errors and improve patient outcomes leveraging robotic accuracy and repeatability during surgical procedures. However, full automation of RAS in complex surgical environments is still not feasible and collaboration with the surgeon is required for safe and effective use. In this work, we utilize our Smart Tissue Autonomous Robot (STAR) to develop and evaluate a shared control strategy for the collaboration of the robot with a human operator in surgical scenarios. We consider 2D pattern cutting tasks with partial blood occlusion of the cutting pattern using a robotic electrocautery tool. For this surgical task and RAS system, we i) develop a confidence-based shared control strategy, ii) assess the pattern tracking performances of manual and autonomous controls and identify the confidence models for human and robot as well as a confidence-based control allocation function, and iii) experimentally evaluate the accuracy of our proposed shared control strategy. In our experiments on porcine fat samples, by combining the best elements of autonomous robot controller with complementary skills of a human operator, our proposed control strategy improved the cutting accuracy by 6.4%, while reducing the operator work time to 44% compared to a pure manual control.
ER  - 

TY  - CONF
TI  - A 3D Laparoscopic Imaging System Based on Stereo-Photogrammetry with Random Patterns
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1276
EP  - 1282
AU  - C. Sui
AU  - Z. Wang
AU  - Y. Liu
PY  - 2018
KW  - image reconstruction
KW  - image sensors
KW  - lenses
KW  - medical image processing
KW  - photogrammetry
KW  - stereo image processing
KW  - surgery
KW  - high frame rate image acquisition
KW  - stereo-photogrammetry
KW  - coded structured patterns projection
KW  - stereo matching
KW  - 3D surface reconstruction
KW  - stereo vision feedback
KW  - novel 3D laparoscopic imaging system
KW  - frequency 4.0 kHz
KW  - Three-dimensional displays
KW  - Laparoscopes
KW  - Lenses
KW  - Imaging
KW  - Probes
KW  - Image resolution
KW  - Surface reconstruction
DO  - 10.1109/IROS.2018.8593733
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose a novel 3D laparoscopic imaging system based on stereo-photogrammetry which is assisted by projecting patterns on the tissue surface. The proposed laparoscopic imaging system has three optic channels, two of which are responsible for stereo vision feedback and the other one is used for coded structured patterns projection. The projected patterns provide the robustness to homogeneous tissue surface since they add more features that can be relied on in the stereo matching. Image fiber bundles (100k pixels) and Gradient-index (GRIN) lenses are utilized to facilitate the remote image acquisition and miniaturization of the laparoscopic probe. Moreover, we adopt a digital micromirror device (DMD) and high-speed cameras to achieve fast pattern switching (up to 4 kHz) and high frame rate image acquisition. The system configuration allows for implementation of the time multiplexing pattern codification strategy in the 3D laparoscopic imaging system to enhance the reliability and resolution of the 3D surface reconstruction. A prototype is established, and various experiments are conducted. Comparative experimental results prove the advantages of our system design. The static and dynamic 3D reconstruction results validate the performance of the proposed 3D laparoscopic imaging system quantitatively and qualitatively.
ER  - 

TY  - CONF
TI  - Magnetic- Visual Sensor Fusion-based Dense 3D Reconstruction and Localization for Endoscopic Capsule Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1283
EP  - 1289
AU  - M. Turan
AU  - Y. Almalioglu
AU  - E. P. Ornek
AU  - H. Araujo
AU  - M. F. Yanik
AU  - M. Sitti
PY  - 2018
KW  - biomedical optical imaging
KW  - cameras
KW  - endoscopes
KW  - image fusion
KW  - image reconstruction
KW  - medical image processing
KW  - medical robotics
KW  - robot vision
KW  - visual sensor fusion-based dense 3D reconstruction
KW  - real-time 3D reconstruction
KW  - actively controlled capsule endoscopic robots
KW  - minimally invasive diagnostic technology
KW  - therapeutic technology
KW  - gastrointestinal tract
KW  - intraoperative map fusion approach
KW  - actively controlled endoscopic capsule robot applications
KW  - magnetic vision-based localization
KW  - nonrigid deformations
KW  - frame-to-model map fusion
KW  - ex-vivo porcine stomach models
KW  - root mean square surface reconstruction errors
KW  - endoscopic camera
KW  - Magnetic resonance imaging
KW  - Robot sensing systems
KW  - Magnetic separation
KW  - Three-dimensional displays
KW  - Cameras
KW  - Endoscopes
DO  - 10.1109/IROS.2018.8594485
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Reliable and real-time 3D reconstruction and localization functionality is a crucial prerequisite for the navigation of actively controlled capsule endoscopic robots as an emerging, minimally invasive diagnostic and therapeutic technology for use in the gastrointestinal (GI) tract. In this study, we propose a fully dense, non-rigidly deformable, strictly real-time, intraoperative map fusion approach for actively controlled endoscopic capsule robot applications which combines magnetic and vision-based localization, with non-rigid deformations based frame-to-model map fusion. The performance of the proposed method is evaluated using four different ex-vivo porcine stomach models. Across different trajectories of varying speed and complexity, and four different endoscopic cameras, the root mean square surface reconstruction errors vary from 1.58 to 2.17 cm.
ER  - 

TY  - CONF
TI  - Robust Generalized Point Cloud Registration with Expectation Maximization Considering Anisotropic Positional Uncertainties
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1290
EP  - 1297
AU  - Z. Min
AU  - J. Wang
AU  - S. Song
AU  - M. Q. -. Meng
PY  - 2018
KW  - expectation-maximisation algorithm
KW  - Gaussian distribution
KW  - image registration
KW  - matrix algebra
KW  - optimisation
KW  - anisotropic positional uncertainties
KW  - E-step
KW  - correspondence probabilities
KW  - M-step
KW  - transformation matrix
KW  - constrained optimization problem
KW  - expectation conditional maximization framework
KW  - multivariate Gaussian distribution
KW  - positional error
KW  - generalized point cloud registration problem
KW  - computer-assisted surgery
KW  - medical robotics
KW  - robust generalized point cloud registration
KW  - Three-dimensional displays
KW  - Hidden Markov models
KW  - Covariance matrices
KW  - Surgery
KW  - Optimization
KW  - Mixture models
KW  - Linear programming
DO  - 10.1109/IROS.2018.8593558
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Alignment of two point clouds is an essential problem in medical robotics and computer-assisted surgery. In this paper, we first formally formulate the generalized point cloud registration problem in a probabilistic manner. Specifically, not only positional but also the orientational information are incorporated into registration. Notably, the positional error is assumed to obey a multivariate Gaussian distribution to accommodate anisotropic cases. Expectation conditional maximization framework is utilized to solve the problem. In E-step, the correspondence probabilities between points in two generalized point clouds are computed. In M -step, the constrained optimization problem with respect to the transformation matrix is re-formulated as an unconstrained one. Extensive experiments are conducted to compare the proposed algorithm with the state-of-the-art registration methods. The experimental results demonstrate the algorithm's robustness to noise and outliers, fast convergence speed.
ER  - 

TY  - CONF
TI  - Vision-Based Surgical Tool Pose Estimation for the da Vinci¬Æ Robotic Surgical System
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1298
EP  - 1305
AU  - R. Hao
AU  - O. √ñzg√ºner
AU  - M. C. √áavu≈üoƒülu
PY  - 2018
KW  - Bayes methods
KW  - computer vision
KW  - endoscopes
KW  - medical image processing
KW  - medical robotics
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - rendering (computer graphics)
KW  - robot vision
KW  - stereo image processing
KW  - surgery
KW  - virtual reality
KW  - robot endoscopes
KW  - defined tool geometry
KW  - virtual images
KW  - silhouette rendering algorithm
KW  - Bayesian state estimation
KW  - computer vision techniques
KW  - robot kinematics
KW  - stereo vision
KW  - vision-based Surgical tool pose estimation
KW  - surgical robotic system
KW  - surgical tool tracking
KW  - endoscopic stereo image streams
KW  - virtual rendering method
KW  - Tools
KW  - Solid modeling
KW  - Rendering (computer graphics)
KW  - Robots
KW  - Cameras
KW  - Bayes methods
KW  - Geometry
DO  - 10.1109/IROS.2018.8594471
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents an approach to surgical tool tracking using stereo vision for the da Vinci¬Æ Surgical Robotic System. The proposed method is based on robot kinematics, computer vision techniques and Bayesian state estimation. The proposed method employs a silhouette rendering algorithm to create virtual images of the surgical tool by generating the silhouette of the defined tool geometry under the da Vinci¬Æ robot endoscopes. The virtual rendering method provides the tool representation in image form, which makes it possible to measure the distance between the rendered tool and real tool from endoscopic stereo image streams. Particle Filter algorithm employing the virtual rendering method is then used for surgical tool tracking. The tracking performance is evaluated on an actual da Vinci¬Æ surgical robotic system and a ROS/Gazebo-based simulation of the da Vinci¬Æ system.
ER  - 

TY  - CONF
TI  - A Parallel Robotic Mechanism for the Stabilization and Guidance of an Endoscope Tip in Laser Osteotomy
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1306
EP  - 1311
AU  - M. Eugster
AU  - P. C. Cattin
AU  - A. Zam
AU  - G. Rauter
PY  - 2018
KW  - bone
KW  - endoscopes
KW  - laser applications in medicine
KW  - medical robotics
KW  - orthopaedics
KW  - surgery
KW  - parallel robotic mechanism
KW  - laser osteotomy
KW  - endoscope tip stabilization
KW  - robot-assisted minimally invasive laser osteotome
KW  - robust platform
KW  - sub-millimeter range
KW  - endoscope tip motion
KW  - Endoscopes
KW  - Bones
KW  - Laser beam cutting
KW  - Rails
KW  - Kinematics
KW  - End effectors
DO  - 10.1109/IROS.2018.8594188
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a parallel robotic mechanism for endoscope tip stabilization and guidance for a robot-assisted minimally invasive laser osteotome. The mechanism attaches to the bone of the patient, providing a stable and robust platform for the laser integrated in the endoscope tip which has to be moved precisely in the sub-millimeter range along a preoperatively planned path. This method is only possible because cutting bone with laser instead of using conventional bone drills and saws involves considerably lower interaction forces. The design, kinematics, control, and motion performance of the concept are presented for an upscaled prototype. The obtained deviation of the endoscope tip motion from the reference path lies in the sub-millimeter range. This result allows us to conclude that the concept is more than promising. Furthermore, we expect that the herein presented principle will influence the way osteotomies will be performed in the future.
ER  - 

TY  - CONF
TI  - RoboTracker: Collaborative robotic assistant device with electromechanical patient tracking for spinal surgery
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1312
EP  - 1317
AU  - A. Amarillo
AU  - J. O√±ativia
AU  - E. Sanchez
PY  - 2018
KW  - biomechanics
KW  - bone
KW  - medical robotics
KW  - neurophysiology
KW  - orthopaedics
KW  - surgery
KW  - RoboTracker
KW  - electromechanical patient tracking
KW  - spinal surgery
KW  - neural damage
KW  - spinal surgical procedures
KW  - pedicle screw fixation
KW  - technological improvements
KW  - surgeons
KW  - optical tracking navigation
KW  - stringent limitation
KW  - tracked elements
KW  - miniature robot
KW  - novel robotic assisted surgery system
KW  - surgical instruments
KW  - patient motion
KW  - electromechanical tracking device
KW  - collaborative robotic assistant device
KW  - Fasteners
KW  - Tracking
KW  - Surgery
KW  - Robot kinematics
KW  - Kinematics
KW  - Mathematical model
DO  - 10.1109/IROS.2018.8594467
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Due to the risks of muscle, bone and neural damage in spinal surgical procedures that require pedicle screw fixation, technological improvements have appeared to help surgeons perform the procedures with higher accuracy. Systems based on optical tracking navigation impose a stringent limitation in the workflow of surgeons since a clear line of sight has to be kept between the cameras and the tracked elements. Other solutions are based on mounting a miniature robot on the spine of the patient, which is very invasive and entails some risks. For these reasons, a novel robotic assisted surgery system capable to guide surgical instruments with minimal deviations compensating patient motion is being developed. This paper presents the system and the electromechanical tracking device used to sense patient motion.
ER  - 

TY  - CONF
TI  - A Sliding Mode Control Architecture for Human-Manipulator Cooperative Surface Treatment Tasks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1318
EP  - 1325
AU  - L. Gracia
AU  - J. E. Solanes
AU  - P. Mu√±oz-Benavent
AU  - J. V. Miro
AU  - C. Perez-Vidal
AU  - J. Tornero
PY  - 2018
KW  - control engineering computing
KW  - deburring
KW  - end effectors
KW  - force sensors
KW  - industrial manipulators
KW  - industrial robots
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - position control
KW  - variable structure systems
KW  - redundant 7R manipulator
KW  - robotic surface treatment
KW  - novel collaborative controller
KW  - robot motion
KW  - robotic tool
KW  - conditioning task
KW  - robot end-effector
KW  - surface treatment tool
KW  - nonconventional sliding mode control
KW  - task prioritization
KW  - control scheme
KW  - autonomous physical agent
KW  - human operator propioceptive abilities
KW  - shared strategy effectively couples
KW  - robotic manipulator partner
KW  - physical strength
KW  - surface treatment tasks
KW  - human-manipulator
KW  - sliding mode control architecture
KW  - Robot sensing systems
KW  - Surface treatment
KW  - Task analysis
KW  - Tools
KW  - Surface morphology
KW  - Manipulators
DO  - 10.1109/IROS.2018.8593444
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a control architecture readily suitable for surface treatment tasks such as polishing, grinding, finishing or deburring as carried out by a human operator, with the added benefit of accuracy, recurrence and physical strength as administered by a robotic manipulator partner. The shared strategy effectively couples the human operator propioceptive abilities and fine skills through his interactions with the autonomous physical agent. The novel proposed control scheme is based on task prioritization and a non-conventional sliding mode control, which is considered to benefit from its inherent robustness and low computational cost. The system relies on two force sensors, one located between the last link of the robot and the surface treatment tool, and the other located in some place of the robot end-effector: the former is used to suitably accomplish the conditioning task, while the latter is used by the operator to manually guide the robotic tool. When the operator chooses to cease guiding the tool, the robot motion safely switches back to an automatic reference tracking. The paper presents the theories for the novel collaborative controller, whilst its effectiveness for robotic surface treatment is substantiated by experimental results using a redundant 7R manipulator and a mock-up conditioning tool.
ER  - 

TY  - CONF
TI  - Human Intention Estimation based on Neural Networks for Enhanced Collaboration with Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1326
EP  - 1333
AU  - D. Nicolis
AU  - A. M. Zanchettin
AU  - P. Rocco
PY  - 2018
KW  - damping
KW  - force sensors
KW  - human-robot interaction
KW  - industrial robots
KW  - motion control
KW  - recurrent neural nets
KW  - sensors
KW  - stability
KW  - trajectory control
KW  - recurrent neural networks
KW  - RNNs
KW  - force sensor
KW  - human-robot collaboration
KW  - human intention estimation
KW  - ABB IRB140 industrial robot
KW  - model-based generated data
KW  - proactive robot behavior
KW  - variable impedance controllers
KW  - admittance behavior
KW  - stability requirements
KW  - Trajectory
KW  - Task analysis
KW  - Robot sensing systems
KW  - Neural networks
KW  - Damping
KW  - Estimation
DO  - 10.1109/IROS.2018.8594415
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In human-robot collaboration, the robot is required to provide assistance to the user by facilitating task execution. However, due to stability requirements, a well-damped admittance behavior of the robot is necessary during interaction, thus inducing fatigue in the operator. While available schemes involve variable impedance controllers to mitigate this effect, here we propose an alternative approach entailing a proactive robot behavior that assists in the cooperative execution of trajectories towards desired goals, by estimating the user intention. To this end, we make use of Recurrent Neural Networks (RNNs) to predict and classify cooperative motions, on the basis of a set of predefined goals in the workspace and model-based generated data of human movements. Manual guidance validation experiments are conducted on a 6 d.o.f. ABB IRB140 industrial robot equipped with a force sensor.
ER  - 

TY  - CONF
TI  - Variable Admittance Control for Human-Robot Collaboration based on Online Neural Network Training
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1334
EP  - 1339
AU  - A. Sharkawy
AU  - P. N. Koustournpardis
AU  - N. Aspragathos
PY  - 2018
KW  - backpropagation
KW  - feedforward neural nets
KW  - human-robot interaction
KW  - manipulators
KW  - motion control
KW  - neurocontrollers
KW  - variable admittance control
KW  - human-robot collaboration
KW  - online neural network training
KW  - human-robot cooperation
KW  - multilayer feedforward neural network
KW  - Cartesian velocity
KW  - admittance controller
KW  - error backpropagation algorithm
KW  - KUKA LWR robot
KW  - virtual damping
KW  - point-to-point cooperative motion
KW  - Admittance
KW  - Artificial neural networks
KW  - Damping
KW  - Trajectory
KW  - Robot kinematics
KW  - Training
KW  - Variable Admittance Control
KW  - Neural Networks
KW  - Error Backpropagation
KW  - Minimum Jerk Trajectory
DO  - 10.1109/IROS.2018.8593526
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, a method for variable admittance control in human-robot cooperation is proposed. A multilayer feedforward neural network is designed using the Cartesian velocity of the robot and the applied force by the operator as its inputs to modify online the virtual damping of the admittance controller. The neural network is trained online using the error backpropagation algorithm based on the error between the velocity of the minimum jerk trajectory model and the measured velocity of the robot. The performance of the proposed controller and the NN generalization ability are evaluated by conducting a point-to-point cooperative motion with multiple subjects using the KUKA LWR robot.
ER  - 

TY  - CONF
TI  - Online Human Muscle Force Estimation for Fatigue Management in Human-Robot Co-Manipulation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1340
EP  - 1346
AU  - L. Peternel
AU  - C. Fang
AU  - N. Tsagarakis
AU  - A. Ajoudani
PY  - 2018
KW  - electromyography
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - medical computing
KW  - medical robotics
KW  - muscle
KW  - arm configurations
KW  - human-robot comanipulation
KW  - optimisation
KW  - online human muscle force estimation
KW  - human operator
KW  - robot sensory system
KW  - task force
KW  - specific fatigued muscles
KW  - task execution
KW  - fatigue management system
KW  - muscle fatigue levels
KW  - model-based estimation
KW  - estimated muscle forces
KW  - endpoint interaction forces
KW  - online predictions
KW  - machine learning technique
KW  - human arm
KW  - less-fatigued muscles
KW  - anticipatory robotic responses
KW  - individual muscle group
KW  - excessive fatigue levels
KW  - selective management
KW  - Muscles
KW  - Force
KW  - Fatigue
KW  - Task analysis
KW  - Robot sensing systems
KW  - Optimization
DO  - 10.1109/IROS.2018.8593705
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose a novel method for selective management of muscle fatigue in human-robot co-manipulation. The proposed framework enables the detection of excessive fatigue levels of an individual muscle group while executing a certain task, and provides anticipatory robotic responses to distribute the effort among less-fatigued muscles of human arm. Our approach uses a machine learning technique to enable online predictions of muscle forces in different arm configurations and endpoint interaction forces. The estimated muscle forces are then used for the model-based estimation of muscle fatigue levels. Through optimisation, the fatigue management system can alter the task execution in a way that specific fatigued muscles are offloaded, while at the same time enables the production of task force using muscles with lower levels of fatigue. The main advantage of the proposed method is that it can operate online, and that all the measurements are performed by the robot sensory system, which can significantly increase the applicability in real-world scenarios. To validate the proposed method, we performed proof-of-concept experiments where the task of the human operator was to use a tool to polish an object that was manipulated by the robot.
ER  - 

TY  - CONF
TI  - Evolutionary Motion Control Optimization in Physical Human-Robot Interaction
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1347
EP  - 1353
AU  - N. A. Nadeau
AU  - I. A. Bonev
PY  - 2018
KW  - biomedical ultrasonics
KW  - evolutionary computation
KW  - force control
KW  - human-robot interaction
KW  - medical robotics
KW  - motion control
KW  - optimisation
KW  - phantoms
KW  - trajectory control
KW  - evolutionary motion control optimization
KW  - medical freehand ultrasound
KW  - trajectory planning
KW  - optimal trajectories
KW  - human leg phantom
KW  - physical human-robot interaction
KW  - online tuning
KW  - collaborative robot
KW  - medical ultrasound motion
KW  - parallel force-impedance control
KW  - differential evolution
KW  - pHRI
KW  - mean absolute error
KW  - Ultrasonic imaging
KW  - Task analysis
KW  - Tuning
KW  - Legged locomotion
KW  - Force
KW  - Robot kinematics
DO  - 10.1109/IROS.2018.8593598
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Given that the success of an interaction task depends on the capability of the robot system to handle physical contact with its environment, pure motion control is often insufficient. This is especially true in the context of medical freehand ultrasound where the human body is a deformable surface and an unstructured environment, representing both a safety concern and a challenge for trajectory planning and control. The systematic tuning of practical high degree-of-freedom physical human-robot interaction (pHRI) tasks is not trivial and there are many parameters to be tuned. While traditional tuning is generally performed ad hoc and requires knowledge of the robot and environment dynamics, we propose a simple and effective online tuning framework using differential evolution (DE) to optimize the motion parameters for parallel force/impedance control in a pHRI and medical ultrasound motion application. Through real-world experiments with a KUKA LBR iiwa 7 R800 collaborative robot, the DE framework tuned motion control for optimal and safe trajectories along a human leg phantom. The optimization process was able to successfully reduce the mean absolute error of the motion contact force to 0.537 N through the evolution of eight motion control parameters.
ER  - 

TY  - CONF
TI  - Human-Robot Cooperative Object Manipulation with Contact Changes
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1354
EP  - 1360
AU  - M. Gienger
AU  - D. Ruiken
AU  - T. Bates
AU  - M. Regaieg
AU  - M. MeiBner
AU  - J. Kober
AU  - P. Seiwald
AU  - A. Hildebrandt
PY  - 2018
KW  - cooperative systems
KW  - human-robot interaction
KW  - interactive systems
KW  - manipulators
KW  - mobile robots
KW  - path planning
KW  - physical interaction system
KW  - bi-manual physical cooperation
KW  - force interaction cues
KW  - interactive search-based planning
KW  - online trajectory
KW  - motion generation
KW  - mixed initiative collaboration strategy
KW  - human-robot cooperative object manipulation
KW  - human-robot interaction
KW  - bi-manual mobile robot
KW  - Robot sensing systems
KW  - Task analysis
KW  - Planning
KW  - Robot kinematics
KW  - Trajectory
KW  - Synchronization
DO  - 10.1109/IROS.2018.8594140
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a system for cooperatively manipulating large objects between a human and a robot. This physical interaction system is designed to handle, transport, or manipulate large objects of different shapes in cooperation with a human. Unique points are the bi-manual physical cooperation, the sequential characteristic of the cooperation including contact changes, and a novel architecture combining force interaction cues, interactive search-based planning, and online trajectory and motion generation. The resulting system implements a mixed initiative collaboration strategy, deferring to the human when his intentions are unclear, and driving the task once understood. This results in an easy and intuitive human-robot interaction. It is evaluated in simulations and on a bi-manual mobile robot with 32 degrees of freedom.
ER  - 

TY  - CONF
TI  - From Human Physical Interaction To Online Motion Adaptation Using Parameterized Dynamical Systems
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1361
EP  - 1366
AU  - M. Khoramshahi
AU  - A. Laurens
AU  - T. Triquet
AU  - A. Billard
PY  - 2018
KW  - adaptive control
KW  - control engineering computing
KW  - human-robot interaction
KW  - manipulator dynamics
KW  - motion control
KW  - path planning
KW  - trajectory control
KW  - parameterized time-independent dynamical systems
KW  - motion flexibility
KW  - motion generation
KW  - impedance-controlled robots
KW  - adaptive motion planning approach
KW  - parameterized dynamical systems
KW  - online motion adaptation
KW  - human physical interaction
KW  - Task analysis
KW  - Trajectory
KW  - Impedance
KW  - Service robots
KW  - Convergence
KW  - Planning
DO  - 10.1109/IROS.2018.8594366
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this work, we present an adaptive motion planning approach for impedance-controlled robots to modify their tasks based on human physical interactions. We use a class of parameterized time-independent dynamical systems for motion generation where the modulation of such parameters allows for motion flexibility. To adapt to human interactions, we update the parameters of our dynamical system in order to reduce the tracking error (i.e., between the desired trajectory generated by the dynamical system and the real trajectory influenced by the human interaction). We provide analytical analysis and several simulations of our method. Finally, we investigate our approach through real world experiments with a 7-DOF KUKA LWR 4+ robot performing tasks such as polishing and pick-and-place.
ER  - 

TY  - CONF
TI  - A Series Elastic Brake Pedal to Preserve Conventional Pedal Feel under Regenerative Braking
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1367
EP  - 1373
AU  - U. Caliskan
AU  - A. Apaydin
AU  - A. Otaran
AU  - V. Patoglu
PY  - 2018
KW  - actuators
KW  - brakes
KW  - closed loop systems
KW  - elasticity
KW  - force control
KW  - force feedback
KW  - regenerative braking
KW  - robust control
KW  - series elastic brake pedal
KW  - force-feedback brake pedal
KW  - series elastic actuation
KW  - closed-loop force control
KW  - pedal feel compensation
KW  - regenerative braking
KW  - robust controller
KW  - fidelity force control
KW  - impedance characteristic
KW  - frequency spectrum
KW  - Brakes
KW  - Force
KW  - Friction
KW  - Force control
KW  - Actuators
KW  - Vehicles
KW  - Couplings
DO  - 10.1109/IROS.2018.8594317
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose a force-feedback brake pedal with series elastic actuation to preserve the conventional brake pedal feel during cooperative regenerative braking. The novelty of the proposed design is due to the deliberate introduction of a compliant element between the actuator and the brake pedal whose deflections are measured to estimate interaction forces and to perform closed-loop force control. Thanks to its series elasticity, the force-feedback brake pedal can utilize robust controllers to achieve high fidelity force control, possesses favorable output impedance characteristics over the entire frequency spectrum, and can be implemented in a compact package using low-cost components. The applicability and effectiveness of the proposed series elastic brake pedal have been tested through human subject experiments that evaluate simulated cooperative regenerative braking scenarios with and without pedal feel compensation. The experimental results and responses to the accompanying questionnaire indicate that pedal feel compensation through the series elastic brake pedal can significantly decrease hard braking instances, improving safety and driver experience.
ER  - 

TY  - CONF
TI  - Unmanned Aerial Auger for Underground Sensor Installation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1374
EP  - 1381
AU  - Y. Sun
AU  - A. Plowcha
AU  - M. Nail
AU  - S. Elbaum
AU  - B. Terry
AU  - C. Detweiler
PY  - 2018
KW  - Auger effect
KW  - autonomous aerial vehicles
KW  - geophysical equipment
KW  - geophysical techniques
KW  - sensors
KW  - soil
KW  - underground equipment
KW  - digging mechanism
KW  - power consumption
KW  - unmanned aerial systems
KW  - target soil sensors
KW  - unmanned aerial auger performance
KW  - UAS
KW  - underground sensor installation
KW  - depth 120.0 mm
KW  - Force
KW  - Robot sensing systems
KW  - Substrates
KW  - Fasteners
KW  - Monitoring
KW  - Soil moisture
DO  - 10.1109/IROS.2018.8593824
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Using an Unmanned Aerial Systems (UAS) to autonomously deploy soil sensors enables their installation in otherwise hard to access locations. In this paper, we present a system that integrates a UAS and a digging mechanism which can carry, secure, and install a small sensor into dirt effectively and efficiently. The integrated system includes 1) a low profile, light-weight, inexpensive auger mechanism, 2) a sensor carrying and deploying mechanism with low power consumption, and 3) sensors and software that control and evaluate the auger performance during digging. When tested on a suite of target soils and a target depth of 120mm, the system achieved a success rate of 100% for indoor tests and 92.5% for outdoors, verifying the potential of the approach.
ER  - 

TY  - CONF
TI  - Enhanced Non-Steady Gliding Performance of the MultiMo-Bat through Optimal Airfoil Configuration and Control Strategy
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1382
EP  - 1388
AU  - H. Kim
AU  - M. A. Woodward
AU  - M. Sitti
PY  - 2018
KW  - aerodynamics
KW  - aerospace components
KW  - autonomous aerial vehicles
KW  - design engineering
KW  - drag
KW  - mobile robots
KW  - optimal control
KW  - pitch control (position)
KW  - robot dynamics
KW  - active pitch control strategy
KW  - center-of-mass location
KW  - morphological intelligence
KW  - optimal control strategy
KW  - collapsible airfoils
KW  - nonsteady-state gliding performance
KW  - gliding robots
KW  - drag coefficients
KW  - aerodynamic complexities
KW  - Robots
KW  - Automotive components
KW  - Aerodynamics
KW  - Optimization
KW  - Atmospheric modeling
KW  - Trajectory
KW  - Springs
DO  - 10.1109/IROS.2018.8593613
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Many robots make use of gravitational potential energy, generated by another mode, to enhance mobility through gliding locomotion. However, unstructured environments can create situations in which the initial conditions for steady-state gliding cannot be achieved; for example, jumping out of a hole, where the obstacle is very close to the robot. This paper suggests an optimization methodology for finding airfoil configurations and control strategies to maximize the effective non-steady-state gliding ratio for the most challenging initial condition, that of zero velocity. Parameters for the optimization are a location of a robot's center-of-mass in relation to its center-of-pressure and, through the addition of a tail, an active pitch control strategy. The optimal center-of-mass location produces the best passive gliding performance (morphological intelligence), and the optimal control strategy improves the gliding distance. Due to the aerodynamic complexities of modeling the collapsible airfoils, we find the optimal location of the center-of-mass from gliding experiments performed on the robot at different center-of-mass locations and initial pitch angles. An optimal location of the center-of-mass was found to be 40% of the wing chord for our robotic platform; measured from the wing's leading edge. The optimal location has a wide range of initial pitch angles which result in stable, yet non-steady-state, gliding behaviors. The morphological intelligence built into our robotic platform creates two observable dynamic behaviors, that of horizontal velocity gain and sink rate minimization. We then estimate the drag coefficients from the experiments, and conduct dynamic simulations to optimize the pitch control strategy. The design methodology presented here can enhance the non-steady-state gliding performance of a broad range of gliding robots, and the control strategy can further enhance performance on those which utilize an active tail.
ER  - 

TY  - CONF
TI  - Active Range and Bearing-based Radiation Source Localization
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1389
EP  - 1394
AU  - M. S. Lee
AU  - D. Shy
AU  - W. R. Whittaker
AU  - N. Michael
PY  - 2018
KW  - cameras
KW  - image sensors
KW  - radioactive sources
KW  - static step size
KW  - radiation mapping approach
KW  - active source localization approach
KW  - adaptive step size
KW  - localization time
KW  - 3D radiation source localization
KW  - bearing sensor
KW  - Compton gamma camera
KW  - image radiation
KW  - source locations
KW  - active source localization framework
KW  - Fisher Information
KW  - bearing-based radiation source localization
KW  - passive source localization
KW  - size 0.26 m
KW  - Sensors
KW  - Cameras
KW  - Photonics
KW  - Three-dimensional displays
KW  - Image sensors
KW  - Position measurement
KW  - Two dimensional displays
DO  - 10.1109/IROS.2018.8593625
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - 3D radiation source localization is a common task across applications such as decommissioning, disaster response, and security, but traditional count-based sensors struggle to efficiently disambiguate between symmetries in sensor, source, and environment configurations. Recent works have demonstrated successful passive source localization using a bearing sensor called the Compton gamma camera that can image radiation. This paper first presents an approach to mapping the spatial distribution of radiation with a gamma camera to estimate source locations. An active source localization framework is then developed that greedily selects new waypoints that maximize the Fisher Information provided by the camera's range and bearing observations for source localization. Finally the common assumption of a static step size in between waypoints is relaxed to allow step sizes to adapt online to the observed information. The proposed radiation mapping approach is evaluated in 5√ó4 m2 and 14√ó6 m2 laboratory environments, where multiple point sources were localized to within an average of 0.26 m or 0.6% of the environment dimensions. The active source localization approach is evaluated in simulation and an adaptive step size yields a 27% decrease in the localization time and a 16% decrease in the distance traveled to localize a source in a 15√ó15√ó15 m3 environment.
ER  - 

TY  - CONF
TI  - Development of Camber-Flat Wing Structure Convert Mechanism for Asymmetric Flapping Micro Air Vehicle
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1395
EP  - 1400
AU  - J. Jang
AU  - G. Yang
PY  - 2018
KW  - aerodynamics
KW  - aerospace components
KW  - autonomous aerial vehicles
KW  - design engineering
KW  - vehicle dynamics
KW  - r asymmetric flapping micro air vehicle
KW  - rigidity
KW  - camber-flat wing structure convert mechanism
KW  - Force
KW  - Muscles
KW  - Robots
KW  - Drag
KW  - Force measurement
KW  - Insects
KW  - Actuators
DO  - 10.1109/IROS.2018.8594104
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This study presents principle of the camber-flat wing structure conversion mechanism, which is inspired by a dragonfly, and its applicability to MAV. The camber-flat wing structure convert mechanism makes MAV flight using asymmetric flapping pattern through control of angle of attack without complicate structure. This mechanism was inspired from the dragonfly's feature that the camber structure of the wing increases the rigidity of wing structure and makes dragonfly has asymmetric flapping pattern. Experimental results show that MAV has asymmetric flapping pattern that can more stable flight performance when hovering flight with a camber structure and superior performance when the forward flight with a flat structure. The average lift force in the camber wing structure was 0.02N, the average thrust force was 0.02N and the average lift force was 0.011N in the flat wing structure at 20 Hz flapping frequency.
ER  - 

TY  - CONF
TI  - Robotic Boreblending: The Future of In-Situ Gas Turbine Repair
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1401
EP  - 1406
AU  - D. Alatorre
AU  - B. Nasser
AU  - A. Rabani
AU  - A. Nagy-Sochacki
AU  - X. Dong
AU  - D. Axinte
AU  - J. Kell
PY  - 2018
KW  - compressors
KW  - gas turbines
KW  - industrial robots
KW  - inspection
KW  - maintenance engineering
KW  - maintenance
KW  - robot flexible joints
KW  - kinematic analysis
KW  - In-Situ Gas Turbine Repair
KW  - robotic boreblending
KW  - Tools
KW  - Blades
KW  - Maintenance engineering
KW  - Turbines
KW  - Joints
KW  - End effectors
DO  - 10.1109/IROS.2018.8594155
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Automation of inspection and repair tasks on complex installations is gaining attention from industries with high-value assets such as aerospace, nuclear and marine. This paper reports on a five degrees of freedom robotic system capable of performing accurate and repeatable repair procedures through a narrow inspection port, which minimizes the cost and downtime associated with unscheduled maintenance. Careful study of the target working volume and repair process informed the design of a robotic probe capable of replicating the operation. Kinematic analysis of the robot's flexible, prismatic and rotary joints was used to define accurate machining paths in 3D space, and the results were verified using an optical motion capture system (accuracy of 0.25 mm). After comprehensive verifications of the constitutive elements, the robotic system was successfully demonstrated for repair of a high-pressure compressor aerofoil in a gas turbine. The results not only proves the ability of the system to address such difficult repair scenarios but also highlights a domain of opportunities in developing specialist robotics for repair of high-value assets, which is a subject to growing global demand.
ER  - 

TY  - CONF
TI  - Design of an Autonomous Robot for Mapping, Navigation, and Manipulation in Underground Mines
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1407
EP  - 1412
AU  - R. L√∂sch
AU  - S. Grehl
AU  - M. Donner
AU  - C. Buhl
AU  - B. Jung
PY  - 2018
KW  - cameras
KW  - inertial systems
KW  - manipulators
KW  - mining
KW  - mobile robots
KW  - robot vision
KW  - sensors
KW  - autonomous driving
KW  - autonomous robot
KW  - manipulation
KW  - underground mines
KW  - dangerous working environment
KW  - harsh environment
KW  - robot design
KW  - underground objects
KW  - manipulating objects
KW  - robotic arm
KW  - robust four wheeled platform
KW  - depth cameras
KW  - inertial measurement unit
KW  - autonomous navigation
KW  - Robot sensing systems
KW  - Manipulators
KW  - Cameras
KW  - Navigation
KW  - Mobile robots
DO  - 10.1109/IROS.2018.8594190
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Underground mines are a dangerous working environment and, therefore, robots could help putting less humans at risk. Traditional robots, sensors, and software often do not work reliably underground due to the harsh environment. This paper analyzes requirements and presents a robot design capable of navigating autonomously underground and manipulating objects with a robotic arm. The robot's base is a robust four wheeled platform powered by electric motors and able to withstand the harsh environment. It is equipped with color and depth cameras, lighting, laser scanners, an inertial measurement unit, and a robotic arm. We conducted two experiments testing mapping and autonomous navigation. Mapping a 75 meters long route including a loop closure results in a map that qualitatively matches the original map to a good extent. Testing autonomous driving on a previously created map of a second, straight, 150 meters long route was also successful. However, without loop closure, rotation errors cause apparent deviations in the created map. These first experiments showed the robot's operability underground.
ER  - 

TY  - CONF
TI  - Design and Performance Evaluation of an Infotaxis-Based Three-Dimensional Algorithm for Odor Source Localization
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1413
EP  - 1420
AU  - J. Ruddick
AU  - A. Marjovi
AU  - F. Rahbar
AU  - A. Martinoli
PY  - 2018
KW  - electronic noses
KW  - gases
KW  - mobile robots
KW  - probability
KW  - wind tunnels
KW  - gaseous leak source
KW  - high wind speeds
KW  - environmental conditions
KW  - environmental parameters
KW  - multiple algorithmic parameters
KW  - wind tunnel
KW  - probabilistic Infotaxis algorithm
KW  - odor source localization
KW  - infotaxis-based three-dimensional algorithm
KW  - Robots
KW  - Entropy
KW  - Atmospheric modeling
KW  - Mathematical model
KW  - Probabilistic logic
KW  - Numerical models
KW  - Probability
DO  - 10.1109/IROS.2018.8593997
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper we tackle the problem of finding the source of a gaseous leak with a robot in a three-dimensional (3-D) physical space. The proposed method extends the operational range of the probabilistic Infotaxis algorithm [1] into 3-D and makes multiple improvements in order to increase its performance in such settings. The method has been tested systematically through high-fidelity simulations and in a wind tunnel emulating realistic conditions. The impact of multiple algorithmic and environmental parameters has been studied in the experiments. The algorithm shows good performance in various environmental conditions, particularly in high wind speeds and different source release rates.
ER  - 

TY  - CONF
TI  - Cognition-enabled Framework for Mixed Human-Robot Rescue Teams
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1421
EP  - 1428
AU  - F. Yazdani
AU  - G. Kazhoyan
AU  - A. K. Bozcuoƒülu
AU  - A. Haidu
AU  - F. B√°lint-Bencz√©di
AU  - D. Be√üler
AU  - M. Pomarlan
AU  - M. Beetz
PY  - 2018
KW  - cognition
KW  - human-robot interaction
KW  - mobile robots
KW  - telerobotics
KW  - cognition-enabled framework
KW  - rescue missions
KW  - cognitive capabilities
KW  - robot behavior
KW  - human-robot rescue teams
KW  - human-robot interaction
KW  - visibility areas
KW  - robotic systems teleoperation
KW  - locomotion areas
KW  - belief state representations
KW  - Cognition
KW  - Robot sensing systems
KW  - Geographic information systems
KW  - Task analysis
KW  - Lakes
KW  - Bridges
DO  - 10.1109/IROS.2018.8594311
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - With the advancements in robotic technology and the progress in human-robot interaction research, the interest in deploying mixed human-robot teams in rescue missions is increasing. Due to their complementary capabilities in terms of locomotion, visibility and reachability of areas, human-robot teams are considerably deployed in real-world settings, albeit the robotic agents in such scenarios are normally fully teleoperated. A major barrier to successful and efficient mission execution in those teams is the lack of cognitive skills in robotic systems. In this paper, we present a cognition-enabled framework and an implemented system where robotic agents are equipped with cognitive capabilities to naturally communicate with humans and autonomously perform tasks. The framework allows for natural tasking of robots, reasoning about robot behavior, capabilities and actions, and a common belief state representation for shared mission awareness of robots and human operators.
ER  - 

TY  - CONF
TI  - Pulleys and Force Sensors Influence on Payload Estimation of Cable-Driven Parallel Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1429
EP  - 1436
AU  - √â. Picard
AU  - S. Caro
AU  - F. Claveau
AU  - F. Plestan
PY  - 2018
KW  - cables (mechanical)
KW  - feedforward
KW  - force sensors
KW  - manipulators
KW  - motion control
KW  - pulleys
KW  - robot kinematics
KW  - robust control
KW  - torque control
KW  - Cable-Driven Parallel robots
KW  - suspended Cable-Driven Parallel Robot
KW  - heavy objects
KW  - heterogeneous objects
KW  - payload mass
KW  - robust control
KW  - pulleys
KW  - payload estimation
KW  - geometric model
KW  - mass estimations
KW  - cable tensions
KW  - torque controller
KW  - real-time mass compensation
KW  - CDPR prototype
KW  - force sensors influence
KW  - pick-and-place operations
KW  - Pulleys
KW  - Payloads
KW  - Mechanical cables
KW  - Estimation
KW  - Robots
KW  - Prototypes
KW  - Trajectory
DO  - 10.1109/IROS.2018.8594171
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The subject of this paper is about the use of a suspended Cable-Driven Parallel Robot (CDPR) for pick-and-place operations of heavy and heterogeneous objects. The knowledge of the payload mass and its center of mass in realtime is an asset for robust control of the device, which is required to ensure a good stability, especially when the objects have different shapes, sizes and masses. Accordingly, this paper aims at experimentally evaluating the effects of (i) the pulleys modeling and (ii) the use of force sensors for the payload estimation. It turns out that the consideration of the pulleys into the geometric model of the robot improves the mass and center of mass estimations of the payload. A comparison is made between the estimation of cable tensions from force sensors and from motor currents. Finally, a torque controller with a feedforward term for real-time mass compensation is proposed and implemented on a CDPR prototype.
ER  - 

TY  - CONF
TI  - 3D-printed flexure-based finger joints for anthropomorphic hands
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1437
EP  - 1442
AU  - L. Garcia
AU  - M. Naves
AU  - D. M. Brouwer
PY  - 2018
KW  - dexterous manipulators
KW  - grippers
KW  - hinges
KW  - production engineering computing
KW  - prosthetics
KW  - springs (mechanical)
KW  - three-dimensional printing
KW  - 3D printed flexure-based finger joints
KW  - grasping force
KW  - load bearing capacity
KW  - anthropomorphic hands
KW  - nonflexure-based prosthetic hands
KW  - presented joints power grasping capability outperform current state flexure-base hands
KW  - Angle Three-Flexure Cross Hinge
KW  - Fasteners
KW  - Force
KW  - Topology
KW  - Grasping
KW  - Stress
KW  - Optimization
KW  - Load modeling
KW  - Compliant joints
KW  - flexures
KW  - robotic hand
KW  - prosthetic hand
KW  - anthropomorphic
KW  - additive manufacturing
DO  - 10.1109/IROS.2018.8594102
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Flexure-based finger joints for prosthetic hands have been studied, but until now they lack stiffness and load bearing capacity. In this paper we present a design which combines large range of motion, stiffness and load bearing capacity, with an overload protection mechanism. Several planar and non-planar hinge topologies are studied to determine load capacity over the range of motion. Optimized topologies are compared, in 30 degrees deflected state, in terms of stresses by deflection and grasping forces. Additionally, support stiffnesses were computed for all hinges in the whole range of motion (45 degrees). The Hole Cross Hinge presented the best performance over the range of motion with a grasping force up to 15 N while deflected 30 degrees. A new concept, the Angle Three-Flexure Cross Hinge, provides outstanding performance for deflections from 17.5 up to 30 degrees with a 20 N maximum grasping force when fully deflected. Experimental verification of the support stiffness over the range of motion shows some additional compliances, but the stiffness trend of the printed hinge is in line with the model. The presented joints power grasping capability outperform current state flexure-base hands and are comparable to commercial non-flexure-based prosthetic hands. In the event of excessive loads, an overload protection mechanism is in place to protect the flexure- hinges.
ER  - 

TY  - CONF
TI  - Body-Mounted Robot for Image-Guided Percutaneous Interventions: Mechanical Design and Preliminary Accuracy Evaluation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1443
EP  - 1448
AU  - N. A. Patel
AU  - J. Yan
AU  - D. Levi
AU  - R. Monfaredi
AU  - K. Cleary
AU  - I. Iordachita
PY  - 2018
KW  - biomedical MRI
KW  - computerised tomography
KW  - diagnostic radiography
KW  - image registration
KW  - manipulators
KW  - medical image processing
KW  - medical robotics
KW  - needles
KW  - robot kinematics
KW  - surgery
KW  - needle-based percutaneous interventions
KW  - biopsy seed placement
KW  - brachytherapy seed placement
KW  - robot mechanism
KW  - Magnetic Resonance Imaging
KW  - repeatable robot registration
KW  - robot kinematics
KW  - robot calibration procedure
KW  - robotic manipulator
KW  - body-mounted robot
KW  - image-guided percutaneous interventions
KW  - MRI guidance
KW  - Computed Tomography
KW  - shoulder arthrography
KW  - Robot kinematics
KW  - Magnetic resonance imaging
KW  - Needles
KW  - Manipulators
KW  - Computed tomography
KW  - Calibration
DO  - 10.1109/IROS.2018.8593807
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a body-mounted, four degree-of-freedom (4-DOF) parallel mechanism robot for image-guided percutaneous interventions. The design of the robot is optimized to be light weight and compact such that it could be mounted to the patient body. It has a modular design that can be adopted for assisting various image-guided, needle-based percutaneous interventions such as arthrography, biopsy and brachytherapy seed placement. The robot mechanism and the control system are designed and manufactured with components compatible with imaging modalities including Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). The current version of the robot presented in this paper is optimized for shoulder arthrography under MRI guidance; a Z-shaped fiducial frame is attached to the robot, providing accurate and repeatable robot registration with the MR scanner coordinate system. Here we present the mechanical design of the manipulator, robot kinematics, robot calibration procedure, and preliminary bench-top accuracy assessment. The bench-top accuracy evaluation of the robotic manipulator shows average translational error of 1.01 mm and 0.96 mm in X and Z axes, respectively, and average rotational error of 3.06 degrees and 2.07 degrees about the X and Z axes, respectively.
ER  - 

TY  - CONF
TI  - HERI II: A Robust and Flexible Robotic Hand based on Modular Finger design and Under Actuation Principles
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1449
EP  - 1455
AU  - Z. Ren
AU  - N. Kashiri
AU  - C. Zhou
AU  - N. G. Tsagarakis
PY  - 2018
KW  - actuators
KW  - elasticity
KW  - manipulators
KW  - position measurement
KW  - design effectiveness
KW  - resilient manipulation
KW  - robust manipulation
KW  - Centauro Robot
KW  - transmission system
KW  - motor current readings
KW  - finger phalanxes
KW  - contact pressure
KW  - absolute position measurements
KW  - precise grasping
KW  - delicate grasping
KW  - sensory system
KW  - under-actuated transmission
KW  - single actuator
KW  - finger module
KW  - finger arrangement
KW  - highly integrated modular finger units
KW  - under-actuated hand
KW  - actuation principles
KW  - modular finger design
KW  - HERI II
KW  - Grasping
KW  - Tendons
KW  - Force
KW  - Pulleys
KW  - Thumb
KW  - Robots
DO  - 10.1109/IROS.2018.8594507
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper introduces the design of a novel under-actuated hand with highly integrated modular finger units, which can be easily reconfigured in terms of finger arrangement and number to account for the manipulation needs of different applications. Each finger module is powered by a single actuator through an under-actuated transmission and equipped with a sensory system for delicate and precise grasping, which includes absolute position measurements, contact pressure sensing at finger phalanxes and motor current readings. Finally, intrinsic elasticity integrated in the transmission system make the hand robust and adaptive to impacts when interacting with the objects and environment. This highly integrated hand (HERI II) was developed for the Centauro Robot to enable robust and resilient manipulation. A set of experiments demonstrating the hand's grasping performance were carried out and fully verified the design effectiveness of the proposed hand.
ER  - 

TY  - CONF
TI  - Design, Modeling and Control of a Soft Robotic Arm
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1456
EP  - 1463
AU  - M. Hofer
AU  - R. D'Andrea
PY  - 2018
KW  - closed loop systems
KW  - design engineering
KW  - linear quadratic Gaussian control
KW  - manipulators
KW  - nonlinear control systems
KW  - pendulums
KW  - pressure control
KW  - stability
KW  - valves
KW  - soft robotic arm
KW  - hybrid robotic arm
KW  - soft bladders
KW  - inflatable bladders
KW  - low cost switching valves
KW  - pressure control
KW  - valve model
KW  - system identification
KW  - linear quadratic Gaussian controller
KW  - closed loop control performance
KW  - stabilization
KW  - rotational inverted pendulum
KW  - Furuta pendulum
KW  - Actuators
KW  - Valves
KW  - Manipulators
KW  - Fabrics
KW  - Welding
KW  - Switches
DO  - 10.1109/IROS.2018.8594221
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper we present the design of a hybrid robotic arm using soft, inflatable bladders for actuation. Low cost switching valves are used for pressure control, where the valve model is identified experimentally. A model of the robotic arm is derived based on system identification and used to derive a linear quadratic Gaussian controller. A method to solve limitations of the employed switching valves is proposed and experimentally proven to improve tracking performance. The closed loop control performance of the robotic arm is demonstrated by stabilizing a rotational inverted pendulum known as the Furuta pendulum.
ER  - 

TY  - CONF
TI  - Energy-Efficient Design and Control of a Vibro-Driven Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1464
EP  - 1469
AU  - P. Liu
AU  - G. Neumann
AU  - Q. Fu
AU  - S. Pearson
AU  - H. Yu
PY  - 2018
KW  - control system synthesis
KW  - feedback
KW  - friction
KW  - mobile robots
KW  - motion control
KW  - nonlinear control systems
KW  - pendulums
KW  - position control
KW  - robot dynamics
KW  - springs (mechanical)
KW  - stick-slip
KW  - trajectory generation profile
KW  - VDR systems
KW  - nonlinear-motion prototype
KW  - physical robot
KW  - dynamic contributions
KW  - driving pendulum
KW  - partial feedback controller
KW  - tracking control
KW  - noncollocated constraint conditions
KW  - travelling distance
KW  - passive dynamics
KW  - friction-induced stick-slip motions
KW  - spring-augmented pendulum
KW  - open problems
KW  - underactuated nature
KW  - locomotion
KW  - vibro-driven robotic systems
KW  - energy-efficient design
KW  - Robots
KW  - Trajectory
KW  - Force
KW  - Dynamics
KW  - Friction
KW  - Energy efficiency
KW  - Acceleration
DO  - 10.1109/IROS.2018.8594322
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Vibro-driven robotic (VDR) systems use stick-slip motions for locomotion. Due to the underactuated nature of the system, efficient design and control are still open problems. We present a new energy preserving design based on a spring-augmented pendulum. We indirectly control the friction-induced stick-slip motions by exploiting the passive dynamics in order to achieve an improvement in overall travelling distance and energy efficiency. Both collocated and non-collocated constraint conditions are elaborately analysed and considered to obtain a desired trajectory generation profile. For tracking control, we develop a partial feedback controller for the driving pendulum which counteracts the dynamic contributions from the platform. Comparative simulation studies show the effectiveness and intriguing performance of the proposed approach, while its feasibility is experimentally verified through a physical robot. Our robot is to the best of our knowledge the first nonlinear-motion prototype in literature towards the VDR systems.
ER  - 

TY  - CONF
TI  - Design of Compliant Mechanosensory Composite (CMC) and its Application Toward the Sensible Mesoscale Robotics
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1470
EP  - 1475
AU  - B. Kwak
AU  - J. Bae
PY  - 2018
KW  - angular measurement
KW  - coils
KW  - composite materials
KW  - conducting polymers
KW  - contact resistance
KW  - grippers
KW  - intelligent sensors
KW  - mobile robots
KW  - motion measurement
KW  - embedded sensing ability
KW  - conductive polymer PEDOT:PSS
KW  - CMC process
KW  - sensible mesoscale robotics
KW  - macroscale robots
KW  - compliant mechanosensory composite design
KW  - locomotory modulation
KW  - electric contact resistance
KW  - ECR
KW  - bending angle estimation
KW  - cyclic bending analysis
KW  - sparsely printed serpentine pattern
KW  - SMA coil
KW  - shape memory alloy coil
KW  - embedded sensors
KW  - size 0.1 mm to 10.0 mm
KW  - Robot sensing systems
KW  - Resistance
KW  - Fabrication
KW  - Contact resistance
KW  - Manufacturing processes
DO  - 10.1109/IROS.2018.8593590
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Sensed information greatly helps a robot to adjust its motion or modulate the locomotory behavior. While many sensing components have been developed for macroscale robots, such off-the-shelf sensors are hardly integrated with a mesoscale (i.e., 0.1 mm to 10 mm) robot due to the size limitation. In this work, we propose a Compliant Mechanosensory Composite (CMC) to fabricate a small compliant mechanism with embedded sensing ability. As the first demonstration of CMC, we directly print a conductive polymer PEDOT:PSS onto the flexible joint of a compliant mechanism to sense the motion of the flexible joint itself. Owing to the variation of electric contact resistance (ECR) upon bending, the CMC could estimate its bending angle. The performance of the CMC was verified by analyzing the cyclic bending, transient and stationary response. Overall, a sparsely printed serpentine pattern with thicker line exhibited consistent response without a noticeable hysteresis. To demonstrate the applicability of the CMC process, a small gripper actuated by a SMA (shape memory alloy) coil was fabricated, and its motion was successfully measured using the embedded sensors. We expect the proposed CMC will enable a small robot to become sensible at its self motion, external load, and physical contacts in future.
ER  - 


