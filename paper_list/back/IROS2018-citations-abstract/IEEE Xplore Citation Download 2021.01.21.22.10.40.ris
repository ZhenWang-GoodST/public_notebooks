TY  - CONF
TI  - [Front cover]
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1
EP  - 1
PY  - 2018
DO  - 10.1109/IROS.2018.8593956
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Presents the front cover or splash screen of the proceedings record.
ER  - 

TY  - CONF
TI  - Program at a Glance
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 4
EP  - 4
PY  - 2018
DO  - 10.1109/IROS.2018.8594269
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Provides a schedule of conference events and a listing of which papers were presented in each session.
ER  - 

TY  - CONF
TI  - Welcome message
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 11
EP  - 12
PY  - 2018
DO  - 10.1109/IROS.2018.8593673
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
ER  - 

TY  - CONF
TI  - 3. Conference Application
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 6
EP  - 10
PY  - 2018
DO  - 10.1109/IROS.2018.8594286
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - IROS 2018 Proceedings will be given in electronic iProceeding or eProceeding format, based on availability, to every full-registered person in the conference.
ER  - 

TY  - CONF
TI  - Organizing Committee
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 13
EP  - 21
PY  - 2018
DO  - 10.1109/IROS.2018.8594191
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Provides a listing of current committee members and society officers.
ER  - 

TY  - CONF
TI  - About Madrid
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 26
EP  - 32
PY  - 2018
DO  - 10.1109/IROS.2018.8593784
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Presents information on the conference venue.
ER  - 

TY  - CONF
TI  - Sponsors
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 33
EP  - 37
PY  - 2018
DO  - 10.1109/IROS.2018.8593949
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The conference organizers greatly appreciate the support of the various corporate sponsors listed.
ER  - 

TY  - CONF
TI  - Plenary sessions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 38
EP  - 54
PY  - 2018
KW  - Artificial intelligence
KW  - Humanoid robots
KW  - Robot sensing systems
KW  - Collaboration
KW  - Biographies
KW  - Neural networks
DO  - 10.1109/IROS.2018.8594490
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Provides an abstract for each of the plenary presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
ER  - 

TY  - CONF
TI  - Forums
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 55
EP  - 65
PY  - 2018
KW  - Service robots
KW  - Artificial intelligence
KW  - Companies
KW  - Europe
KW  - Smart cities
DO  - 10.1109/IROS.2018.8594044
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Provides an abstract for each of the Forum presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
ER  - 

TY  - CONF
TI  - Workshops
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 68
EP  - 90
PY  - 2018
KW  - Conferences
KW  - Service robots
KW  - Diseases
KW  - Task analysis
KW  - Grasping
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8593620
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Provides an abstract for each of the workshop presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.
ER  - 

TY  - CONF
TI  - IROS 2018 Technical Program
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1
EP  - 12
PY  - 2018
DO  - 10.1109/IROS.2018.8593782
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Provides a schedule of conference events and a listing of which papers were presented in each session.
ER  - 

TY  - CONF
TI  - Content List
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1
EP  - 118
PY  - 2018
DO  - 10.1109/IROS.2018.8593802
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Presents the table of contents/splash page of the proceedings record.
ER  - 

TY  - CONF
TI  - IROS 2018 Author Index
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1
EP  - 41
PY  - 2018
DO  - 10.1109/IROS.2018.8594323
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Presents an index of the authors whose articles are published in the conference proceedings record.
ER  - 

TY  - CONF
TI  - Index of papers
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 1
EP  - 31
PY  - 2018
DO  - 10.1109/IROS.2018.8593509
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Presents the table of contents/splash page of the proceedings record.
ER  - 

TY  - CONF
TI  - Real-time Convolutional Networks for Depth-based Human Pose Estimation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 41
EP  - 47
AU  - A. Martínez-González
AU  - M. Villamizar
AU  - O. Canévet
AU  - J. Odobez
PY  - 2018
KW  - convolutional neural nets
KW  - feature extraction
KW  - human-robot interaction
KW  - image colour analysis
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - convolutional neural networks models
KW  - human robot interaction
KW  - depth-based human pose estimation
KW  - pose inference
KW  - residual blocks
KW  - body landmark localization
KW  - depth imaging
KW  - human bodies
KW  - human detection
KW  - RGB images
KW  - Feature extraction
KW  - Pose estimation
KW  - Computational modeling
KW  - Three-dimensional displays
KW  - Shape
KW  - Detectors
KW  - Cameras
DO  - 10.1109/IROS.2018.8593383
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose to combine recent Convolutional Neural Networks (CNN) models with depth imaging to obtain a reliable and fast multi-person pose estimation algorithm applicable to Human Robot Interaction (HRI) scenarios. Our hypothesis is that depth images contain less structures and are easier to process than RGB images while keeping the required information for human detection and pose inference, thus allowing the use of simpler networks for the task. Our contributions are threefold. (i) we propose a fast and efficient network based on residual blocks (called RPM) for body landmark localization from depth images; (ii) we created a public dataset DIH comprising more than 170k synthetic images of human bodies with various shapes and viewpoints as well as real (annotated) data for evaluation; (iii) we show that our model trained on synthetic data from scratch can perform well on real data, obtaining similar results to larger models initialized with pre-trained networks. It thus provides a good trade-off between performance and computation. Experiments on real data demonstrate the validity of our approach.
ER  - 

TY  - CONF
TI  - Detection- Tracking for Efficient Person Analysis: The DetTA Pipeline
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 48
EP  - 53
AU  - S. Breuers
AU  - L. Beyer
AU  - U. Rafi
AU  - B. Leibel
PY  - 2018
KW  - feature extraction
KW  - human-robot interaction
KW  - image filtering
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - robot vision
KW  - DetTA pipeline
KW  - people detection
KW  - dynamic information
KW  - social robot-person interaction
KW  - fully modular detection-tracking-analysis pipeline
KW  - temporal filtering
KW  - person attribute
KW  - track ID
KW  - person analysis
KW  - GPU-memory
KW  - power consumption
KW  - head pose
KW  - skeleton pose
KW  - deep learning methods
KW  - Robots
KW  - Pipelines
KW  - Skeleton
KW  - Head
KW  - Estimation
KW  - Detectors
KW  - Trajectory
DO  - 10.1109/IROS.2018.8594335
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In the past decade many robots were deployed in the wild, and people detection and tracking is an important component of such deployments. On top of that, one often needs to run modules which analyze persons and extract higher level attributes such as age and gender, or dynamic information like gaze and pose. The latter ones are especially necessary for building a reactive, social robot-person interaction. In this paper, we combine those components in a fully modular detection-tracking-analysis pipeline, called DetTA. We investigate the benefits of such an integration on the example of head and skeleton pose, by using the consistent track ID for a temporal filtering of the analysis modules' observations, showing a slight improvement in a challenging real-world scenario. We also study the potential of a so-called “free-flight” mode, where the analysis of a person attribute only relies on the filter's predictions for certain frames. Here, our study shows that this boosts the runtime dramatically, while the prediction quality remains stable. This insight is especially important for reducing power consumption and sharing precious (GPU-)memory when running many analysis components on a mobile platform, especially so in the era of expensive deep learning methods.
ER  - 

TY  - CONF
TI  - 3D Human Pose Estimation on a Configurable Bed from a Pressure Image
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 54
EP  - 61
AU  - H. M. Clever
AU  - A. Kapusta
AU  - D. Park
AU  - Z. Erickson
AU  - Y. Chitalia
AU  - C. C. Kemp
PY  - 2018
KW  - convolutional neural nets
KW  - manipulators
KW  - Monte Carlo methods
KW  - pose estimation
KW  - stereo image processing
KW  - single pressure image
KW  - convolutional neural networks
KW  - flat beds
KW  - pressure-sensing mat
KW  - bedding materials
KW  - robots
KW  - configurable bed
KW  - 3D human pose estimation
KW  - estimated kinematic model
KW  - pressure mat
KW  - mean joint position error
KW  - bed configurations
KW  - limb lengths
KW  - 3D joint positions
KW  - size 77.0 mm
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Kinematics
KW  - Skeleton
KW  - Robot sensing systems
KW  - Two dimensional displays
DO  - 10.1109/IROS.2018.8593545
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robots have the potential to assist people in bed, such as in healthcare settings, yet bedding materials like sheets and blankets can make observation of the human body difficult for robots. A pressure-sensing mat on a bed can provide pressure images that are relatively insensitive to bedding materials. However, prior work on estimating human pose from pressure images has been restricted to 2D pose estimates and flat beds. In this work, we present two convolutional neural networks to estimate the 3D joint positions of a person in a configurable bed from a single pressure image. The first network directly outputs 3D joint positions, while the second outputs a kinematic model that includes estimated joint angles and limb lengths. We evaluated our networks on data from 17 human participants with two bed configurations: supine and seated. Our networks achieved a mean joint position error of 77 mm when tested with data from people outside the training set, outperforming several baselines. We also present a simple mechanical model that provides insight into ambiguity associated with limbs raised off of the pressure mat, and demonstrate that Monte Carlo dropout can be used to estimate pose confidence in these situations. Finally, we provide a demonstration in which a mobile manipulator uses our network's estimated kinematic model to reach a location on a person's body in spite of the person being seated in a bed and covered by a blanket.
ER  - 

TY  - CONF
TI  - Estimating Metric Poses of Dynamic Objects Using Monocular Visual-Inertial Fusion
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 62
EP  - 68
AU  - K. Qiu
AU  - T. Qin
AU  - H. Xie
AU  - S. Shen
PY  - 2018
KW  - augmented reality
KW  - cameras
KW  - feature extraction
KW  - image fusion
KW  - image sequences
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - robot vision
KW  - state estimation
KW  - metric pose estimation
KW  - state estimation
KW  - 3D tracking performance
KW  - tracking accuracy
KW  - correlation analysis-based metric scale estimator
KW  - 2D object tracker
KW  - monocular camera
KW  - visual-inertial system
KW  - monocular sensing suite
KW  - scale observability
KW  - fixed multicamera
KW  - visual-inertial tracking system
KW  - arbitrary dynamic object
KW  - monocular 3D object tracking system
KW  - monocular visual-inertial fusion
KW  - dynamic objects
KW  - Cameras
KW  - Three-dimensional displays
KW  - Visualization
KW  - Estimation
KW  - Two dimensional displays
KW  - Tracking
DO  - 10.1109/IROS.2018.8593748
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - A monocular 3D object tracking system generally has only up-to-scale pose estimation results without any prior knowledge of the tracked object. In this paper, we propose a novel idea to recover the metric scale of an arbitrary dynamic object by optimizing the trajectory of the objects in the world frame, without motion assumptions. By introducing an additional constraint in the time domain, our monocular visual-inertial tracking system can obtain continuous six degree of freedom (6-DoF) pose estimation without scale ambiguity. Our method requires neither fixed multi-camera nor depth sensor settings for scale observability, instead, the IMU inside the monocular sensing suite provides scale information for both camera itself and the tracked object. We build the proposed system on top of our monocular visual-inertial system (VINS) to obtain accurate state estimation of the monocular camera in the world frame. The whole system consists of a 2D object tracker, an object region-based visual bundle adjustment (BA), VINS and a correlation analysis-based metric scale estimator. Experimental comparisons with ground truth demonstrate the tracking accuracy of our 3D tracking performance while a mobile augmented reality (AR) demo shows the feasibility of potential applications.
ER  - 

TY  - CONF
TI  - Geometric-based Line Segment Tracking for HDR Stereo Sequences
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 69
EP  - 74
AU  - R. Gomez-Ojeda
AU  - J. Gonzalez-Jimenez
PY  - 2018
KW  - compressed sensing
KW  - convex programming
KW  - image matching
KW  - image segmentation
KW  - image sequences
KW  - stereo image processing
KW  - video signal processing
KW  - robust tracking
KW  - art techniques
KW  - appearance-based methods
KW  - High Dynamic Range environments
KW  - HDR stereo sequences
KW  - geometric-based line segment tracking
KW  - stereo streams
KW  - appearance-based matching techniques
KW  - video sequences
KW  - Image segmentation
KW  - Lighting
KW  - Tracking
KW  - Feature extraction
KW  - Video sequences
KW  - Motion segmentation
KW  - Simultaneous localization and mapping
DO  - 10.1109/IROS.2018.8593646
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this work, we propose a purely geometrical approach for the robust matching of line segments for challenging stereo streams with severe illumination changes or High Dynamic Range (HDR) environments. To that purpose, we exploit the univocal nature of the matching problem, i.e. every observation must be corresponded with a single feature or not corresponded at all. We state the problem as a sparse, convex, ℓ1-minimization of the matching vector regularized by the geometric constraints. This formulation allows for the robust tracking of line segments along sequences where traditional appearance-based matching techniques tend to fail due to dynamic changes in illumination conditions. Moreover, the proposed matching algorithm also results in a considerable speed-up of previous state of the art techniques making it suitable for real-time applications such as Visual Odometry (VO). This, of course, comes at expense of a slightly lower number of matches in comparison with appearance-based methods, and also limits its application to continuous video sequences, as it is rather constrained to small pose increments between consecutive frames. We validate the claimed advantages by first evaluating the matching performance in challenging video sequences, and then testing the method in a benchmarked point and line based VO algorithm.
ER  - 

TY  - CONF
TI  - Adversarial Transfer Networks for Visual Tracking
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 75
EP  - 81
AU  - L. Liu
AU  - J. Lu
AU  - J. Zhou
PY  - 2018
KW  - learning (artificial intelligence)
KW  - object tracking
KW  - video signal processing
KW  - visual tracking
KW  - domain-specific information
KW  - target-domain samples
KW  - adversarial transfer networks
KW  - unmanned systems
KW  - offline video training data
KW  - ATNet
KW  - source-domain samples
KW  - adversarial transfer learning
KW  - Target tracking
KW  - Videos
KW  - Training
KW  - Visualization
KW  - Task analysis
KW  - Feature extraction
KW  - Learning systems
DO  - 10.1109/IROS.2018.8593585
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Visual tracking plays an important role in unmanned systems. In many cases, the system needs to keep track of targets it has never seen before, and the only training sample available is the specified object in the initial frame. In this paper, we propose a deep architecture called adversarial transfer networks (ATNet), which aims to make well use of offline video training data and solve the problem of lacking training samples in visual tracking. Different from most existing trackers which neglect significant differences between videos and gulp the training data all together, our method utilizes the special nature of tracking problem and concentrates on transferring domain-specific information across similar tracking tasks. We first propose an efficient way to select a training video that is most similar to online tracking task and regard it as source domain. With the labeled data in the selected source domain, we apply adversarial transfer learning to make the feature distribution of source-domain samples and target-domain samples as similar as possible. Therefore, the transferred source-domain samples can provide various possible appearance of tracked target for training and boost the tracking performance. Experimental results on three OTB tracking benchmarks show that our method outperforms the state-of-the-art trackers in both accuracy and robustness.
ER  - 

TY  - CONF
TI  - Predicting Out-of-View Feature Points for Model-Based Camera Pose Estimation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 82
EP  - 88
AU  - O. Moolan-Feroze
AU  - A. Calway
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - inspection
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object tracking
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - recurrent neural nets
KW  - rich feature information
KW  - recurrent neural network architecture
KW  - network training
KW  - autonomous inspection robots
KW  - model-based tracking
KW  - input image
KW  - object feature points
KW  - deep learning
KW  - model-based camera pose estimation
KW  - out-of-view feature points
KW  - optimisation based tracker
KW  - Cameras
KW  - Heating systems
KW  - Feature extraction
KW  - Pose estimation
KW  - Computational modeling
KW  - Two dimensional displays
KW  - Predictive models
DO  - 10.1109/IROS.2018.8594297
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this work we present a novel framework that uses deep learning to predict object feature points that are out-of-view in the input image. This system was developed with the application of model-based tracking in mind, particularly in the case of autonomous inspection robots, where only partial views of the object are available. Out-of-view prediction is enabled by applying scaling to the feature point labels during network training. This is combined with a recurrent neural network architecture designed to provide the final prediction layers with rich feature information from across the spatial extent of the input image. To show the versatility of these out-of-view predictions, we describe how to integrate them in both a particle filter tracker and an optimisation based tracker. To evaluate our work we compared our framework with one that predicts only points inside the image. We show that as the amount of the object in view decreases, being able to predict outside the image bounds adds robustness to the final pose estimation.
ER  - 

TY  - CONF
TI  - A modular framework for model-based visual tracking using edge, texture and depth features
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 89
EP  - 96
AU  - S. Trinh
AU  - F. Spindler
AU  - E. Marchand
AU  - F. Chaumette
PY  - 2018
KW  - feature extraction
KW  - image colour analysis
KW  - image sensors
KW  - object tracking
KW  - modular framework
KW  - confidence index
KW  - multiple vision sensors
KW  - depth map
KW  - textured points
KW  - edge points
KW  - real-time model-based visual tracker
KW  - depth features
KW  - model-based visual tracking using edge
KW  - Cameras
KW  - Visualization
KW  - Solid modeling
KW  - Image edge detection
KW  - Three-dimensional displays
KW  - Sensors
KW  - Robustness
DO  - 10.1109/IROS.2018.8594003
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present in this paper a modular real-time model-based visual tracker. It is able to fuse different types of measurement, that is, edge points, textured points, and depth map, provided by one or multiple vision sensors. A confidence index is also proposed for determining if the outputs of the tracker are reliable or not. As expected, experimental results show that the more various measurements are combined, the more accurate and robust is the tracker. The corresponding C++ source code is available for the community in the ViSP library.
ER  - 

TY  - CONF
TI  - FSG: A statistical approach to line detection via fast segments grouping
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 97
EP  - 102
AU  - I. Suárez
AU  - E. Muñoz
AU  - J. M. Buenaposada
AU  - L. Baumela
PY  - 2018
KW  - feature extraction
KW  - image segmentation
KW  - robot vision
KW  - fast segments grouping
KW  - line segment detection algorithms
KW  - segment grouping methods
KW  - vanishing points detection
KW  - statistical approach
KW  - high level robot localization task
KW  - plausible line candidates
KW  - robust line detection algorithm
KW  - FSG
KW  - low textured scenes
KW  - visual robotic tasks
KW  - line extraction
KW  - Image segmentation
KW  - Probabilistic logic
KW  - Estimation
KW  - Simultaneous localization and mapping
KW  - Task analysis
KW  - Detection algorithms
DO  - 10.1109/IROS.2018.8594434
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Line extraction is a preliminary step in various visual robotic tasks performed in low textured scenes such as city and indoor settings. Several efficient line segment detection algorithms such as LSD and EDLines have recently emerged. However, the state of the art segment grouping methods are not robust enough or not amenable for detecting lines in real-time. In this paper we present FSG, a fast and robust line detection algorithm. It is based on two independent components. A proposer that greedily cluster segments suggesting plausible line candidates and a probabilistic model that decides if a group of segments is an actual line. In the experiments we show that our procedure is more robust and faster than the best methods in the literature and achieves state-of-the art performance in a high level robot localization task such as vanishing points detection.
ER  - 

TY  - CONF
TI  - Optimized Contrast Enhancements to Improve Robustness of Visual Tracking in a SLAM Relocalisation Context
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 103
EP  - 108
AU  - X. Wang
AU  - M. Christie
AU  - E. Marchand
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - image colour analysis
KW  - image enhancement
KW  - image representation
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - video signal processing
KW  - optimized contrast enhancements
KW  - visual tracking
KW  - SLAM relocalisation context
KW  - indirect SLAM techniques
KW  - robotics community
KW  - feature points
KW  - multilayered image representation
KW  - contrast enhanced version
KW  - tracking process
KW  - detection
KW  - matching
KW  - dynamic contrast enhancements
KW  - dynamic light changing conditions
KW  - ORB-SLAM
KW  - light changed condition
KW  - reference video
KW  - Mutual information
KW  - Lighting
KW  - Robustness
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Entropy
KW  - Visualization
DO  - 10.1109/IROS.2018.8593366
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robustness of indirect SLAM techniques to light changing conditions remains a central issue in the robotics community. With the change in the illumination of a scene, feature points are either not extracted properly due to low contrasts, or not matched due to large differences in descriptors. In this paper, we propose a multi-layered image representation (MLI) in which each layer holds a contrast enhanced version of the current image in the tracking process in order to improve detection and matching. We show how Mutual Information can be used to compute dynamic contrast enhancements on each layer. We demonstrate how this approach dramatically improves the robustness in dynamic light changing conditions on both synthetic and real environments compared to default ORB-SLAM. This work focalises on the specific case of SLAM relocalisation in which a first pass on a reference video constructs a map, and a second pass with a light changed condition relocalizes the camera in the map.
ER  - 

TY  - CONF
TI  - Key-frame Selection for Multi-robot Simultaneous Localization and Tracking in Robot Soccer Field
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 109
EP  - 116
AU  - W. Fu
AU  - K. Lin
AU  - C. Shih
PY  - 2018
KW  - entropy
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - SLAM (robots)
KW  - traditional key-frame selection algorithms
KW  - temporal relationship
KW  - spatial relationship
KW  - pre-defined field
KW  - information entropy
KW  - selection ratio
KW  - key-frames
KW  - localization results
KW  - robot soccer field
KW  - optical images
KW  - extensive computation resources
KW  - key-frame selection algorithm
KW  - multiple robots simultaneous localization
KW  - multirobot soccer games
KW  - Entropy
KW  - Robot sensing systems
KW  - Object detection
KW  - Sports
KW  - Cameras
KW  - Legged locomotion
DO  - 10.1109/IROS.2018.8593785
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Optical images provide rich features but require extensive computation resources to process for SLAM. When there are limited computation resources on the robots, it becomes a heavy burden to process the images in real-time. This paper presents the design and implementation of key-frame selection algorithm for multiple robots simultaneous localization and tracking on the multi-robot soccer games which have pre-defined field and objects. Compared to traditional key-frame selection algorithms, this work makes use of the temporal and spatial relationship among objects on the pre-defined field to compute the information entropy. The selection ratio can be adjusted by two parameters: entropy threshold and the maximum moving distance. The experimental results show that the developed method can effectively detect the change of scene using selected key-frames. And comparing with the localization results using all the images, using less than 20% of all images after walking 11,203mm it only increase up to 0.87% trajectory errors.
ER  - 

TY  - CONF
TI  - Weighted Total Least Squares based Online Calibration Method for RSS based Localization
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 117
EP  - 122
AU  - J. Kim
AU  - D. Kim
PY  - 2018
KW  - calibration
KW  - distance measurement
KW  - error compensation
KW  - least squares approximations
KW  - regression analysis
KW  - linear regression model
KW  - partial input elements
KW  - weighted total least squares techniques
KW  - WTLS techniques
KW  - received signal strength based localization algorithm
KW  - RSS-to-distance based localization algorithm
KW  - improved online model-based calibration approach
KW  - distance estimation
KW  - error compensation
KW  - Calibration
KW  - Linear regression
KW  - Estimation
KW  - Shadow mapping
KW  - Measurement uncertainty
KW  - Taylor series
KW  - Convergence
DO  - 10.1109/IROS.2018.8594416
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In the received signal strength (RSS) based localization, a model-based calibration approach has been usually done by relating RSS-to-distance among anchor nodes. In this paper, an improved calibration method is proposed. For that purpose, RSS and estimated distance between any pairs of an-chor/unknown nodes is considered under the linear regression model. Unfortunately in this model, partial input elements are erroneous due to the inaccurate localization of unknown nodes. To obtain its solution under consideration of such an error, the weighted total least squares (WTLS) techniques are employed here. With the help of the WTLS techniques, several errors involved in the model can be effectively compensated. To show the efficiency of the proposed calibration, it is combined with several localization algorithms and its performance is verified by various simulations. The results show that the proposed calibration can give a very similar localization performance to that of each localization algorithm when true model parameters are known.
ER  - 

TY  - CONF
TI  - LIPS: LiDAR-Inertial 3D Plane SLAM
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 123
EP  - 130
AU  - P. Geneva
AU  - K. Eckenhoff
AU  - Y. Yang
AU  - G. Huang
PY  - 2018
KW  - graph theory
KW  - image representation
KW  - mobile robots
KW  - optical radar
KW  - optimisation
KW  - robot vision
KW  - SLAM (robots)
KW  - inertial preintegratation measurement
KW  - LiDAR-inertial 3D plane SLAM
KW  - simultaneous localization and mapping
KW  - singularity free plane factor
KW  - closest point plane representation
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Optimization
KW  - Lips
DO  - 10.1109/IROS.2018.8594463
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents the formalization of the closest point plane representation and an analysis of its incorporation in 3D indoor simultaneous localization and mapping (SLAM). We present a singularity free plane factor leveraging the closest point plane representation, and demonstrate its fusion with inertial preintegratation measurements in a graph-based optimization framework. The resulting LiDAR-inertial 3D plane SLAM (LIPS) system is validated both on a custom made LiDAR simulator and on a real-world experiment.
ER  - 

TY  - CONF
TI  - Scan Similarity-based Pose Graph Construction method for Graph SLAM
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 131
EP  - 136
AU  - W. Yoo
AU  - H. Kim
AU  - H. Hong
AU  - B. H. Lee
PY  - 2018
KW  - graph theory
KW  - mobile robots
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - scan similarity-based pose graph construction method
KW  - constructed graph
KW  - loop closure detection method
KW  - real world dataset
KW  - benchmark dataset
KW  - odometry estimation process
KW  - error accumulation phenomenon
KW  - pose graph SLAM
KW  - scan similarity computation method
KW  - graph accuracy
KW  - high quality graph
KW  - Simultaneous localization and mapping
KW  - Lasers
KW  - Estimation
KW  - Heuristic algorithms
KW  - Optimization
DO  - 10.1109/IROS.2018.8593605
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Scan similarity-based pose graph construction method for graph SLAM is proposed. To perform delicate pose graph SLAM, front-end that constructs a graph as well as back-end that optimizes the constructed graph is an important task. Generally, there is an error accumulation phenomenon during the odometry estimation process. This paper focuses on the method of creating a high quality graph by suggesting ways to improve the graph accuracy since the accumulated errors in the graph might degrade the performance of the entire graph SLAM. We deal with one of our previous works, dynamic keyframe selection technique, based on scan similarity computation method more precisely and suggest a loop closure detection method by exploiting previously proposed 2-D laser scan descriptor. To verify objective performance of the proposed method, the experimental results of the odometry estimation are shown by using the benchmark dataset and the real world dataset. Additionally, results of the pose graph SLAM are shown for the real world dataset which include the loop clorues.
ER  - 

TY  - CONF
TI  - Egocentric Spatial Memory
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 137
EP  - 144
AU  - M. Zhang
AU  - K. T. Ma
AU  - S. Yen
AU  - J. H. Lim
AU  - Q. Zhao
AU  - J. Feng
PY  - 2018
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neurophysiology
KW  - recurrent neural nets
KW  - robot vision
KW  - place recognition
KW  - robotic control
KW  - 3D virtual mazes
KW  - deep learning based mapping system
KW  - ESM network
KW  - external memory
KW  - recurrent neural network
KW  - spatially extended environment
KW  - 2D global maps
KW  - integrated deep neural network architecture
KW  - egocentric perspective
KW  - spatial information
KW  - memory system
KW  - egocentric spatial memory
KW  - Computer architecture
KW  - Cameras
KW  - Navigation
KW  - Microprocessors
KW  - Sensors
KW  - Task analysis
KW  - Motion measurement
DO  - 10.1109/IROS.2018.8593435
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Egocentric spatial memory (ESM) defines a memory system with encoding, storing, recognizing and recalling the spatial information about the environment from an egocentric perspective. We introduce an integrated deep neural network architecture for modeling ESM. It learns to estimate the occupancy state of the world and progressively construct top-down 2D global maps from egocentric views in a spatially extended environment. During the exploration, our proposed ESM model updates belief of the global map based on local observations using a recurrent neural network. It also augments the local mapping with a novel external memory to encode and store latent representations of the visited places over longterm exploration in large environments which enables agents to perform place recognition and hence, loop closure. Our proposed ESM network contributes in the following aspects: (1) without feature engineering, our model predicts free space based on egocentric views efficiently in an end-to-end manner; (2) different from other deep learning-based mapping system, ESMN deals with continuous actions and states which is vitally important for robotic control in real applications. In the experiments, we demonstrate its accurate and robust global mapping capacities in 3D virtual mazes and realistic indoor environments by comparing with several competitive baselines.
ER  - 

TY  - CONF
TI  - Predicting Objective Function Change in Pose-Graph Optimization
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 145
EP  - 152
AU  - F. Bai
AU  - T. Vidal-Calleja
AU  - S. Huang
AU  - R. Xiong
PY  - 2018
KW  - graph theory
KW  - optimisation
KW  - SLAM (robots)
KW  - outlier detection
KW  - robust online incremental SLAM applications
KW  - graph pruning
KW  - information-theoretic metrics
KW  - pose-graph optimization scheme
KW  - Linear programming
KW  - Optimization
KW  - Simultaneous localization and mapping
KW  - Measurement errors
KW  - Noise measurement
KW  - Reliability
DO  - 10.1109/IROS.2018.8594248
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robust online incremental SLAM applications require metrics to evaluate the impact of current measurements. Despite its prevalence in graph pruning, information-theoretic metrics solely are insufficient to detect outliers. The optimal value of the objective function is a better choice to detect outliers but cannot be computed unless the problem is solved. In this paper, we show how the objective function change can be predicted in an incremental pose-graph optimization scheme, without actually solving the problem. The predicted objective function change can be used to guide online decisions or detect outliers. Experiments validate the accuracy of the predicted objective function, and an application to outlier detection is also provided, showing its advantages over M-estimators.
ER  - 

TY  - CONF
TI  - Efficient Long-term Mapping in Dynamic Environments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 153
EP  - 160
AU  - M. T. Lázaro
AU  - R. Capobianco
AU  - G. Grisetti
PY  - 2018
KW  - graph theory
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - mapping problem
KW  - longterm SLAM datasets
KW  - graph coherency
KW  - intra-session loop closure detections
KW  - out-dated nodes
KW  - graph complexity
KW  - nonstatic entities
KW  - merging procedure
KW  - efficient ICP-based alignment
KW  - up-to-date state
KW  - 2D point cloud data
KW  - local maps
KW  - graph SLAM paradigm
KW  - multiple mapping sessions
KW  - single mapping sessions
KW  - SLAM system
KW  - autonomous robots
KW  - dynamic environments
KW  - long-term robot operation
KW  - Simultaneous localization and mapping
KW  - Cloud computing
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Merging
KW  - Optimization
DO  - 10.1109/IROS.2018.8594310
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.
ER  - 

TY  - CONF
TI  - Localization of Classified Objects in SLAM using Nonparametric Statistics and Clustering
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 161
EP  - 168
AU  - A. Iqbal
AU  - N. R. Gans
PY  - 2018
KW  - feature extraction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - nonparametric statistics
KW  - object detection
KW  - pattern clustering
KW  - robot vision
KW  - SLAM (robots)
KW  - statistical analysis
KW  - nonparametric statistical approach
KW  - data association
KW  - mapping process
KW  - object detection
KW  - machine learning
KW  - semantic information
KW  - nonparametric statistics
KW  - classified objects
KW  - locating objects
KW  - SLAM
KW  - unsupervised clustering method
KW  - detected objects
KW  - Simultaneous localization and mapping
KW  - Semantics
KW  - Object detection
KW  - Cameras
KW  - Three-dimensional displays
DO  - 10.1109/IROS.2018.8593541
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Traditional Simultaneous Localization and Mapping (SLAM) approaches build maps based on points, lines or planes. These maps visually resemble the environment but without any semantic or information about the objects in the environment. Recent advancements in machine learning have made object detection highly accurate and reliable with large set of objects. Object detection can effectively help SLAM to incorporate semantics in the mapping process. One of the main obstacles is data association between detected objects over time. We demonstrate a nonparametric statistical approach to solve the data association between detected objects over consecutive frames. Then we use an unsupervised clustering method to identify the existence of objects in the map. The complete process can be run in parallel with SLAM. The performance of our algorithm is demonstrated on several public datasets, which shows promising results in locating objects in SLAM.
ER  - 

TY  - CONF
TI  - A distributed vision-based consensus model for aerial-robotic teams
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 169
EP  - 176
AU  - F. Poiesi
AU  - A. Cavallaro
PY  - 2018
KW  - autonomous aerial vehicles
KW  - geometry
KW  - mobile robots
KW  - object detection
KW  - position control
KW  - robot vision
KW  - target tracking
KW  - target position
KW  - PID-controlled steering responses
KW  - autonomous aerial robots
KW  - aerial-robotic teams
KW  - distributed vision-based consensus model
KW  - noisy detections
KW  - steering commands
KW  - ego-centric view
KW  - geometric constraints
KW  - Robot kinematics
KW  - Robot sensing systems
KW  - Cameras
KW  - Task analysis
KW  - Noise measurement
DO  - 10.1109/IROS.2018.8593388
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present a distributed model for a team of autonomous aerial robots to collaboratively track a target without external control. The model uses distributed consensus to coordinate actions and to maintain formation via geometric constraints. Each robot uses its ego-centric view of a target and the relative distance from its two closest neighbors to infer its steering commands. To account for noisy and missing target detections, the robots exchange their estimated target position and formation configuration through shared PID-controlled steering responses. We show that the proposed model enables the team to maintain the view of a maneuvering target with varying acceleration under noisy detections and failures up to situations when all robots but one lose the target from their field of view.
ER  - 

TY  - CONF
TI  - Fast Kinodynamic Bipedal Locomotion Planning with Moving Obstacles
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 177
EP  - 184
AU  - J. Ahn
AU  - O. Campbell
AU  - D. Kim
AU  - L. Sentis
PY  - 2018
KW  - collision avoidance
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - pendulums
KW  - robot dynamics
KW  - robot kinematics
KW  - wheels
KW  - moving obstacles
KW  - bipedal robot
KW  - complex environments
KW  - footstep planning algorithms
KW  - footstep locations
KW  - biped dynamics
KW  - temporal duration
KW  - dynamically consistent description
KW  - PSP
KW  - collision-free route
KW  - nonholonomic wheeled robots
KW  - kinematic constraints
KW  - bipedal motion
KW  - body dynamic walking behavior
KW  - 3D physics-based simulation
KW  - linear inverted pendulum model dynamics
KW  - dynamic constraints
KW  - kinodynamic bipedal locomotion planning
KW  - sampling-based kino-dynamic planning
KW  - LIPM
KW  - phase space planner
KW  - steering method
KW  - Planning
KW  - Heuristic algorithms
KW  - Robot kinematics
KW  - Legged locomotion
KW  - Collision avoidance
DO  - 10.1109/IROS.2018.8594156
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we present a sampling-based kino-dynamic planning framework for a bipedal robot in complex environments. Unlike other footstep planning algorithms which typically plan footstep locations and the biped dynamics in separate steps, we handle both simultaneously. Three primary advantages of this approach are (1) the ability to differentiate alternate routes while selecting footstep locations based on the temporal duration of the route as determined by the Linear Inverted Pendulum Model (LIPM) dynamics, (2) the ability to perform collision checking through time so that collisions with moving obstacles are prevented without avoiding their entire trajectory, and (3) the ability to specify a minimum forward velocity for the biped. To generate a dynamically consistent description of the walking behavior, we exploit the Phase Space Planner (PSP) [1] [2]. To plan a collision-free route toward the goal, we adapt planning strategies from non-holonomic wheeled robots to gather a sequence of inputs for the PSP. This allows us to efficiently approximate dynamic and kinematic constraints on bipedal motion, to apply a sampling-based planning algorithm such as RRT or RRT*, and to use the Dubin's path [3] as the steering method to connect two points in the configuration space. The results of the algorithm are sent to a Whole Body Controller [1] to generate full body dynamic walking behavior. Our planning algorithm is tested in a 3D physics-based simulation of the humanoid robot Valkyrie.
ER  - 

TY  - CONF
TI  - Artificial Invariant Subspace for Humanoid Robot Balancing in Locomotion
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 185
EP  - 192
AU  - X. Deng
AU  - D. D. Lee
PY  - 2018
KW  - damping
KW  - feedback
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - nonlinear control systems
KW  - robot dynamics
KW  - stability
KW  - humanoid robots
KW  - biped robots
KW  - swing foot
KW  - damped harmonic oscillators
KW  - continuous feedback control
KW  - nominal walking cycle
KW  - rigid body dynamics
KW  - NAO robot
KW  - artificial invariant subspace
KW  - locomotion
KW  - compliant actuators
KW  - damping
KW  - nonlinear controller
KW  - robustness
KW  - bio-inspired legged robots
KW  - predictive foot stepping
KW  - asymptotic convergence
KW  - flat terrains
KW  - Legged locomotion
KW  - Foot
KW  - Humanoid robots
KW  - Orbits
KW  - Perturbation methods
KW  - Robustness
DO  - 10.1109/IROS.2018.8594423
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Legged robots that make use of compliant actuators have demonstrated greater robustness of locomotion than their rigid counterparts. Stiffness and damping are key parameters that characterize the adaptation to perturbations. In this work, by drawing inspirations from controllable compliance and damping in existing soft and bio-inspired legged robots, we propose an approach to design a nonlinear controller for the balancing of humanoid robots with rigid bodies. Existing literature has proposed simplified dynamical models of biped robots in order to predict the timing and placement of swing foot for walking without falling. We further employ the properties of invariance to perturbations in damped harmonic oscillators and formulate continuous feedback control in combination with predictive foot stepping in order to achieve continuous adaptive recoveries of the nominal walking cycle from unexpected physical disturbances. Our method allows asymptotic convergence of the rigid body dynamics to a subspace with the desired energy level. We demonstrate the robustness of the proposed algorithm base on extensive push recovery experiments on a NAO robot on flat terrains.
ER  - 

TY  - CONF
TI  - Classification of EEG signals for a hypnotrack BCI system
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 240
EP  - 245
AU  - M. Alimardani
AU  - S. Keshmiri
AU  - H. Sumioka
AU  - K. Hiraki
PY  - 2018
KW  - electroencephalography
KW  - feature extraction
KW  - medical signal processing
KW  - neurophysiology
KW  - patient treatment
KW  - signal classification
KW  - support vector machines
KW  - hypnotrack BCI system
KW  - EEG signals
KW  - clustering-based feature refinement strategy
KW  - support vector machine
KW  - clinical hypnotherapy sessions
KW  - electroencephalography signals
KW  - hypnosis intervention
KW  - Electroencephalography
KW  - Entropy
KW  - Electrodes
KW  - Brain
KW  - Feature extraction
KW  - Support vector machines
KW  - Medical treatment
DO  - 10.1109/IROS.2018.8594136
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - People's responses to a hypnosis intervention is diverse and unpredictable. A system that predicts user's level of susceptibility from their electroencephalography (EEG) signals can be helpful in clinical hypnotherapy sessions. In this paper, we extracted differential entropy (DE) of the recorded EEGs from two groups of subjects with high and low hypnotic susceptibility and built a support vector machine on these DE features for the classification of susceptibility trait. Moreover, we proposed a clustering-based feature refinement strategy to improve the estimation of such trait. Results showed a high classification performance in detection of subjects' level of susceptibility before and during hypnosis. Our results suggest the usefulness of this classifier in development of future Bel systems applied in the domain of therapy and healthcare.
ER  - 

TY  - CONF
TI  - Real-time Control of Whole-body Robot Motion and Trajectory Generation for Physiotherapeutic Juggling in VR
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 270
EP  - 277
AU  - P. Mohammadi
AU  - M. Malekzadeh
AU  - J. Kodl
AU  - A. Mukovskiy
AU  - D. L. Wigand
AU  - M. Giese
AU  - J. J. Steil
PY  - 2018
KW  - brain
KW  - control engineering computing
KW  - diseases
KW  - humanoid robots
KW  - medical computing
KW  - medical robotics
KW  - motion control
KW  - neurophysiology
KW  - patient rehabilitation
KW  - patient treatment
KW  - quadratic programming
KW  - trajectory control
KW  - virtual reality
KW  - whole-body robot motion
KW  - trajectory generation
KW  - physiotherapeutic juggling
KW  - motor rehabilitation
KW  - functional motor impairments
KW  - cerebellar ataxia
KW  - Parkinson's disease
KW  - juggling physiotherapy
KW  - brain plasticity
KW  - physical strain
KW  - juggling games
KW  - throwing motions
KW  - whole-body motion
KW  - real-time architecture
KW  - controller device
KW  - VR setting
KW  - physiotherapeutic robotic juggling
KW  - real-time operation
KW  - physical robot
KW  - virtual reality
KW  - humanoid robot COMAN wrist
KW  - quadratic program
KW  - real-time control
KW  - Trajectory
KW  - Real-time systems
KW  - Robot kinematics
KW  - Task analysis
KW  - Medical treatment
KW  - Switches
DO  - 10.1109/IROS.2018.8593632
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Motor rehabilitation is in increasingly high demand to deal with minor functional motor impairments resulting from stroke, cerebellar ataxia, or Parkinson's disease. Juggling physiotherapy has shown to induce brain plasticity and to improve coordination and balance in this context. The physiotherapy, however, relies on large number of repetitions to be effective which prompts to deploy robots to release the burden on therapists both in terms of time as well as physical strain. This paper provides a framework to enable juggling games for patients in interacting with robots through Virtual Reality (VR). A set of throwing motions is recorded from the therapist and is retargeted to the humanoid robot COMAN's wrist. The respective whole-body motion is then solved in a stack of Quadratic Programs (QP) in a real-time architecture that integrates OROCOS and Gazebo. The resulting motion is finally streamed to VR for animation of the robot and the thrown ball, which the user can catch in VR using a controller device. We regard the VR setting as an essential step towards physiotherapeutic robotic juggling, because it ensures safety of the patients and effective testing of the methods and already has potential for actual therapeutic intervention. The control framework, however, is already validated in this paper for switching to full real-time operation on the physical robot.
ER  - 

TY  - CONF
TI  - A Novel Fabrication of PDMS Chip using Atmospheric Pressure Plasma Jet: Hydrophobicity Modification and Feasibility Test
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 278
EP  - 283
AU  - Y. Yu
AU  - L. Kuo
AU  - M. Wu
AU  - J. Wu
AU  - C. D. Tsai
PY  - 2018
KW  - bioMEMS
KW  - cellular biophysics
KW  - hydrophobicity
KW  - lab-on-a-chip
KW  - microchannel flow
KW  - microfabrication
KW  - optimisation
KW  - plasma jets
KW  - plasma materials processing
KW  - polymers
KW  - Taguchi methods
KW  - plasma parameters
KW  - microfluidic channels
KW  - PDMS chip
KW  - atmospheric pressure plasma jet
KW  - hydrophobicity modification
KW  - feasibility test
KW  - polydimethylsiloxane surface
KW  - microfluidic chips
KW  - simple cost method
KW  - low-cost method
KW  - fluidic system fabrication
KW  - optimization
KW  - regenerative medicine
KW  - cultured cells
KW  - Taguchi method
KW  - Plasmas
KW  - Surface treatment
KW  - Optimization
KW  - Argon
KW  - Power supplies
KW  - Plasma measurements
KW  - Fabrication
DO  - 10.1109/IROS.2018.8594446
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a new application of atmospheric pressure plasma jet (APPJ) aiming for fabricating a microfluidic system on a polydimethylsiloxane (PDMS) surface. While PDMS is widely used for microfluidic chips, the fabrication of a chip requires different instruments which are not easily accessible for small-scale companies or laboratories. Therefore, we are motivated to develop a simple and low-cost method for such a fluidic system fabrication. The idea of this work is to directly pattern a fluidic channel on a PDMS surface with a plasma jet, which is known for its capability of modifying the hydrophobicity on a surface. The feasibility test first showed that fluid only flows in plasma-treated regions as having physical walls. The plasma parameters were then optimized using Taguchi method based on experiments. The optimization significantly reduce the required plasma treating time from more than 30 treating rounds to only 3 treating rounds, over ten times improved. Methods for further improving the resolution to micrometer-scale have been discussed. In addition to the advantages of fast and low-cost of the proposed method, making microfluidic channels on the surfaces of PDMS chip is also convenient for recollecting cultured cells on a chip in the field of regenerative medicine.
ER  - 

TY  - CONF
TI  - Deep Neural Object Analysis by Interactive Auditory Exploration with a Humanoid Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 284
EP  - 289
AU  - M. Eppe
AU  - M. Kerzel
AU  - E. Strahl
AU  - S. Wermter
PY  - 2018
KW  - audio signal processing
KW  - humanoid robots
KW  - neural net architecture
KW  - signal classification
KW  - signal denoising
KW  - deep neural object analysis
KW  - interactive auditory exploration
KW  - humanoid robot
KW  - interactive auditory object analysis
KW  - robot elicits sensory information
KW  - robotic ears
KW  - neural network architecture
KW  - audio signals
KW  - microphone
KW  - material classification
KW  - weight prediction
KW  - Robot sensing systems
KW  - Humanoid robots
KW  - Robot kinematics
KW  - Mel frequency cepstral coefficient
KW  - Microsoft Windows
KW  - Plastics
DO  - 10.1109/IROS.2018.8593838
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present a novel approach for interactive auditory object analysis with a humanoid robot. The robot elicits sensory information by physically shaking visually indistinguishable plastic capsules. It gathers the resulting audio signals from microphones that are embedded into the robotic ears. A neural network architecture learns from these signals to analyze properties of the contents of the containers. Specifically, we evaluate the material classification and weight prediction accuracy and demonstrate that the framework is fairly robust to acoustic real-world noise.
ER  - 

TY  - CONF
TI  - Cloud services for robotic nurses? Assessing legal and ethical issues in the use of cloud services for healthcare robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 290
EP  - 296
AU  - E. Fosch-Villaronga
AU  - H. Felzmann
AU  - M. Ramos-Montero
AU  - T. Mahler
PY  - 2018
KW  - cloud computing
KW  - ethical aspects
KW  - health care
KW  - legislation
KW  - medical robotics
KW  - mobile robots
KW  - security of data
KW  - cyber- aspects
KW  - data protection requirements
KW  - data security
KW  - healthcare cloud robotics
KW  - ethical issues
KW  - legal issues
KW  - robotic nurses
KW  - Cloud computing
KW  - Medical services
KW  - Robot kinematics
KW  - Robot sensing systems
KW  - Law
DO  - 10.1109/IROS.2018.8593591
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper explores ethical and legal implications arising from the intertwinement of cloud services, healthcare and robotics. It closes an existing gap in the literature by highlighting the distinctive ethical and legal concerns associated with the inter-dependence of the cyber- and the physical aspects of healthcare cloud robotics. The identified core concerns include uncertainties with regard to data protection requirements; distributed responsibilities for unintended harm; achievement of transparency and consent for cloud robot services especially for vulnerable robot users; secondary uses of cloud data derived from robot activities; data security; and wider social issues. The paper aims to raise awareness and stimulate reflection of the legal and ethical impacts on different stakeholders arising from the use of cloud services in healthcare robotics. We show that due to the complexity of these concerns the design and implementation of such robots in healthcare requires an interdisciplinary development and impact assessment process. In light of legal requirements and ethical responsibilities towards end-users and other stakeholders, we draw practical considerations for engineers developing cloud services for robots in healthcare.
ER  - 

TY  - CONF
TI  - Towards Norm Realization in Institutions Mediating Human-Robot Societies
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 297
EP  - 304
AU  - A. Wasik
AU  - S. Tomic
AU  - A. Saffiotti
AU  - F. Pecora
AU  - A. Martinoli
AU  - P. U. Lima
PY  - 2018
KW  - human-robot interaction
KW  - social sciences computing
KW  - human-robot societies
KW  - norm realization
KW  - social interactions
KW  - robotic systems
KW  - robotic language
KW  - human language
KW  - human society
KW  - social norms
KW  - Robot kinematics
KW  - Grounding
KW  - Art
KW  - Cognition
KW  - Decision making
KW  - Semantics
DO  - 10.1109/IROS.2018.8594079
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Social norms are the understandings that govern the behavior of members of a society. As such, they regulate communication, cooperation and other social interactions. Robots capable of reasoning about social norms are more likely to be recognized as an extension of our human society. However, norms stated in a form of the human language are inherently vague and abstract. This allows for applying norms in a variety of situations, but if the robots are to adhere to social norms, they must be capable of translating abstract norms to the robotic language. In this paper we use a notion of institution to realize social norms in real robotic systems. We illustrate our approach in a case study, where we translate abstract norms into concrete constraints on cooperative behaviors of humans and robots. We investigate the feasibility of our approach and quantitatively evaluate the performance of our framework in 30 real experiments with user-based evaluation with 40 participants.
ER  - 

TY  - CONF
TI  - “Oh! I am so sorry!”: Understanding User Physiological Variation while Spoiling a Game Task
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 313
EP  - 319
AU  - R. Agrigoroaie
AU  - A. Cruz-Maya
AU  - A. Tapus
PY  - 2018
KW  - computer games
KW  - human-robot interaction
KW  - psychology
KW  - Jenga game
KW  - galvanic skin response
KW  - psychological questionnaires
KW  - multiple GSR parameters
KW  - user physiological variation
KW  - game task
KW  - tower fall down
KW  - Poles and towers
KW  - Games
KW  - Collision avoidance
KW  - Physiology
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8593395
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper investigates how individuals react in a situation when an experimenter (human or robot) either tells them to stop in the middle of playing the Jenga game, or accidentally bumps into a table and makes the tower fall down. The mood of the participants and different physiological parameters (i.e., galvanic skin response (GSR) and facial temperature variation) are extracted and analysed based on the condition, experimenter, and psychological questionnaires (i.e., TEQ, TEIQ, RST-PQ). This study was a between participants study with 23 participants. Our results show that multiple GSR parameters (e.g., latency, amplitude, number of peaks) differ significantly based on the condition and the experimenter the participants interacted with. The temperature variation in three regions of interest (i.e., forehead, left, and right periorbital regions) are good indicators of how ready an individual is to react in an unforeseen situation.
ER  - 

TY  - CONF
TI  - An Extended Bayesian User Model (BUM) for Capturing Cultural Attributes with a Social Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 320
EP  - 325
AU  - L. Santos
AU  - G. S. Martins
AU  - J. Dias
PY  - 2018
KW  - Bayes methods
KW  - belief networks
KW  - decision making
KW  - human-robot interaction
KW  - pattern classification
KW  - pattern clustering
KW  - service robots
KW  - user modelling
KW  - human-robot interaction
KW  - extended Bayesian User Model
KW  - social robotics
KW  - culture-awareness
KW  - specific subtleties
KW  - highly accurate classification framework
KW  - share similar attributes
KW  - n-dimensional semantic attribute space
KW  - capture unitary attributes
KW  - Bayesian classifiers
KW  - robotic technologies
KW  - latest advances
KW  - heterogeneous information
KW  - unified representation
KW  - cultural attributes
KW  - Robot sensing systems
KW  - Cultural differences
KW  - Bayes methods
KW  - Statistics
KW  - Indexes
KW  - Computational modeling
KW  - Culture Aware Social Robots
KW  - Robot Perception
KW  - Multimodal Human-Robot Interaction
KW  - User Models
DO  - 10.1109/IROS.2018.8593970
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this work we propose a Bayesian User Model which is able capture a unified representation of cultural attributes from heterogeneous information in the context of Human-Robot Interaction. Despite the latest advances in robotic technologies, virtually no robots are able to cope with the specificities of the “modus vivendi” of different cultures. We start by proposing Bayesian classifiers to capture unitary attributes of different users, clustering them in a n-dimensional semantic attribute space, aggregating groups of persons that share similar attributes. Results show a highly accurate classification framework, both capable of detecting specific subtleties in user's properties, and generalizing them into representative profiles. We then discuss its application towards adapting the actions of a robot and its potential impact on culture-awareness, demonstrating how the proposed framework can enable culture-awareness, exploring this new frontier in social robotics.
ER  - 

TY  - CONF
TI  - Culturally aware Planning and Execution of Robot Actions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 326
EP  - 332
AU  - A. A. Khaliq
AU  - U. Köckemann
AU  - F. Pecora
AU  - A. Saffiotti
AU  - B. Bruno
AU  - C. T. Recchiuto
AU  - A. Sgorbissa
AU  - H. Bui
AU  - N. Y. Chong
PY  - 2018
KW  - cultural aspects
KW  - mobile robots
KW  - path planning
KW  - robot actions
KW  - cultural group
KW  - cultural adaptation
KW  - interpersonal distance
KW  - robot plans generation
KW  - cultural preferences
KW  - CARESSES project
KW  - assistive robots
KW  - culturally aware planning
KW  - Cultural differences
KW  - Robot sensing systems
KW  - Planning
KW  - Knowledge based systems
KW  - Computer architecture
KW  - Cognition
DO  - 10.1109/IROS.2018.8593570
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The way in which humans behave, speak and interact is deeply influenced by their culture. For example, greeting is done differently in France, in Sweden or in Japan; and the average interpersonal distance changes from one cultural group to the other. In order to successfully coexist with humans, robots should also adapt their behavior to the culture, customs and manners of the persons they interact with. In this paper, we deal with an important ingredient of cultural adaptation: how to generate robot plans that respect given cultural preferences, and how to execute them in a way that is sensitive to those preferences. We present initial results in this direction in the context of the CARESSES project, a joint EU-Japan effort to build culturally competent assistive robots.
ER  - 

TY  - CONF
TI  - Trait-based Culture and its Organization: Developing a Culture Enabler for Artificial Agents
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 333
EP  - 338
AU  - S. Borgo
AU  - E. Blanzieri
PY  - 2018
KW  - artificial intelligence
KW  - multi-agent systems
KW  - social sciences computing
KW  - software agents
KW  - trait-based culture
KW  - culture enabler
KW  - artificial agent
KW  - human interests
KW  - human culture
KW  - trait types
KW  - trait module
KW  - Cultural differences
KW  - Global communication
KW  - Organizations
KW  - Knowledge based systems
KW  - Standards organizations
KW  - Intelligent robots
KW  - Buildings
DO  - 10.1109/IROS.2018.8593369
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Artificial agents might not understand human interests and actions if these agents cannot anticipate how a person understands a situation and, based on this, what could be his/her expectations. In many cases, understanding, expectations and behaviors are constrained, if not driven, by culture. Can we provide human culture to an artificial agent? Can we provide formal representations of different cultures? In this paper we discuss the (elusive) notion of culture and propose an approach based on the notion of trait which, we argue, allows building formal modules suitable to represent culture (broadly understood). We distinguish the trait types (knowledge, rule, behavior, interpretation) that such modules should contain and briefly discuss how they could be organized. Finally, we exemplify the role of a trait module in the flow of information internal to an agent highlighting surprising potentialities.
ER  - 

TY  - CONF
TI  - CultureNet: A Deep Learning Approach for Engagement Intensity Estimation from Face Images of Children with Autism
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 339
EP  - 346
AU  - O. Rudovic
AU  - Y. Utsumi
AU  - J. Lee
AU  - J. Hernandez
AU  - E. C. Ferrer
AU  - B. Schuller
AU  - R. W. Picard
PY  - 2018
KW  - cultural aspects
KW  - face recognition
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - medical disorders
KW  - medical robotics
KW  - paediatrics
KW  - patient treatment
KW  - deep learning model
KW  - cultural backgrounds
KW  - image data
KW  - target culture
KW  - multicultural data
KW  - child-dependent settings
KW  - across-culture evaluations
KW  - target task
KW  - deep architecture
KW  - robot-assisted autism therapy
KW  - video data
KW  - automated engagement estimation
KW  - deep learning models
KW  - neu-rotypical peers
KW  - autism spectrum
KW  - engagement intensity estimation
KW  - face images
KW  - cultural differences
KW  - individual differences
KW  - estimation performance
KW  - model learning
KW  - target children
KW  - target engagement levels
KW  - poor estimation
KW  - child-independent models
KW  - Face
KW  - Autism
KW  - Deep learning
KW  - Estimation
KW  - Cultural differences
KW  - Task analysis
KW  - Robots
DO  - 10.1109/IROS.2018.8594177
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Many children on autism spectrum have atypical behavioral expressions of engagement compared to their neu-rotypical peers. In this paper, we investigate the performance of deep learning models in the task of automated engagement estimation from face images of children with autism. Specifically, we use the video data of 30 children with different cultural backgrounds (Asia vs. Europe) recorded during a single session of a robot-assisted autism therapy. We perform a thorough evaluation of the proposed deep architectures for the target task, including within- and across-culture evaluations, as well as when using the child-independent and child-dependent settings. We also introduce a novel deep learning model, named CultureNet, which efficiently leverages the multi-cultural data when performing the adaptation of the proposed deep architecture to the target culture and child. We show that due to the highly heterogeneous nature of the image data of children with autism, the child-independent models lead to overall poor estimation of target engagement levels. On the other hand, when a small amount of data of target children is used to enhance the model learning, the estimation performance on the held-out data from those children increases significantly. This is the first time that the effects of individual and cultural differences in children with autism have empirically been studied in the context of deep learning performed directly from face images.
ER  - 

TY  - CONF
TI  - Object Assembly Guidance in Child-Robot Interaction using RGB-D based 3D Tracking
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 347
EP  - 354
AU  - J. Hadfield
AU  - P. Koutras
AU  - N. Efthymiou
AU  - G. Potamianos
AU  - C. S. Tzafestas
AU  - P. Maragos
PY  - 2018
KW  - gesture recognition
KW  - human computer interaction
KW  - humanoid robots
KW  - human-robot interaction
KW  - image colour analysis
KW  - mobile robots
KW  - object detection
KW  - object recognition
KW  - object tracking
KW  - particle filtering (numerical methods)
KW  - verbal response
KW  - 3D object tracking algorithm
KW  - RGB-D data
KW  - object assembly task
KW  - autonomous humanoid robot
KW  - RGB-D based 3D
KW  - object assembly guidance
KW  - resulting Child-Robot Interaction scenario
KW  - assembly state estimation
KW  - gestural response
KW  - assembly part
KW  - degrees-of-freedom
KW  - depth data stream
KW  - particle filter
KW  - image plane
KW  - color stream
KW  - tracking-by-detection scheme
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Robotic assembly
KW  - Robot kinematics
KW  - Tracking
KW  - Streaming media
DO  - 10.1109/IROS.2018.8594187
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This work examines how and to what benefit an autonomous humanoid robot can supervise a child in an object assembly task. In order to understand the child's actions, a novel 3D object tracking algorithm for RGB-D data is employed. The tracker consists of two stages: the first performs a tracking-by-detection scheme on the color stream, to locate the objects on the image plane, while the second uses a particle filter that operates on the depth data stream to refine the first stage output and infer the objects' rotations. Given the six degrees-of-freedom of the assembly part poses, the system is able to recognize which connections have been completed at any given time. This information is then used to select an appropriate verbal or gestural response for the robot. Experimental results show that (a) the tracking algorithm is accurate, fast and robust to severe occlusions and fast movements, (b) the proposed method of assembly state estimation is indeed effective, and (c) the resulting Child-Robot Interaction scenario is educational and enjoyable for the children involved.
ER  - 

TY  - CONF
TI  - In pixels we trust: From Pixel Labeling to Object Localization and Scene Categorization
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 355
EP  - 361
AU  - C. Herranz-Perdiguero
AU  - C. Redondo-Cabrera
AU  - R. J. López-Sastre
PY  - 2018
KW  - image classification
KW  - image segmentation
KW  - object detection
KW  - pixels
KW  - image pixel labeling
KW  - object detection
KW  - pixel labeling
KW  - scene understanding
KW  - semantic segmentation mask
KW  - Semantics
KW  - Labeling
KW  - Image segmentation
KW  - Task analysis
KW  - Object detection
KW  - Histograms
KW  - Kernel
DO  - 10.1109/IROS.2018.8593736
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - While there has been significant progress in solving the problems of image pixel labeling, object detection and scene classification, existing approaches normally address them separately. In this paper, we propose to tackle these problems from a bottom-up perspective, where we simply need a semantic segmentation of the scene as input. We employ the DeepLab architecture, based on the ResNet deep network, which leverages multi-scale inputs to later fuse their responses to perform a precise pixel labeling of the scene. This semantic segmentation mask is used to localize the objects and to recognize the scene, following two simple yet effective strategies. We evaluate the benefits of our solutions, performing a thorough experimental evaluation on the NYU Depth V2 dataset. Our approach achieves a performance that beats the leading results by a significant margin, defining the new state of the art in this benchmark for the three tasks comprising the scene understanding: semantic segmentation, object detection and scene categorization.
ER  - 

TY  - CONF
TI  - Self-Supervised Learning of the Drivable Area for Autonomous Vehicles
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 362
EP  - 369
AU  - J. Mayr
AU  - C. Unger
AU  - F. Tombari
PY  - 2018
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - stereo image processing
KW  - road segmentation
KW  - automatic labeling pipeline
KW  - deterministic stereo-based approach
KW  - ground plane detection
KW  - KITTI dataset
KW  - semantic segmentation
KW  - good segmentation results
KW  - self-supervised learning
KW  - autonomous vehicles
KW  - training data
KW  - drivable area segmentation
KW  - deep neural networks
KW  - impressive progress
KW  - deep learning
KW  - traditional machine learning
KW  - deterministic algorithms
KW  - large-scale datasets
KW  - associated ground truth labels
KW  - expensive labor-intensive problem
KW  - off-the-shelf DNN
KW  - Image segmentation
KW  - Training data
KW  - Labeling
KW  - Generators
KW  - Histograms
KW  - Training
KW  - Cameras
DO  - 10.1109/IROS.2018.8594480
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose a new approach for generating training data for the task of drivable area segmentation with deep neural networks (DNN). The impressive progress of deep learning in recent years demonstrated a superior performance of DNNs over traditional machine learning and deterministic algorithms for various tasks. Nevertheless, the acquisition of large-scale datasets with associated ground truth labels still poses an expensive and labor-intensive problem. We contribute to the solution of this problem for the task of road segmentation by proposing an automatic labeling pipeline which leverages a deterministic stereo-based approach for ground plane detection to create large datasets suitable for training neural networks. Based on the popular Cityscapes [1] and KITTI dataset [2] and two off-the-shelf DNNs for semantic segmentation, we show that we can achieve good segmentation results on monocular images, which substantially exceed the performance of the algorithm employed for automatic labeling without the need of any manual annotation.
ER  - 

TY  - CONF
TI  - Reachset Conformance of Forward Dynamic Models for the Formal Analysis of Robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 370
EP  - 376
AU  - S. B. Liu
AU  - M. Althoff
PY  - 2018
KW  - control engineering computing
KW  - human-robot interaction
KW  - program testing
KW  - reachability analysis
KW  - safety-critical software
KW  - human-robot co-existence scenario
KW  - robots
KW  - formal analysis
KW  - forward dynamic models
KW  - reachset conformance
KW  - reachability analysis
KW  - robotic models
KW  - model-based testing
KW  - safety-critical applications
KW  - classical robotic applications
KW  - design flaws
KW  - robotic systems
KW  - model-based design
KW  - Friction
KW  - Uncertainty
KW  - Manipulators
KW  - Mathematical model
KW  - Computational modeling
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8593975
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Model-based design of robotic systems has many advantages, among them faster development cycles and reduced costs due to early detections of design flaws. Approximate models are sufficient for many classical robotic applications; however, they no longer suffice for safety-critical applications. For instance, a dangerous situation which has not been detected by model-based testing might occur in a human-robot co-existence scenario since models do not exactly replicate behaviors of real systems-this problem arises no matter how accurate a model is, since even disturbances and sensor noise can cause a mismatch. We address this issue by adding non-determinism to robotic models and by computing the whole set of possible behaviors using reachability analysis. By using reachset conformance, we automatically adjust the required non-determinism so that all recorded behaviors are captured. For the first time this approach is demonstrated for a real robot.
ER  - 

TY  - CONF
TI  - Timestamp Offset Calibration for an IMU-Camera System Under Interval Uncertainty
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 377
EP  - 384
AU  - R. Voges
AU  - B. Wagner
PY  - 2018
KW  - calibration
KW  - cameras
KW  - data acquisition
KW  - inertial systems
KW  - measurement uncertainty
KW  - time measurement
KW  - orientation estimation determination
KW  - data acqusition
KW  - sensors
KW  - robotics applications
KW  - IMU-camera system
KW  - timestamp offset calibration
KW  - calibration data
KW  - bounded-error approach
KW  - interval uncertainty
KW  - time 20.0 ms
KW  - Cameras
KW  - Calibration
KW  - Uncertainty
KW  - Sensor fusion
KW  - Electron tubes
KW  - Sensor systems
DO  - 10.1109/IROS.2018.8594237
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - To properly fuse IMU and camera information for robotics applications, the relative timestamp offset between both sensors' data streams has to be considered. However, finding the exact timestamp offset is often impossible. Thus, it is necessary to additionally consider the offset's uncertainty if we want to produce reliable results. In order to find the offset and its uncertainty, we determine orientation estimates from IMU and camera under interval uncertainty. Subsequently, these intervals are used as a common representation for our bounded-error approach that finds an interval enclosing the true offset while also modeling the uncertainty. Calibration data can be acquired in a few seconds using a simple setup of IMU, camera and camera target. Results using both simulated and real data demonstrate that we are able to determine the offset to an accuracy of 20 ms with a computation time that is suitable for future online applications. Here, our approach could be used to monitor the timestamp offset in a guaranteed way. Additionally, our method can be adapted to determine an interval for the rotation between both sensors. While this increases the computation time drastically, it also enhances the accuracy of the timestamp offset to less than 10 ms.
ER  - 

TY  - CONF
TI  - Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 385
EP  - 392
AU  - Y. Nakajima
AU  - K. Tateno
AU  - F. Tombari
AU  - H. Saito
PY  - 2018
KW  - image segmentation
KW  - probability
KW  - SLAM (robots)
KW  - stereo image processing
KW  - SLAM framework
KW  - NYUv2 dataset
KW  - computational efficiency
KW  - frame-wise segmentation result
KW  - computationally intensive stages
KW  - segmentation label
KW  - updating class probabilities
KW  - processing components
KW  - geometric-based segmentation method
KW  - geometric-based incremental segmentation
KW  - Semantics
KW  - Three-dimensional displays
KW  - Image segmentation
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Real-time systems
KW  - Two dimensional displays
DO  - 10.1109/IROS.2018.8593993
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We propose an efficient and scalable method for incrementally building a dense, semantically annotated 3D map in real-time. The proposed method assigns class probabilities to each region, not each element (e.g., surfel and voxel), of the 3D map which is built up through a robust SLAM framework and incrementally segmented with a geometric-based segmentation method. Differently from all other approaches, our method has a capability of running at over 30Hz while performing all processing components, including SLAM, segmentation, 2D recognition, and updating class probabilities of each segmentation label at every incoming frame, thanks to the high efficiency that characterizes the computationally intensive stages of our framework. By utilizing a specifically designed CNN to improve the frame-wise segmentation result, we can also achieve high accuracy. We validate our method on the NYUv2 dataset by comparing with the state of the art in terms of accuracy and computational efficiency, and by means of an analysis in terms of time and space complexity.
ER  - 

TY  - CONF
TI  - Semantic Monocular SLAM for Highly Dynamic Environments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 393
EP  - 400
AU  - N. Brasch
AU  - A. Bozic
AU  - J. Lallemand
AU  - F. Tombari
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - image motion analysis
KW  - image sequences
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - static environment
KW  - semantic monocular SLAM framework
KW  - semantic information
KW  - explicit probabilistic model
KW  - dynamic environments
KW  - Virtual KITTI
KW  - Synthia datasets
KW  - pose estimation
KW  - Semantics
KW  - Simultaneous localization and mapping
KW  - Feature extraction
KW  - Dynamics
KW  - Cameras
KW  - Pose estimation
KW  - Probabilistic logic
DO  - 10.1109/IROS.2018.8593828
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Recent advances in monocular SLAM have enabled real-time capable systems which run robustly under the assumption of a static environment, but fail in presence of dynamic scene changes and motion, since they lack an explicit dynamic outlier handling. We propose a semantic monocular SLAM framework designed to deal with highly dynamic environments, combining feature-based and direct approaches to achieve robustness under challenging conditions. The proposed approach exploits semantic information extracted from the scene within an explicit probabilistic model, which maximizes the probability for both tracking and mapping to rely on those scene parts that do not present a relative motion with respect to the camera. We show more stable pose estimation in dynamic environments and comparable performance to the state of the art on static sequences on the Virtual KITTI and Synthia datasets.
ER  - 

TY  - CONF
TI  - Path-Following through Control Funnel Functions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 401
EP  - 408
AU  - H. Ravanbakhsh
AU  - S. Aghli
AU  - C. Heckman
AU  - S. Sankaranarayanan
PY  - 2018
KW  - control system synthesis
KW  - feedback
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - road vehicles
KW  - robot dynamics
KW  - robust control
KW  - trajectory control
KW  - vehicle dynamics
KW  - control feedback laws
KW  - control funnel functions
KW  - path following
KW  - reference trajectory
KW  - autonomous vehicles
KW  - robustness
KW  - timing law
KW  - mathematical model
KW  - vehicle dynamics
KW  - demonstration-based learning algorithm
KW  - autonomous vehicle
KW  - Parkour car
KW  - trajectory tracking
KW  - Trajectory
KW  - Robustness
KW  - Autonomous vehicles
KW  - Timing
KW  - Vehicle dynamics
KW  - Automobiles
KW  - Trajectory tracking
DO  - 10.1109/IROS.2018.8593637
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We present an approach to path following using so-called control funnel functions. Synthesizing controllers to “robustly” follow a reference trajectory is a fundamental problem for autonomous vehicles. Robustness, in this context, requires our controllers to handle a specified amount of deviation from the desired trajectory. Our approach considers a timing law that describes how fast to move along a given reference trajectory and a control feedback law for reducing deviations from the reference. We synthesize both feedback laws using “control funnel functions” that jointly encode the control law as well as its correctness argument over a mathematical model of the vehicle dynamics. We adapt a previously described demonstration-based learning algorithm to synthesize a control funnel function as well as the associated feedback law. We implement this law on top of a 1/8th scale autonomous vehicle called the Parkour car. We compare the performance of our path following approach against a trajectory tracking approach by specifying trajectories of varying lengths and curvatures. Our experiments demonstrate the improved robustness obtained from the use of control funnel functions.
ER  - 

TY  - CONF
TI  - Online inference of human belief for cooperative robots
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 409
EP  - 415
AU  - M. C. Buehler
AU  - T. H. Weisswange
PY  - 2018
KW  - belief networks
KW  - cognition
KW  - cognitive systems
KW  - cooperative systems
KW  - human-robot interaction
KW  - inference mechanisms
KW  - interactive systems
KW  - mobile robots
KW  - multi-robot systems
KW  - online inference
KW  - natural interaction
KW  - human-human cooperation
KW  - model-based belief filter
KW  - human action
KW  - cognitive processes
KW  - perception
KW  - action selection
KW  - double inference process
KW  - environmental state
KW  - human-robot cooperation experiment
KW  - situation awareness
KW  - cognitive states
KW  - Task analysis
KW  - Robots
KW  - Manufacturing
KW  - Collaboration
KW  - Probability distribution
KW  - Estimation
KW  - Mathematical model
DO  - 10.1109/IROS.2018.8594076
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - For human-robot cooperation, inferring a hu-man's cognitive state is very important for an efficient and natural interaction. Similar to human-human cooperation, understanding what the partner plans and knowing, if he is situation aware, is necessary to prevent collisions, offer support at the right time, correct mistakes before they happen or choose the best actions for oneself as early as possible. We propose a model-based belief filter to extract relevant aspects of a human's mental state online during cooperation. It performs inference based on human actions and its own task knowledge, modeling cognitive processes like perception and action selection. In contrast to most prior work, we explicitly estimate the human belief instead of inferring only a single mode or intention. Since this is a double inference process, we focus on representing the human estimates of environmental state and task as well as corresponding uncertainties. We designed a human-robot cooperation experiment that allowed for a variety of cognitive states of both agents and collected data to test and evaluate the proposed belief filter. The results are promising, as our system can be used to provide reasonable predictions of the human action and insights into his situation awareness. At the same time it is inferring interpretable information about the underlying cognitive states - A belief about the human's belief about the environment.
ER  - 

TY  - CONF
TI  - An Omnidirectional Jumper with Expanded Movability via Steering, Self-Righting and Take-off Angle Adjustment
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 416
EP  - 421
AU  - S. Yim
AU  - S. Baek
AU  - G. Jung
AU  - K. Cho
PY  - 2018
KW  - biomechanics
KW  - mobile robots
KW  - steering shares
KW  - modified active triggering mechanism
KW  - jumping performance
KW  - expanded locomotion capabilities
KW  - angle adjustment
KW  - self-righting
KW  - expanded movability
KW  - omnidirectional jumper
KW  - Robots
KW  - Couplings
KW  - Gears
KW  - Windings
KW  - Pulleys
KW  - Wheels
KW  - Energy storage
DO  - 10.1109/IROS.2018.8594372
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose an omnidirectional jumper with expanded locomotion capabilities. The mechanisms for four functions-jumping, steering, self-righting and take-off angle adjustment-are designed using only two motors to maximize the jumping performance. Jumping uses the modified active triggering mechanism with one motor. Steering shares this motor and uses the wheel touching the ground. The take-off angle is adjusted by changing the angle between the body and the foot using another motor. Self-righting is possible by utilizing combinations of the movements that occur in the energy storing and angle adjustment processes. With these four functions, the robot is capable of jumping in all directions and can jump anywhere in between the maximum height and maximum distance. It can also jump multiple times by self-righting. The robot, with a mass of 64.4 g, jumps up to 113 cm in vertical height, and 170 cm in horizontal distance. This robot can be deployed to explore various environments. Moreover, the design method to implement more functions than the number of motors can be applied to design other small-scale robots.
ER  - 

TY  - CONF
TI  - Delineating boundaries of feasibility between robot designs
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 422
EP  - 429
AU  - S. Ghasemlou
AU  - J. M. O'Kane
AU  - D. A. Shell
PY  - 2018
KW  - decision trees
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - pattern classification
KW  - planning (artificial intelligence)
KW  - sensors
KW  - actuators
KW  - classic search methods
KW  - planning problem
KW  - actuator resources
KW  - effective robots
KW  - robot designs
KW  - delineating boundaries
KW  - domain knowledge
KW  - design space
KW  - interactive tools
KW  - interactive process
KW  - boundary subject
KW  - compact implicit representation
KW  - decision tree learning method
KW  - discriminatory features
KW  - Robot sensing systems
KW  - Planning
KW  - Actuators
KW  - Task analysis
KW  - Tools
KW  - Decision trees
DO  - 10.1109/IROS.2018.8593811
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Motivated by the need for tools to aid in the design of effective robots, we examine how to determine the role that particular sensing and actuator resources play in enabling a robot to achieve useful ends. Rather than merely asking “will this sensor suffice?” we classify general modifications to the set of sensors and actuators based on the feasibility of accomplishing given tasks using these sets. The goal is to probe the boundary between modifications that are destructive on a given planning problem, and modifications that are not. Since this boundary itself can be impractically large, classic search methods are of no avail to summarize discriminatory features on this boundary. Instead, we propose a decision tree learning method to efficiently construct a compact implicit representation of the boundary. The idea is to allow the designer to use prior knowledge to constrain the search, then use the tool to probe the boundary subject to those constraints, gaining insight into the information necessary for a robot to ensure task achievement. Ultimately we envision a interactive process where additional constraints are repeatedly included as new light is shed. We aim to pave the way for interactive tools that help the roboticist navigate the complexities of the design space. We describe an implementation of this approach along with experimental results that show that the method can construct decision trees with explanatory value. Our experiments suggest that some domain knowledge (specifically picking features that emphasize monotonicity) substantially improves running-time with only negligible reduction in accuracy.
ER  - 

TY  - CONF
TI  - Discrete Configuration Space Methods for Determining Modular Connector Area of Acceptance in Higher Dimensions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 430
EP  - 435
AU  - N. Eckenstein
AU  - M. Yim
PY  - 2018
KW  - collision avoidance
KW  - discrete systems
KW  - mobile robots
KW  - discrete configuration space methods
KW  - physical connectors
KW  - docking process
KW  - robotic control systems
KW  - automatic control systems
KW  - robotic self-reconfiguration
KW  - air-to-air refueling
KW  - configuration space obstacle model
KW  - modular connector area of acceptance
KW  - self-aligning geometry
KW  - Meyer's flooding algorithm
KW  - Connectors
KW  - Geometry
KW  - Robots
KW  - Contacts
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Image segmentation
DO  - 10.1109/IROS.2018.8594072
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Physical connectors with self-aligning geometry aid in the docking process for many robotic and automatic control systems such as robotic self-reconfiguration and air-to-air refueling. This self-aligning geometry provides a wider range of acceptable error tolerance in relative pose between the two rigid objects, increasing successful docking chances. We present a new method for computing the error range (or area of acceptance) for a pair of rigid connector objects with self-aligning geometry capable of higher dimensional analysis which was previously limited to three. The method is based on the configuration space obstacle model, which gives us a representation of the space of contact states between the two objects. Using an approach direction as analogous to gravity, and assuming the target docked configuration is stable, the set of misaligned points that lead to docking is the target configuration's watershed for an arbitrarily dimensioned configuration space obstacle. It is well known that the watershed of a height map on a discrete grid can be found using any number of algorithms from image segmentation. We present an implementation based on Meyer's flooding algorithm to determine this watershed and measure the AA for simple connectors in 2D and 3D. Results are presented for systems including unconstrained motion in SE(2) and motion constrained to four dimensions (ie. x,y,z,pitch) in SE(3).
ER  - 

TY  - CONF
TI  - An Origami-Inspired Flexible Pneumatic Actuator
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 436
EP  - 441
AU  - F. Schmitt
AU  - O. Piccin
AU  - L. Barbé
AU  - B. Bayle
PY  - 2018
KW  - architecture
KW  - hinges
KW  - mechanical testing
KW  - plates (structures)
KW  - pneumatic actuators
KW  - prototypes
KW  - rapid prototyping (industrial)
KW  - shear modulus
KW  - three-dimensional printing
KW  - prototype
KW  - origami-inspired flexible pneumatic actuator design
KW  - multimaterial additive manufacturing process
KW  - mechanical testing
KW  - airtight chamber
KW  - flexible origami-inspired architecture
KW  - short stroke displacements
KW  - material resistance
KW  - flexible hinges
KW  - rigid plates
KW  - Actuators
KW  - Geometry
KW  - Prototypes
KW  - Shape
KW  - Soft robotics
KW  - Three-dimensional printing
DO  - 10.1109/IROS.2018.8593423
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a new actuator designed to produce forces under short stroke displacements. Two variants of the prototype have been manufactured using Multi-Material Additive Manufacturing process, based on a flexible origami-inspired architecture. The structure consists of an airtight chamber constituted by rigid plates combined with flexible hinges and surfaces in order to allow the generation of motion. We propose several insights on integration issues such as limited material resistance and maximum range of motion. Both versions of the prototype are then tested to assess their performances for single strokes and cyclic loading.
ER  - 

TY  - CONF
TI  - Design and Development of Biaxial Active Nozzle with Flexible Flow Channel for Air Floating Active Scope Camera
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 442
EP  - 449
AU  - A. Ishii
AU  - Y. Ambe
AU  - Y. Yamauchi
AU  - H. Ando
AU  - M. Konyo
AU  - K. Tadakuma
AU  - S. Tadokoro
PY  - 2018
KW  - buckling
KW  - cameras
KW  - deformation
KW  - design engineering
KW  - jets
KW  - mobile robots
KW  - motion control
KW  - nozzles
KW  - pneumatic actuators
KW  - position control
KW  - service robots
KW  - shapes (structures)
KW  - flexible robot
KW  - shape deformation
KW  - air floating active scope camera
KW  - pneumatic actuators
KW  - geometric parameters
KW  - ASC
KW  - rescue operations
KW  - reaction force direction
KW  - flexible air tube
KW  - air jet direction
KW  - head motion
KW  - flexible flow channel
KW  - biaxial active nozzle
KW  - Electron tubes
KW  - Robots
KW  - Force
KW  - Shape
KW  - Cameras
KW  - Strain
KW  - Pneumatic systems
DO  - 10.1109/IROS.2018.8594437
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Long flexible continuum robots have a high potential for search and rescue operations that explore deep layered debris. A general problem of these robots is in the control of the head motion because their thin bodies limit the space available to mount multiple actuators. This paper develops a biaxial active nozzle which can rotate the air jet direction along a roll and pitch axis in order to control the direction of reaction force and the head motion of a long flexible robot. A major challenge is how to change the air jet direction without a large resistance to the flow, which reduces the reaction force induced by the air jet. We propose a nozzle whose outlet is connected with a flexible air tube. The direction of the air jet is controlled by the smooth shape deformation of the tube. The nozzle should be compact enough to be installed on a thin robot, although the shape deformation of the tube may cause buckling. The flexible tube is modeled and simulated by a multiple link model used to derive the geometric parameters of the nozzle so that the nozzle is compact and the tube does not buckle. Based on the derived parameters, the biaxial active nozzle was developed. A basic performance experiment shows that the nozzle can change the reaction force direction by deforming the tube shape, while the magnitude of the reaction force is almost constant. We integrated the proposed nozzle with a conventional Active Scope Camera (ASC). The range where the robot can look around in a vertical exploration was significantly improved, which was three times larger than the previous ASC whose head was controlled by pneumatic actuators. The rubble field test demonstrates that the integrated ASC could move over rubble (maximum height of 200 mm) and steer the course.
ER  - 

TY  - CONF
TI  - Design and Implementation of Programmable Drawing Automata based on Cam Mechanisms for Representing Spatial Trajectory
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 450
EP  - 455
AU  - T. Takahashi
AU  - H. G. Okuno
PY  - 2018
KW  - cams (mechanical)
KW  - computer animation
KW  - data visualisation
KW  - graphical user interfaces
KW  - spatial trajectory representation
KW  - RSSR linkage
KW  - revolute-spherical-S-R linkage
KW  - user-specified 2D/3D trajectory
KW  - GUI
KW  - 3D animation
KW  - user programs PDA-0
KW  - PDA-0 programmable
KW  - programmable drawing automaton
KW  - cam mechanisms
KW  - programmable drawing automata
KW  - Automata
KW  - Couplings
KW  - Trajectory
KW  - Three-dimensional displays
KW  - Shape
KW  - Animation
KW  - Kinematics
DO  - 10.1109/IROS.2018.8594443
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents the design and implementation of a preliminary version of a programmable drawing automaton (PDA-0) that draws a user-specified 3D trajectory. PDA-0 is strongly inspired by Jaquet Droz's a programmable drawing automaton built in the 1770s using 6,000 moving parts, which was hand-coded. PDA-0 consists of RSSR (Revolute-Spherical-S-R) linkage and cam mechanisms with three interchangeable cams. Interchangeable cams make PDA-0 programmable because a user-specified 2D/3D trajectory is encoded into the set of three cams. The user programs PDA-0 by specifying a trajectory via a GUI or 3D animation. Subsequently, the compiler estimates a 3D trajectory mathematically from the user-specified 2D/3D trajectory and calculates the shape of the three cams, i.e., a code for PDA-0 by solving kinematic constraints. Finally, PDA-0 with the 3D-printed cams executes the code to draw the user-specified trajectory. The current PDA-0 with three cams demonstrates drawing simple trajectories such as letters and symbols.
ER  - 

TY  - CONF
TI  - Auxetic Sleeves for Soft Actuators with Kinematically Varied Surfaces
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 464
EP  - 471
AU  - A. Sedal
AU  - M. Fisher
AU  - J. Bishop-Moser
AU  - A. Wineman
AU  - S. Kota
PY  - 2018
KW  - actuators
KW  - auxetics
KW  - bending
KW  - Poisson ratio
KW  - prototypes
KW  - robot kinematics
KW  - radial expansion
KW  - bending
KW  - prototypes
KW  - auxetic sleeves
KW  - representative auxetic element
KW  - kinematic model
KW  - Poisson's ratio
KW  - RAE-based design scheme
KW  - RAE-patterned actuators
KW  - soft robots
KW  - soft actuator
KW  - Actuators
KW  - Auxetic materials
KW  - Kinematics
KW  - Finite element analysis
KW  - Shape
KW  - Strain
KW  - Stress
DO  - 10.1109/IROS.2018.8594212
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Soft actuators with auxetic, or negative Poisson's ratio (NPR), behavior offer a way to create soft robots with novel kinematic behavior. This paper presents an original framework for reinforcement of a soft actuator using a generalized NPR element, called a Representative Auxetic Element (RAE), and an experimental validation of the kinematic behavior that it enables. We build a generalized kinematic model that enables the design of RAE-patterned actuators and reveal the distinct auxetic behavior of RAE actuators with comparable model accuracy to the legacy McKibben actuators. A simple, reproducible way of designing and fabricating RAE actuators is described and varied prototypes are shown. This RAE-based design scheme can be used to create actuators with specified kinematics like bending, extension, and radial expansion, which can also vary across the actuator's surface both circumferentially and axially in a tractable, scalable manner.
ER  - 

TY  - CONF
TI  - A Unified Controller for Region-reaching and Deforming of Soft Objects
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 472
EP  - 478
AU  - Z. Wang
AU  - X. Li
AU  - D. Navarro-Alarcon
AU  - Y. Liu
PY  - 2018
KW  - cameras
KW  - closed loop systems
KW  - deformation
KW  - end effectors
KW  - manipulators
KW  - mobile robots
KW  - robot vision
KW  - stability
KW  - uncertain deformation model
KW  - active deformable object manipulation
KW  - unified controller
KW  - soft objects
KW  - robotic manipulation
KW  - robot control
KW  - region reaching
KW  - region deforming
KW  - uncalibrated cameras
KW  - closed-loop system stability
KW  - end-effector
KW  - Strain
KW  - Deformable models
KW  - Cameras
KW  - End effectors
KW  - Robot vision systems
KW  - Adaptation models
DO  - 10.1109/IROS.2018.8593543
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Emerging applications of robotic manipulation of deformable objects have opened up new challenges in robot control. While several control techniques have been developed to manipulate deformable objects, the performance of existing methods is commonly limited by two issues: 1) implicit assumption that the physical contact between the end-effector and the object is always maintained, and 2) requirements of exact parameters of deformation model, which are difficult to obtain. This paper presents a new control scheme for robotic manipulation of deformable objects, which allows the robot to automatically contact then actively deform the deformable object by assessing the status of deformation in real time. Instead of designing multiple controllers and switching among them, the proposed method smoothly and stably integrates two control phases (i.e. region reaching and active deforming) into a single controller. The stability of the closed-loop system is rigorously proved with the consideration of the uncertain deformation model and uncalibrated cameras. Hence, the proposed control scheme enhances the autonomous capability of active deformable object manipulation. Experimental studies are conducted with different initial conditions to demonstrate the performance of the proposed controller.
ER  - 

TY  - CONF
TI  - Dual-arm robotic manipulation of flexible cables
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 479
EP  - 484
AU  - J. Zhu
AU  - B. Navarro
AU  - P. Fraisse
AU  - A. Crosnier
AU  - A. Cherubini
PY  - 2018
KW  - cables (mechanical)
KW  - deformation
KW  - Fourier series
KW  - manipulator dynamics
KW  - manipulators
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - velocity control
KW  - arm robotic manipulation
KW  - flexible cables
KW  - trivial task
KW  - multiple robot manipulators
KW  - local deformation model
KW  - shape parameters
KW  - dual-arm manipulator
KW  - cable shape manipulation
KW  - Shape
KW  - Strain
KW  - Power cables
KW  - Deformable models
KW  - Manipulators
KW  - Task analysis
DO  - 10.1109/IROS.2018.8593780
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Deforming a cable to a desired (reachable) shape is a trivial task for a human to do without even knowing the internal dynamics of the cable. This paper proposes a framework for cable shapes manipulation with multiple robot manipulators. The shape is parameterized by a Fourier series. A local deformation model of the cable is estimated on-line with the shape parameters. Using the deformation model, a velocity control law is applied on the robot to deform the cable into the desired shape. Experiments on a dual-arm manipulator are conducted to validate the framework.
ER  - 

TY  - CONF
TI  - Towards vision-based manipulation of plastic materials
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 485
EP  - 490
AU  - A. Cherubini
AU  - J. Leitner
AU  - V. Ortenzi
AU  - P. Corke
PY  - 2018
KW  - deformation
KW  - manipulators
KW  - object tracking
KW  - plastic products
KW  - robot vision
KW  - vision-based manipulation
KW  - plastic materials
KW  - object deformation
KW  - visual tracking
KW  - visual error
KW  - deformable objects
KW  - kinetic sand shaping
KW  - Task analysis
KW  - Robots
KW  - Shape
KW  - Plastics
KW  - Visualization
KW  - Deformable models
KW  - Strain
KW  - Manipulation
KW  - visual servoing
KW  - human studies
KW  - learning
DO  - 10.1109/IROS.2018.8594108
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper represents a step towards vision-based manipulation of plastic materials. Manipulating deformable objects is made challenging by: 1) the absence of a model for the object deformation, 2) the inherent difficulty of visual tracking of deformable objects, 3) the difficulty in defining a visual error and 4) the difficulty in generating control inputs to minimise the visual error. We propose a novel representation of the task of manipulating deformable objects. In this preliminary case study, the shaping of kinetic sand, we assume a finite set of actions: pushing, tapping and incising. We consider that these action types affect only a subset of the state, i.e., their effect does not affect the entire state of the system (specialized actions). We report the results of a user study to validate these hypotheses and release the recorded dataset. The actions (pushing, tapping and incising) are clearly adopted during the task, although it is clear that 1) participants use also mixed actions and 2) actions' effects can marginally affect the entire state, requesting a relaxation of our specialized actions hypothesis. Moreover, we compute task errors and corresponding control inputs (in the image space) using image processing. Finally, we show how machine learning can be applied to infer the mapping from error to action on the data extracted from the user study.
ER  - 

TY  - CONF
TI  - Capturing Deformations of Interacting Non-rigid Objects Using RGB-D Data
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 491
EP  - 497
AU  - A. Petit
AU  - S. Cotin
AU  - V. Lippiello
AU  - B. Siciliano
PY  - 2018
KW  - computational geometry
KW  - finite element analysis
KW  - image colour analysis
KW  - image registration
KW  - image segmentation
KW  - image sequences
KW  - segmented point clouds
KW  - collision detection
KW  - joint registration framework
KW  - RGB-D sensor
KW  - point cloud data
KW  - elastic deformations
KW  - RGB-D data
KW  - interacting nonrigid objects
KW  - FEM elastic model
KW  - geometrical point-to-point correspondences
KW  - ICP algorithm
KW  - rigid transformations
KW  - RGB images
KW  - visual segmentation
KW  - Strain
KW  - Three-dimensional displays
KW  - Finite element analysis
KW  - Deformable models
KW  - Computational modeling
KW  - Collision avoidance
KW  - Visualization
DO  - 10.1109/IROS.2018.8593756
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a method for tracking multiple interacting deformable objects undergoing rigid motions, elastic deformations and contacts, using image and point cloud data provided by an RGB-D sensor. A joint registration frame-work is proposed, based on physical Finite Element Method (FEM) elastic and interaction models. It first relies on a visual segmentation of the considered objects in the RGB images. The different segmented point clouds are then processed to estimate rigid transformations with on an ICP algorithm, and to determine geometrical point-to-point correspondences with the meshes. External forces resulting from these correspondences and between the current and the rigidly transformed mesh can then be derived. It provides both non-rigid and rigid data cues. A classical collision detection and response model is also integrated, giving contact forces between the objects. The deformations of the objects are estimated by solving a dynamic system balancing these external and contact forces with the internal or regularization forces computed through the FEM elastic model. This approach has been here tested on different scenarios involving two or three interacting deformable objects of various shapes, with promising results.
ER  - 

TY  - CONF
TI  - Contact Detection and Size Estimation Using a Modular Soft Gripper with Embedded Flex Sensors
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 498
EP  - 503
AU  - K. Elgeneidy
AU  - G. Neumann
AU  - S. Pearson
AU  - M. Jackson
AU  - N. Lohse
PY  - 2018
KW  - bending
KW  - biomechanics
KW  - deformation
KW  - elastomers
KW  - feedback
KW  - grippers
KW  - mechanical contact
KW  - pneumatic actuators
KW  - pressure sensors
KW  - sensors
KW  - time series
KW  - modular soft gripper
KW  - utilizing interchangeable soft pneumatic actuators
KW  - embedded flex sensors
KW  - simple sensory feedback
KW  - pressure sensors
KW  - contact state
KW  - grasped object size
KW  - contact type
KW  - final flex sensor
KW  - object weight
KW  - soft fingers
KW  - flex sensor readings
KW  - soft elastomers
KW  - deformable objects
KW  - contact forces
KW  - contact feedback
KW  - Grippers
KW  - Sensors
KW  - Flexible printed circuits
KW  - Grasping
KW  - Thumb
KW  - Three-dimensional displays
DO  - 10.1109/IROS.2018.8593399
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Grippers made from soft elastomers are able to passively and gently adapt to their targets allowing deformable objects to be grasped safely without causing bruise or damage. However, it is difficult to regulate the contact forces due to the lack of contact feedback for such grippers. In this paper, a modular soft gripper is presented utilizing interchangeable soft pneumatic actuators with embedded flex sensors as fingers of the gripper. The fingers can be assembled in different configurations using 3D printed connectors. The paper investigates the potential of utilizing the simple sensory feedback from the flex and pressure sensors to make additional meaningful inferences regarding the contact state and grasped object size. We study the effect of the grasped object size and contact type on the combined feedback from the embedded flex sensors of opposing fingers. Our results show that a simple linear relationship exists between the grasped object size and the final flex sensor reading at fixed input conditions, despite the variation in object weight and contact type. Additionally, by simply monitoring the time series response from the flex sensor, contact can be detected by comparing the response to the known free-bending response at the same input conditions. Furthermore, by utilizing the measured internal pressure supplied to the soft fingers, it is possible to distinguish between power and pinch grasps, as the contact type affects the rate of change in the flex sensor readings against the internal pressure.
ER  - 

TY  - CONF
TI  - Online Shape Estimation based on Tactile Sensing and Deformation Modeling for Robot Manipulation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 504
EP  - 511
AU  - J. Sanchez
AU  - C. M. Mateo
AU  - J. A. Corrales
AU  - B. Bouzgarrou
AU  - Y. Mezouar
PY  - 2018
KW  - dexterous manipulators
KW  - force control
KW  - image sensors
KW  - tactile sensors
KW  - deformation model
KW  - tactile sensor
KW  - deformable object
KW  - sensor model
KW  - online shape estimation
KW  - tactile sensing
KW  - deformation modeling
KW  - visual sensing
KW  - soft object
KW  - robot manipulation
KW  - shadow dexterous hand
KW  - BioTac sensors
KW  - RGB-D sensor
KW  - Strain
KW  - Robot sensing systems
KW  - Deformable models
KW  - Force
KW  - Shape
KW  - Computational modeling
DO  - 10.1109/IROS.2018.8594314
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Precise robot manipulation of deformable objects requires an accurate and fast estimation of their shape as they deform. So far, visual sensing has been mostly used to solve this issue, but vision sensors are sensitive to occlusions, which might be inevitable when manipulating an object with robot. To address this issue, we present a modular pipeline to track the shape of a soft object in an online manner by coupling tactile sensing with a deformation model. Using a model of a tactile sensor, we compute the magnitude and location of a contact force and apply it as an external force to the deformation model. The deformation model then updates the nodal positions of a mesh that describes the shape of the deformable object. The proposed sensor model and pipeline, are evaluated using a Shadow Dexterous Hand equipped with BioTac sensors on its fingertips and an RGB-D sensor.
ER  - 

TY  - CONF
TI  - Accounting for Directional Rigidity and Constraints in Control for Manipulation of Deformable Objects without Physical Simulation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 512
EP  - 519
AU  - M. Ruan
AU  - D. McConachie
AU  - D. Berenson
PY  - 2018
KW  - biomechanics
KW  - collision avoidance
KW  - deformation
KW  - finite element analysis
KW  - grippers
KW  - motion control
KW  - physiological models
KW  - shear modulus
KW  - springs (mechanical)
KW  - stress analysis
KW  - directional rigidity
KW  - deformable objects
KW  - physical simulation
KW  - physical models
KW  - control loop
KW  - practical controller
KW  - deformable object manipulation
KW  - effective controller
KW  - accurate geometric model
KW  - gripper motion
KW  - novel stretching avoidance constraint
KW  - physical robot
KW  - Grippers
KW  - Deformable models
KW  - Computational modeling
KW  - Robots
KW  - Predictive models
KW  - Finite element analysis
KW  - Adaptation models
DO  - 10.1109/IROS.2018.8594520
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Deformable objects like cloth and rope are challenging to manipulate because it is difficult to predict the state of the object given a motion of the gripper(s) holding it. In much previous work, physical models (such as Mass-Spring or Finite-Element) have been used to model such affects. However, these models often require significant parameter tuning for each scenario and can be expensive to simulate inside a control loop. Furthermore, it is difficult to create a practical controller for deformable object manipulation that preserves constraints, especially avoiding overstretching the object. In this paper, we developed a more effective controller than previous work by (1) constructing a more accurate geometric model of how the direction of gripper motion and obstacles affect deformable objects; and (2) specifying a novel stretching avoidance constraint to prevent the object from being overstretched by the robot. Experiments comparing our new method to the previous method in simulation and on a physical robot suggest that our new model captures the behavior of the object more accurately. We also find that our controller is able to prevent tearing that would occur when using the previous method.
ER  - 

TY  - CONF
TI  - A Series Elastic Tactile Sensing Array for Tactile Exploration of Deformable and Rigid Objects
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 520
EP  - 525
AU  - Z. Kappassov
AU  - D. Baimukashev
AU  - O. Adiyatov
AU  - S. Salakchinov
AU  - Y. Massalin
AU  - H. A. Varol
PY  - 2018
KW  - elasticity
KW  - manipulators
KW  - position control
KW  - tactile sensors
KW  - deformable objects
KW  - rigid objects
KW  - series elastic elements
KW  - sixteen compliant sensing elements
KW  - position-controlled robot manipulator
KW  - series elastic tactile array
KW  - contact location
KW  - tactile arrays
KW  - multiple sensing elements
KW  - vision-based sensors
KW  - robotic systems
KW  - tactile sensing arrays
KW  - tactile exploration
KW  - series elastic tactile sensing array
KW  - Magnetic sensors
KW  - Pins
KW  - Tactile sensors
KW  - Saturation magnetization
DO  - 10.1109/IROS.2018.8593755
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Tactile sensing arrays are used to detect contacts of robotic systems with the environment. They are particularly useful for scenarios in which vision-based sensors cannot be used. Thanks to the presence of multiple sensing elements, tactile arrays also provide spatial information about the contact location. In this work, we present our series elastic tactile array to enable tactile exploration for position-controlled robot manipulators. Sixteen compliant sensing elements are arranged as a 4×4 array. This allows the position-controlled robot to explore objects via palpation. Tactile sensing was accomplished by measuring the change of the magnetic field caused by neodymium magnets embedded into the series elastic elements. We demonstrate the efficacy of our sensor with two sets of experiments involving physical interaction scenarios. Firstly, we show that the sensor can be used to differentiate between rigid and deformable objects. Secondly, we show that point clouds of objects can be generated quickly with our sensor module attached to a position-controlled robot manipulator as an end-effector.
ER  - 

TY  - CONF
TI  - Learning Symbolic Representations for Planning with Parameterized Skills
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 526
EP  - 533
AU  - B. Ames
AU  - A. Thackston
AU  - G. Konidaris
PY  - 2018
KW  - control engineering computing
KW  - humanoid robots
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - critical capability
KW  - generally intelligent robot behavior
KW  - goal-directed planning
KW  - black-box motor skills
KW  - intelligent robots
KW  - parametrized motor skills
KW  - simple discrete abstract representation
KW  - fixed plan
KW  - abstract symbolic representation
KW  - robot manipulation task
KW  - angry birds
KW  - virtual domain
KW  - Planning
KW  - Task analysis
KW  - Probabilistic logic
KW  - Intelligent robots
KW  - Birds
KW  - Computer science
DO  - 10.1109/IROS.2018.8594313
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - A critical capability required for generally intelligent robot behavior is the ability to sequence motor skills to reach a goal. This requires a (typically abstract) representation that supports goal-directed planning, which raises the question of how to construct such a representation. Previous work has addressed this question in the context of simple black-box motor skills, which are insufficiently flexible to support the wide range of behavior required of intelligent robots. We now extend that work to include parametrized motor skills, where a robot must both select an action to execute and also decide how to parametrize it. We show how to construct a representation suitable for planning with parametrized motor skills, and specify conditions which are sufficient to separate the selection of motor skills from the parametrization of those skills. Our method results in a simple discrete abstract representation for planning followed by a parameter selection process that operates on a fixed plan. We first demonstrate learning this representation in a virtual domain based on Angry Birds and then learn an abstract symbolic representation for a robot manipulation task.
ER  - 

TY  - CONF
TI  - Regularizing Reinforcement Learning with State Abstraction
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 534
EP  - 539
AU  - R. Akrour
AU  - F. Veiga
AU  - J. Peters
AU  - G. Neumann
PY  - 2018
KW  - learning (artificial intelligence)
KW  - optimisation
KW  - pattern clustering
KW  - deep reinforcement learning performance
KW  - optimal sub-policies
KW  - state space clustering
KW  - hierarchical reinforcement learning algorithm
KW  - near-optimal policy
KW  - state cluster
KW  - abstract state
KW  - continuous action reinforcement learning
KW  - similar optimal action
KW  - discrete reinforcement
KW  - state abstraction
KW  - learned policy
KW  - Reinforcement learning
KW  - Complexity theory
KW  - Convergence
KW  - Shape
KW  - Clustering algorithms
KW  - Task analysis
KW  - Partitioning algorithms
DO  - 10.1109/IROS.2018.8594201
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - State abstraction in a discrete reinforcement learning setting clusters states sharing a similar optimal action to yield an easier to solve decision process. In this paper, we generalize the concept of state abstraction to continuous action reinforcement learning by defining an abstract state as a state cluster over which a near-optimal policy of simple shape exists. We propose a hierarchical reinforcement learning algorithm that is able to simultaneously find the state space clustering and the optimal sub-policies in each cluster. The main advantage of the proposed framework is to provide a straightforward way of regularizing reinforcement learning by controlling the behavioral complexity of the learned policy. We apply our algorithm on several benchmark tasks and a robot tactile manipulation task and show that we can match state-of-the-art deep reinforcement learning performance by combining a small number of linear policies.
ER  - 

TY  - CONF
TI  - CReaM: Condensed Real-time Models for Depth Prediction using Convolutional Neural Networks
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 540
EP  - 547
AU  - A. Spek
AU  - T. Dharmasiri
AU  - T. Drummond
PY  - 2018
KW  - convolutional neural nets
KW  - image classification
KW  - image segmentation
KW  - mobile robots
KW  - neurocontrollers
KW  - robot vision
KW  - CNNs
KW  - robotic vision community
KW  - semantic segmentation
KW  - surface curvature
KW  - robotic society
KW  - real-time structure prediction framework
KW  - NVIDIA-TX2
KW  - CReaM
KW  - real-time models
KW  - depth prediction
KW  - convolutional neural networks
KW  - classification
KW  - mobile platform
KW  - condensed model architectures
KW  - Real-time systems
KW  - Predictive models
KW  - Robots
KW  - Training
KW  - Modeling
KW  - Task analysis
KW  - Semantics
DO  - 10.1109/IROS.2018.8594243
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Since the resurgence of CNNs the robotic vision community has developed a range of algorithms that perform classification, semantic segmentation and structure prediction (depths, normals, surface curvature) using neural networks. While some of these models achieve state-of-the art results and super human level performance, deploying these models in a time critical robotic environment remains an ongoing challenge. Real-time frameworks are of paramount importance to build a robotic society where humans and robots integrate seamlessly. To this end, we present a novel real-time structure prediction framework that predicts depth at 30 frames per second on an NVIDIA-TX2. At the time of writing, this is the first piece of work to showcase such a capability on a mobile platform. We also demonstrate with extensive experiments that neural networks with very large model capacities can be leveraged in order to train accurate condensed model architectures in a “from teacher to student” style knowledge transfer.
ER  - 

TY  - CONF
TI  - Generating Adaptive Attending Behaviors using User State Classification and Deep Reinforcement Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 548
EP  - 555
AU  - Y. Kohari
AU  - J. Miura
AU  - S. Oishi
PY  - 2018
KW  - behavioural sciences computing
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - pattern classification
KW  - deep deterministic policy gradient
KW  - user state classification
KW  - deep reinforcement learning
KW  - user information
KW  - adaptive attending behavior generation
KW  - DDPG
KW  - mobile robots
KW  - Legged locomotion
KW  - Reinforcement learning
KW  - Two dimensional displays
KW  - Cameras
KW  - Acceleration
KW  - Robot sensing systems
DO  - 10.1109/IROS.2018.8594427
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper describes a method of generating attending behaviors adaptively to the user state. The method classifies the user state based on user information such as the relative position and the orientation. For each classified state, the method executes the corresponding policy for behavior generation, which has been trained using a deep reinforcement learning, namely DDPG (deep deterministic policy gradient). We use as a state space of DDPG a distance-transformed local map with person information, and define reward functions suitable for respective user states. We conducted attending experiments both in a simulated and a real environment to show the effectiveness of the proposed method.
ER  - 

TY  - CONF
TI  - A Bio-inspired Reinforcement Learning Rule to Optimise Dynamical Neural Networks for Robot Control
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 556
EP  - 561
AU  - T. Wei
AU  - B. Webb
PY  - 2018
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - neurocontrollers
KW  - recurrent neural nets
KW  - back-propagation
KW  - 2D bipedal walking simulation
KW  - biological neural circuits
KW  - robot control
KW  - optimise dynamical neural networks
KW  - bio-inspired reinforcement
KW  - bio-inspired central pattern generator layer
KW  - recurrent neural network
KW  - learning rule
KW  - network weights
KW  - biological synapses
KW  - reinforcement learning approach
KW  - Neurons
KW  - Robots
KW  - Synapses
KW  - Oscillators
KW  - Task analysis
KW  - Biological neural networks
DO  - 10.1109/IROS.2018.8594017
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Most approaches for optimisation of neural networks are based on variants of back-propagation. This requires the network to be time invariant and differentiable; neural networks with dynamics are thus generally outside the scope of these methods. Biological neural circuits are highly dynamic yet clearly able to support learning. We propose a reinforcement learning approach inspired by the mechanisms and dynamics of biological synapses. The network weights undergo spontaneous fluctuations, and a reward signal modulates the centre and amplitude of fluctuations to converge to a desired network behaviour. We test the new learning rule on a 2D bipedal walking simulation, using a control system that combines a recurrent neural network, a bio-inspired central pattern generator layer and proportional-integral control, and demonstrate the first successful solution to this benchmark task.
ER  - 

TY  - CONF
TI  - Teaching Robots to Predict Human Motion
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 562
EP  - 567
AU  - L. Gui
AU  - K. Zhang
AU  - Y. Wang
AU  - X. Liang
AU  - J. M. F. Moura
AU  - M. Veloso
PY  - 2018
KW  - human-robot interaction
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - robot vision
KW  - deep learning
KW  - superior prediction performance
KW  - human moves
KW  - human motion
KW  - teaching robots
KW  - motion GAN model
KW  - predicted sequence
KW  - generative adversarial networks
KW  - forecasting algorithms
KW  - high-level fidelity validation
KW  - motion predictor
KW  - historical sequence
KW  - OpenPose library
KW  - robot camera
KW  - computer vision techniques
KW  - prediction ability
KW  - human-robot interaction
KW  - historical human movements
KW  - Robots
KW  - Gallium nitride
KW  - Generative adversarial networks
KW  - Predictive models
KW  - Cameras
KW  - Skeleton
KW  - Decoding
DO  - 10.1109/IROS.2018.8594452
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Teaching a robot to predict and mimic how a human moves or acts in the near future by observing a series of historical human movements is a crucial first step in human-robot interaction and collaboration. In this paper, we instrument a robot with such a prediction ability by leveraging recent deep learning and computer vision techniques. First, our system takes images from the robot camera as input to produce the corresponding human skeleton based on real-time human pose estimation obtained with the OpenPose library. Then, conditioning on this historical sequence, the robot forecasts plausible motion through a motion predictor, generating a corresponding demonstration. Because of a lack of high-level fidelity validation, existing forecasting algorithms suffer from error accumulation and inaccurate prediction. Inspired by generative adversarial networks (GANs), we introduce a global discriminator that examines whether the predicted sequence is smooth and realistic. Our resulting motion GAN model achieves superior prediction performance to state-of-the-art approaches when evaluated on the standard H3.6M dataset. Based on this motion GAN model, the robot demonstrates its ability to replay the predicted motion in a human-like manner when interacting with a person.
ER  - 

TY  - CONF
TI  - Variational Autoencoder for End-to-End Control of Autonomous Driving with Novelty Detection and Training De-biasing
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 568
EP  - 575
AU  - A. Amini
AU  - W. Schwarting
AU  - G. Rosman
AU  - B. Araki
AU  - S. Karaman
AU  - D. Rus
PY  - 2018
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - traffic engineering computing
KW  - variational autoencoder
KW  - end-to-end control
KW  - autonomous driving
KW  - novelty detection
KW  - end-to-end training
KW  - deep neural networks
KW  - DNN training
KW  - sufficient training data
KW  - trained models
KW  - insufficient training data
KW  - biased training data
KW  - self-supervised learning
KW  - latent variables
KW  - insufficiently trained situations
KW  - training data imbalance
KW  - latent distributions
KW  - training pipeline
KW  - full-scale autonomous vehicle
KW  - end-to-end controller
KW  - Training
KW  - Autonomous vehicles
KW  - Aerospace electronics
KW  - Image reconstruction
KW  - Training data
KW  - Data models
KW  - Robots
DO  - 10.1109/IROS.2018.8594386
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper introduces a new method for end-to-end training of deep neural networks (DNNs) and evaluates it in the context of autonomous driving. DNN training has been shown to result in high accuracy for perception to action learning given sufficient training data. However, the trained models may fail without warning in situations with insufficient or biased training data. In this paper, we propose and evaluate a novel architecture for self-supervised learning of latent variables to detect the insufficiently trained situations. Our method also addresses training data imbalance, by learning a set of underlying latent variables that characterize the training data and evaluate potential biases. We show how these latent distributions can be leveraged to adapt and accelerate the training pipeline by training on only a fraction of the total dataset. We evaluate our approach on a challenging dataset for driving. The data is collected from a full-scale autonomous vehicle. Our method provides qualitative explanation for the latent variables learned in the model. Finally, we show how our model can be additionally trained as an end-to-end controller, directly outputting a steering control command for an autonomous vehicle.
ER  - 

TY  - CONF
TI  - Virtual-to-Real-World Transfer Learning for Robots on Wilderness Trails
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 576
EP  - 582
AU  - M. L. Iuzzolino
AU  - M. E. Walker
AU  - D. Szafir
PY  - 2018
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - pattern classification
KW  - virtual-to-real-world transfer learning
KW  - deep learning models
KW  - virtual environments
KW  - model training
KW  - real-world trail data
KW  - robots
KW  - wilderness trails
KW  - outdoor trails
KW  - classification models
KW  - Cameras
KW  - Robot vision systems
KW  - Deep learning
KW  - Training
KW  - Navigation
KW  - Data collection
DO  - 10.1109/IROS.2018.8593883
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robots hold promise in many scenarios involving outdoor use, such as search-and-rescue, wildlife management, and collecting data to improve environment, climate, and weather forecasting. However, autonomous navigation of outdoor trails remains a challenging problem. Recent work has sought to address this issue using deep learning. Although this approach has achieved state-of-the-art results, the deep learning paradigm may be limited due to a reliance on large amounts of annotated training data. Collecting and curating training datasets may not be feasible or practical in many situations, especially as trail conditions may change due to seasonal weather variations, storms, and natural erosion. In this paper, we explore an approach to address this issue through virtual-to-real-world transfer learning using a variety of deep learning models trained to classify the direction of a trail in an image. Our approach utilizes synthetic data gathered from virtual environments for model training, bypassing the need to collect a large amount of real images of the outdoors. We validate our approach in three main ways. First, we demonstrate that our models achieve classification accuracies upwards of 95% on our synthetic data set. Next, we utilize our classification models in the control system of a simulated robot to demonstrate feasibility. Finally, we evaluate our models on real-world trail data and demonstrate the potential of virtual-to-real-world transfer learning.
ER  - 

TY  - CONF
TI  - High-frame-rate Target Tracking with CNN-based Object Recognition
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 599
EP  - 606
AU  - M. Jiang
AU  - Y. Gu
AU  - T. Takaki
AU  - I. Ishii
PY  - 2018
KW  - cameras
KW  - computer vision
KW  - feature extraction
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object recognition
KW  - object tracking
KW  - target tracking
KW  - tracking
KW  - vision platform
KW  - visual feedback
KW  - pre-learned objects
KW  - tracking performance
KW  - pan-tilt active vision system
KW  - complex-shaped target
KW  - hybridized tracking algorithm
KW  - prototype intelligent mechanical tracking system
KW  - deep learning-based recognition algorithm
KW  - fast tracking method
KW  - intelligent tracking method
KW  - CNN-based object recognition
KW  - high-frame-rate target tracking
KW  - frequency 500.0 Hz
KW  - Target tracking
KW  - Correlation
KW  - Visualization
KW  - Cameras
KW  - Object recognition
KW  - Deep learning
DO  - 10.1109/IROS.2018.8594300
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper proposes an intelligent and fast tracking method for robust trackability against appearance changes. The method hybridizes a correlation-based tracking algorithm operating at hundreds of frames per second (fps) with a deep learning-based recognition algorithm operating at dozens of fps. A prototype intelligent mechanical tracking system was developed by implementing our hybridized tracking algorithm on a 500-fps vision platform. A complex-shaped target can be robustly tracked at the center of the camera view in real time by controlling a pan-tilt active vision system with 500 Hz visual feedback. The tracking performance of our proposed algorithm was verified by showing several experimental results for pre-learned objects, which were quickly manipulated against complex backgrounds.
ER  - 

TY  - CONF
TI  - Real-Time Edge Template Tracking via Homography Estimation
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 607
EP  - 612
AU  - X. Qin
AU  - S. He
AU  - Z. Zhang
AU  - M. Dehghan
AU  - J. Jin
AU  - M. Jagersand
PY  - 2018
KW  - edge detection
KW  - feature extraction
KW  - image sampling
KW  - image sequences
KW  - image texture
KW  - target tracking
KW  - transforms
KW  - video signal processing
KW  - homography estimation
KW  - planar edge templates
KW  - homography transformations
KW  - sampled edge pixels
KW  - Lucas-Kanade-like algorithm
KW  - low textured targets
KW  - edge template tracking
KW  - nonLambertian objects
KW  - video sequences
KW  - Image edge detection
KW  - Target tracking
KW  - Real-time systems
KW  - Video sequences
KW  - Cost function
KW  - Transforms
DO  - 10.1109/IROS.2018.8593551
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose a novel real-time method for tracking planar edge templates. This method tracks an edge template by estimating its homography transformations with respect to the sampled edge pixels detected from the incoming frames. Particularly, we define a cost function based on a new feature map of the to-be-tracked edge template and optimize it by a Lucas-Kanade-like algorithm. The feature map is defined as the fourth root of the distance transform. Our method operates on just edges so that it is good at tracking those low textured targets, such as hollow targets (mug rim), thin targets (cable, ring) and non-Lambertian objects (disc). We validate and compare our method with four other methods on five newly collected real-world video sequences. The results achieves the lowest overall average error (1.58 pixels) and also outperforms others in terms of success rate. The per frame processing time of about 30 ms proves that our method is acceptable in realtime applications. The code and dataset are publicly available at: http://webdocs.cs.ualberta.ca/~xuebin/.
ER  - 

TY  - CONF
TI  - Robust Model-Predictive Deformation Control of a Soft Object by Using a Flexible Continuum Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 613
EP  - 618
AU  - B. Ouyang
AU  - H. Mo
AU  - H. Chen
AU  - Y. Liu
AU  - D. Sun
PY  - 2018
KW  - deformation
KW  - Jacobian matrices
KW  - manipulators
KW  - mobile robots
KW  - nonlinear control systems
KW  - predictive control
KW  - robust control
KW  - surgery
KW  - robust model-predictive deformation control
KW  - soft object
KW  - flexible continuum robot
KW  - soft tissues
KW  - prediction horizon-based controller
KW  - Strain
KW  - Jacobian matrices
KW  - Force
KW  - Deformable models
KW  - Uncertainty
KW  - End effectors
DO  - 10.1109/IROS.2018.8593880
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Flexible continuum robots have exhibited unique advantages in working in an unstructured environment. Many applications require robots to actively control the deformation of soft objects, such as soft tissues in surgery. Thus, this study presents a robust model-predictive deformation control of a soft object using a flexible continuum robot. A linear approximation model for mapping from actuation space of a continuum robot to deformation space of a soft object is established. Jacobian matrix is estimated online by using a robust Geman-McClure estimator. Then, the deformation of the soft object is regulated by using a prediction horizon-based controller with exponential weighting for model uncertainty. The proposed control approach is effective in manipulating a soft object with a flexible continuum robot that is in contact with obstacles.
ER  - 

TY  - CONF
TI  - Closed form solution for Rotation Estimation using Photometric Spherical Moments
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 627
EP  - 634
AU  - H. Hadj-Abdelkader
AU  - O. Tahri
AU  - H. Benseddik
PY  - 2018
KW  - cameras
KW  - motion estimation
KW  - photometry
KW  - 3D rotation estimation
KW  - geometrical features
KW  - catadioptric camera
KW  - synthetic images
KW  - cost function
KW  - spherical moment properties
KW  - spherical images
KW  - photometric spherical moments
KW  - Cameras
KW  - Estimation
KW  - Three-dimensional displays
KW  - Visual servoing
KW  - Closed-form solutions
KW  - Motion estimation
DO  - 10.1109/IROS.2018.8593920
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents new schemes to estimate 3D rotation from spherical images. Unlike existing approaches, spherical moment properties are exploited to obtain a closed form solution without iteratively mimimizing a cost function. Actually, three methods using spherical moments are proposed: two of them can be classified as dense approaches, while the third one is hybrid combining geometrical features with dense ones. Experimental results using both synthetic images and acquired images using catadioptric cameras with different scenarios show the effectiveness of our approach.
ER  - 

TY  - CONF
TI  - City-Scale Road Audit System using Deep Learning
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 635
EP  - 640
AU  - S. Yarram
AU  - G. Varma
AU  - C. V. Jawahar
PY  - 2018
KW  - Global Positioning System
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - roads
KW  - traffic engineering computing
KW  - image tagging
KW  - GPS
KW  - multistep deep learning
KW  - road networks
KW  - city-scale road audit system
KW  - label hierarchy
KW  - road defects
KW  - semantic segmentation
KW  - Roads
KW  - Image segmentation
KW  - Semantics
KW  - Deep learning
KW  - Global Positioning System
KW  - Cameras
KW  - Real-time systems
DO  - 10.1109/IROS.2018.8594363
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Road networks in cities are massive and is a critical component of mobility. Fast response to defects, that can occur not only due to regular wear and tear but also because of extreme events like storms, is essential. Hence there is a need for an automated system that is quick, scalable and cost-effective for gathering information about defects. We propose a system for city-scale road audit, using some of the most recent developments in deep learning and semantic segmentation. For building and benchmarking the system, we curated a dataset which has annotations required for road defects. However, many of the labels required for road audit have high ambiguity which we overcome by proposing a label hierarchy. We also propose a multi-step deep learning model that segments the road, subdivide the road further into defects, tags the frame for each defect and finally localizes the defects on a map gathered using GPS. We analyze and evaluate the models on image tagging as well as segmentation at different levels of the label hierarchy.
ER  - 

TY  - CONF
TI  - Closed-Loop Single-Beacon Passive Acoustic Navigation for Low-Cost Autonomous Underwater Vehicles
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 641
EP  - 648
AU  - N. R. Rypkema
AU  - E. M. Fischel
AU  - H. Schmidt
PY  - 2018
KW  - autonomous underwater vehicles
KW  - closed loop systems
KW  - computational complexity
KW  - hydrophones
KW  - inertial navigation
KW  - marine navigation
KW  - mobile robots
KW  - particle filtering (numerical methods)
KW  - position control
KW  - localization
KW  - autonomous underwater vehicles
KW  - Doppler velocity log
KW  - positional error
KW  - acoustic beacon
KW  - DVL-aided INS
KW  - LBL system
KW  - SandShark AUV
KW  - underwater navigation
KW  - computational complexity
KW  - phased-array beamforming
KW  - closed-loop operation
KW  - particle filter
KW  - vehicle-mounted passive hydrophone receiver-array
KW  - multiAUV operations
KW  - power requirements
KW  - inertial navigation system
KW  - robotic vehicle
KW  - Acoustics
KW  - Navigation
KW  - Array signal processing
KW  - Receivers
KW  - Acoustic measurements
KW  - Transponders
KW  - Time-frequency analysis
DO  - 10.1109/IROS.2018.8593626
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Accurate localization is critical for a robotic vehicle to navigate autonomously. Conventional autonomous underwater vehicles (AUV s) typically rely on an inertial navigation system (INS) aided by a Doppler velocity log (DVL) in order to reduce the rate of positional error growth of dead-reckoning to a level suitable for reliable navigation underwater. The size, cost, and power requirements of these systems result in vehicles that are prohibitively large and expensive for multi-AUV operations. In this work we present the first results of closed-loop experiments using a miniature, low-cost SandShark AUV and a custom-designed, inexpensive acoustic system first described in our previous work. Results are validated using an independent LBL system, and indicate that our approach is suitably accurate to enable the self-localization of such AUVs without the use of an expensive DVL-aided INS. Self-localization is performed by obtaining acoustic range and angle measurements from the AUV to a single acoustic beacon using a vehicle-mounted passive hydrophone receiver-array, and fusing these measurements using a particle filter. A critical aspect of our approach that allows for real-time, closed-loop operation is the close coupling of conventional phased-array beamforming and particle filtering - this implementation detail reduces the computational complexity associated with our previously described two-stage beamforming plus particle filtering process, and consequently also enables an increase in particle count and an improvement in navigational accuracy. Experimental results are provided for two cases: first, absolute navigation in the case where the beacon is fixed at a known position; and second, relative navigation with a moving beacon, a novel operating paradigm for AUVs which promises to enable multi-AUV operations while maintaining bounded navigation error.
ER  - 

TY  - CONF
TI  - Unscented Kalman Filter on Lie Groups for Visual Inertial Odometry
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 649
EP  - 655
AU  - M. Brossard
AU  - S. Bonnabel
AU  - A. Barrau
PY  - 2018
KW  - distance measurement
KW  - Kalman filters
KW  - Lie groups
KW  - nonlinear filters
KW  - SLAM (robots)
KW  - state estimation
KW  - stereo image processing
KW  - unscented Kalman filter
KW  - Lie groups
KW  - visual information
KW  - inertial measurements
KW  - state estimation
KW  - robust estimation
KW  - computational efficiency
KW  - low-cost aerial vehicles
KW  - processor power
KW  - innovative filter
KW  - stereo visual inertial odometry building
KW  - invariant filtering theory
KW  - computational complexity
KW  - stereo multistate constraint Kalman filter
KW  - EuRoC dataset
KW  - MAV outdoor dataset
KW  - Cameras
KW  - Kalman filters
KW  - Visualization
KW  - Computational modeling
KW  - Uncertainty
KW  - Robustness
KW  - Noise measurement
KW  - Lie groups
KW  - unscented Kalman filter
KW  - visual inertial odometry
KW  - aerial vehicle
KW  - localization
DO  - 10.1109/IROS.2018.8593627
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Fusing visual information with inertial measurements for state estimation has aroused major interests in recent years. However, combining a robust estimation with computational efficiency remains challenging, specifically for low-cost aerial vehicles in which the quality of the sensors and the processor power are constrained by size, weight and cost. In this paper, we present an innovative filter for stereo visual inertial odometry building on: (i) the recently introduced stereo multistate constraint Kalman filter; (ii) the invariant filtering theory; and (iii) the unscented Kalman filter (UKF) on Lie groups. Our solution combines accuracy, robustness and versatility of the UKF. We then compare our approach to state-of-art solutions in terms of accuracy, robustness and computational complexity on the EuRoC dataset and a challenging MAV outdoor dataset.
ER  - 

TY  - CONF
TI  - A Multi-Position Joint Particle Filtering Method for Vehicle Localization in Urban Area
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 656
EP  - 662
AU  - S. Gu
AU  - Z. Xiang
AU  - Y. Zhang
AU  - Q. Qian
PY  - 2018
KW  - distance measurement
KW  - image matching
KW  - mobile robots
KW  - particle filtering (numerical methods)
KW  - path planning
KW  - probability
KW  - robot vision
KW  - flexible multiposition joint particle filtering
KW  - position error
KW  - anchor point
KW  - curving roads
KW  - ego-trajectory
KW  - probabilistic filtering method
KW  - flexible road map
KW  - long range navigation
KW  - error accumulation
KW  - visual odometry
KW  - traditional visual localization methods
KW  - autonomous vehicles
KW  - robust localization
KW  - urban area
KW  - vehicle localization
KW  - dense parallel road branches
KW  - Roads
KW  - Trajectory
KW  - Filtering
KW  - Urban areas
KW  - Wheels
KW  - Simultaneous localization and mapping
KW  - Navigation
DO  - 10.1109/IROS.2018.8593781
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Robust localization is a prerequisite for autonomous vehicles. Traditional visual localization methods like visual odometry suffer error accumulation on long range navigation. In this paper, a flexible road map based probabilistic filtering method is proposed to tackle this problem. To effectively match the ego-trajectory to various curving roads in map, a new representation based on anchor point (AP) which captures the main curving points on the trajectory is presented. Based on APs of the map and trajectory, a flexible Multi-Position Joint Particle Filtering (MPJPF) framework is proposed to correct the position error. The method features the capability of adaptively estimating a series of APs jointly and only updates the estimation at situations with low uncertainty. It explicitly avoids the drawbacks of obliging to determine the current position at large uncertain situations such as dense parallel road branches. The experiments carried out on KITTI benchmark demonstrate our success.
ER  - 

TY  - CONF
TI  - Courteous Autonomous Cars
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 663
EP  - 670
AU  - L. Sun
AU  - W. Zhan
AU  - M. Tomizuka
AU  - A. D. Dragan
PY  - 2018
KW  - automobiles
KW  - road traffic
KW  - traffic engineering computing
KW  - courteous autonomous cars
KW  - driving quality
KW  - cost function
KW  - purely selfish cost
KW  - interactive drivers
KW  - autonomous car
KW  - courtesy term
KW  - robot car
KW  - human behavior
KW  - courteous robot cars
KW  - human driver behavior
KW  - Autonomous automobiles
KW  - Vehicles
KW  - Cost function
KW  - Planning
KW  - Robot kinematics
KW  - Safety
DO  - 10.1109/IROS.2018.8593969
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Typically, autonomous cars optimize for a combination of safety, efficiency, and driving quality. But as we get better at this optimization, we start seeing behavior go from too conservative to too aggressive. The car's behavior exposes the incentives we provide in its cost function. In this work, we argue for cars that are not optimizing a purely selfish cost, but also try to be courteous to other interactive drivers. We formalize courtesy as a term in the objective that measures the increase in another driver's cost induced by the autonomous car's behavior. Such a courtesy term enables the robot car to be aware of possible irrationality of the human behavior, and plan accordingly. We analyze the effect of courtesy in a variety of scenarios. We find, for example, that courteous robot cars leave more space when merging in front of a human driver. Moreover, we find that such a courtesy term can help explain real human driver behavior on the NGSIM dataset.
ER  - 

TY  - CONF
TI  - Joint Ego-motion Estimation Using a Laser Scanner and a Monocular Camera Through Relative Orientation Estimation and 1-DoF ICP
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 671
EP  - 677
AU  - K. Huang
AU  - C. Stachniss
PY  - 2018
KW  - automobiles
KW  - cameras
KW  - iterative methods
KW  - laser ranging
KW  - mobile robots
KW  - motion estimation
KW  - optical scanners
KW  - pose estimation
KW  - sensor fusion
KW  - SLAM (robots)
KW  - joint ego-motion estimation
KW  - laser scanner
KW  - monocular camera
KW  - autonomous vehicles
KW  - SLAM algorithms
KW  - sensor suite
KW  - laser range finder
KW  - 3D point clouds
KW  - iterative closest point problem
KW  - sensor modality
KW  - orientation estimation
KW  - autonomous cars
KW  - pose estimation
KW  - autonomous robots
KW  - 1-DoF ICP
KW  - data association
KW  - Cameras
KW  - Iterative closest point algorithm
KW  - Lasers
KW  - Three-dimensional displays
KW  - Robot vision systems
KW  - Image color analysis
DO  - 10.1109/IROS.2018.8593965
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Pose estimation and mapping are key capabilities of most autonomous vehicles and thus a number of localization and SLAM algorithms have been developed in the past. Autonomous robots and cars are typically equipped with multiple sensors. Often, the sensor suite includes a camera and a laser range finder. In this paper, we consider the problem of incremental ego-motion estimation, using both, a monocular camera and a laser range finder jointly. We propose a new algorithm, that exploits the advantages of both sensors-the ability of cameras to determine orientations well and the ability of laser range finders to estimate the scale and to directly obtain 3D point clouds. Our approach estimates the 5 degrees of freedom relative orientation from image pairs through feature point correspondences and formulates the remaining scale estimation as a new variant of the iterative closest point problem with only one degree of freedom. We furthermore exploit the camera information in a new way to constrain the data association between laser point clouds. The experiments presented in this paper suggest that our approach is able to accurately estimate the ego-motion of a vehicle and that we obtain more accurate frame-to-frame alignments than with one sensor modality alone.
ER  - 

TY  - CONF
TI  - LandmarkBoost: Efficient visualContext Classifiers for Robust Localization
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 677
EP  - 684
AU  - M. Dymczyk
AU  - I. Gilitschenski
AU  - J. Nieto
AU  - S. Lynen
AU  - B. Zeisl
AU  - R. Siegwart
PY  - 2018
KW  - image capture
KW  - image classification
KW  - image matching
KW  - image retrieval
KW  - nearest neighbour methods
KW  - pose estimation
KW  - search problems
KW  - stereo image processing
KW  - metric pose retrieval algorithms
KW  - image plane
KW  - state-of-the-art descriptor matching methods
KW  - visualContext classifiers
KW  - binary descriptors
KW  - robust localization
KW  - Landmark-Boost
KW  - boosting framework
KW  - contextual information
KW  - landmark observations
KW  - boosted classifier
KW  - landmark classification task
KW  - 2D-3D matching methods
KW  - visual context
KW  - mobile platforms
KW  - nearest neighbor search
KW  - reliable pose retrieval algorithms
KW  - Visualization
KW  - Feature extraction
KW  - Measurement
KW  - Robots
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Context modeling
DO  - 10.1109/IROS.2018.8594100
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - The growing popularity of autonomous systems creates a need for reliable and efficient metric pose retrieval algorithms. Currently used approaches tend to rely on nearest neighbor search of binary descriptors to perform the 2D-3D matching and guarantee realtime capabilities on mobile platforms. These methods struggle, however, with the growing size of the map, changes in viewpoint or appearance, and visual aliasing present in the environment. The rigidly defined descriptor patterns only capture a limited neighborhood of the keypoint and completely ignore the overall visual context. We propose LandmarkBoost - an approach that, in contrast to the conventional 2D-3D matching methods, casts the search problem as a landmark classification task. We use a boosted classifier to classify landmark observations and directly obtain correspondences as classifier scores. We also introduce a formulation of visual context that is flexible, efficient to compute, and can capture relationships in the entire image plane. The original binary descriptors are augmented with contextual information and informative features are selected by the boosting framework. Through detailed experiments, we evaluate the retrieval quality and performance of Landmark-Boost, demonstrating that it outperforms common state-of-the-art descriptor matching methods.
ER  - 

TY  - CONF
TI  - Fire-Aware Planning of Aerial Trajectories and Ignitions
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 685
EP  - 692
AU  - E. Beachly
AU  - C. Detweiler
AU  - S. Elbaum
AU  - B. Duncan
AU  - C. Hildebrandt
AU  - D. Twidwell
AU  - C. Allen
PY  - 2018
KW  - aerospace computing
KW  - aerospace control
KW  - autonomous aerial vehicles
KW  - computer simulation
KW  - helicopters
KW  - ignition
KW  - path planning
KW  - trajectory control
KW  - wildfires
KW  - fire-aware planning
KW  - aerial trajectories
KW  - fire vectors
KW  - fire simulation
KW  - fire-aware planner
KW  - fire simulator predictions
KW  - ignition spheres
KW  - unmanned aerial system for prescribed fires
KW  - helicopter
KW  - UAS-Rx Android application
KW  - Ignition
KW  - Robots
KW  - Computational modeling
KW  - Planning
KW  - Sensors
KW  - Mathematical model
KW  - Trajectory
DO  - 10.1109/IROS.2018.8593568
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Prescribed fires can lessen wildfire severity and control invasive species, but they can also be risky and costly. Unmanned aerial systems can reduce those drawbacks by, for example, dropping ignition spheres to ignite the most hazardous areas. Existing systems, however, lack awareness of the fire vectors to operate autonomously, safely, and efficiently. In this work we address that limitation, introducing an approach that integrates a lightweight fire simulator and a planner for trajectories and ignition sphere drop waypoints. Both components are unique in that they are amenable to input from the system's sensors and the fire crew to increase fire awareness. We conducted a preliminary study that confirms that such inputs improve the accuracy of the fire simulation to counter the unpredictability of the target environment. The field study of the system showed that the fire-aware planner generated safe trajectories with effective ignitions leveraging the fire simulator predictions.
ER  - 

TY  - CONF
TI  - Embedding Temporally Consistent Depth Recovery for Real-time Dense Mapping in Visual-inertial Odometry
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 693
EP  - 698
AU  - H. Cheng
AU  - Z. Zheng
AU  - J. He
AU  - C. Chen
AU  - K. Wang
AU  - L. Lin
PY  - 2018
KW  - distance measurement
KW  - image reconstruction
KW  - interpolation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - real-time dense mapping
KW  - visual-inertial odometry
KW  - dense scene information
KW  - fast self-localization
KW  - VIO-based SLAM systems
KW  - VIO depth estimations
KW  - subspace-based stabilization scheme
KW  - temporal consistency
KW  - edge-preserving depth interpolation
KW  - simultaneous localization and mapping
KW  - learning-based methods
KW  - embedding temporally consistent depth recovery
KW  - Simultaneous localization and mapping
KW  - Real-time systems
KW  - Feature extraction
KW  - Pipelines
KW  - Interpolation
KW  - Three-dimensional displays
DO  - 10.1109/IROS.2018.8593917
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Dense mapping is always the desire of simultaneous localization and mapping (SLAM), especially for the applications that require fast and dense scene information. Visual-inertial odometry (VIO) is a light-weight and effective solution to fast self-localization. However, VIO-based SLAM systems have difficulty in providing dense mapping results due to the spatial sparsity and temporal instability of the VIO depth estimations. Although there have been great efforts on real-time mapping and depth recovery from sparse measurements, the existing solutions for VIO-based SLAM still fail to preserve sufficient geometry details in their results. In this paper, we propose to embed depth recovery into VIO-based SLAM for real-time dense mapping. In the proposed method, we present a subspace-based stabilization scheme to maintain the temporal consistency and design a hierarchical pipeline for edge-preserving depth interpolation to reduce the computational burden. Numerous experiments demonstrate that our method can achieve an accuracy improvement of up to 49.1 cm compared to state-of-the-art learning-based methods for depth recovery and reconstruct sufficient geometric details in dense mapping when only 0.07% depth samples are available. Since a simple CPU implementation of our method already runs at 10-20 fps, we believe our method is very favorable for practical SLAM systems with critical computational requirements.
ER  - 

TY  - CONF
TI  - Fractional-Order Trajectory-Following Control for Two-Legged Dynamic Walking
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 699
EP  - 704
AU  - K. Leyden
AU  - B. Goodwine
PY  - 2018
KW  - control system synthesis
KW  - energy conservation
KW  - legged locomotion
KW  - motion control
KW  - PD control
KW  - position control
KW  - robot dynamics
KW  - simulated walker
KW  - two-legged dynamic walking
KW  - walking robots
KW  - energy consumption
KW  - energy efficiency
KW  - fractional-order trajectory-following control
KW  - proportional-derivative architecture
KW  - Legged locomotion
KW  - Mathematical model
KW  - Gravity
KW  - Aerodynamics
KW  - PD control
KW  - Trajectory
DO  - 10.1109/IROS.2018.8593749
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This research seeks greater efficiency for walking robots. Efficiency can be improved in two ways: better performance (i.e., less wasted motion) and reduced energy consumption. Fractional-order control is a pathway to both of these improvements because of the flexibility it offers in designing a control strategy. Compared to the existing proportional-derivative architecture, changing the order of the derivative - the number of derivatives taken - to real numbers other than 1 has yielded both types of improvement for a simulated walker. The evidence of better performance is the leg angles' improvement in maintaining a desired relationship with respect to one another. Depending on the controller chosen, the walker can also be made to achieve the original level of performance with reduced control signals and less torque delivered to the hip joint, implying greater energy efficiency.
ER  - 

TY  - CONF
TI  - Walking on a Steep Slope Using a Rope by a Life-Size Humanoid Robot
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 705
EP  - 712
AU  - M. Bando
AU  - M. Murooka
AU  - S. Nozawa
AU  - K. Okada
AU  - M. Inaba
PY  - 2018
KW  - friction
KW  - humanoid robots
KW  - least squares approximations
KW  - mobile robots
KW  - motion control
KW  - friction force
KW  - linear least-square problem
KW  - life-size humanoid robot HRP-2
KW  - rope
KW  - steep slope
KW  - Legged locomotion
KW  - Friction
KW  - Robot kinematics
KW  - Humanoid robots
KW  - Force
KW  - Foot
DO  - 10.1109/IROS.2018.8594292
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we propose methods for walking on a steep slope using a rope by a humanoid robot. There are two difficulties for walking on a steep slope without a rope. First, range of motion of ankle joints get limited. Second, feet of a robot slip on a steep slope. For these problems, using a rope is effective solution because the robot can receive enough friction force from the slope and walk on a steep slope by pulling a rope with proper tension. In addition, the robot pulling a rope on a slope can relax limitations of ankle joints. Therefore, we propose methods to determine tension of a grasped rope by solving a linear least-square problem considering deformability of a rope. With these methods, a life-size humanoid robot HRP-2 could walk on a steep slope which angle is 40 degree.
ER  - 

TY  - CONF
TI  - Perception Based Locomotion System for a Humanoid Robot with Adaptive Footstep Compensation under Task Constraints
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 713
EP  - 719
AU  - I. Kumagai
AU  - M. Morisawa
AU  - S. Nakaoka
AU  - T. Sakaguchi
AU  - H. Kaminaga
AU  - K. Kaneko
AU  - F. Kanehiro
PY  - 2018
KW  - adaptive control
KW  - collision avoidance
KW  - humanoid robots
KW  - interpolation
KW  - legged locomotion
KW  - path planning
KW  - task constraints
KW  - humanoid robot
KW  - adaptive footstep compensation
KW  - adaptive locomotion system
KW  - local error correction
KW  - perception based locomotion system
KW  - locomotion error
KW  - locomotion planning
KW  - point cloud
KW  - environmental measurements
KW  - plane estimation
KW  - space interpolation
KW  - collision avoidance
KW  - laser scans
KW  - Humanoid robots
KW  - Task analysis
KW  - Planning
KW  - Foot
KW  - Three-dimensional displays
KW  - Estimation
DO  - 10.1109/IROS.2018.8593553
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In order to accurately reach a target position while executing a task which imposes occlusion or constraints of the posture, a humanoid robot requires an adaptive locomotion system, which can comprehensively integrate localization, environmental mapping, global locomotion planning and local error correction. In this paper, we propose a method of constructing a perception based locomotion system for a humanoid robot. The major contribution of this paper is solving a problem of the locomotion error caused by the task constraints, by locally compensating footsteps and assessing the need for global footstep re-planning online based on environmental measurements. The proposed system provides an accurate and dense ground point cloud, called HeightField, using plane estimation and space interpolation, and obstacle point cloud for frequent collision avoidance by accumulating laser scans. This environmental perception enables a humanoid robot to plan footsteps globally even in the situation where the sight of the robot is limited and compensate footsteps while estimating landing state during locomotion online with the localization result. We evaluated the practicality of the proposed system by applying it to our humanoid robot carrying a heavy object in a construction site and confirmed that the proposed system contributed to improved locomotion abilities of a humanoid robot engaging in heavy-duty or dangerous tasks.
ER  - 

TY  - CONF
TI  - Adaptive step rotation in biped walking
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 720
EP  - 725
AU  - N. Bohórquez
AU  - P. Wieber
PY  - 2018
KW  - legged locomotion
KW  - predictive control
KW  - quadratic programming
KW  - adaptive step rotation
KW  - biped walking
KW  - fixed feet rotation
KW  - nonlinear solvers
KW  - safe linear constraints
KW  - model predictive control schemes
KW  - robot walking
KW  - sequential quadratic program
KW  - Legged locomotion
KW  - Foot
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Dynamics
KW  - Predictive control
DO  - 10.1109/IROS.2018.8594431
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - We want to enable the robot to reorient its feet in order to face its direction of motion. Model Predictive Control schemes for biped walking usually assume fixed feet rotation since adapting them online leads to a nonlinear problem. Nonlinear solvers do not guarantee the satisfaction of nonlinear constraints at every iterate and this can be problematic for the real-time operation of robots. We propose to define safe linear constraints that are always inside the intersection of the nonlinear constraints. We make simulations of the robot walking on a crowd and compare the performance of the proposed method with respect to the original nonlinear problem solved as a Sequential Quadratic Program.
ER  - 

TY  - CONF
TI  - Implementing Full-body Torque Control in Humanoid Robot with High Gear Ratio Using Pulse Width Modulation Voltage
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 726
EP  - 732
AU  - K. Lee
AU  - O. Sim
AU  - H. Jeong
AU  - J. Oh
AU  - H. Bae
AU  - S. Hong
AU  - J. Oh
PY  - 2018
KW  - electric current control
KW  - gears
KW  - humanoid robots
KW  - legged locomotion
KW  - mobile robots
KW  - motion control
KW  - position control
KW  - robot dynamics
KW  - torque control
KW  - voltage control
KW  - full-body torque control
KW  - high gear ratio
KW  - pulse width modulation voltage
KW  - motor torque
KW  - current control
KW  - joint torque control
KW  - robot dynamics
KW  - humanoid robot
KW  - position control
KW  - legged robots
KW  - Torque
KW  - Robots
KW  - Resistance
KW  - Aerodynamics
KW  - Temperature sensors
KW  - Modeling
KW  - Torque control
DO  - 10.1109/IROS.2018.8593908
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - Most state-of-the-art torque control-based legged robots show excellent performance, exceeding that of conventional position control-based robots. Many conventional position control-based legged robots have high gear ratios, but do not have joint torque sensors. In addition, some robots cannot generate current for controlling the motor torque. To apply torque control-based walking algorithms to a position control-based humanoid robot, we proposed current control using a motor thermal model and realized joint torque control by compensating for the joint dynamics and robot dynamics. We conducted experiments to verify the performance of the Hubo2 platform developed in 2008 by applying a full-body dynamics control framework. The results confirmed the possibility of using torque control algorithms with existing position-based robots.
ER  - 

TY  - CONF
TI  - Towards Minimal Intervention Control with Competing Constraints
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 733
EP  - 738
AU  - Y. Huang
AU  - J. Silvério
AU  - D. G. Caldwell
PY  - 2018
KW  - control engineering computing
KW  - control system synthesis
KW  - learning (artificial intelligence)
KW  - linear quadratic control
KW  - optimal control
KW  - robot programming
KW  - trajectory control
KW  - task execution
KW  - trajectory constraints
KW  - information-theory
KW  - finite horizon linear quadratic regulator
KW  - Cartesian space
KW  - pure trajectory generation
KW  - imitation learning algorithms
KW  - simulated robot
KW  - robot null space
KW  - optimal control
KW  - minimal intervention control strategy
KW  - Aerospace electronics
KW  - Trajectory
KW  - Null space
KW  - Task analysis
KW  - Probabilistic logic
KW  - End effectors
DO  - 10.1109/IROS.2018.8594235
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - As many imitation learning algorithms focus on pure trajectory generation in either Cartesian space or joint space, the problem of considering competing trajectory constraints from both spaces still presents several challenges. In particular, when perturbations are applied to the robot, the underlying controller should take into account the importance of each space for the task execution, and compute the control effort accordingly. However, no such controller formulation exists. In this paper, we provide a minimal intervention control strategy that simultaneously addresses the problems of optimal control and competing constraints between Cartesian and joint spaces. In light of the inconsistency between Cartesian and joint constraints, we exploit the robot null space from an information-theory perspective so as to reduce the corresponding conflict. An optimal solution to the aforementioned controller is derived and furthermore a connection to the classical finite horizon linear quadratic regulator (LQR) is provided. Finally, a writing task in a simulated robot verifies the effectiveness of our approach.
ER  - 

TY  - CONF
TI  - Design and Evaluation of Torque Based Bipedal Walking Control System That Prevent Fall Over by Impulsive Disturbance
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 739
EP  - 746
AU  - T. Shirai
AU  - Y. Nagamatsu
AU  - H. Suzuki
AU  - S. Nozawa
AU  - K. Okada
AU  - M. Inaba
PY  - 2018
KW  - legged locomotion
KW  - motion control
KW  - torque control
KW  - torque based bipedal walking control system
KW  - impulsive disturbance
KW  - bipedal robot control system
KW  - robust online walking controller
KW  - leg sweep disturbance
KW  - distributed system
KW  - sensorless whole body torque control method
KW  - Robot sensing systems
KW  - Torque
KW  - Legged locomotion
KW  - Actuators
KW  - Humanoid robots
DO  - 10.1109/IROS.2018.8594334
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - In this paper, we develop a bipedal robot control system that has an ability to perform instantaneous high power and flexibility to absorb an impulsive disturbance. We utilize a sensor-less whole body torque control method executed in a high responsive realtime distributed system. This system also includes a robust online walking controller that can avoid fall over caused by a strong collision with the robot's legs. We evaluated the proposed control system by hitting a rubber ball or adding a leg sweep disturbance and verified the functionality of the absorbing motion and the balance restoring motion.
ER  - 

TY  - CONF
TI  - Humanoid Robot COM Kinematics Estimation based on Compliant Inverted Pendulum Model and Robust State Estimator
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 747
EP  - 753
AU  - H. Bae
AU  - H. Jeong
AU  - J. Oh
AU  - K. Lee
AU  - J. Oh
PY  - 2018
KW  - elastic constants
KW  - estimation theory
KW  - humanoid robots
KW  - mobile robots
KW  - pendulums
KW  - robot kinematics
KW  - state estimation
KW  - mechanical deformation
KW  - damper
KW  - limited structural stiffness
KW  - humanoid robot COM kinematics estimation
KW  - center of mass
KW  - sing-mass model
KW  - robust state estimator
KW  - compliant inverted pendulum model
KW  - Humanoid robots
KW  - Mathematical model
KW  - State estimation
KW  - Kinematics
KW  - Computational modeling
DO  - 10.1109/IROS.2018.8593966
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This work proposes a humanoid robot center of mass (COM) estimation framework based on the compliant inverted pendulum model and the robust estimator. Humanoids' limited structural stiffness and relatively long legs result in undesired flexibility, and this undesired motion hinders the state estimation. The models used in previous studies were either not suitable for estimation or too simple to express these key characteristics of humanoid robots. Here, to enhance the estimation performance, the compliant inverted pendulum model, which is developed by attaching a spring and damper to the original pendulum, is adopted. The additional elements can represent the mechanical deformation and undesired flexibility. This model can reflect the important characteristics of the humanoid robot while taking advantage of the merits of the sing-mass model. In addition, a robust state estimator that was proposed in our previous work is adopted to compensate for an estimation error caused by a modeling error. Using these two factors, an improved COM kinematics estimates could be obtained.
ER  - 

TY  - CONF
TI  - Robotic Sewing and Knot Tying for Personalized Stent Graft Manufacturing
T2  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
SP  - 754
EP  - 760
AU  - Y. Hu
AU  - L. Zhang
AU  - W. Li
AU  - G. Yang
PY  - 2018
KW  - closed loop systems
KW  - medical robotics
KW  - robot vision
KW  - servomechanisms
KW  - stents
KW  - surgery
KW  - textile technology
KW  - visual servoing
KW  - robotic system
KW  - stitch size planning
KW  - 3D industrial sewing
KW  - successive knot
KW  - tension control
KW  - thread management
KW  - thread manipulator
KW  - stitch sizes
KW  - sewing accuracy
KW  - automated knot tying
KW  - closed-loop visual servoing control
KW  - customized robotic sewing device
KW  - 3D structured object
KW  - personalized stent graft manufacturing
KW  - size 2.0 mm to 5.0 mm
KW  - Needles
KW  - Robot kinematics
KW  - Cameras
KW  - Yarn
KW  - Robot vision systems
DO  - 10.1109/IROS.2018.8594021
JO  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
IS  - 
SN  - 2153-0866
VO  - 
VL  - 
JA  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Y1  - 1-5 Oct. 2018
AB  - This paper presents a versatile robotic system for sewing a 3D structured object. Leveraging on using a customized robotic sewing device and closed-loop visual servoing control, an all-in-one solution for sewing personalized stent graft is demonstrated. Stitch size planning and automated knot tying are proposed as two key functions of the system. By using effective stitch size planning, sub-millimetre sewing accuracy is achieved for stitch sizes ranging from 2mm to 5mm. In addition, a thread manipulator for thread management and tension control is also proposed to perform successive knot tying to secure each stitch. Detailed laboratory experiments have been performed to evaluate the proposed instruments and allied algorithms. The proposed framework can be generalised to a wide range of applications including 3D industrial sewing, as well as transferred to other clinical areas such as surgical suturing.
ER  - 


