total paper: 95
Title: Deep Neural Networks for Multiple Speaker Detection and Localization
Key Words: acoustic generators  encoding  human-robot interaction  microphone arrays  neural nets  speaker recognition  deep neural networks  multiple speaker detection  simultaneous detection  multiple sound sources  human-robot interaction  neural network-based sound source localization methods  single sound source  likelihood-based encoding  network output  sound mixtures  spatial spectrum-based approaches  Encoding  Delays  Robots  Artificial neural networks  Microphones  Estimation 
Abstract: We propose to use neural networks for simultaneous detection and localization of multiple sound sources in human-robot interaction. In contrast to conventional signal processing techniques, neural network-based sound source localization methods require fewer strong assumptions about the environment. Previous neural network-based methods have been focusing on localizing a single sound source, which do not extend to multiple sources in terms of detection and localization. In this paper, we thus propose a likelihood-based encoding of the network output, which naturally allows the detection of an arbitrary number of sources. In addition, we investigate the use of sub-band cross-correlation information as features for better localization in sound mixtures, as well as three different network architectures based on different motivations. Experiments on real data recorded from a robot show that our proposed methods significantly outperform the popular spatial spectrum-based approaches.


Title: iMag: Accurate and Rapidly Deployable Inertial Magneto-Inductive Localisation
Key Words: Global Positioning System  sensor placement  SLAM (robots)  wireless sensor networks  inertial magneto-inductive localisation  short-term construction work  iMag  robust simultaneous localisation  inertial measurement units  Transmitters  Robustness  Simultaneous localization and mapping  Magnetic resonance imaging  Distortion  Trajectory  Magneto-inductive device  Inertial measurements  Localisation  SLAM 
Abstract: Localisation is of importance for many applications. Our motivating scenarios are short-term construction work and emergency rescue. Not only is accuracy necessary, these scenarios also require rapid setup and robustness to environmental conditions. These requirements preclude the use of many traditional methods e.g. vision-based, laser-based, Ultra-wide band (UWB) and Global Positioning System (GPS)-based localisation systems. To solve these challenges, we introduce iMag, an accurate and rapidly deployable inertial magneto-inductive (MI) localisation system. It localises monitored workers using a single MI transmitter and inertial measurement units with minimal setup effort. However, MI location estimates can be distorted and ambiguous. To solve this problem, we suggest a novel method to use MI devices for sensing environmental distortions, and use these to correctly close inertial loops. By applying robust simultaneous localisation and mapping (SLAM), our proposed localisation method achieves excellent tracking accuracy, and can improve performance significantly compared with only using an inertial measurement unit (IMU) and MI device for localisation.


Title: Parallel Pick and Place Using Two Independent Untethered Mobile Magnetic Microgrippers
Key Words: end effectors  freight handling  grippers  industrial robots  micromanipulators  microrobots  mobile robots  position control  independent untethered mobile magnetic microgrippers  parallel targeted cargo delivery  two-microgripper pair  local magnetic interactions  global magnetic field  end effectors  parallel pick and place  3D microgrippers configuration  Grippers  Magnetic separation  Barium  Magnetic hysteresis  Magnetoelasticity  Micromagnetics  Task analysis  magnetic microgripper  multi-agent control at microscales  soft robotics  targeted cargo delivery 
Abstract: Untethered mobile microgrippers exhibit flexibility and agility in small and constrained environments as precise and accurate robotic end-effectors, with promising potential applications in cell manipulation and microassembly. Here, we propose the first scheme to independently and simultaneously position two microgrippers on a horizontal plane for parallel targeted cargo delivery using a single global input. The separation and orientation of the two-microgripper pair are modulated by the local magnetic interactions between the two microgrippers, which are governed by a global magnetic field. The microgripper action of grasping or releasing cargoes is fully controlled by the global magnetic field without requiring additional thermal, chemical, or other stimuli. Thus, the proposed strategy only requires a single input, i.e., a global magnetic field, to control two microgrippers and therefore is simple to implement and fast-acting. As a demonstration, two microgrippers are maneuvered by a global magnetic field to pick up two cargoes and deliver them to their respective destinations. The parallel operation of two microgrippers can potentially double the overall throughput and enable the tasks that require team cooperations. The two 3D microgrippers configuration is intuitive in teleoperations, since it imitates the two-hand case of human beings.


Title: Comparison Study of Nonlinear Optimization of Step Durations and Foot Placement for Dynamic Walking
Key Words: humanoid robots  legged locomotion  nonlinear control systems  optimisation  pendulums  robot dynamics  dynamic walking  nonlinear optimization problem  continuous dynamics  discrete dynamics  remaining step duration  foot location  motion model captures  mass dynamics  low-dimensionality  holistic approach  three-dimensional parametric space  computational efficiency  sequential approach  customized optimization  current step duration  optimal solutions  bipedal locomotion  Optimization  Foot  Legged locomotion  Robustness  Lips  Dynamics  Computational modeling 
Abstract: This paper studies bipedal locomotion as a nonlinear optimization problem based on continuous and discrete dynamics, by simultaneously optimizing the remaining step duration, the next step duration and the foot location to achieve robustness. The linear inverted pendulum as the motion model captures the center of mass dynamics and its low-dimensionality makes the problem more tractable. We first formulate a holistic approach to search for optimality in the three-dimensional parametric space and use these results as baseline. To further improve computational efficiency, our study investigates a sequential approach with two stages of customized optimization that first optimizes the current step duration, and subsequently the duration and location of the next step. The effectiveness of both approaches is successfully demonstrated in simulation by applying different perturbations. The comparison study shows that these two approaches find mostly the same optimal solutions, but the latter requires considerably less computational time, which suggests that the proposed sequential approach is well suited for real-time implementation with a minor trade-off in optimality.


Title: Workspace Fixation for Free-Floating Space Robot Operations
Key Words: aerospace robotics  end effectors  mobile robots  CoM  degree-of-freedom  free-floating space robot operations  workspace fixation  6DOF moving base  center-of-mass regulation  end-effector  Manipulators  Symmetric matrices  Fuels  Robot kinematics  Satellites 
Abstract: When a space robot accidentally or voluntarily comes in contact with a target object, a workspace shift happens due to exchange of momentum between the objects. The problem of workspace adjustment is addressed herein. A novel controller is derived to simultaneously adjust the workspace and control the end-effector pose. The controller is based on a center-of-mass (CoM) regulation which fixes the workspace in the inertial space while leaving the base free to move, resulting in fuel efficiency. The control is validated on hardware using a robotic simulator composed of a seven degree-of-freedom (DOF) arm mounted on a 6DOF moving base.


Title: Robust Visual Localization for Hopping Rovers on Small Bodies
Key Words: cameras  planetary rovers  pose estimation  robot vision  SLAM (robots)  space vehicles  visual SLAM algorithms  ORB-SLAM2  orbiting primary spacecraft  onboard visual simultaneous localization and mapping  visual SLAM implementation  wide field of view camera  off-nadir camera pointing angles  narrow FOV camera  orbiting spacecraft  visual appearance  high-contrast shadows  hopping rover  illumination angles  Solar System bodies  collaborative visual localization method  robust visual localization  time 1.0 hour to 12.0 hour  Cameras  Space vehicles  Visualization  Simultaneous localization and mapping  Optimization  Lighting  Solar system 
Abstract: We present a collaborative visual localization method for rovers designed to hop and tumble across the surface of small Solar System bodies, such as comets and asteroids. In a two-phase approach, an orbiting primary spacecraft first maps the surface of a body by capturing images from various poses and illumination angles; these images are processed to create a prior map of 3D landmarks. In the second phase, a hopping rover is deployed to the surface where it uses a camera to relocalize to the prior map and to perform onboard visual simultaneous localization and mapping (SLAM). Small bodies present several unique challenges to existing visual SLAM algorithms, such as high-contrast shadows that move quickly over the surface due to the short (e.g. 1-12 hour) rotational periods, and large changes in visual appearance between orbit and the surface, where image scale varies by many orders of magnitude (kilometers to centimeters). In this work, we describe how to augment ORB-SLAM2-a state of the art visual SLAM implementation-to handle large variations in illumination by fusing prior images with varying illumination angles. We demonstrate how a hopping rover can use a wide field of view (FOV) camera to relocalize to prior maps captured by an orbiting spacecraft with a narrow FOV camera, and how the growth of pose and scale errors can be bounded by periodic loop closures during large hops. The proposed method is evaluated with sequences of images captured around a mock asteroid; it is shown to be robust to varying illumination angles, scene scale changes, and off-nadir camera pointing angles.


Title: Bounding Drift in Cooperative Localisation Through the Sharing of Local Loop Closures
Key Words: graph theory  mobile robots  robot vision  SLAM (robots)  direct intervehicle observations  fuses single vehicle SLAM  cooperative localisation  data association  map data  local subgraphs  shared states  localisation accuracy  bounding drift  local loop closures  robotic scenarios  data consistency  bandwidth limitations  single vehicle visual SLAM framework  Information matrix  Simultaneous localization and mapping  Bandwidth  Jacobian matrices  Visualization  Message systems 
Abstract: Handling loop closures and intervehicle observations in cooperative robotic scenarios remains a challenging problem due to data consistency, bandwidth limitations and increased computation requirements. This paper develops a general cooperative localisation and single vehicle Visual SLAM framework that includes direct intervehicle observations and pose to pose loop closures on each vehicle with states shared as required. This fuses single vehicle SLAM with cooperative localisation and avoids data association of map data across limited communication networks. The base problem is developed as a factor graph with each vehicle solving local subgraphs that are split based on intervehicle observations. We modify the order of variable elimination in subgraphs through manipulation of the square-root of the Information matrix to extract updates that include the historic states involved in the loop closures and do not require transmission of other states not involved in the measurement or retransmission of previously shared states. We demonstrate the effect on localisation accuracy and bandwidth using data captured from a set of five robots observing each other and landmarks compared to both single vehicle SLAM, pure cooperative localisation and a centralised solution.


Title: Detection and Resolution of Motion Conflict in Visual Inertial Odometry
Key Words: distance measurement  inertial navigation  motion estimation  motion conflict detection  motion estimation  motion conflict resolution  visual-inertial odometry  Motion Conflict aware Visual Inertial Odometry  Visualization  Cameras  Estimation  Robustness  Simultaneous localization and mapping  Hidden Markov models 
Abstract: In this paper, we present a novel method to detect and resolve motion conflicts in visual-inertial odometry. Recently, it has been common to integrate an IMU sensor with visual odometry in order to improve localization accuracy and robustness. However, when a disagreement between the two sensor modalities occurs, the localization accuracy reduces drastically and leads to irreversible errors. In such conditions, multiple motion estimates based on the set of observations used are possible. This creates a conflict (motion conflict) in determining which observations to use for accurate ego-motion estimation. Therefore, we present a method to detect motion conflicts based on per-frame positional estimate discrepancy and per-landmark reprojection errors. Additionally, we also present a method to resolve motion conflicts by eliminating inconsistent IMU and landmark measurements. Finally, we implement Motion Conflict aware Visual Inertial Odometry (MC-VIO) by combining both detection and resolution of motion conflicts. We perform quantitative and qualitative evaluation of MC-VIO on visually and inertially challenging datasets. Experimental results indicate that the MC-VIO algorithm reduces the increase in absolute trajectory error by 80% and the relative pose error by 60% for scenes with motion conflict, in comparison to the state-of-the-art reference VIO algorithm.


Title: Multi-Robot Realization Based on Goal Adjacency Constraints
Key Words: closed loop systems  matrix algebra  mobile robots  multi-robot systems  navigation  path planning  position control  velocity control  pairwise distances  velocity-controlled robot  multirobot realization  goal adjacency constraints  robot positions  exact goal positions  relative distances  pairwise adjacency constraints  multirobots  adjacency matrix  adjacency threshold  robot pairs  coordinated navigation  closed-loop dynamics  Conferences  Automation  Australia 
Abstract: This paper considers the problem of multi-robot realization. A realization is a set of robot positions where pairwise distances are either bounded from above or from below by a given adjacency threshold - depending on whether the respective robot pairs are to be adjacent or not. In the realization problem, unlike the related coordinated navigation or formation control problems, exact goal positions or relative distances need not be specified. Rather, only pairwise adjacency constraints are given and the robots' positions are required to satisfy these constraints. Applications of realization problem include multi-robots involved in team games (playing soccer, etc), patrolling and area coverage. We present a novel solution to this problem in which the robots simultaneously navigate to find a realization of a given adjacency matrix without colliding with each other along the way. In this solution, complete information about pairwise distances and free configuration space are encoded using an artificial potential function over the cross product space of the robots' simultaneous positions and proximity variables. The closed-loop dynamics governing the motion of each velocity-controlled robot take the form of the appropriate projection of the gradient of this function while pairwise distances are adjusted accordingly. Our extensive simulations demonstrate that the proposed approach has considerably higher realization percentage and shorter movement distances in comparison to a standard 2-stage approach.


Title: Time-Contrastive Networks: Self-Supervised Learning from Video
Key Words: image representation  learning (artificial intelligence)  pose estimation  robot programming  robot vision  video signal processing  time-contrastive networks  robotic behaviors  robotic imitation settings  human poses  viewpoint-invariant representation  end-effectors  reinforcement learning algorithm  self-supervised learning  robotic systems  Robots  Task analysis  Visualization  Learning (artificial intelligence)  Training  Liquids  Lighting 
Abstract: We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.


Title: Relocalization, Global Optimization and Map Merging for Monocular Visual-Inertial SLAM
Key Words: cameras  distance measurement  graph theory  inertial navigation  mobile robots  optimisation  path planning  pose estimation  robot vision  SLAM (robots)  relocalization  global optimization  monocular visual-inertial SLAM  visual-inertial system  low-cost inertial measurement unit  state estimation  visual-inertial odometry  absolute pose estimation  visual-inertial SLAM system  global pose graph optimization  map merging ability  map reuse  pose graph optimization  Cameras  Optimization  Visualization  Feature extraction  Microsoft Windows  Simultaneous localization and mapping  Real-time systems 
Abstract: The monocular visual-inertial system (VINS), which consists one camera and one low-cost inertial measurement unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate visual-inertial odometry is prone to drift and cannot provide absolute pose estimation. Leveraging history information to relocalize and correct drift has become a hot topic. In this paper, we propose a monocular visual-inertial SLAM system, which can relocalize camera and get the absolute pose in a previous-built map. Then 4-DOF pose graph optimization is performed to correct drifts and achieve global consistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual drifted direction in the visual-inertial system. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. Current map and previous map can be merged together by the global pose graph optimization. We validate the accuracy of our system on public datasets and compare against other state-of-the-art algorithms. We also evaluate the map merging ability of our system in the large-scale outdoor environment. The source code of map reuse is integrated into our public code, VINS-Monol11https://github.com/HKUST-Aerial-Robotics/VINS-Mono.


Title: Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM
Key Words: image reconstruction  mobile robots  optical radar  optimisation  probability  radar imaging  robot vision  sensor fusion  SLAM (robots)  dense map-centric continuous-time SLAM  CT-SLAM  computational complexity  surfel fusion  global batch trajectory optimization  probabilistic surface element fusion  map deformation  global trajectory optimization  Continuous-Time SLAM  global batch optimization  multimodal sensor fusion  continuous-time trajectory representation  elastic LiDAR fusion  Laser radar  Trajectory optimization  Simultaneous localization and mapping  Strain  Interpolation 
Abstract: The concept of continuous-time trajectory representation has brought increased accuracy and efficiency to multi-modal sensor fusion in modern SLAM. However, regardless of these advantages, its offline property caused by the requirement of global batch optimization is critically hindering its relevance for real-time and life-long applications. In this paper, we present a dense map-centric SLAM method based on a continuous-time trajectory to cope with this problem. The proposed system locally functions in a similar fashion to conventional Continuous-Time SLAM (CT-SLAM). However, it removes the need for global trajectory optimization by introducing map deformation. The computational complexity of the proposed approach for loop closure does not depend on the operation time, but only on the size of the space it explored before the loop closure. It is therefore more suitable for long term operation compared to the conventional CT-SLAM. Furthermore, the proposed method reduces uncertainty in the reconstructed dense map by using probabilistic surface element (surfel) fusion. We demonstrate that the proposed method produces globally consistent maps without global batch trajectory optimization, and effectively reduces LiDAR noise by surfel fusion.


Title: Reliably Arranging Objects in Uncertain Domains
Key Words: learning (artificial intelligence)  manipulators  mobile robots  multi-robot systems  path planning  control uncertainty  conformant planning approach  robot manipulation  multiple planar objects  specified arrangement  external sensing  belief-state planning problem  initial belief state  forward belief-state planning  deterministic belief-state transition model  off-line physics simulations  on-line physics-based manipulation approach  physical robot experiments  uncertain domains  Planning  Robot sensing systems  Task analysis  Reliability  Computational modeling  Uncertainty 
Abstract: A crucial challenge in robotics is achieving reliable results in spite of sensing and control uncertainty. In this work, we explore the conformant planning approach to robot manipulation. In particular, we tackle the problem of pushing multiple planar objects simultaneously to achieve a specified arrangement without external sensing. Conformant planning is a belief-state planning problem. A belief state is the set of all possible states of the world, and the goal is to find a sequence of actions that will bring an initial belief state to a goal belief state. To do forward belief-state planning, we created a deterministic belief-state transition model from supervised learning based on off-line physics simulations. We compare our method with an on-line physics-based manipulation approach and show significantly reduced planning times and increased robustness in simulated experiments. Finally, we demonstrate the success of this approach in simulations and physical robot experiments.


Title: Backprop-MPDM: Faster Risk-Aware Policy Evaluation Through Efficient Gradient Optimization
Key Words: decision making  gradient methods  learning (artificial intelligence)  Markov processes  optimisation  stochastic processes  multipolicy decision-making  gradient optimization  risk-aware policy evaluation  backprop-MPDM policy  robot platform  easily-differentiable heuristic function  random sampling  stochastic gradient optimization algorithms  decision making process  risk-aware formulations  Robots  Computational modeling  Trajectory  Navigation  Decision making  Cost function 
Abstract: In Multi-Policy Decision-Making (MPDM), many computationally-expensive forward simulations are performed in order to predict the performance of a set of candidate policies. In risk-aware formulations of MPDM, only the worst outcomes affect the decision making process, and efficiently finding these influential outcomes becomes the core challenge. Recently, stochastic gradient optimization algorithms, using a heuristic function, were shown to be significantly superior to random sampling. In this paper, we show that accurate gradients can be computed - even through a complex forward simulation - using approaches similar to those in deep networks. We show that our proposed approach finds influential outcomes more reliably, and is faster than earlier methods, allowing us to evaluate more policies while simultaneously eliminating the need to design an easily-differentiable heuristic function. We demonstrate significant performance improvements in simulation as well as on a real robot platform navigating a highly dynamic environment.


Title: Simultaneous Optimization of ZMP and Footsteps Based on the Analytical Solution of Divergent Component of Motion
Key Words: legged locomotion  linear systems  motion control  nonlinear control systems  path planning  pendulums  quadratic programming  DCM  ZMP  foot placements  bipedal research  quadratic programming problem  optimization  divergent component of motion  Real-time planning  linear inverted pendulum  zero moment point  push recovery experiment  disturbance compensation  Planning  Lips  Trajectory  Foot  Real-time systems  Optimization  Laplace equations 
Abstract: Real-time planning of footsteps has been a big challenge for bipedal research. A large number of methods have been proposed over the last several years. In addition, divergent component of motion (DCM) of linear inverted pendulum (LIP) has been applied to solve this problem thus attracting a great deal of attention. In this paper, we derive an analytical solution of DCM for an arbitrary input function and propose a novel quadratic programming (QP) problem for the simultaneous optimization of zero moment point (ZMP) and foot placements based on the analytical solution. To validate the method, we conducted a push recovery experiment on real hardware. The result of the experiment shows that our new algorithm realizes a hierarchical strategy for disturbance compensation.


Title: Grasp-training Robot to Activate Neural Control Loop for Reflex and Experimental Verification
Key Words: electromyography  handicapped aids  medical robotics  neurocontrollers  neurophysiology  patient rehabilitation  grasp rehabilitation  robot design  grasping movements  mechanical motions  reflex response  motion intention  rehabilitation robot  activate neural control loop  grasp-training robot  paralyzed hand  grasp reflex  elastic bar  Robots  Grasping  Shafts  Bars  Force  Electromyography  Rubber 
Abstract: Using a rehabilitation robot to activate motion intention and reflex response simultaneously is an effective approach to aiding recovery from paralysis caused by neurological disorders. Mechanical motions supported by conventional robots are, however, not enough to activate reflex. In this paper, we propose a grasp-training robot that can stimulate the grasp reflex of a paralyzed hand by pushing the hand onto an elastic bar while supporting the grasping movements. In addition to this feature, we discuss the robot design in relation to its usability and wearability for ease of use in clinical practice. Experimental results obtained from healthy subjects show that the proposed robot can support grasping in a way similar to the traditional range-of-motion exercise used by therapists for grasp rehabilitation. Combining this appropriate grasping-motion support and the mechanism for pushing the hand onto an elastic bar succeeds in activating the grasp reflex of a completely paralyzed patient in a clinical test that involves monitoring electromyography signals from the paralyzed hand.


Title: Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping
Key Words: image fusion  image reconstruction  image segmentation  learning (artificial intelligence)  multiview frame fusion technique  perceived 3D structure  semantic labelling task  human interaction  verbal references  location related services  multiview 3D entangled forest  semantic maps  offline reconstruction  single frames  online multiview semantic segmentation  batch approach  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Forestry  Labeling  Context modeling  Standards 
Abstract: Applications that provide location related services need to understand the environment in which humans live such that verbal references and human interaction are possible. We formulate this semantic labelling task as the problem of learning the semantic labels from the perceived 3D structure. In this contribution we propose a batch approach and a novel multi-view frame fusion technique to exploit multiple views for improving the semantic labelling results. The batch approach works offline and is the direct application of an existing single-view method to scene reconstructions with multiple views. The multi-view frame fusion works in an incremental fashion accumulating the single-view results, hence allowing the online multi-view semantic segmentation of single frames and the offline reconstruction of semantic maps. Our experiments show the superiority of the approaches based on our fusion scheme, which leads to a more accurate semantic labelling.


Title: A Hierarchical Model for Action Recognition Based on Body Parts
Key Words: gesture recognition  image motion analysis  image representation  object detection  vectors  hierarchical model  body parts  human action recognition  human actions  human skeleton  discriminative body-parts selection  Fisher vectors  hierarchical representations  hierarchical RRV descriptors  Rotation and Relative Velocity descriptors  Skeleton  Feature extraction  Three-dimensional displays  Task analysis  Trajectory  Couplings  Biological system modeling 
Abstract: As increasing attention is paid on human action recognition from skeleton data, this paper focuses on such tasks by proposing a hierarchical model to discover the structure information of body-parts involved in human actions. Considering human actions as simultaneous motions of different body-parts of the human skeleton, we propose a hierarchical model to simultaneously apply discriminative body-parts selection at a same scale and group coupling of bundles of body-parts at different scales, while we decompose the human skeleton into a hierarchy of body-parts of varying scales. To represent such hierarchy of body-parts, we accordingly build a hierarchical RRV (Rotation and Relative Velocity) descriptors. The hierarchical representations encoded by Fisher vectors of the hierarchical RRV descriptors are properly formulated into the hierarchical model via the proposed hierarchical mixed norm, to apply sparse selection of body-parts and regularize the structure of such hierarchy of body-parts. The extensive evaluations on three challenging datasets demonstrate the effectiveness of our proposed approach, which achieves superior performance compared to state-of-the-art results on different sizes of datasets, showing it is more widely applicable than existing approaches.


Title: End-to-End Race Driving with Deep Reinforcement Learning
Key Words: automobiles  cameras  computer games  image colour analysis  learning (artificial intelligence)  traffic engineering computing  deep reinforcement learning  mediated perception  object recognition  scene understanding  learning strategies  RGB image  forward facing camera  car control  road structures  end-to-end race driving  reinforcement learning algorithm  legal speed limits  asynchronous actor critic framework  rally game  temperature 3.0 C  Automobiles  Training  Brakes  Games  Roads  Physics  Computer architecture 
Abstract: We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.


Title: Encoderless Gimbal Calibration of Dynamic Multi-Camera Clusters
Key Words: angular measurement  calibration  cameras  encoderless gimbal calibration  dynamic multiCamera Clusters  Dynamic Camera Clusters  multicamera systems  cameras  joint angle measurements  time-varying transformation  static camera  motor encoders  transformation chain  encoderless gimbal mechanism  online estimation  Cameras  Calibration  Robot vision systems  Estimation  Reluctance motors  Kinematics  Vehicle dynamics 
Abstract: Dynamic Camera Clusters (DCCs) are multi-camera systems where one or more cameras are mounted on actuated mechanisms such as a gimbal. Existing methods for DCC calibration rely on joint angle measurements to resolve the time-varying transformation between the dynamic and static camera. This information is usually provided by motor encoders, however, joint angle measurements are not always readily available on off-the-shelf mechanisms. In this paper, we present an encoderless approach for DCC calibration which simultaneously estimates the kinematic parameters of the transformation chain as well as the unknown joint angles. We also demonstrate the integration of an encoderless gimbal mechanism with a state-of-the art VIO algorithm, and show the extensions required in order to perform simultaneous online estimation of the joint angles and vehicle localization state. The proposed calibration approach is validated both in simulation and on a physical DCC composed of a 2-DOF gimbal mounted on a UAV. Finally, we show the experimental results of the calibrated mechanism integrated into the OKVIS VIO package, and demonstrate successful online joint angle estimation while maintaining localization accuracy that is comparable to a standard static multi-camera configuration.


Title: High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion
Key Words: feedforward neural nets  optical radar  radar imaging  stereo image processing  LiDAR  compact convolution module  dense stereo depth information  sparse 3D LiDAR  deep convolutional neural network architecture  stereo fusion  high-precision depth estimation  off-the-shelf stereo algorithm  Three-dimensional displays  Laser radar  Estimation  Computer architecture  Sensors  Reliability  Image color analysis 
Abstract: We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LiDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.


Title: Optimizing Placement and Number of RF Beacons to Achieve Better Indoor Localization
Key Words: indoor radio  mobility management (mobile radio)  optimisation  RF beacons  RF signal propagation  indoor infrastructure  cost function  Radio Frequency beacons  indoor localization  Sensors  Optimization  Wireless sensor networks  Lattices  Genetic algorithms  Radio frequency  RF signals  Indoor localization  Beacon deployment  Bluetooth Low Energy (BLE)  k-Coverage  Wireless sensor networks (WSN) 
Abstract: In this paper, we propose a novel solution to optimize the deployment of Radio Frequency (RF) beacons for the purpose of indoor localization. We propose a system that optimizes both the number of beacons and their placement in a given environment. We propose a novel cost-function, called CovBsm, that allows to simultaneously optimize the 3-coverage while maximizing the beacon spreading. Using this cost function, we propose a framework that maximize both the number of beacons and their placement in a given environment. The proposed solution accounts for the indoor infrastructure and its influence on the RF signal propagation by embedding a realistic simulator into the optimization process.


Title: Addressing Challenging Place Recognition Tasks Using Generative Adversarial Networks
Key Words: feature extraction  image recognition  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  visual perception  perception task  place recognition tasks  simultaneous localization and mapping  SLAM  coupled Generative Adversarial Networks  domain translation task  Task analysis  Gallium nitride  Lighting  Generators  Image recognition  Feature extraction  Visualization 
Abstract: Place recognition is an essential component of Simultaneous Localization And Mapping (SLAM). Under severe appearance change, reliable place recognition is a difficult perception task since the same place is perceptually very different in the morning, at night, or over different seasons. This work addresses place recognition as a domain translation task. Using a pair of coupled Generative Adversarial Networks (GANs), we show that it is possible to generate the appearance of one domain (such as summer) from another (such as winter) without requiring image-to-image correspondences across the domains. Mapping between domains is learned from sets of images in each domain without knowing the instance-to-instance correspondence by enforcing a cyclic consistency constraint. In the process, meaningful feature spaces are learned for each domain, the distances in which can be used for the task of place recognition. Experiments show that learned features correspond to visual similarity and can be effectively used for place recognition across seasons.


Title: Learning Coupled Forward-Inverse Models with Combined Prediction Errors
Key Words: learning (artificial intelligence)  robots  multiple solutions  inverse space  forward models  paired forward-inverse models  multiple modules  local minima  training multiple models-that  monolithic complex network  efficient alternative  multiple simple models  complex models  unstructured environments  combined prediction errors  coupled forward-inverse models  Inverse problems  Computational modeling  Data models  Predictive models  Adaptation models  Robots  Context modeling 
Abstract: Challenging tasks in unstructured environments require robots to learn complex models. Given a large amount of information, learning multiple simple models can offer an efficient alternative to a monolithic complex network. Training multiple models-that is, learning their parameters and their responsibilities-has been shown to be prohibitively hard as optimization is prone to local minima. To efficiently learn multiple models for different contexts, we thus develop a new algorithm based on expectation maximization (EM). In contrast to comparable concepts, this algorithm trains multiple modules of paired forward-inverse models by using the prediction errors of both forward and inverse models simultaneously. In particular, we show that our method yields a substantial improvement over only considering the errors of the forward models on tasks where the inverse space contains multiple solutions.


Title: Data-Efficient Decentralized Visual SLAM
Key Words: cameras  data mining  graph theory  image sensors  multi-robot systems  optimisation  pose estimation  robot vision  SLAM (robots)  decentralized visual SLAM system  decentralized SLAM components  data-efficient decentralized visual SLAM  pose-graph optimization method  data association scales  robot count  data transfers  robots  map data  visual SLAM systems exchange  versatile cameras  lightweight cameras  cheap cameras  multirobot applications  mapping  Supplementary Material Data  Simultaneous localization and mapping  Visualization  Optimization  Pose estimation  Trajectory  Bandwidth 
Abstract: Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code. Supplementary Material Data and code are at: https://github.com/uzh-rpg/dslam_open.


Title: IMLS-SLAM: Scan-to-Model Matching Based on 3D Data
Key Words: collision avoidance  mobile robots  optical radar  remotely operated vehicles  road traffic control  robot vision  SLAM (robots)  stereo image processing  robotics community  stereo cameras  depth sensors  Velodyne LiDAR  autonomous driving  low-drift SLAM algorithm  3D LiDAR data  scan-to-model matching framework  specific sampling strategy  LiDAR scans  Velodyne HDL32  Velodyne HDL64  global drift  IMLS-SLAM  3D data  simultaneous localization and mapping  localized LiDAR sweeps  IMLS surface representation  implicit moving least squares  size 4.0 km  size 16.0 m  time 10.0 year  Three-dimensional displays  Laser radar  Simultaneous localization and mapping  Two dimensional displays  Iterative closest point algorithm  Observability 
Abstract: The Simultaneous Localization And Mapping (SLAM) problem has been well studied in the robotics community, especially using mono, stereo cameras or depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the last 10 years to be very useful to perceive the environment in autonomous driving, but few methods exist that directly use these 3D data for odometry. We present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method relies on a scan-to-model matching framework. We first have a specific sampling strategy based on the LiDAR scans. We then define our model as the previous localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface representation. We show experiments with the Velodyne HDL32 with only 0.40% drift over a 4 km acquisition without any loop closure (i.e., 16 m drift after 4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and ranked among the best methods (against mono, stereo and LiDAR methods) with a global drift of only 0.69%.


Title: ApriISAM: Real-Time Smoothing and Mapping
Key Words: error analysis  matrix decomposition  mobile robots  SLAM (robots)  sparse matrices  fixed computational budget  dynamic variable reordering algorithm  ApriISAM  real-time smoothing  online robots  incremental SLAM algorithms  batch algorithms  absolute error  incremental Cholesky factorizations  marginalization order  iSAM  re-linearize  Simultaneous localization and mapping  Heuristic algorithms  Smoothing methods  Sparse matrices  Clustering algorithms  Real-time systems 
Abstract: For online robots, incremental SLAM algorithms offer huge potential computational savings over batch algorithms. The dominant incremental algorithms are iSAM and iSAM2 which offer radically different approaches to computing incremental updates, balancing issues like 1) the need to re-linearize, 2) changes in the desirable variable marginalization order, and 3) the underlying conceptual approach (i.e. the “matrix” story versus the “factor graph” story). In this paper, we propose a new incremental algorithm that computes solutions with lower absolute error and generally provides lower error solutions for a fixed computational budget than either iSAM or iSAM2. Key to AprilSAM's performance are a new dynamic variable reordering algorithm for fast incremental Cholesky factorizations, a method for reducing the work involved in backsubstitutions, and a new algorithm for deciding between incremental and batch updates.


Title: Fast Nonlinear Approximation of Pose Graph Node Marginalization
Key Words: approximation theory  graph theory  mobile robots  optimisation  path planning  pose estimation  SLAM (robots)  pose graph node marginalization  longterm localization  longterm mapping  longterm navigation  pose graph structure  absolute-to relative-pose spaces  pose-composition approach scaled version  approximate subgraph  fast nonlinear approximation method  Topology  Jacobian matrices  Covariance matrices  Gaussian distribution  Simultaneous localization and mapping  Approximation methods 
Abstract: We present a fast nonlinear approximation method for marginalizing out nodes on pose graphs for longterm simultaneous localization, mapping, and navigation. Our approximation preserves the pose graph structure to leverage the rich literature of pose graphs and optimization schemes. By re-parameterizing from absolute-to relative-pose spaces, our method does not suffer from the choice of linearization points as in previous works. We then join our approximation process with a scaled version of the recently-demoted pose-composition approach. Our approach eschews the expenses of many state-of-the-art convex optimization schemes through our efficient and simple O(N2) implementation for a given known topology of the approximate subgraph. We demonstrate its speed and near optimality in practice by comparing against state-of-the-art techniques on popular datasets.


Title: A Monocular SLAM System Leveraging Structural Regularity in Manhattan World
Key Words: cameras  feature extraction  mobile robots  optimisation  pose estimation  robot vision  SLAM (robots)  rotation optimization strategy  parallelism  global binding method  absolute rotation  relative rotation  translation optimization strategy leveraging coplanarity  coplanar features  relative translation  optimal absolute translation  3D line optimization strategy  structural line segments  structural features  structural feature-based optimization module  3D map  structural regularity  optimization strategies  monocular SLAM systems  Manhattan World  camera poses  Three-dimensional displays  Cameras  Optimization  Parallel processing  Simultaneous localization and mapping  Robustness  Estimation 
Abstract: The structural features in Manhattan world encode useful geometric information of parallelism, orthogonality and/or coplanarity in the scene. By fully exploiting these structural features, we propose a novel monocular SLAM system which provides accurate estimation of camera poses and 3D map. The foremost contribution of the proposed system is a structural feature-based optimization module which contains three novel optimization strategies. First, a rotation optimization strategy using the parallelism and orthogonality of 3D lines is presented. We propose a global binding method to compute an accurate estimation of the absolute rotation of the camera. Then we propose an approach for calculating the relative rotation to further refine the absolute rotation. Second, a translation optimization strategy leveraging coplanarity is proposed. Coplanar features are effectively identified, and we leverage them by a unified model handling both points and lines to calculate the relative translation, and then the optimal absolute translation. Third, a 3D line optimization strategy utilizing parallelism, orthogonality and coplanarity simultaneously is proposed to obtain an accurate 3D map consisting of structural line segments with low computational complexity. Experiments in man-made environments have demonstrated that the proposed system outperforms existing state-of-the-art monocular SLAM systems in terms of accuracy and robustness.


Title: Visual Saliency-Aware Receding Horizon Autonomous Exploration with Application to Aerial Robotics
Key Words: mobile robots  optimisation  path planning  robot vision  trees (mathematics)  visual saliency-aware receding horizon autonomous exploration  reobserving salient regions  environment exploration rate  robot endurance  random tree  two-step optimization paradigm  salient objects  path planner  visual attention  aerial robotics  Visualization  Robot sensing systems  Planning  Computational modeling  Path planning 
Abstract: This paper presents a novel strategy for autonomous visual saliency-aware receding horizon exploration of unknown environments using aerial robots. Through a model of visual attention, incrementally built maps are annotated regarding the visual importance and saliency of different objects and entities in the environment. Provided this information, a path planner that simultaneously optimizes for exploration of unknown space, and also directs the robot's attention to focus on the most salient objects, is developed. Following a two-step optimization paradigm, the algorithm first samples a random tree and identifies the branch maximizing for new volume to be explored. The first viewpoint of this path is then provided as a reference to the second planning step. Within that, a new tree is spanned, admissible branches arriving at the reference viewpoint while respecting a time budget dependent on the robot endurance and its environment exploration rate are found and evaluated in terms of reobserving salient regions at sufficient resolution. The best branch is then selected and executed by the robot, and the whole process is iteratively repeated. The proposed method is evaluated regarding its ability to provide increased attention toward salient objects, is verified to run onboard a small aerial robot, and is demonstrated in a set of challenging experimental studies.


Title: Viewpoint-Tolerant Place Recognition Combining 2D and 3D Information for UAV Navigation
Key Words: autonomous aerial vehicles  distance measurement  geometry  mobile robots  path planning  robot vision  stereo image processing  3D information  UAV navigation  Unmanned Aerial Vehicles  vision-based odometry  loop-closure detection  place recognition framework  local 3D geometry  viewpoint-tolerant place recognition  2D Information  hand-held datasets  perceptual aliasing  binary features  Simultaneous localization and mapping  Three-dimensional displays  Visualization  Vocabulary  Image recognition  Navigation 
Abstract: The booming interest in Unmanned Aerial Vehicles (UAV s) is fed by their potentially great impact, however progress is hindered by their limited perception capabilities. While vision-based odometry was shown to run successfully onboard UAV s, loop-closure detection to correct for drift or to recover from tracking failures, has so far, proven particularly challenging for UAVs. At the heart of this is the problem of viewpoint-tolerant place recognition; in stark difference to ground robots, UAVs can revisit a scene from very different viewpoints. As a result, existing approaches struggle greatly as the task at hand violates underlying assumptions in assessing scene similarity. In this paper, we propose a place recognition framework, which exploits both efficient binary features and noisy estimates of the local 3D geometry, which are anyway computed for visual-inertial odometry onboard the UAV. Attaching both an appearance and a geometry signature to each `location', the proposed approach demonstrates unprecedented recall for perfect precision as well as high quality loop-closing transformations on both flying and hand-held datasets exhibiting large viewpoint and appearance changes as well as perceptual aliasing.


Title: Pairwise Consistent Measurement Set Maximization for Robust Multi-Robot Map Merging
Key Words: expectation-maximisation algorithm  graph theory  mobile robots  multi-robot systems  optimisation  robot vision  SLAM (robots)  PCM  robust multirobot map  robust selection  robust SLAM methods  multirobot case  simultaneous localization and mapping  pairwise consistency set maximization  pairwise consistent measurement set maximization  odometry backbone  Simultaneous localization and mapping  Robot kinematics  Phase change materials  Trajectory  Robustness  Merging 
Abstract: This paper reports on a method for robust selection of inter-map loop closures in multi-robot simultaneous localization and mapping (SLAM). Existing robust SLAM methods assume a good initialization or an “odometry backbone” to classify inlier and outlier loop closures. In the multi-robot case, these assumptions do not always hold. This paper presents an algorithm called Pairwise Consistency Maximization (PCM) that estimates the largest pairwise internally consistent set of measurements. Finding the largest pairwise internally consistent set can be transformed into an instance of the maximum clique problem from graph theory, and by leveraging the associated literature it can be solved in realtime. This paper evaluates how well PCM approximates the combinatorial gold standard using simulated data. It also evaluates the performance of PCM on synthetic and real-world data sets in comparison with DCS, SCGP, and RANSAC, and shows that PCM significantly outperforms these methods.


Title: Active Motion-Based Communication for Robots with Monocular Vision
Key Words: Bayes methods  decoding  estimation theory  image classification  Kalman filters  Monte Carlo methods  robot vision  online Bayesian estimation algorithm  monocular camera  receiver robot  sending robot  active motion-based communication  accurate trajectory classification  trajectory class distribution  active vision-based control policy  message decoding  trajectory identification  monocular vision model  receiving robot  Trajectory  Receivers  Cameras  Robot vision systems  Bayes methods  Estimation 
Abstract: In this paper, we consider motion as a means of sending messages between robots. We focus on a scenario in which a message is encoded in a sending robot's trajectory, and decoded by a receiver robot equipped with a monocular camera. The relative pose between the robots is unknown. We introduce an online Bayesian estimation algorithm based on the Multi-hypothesis Extended Kalman Filter for the receiving robot to simultaneously estimate its relative pose to the sender, and the trajectory class of the sender. The difficulty in this problem arises from the monocular vision model of the receiver and the unknown relative pose between robots, which brings inherent ambiguity into the trajectory identification, and hence the message decoding. An active vision-based control policy is derived and combined with the Bayesian estimation in order to deal with this difficulty. The policy is constructed online based on Monte Carlo Tree Search and aims at reducing the entropy over the trajectory class distribution. The algorithm has broad applications, e.g., to intent modeling and motion prediction for autonomous driving and autonomous drone operations. Simulation results demonstrate that the proposed estimation algorithm and the control policy result in an accurate trajectory classification.


Title: Using a Memory of Motion to Efficiently Warm-Start a Nonlinear Predictive Controller
Key Words: autonomous aerial vehicles  iterative methods  learning (artificial intelligence)  nonlinear control systems  optimal control  optimisation  path planning  predictive control  sampling methods  trajectory optimisation (aerospace)  direct optimal control  control policy  optimal state-control trajectories  nonlinear predictive controller  nonlinear optimization problem  model-based methodology  control cycle  kinodynamic probabilistic roadmap  nonlinear solver  unmanned aerial vehicle  UAV  complex dynamical systems  sampling-based planning  policy learning  Computational modeling  Approximation algorithms  Optimal control  Planning  Robots  Trajectory optimization 
Abstract: Predictive control is an efficient model-based methodology to control complex dynamical systems. In general, it boils down to the resolution at each control cycle of a large nonlinear optimization problem. A critical issue is then to provide a good guess to initialize the nonlinear solver so as to speed up convergence. This is particularly important when disturbances or changes in the environment prevent the use of the trajectory computed at the previous control cycle as initial guess. In this paper, we introduce an original and very efficient solution to automatically build this initial guess. We propose to rely on off-line computation to build an approximation of the optimal trajectories, that can be used on-line to initialize the predictive controller. To that end, we combined the use of sampling-based planning, policy learning with generic representations (such as neural networks), and direct optimal control. We first propose an algorithm to simultaneously build a kinodynamic probabilistic roadmap (PRM) and approximate value function and control policy. This algorithm quickly converges toward an approximation of the optimal state-control trajectories (along with an optimal PRM). Then, we propose two methods to store the optimal trajectories and use them to initialize the predictive controller. We experimentally show that directly storing the state-control trajectories leads the predictive controller to quickly converges (2 to 5 iterations) toward the (global) optimal solution. The results are validated in simulation with an unmanned aerial vehicle (UAV) and other dynamical systems.


Title: Simultaneous Planning and Estimation Based on Physics Reasoning in Robot Manipulation
Key Words: manipulators  multi-robot systems  path planning  physics reasoning  robot manipulation  manipulation planning  human-robot cooperation  multi-robot cooperation  Planning  Cognition  Estimation  Force  Robot sensing systems 
Abstract: For robots to autonomously achieve manipulation tasks in various scenes, advanced operational skills such as tool use, learning from demonstration, and multi-robot/human-robot cooperation are necessary. In this research, we devise a method for robots to realize such operational skills in a unified manner by evaluating physical consistency (referred to as “physics reasoning”) based on the formulation of the manipulation statics constraints. First, we propose manipulation planning and estimation methods in which the operational feasibility and properties' likelihood are derived by physics reasoning. In addition, we propose a framework to manipulate an object with unknown physical properties by executing planning and estimation both sequentially and in parallel. We demonstrate the effectiveness of the proposed methods by performing experiments in which real humanoid robots achieve various manipulation tasks with advanced operational skills.


Title: Multimodal Probabilistic Model-Based Planning for Human-Robot Interaction
Key Words: decision making  human-robot interaction  intelligent transportation systems  learning (artificial intelligence)  probability  highway on-ramp-off-ramps  human-robot interaction policies  multimodal probabilistic model-based planning  traffic weaving scenario  human-in-the-loop simulation  candidate future robot actions  interaction history  action distributions  direct learning  candidate robot action sequences  human responses  massively parallel sampling  real-time robot policy construction  human-human exemplars  future human actions  multimodal probability distributions  inherent multimodal uncertainty  experienced drivers  entering exiting cars  decision making  Robots  Vehicles  Predictive models  History  Cognition  Probabilistic logic  Weaving 
Abstract: This paper presents a method for constructing human-robot interaction policies in settings where multimodality, i.e., the possibility of multiple highly distinct futures, plays a critical role in decision making. We are motivated in this work by the example of traffic weaving, e.g., at highway on-ramps/off-ramps, where entering and exiting cars must swap lanes in a short distance-a challenging negotiation even for experienced drivers due to the inherent multimodal uncertainty of who will pass whom. Our approach is to learn multimodal probability distributions over future human actions from a dataset of human-human exemplars and perform real-time robot policy construction in the resulting environment model through massively parallel sampling of human responses to candidate robot action sequences. Direct learning of these distributions is made possible by recent advances in the theory of conditional variational autoencoders (CVAEs), whereby we learn action distributions simultaneously conditioned on the present interaction history, as well as candidate future robot actions in order to take into account response dynamics. We demonstrate the efficacy of this approach with a human-in-the-loop simulation of a traffic weaving scenario.


Title: Vision-Based Robotic Grasping and Manipulation of USB Wires
Key Words: closed loop systems  grippers  industrial manipulators  Lyapunov methods  peripheral interfaces  robot vision  stability  USB cables  vision-based controller  wire alignment  USB color code  vision-based robotic grasping  USB wires  two-level structure  dynamic stability  closed-loop system  Lyapunov methods  Wires  Universal Serial Bus  Robots  Grasping  Grippers  Strain  Image color analysis 
Abstract: The fast expanding 3C (Computer, Communication, and Consumer electronics) manufacturing leads to a high demand on the fabrication of USB cables. While several commercial machines have been developed to automate the process of stripping and soldering of USB cables, the operation of manipulating USB wires according to the color code is heavily dependent on manual works because of the deformation property of wires, probably resulting in the falling-off or the escape of wires during manipulation. In this paper, a new vision-based controller is proposed for robotic grasping and manipulation of USB wires. A novel two-level structure is developed and embedded into the controller, where Level-I is referred to as the grasping and manipulation of wires, and Level-II is referred to as the wire alignment by following the USB color code. The proposed formulation allows the robot to automatically grasp, manipulate, and align the wires in a sequential, simultaneous, and smooth manner, and hence to deal with the deformation of wires. The dynamic stability of the closed-loop system is rigorously proved with Lyapunov methods, and experiments are performed to validate the proposed controller.


Title: Vision-Based Global Localization Using Ceiling Space Density
Key Words: cameras  mobile robots  robot vision  service robots  home environments  free space density  available blueprint information  ceiling vision  robust localization information  robotic vacuum  superior localization results  vision-based global localization  ceiling space density  service robots  homes  self-localize  man-made constructions  documented blueprint  robot localization  smart home applications  movable objects  complicated task  horizontal range-finders  effective global localization approach  Cameras  Kernel  Three-dimensional displays  Robot vision systems  Simultaneous localization and mapping 
Abstract: Service robots are becoming a reality across homes. Still, the range of applications supported by such robots remains tied to their ability to self-localize in the environment. Man-made constructions often have a documented blueprint which can be used as input information for robot localization and smart home applications. However, home environments commonly include movable objects and furniture, which can make localization a complicated task, especially for forward-facing horizontal range-finders. In this paper, we present a new and effective global localization approach for home environments which adapts the notion of free space density to a camera pointing to the ceiling. We exploit the available blueprint information, as well as evidence that ceiling vision can provide robust localization information, even in the presence of occlusions. We perform real-world experiments using a robotic vacuum cleaner equipped with an upward-facing camera in two different apartments across multiple trajectories and compare the proposed method with competing approaches. Our solution shows superior localization results using maps where neither furniture or movable objects are not modeled.


Title: Feature-Based SLAM for Imaging Sonar with Under-Constrained Landmarks
Key Words: feature extraction  image reconstruction  image sensors  reliability  SLAM (robots)  sonar imaging  sonar imaging  point landmark identification  feature-point extraction  general-purpose method  planar scene assumption  underwater feature-based SLAM  under-constrained landmarks  Feature extraction  Imaging  Sonar measurements  Simultaneous localization and mapping  Three-dimensional displays  Image reconstruction 
Abstract: Recent algorithms have demonstrated the feasibility of underwater feature-based SLAM using imaging sonar. But previous methods have either relied on manual feature extraction and correspondence or used prior knowledge of the scene, such as the planar scene assumption. Our proposed system provides a general-purpose method for feature-point extraction and correspondence in arbitrary scenes. Additionally, we develop a method of identifying point landmarks that are likely to be well-constrained and reliably reconstructed. Finally, we demonstrate that while under-constrained landmarks cannot be accurately reconstructed themselves, they can still be used to constrain and correct the sensor motion. These advances represent a large step towards general-purpose, feature-based SLAM with imaging sonar.


Title: SLAMBench2: Multi-Objective Head-to-Head Benchmarking for Visual SLAM
Key Words: augmented reality  autonomous aerial vehicles  mobile computing  mobile robots  navigation  robot vision  SLAM (robots)  visual SLAM  augmented reality systems  nonfunctional requirements  mobile phone-based AR application  tight energy budget  UAV navigation system  SLAMBench2  benchmarking framework  open source  close source  performance metrics  ORB-SLAM2  publicly-available software framework  SLAM applications  SLAM systems  SLAM algorithms  multiobjective head-to-head benchmarking  functional requirements  Simultaneous localization and mapping  Measurement  Trajectory  Benchmark testing  User interfaces  C++ languages 
Abstract: SLAM is becoming a key component of robotics and augmented reality (AR) systems. While a large number of SLAM algorithms have been presented, there has been little effort to unify the interface of such algorithms, or to perform a holistic comparison of their capabilities. This is a problem since different SLAM applications can have different functional and non-functional requirements. For example, a mobile phone-based AR application has a tight energy budget, while a UAV navigation system usually requires high accuracy. SLAMBench2 is a benchmarking framework to evaluate existing and future SLAM systems, both open and close source, over an extensible list of datasets, while using a comparable and clearly specified list of performance metrics. A wide variety of existing SLAM algorithms and datasets is supported, e.g. ElasticFusion, InfiniTAM, ORB-SLAM2, OKVIS, and integrating new ones is straightforward and clearly specified by the framework. SLAMBench2 is a publicly-available software framework which represents a starting point for quantitative, comparable and val-idatable experimental research to investigate trade-offs across SLAM systems.


Title: Delight: An Efficient Descriptor for Global Localisation Using LiDAR Intensities
Key Words: mobile robots  navigation  optical information processing  optical radar  path planning  robot vision  intensity information  DELIGHT  distributed histograms  chi-squared tests  two-stage solution  geometry-based verification  range information  GPS-denied areas  robot position  kidnapped robot problems  mobile robotics  place recognition  global localisation  intensity-based prior estimation  LiDAR intensities  Laser radar  Histograms  Three-dimensional displays  Simultaneous localization and mapping 
Abstract: Place recognition is a key element of mobile robotics. It can assist with the “wake-up” and “kidnapped robot” problems, where the robot position needs to be estimated without prior information. Among the different sensors that can be used for the task (e.g., camera, GPS, LiDAR), LiDAR has the advantage of operating in the dark and in GPS-denied areas. We propose a new method that uses solely the LiDAR data and that can be performed without robot motion. In contrast to other methods, our system leverages intensity information (as opposed to only range information) which is encoded into a novel descriptor of LiDAR intensities as a group of histograms, named DELIGHT. The descriptor encodes the distributed histograms of intensity of the surroundings which are compared using chi-squared tests. Our pipeline is a two-stage solution consisting of an intensity-based prior estimation and a geometry-based verification. For a map of 220k square meters, the method achieves localisation in around 3s with a success rate of 97%, illustrating the applicability of the method in real environments.


Title: Online Probabilistic Change Detection in Feature-Based Maps
Key Words: compressed sensing  feature extraction  image representation  object detection  probability  sensor fusion  terrain mapping  online data association decisions  online probabilistic change detection  sparse feature-based maps  compact representation  static map features  feature repeatability  probabilistically principled approach  sparse mapping model  Feature extraction  Simultaneous localization and mapping  Robustness  Heuristic algorithms  Probabilistic logic  Visualization 
Abstract: Sparse feature-based maps provide a compact representation of the environment that admit efficient algorithms, for example simultaneous localization and mapping. These representations typically assume a static world and therefore contain static map features. However, since the world contains dynamic elements, determining when map features no longer correspond to the environment is essential for long-term utility. This work develops a feature-based model of the environment which evolves over time through feature persistence. Moreover, we augment the state-of-the-art sparse mapping model with a correlative structure that captures spatio-temporal properties, e.g. that nearby features frequently have similar persistence. We show that such relationships, typically addressed through an ad hoc formalism focusing only on feature repeatability, are crucial to evaluate through a probabilistically principled approach. The joint posterior over feature persistence can be computed efficiently and used to improve online data association decisions for localization. The proposed algorithms are validated in numerical simulation and using publicly available data sets.


Title: Towards Globally Consistent Visual-Inertial Collaborative SLAM
Key Words: autonomous aerial vehicles  mobile robots  path planning  robot vision  SLAM (robots)  globally consistent tracking  autonomous robot navigation  monocular-inertial odometry  vision-based perception  metric scale estimation  benchmarking datasets  UAVs  monocular-inertial sensor suite  unmanned aerial vehicles  visual-inertial collaborative SLAM  drift correction  Simultaneous localization and mapping  Collaboration  Unmanned aerial vehicles  Optimization  Measurement  Trajectory 
Abstract: Motivated by the need for globally consistent tracking and mapping before autonomous robot navigation becomes realistically feasible, this paper presents a novel backend to monocular-inertial odometry. As some of the most challenging platforms for vision-based perception, we evaluate the performance of our system using Unmanned Aerial Vehicles (UAV s). Our experimental validation demonstrates that the proposed approach achieves drift correction and metric scale estimation from a single UAV on benchmarking datasets. Furthermore, the generality of our approach is demonstrated to achieve globally consistent maps built in a collaborative manner from two UAVs, each equipped with a monocular-inertial sensor suite, showing the possible gains opened by collaboration amongst robots to perform SLAM. Video - https://youtu.be/wbX36HBu2Eg.


Title: Distributed Simultaneous Action and Target Assignment for Multi-Robot Multi-Target Tracking
Key Words: computational complexity  distributed control  multi-robot systems  target tracking  O(hlog1/ε) communication rounds  distributed simultaneous action  multirobot multitarget tracking  multirobot assignment problems  Robot sensing systems  Target tracking  Approximation algorithms  Partitioning algorithms 
Abstract: We study two multi-robot assignment problems for multi-target tracking. We consider distributed approaches in order to deal with limited sensing and communication ranges. We seek to simultaneously assign trajectories and targets to the robots. Our focus is on local algorithms that achieve performance close to the optimal algorithms with limited communication. We show how to use a local algorithm that guarantees a bounded approximate solution within O(hlog1/ε) communication rounds. We compare with a greedy approach that achieves a 2-approximation in as many rounds as the number of robots. Simulation results show that the local algorithm is an effective solution to the assignment problem.


Title: Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration
Key Words: learning (artificial intelligence)  manipulators  recurrent neural nets  robot vision  nonprehensile manipulation  recurrent neural network  raw images  VAE-GAN-based reconstruction  autoregressive multimodal action prediction  complex manipulation tasks  towel  weight  reconstruction-based regularization  vision-based multitask manipulation  end-to-end learning  multitask learning  low-cost robotic arm  robot arm trajectories  complex picking and placing tasks  Task analysis  Robots  Feature extraction  Neural networks  Image reconstruction  Training  Visualization 
Abstract: We propose a technique for multi-task learning from demonstration that trains the controller of a low-cost robotic arm to accomplish several complex picking and placing tasks, as well as non-prehensile manipulation. The controller is a recurrent neural network using raw images as input and generating robot arm trajectories, with the parameters shared across the tasks. The controller also combines VAE-GAN-based reconstruction with autoregressive multimodal action prediction. Our results demonstrate that it is possible to learn complex manipulation tasks, such as picking up a towel, wiping an object, and depositing the towel to its previous position, entirely from raw images with direct behavior cloning. We show that weight sharing and reconstruction-based regularization substantially improve generalization and robustness, and training on multiple tasks simultaneously increases the success rate on all tasks.


Title: Distributed Learning for the Decentralized Control of Articulated Mobile Robots
Key Words: decentralised control  distributed control  learning systems  mobile robots  multi-agent systems  highly cluttered evaluation environments  decentralized control architectures  central pattern generators  spatially distributed portions  articulated bodies  system-level objectives  reinforcement learning  independent agents  parallel environments  meta-level agent  homogeneous decentralized control  articulated locomotion  distributed learning  asynchronous advantage actor-critic algorithm  A3C  decentralized control policies  independently controlled portion  autonomous decentralized compliant control framework  compliant control baseline  articulated mobile robots  Shape  Decentralized control  Aerospace electronics  Robot kinematics  Admittance  Hardware 
Abstract: Decentralized control architectures, such as those conventionally defined by central pattern generators, independently coordinate spatially distributed portions of articulated bodies to achieve system-level objectives. State of the art distributed algorithms for reinforcement learning employ a different but conceptually related idea; independent agents simultaneously coordinating their own behaviors in parallel environments while asynchronously updating the policy of a system-or, rather, meta-level agent. This work, to the best of the authors' knowledge, is the first to explicitly explore the potential relationship between the underlying concepts in homogeneous decentralized control for articulated locomotion and distributed learning. We present an approach that leverages the structure of the asynchronous advantage actor-critic (A3C) algorithm to provide a natural framework for learning decentralized control policies on a single platform. Our primary contribution shows an individual agent in the A3C algorithm can be defined by an independently controlled portion of the robot's body, thus enabling distributed learning on a single platform for efficient hardware implementation. To this end, we show how the system is trained offline using hardware experiments implementing an autonomous decentralized compliant control framework. Our experimental results show that the trained agent outperforms the compliant control baseline by more than 40% in terms of steady progression through a series of randomized, highly cluttered evaluation environments.


Title: Topomap: Topological Mapping and Navigation Based on Visual SLAM Maps
Key Words: mobile robots  navigation  path planning  robot vision  SLAM (robots)  three-dimensional topological map  noisy sparse point cloud  convex free-space clusters  global planning  mobile robotic platform  Topomap  visual SLAM  visual robot navigation  navigation task  sparse feature-based map  path planning algorithms  visual simultaneous localization and mapping system  Navigation  Visualization  Simultaneous localization and mapping  Path planning  Three-dimensional displays  Planning 
Abstract: Visual robot navigation within large-scale, semistructured environments deals with various challenges such as computation intensive path planning algorithms or insufficient knowledge about traversable spaces. Moreover, many state-of-the-art navigation approaches only operate locally instead of gaining a more conceptual understanding of the planning objective. This limits the complexity of tasks a robot can accomplish and makes it harder to deal with uncertainties that are present in the context of real-time robotics applications. In this work, we present Topomap, a framework which simplifies the navigation task by providing a map to the robot which is tailored for path planning use. This novel approach transforms a sparse feature-based map from a visual Simultaneous Localization And Mapping (SLAM) system into a three-dimensional topological map. This is done in two steps. First, we extract occupancy information directly from the noisy sparse point cloud. Then, we create a set of convex free-space clusters, which are the vertices of the topological map. We show that this representation improves the efficiency of global planning, and we provide a complete derivation of our algorithm. Planning experiments on real world datasets demonstrate that we achieve similar performance as RRT* with significantly lower computation times and storage requirements. Finally, we test our algorithm on a mobile robotic platform to prove its advantages.


Title: PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor Fusion and Hardware Co-Design
Key Words: cameras  image fusion  robot vision  SLAM (robots)  stereo image processing  synchronisation  embedded simultaneous localization and mapping algorithm  multi-core processor  public visual-inertial datasets  PerceptIn Robotics Vision System  Hardware Co-Design  advanced visual-inertial SLAM System  state-of-the-art visual-inertial algorithms  additional sensor modalities  inertial measurements  visual measurements  flexible sensor fusion approach  PIRVS software features  precise hardware synchronization  global-shutter stereo camera  PIRVS hardware  visual-inertial computing hardware  Simultaneous localization and mapping  Hardware  Cameras  Feature extraction  Synchronization  Visualization 
Abstract: In this paper, we present the PerceptIn Robotics Vision System (PIRVS), a visual-inertial computing hardware with embedded simultaneous localization and mapping (SLAM) algorithm. The PIRVS hardware is equipped with a multi-core processor, a global-shutter stereo camera, and an IMU with precise hardware synchronization. The PIRVS software features a flexible sensor fusion approach to not only tightly integrate visual measurements with inertial measurements and also to loosely couple with additional sensor modalities. It runs in real-time on both PC and the PIRVS hardware. We perform a thorough evaluation of the proposed system using multiple public visual-inertial datasets. Experimental results demonstrate that our system reaches comparable accuracy of state-of-the-art visual-inertial algorithms on PC, while being more efficient on the PIRVS hardware.


Title: ProSLAM: Graph SLAM from a Programmer's Perspective
Key Words: C++ language  data structures  graph theory  public domain software  robot vision  SLAM (robots)  stereo image processing  Graph SLAM  data structures  C++ programming language  standard libraries  lightweight open-source stereo visual SLAM system  programmer  ProSLAM  algorithmic aspects  mathematical aspects  highly modular system  Simultaneous localization and mapping  Visualization  Three-dimensional displays  Cameras  Data structures  Benchmark testing 
Abstract: In this paper we present ProSLAM, a lightweight open-source stereo visual SLAM system designed with simplicity in mind. This work stems from the experience gathered by the authors while teaching SLAM and aims at providing a highly modular system that can be easily implemented and understood. Rather than focusing on the well known mathematical aspects of stereo visual SLAM, we highlight the data structures and the algorithmic aspects required to realize such a system. We implemented ProSLAM using the C++ programming language in combination with a minimal set of standard libraries. The results of a thorough validation performed on several standard benchmark datasets show that ProSLAM achieves precision comparable to state-of-the-art approaches, while requiring substantially less computation.


Title: Talk Resource-Efficiently to Me: Optimal Communication Planning for Distributed Loop Closure Detection
Key Words: mobile robots  multi-robot systems  robot vision  SLAM (robots)  cooperative simultaneous localization and mapping  inter-robot loop closures  general resource-efficiency communication planning  sensory data sharing  distributed loop closure detection  optimal communication planning  CSLAM  Robot sensing systems  Distributed databases  Planning  Trajectory  Visualization  Metadata 
Abstract: Due to the distributed nature of cooperative simultaneous localization and mapping (CSLAM), detecting inter-robot loop closures necessitates sharing sensory data with other robots. A naïve approach to data sharing can easily lead to a waste of mission-critical resources. This paper investigates the logistical aspects of CSLAM. Particularly, we present a general resource-efficient communication planning framework that takes into account both the total amount of exchanged data and the induced division of labor between the participating robots. Compared to other state-of-the-art approaches, our framework is able to verify the same set of potential inter-robot loop closures while exchanging considerably less data and influencing the induced workloads. We develop a fast algorithm for finding globally optimal communication policies, and present theoretical analysis to characterize the necessary and sufficient conditions under which simpler strategies are optimal. The proposed framework is extensively evaluated with data from the KITTI odometry benchmark datasets.


Title: StaticFusion: Background Reconstruction for Dense RGB-D SLAM in Dynamic Environments
Key Words: cameras  image colour analysis  image filtering  image motion analysis  image reconstruction  image segmentation  image sensors  image sequences  motion estimation  object detection  object tracking  pose estimation  probability  robot vision  SLAM (robots)  frame-to-model alignment  3D model estimation  outlier filtering techniques  moving object detection  camera pose tracking  probabilistic static-dynamic segmentation  background structure reconstruction  dynamic scenes  static environments  dynamic sequences  static sequences  camera motion estimation  weighted dense RGB-D fusion  current RGB-D image pair  implicit robust penalisers  background structure  robust dense RGB-D SLAM  visual SLAM  dynamic environments  Cameras  Robustness  Image segmentation  Motion segmentation  Dynamics  Three-dimensional displays  Image reconstruction 
Abstract: Dynamic environments are challenging for visual SLAM as moving objects can impair camera pose tracking and cause corruptions to be integrated into the map. In this paper, we propose a method for robust dense RGB-D SLAM in dynamic environments which detects moving objects and simultaneously reconstructs the background structure. While most methods employ implicit robust penalisers or outlier filtering techniques in order to handle moving objects, our approach is to simultaneously estimate the camera motion as well as a probabilistic static/dynamic segmentation of the current RGB-D image pair. This segmentation is then used for weighted dense RGB-D fusion to estimate a 3D model of only the static parts of the environment. By leveraging the 3D model for frame-to-model alignment, as well as static/dynamic segmentation, camera motion estimation has reduced overall drift - as well as being more robust to the presence of dynamics in the scene. Demonstrations are presented which compare the proposed method to related state-of-the-art approaches using both static and dynamic sequences. The proposed method achieves similar performance in static environments and improved accuracy and robustness in dynamic scenes.


Title: Displacement Amplifier Mechanism for Piezoelectric Actuators Design Using SIMP Topology Optimization Approach
Key Words: finite element analysis  optimisation  piezoelectric actuators  Rhombus mechanism  SIMP topology optimization method  displacement range  inherent crystalline properties piezoelectric actuators  SIMP topology optimization approach  piezoelectric actuators design  displacement amplifier mechanism  Optimization  Topology  Force  Piezoelectric actuators  Sensitivity  Mathematical model  piezoelectric actuators  optimal design  compliant structure  SIMP topology optimization 
Abstract: Due to their inherent crystalline properties piezoelectric actuators have a limited deformation. This intrinsic drawback deprives to exploit the potential of these actuators such as, high bandwidth and high resolution in applications that require large displacement range. To overcome this limitation, classical as well as systematic approaches were proposed to design amplification mechanisms. The classical approach leads to empirical mechanisms which are not trivial and needs much experience and intuition. In contrast, systematic approach uses topology optimization method which permits to automatically derive optimal designs that can satisfy specified performances and imposed constraints simultaneously, this with a reasonable time and cost. This paper proposes the design of a mechanism devoted to amplify the displacement of a piezoelectric actuators (PEA). Based on the SIMP topology optimization method, the approach permits to derive a design with a displacement amplification ratio of 4.5, which is higher than with the existing method of Rhombus mechanism. Both finite element (FE) simulation and experimental results confirm and demonstrate the efficiency of the approach.


Title: Constructing Category-Specific Models for Monocular Object-SLAM
Key Words: cameras  feature extraction  mobile robots  object detection  pose estimation  robot vision  SLAM (robots)  category-specific models  real-time object-oriented SLAM  monocular camera  object-level models  category-level models  object deformations  discriminative object features  category models  object landmark observations  generic monocular SLAM framework  2D object features  sparse feature-based monocular SLAM  object instance retrieval  instance-independent monocular object-SLAM system  feature-based SLAM methods  time 2.0 d  time 3.0 d  Solid modeling  Simultaneous localization and mapping  Three-dimensional displays  Object oriented modeling  Pipelines  Two dimensional displays  Shape 
Abstract: We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fty, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show - to the best of our knowledge - first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.


Title: A Deep Learning Based Behavioral Approach to Indoor Autonomous Navigation
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  navigational behaviors  deep learning architectures  semantic abstraction  navigation tasks  navigational missions  behavioral approach  indoor autonomous navigation  semantically rich graph representation  indoor robotic navigation  semantic locations  Navigation  Semantics  Visualization  Simultaneous localization and mapping  Robustness  Measurement 
Abstract: We present a semantically rich graph representation for indoor robotic navigation. Our graph representation encodes: semantic locations such as offices or corridors as nodes, and navigational behaviors such as enter office or cross a corridor as edges. In particular, our navigational behaviors operate directly from visual inputs to produce motor controls and are implemented with deep learning architectures. This enables the robot to avoid explicit computation of its precise location or the geometry of the environment, and enables navigation at a higher level of semantic abstraction. We evaluate the effectiveness of our representation by simulating navigation tasks in a large number of virtual environments. Our results show that using a simple sets of perceptual and navigational behaviors, the proposed approach can successfully guide the way of the robot as it completes navigational missions such as going to a specific office. Furthermore, our implementation shows to be effective to control the selection and switching of behaviors.


Title: Safe Distributed Lane Change Maneuvers for Multiple Autonomous Vehicles Using Buffered Input Cells
Key Words: collision avoidance  computational geometry  decision making  feedback  mobile robots  position control  road safety  road vehicles  safe distributed lane change maneuvers  multiple autonomous vehicles  reciprocal collision avoidance method  autonomous cars  linear dynamics  buffered input cell  Voronoi cell  Voronoi diagrams  vehicles control input  control stack  freeway driving scenario  decision-making layer  trajectory planning layer  feedback controller  BIC method  human-driven car  Collision avoidance  Robots  Vehicle dynamics  Aerospace electronics  Traffic control  Autonomous vehicles  Planning 
Abstract: This paper introduces the Buffered Input Cell as a reciprocal collision avoidance method for multiple vehicles with high-order linear dynamics, extending recently proposed methods based on the Buffered Voronoi Cell [1] and generalized Voronoi diagrams [2]. We prove that if each vehicle's control input remains in its Buffered Input Cell at each time step, collisions will be avoided indefinitely. The method is fast, reactive, and only requires that each vehicle measures the relative position of neighboring vehicles. We incorporate this collision avoidance method as one layer of a complete lane change control stack for autonomous cars in a freeway driving scenario. The lane change control stack comprises a decision-making layer, a trajectory planning layer, a trajectory following feedback controller, and the Buffered Input Cell for collision avoidance. We show in simulations that collisions are avoided with multiple vehicles simultaneously changing lanes on a freeway. We also show in simulations that autonomous cars using the BIC method effectively avoid collisions with an aggressive human-driven car.


Title: Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image
Key Words: image colour analysis  image reconstruction  image resolution  image sampling  image segmentation  image sensors  learning (artificial intelligence)  mean square error methods  optical radar  random processes  regression analysis  SLAM (robots)  sparse matrices  prediction root-mean-square error  sparse maps  dense maps  sparse-to-dense  dense depth prediction  sparse set  depth measurements  single RGB image  depth estimation  monocular images  low-resolution depth sensor  single deep regression network  RGB-D raw data  sparse depth samples  visual simultaneous localization and mapping algorithms  plug-in module  NYU-depth-v2 indoor dataset  LiDARs  Training  Laser radar  Image reconstruction  Estimation  Prediction algorithms  Simultaneous localization and mapping 
Abstract: We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software2 and video demonstration3 are publicly available.


Title: On Geometric Models and Their Accuracy for Extrinsic Sensor Calibration
Key Words: calibration  geometry  numerical analysis  sensors  extrinsic sensor calibration methods  robotics  numerical simulation  abstract geometric model  Calibration  Cameras  Estimation  Task analysis  Simultaneous localization and mapping 
Abstract: Extrinsic sensor calibration is an important task in robotics. There are various ways to perform the calibration task, but it often remains unclear which methods are better than the others. In this paper, we provide a systematic study about the calibration accuracy of three types of calibration methods, each represented by an abstract geometric model based on the sensor configuration and the calibration setup. We discuss the advantages and disadvantages of each model and perform a rigorous study on their noise sensitivity from a geometric perspective. As a result, we can reveal and quantify the relative calibration accuracies of the three models, thus answering the question of “which model is better and why?”. Beside our analytical analysis, we also provide numerical simulation experiments that validate our findings.


Title: Just-in-Time Reconstruction: Inpainting Sparse Maps Using Single View Depth Predictors as Priors
Key Words: convolution  feature extraction  image colour analysis  image fusion  image reconstruction  image sensors  iterative methods  neural nets  pose estimation  recurrent neural nets  robot vision  SLAM (robots)  stereo image processing  CRF model  RGB image  confidence-based fusion  realtime inpainting  convolutional neural networks  CNN  ORB-SLAM  Kinect  conditional depth error distributions  pixel-wise confidence weights  input depth map  fused depth map  virtual depth sensor  single-view depth prediction network  sparse sensor  monocular visual SLAM system  fully dense depth map  realtime image-guided inpainting  just-in-time reconstruction  single view depth predictors  scale-invariant depth error  outlier input depth  LIDAR depth maps  arbitrary scale  sparse map  Image reconstruction  Simultaneous localization and mapping  Visualization  Three-dimensional displays  Real-time systems  Uncertainty 
Abstract: We present “just-in-time reconstruction” as realtime image-guided inpainting of a map with arbitrary scale and sparsity to generate a fully dense depth map for the image. In particular, our goal is to inpaint a sparse map - obtained from either a monocular visual SLAM system or a sparse sensor - using a single-view depth prediction network as a virtual depth sensor. We adopt a fairly standard approach to data fusion, to produce a fused depth map by performing inference over a novel fully-connected Conditional Random Field (CRF) which is parameterized by the input depth maps and their pixel-wise confidence weights. Crucially, we obtain the confidence weights that parameterize the CRF model in a data-dependent manner via Convolutional Neural Networks (CNNs) which are trained to model the conditional depth error distributions given each source of input depth map and the associated RGB image. Our CRF model penalises absolute depth error in its nodes and pairwise scale-invariant depth error in its edges, and the confidence-based fusion minimizes the impact of outlier input depth values on the fused result. We demonstrate the flexibility of our method by real-time inpainting of ORB-SLAM, Kinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary scale and varied amount of irregular sparsity.


Title: Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping
Key Words: continuous time systems  entropy  graph theory  image registration  image resolution  laser ranging  optical radar  SLAM (robots)  solid modelling  stereo image processing  laser-range scanners  high data rate  3D laser scanner  surfel-based registration  recursive state estimation  multiresolution maps  continuous-time SLAM  3D lidar-based online mapping  online simultaneous localization and mapping  Three-dimensional displays  Measurement by laser beam  Optimization  Trajectory  Laser modes  Simultaneous localization and mapping 
Abstract: Modern 3D laser-range scanners have a high data rate, making online simultaneous localization and mapping (SLAM) computationally challenging. Recursive state estimation techniques are efficient but commit to a state estimate immediately after a new scan is made, which may lead to misalignments of measurements. We present a 3D SLAM approach that allows for refining alignments during online mapping. Our method is based on efficient local mapping and a hierarchical optimization back-end. Measurements of a 3D laser scanner are aggregated in local multiresolution maps by means of surfel-based registration. The local maps are used in a multi-level graph for allocentric mapping and localization. In order to incorporate corrections when refining the alignment, the individual 3D scans in the local map are modeled as a sub-graph and graph optimization is performed to account for drift and misalignments in the local maps. Furthermore, in each sub-graph, a continuous-time representation of the sensor trajectory allows to correct measurements between scan poses. We evaluate our approach in multiple experiments by showing qualitative results. Furthermore, we quantify the map quality by an entropy-based measure.


Title: Control of Multiple Passive-Follower Type Robots Based on Feasible Braking Control Region Analysis
Key Words: brakes  braking  mobile robots  motion control  multi-robot systems  wheels  braking control region analysis  passive mobile robot  wheel  formation control  control law  passive robot  fundamental control method  active leader  multiple mobile robots  external pulling force  servo brakes  multiple passive-follower type robots  Mobile robots  Force  Wheels  Robot kinematics  Brakes  Torque 
Abstract: Ahstract- In this study, we propose a development and formation control of a passive mobile robot equipped only with servo brakes and no motors. The developed passive mobile robot can move forward by utilizing an external pulling force, and steers itself by controlling the servo brakes attached to each wheel. Systems composed of multiple mobile robots are very effective at exploring vast areas. However the coordination and simultaneous control of several robots utilizing simple information is a difficult task. Therefore we propose a leader follower architecture composed of multiple passive follower robots tethered to one active leader. In this paper, we first introduce the developed passive mobile robot. Then, we analyze the fundamental control method of this passive mobile robot from the perspective of the feasible braking control region, which considers the slip of the passive robot and the limitation of the external pulling force from the active leader. Lastly, a control law for the servo brakes attached to each wheel is proposed, followed by a feasibility study to determine whether the passive followers can stay in formation by controlling the servo brakes with the proposed control law.


Title: Direct Line Guidance Odometry
Key Words: distance measurement  feature extraction  robot vision  SLAM (robots)  direct line guidance odometry  pixel intensities  line-based features  point-based direct monocular visual odometry method  visual odometry algorithms  feature extraction  keypoint selection  Feature extraction  IP networks  Cameras  Optimization  Visual odometry  Simultaneous localization and mapping  Computational efficiency 
Abstract: Modern visual odometry algorithms utilize sparse point-based features for tracking due to their low computational cost. Current state-of-the-art methods are split between indirect methods that process features extracted from the image, and indirect methods that deal directly on pixel intensities. In recent years, line-based features have been used in SLAM and have shown an increase in performance albeit with an increase in computational cost. In this paper, we propose an extension to a point-based direct monocular visual odometry method. Here we that uses lines to guide keypoint selection rather than acting as features. Points on a line are treated as stronger keypoints than those in other parts of the image, steering point-selection away from less distinctive points and thereby increasing efficiency. By combining intensity and geometry information from a set of points on a line, accuracy may also be increased.


Title: Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System
Key Words: cameras  image matching  motion estimation  motion measurement  optical radar  optical sensors  optical tracking  optical windows  portable instruments  SLAM (robots)  sparse depth information  motion estimation  pose-graph SLAM  KITTI odometry benchmark datasets  direct visual SLAM  monocular camera  light detection and ranging  portable camera-LiDAR mapping system  direct visual simultaneous localization and mapping  sliding window-based tracking method  depth-integrated frame matching  feature-based visual LiDAR mapping  sensors  Cameras  Laser radar  Three-dimensional displays  Visualization  Simultaneous localization and mapping  Optimization 
Abstract: This paper describes a framework for direct visual simultaneous localization and mapping (SLAM) combining a monocular camera with sparse depth information from Light Detection and Ranging (LiDAR). To ensure realtime performance while maintaining high accuracy in motion estimation, we present (i) a sliding window-based tracking method, (ii) strict pose marginalization for accurate pose-graph SLAM and (iii) depth-integrated frame matching for large-scale mapping. Unlike conventional feature-based visual and LiDAR mapping, the proposed approach is direct, eliminating the visual feature in the objective function. We evaluated results using our portable camera-LiDAR system as well as KITTI odometry benchmark datasets. The experimental results prove that the characteristics of two complementary sensors are very effective in improving real-time performance and accuracy. Via validation, we achieved low drift error of 0.98 % in the KITTI benchmark including various environments such as a highway and residential areas.


Title: Bayesian Scale Estimation for Monocular SLAM Based on Generic Object Detection for Correcting Scale Drift
Key Words: Bayes methods  learning (artificial intelligence)  object detection  robot vision  SLAM (robots)  monocular SLAM system  Bayesian framework  deep-learning based generic object detector  detection region  scale drift  monocular systems  Bayesian scale estimation  generic object detection  local scale correction  object class detection  KITTI dataset  quantitative evaluations  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Trajectory  Bayes methods  Object detection  Image reconstruction 
Abstract: We propose a novel real-time algorithm for estimating the local scale correction of a monocular SLAM system, to obtain a correctly scaled version of the 3D map and of the camera trajectory. Within a Bayesian framework, it integrates observations from a deep-learning based generic object detector and landmarks from the map whose projection lie inside a detection region, to produce scale correction estimates from single frames. For each observation, a prior distribution on the height of the detected object class is used to define the observation's likelihood. Due to the scale drift inherent to monocular SLAM systems, we also incorporate a rough model on the dynamics of scale drift. Quantitative evaluations are presented on the KITTI dataset, and compared with different approaches. The results show a superior performance of our proposal in terms of relative translational error when compared to other monocular systems based on object detection.


Title: Efficient Active SLAM Based on Submap Joining, Graph Topology and Convex Optimization
Key Words: computational complexity  convex programming  least squares approximations  minimisation  mobile robots  path planning  predictive control  quadratic programming  robot vision  SLAM (robots)  graph topology  active SLAM problem  robot trajectory  area coverage task  model predictive control framework  uncertainty minimization MPC problem  graphical structure  2D feature-based SLAM  variable substitutions  convex optimization method  MPC framework  sequential quadratic programming method  linear SLAM  submap joining approach  planning  simultaneous localization and mapping  nonconvex constrained least-squares problem  Optimized production technology  Simultaneous localization and mapping  Uncertainty  Task analysis  Robot kinematics 
Abstract: The active SLAM problem considered in this paper aims to plan a robot trajectory for simultaneous localization and mapping (SLAM) as well as for an area coverage task with robot pose uncertainty. Based on a model predictive control (MPC) framework, these two problems are solved respectively by different methods. For the uncertainty minimization MPC problem, based on the graphical structure of the 2D feature-based SLAM, a non-convex constrained least-squares problem is presented to approximate the original problem. Then, using variable substitutions, it is further transformed into a convex problem, and then solved by a convex optimization method. For the coverage task considering robot pose uncertainty, it is formulated and solved by the MPC framework and the sequential quadratic programming (SQP) method. In the whole process, considering the computation complexity, we use linear SLAM, which is a submap joining approach, to reduce the time for planning and estimation. Finally, various simulations are presented to validate the effectiveness of the proposed approach.


Title: 2D SLAM Correction Prediction in Large Scale Urban Environments
Key Words: image representation  mobile robots  multilayer perceptrons  pose estimation  robot vision  SLAM (robots)  autonomous mobile robots  large scale urban environments  simultaneous location and mapping  hybrid correction module  likelihood distributions  2D likelihood SLAM approaches  successive estimated poses  Ensemble Multilayer Perceptron model  SLAM estimations  systematic errors  sensor measurement errors  SLAM map representation  observation model  motion model  probabilistic formulation  Simultaneous localization and mapping  Two dimensional displays  Estimation  Neural networks  Predictive models  Kalman filters 
Abstract: Simultaneous Localization And Mapping (SLAM) is one of the major bricks needed to build truly autonomous mobile robots. The probabilistic formulation of SLAM is based on two models: the motion model and the observation model. In practice, these models, together with the SLAM map representation, do not model perfectly the robot's real dynamics, the sensor measurement errors and the environment. Consequently, systematic errors affect SLAM estimations. In this paper, we propose two approaches to predict corrections to be applied to SLAM estimations. Both are based on the Ensemble Multilayer Perceptron model. The first approach uses successive estimated poses to predict the errors, with no assumptions on the underlying SLAM process or sensor used. The second method is specific to 2D likelihood SLAM approaches, thus, the likelihood distributions are used to predict the corrections, making this second approach independent of the sensor used. We also build a hybrid correction module based on successive estimated poses and the likelihood distributions. The validity of both approaches is evaluated through two experiments using different evaluation metrics and sensor configurations.


Title: Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition
Key Words: mobile robots  object recognition  robot vision  sensor fusion  SLAM (robots)  omnidirectional multisensory perception fusion  long-term place recognition  long-term autonomy  omnidirectional sensors  omnidirectional observation  multidirectional place recognition  omnidirectional multisensory data  appearance variations  Simultaneous Localization and Mapping  Feature extraction  Sensor phenomena and characterization  Simultaneous localization and mapping  Optimization 
Abstract: Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.


Title: Online Initialization and Automatic Camera-IMU Extrinsic Calibration for Monocular Visual-Inertial SLAM
Key Words: accelerometers  calibration  cameras  gyroscopes  inertial navigation  iterative methods  mobile robots  optimisation  robot vision  SLAM (robots)  extrinsic orientation  extrinsic translation  accelerometer bias  camera-IMU extrinsic parameters  initial values  visual scale  initialization stage  mechanical configuration  sensor suite changes  online initialization method  translation calibration  initialization procedure  gyroscope bias  monocular visual-inertial SLAM techniques  gyroscope  gravitational magnitude  Gyroscopes  Cameras  Quaternions  Accelerometers  Calibration  Simultaneous localization and mapping  Gravity 
Abstract: Most of the existing monocular visual-inertial SLAM techniques assume that the camera-IMU extrinsic parameters are known, therefore these methods merely estimate the initial values of velocity, visual scale, gravity, biases of gyroscope and accelerometer in the initialization stage. However, it's usually a professional work to carefully calibrate the extrinsic parameters, and it is required to repeat this work once the mechanical configuration of the sensor suite changes slightly. To tackle this problem, we propose an online initialization method to automatically estimate the initial values and the extrinsic parameters without knowing the mechanical configuration. The biases of gyroscope and accelerometer are considered in our method, and a convergence criteria for both orientation and translation calibration is introduced to identify the convergence and to terminate the initialization procedure. In the three processes of our method, an iterative strategy is firstly introduced to iteratively estimate the gyroscope bias and the extrinsic orientation. Secondly, the scale factor, gravity, and extrinsic translation are approximately estimated without considering the accelerometer bias. Finally, these values are further optimized by a refinement algorithm in which the accelerometer bias and the gravitational magnitude are taken into account. Extensive experimental results show that our method achieves competitive accuracy compared with the state-of-the-art with less calculation.


Title: Sonar Visual Inertial SLAM of Underwater Structures
Key Words: oceanographic techniques  SLAM (robots)  sonar  underwater sound  underwater vehicles  underwater structures  acoustic range data  sonar visual inertial SLAM  visual-inertial state estimation package  resource management  marine archaeology  underwater acoustic sensor  underwater cave  underwater wrecks  underwater domain  Sonar  Cameras  Visualization  Sonar navigation  Simultaneous localization and mapping  Underwater structures 
Abstract: This paper presents an extension to a state of the art Visual-Inertial state estimation package (OKVIS) in order to accommodate data from an underwater acoustic sensor. Mapping underwater structures is important in several fields, such as marine archaeology, search and rescue, resource management, hydrogeology, and speleology. Collecting the data, however, is a challenging, dangerous, and exhausting task. The underwater domain presents unique challenges in the quality of the visual data available; as such, augmenting the exteroceptive sensing with acoustic range data results in improved reconstructions of the underwater structures. Experimental results from underwater wrecks, an underwater cave, and a submerged bus demonstrate the performance of our approach.


Title: Model Predictive Control of a Multi-Rotor with a Suspended Load for Avoiding Obstacles
Key Words: aerospace robotics  collision avoidance  helicopters  mobile robots  predictive control  robot dynamics  vehicle dynamics  path planning  trajectory generation algorithms  MPC  sequential linear quadratic  SLQ  obstacle-avoidance algorithm  Model Predictive Control  dynamic environments  planning algorithms  multirotor  suspended load  Heuristic algorithms  Trajectory  Cost function  Mathematical model  Vehicle dynamics  Load modeling  Computational modeling 
Abstract: This paper investigates a multi-rotor with a suspended load in perspectives of 1) real-time path planning, 2) obstacle avoidance, and 3) transportation of a suspended object. A suspended load cannot be controlled with conventional controllers designed for nominal multi-rotors due to the dynamic coupling between the multi-rotor and load. Although several control and planning algorithms have been proposed based on elaborately derived dynamic equations, most existing studies separate control and path planning problems by following predefined trajectories after trajectory generation. Moreover, many state-of-the-art trajectory generation algorithms cannot work real-time for a system with high degrees of freedom, which makes it not suitable to operate the system in dynamic environments where obstacles appear abruptly or move unexpectedly. With this in mind, we apply Model Predictive Control (MPC) with Sequential Linear Quadratic (SLQ) solver to compute feasible and optimal trajectory real-time and to operate a multi-rotor with a suspended load in dynamic environments. We design an obstacle-avoidance algorithm suitable for the current platform flying in cluttered environments. Flight experiments shows that the proposed algorithm successfully controls the multi-rotor and allows to avoid obstacles simultaneously.


Title: Cooperative Manipulation and Identification of a 2-DOF Articulated Object by a Dual-Arm Robot
Key Words: manipulators  dual-arm robot  dual-arm manipulation  motion directions  motion constraints  coordinated task space frameworks  redundancy exploitation  robot arms  two degrees-of-freedom articulated object  Task analysis  Robot kinematics  Manipulators  Uncertainty  Kinematics  Estimation 
Abstract: In this work, we address the dual-arm manipulation of a two degrees-of-freedom articulated object that consists of two rigid links. This can include a linkage constrained along two motion directions, or two objects in contact, where the contact imposes motion constraints. We formulate the problem as a cooperative task, which allows the employment of coordinated task space frameworks, thus enabling redundancy exploitation by adjusting how the task is shared by the robot arms. In addition, we propose a method that can estimate the joint location and the direction of the degrees-of-freedom, based on the contact forces and the motion constraints imposed by the object. Experimental results demonstrate the performance of the system in its ability to estimate the two degrees of freedom independently or simultaneously.


Title: First Autonomous Multi-Room Exploration with an Insect-Inspired Flapping Wing Vehicle
Key Words: autonomous aerial vehicles  image colour analysis  mobile robots  path planning  robot vision  stereo image processing  multiroom exploration task  DelFly Explorer  autonomous indoor exploration mission  room exploration  stereo-vision based droplet algorithm  heading-based door passage algorithm  flapping wing vehicles  autonomous exploration tasks  autonomous multiroom exploration  wing vehicle  MAVs  autonomous indoor navigation  rotary wings  flapping wing MAV  stereo vision system  microair vehicles  monocular color based Snake-gate algorithm  Task analysis  Robot sensing systems  Navigation  Collision avoidance  Cameras  Image color analysis 
Abstract: One of the emerging tasks for Micro Air Vehicles (MAVs) is autonomous indoor navigation. While commonly employed platforms for such tasks are micro-quadrotors, insect-inspired flapping wing MAVs can offer many advantages, such as being inherently safe due to their low inertia, reciprocating wings bouncing of objects or potentially lower noise levels compared to rotary wings. Here, we present the first flapping wing MAV to perform an autonomous multi-room exploration task. Equipped with an on-board autopilot and a 4 g stereo vision system, the DelFly Explorer succeeded in combining the two most common tasks of an autonomous indoor exploration mission: room exploration and door passage. During the room exploration, the vehicle uses stereo-vision based droplet algorithm to avoid and navigate along the walls and obstacles. Simultaneously, it is running a newly developed monocular color based Snake-gate algorithm to locate doors. A successful detection triggers the heading-based door passage algorithm. In the real-world test, the vehicle could successfully navigate, multiple times in a row, between two rooms separated by a corridor, demonstrating the potential of flapping wing vehicles for autonomous exploration tasks.


Title: Topological Multi-Robot Belief Space Planning in Unknown Environments
Key Words: Bayes methods  graph theory  multi-robot systems  path planning  topology  graph pruning  topological properties  factor graphs  topological space  embedded state space  high-dimensional state spaces  announced path approach  topological multirobot belief space planning  BSP approaches  factor graph representation  posterior beliefs  Planning  Robot kinematics  Simultaneous localization and mapping  Linear programming  Aerospace electronics 
Abstract: In this paper we introduce a novel concept, topological belief space planning (BSP), that uses topological properties of the underlying factor graph representation of future posterior beliefs to direct the search for an optimal solution. This concept deviates from state-of-the-art BSP approaches and is motivated by recent results which indicated, in the context of graph pruning, that topological properties of factor graphs dominantly determine the estimation accuracy. Topological space is also often less dimensional than the embedded state space. In particular, we show how this novel concept can be used in multi-robot belief space planning in high-dimensional state spaces to overcome drawbacks of state-of-the-art approaches: computational intractability of an exhaustive objective evaluation for all candidate path combinations from different robots and dependence on the initial guess in the announced path approach, which can lead to a local minimum of the objective function. We demonstrate our approach in a synthetic simulation.


Title: Central Pattern Generator With Inertial Feedback for Stable Locomotion and Climbing in Unstructured Terrain
Key Words: feedback  legged locomotion  motion control  neurophysiology  sensory feedback  inertial feedback  open-loop control strategy  complexity  terrain steepness  challenging terrains  steep terrains  hexapod robot  steep terrain  legged locomotion  body posture  CPG framework  level terrain  open-loop gait generation  CPG models  locomotive performance  gait adaptation  swimming legged robots  crawling legged robots  articulated robots  gaits  central pattern generator models  unstructured terrain  stable locomotion  Legged locomotion  Robot sensing systems  Limit-cycles  Robot kinematics  Adaptation models  Oscillators 
Abstract: Inspired by the locomotor nervous system of vertebrates, central pattern generator (CPG) models can be used to design gaits for articulated robots, such as crawling, swimming or legged robots. Incorporating sensory feedback for gait adaptation in these models can improve the locomotive performance of such robots in challenging terrain. However, many CPG models to date have been developed exclusively for open-loop gait generation for traversing level terrain. In this paper, we present a novel approach for incorporating inertial feedback into the CPG framework for the control of body posture during legged locomotion on steep, unstructured terrain. That is, we adapt the limit cycle of each leg of the robot with time to simultaneously produce locomotion and body posture control. We experimentally validate our approach on a hexapod robot, locomoting in a variety of steep, challenging terrains (grass, rocky slide, stairs). We show how our approach can be used to level the robot's body, allowing it to locomote at a relatively constant speed, even as terrain steepness and complexity prevents the use of an open-loop control strategy.


Title: AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
Key Words: image classification  learning (artificial intelligence)  object detection  multiple object detection  AffordanceNet  object localization  object classification  affordance label  robust resizing strategy  deconvolutional layer sequence  real-time robotic applications  testing environments  end-to-end architecture  multitask loss function  affordance mask  RGB images  object affordance detection  end-to-end deep learning approach  Feature extraction  Robots  Computer architecture  Object detection  Training  Image segmentation  Machine learning 
Abstract: We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net.


Title: Assigning Visual Words to Places for Loop Closure Detection
Key Words: image matching  image recognition  image representation  image segmentation  mobile robots  probability  robot vision  SLAM (robots)  simultaneous localization and mapping  image stream  image match  dynamic segmentation  nearest neighbor voting scheme  image descriptors  query time  on-line clustering algorithm  visual vocabulary construction  robotic applications  LCD  place recognition  loop closure detection  visual words  Visualization  Liquid crystal displays  Robots  Databases  Pipelines  Feature extraction  Vocabulary 
Abstract: Place recognition of pre-visited areas, widely known as Loop Closure Detection (LCD), constitutes one of the most important components in robotic applications, where the robot needs to estimate its pose while navigating through the field (e.g., simultaneous localization and mapping). In this paper, we present a novel approach for LCD based on the assignment of Visual Words (VWs) to particular places of the traversed path. The system operates in real time and does not require any pre-training procedure, such as visual vocabulary construction or descriptor-space dimensionality reduction. A place is defined through a dynamic segmentation of the incoming image stream and is assigned with VWs through the usage of an on-line clustering algorithm. At query time, image descriptors are converted into VWs on the map accumulating votes to the corresponding places. By means of a probability function, the mechanism is capable of identifying a loop closing candidate place. A nearest neighbor voting scheme on the descriptors' space allows the system to select the most appropriate image match at the chosen place. Geometrical and temporal consistency checks are applied on the proposed loop closing pair increasing the system's performance. Evaluation took place on several publicly available and challenging datasets offering high precision and recall scores as compared to other state-of-the-art approaches.


Title: Localization Under Topological Uncertainty for Lane Identification of Autonomous Vehicles
Key Words: hidden Markov models  mobile robots  position control  remotely operated vehicles  road traffic control  topology  VSM-HMM  topological uncertainty  lane membership  topological localization process  topological structure estimation  AV lane estimation  lane identification  autonomous vehicles  topological location  decision-making  public roads  variable structure multiple hidden Markov model  metric location  Earth mover distance  Hidden Markov models  Computational modeling  Topology  Roads  Uncertainty  Measurement  Vehicle dynamics 
Abstract: Autonomous vehicles (AVs) require accurate metric and topological location estimates for safe, effective navigation and decision-making. Although many high-definition (HD) roadmaps exist, they are not always accurate since public roads are dynamic, shaped unpredictably by both human activity and nature. Thus, AVs must be able to handle situations in which the topology specified by the map does not agree with reality. We present the Variable Structure Multiple Hidden Markov Model (VSM-HMM) as a framework for localizing in the presence of topological uncertainty, and demonstrate its effectiveness on an AV where lane membership is modeled as a topological localization process. VSM-HMMs use a dynamic set of HMMs to simultaneously reason about location within a set of most likely current topologies and therefore may also be applied to topological structure estimation as well as AV lane estimation. In addition, we present an extension to the Earth Mover's Distance which allows uncertainty to be taken into account when computing the distance between belief distributions on simplices of arbitrary relative sizes.


Title: Stabilizing Traffic with Autonomous Vehicles
Key Words: frequency-domain analysis  intelligent transportation systems  linear systems  mobile robots  nonlinear programming  optimal control  road safety  road traffic control  road vehicles  stability  autonomously controlled vehicles  autonomous vehicles  human-driven vehicles  traffic stabilization  safer roads  energy savings  single-lane system stabilization  linear string stability  optimality conditions  frequency-domain analysis  nonlinear optimization problem  safety constraint  optimal linear controller  traffic conditions  human driver behavior  Autonomous vehicles  Vehicle dynamics  Stability criteria  Optimization  Mathematical model 
Abstract: Autonomous vehicles promise safer roads, energy savings, and more efficient use of existing infrastructure, among many other benefits. Although the effect of autonomous vehicles has been studied in the limits (near-zero or full penetration), the transition range requires new formulations, mathematical modeling, and control analysis. In this article, we study the ability of small numbers of autonomous vehicles to stabilize a single-lane system of human-driven vehicles. We formalize the problem in terms of linear string stability, derive optimality conditions from frequency-domain analysis, and pose the resulting nonlinear optimization problem. In particular, we introduce two conditions which simultaneously stabilize traffic while imposing a safety constraint on the autonomous vehicle and limiting degradation of performance. With this optimal linear controller in a system with typical human driver behavior, we can numerically determine that only a 6% uniform penetration of autonomously controlled vehicles (i.e. one per string of up to 16 human-driven vehicles) is necessary to stabilize traffic across all traffic conditions.


Title: Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning
Key Words: collision avoidance  decentralised control  gradient methods  learning (artificial intelligence)  mobile robots  multi-robot systems  multiscenario multistage training framework  optimal policy  policy gradient  reinforcement learning algorithm  learned sensor-level collision avoidance policy  final learned policy  collision-free paths  large-scale robot system  deep reinforcement learning  safe collision avoidance policy  efficient collision avoidance policy  optimally decentralized multirobot collision avoidance  agent-level feature extraction  decentralized methods  maps raw sensor measurements  multirobot systems  decentralized sensor-level collision avoidance policy  local collision-free action  distributed multirobot collision avoidance systems  Collision avoidance  Robot sensing systems  Robot kinematics  Navigation  Robustness  Training 
Abstract: Developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths without observing other robots' states and intents. While other distributed multi-robot collision avoidance systems exist, they often require extracting agent-level features to plan a local collision-free action, which can be computationally prohibitive and not robust. More importantly, in practice the performance of these methods are much lower than their centralized counterparts. We present a decentralized sensor-level collision avoidance policy for multi-robot systems, which directly maps raw sensor measurements to an agent's steering commands in terms of movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots on rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. We validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient, collision-free paths for a large-scale robot system. We also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period, including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots. Videos are available at https://sites.google.com/view/drlmaca.


Title: Complex Urban LiDAR Data Set
Key Words: graph theory  mobile robots  optical radar  pose estimation  radar computing  SLAM (robots)  complex urban environments  light detection and ranging data set  fiber optic gyro  inertial measurement unit  Global Positioning System  vehicle pose estimation  graph simultaneous location and mapping algorithm  graph SLAM algorithm  Robot Operating System environment  raw sensor data  2D LiDAR  16-ray 3D LiDARs  LiDAR sensors  three-dimensional LiDAR  building complexes  high-rise buildings  complex urban LiDAR data set  frequency 100.0 Hz  Laser radar  Three-dimensional displays  Global Positioning System  Two dimensional displays  Sensor systems  Urban areas 
Abstract: This paper presents a Light Detection and Ranging (LiDAR) data set that targets complex urban environments. Urban environments with high-rise buildings and congested traffic pose a significant challenge for many robotics applications. The presented data set is unique in the sense it is able to capture the genuine features of an urban environment (e.g. metropolitan areas, large building complexes and underground parking lots). Data of two-dimensional (2D) and three-dimensional (3D) LiDAR, which are typical types of LiDAR sensors, are provided in the data set. The two 16-ray 3D LiDARs are tilted on both sides for maximal coverage. One 2D LiDAR faces backward while the other faces forwards to collect data of roads and buildings, respectively. Raw sensor data from Fiber Optic Gyro (FOG), Inertial Measurement Unit (IMU), and the Global Positioning System (GPS) are presented in a file format for vehicle pose estimation. The pose information of the vehicle estimated at 100 Hz is also presented after applying the graph simultaneous localization and mapping (SLAM) algorithm. For the convenience of development, the file player and data viewer in Robot Operating System (ROS) environment were also released via the web page. The full data sets are available at: http://irap.kaist.ac.kr/dataset. In this website, 3D preview of each data set is provided using WebGL.


Title: Live Structural Modeling Using RGB-D SLAM
Key Words: image colour analysis  image fusion  image texture  robot vision  SLAM (robots)  solid modelling  live structural modeling  dense point cloud  shape map  single point cloud  metric primitive modeling  RGB-D SLAM  primitive shape localization  shape fusion  Shape  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Computational modeling  History  Estimation 
Abstract: This paper presents a method for localizing primitive shapes in a dense point cloud computed by the RGB-D SLAM system. To stably generate a shape map containing only primitive shapes, the primitive shape is incrementally modeled by fusing the shapes estimated at previous frames in the SLAM, so that an accurate shape can be finally generated. Specifically, the history of the fusing process is used to avoid the influence of error accumulation in the SLAM. The point cloud of the shape is then updated by fusing the points in all the previous frames into a single point cloud. In the experimental results, we show that metric primitive modeling in texture-less and unprepared environments can be achieved online.


Title: Adaptive Sampling and Online Learning in Multi-Robot Sensor Coverage with Mixture of Gaussian Processes
Key Words: Gaussian processes  learning (artificial intelligence)  multi-robot systems  optimisation  online learning  multirobot sensor coverage  online environmental sampling  multirobot coverage control  environmental phenomenon  robot team  Gaussian Process  locally learned Gaussian Processes  collective model learning  simultaneous adaptive sampling  density function  sensing performance optimization  Robot sensing systems  Adaptation models  Density functional theory  Temperature distribution  Data models 
Abstract: We consider the problem of online environmental sampling and modeling for multi-robot sensor coverage, where a team of robots spread out over the workspace in order to optimize the overall sensing performance. In contrast to most existing works on multi-robot coverage control that assume prior knowledge of the distribution of environmental phenomenon, also known as density function, we relax this assumption and enable the robot team to efficiently learn the model of the unknown density function Online using adaptive sampling and non-parametric inference such as Gaussian Process (GP). To capture significantly different components of the environmental phenomenon, we propose a new approach with mixture of locally learned Gaussian Processes for collective model learning and an information-theoretic criterion for simultaneous adaptive sampling in multi-robot coverage. Our approach demonstrates a better generalization of the environment modeling and thus the improved performance of coverage without assuming the density function is known a priori. We demonstrate the effectiveness of our algorithm via simulations of information gathering from indoor static sensors.


Title: Robust Localization of Mobile Robots Considering Reliability of LiDAR Measurements
Key Words: mobile robots  optical radar  optical sensors  pose estimation  reliability  robot vision  mobile robot  LiDAR measurements  localization errors  LiDAR sensor-based localization  optical sensors  robust localization  range measurements  reliability  Light Detection and Ranging sensor  pose estimation  Reliability  Laser radar  Robot sensing systems  Optical sensors  Glass  Adaptive optics  Optical variables measurement 
Abstract: In this study, we propose a novel Light Detection and Ranging (LiDAR) sensor-based localization method for localization of a mobile robot. In localization using the LiDAR sensor, localization errors occur when real range measurements differ from reference distances computed from a map. This study focuses on three factors that cause differences between real range measurements and reference distances. The first factor corresponds to optical characteristics of the LiDAR sensor for objects such as glass walls and mirrors. The second factor corresponds to occlusions by dynamic obstacles. The third factor corresponds to static changes in the environment. In practical applications, three factors often simultaneously occur. Although there have been many previous works, robust localization to overcome these three difficulties is still a challenging problem. This study proposes a novel robust localization scheme that exploits only reliable range measurements. A LiDAR sensor-based localization scheme can be successfully executed by utilizing only a few reliable range measurements. Therefore, the computation of reliability plays a significant role. The computation of reliability is divided into two steps. The first step considers characteristics of optical sensors. The second step mainly deals with the effects of obstacles. The observation likelihood model exploits computed reliability for pose estimation. The proposed scheme was successfully verified through various experiments under challenging situations.


Title: Sparse Gaussian Processes on Matrix Lie Groups: A Unified Framework for Optimizing Continuous-Time Trajectories
Key Words: continuous time systems  Gaussian processes  Lie groups  matrix algebra  mobile robots  motion control  optimisation  path planning  regression analysis  state estimation  trajectory control  nonparametric representation  trajectory distributions  sparse GP regression  robot state  locally linear GPs  state estimation  motion planning tasks  sparse Gaussian processes  continuous-time trajectories  trajectory optimization  matrix Lie groups  robot motion reasoning  Trajectory  Simultaneous localization and mapping  Estimation  Planning  Sparse matrices  Gaussian processes 
Abstract: Continuous-time trajectories are useful for reasoning about robot motion in a wide range of tasks. Sparse Gaussian processes (GPs) can be used as a non-parametric representation for trajectory distributions that enables fast trajectory optimization by sparse GP regression. However, most previous approaches that utilize sparse GPs for trajectory optimization are limited by the fact that the robot state is represented in vector space. In this paper, we first extend previous works to consider the state on general matrix Lie groups by applying a constant-velocity prior and defining locally linear GPs. Next, we discuss how sparse GPs on Lie groups provide a unified continuous-time framework for trajectory optimization for solving a number of robotics problems including state estimation and motion planning. Finally, we demonstrate and evaluate our approach on several different estimation and motion planning tasks with both synthetic and real-world experiments.


Title: Complexity Analysis and Efficient Measurement Selection Primitives for High-Rate Graph SLAM
Key Words: computational complexity  graph theory  iterative methods  Newton method  optimisation  SLAM (robots)  complexity analysis  globally-efficient structure  favorable global structures  Gauss-Newton iteration  factorization step  primary computational bottleneck  graph structure  existing analytic gap  quantitative metric called elimination complexity  significant computation reductions  measurement decimation  simple heuristics  aggressive pruning  significant computational savings  structurally-naïve techniques  global level  edge count  SLAM graph  graph-based SLAM  high-rate graph  efficient measurement selection primitives  Simultaneous localization and mapping  Complexity theory  Optimization  Sparse matrices  Extraterrestrial measurements  Linear algebra 
Abstract: Sparsity has been widely recognized as crucial for efficient optimization in graph-based SLAM. Because the sparsity and structure of the SLAM graph reflect the set of incorporated measurements, many methods for sparsification have been proposed in hopes of reducing computation. These methods often focus narrowly on reducing edge count without regard for structure at a global level. Such structurally-naïve techniques can fail to produce significant computational savings, even after aggressive pruning. In contrast, simple heuristics such as measurement decimation and keyframing are known empirically to produce significant computation reductions. To demonstrate why, we propose a quantitative metric called elimination complexity (EC) that bridges the existing analytic gap between graph structure and computation. EC quantifies the complexity of the primary computational bottleneck: the factorization step of a Gauss-Newton iteration. Using this metric, we show rigorously that decimation and keyframing impose favorable global structures and therefore achieve computation reductions on the order of r2/9 and r3, respectively, where r is the pruning rate. We additionally present numerical results showing EC provides a good approximation of computation in both batch and incremental (iSAM2) optimization and demonstrate that pruning methods promoting globally-efficient structure outperform those that do not.


Title: Dense Planar-Inertial SLAM with Structural Constraints
Key Words: distance measurement  image reconstruction  least squares approximations  optimisation  robot vision  SLAM (robots)  dense visual odometry estimation  planar measurements  SLAM framework  IMU biases  planar landmarks  incremental smoothing  Bayes Tree  IMU data  visual information  modeling planes  IMU states  reconstruction results  SLAM algorithms  structural constraints  DPI-SLAM system  planar-inertial SLAM system  novel dense planar-inertial SLAM  dense 3D models  indoor environments  hand-held RGB-D sensor  inertial measurement unit  preinte-grated IMU measurements  factor graph  incremental mapping  probabilistic global optimization  Simultaneous localization and mapping  Optimization  Three-dimensional displays  Real-time systems  Estimation  Visualization 
Abstract: In this work, we develop a novel dense planar-inertial SLAM (DPI-SLAM) system to reconstruct dense 3D models of large indoor environments using a hand-held RGB-D sensor and an inertial measurement unit (IMU). The preinte-grated IMU measurements are loosely-coupled with the dense visual odometry (VO) estimation and tightly-coupled with the planar measurements in a full SLAM framework. The poses, velocities, and IMU biases are optimized together with the planar landmarks in a global factor graph using incremental smoothing and mapping with the Bayes Tree (iSAM2). With odometry estimation using both RGB-D and IMU data, our system can keep track of the poses of the sensors even without sufficient planes or visual information (e.g. textureless walls) temporarily. Modeling planes and IMU states in the fully probabilistic global optimization reduces the drift that distorts the reconstruction results of other SLAM algorithms. Moreover, structural constraints between nearby planes (e.g. right angles) are added into the DPI-SLAM system, which further recovers the drift and distortion. We test our DPI-SLAM on large indoor datasets and demonstrate its state-of-the-art performance as the first planar-inertial SLAM system.


Title: Collaborative 6DoF Relative Pose Estimation for Two UAVs with Overlapping Fields of View
Key Words: aerospace computing  autonomous aerial vehicles  groupware  image fusion  Kalman filters  multi-robot systems  nonlinear filters  pose estimation  robot vision  stereo image processing  monocular-inertial odometry  Extended Kalman Filter  collaborative scene estimation  monocular camera  variable-baseline stereo rig  inertial sensor  Unmanned Aerial Vehicles  collaborative robot operation  collaborative 6DoF relative pose estimation  UAV  Cameras  Simultaneous localization and mapping  Collaboration  Estimation  Unmanned aerial vehicles 
Abstract: Driven by the promise of leveraging the benefits of collaborative robot operation, this paper presents an approach to estimate the relative transformation between two small Unmanned Aerial Vehicles (UAVs), each equipped with a single camera and an inertial sensor, comprising the first step of any meaningful collaboration. Formation flying and collaborative object manipulation are some of the few tasks that the proposed work has direct applications on, while forming a variable-baseline stereo rig using two UAVs carrying a monocular camera each promises unprecedented effectiveness in collaborative scene estimation. Assuming an overlap in the UAVs' fields of view, in the proposed framework, each UAV runs monocular-inertial odometry onboard, while an Extended Kalman Filter fuses the UAVs' estimates and common image measurements to estimate the metrically scaled relative transformation between them, in realtime. Decoupling the direction of the baseline between the cameras of the two UAVs from its magnitude, this work enables consistent and robust estimation of the uncertainty of the relative pose estimation. Our evaluation on both on simulated data and benchmarking datasets consisting of real aerial data, reveals the power of the proposed methodology in a variety of scenarios. Video - https://youtu.be/AmkkaXa2601.


Title: Simultaneous Optimization of Assignments and Goal Formations for Multiple Robots
Key Words: approximation theory  computational complexity  graph theory  multi-robot systems  optimisation  path planning  O(n3) time complexity  optimal assignments  multiple robots  fixed goal formations  standard assignment problem  transformed problem  formation parameters  linear sum assignment problem  variable goal formation problem  location parameters  Collision avoidance  Robot kinematics  Trajectory  Shape  Cost function 
Abstract: This paper presents algorithms to simultaneously compute the optimal assignments and formation parameters for a team of robots from a given initial formation to a variable goal formation (where the shape of the goal formation is given, and its scale and location parameters must be optimized). We assume the n robots are identical spheres. We use the sum of squared travel distances as the objective function to be minimized, which also ensures that the trajectories are collision free. We show that this assignment with variable goal formation problem can be transformed to a linear sum assignment problem (LSAP) with pseudo costs that we establish are independent of the formation parameters. The transformed problem can then be solved using the Hungarian algorithm in O(n3) time. Thus the assignment problem with variable goal formations using this new approach has the same O(n3) time complexity as the standard assignment problem with fixed goal formations. Results from simulations on 200 and 600 robots are presented to show the algorithm is sufficiently fast for practical applications.


Title: Emulating a Fully Actuated Aerial Vehicle Using Two Actuators
Key Words: actuators  aerodynamics  autonomous aerial vehicles  blades  helicopters  position control  rotors  vehicle dynamics  flat body attitude  fully actuated aerial vehicle  actuators  microair vehicles  quadrotors  downward thrust  spatial trajectories  coaxial helicopter  thrust vector  translation dynamics  cyclic flapping response  Rotors  Blades  Aircraft  Force  Fasteners  Actuators  Trajectory 
Abstract: Micro air vehicles exemplified by quadrotors generate downward thrust in their body fixed frame and may only maneuver spatially by changing their orientation. As a result of this underactuation they are fundamentally incapable of simultaneously regulating orientation and position. Furthermore, their feasible maneuvers are limited to spatial trajectories with continuously differentiable acceleration. We present a coaxial helicopter which emulates full actuation over forces and torques (six degrees of freedom) using only two actuators. The orientation of the thrust vector from each rotor is governed by the drive motor by exciting a cyclic flapping response in special articulated blades. The useful separation of orientation and translation dynamics is demonstrated in flight experiments by tracking spatial trajectories while maintaining flat body attitude as well as tracking desired orientations near hover while station keeping.


Title: ACT: An Autonomous Drone Cinematography System for Action Scenes
Key Words: cinematography  motion estimation  remotely operated vehicles  video cameras  action scenes  aerial filming  autonomous cinematography system  autonomous drone cinematography system  state-of-the-art drone camera system  real-time dynamical camera planning strategy  drone platform  human action  external motion capture systems  drone cinematography systems  aesthetic objectives  Cameras  Drones  Three-dimensional displays  Skeleton  Planning  Robot vision systems  Trajectory 
Abstract: Drones are enabling new forms of cinematography. Aerial filming via drones in action scenes is difficult because it requires users to understand the dynamic scenarios and operate the drone and camera simultaneously. Existing systems allow the user to manually specify the shots and guide the drone to capture footage, while none of them employ aesthetic objectives to automate aerial filming in action scenes. Meanwhile, these drone cinematography systems depend on the external motion capture systems to perceive the human action, which is limited to the indoor environment. In this paper, we propose an Autonomous CinemaTography system “ACT” on the drone platform to address the above the challenges. To our knowledge, this is the first drone camera system which can autonomously capture cinematic shots of action scenes based on limb movements in both indoor and outdoor environments. Our system includes the following novelties. First, we propose an efficient method to extract 3D skeleton points via a stereo camera. Second, we design a real-time dynamical camera planning strategy that fulfills the aesthetic objectives for filming and respects the physical limits of a drone. At the system level, we integrate cameras and GPUs into the limited space of a drone and demonstrate the feasibility of running the entire cinematography system onboard in real-time. Experimental results in both simulation and real-world scenarios demonstrate that our cinematography system “ACT” can capture more expressive video footage of human action than that of a state-of-the-art drone camera system.


Title: Dancing PRM*: Simultaneous Planning of Sampling and Optimization with Configuration Free Space Approximation
Key Words: approximation theory  optimisation  path planning  grid-based approaches  optimization-based planner  resolution-complete factors  spatial information  empirical information  learned information  optimization-based local planner  asymptotic optimal planners  simultaneous planning  configuration free space approximation  optimal motion planning  sampling-based planner  Dancing PRM  Planning  Approximation algorithms  Trajectory  Optimization  Robots  Probabilistic logic  Linear programming 
Abstract: A recent trend in optimal motion planning has broadened the research area toward the hybridization of sampling, optimization and grid-based approaches. We can expect that synergy from such integrations leads to overall performance improvement, but seamless integration and generalization is still an open problem. In this paper, we suggest a hybrid motion planning algorithm utilizing a sampling-based and optimization-based planner while simultaneously approximating a configuration free space. Unlike conventional optimization-based approaches, the proposed algorithm does not depend on a priori information or resolution-complete factors, e.g., a distance field. Ours instead learns spatial information on the fly by exploiting empirical information during the execution, and decentralizes the information over the constructed graph for efficient access. With the help of the learned information, our optimization-based local planner exploits the local area to identify the connectivity of configuration free space without depending on the precomputed domain knowledge. To show the novelty of proposed algorithm, we evaluate it against other asymptotic optimal planners in both synthetic and complex benchmarks with varying degrees of freedom. We also discuss the performance improvement, properties and limitations we have observed.


Title: Hybrid Probabilistic Trajectory Optimization Using Null-Space Exploration
Key Words: humanoid robots  learning systems  manipulator kinematics  probability  trajectory control  joint space  motion constraints  probabilistic formulation  dynamic movement primitives  probabilistic treatment  trajectory constraints  hybrid space learning  motion smoothness  robot null-space  hybrid probabilistic trajectory optimization  null-space exploration  Cartesian space  learning from demonstration  Jacobian-based inverse kinematics  Probabilistic logic  Task analysis  Robot kinematics  Acceleration  Trajectory optimization 
Abstract: In the context of learning from demonstration, human examples are usually imitated in either Cartesian or joint space. However, this treatment might result in undesired movement trajectories in either space. This is particularly important for motion skills such as striking, which typically imposes motion constraints in both spaces. In order to address this issue, we consider a probabilistic formulation of dynamic movement primitives, and apply it to adapt trajectories in Cartesian and joint spaces simultaneously. The probabilistic treatment allows the robot to capture the variability of multiple demonstrations and facilitates the mixture of trajectory constraints from both spaces. In addition to this proposed hybrid space learning, the robot often needs to consider additional constraints such as motion smoothness and joint limits. On the basis of Jacobian-based inverse kinematics, we propose to exploit robot null-space so as to unify trajectory constraints from Cartesian and joint spaces while satisfying additional constraints. Evaluations of hand-shaking and striking tasks carried out with a humanoid robot demonstrate the applicability of our approach.


Title: Feature-constrained Active Visual SLAM for Mobile Robot Navigation
Key Words: collision avoidance  mobile robots  navigation  path planning  robot vision  SLAM (robots)  sensory constraints  iterative motion planning framework  collision avoidance  online mapping  associated map points  distance-optimal path planner  data-driven approach  continuous identification  feature-based Visual Simultaneous Localization  vision-based navigation  failure avoidance  mobile robot navigation  feature-constrained active Visual SLAM  Cameras  Navigation  Collision avoidance  Simultaneous localization and mapping  Planning 
Abstract: This paper focuses on tracking failure avoidance during vision-based navigation to a desired goal in unknown environments. While using feature-based Visual Simultaneous Localization and Mapping (VSLAM), continuous identification and association of map points are required during motion. Thus, we discuss a motion planning framework that takes into account sensory constraints for a reliable navigation. We use information available in the SLAM and propose a data-driven approach to predict the number of map points associated in a given pose. Then, a distance-optimal path planner utilizes the model to constrain paths such that the number of associated map points in each pose is above a threshold. We also include an online mapping of the environment for collision avoidance. Overall, we propose an iterative motion planning framework that enables real-time replanning after the acquisition of more information. Experiments in two environments demonstrate the performance of the proposed framework.


Title: Selection and Compression of Local Binary Features for Remote Visual SLAM
Key Words: feature extraction  feature selection  mobile robots  multi-robot systems  robot vision  SLAM (robots)  feature selection stage  remote visual SLAM  autonomous robotics  collaborative SLAM approaches  multiple robots  feature coding scheme  simultaneous localization and mapping  visual sensors  embedded devices  local binary features extraction  centralized powerful processing node  Visualization  Encoding  Simultaneous localization and mapping  Feature extraction  Task analysis  Image coding 
Abstract: In the field of autonomous robotics, Simultaneous Localization and Mapping (SLAM) is still a challenging problem. With cheap visual sensors attracting more and more attention, various solutions to the SLAM problem using visual cues have been proposed. However, current visual SLAM systems are still computationally demanding, especially on embedded devices. In addition, collaborative SLAM approaches emerge using visual information acquired from multiple robots simultaneously to build a joint map. In order to address both challenges, we present an approach for remote visual SLAM where local binary features are extracted at the robot, compressed and sent over a network to a centralized powerful processing node running the visual SLAM algorithm. To this end, we propose a new feature coding scheme including a feature selection stage which ensures that only relevant information is transmitted. We demonstrate the effectiveness of our approach on well-known datasets. With the proposed approach, it is possible to build an accurate map while limiting the data rate to 75 kbits/frame.


Title: Robust Dense Mapping for Large-Scale Dynamic Environments
Key Words: cameras  image motion analysis  image reconstruction  image segmentation  mobile robots  motion control  object detection  path planning  pose estimation  robot vision  stereo image processing  robust dense mapping  large-scale dynamic environments  stereo-based dense mapping algorithm  large-scale dynamic urban environments  static background  high-level mobile robotic tasks  crowded environments  instance-aware semantic segmentation  sparse scene flow  visual odometry  depth maps  stereo input  map pruning technique  reconstruction accuracy  stationary objects  moving objects detection  path planning  camera poses estimation  frequency 2.5 Hz  Three-dimensional displays  Cameras  Semantics  Vehicle dynamics  Dynamics  Real-time systems  Heuristic algorithms 
Abstract: We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project Websitea.


Title: From Swarms to Stars: Task Coverage in Robot Swarms with Connectivity Constraints
Key Words: distributed control  mobile robots  multi-robot systems  navigation  optimisation  scheduling  Task coverage  Robot swarms  connectivity constraints  swarm robotics  complex tasks  control algorithms  globally coordinated behaviours  spatial coverage  global connectivity  distributed Robot Navigation Controller  RNC  global Task Scheduling Controller  minimal computational load  connectivity assessment  real-life robot experiments  coverage optimality  Task analysis  Robot kinematics  Navigation  Eigenvalues and eigenfunctions  Computational modeling  Multi-robot systems 
Abstract: Swarm robotics carries the potential of solving complex tasks using simple devices. To do so, however, one must define distributed control algorithms capable of producing globally coordinated behaviours. We propose a methodology to address the problem of the spatial coverage of multiple tasks with a swarm of robots that must not lose global connectivity. Our methodology comprises two layers: (i) a distributed Robot Navigation Controller (RNC) is responsible for simultaneously guaranteeing connectivity and pursuit of multiple tasks; and (ii) a global Task Scheduling Controller approximates the optimal strategy for the RNC with minimal computational load. Our contributions include: (i) a qualitative analysis of the literature on connectivity assessment, (ii) our proposed methodology, (iii) simulations in a multi-physics environment, (iv) real-life robot experiments, and (v) the experimental validation of connectivity, coverage optimality, and fault-tolerance.


