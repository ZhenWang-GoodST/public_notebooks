total paper: 219
Title: Workshops & Tutorials
Key Words: Conferences  Tutorials  Service robots  Machine learning  Soft robotics  Estimation 
Abstract: Full Day Workshops & Tutorials


Title: Sponsors and Exhibitors
Key Words: Conferences  Tutorials  Service robots  Machine learning  Soft robotics  Estimation 
Abstract: We thank all sponsors for their generous support and contribution.


Title: Reactive Planar Manipulation with Convex Hybrid MPC
Key Words: closed loop systems  learning (artificial intelligence)  manipulators  optimal control  optimisation  predictive control  Model Predictive Control formulation  optimal sequence  robot motions  desired object motion  multiple contact modes  frictional interactions  combinatorial complexity  optimal mode sequences offline  optimal control inputs  convex hybrid MPC program  planar manipulation experimental setup  convex hybrid MPC formulation  closed-loop performance  reactive planar manipulation  reactive controller  planar manipulation tasks  optimization program  machine learning  Task analysis  Force  Schedules  Friction  Predictive control  Manipulators  Optimization 
Abstract: This paper presents a reactive controller for planar manipulation tasks that leverages machine learning to achieve real-time performance. The approach is based on a Model Predictive Control (MPC) formulation, where the goal is to find an optimal sequence of robot motions to achieve a desired object motion. Due to the multiple contact modes associated with frictional interactions, the resulting optimization program suffers from combinatorial complexity when tasked with determining the optimal sequence of modes. To overcome this difficulty, we formulate the search for the optimal mode sequences offline, separately from the search for optimal control inputs online. Using tools from machine learning, this leads to a convex hybrid MPC program that can be solved in real-time. We validate our algorithm on a planar manipulation experimental setup where results show that the convex hybrid MPC formulation with learned modes achieves good closed-loop performance on a trajectory tracking problem.


Title: Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning
Key Words: learning (artificial intelligence)  manipulators  mobile robots  path planning  potential field-based heuristic exploration strategy  deep Q-network  nonprehensile rearrangement strategy  physical environment  physical world  skillful interaction  tabletop surface  rearranging objects  deep reinforcement learning  nonprehensile manipulation  quicker learning  training process  Planning  Task analysis  Robots  Tools  Cameras  Visualization  Training 
Abstract: Rearranging objects on a tabletop surface by means of nonprehensile manipulation is a task which requires skillful interaction with the physical world. Usually, this is achieved by precisely modeling physical properties of the objects, robot, and the environment for explicit planning. In contrast, as explicitly modeling the physical environment is not always feasible and involves various uncertainties, we learn a nonprehensile rearrangement strategy with deep reinforcement learning based on only visual feedback. For this, we model the task with rewards and train a deep Q-network. Our potential field-based heuristic exploration strategy reduces the amount of collisions which lead to suboptimal outcomes and we actively balance the training set to avoid bias towards poor examples. Our training process leads to quicker learning and better performance on the task as compared to uniform exploration and standard experience replay. We demonstrate empirical evidence from simulation that our method leads to a success rate of 85%, show that our system can cope with sudden changes of the environment, and compare our performance with human level performance.


Title: Contact Point Localization for Articulated Manipulators with Proprioceptive Sensors and Machine Learning
Key Words: control engineering computing  learning (artificial intelligence)  manipulator dynamics  manipulator kinematics  multilayer perceptrons  optimisation  position control  contact point localization  articulated manipulators  proprioceptive sensors  machine learning  joint positions  one-dimensional joint torques  robot arm  RFs  contact link  contact points  Kinova Jaco 2 manipulator  optimization based approach  ML approach  serial manipulator  random forests  multilayer perceptrons  MLP  Force  Torque  Three-dimensional displays  Robot sensing systems  Manipulator dynamics 
Abstract: A model-based Machine Learning (ML) approach is presented to detect and localize external contacts on a 6 degree of freedom (DoF) serial manipulator. This approach only requires the use of proprioceptive sensors (joint positions, velocities and one-dimensional (ID) joint torques already available in the robot arm). Good results are obtained with Random Forests (RFs) and Multi-Layer-Perceptrons (MLPs) leading to a precise localization of the contact link and its orientation. Apart from the link in contact and the orientation of the force, RFs and MLPs are also able to differentiate between contact points on the same link and orientation but with different distances to the joint axis. We experimentally verify this approach on simulated and real data obtained from the Kinova Jaco 2 manipulator and compare it to an optimization based approach.


Title: Unsupervised Contact Learning for Humanoid Estimation and Control
Key Words: end effectors  friction  humanoid robots  legged locomotion  pattern clustering  probability  robot kinematics  state estimation  unsupervised learning  humanoid estimation  contact state estimation  fuzzy clustering  six-dimensional humanoid contacts  proprioceptive sensors - endeffector contact wrench sensors  inertial measurement units  clustering-based contact probability estimator  kinematics-based base state estimator  sensor noise  unsupervised contact learning  IMUs  DoFs  Friction  Sensors  Force  Foot  State estimation  Computational modeling 
Abstract: This work presents a method for contact state estimation using fuzzy clustering to learn contact probability for full, six-dimensional humanoid contacts. The data required for training is solely from proprioceptive sensors - endeffector contact wrench sensors and inertial measurement units (IMUs) - and the method is completely unsupervised. The resulting cluster means are used to efficiently compute the probability of contact in each of the six endeffector degrees of freedom (DoFs) independently. This clustering-based contact probability estimator is validated in a kinematics-based base state estimator in a simulation environment with realistic added sensor noise for locomotion over rough, low-friction terrain on which the robot is subject to foot slip and rotation. The proposed base state estimator which utilizes these six DoF contact probability estimates is shown to perform considerably better than that which determines kinematic contact constraints purely based on measured normal force.


Title: Sample and Feedback Efficient Hierarchical Reinforcement Learning from Human Preferences
Key Words: human-robot interaction  learning (artificial intelligence)  human feedback  reward function  bi-perspective reward learning  simulated robot grasping task  general hierarchical reinforcement learning framework  robot perspective  feedback efficiency  physical robot  informative reward function  human preferences  Robots  Grasping  Task analysis  Trajectory  Learning (artificial intelligence)  Context modeling  Customer relationship management 
Abstract: While reinforcement learning has led to promising results in robotics, defining an informative reward function is challenging. Prior work considered including the human in the loop to jointly learn the reward function and the optimal policy. Generating samples from a physical robot and requesting human feedback are both taxing efforts for which efficiency is critical. We propose to learn reward functions from both the robot and the human perspectives to improve on both efficiency metrics. Learning a reward function from the human perspective increases feedback efficiency by assuming that humans rank trajectories according to a low-dimensional outcome space. Learning a reward function from the robot perspective circumvents the need for a dynamics model while retaining the sample efficiency of model-based approaches. We provide an algorithm that incorporates bi-perspective reward learning into a general hierarchical reinforcement learning framework and demonstrate the merits of our approach on a toy task and a simulated robot grasping task.


Title: Inverse Reinforcement Learning via Function Approximation for Clinical Motion Analysis
Key Words: function approximation  image motion analysis  injuries  learning (artificial intelligence)  medical image processing  neurophysiology  Bellman Optimality Equation  reward learning  inverse reinforcement learning  learned reward function  computationally expensive reinforcement learning problems  function approximation method  clinical motion analysis  Learning (artificial intelligence)  Mathematical model  Trajectory  Function approximation  Spinal cord injury  Markov processes  Estimation 
Abstract: This paper introduces a new method for inverse reinforcement learning in large state spaces, where the learned reward function can be used to control high-dimensional robot systems and analyze complex human movement. To avoid solving the computationally expensive reinforcement learning problems in reward learning, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function to maximize the likelihood of the observed motion. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle large state spaces efficiently. We test the proposed method in a simulated environment on reward learning, and show that it is more accurate than existing methods and significantly better in scalability. We also show that the proposed method can extend many existing methods to large state spaces. We then apply the method to evaluating the effect of rehabilitative stimulations on patients with spinal cord injuries based on the observed patient motions.


Title: Learning User Preferences in Robot Motion Planning Through Interaction
Key Words: human-robot interaction  mobile robots  optimal control  path planning  user preferences  robot motion planning  complex task specifications  human-robot interaction  temporal constraints  complex robot tasks  user constraint  user-optimal path  spatial constraints  Task analysis  Service robots  Roads  Robot motion  Planning  Shortest path problem 
Abstract: In this paper we develop an approach for learning user preferences for complex task specifications through human-robot interaction. We consider the problem of planning robot motion in a known environment, but where a user has specified additional spatial and temporal constraints on allowable robot motions. To illustrate the impact of the user's constraints on performance, we iteratively present users with alternative solutions on an interface. The user provides a ranking of alternate paths, and from this we learn about the importance of different constraints. This allows for an accessible method for specifying complex robot tasks. We present an algorithm that iteratively builds a set of constraints on the relative importance of each user constraint, and prove that with sufficient interaction, the algorithm determines a user-optimal path. We demonstrate the practical performance by simulating realistic material transport scenarios in industrial facilities.


Title: End-to-end Learning of Multi-sensor 3D Tracking by Detection
Key Words: image matching  learning (artificial intelligence)  linear programming  object detection  object tracking  target tracking  3D trajectories  multisensor 3D tracking  end-to-end learning  convolutional networks  linear program  LIDAR data  Trajectory  Three-dimensional displays  Laser radar  Tracking  Cameras  Neural networks  Radar tracking 
Abstract: In this paper we propose a novel approach to tracking by detection that can exploit both cameras as well as LIDAR data to produce very accurate 3D trajectories. Towards this goal, we formulate the problem as a linear program that can be solved exactly, and learn convolutional networks for detection as well as matching in an end-to-end manner. We evaluate our model in the challenging KITTI dataset and show very competitive results.


Title: Deep Forward and Inverse Perceptual Models for Tracking and Prediction
Key Words: deconvolution  feedforward neural nets  image sensors  Kalman filters  learning (artificial intelligence)  mobile robots  nonlinear filters  state estimation  high-dimensional images  deconvolutional methods  robotic system  robot trajectories  convolutional neural network model  photo-realistic images  image generation  video frames  perceptual model  robotics  inverse models  inverse perceptual models  Predictive models  Inverse problems  Robot sensing systems  Kinematics  Training 
Abstract: We consider the problems of learning forward models that map state to high-dimensional images and inverse models that map high-dimensional images to state in robotics. Specifically, we present a perceptual model for generating video frames from state with deep networks, and provide a framework for its use in tracking and prediction tasks. We show that our proposed model greatly outperforms standard deconvolutional methods and GANs for image generation, producing clear, photo-realistic images. We also develop a convolutional neural network model for state estimation and compare the result to an Extended Kalman Filter to estimate robot trajectories. We validate all models on a real robotic system.


Title: Learning Motion Predictors for Smart Wheelchair Using Autoregressive Sparse Gaussian Process
Key Words: adaptive control  autoregressive processes  distance measurement  Gaussian processes  handicapped aids  image capture  interactive devices  learning (artificial intelligence)  mobile robots  motion control  path planning  robot vision  wheelchairs  motion predictors  smart wheelchair  robotics  analog joystick inputs  black-box transformations  intuitive motion control  adaptable motion control  human operators  commercial PWC platform  physical modification  electronic modification  industry standard auxiliary input port  visual odometry  joystick signals  autoregressive sparse Gaussian process model  short-term path prediction experiments  powered wheelchair platform  RGB-D camera  Arduino interface board  motion data capture  standard axle mounted odometers  Cameras  Gaussian processes  Robot sensing systems  Wheelchairs  Mobile robots  Hardware 
Abstract: Constructing a smart wheelchair on a commercially available powered wheelchair (PWC) platform avoids a host of seating, mechanical design and reliability issues but requires methods of predicting and controlling the motion of a device never intended for robotics. Analog joystick inputs are subject to black-box transformations which may produce intuitive and adaptable motion control for human operators, but complicate robotic control approaches; furthermore, installation of standard axle mounted odometers on a commercial PWC is difficult. In this work, we present an integrated hardware and software system for predicting the motion of a commercial PWC platform that does not require any physical or electronic modification of the chair beyond plugging into an industry standard auxiliary input port. This system uses an RGB-D camera and an Arduino interface board to capture motion data, including visual odometry and joystick signals, via ROS communication. Future motion is predicted using an autoregressive sparse Gaussian process model. We evaluate the proposed system on real-world short-term path prediction experiments. Experimental results demonstrate the system's efficacy when compared to a baseline neural network model.


Title: Learning-Based Image Enhancement for Visual Odometry in Challenging HDR Environments
Key Words: convolution  distance measurement  feedforward neural nets  image enhancement  image representation  image sequences  learning (artificial intelligence)  object tracking  robot vision  SLAM (robots)  visual odometry  high dynamic range environments  interest points  bold assumptions  brightness constancy  deep learning perspective  deep neural network  long short term memory  deep networks  VO framework  convolutional neural network  image enhancement  illumination conditions  HDR environments  Robustness  Cameras  Brightness  Lighting  Training  Decoding  Estimation 
Abstract: One of the main open challenges in visual odometry (VO) is the robustness to difficult illumination conditions or high dynamic range (HDR) environments. The main difficulties in these situations come from both the limitations of the sensors and the inability to perform a successful tracking of interest points because of the bold assumptions in VO, such as brightness constancy. We address this problem from a deep learning perspective, for which we first fine-tune a deep neural network with the purpose of obtaining enhanced representations of the sequences for VO. Then, we demonstrate how the insertion of long short term memory allows us to obtain temporally consistent sequences, as the estimation depends on previous states. However, the use of very deep networks enlarges the computational burden of the VO framework; therefore, we also propose a convolutional neural network of reduced size capable of performing faster. Finally, we validate the enhanced representations by evaluating the sequences produced by the two architectures in several state-of-art VO algorithms, such as ORB-SLAM and DSO.


Title: Learning Place-and-Time-Dependent Binary Descriptors for Long-Term Visual Localization
Key Words: computer vision  natural scenes  pose estimation  pose change  place-and-time-dependent binary descriptor  GRIEF evolution algorithm  correspondence generation  single-experience Visual Teach and Repeat system  localization failures  natural scene changes  vision-based navigation  long-term visual localization  extreme illumination changes  binary descriptors  single descriptor scheme  adaptive descriptor  descriptor generation  long-term seasonal variations  short-term illumination changes  Lighting  Visualization  Latches  Navigation  Robustness  Measurement  Evolutionary computation 
Abstract: Vision-based navigation is extremely susceptible to natural scene changes. This can result in localization failures in less than a few hours after map creation. To combat short-term illumination changes as well as long-term seasonal variations, we propose using a place-and-time-dependent binary descriptor that adapts to different scenarios in an online fashion. This is achieved by extending the GRIEF [6] evolution algorithm in two ways: correspondence generation using a known pose change and the inclusion of LATCH triplets in addition to BRIEF comparisons for descriptor generation. We show the adaptive descriptor outperforms a single descriptor scheme for localization within a single-experience Visual Teach and Repeat (VT&R) system while maintaining the efficiency of binary descriptors. By adapting the description function to different environmental conditions, it allows the system to operate for a longer period before a new experience is required. In the presence of extreme illumination changes from day to night, we obtain 40% more inlier matches compared to SURF. In the case of seasonal variations, a 70% increase is demonstrated. The increased correspondences result in more localizable sections along the paths, amounting to a 25% and 150% increase in the lighting and seasonal cases, respectively.


Title: Fusion of Stereo and Still Monocular Depth Estimates in a Self-Supervised Learning Context
Key Words: feedforward neural nets  image colour analysis  intelligent robots  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  monocular depth estimates  autonomous robots  self-supervised learning setup  stereo vision depth  convolutional neural network  fusion method  CNN estimates  autonomous navigation  depth estimation  self-supervised learning  Estimation  Stereo vision  Robot sensing systems  Cameras  Training  Self-supervised learning  monocular depth estimation  stereo vision  convolutional neural networks 
Abstract: We study how autonomous robots can learn by themselves to improve their depth estimation capability. In particular, we investigate a self-supervised learning setup in which stereo vision depth estimates serve as targets for a convolutional neural network (CNN) that transforms a single still image to a dense depth map. After training, the stereo and mono estimates are fused with a novel fusion method that preserves high confidence stereo estimates, while leveraging the CNN estimates in the low-confidence regions. The main contribution of the article is that it is shown that the fused estimates lead to a higher performance than the stereo vision estimates alone. Experiments are performed on the KITTI dataset, and on board of a Parrot SLAMDunk, showing that even rather limited CNNs can help provide stereo vision equipped robots with more reliable depth maps for autonomous navigation.


Title: Predicting Alignment Risk to Prevent Localization Failure
Key Words: feature extraction  image registration  SLAM (robots)  cluttered man-made environments  geometric constraints  spatial overlap  failed alignment  point cloud content  laser-based localization failure  geometric features  point cloud registration  alignment risk  Three-dimensional displays  Cloud computing  Robot sensing systems  Measurement  Iterative closest point algorithm  Octrees 
Abstract: During localization and mapping the success of point cloud registration can be compromised when there is an absence of geometric features or constraints in corridors or across doorways, or when the volumes scanned only partly overlap, due to occlusions or constrictions between subsequent observations. This work proposes a strategy to predict and prevent laser-based localization failure. Our solution relies on explicit analysis of the point cloud content prior to registration. A model predicting the risk of a failed alignment is learned by analysing the degree of spatial overlap between two input point clouds and the geometric constraints available within the region of overlap. We define a novel measure of alignability for these constraints. The method is evaluated against three real-world datasets and compared to baseline approaches. The experiments demonstrate how our approach can help improve the reliability of laser-based localization during exploration of unknown and cluttered man-made environments.


Title: Adversarial Training for Adverse Conditions: Robust Metric Localisation Using Appearance Transfer
Key Words: feature extraction  image filtering  image recognition  adversarial training  adverse conditions  robust metric localisation  appearance transfer  visual place recognition  invertable generator  image transforming filter  feature-matching  dense descriptor maps  output synthetic images  input RGB image  generated images  multiple traversals  reliable localisation  Generators  Detectors  Measurement  Feature extraction  Computer architecture  Training  Pipelines 
Abstract: We present a method of improving visual place recognition and metric localisation under very strong appearance change. We learn an invertable generator that can transform the conditions of images, e.g. from day to night, summer to winter etc. This image transforming filter is explicitly designed to aid and abet feature-matching using a new loss based on SURF detector and dense descriptor maps. A network is trained to output synthetic images optimised for feature matching given only an input RGB image, and these generated images are used to localize the robot against a previously built map using traditional sparse matching approaches. We benchmark our results using multiple traversals of the Oxford RobotCar Dataset over a year-long period, using one traversal as a map and the other to localise. We show that this method significantly improves place recognition and localisation under changing and adverse conditions, while reducing the number of mapping runs needed to successfully achieve reliable localisation.


Title: Data-Driven Approach to Simulating Realistic Human Joint Constraints
Key Words: backpropagation  human-robot interaction  neural nets  optimisation  physical human-robot interaction  physics simulation  implicit equation  human data  physics engine  data-driven approach  human joint limits  joint motion  realistic human joint limits  human joint configurations  realistic human joint constraints  backpropagation  optimization problem  fully connected neural network  Joints  Mathematical model  Physics  Robots  Neural networks  Elbow  Computational modeling 
Abstract: Modeling realistic human joint limits is important for applications involving physical human-robot interaction. However, setting appropriate human joint limits is challenging because it is pose-dependent: the range of joint motion varies depending on the positions of other bones. The paper introduces a new technique to accurately simulate human joint limits in physics simulation. We propose to learn an implicit equation to represent the boundary of valid human joint configurations from real human data. The function in the implicit equation is represented by a fully connected neural network whose gradients can be efficiently computed via back-propagation. Using gradients, we can efficiently enforce realistic human joint limits through constraint forces in a physics engine or as constraints in an optimization problem.


Title: Generative Adversarial Nets in Robotic Chinese Calligraphy
Key Words: character sets  control engineering computing  learning (artificial intelligence)  robot programming  robotic chinese calligraphy  robotic writing  Chinese character strokes  font generation methods  generative adversarial nets-based calligraphic robotic framework  interactive modules  stroke generation module  stroke discriminative module  stroke generative module  calligraphic robot  human-level stroke  robotic autonomous creation ability  reinforcement learning  Writing  Trajectory  Training  Gallium nitride  Manipulators  Probability distribution 
Abstract: Conventional approaches of robotic writing of Chinese character strokes often suffer from limited font generation methods, and thus the writing results often lack of diversity. This has seriously restricted the high quality writing ability of robots. This paper proposes a generative adversarial nets-based calligraphic robotic framework, which enables a robot to learn writing fundamental Chinese strokes with rich diversity and good originality. In particular, the framework considers the learning process of robotic writing as an adversarial procedure which is implemented by three interactive modules including a stroke generation module, a stroke discriminative module and a training module. Noting that the stroke generative module included in the conventional generative adversarial nets cannot solve the non-differentiable problem, the policy gradient commonly used in reinforcement learning is thus adapted in this work to train the generative module by regarding the outputs from the discriminative module as rewards. Experimental results demonstrate that the proposed framework allows a calligraphic robot to successfully write fundamental Chinese strokes with good quality in various styles. The experiment also suggests the proposed approach can achieve human-level stroke writing quality without the requirement of a performance evaluation system. This approach therefore significantly boosts the robotic autonomous creation ability.


Title: Socially Compliant Navigation Through Raw Depth Inputs with Generative Adversarial Imitation Learning
Key Words: learning (artificial intelligence)  mobile robots  path planning  socially compliant navigation  raw depth inputs  mobile robots  socially compliant manner  generative adversarial imitation learning strategy  raw sensory input  GAIL-based approach  behavior cloning policy  social force model  Force  Navigation  Cloning  Sensors  Mobile robots  Learning (artificial intelligence)  Visualization 
Abstract: We present an approach for mobile robots to learn to navigate in dynamic environments with pedestrians via raw depth inputs, in a socially compliant manner. To achieve this, we adopt a generative adversarial imitation learning (GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our approach overcomes the disadvantages of previous methods, as they heavily depend on the full knowledge of the location and velocity information of nearby pedestrians, which not only requires specific sensors, but also the extraction of such state information from raw sensory input could consume much computation time. In this paper, our proposed GAIL-based model performs directly on raw depth inputs and plans in real-time. Experiments show that our GAIL-based approach greatly improves the safety and efficiency of the behavior of mobile robots from pure behavior cloning. The real-world deployment also shows that our method is capable of guiding autonomous vehicles to navigate in a socially compliant manner directly through raw depth inputs. In addition, we release a simulation plugin for modeling pedestrian behaviors based on the social force model.


Title: Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation
Key Words: learning by example  robot programming  video signal processing  context translation  observation-action tuples  supervised learning algorithm  imitation-from-observation  deep reinforcement learning  video prediction  raw video  robotic skills learning  imitation learning  Task analysis  Robots  Context modeling  Learning (artificial intelligence)  Visualization  Tools  Cloning 
Abstract: Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, object positions and types, and other factors. We term this kind of imitation learning “imitation-from-observation,” and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations in the same environment configuration, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show the effectiveness of our approach in learning a wide range of real-world robotic tasks modeled after common household chores from videos of a human demonstrator, including sweeping, ladling almonds, pushing objects as well as a number of tasks in simulation.


Title: Incremental Task Modification via Corrective Demonstrations
Key Words: autoregressive processes  Bayes methods  finite state machines  hidden Markov models  intelligent robots  manipulators  Incremental Task Modification via Corrective Demonstrations  state transition auto-regressive hidden Markov model  probabilistic properties  simulated block sorting domain  real-world pouring task  ITMCD Model Selection  approximate Bayesian model selection  FSA  finite state automaton representation  Hidden Markov models  Task analysis  Robots  Computational modeling  Adaptation models  Probabilistic logic  Bayes methods 
Abstract: In realistic environments, fully specifying a task model such that a robot can perform a task in all situations is impractical. In this work, we present Incremental Task Modification via Corrective Demonstrations (ITMCD), a novel algorithm that allows a robot to update a learned model by making use of corrective demonstrations from an end-user in its environment. We propose three different types of model updates that make structural changes to a finite state automaton (FSA) representation of the task by first converting the FSA into a state transition auto-regressive hidden Markov model (STARHMM). The STARHMM's probabilistic properties are then used to perform approximate Bayesian model selection to choose the best model update, if any. We evaluate ITMCD Model Selection in a simulated block sorting domain and the full algorithm on a real-world pouring task. The simulation results show our approach can choose new task models that sufficiently incorporate new demonstrations while remaining as simple as possible. The results from the pouring task show that ITMCD performs well when the modeled segments of the corrective demonstrations closely comply with the original task model.


Title: Time-Contrastive Networks: Self-Supervised Learning from Video
Key Words: image representation  learning (artificial intelligence)  pose estimation  robot programming  robot vision  video signal processing  time-contrastive networks  robotic behaviors  robotic imitation settings  human poses  viewpoint-invariant representation  end-effectors  reinforcement learning algorithm  self-supervised learning  robotic systems  Robots  Task analysis  Visualization  Learning (artificial intelligence)  Training  Liquids  Lighting 
Abstract: We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.


Title: Learning Sensor Feedback Models from Demonstrations via Phase-Modulated Neural Networks
Key Words: adaptive control  feedback  humanoid robots  learning (artificial intelligence)  mobile robots  path planning  radial basis function networks  tactile sensors  phase-modulated neural networks  feedback model maps  motion plan adaptation  radial basis function network structure  tactile sensor traces  data-driven framework  anthropomorphic robot  Robot sensing systems  Adaptation models  Task analysis  Mathematical model  Neural networks  Quaternions 
Abstract: In order to robustly execute a task under environmental uncertainty, a robot needs to be able to reactively adapt to changes arising in its environment. The environment changes are usually reflected in deviation from expected sensory traces. These deviations in sensory traces can be used to drive the motion adaptation, and for this purpose, a feedback model is required. The feedback model maps the deviations in sensory traces to the motion plan adaptation. In this paper, we develop a general data-driven framework for learning a feedback model from demonstrations. We utilize a variant of a radial basis function network structure -with movement phases as kernel centers- which can generally be applied to represent any feedback models for movement primitives. To demonstrate the effectiveness of our framework, we test it on the task of scraping on a tilt board. In this task, we are learning a reactive policy in the form of orientation adaptation, based on deviations of tactile sensor traces. As a proof of concept of our method, we provide evaluations on an anthropomorphic robot.


Title: Robot Navigation from Human Demonstration: Learning Control Behaviors
Key Words: learning (artificial intelligence)  mobile robots  optimal control  robot navigation  human collaborators  dynamic environments  disaster recovery  unmanned ground vehicle  UGV  fast field adaptation  minimal human supervision  visual perception  inverse optimal control  minimal human supervisory examples  navigation behavior  real-world environment  minimal human demonstration  Navigation  Robots  Trajectory  Entropy  Training  Collision avoidance  Task analysis 
Abstract: When working alongside human collaborators in dynamic environments such as a disaster recovery, an unmanned ground vehicle (UGV) may require fast field adaptation to perform its duties or learn novel tasks. In disaster recovery situations, personnel and equipment are constrained, so training must be accomplished with minimal human supervision. In this paper, we introduce a novel framework which uses learned visual perception and inverse optimal control trained with minimal human supervisory examples. This approach is used to learn to mimic navigation behavior and is demonstrated through extensive evaluation in a real-world environment. Finally, we demonstrate the ability to learn an additional behavior with minimal human demonstration in the field.


Title: Automated Pick-Up of Suturing Needles for Robotic Surgical Assistance
Key Words: cancer  endoscopes  medical image processing  medical robotics  needles  robot vision  surgery  surgical tool  RALP  urethrovesical anastomosis  robotic surgical assistance  robot-assisted laparoscopic prostatectomy  prostate cancer  nerve sparing removal prostate tissue  bladder neck  dexterity demanding tasks  suturing instruments  robotic instruments  vision-guided needle grasping method  suturing needle  grasping process  needle detection algorithm  Needles  Grasping  Robots  Tools  Instruments  Surgery  Task analysis 
Abstract: Robot-assisted laparoscopic prostatectomy (RALP) is a treatment for prostate cancer that involves complete or nerve sparing removal prostate tissue that contains cancer. After removal the bladder neck is successively sutured directly with the urethra. The procedure is called urethrovesical anastomosis and is one of the most dexterity demanding tasks during RALP. Two suturing instruments and a pair of needles are used in combination to perform a running stitch during urethrovesical anastomosis. While robotic instruments provide enhanced dexterity to perform the anastomosis, it is still highly challenging and difficult to learn. In this paper, we presents a vision-guided needle grasping method for automatically grasping the needle that has been inserted into the patient prior to anastomosis. We aim to automatically grasp the suturing needle in a position that avoids hand-offs and immediately enables the start of suturing. The full grasping process can be broken down into: a needle detection algorithm; an approach phase where the surgical tool moves closer to the needle based on visual feedback; and a grasping phase through path planning based on observed surgical practice. Our experimental results show examples of successful autonomous grasping that has the potential to simplify and decrease the operational time in RALP by assisting a small component of urethrovesical anastomosis.


Title: Vehicle Detection, Tracking and Behavior Analysis in Urban Driving Environments Using Road Context
Key Words: driver information systems  learning (artificial intelligence)  object detection  object tracking  optical radar  real-time systems  road traffic  road vehicles  sensor fusion  traffic engineering computing  2D Lidar  deep learning  vehicle tracking  Lidar sensor fusion  global map coordinate system  track management  data association  high precision range estimation  monocular camera  robust fusion system  tracking system  real-time vehicle detection  road context  urban driving environments  behavior analysis  Roads  Laser radar  Sensor fusion  Robot sensing systems  Vehicle detection  Estimation  Autonomous vehicles 
Abstract: We present a real-time vehicle detection and tracking system to accomplish the complex task of driving behavior analysis in urban environments. We propose a robust fusion system that combines a monocular camera and a 2D Lidar. This system takes advantage of three key components: robust vehicle detection using deep learning techniques, high precision range estimation from Lidar, and road context from the prior map knowledge. The camera and Lidar sensor fusion, data association and track management are all performed in the global map coordinate system by taking into account the sensors' characteristics. Lastly, behavior reasoning is performed by examining the tracked vehicle states in the lane coordinate system in which the road context is encoded. We validated our approach by tracking a leading vehicle while it performed usual urban driving behaviors such as lane keeping, stop-and-go at intersections, lane changing, overtaking and turning. The leading vehicle was tracked consistently throughout the 2.3 km route and its behavior was classified reliably.


Title: Deep Inference for Covariance Estimation: Learning Gaussian Noise Models for State Estimation
Key Words: covariance analysis  Gaussian noise  image representation  learning (artificial intelligence)  measurement errors  measurement uncertainty  neural nets  regression analysis  state estimation  raw sensor data  raw sensor measurement  ground-truth measurement error  measurement model  prediction performance  covariance prediction  state Estimation  measurement covariance estimation  deep neural network  Gaussian noise models  measurement uncertainty  deep inference for covariance estimation  predictive sensor modeling  hand-coded features  Robot sensing systems  Measurement uncertainty  Measurement errors  Covariance matrices  Predictive models  Estimation  Neural networks 
Abstract: We present a novel method of measurement covariance estimation that models measurement uncertainty as a function of the measurement itself. Existing work in predictive sensor modeling outperforms conventional fixed models, but requires domain knowledge of the sensors that heavily influences the accuracy and the computational cost of the models. In this work, we introduce Deep Inference for Covariance Estimation (DICE), which utilizes a deep neural network to predict the covariance of a sensor measurement from raw sensor data. We show that given pairs of raw sensor measurement and ground-truth measurement error, we can learn a representation of the measurement model via supervised regression on the prediction performance of the model, eliminating the need for hand-coded features and parametric forms. Our approach is sensor-agnostic, and we demonstrate improved covariance prediction on both simulated and real data.


Title: Locomotion Envelopes for Adaptive Control of Powered Ankle Prostheses
Key Words: adaptive control  gait analysis  Gaussian processes  medical robotics  prosthetics  regression analysis  Gaussian process regression  human subjects walking  locomotion variables  nonlinear manifolds  anthropomorphic control  impedance control  adaptive control  powered ankle foot prosthesis  locomotion envelope  gait-cycle duration  temporal evolution  velocity range  Legged locomotion  Prosthetics  Impedance  Manifolds  Kernel  Gaussian processes  Robustness 
Abstract: In this paper we combine Gaussian process regression and impedance control, to illicit robust, anthropomorphic, adaptive control of a powered ankle prosthesis. We learn the non-linear manifolds which guide how locomotion variables temporally evolve, and regress that surface over a velocity range to create a manifold. The joint set of manifolds, as well as the temporal evolution of the gait-cycle duration is what we term a locomotion envelope. Current powered prostheses have problems adapting across speeds. It is likely that humans rely upon a control strategy which is adaptable, can become more robust and accurate with more data and provides a nonparametric approach which allows the strategy to grow with the number of observations. We demonstrate such a strategy in this study and successfully simulate locomotion well beyond our training data. The method we propose is based on common physical features observed in numerous human subjects walking at different speeds. Based on the derived locomotion envelopes we show that ankle power increases monotonically with speed among all subjects. We demonstrate our methods in simulation and human experiments, on a powered ankle foot prosthesis to demonstrate the effectiveness of the method.


Title: Reliably Arranging Objects in Uncertain Domains
Key Words: learning (artificial intelligence)  manipulators  mobile robots  multi-robot systems  path planning  control uncertainty  conformant planning approach  robot manipulation  multiple planar objects  specified arrangement  external sensing  belief-state planning problem  initial belief state  forward belief-state planning  deterministic belief-state transition model  off-line physics simulations  on-line physics-based manipulation approach  physical robot experiments  uncertain domains  Planning  Robot sensing systems  Task analysis  Reliability  Computational modeling  Uncertainty 
Abstract: A crucial challenge in robotics is achieving reliable results in spite of sensing and control uncertainty. In this work, we explore the conformant planning approach to robot manipulation. In particular, we tackle the problem of pushing multiple planar objects simultaneously to achieve a specified arrangement without external sensing. Conformant planning is a belief-state planning problem. A belief state is the set of all possible states of the world, and the goal is to find a sequence of actions that will bring an initial belief state to a goal belief state. To do forward belief-state planning, we created a deterministic belief-state transition model from supervised learning based on off-line physics simulations. We compare our method with an on-line physics-based manipulation approach and show significantly reduced planning times and increased robustness in simulated experiments. Finally, we demonstrate the success of this approach in simulations and physical robot experiments.


Title: Robotic Cleaning Through Dirt Rearrangement Planning with Learned Transition Models
Key Words: cleaning  learning (artificial intelligence)  manipulators  mobile robots  path planning  sampling methods  service robots  PR2 robots  Fetch robots  primitive dirt-oriented tool actions  heuristic search  cleaning tool  arbitrary amounts  manipulator  learned transition models  dirt rearrangement planning  robotic cleaning  Tools  Planning  Surface cleaning  Manipulators  Task analysis 
Abstract: We address the problem of enabling a manipulator to move arbitrary amounts and configurations of dirt on a surface to a goal region using a cleaning tool. We represent this problem as heuristic search with a set of primitive dirt-oriented tool actions. We present dirt and action representations that allow efficient learning and prediction of future dirt states, given the current dirt state and applied action. We also present a method for sampling promising actions based on a clustering of dirt states and heuristics for planning. We demonstrate the effectiveness of our approach on challenging cleaning tasks through implementations on PR2 and Fetch robots.


Title: Backprop-MPDM: Faster Risk-Aware Policy Evaluation Through Efficient Gradient Optimization
Key Words: decision making  gradient methods  learning (artificial intelligence)  Markov processes  optimisation  stochastic processes  multipolicy decision-making  gradient optimization  risk-aware policy evaluation  backprop-MPDM policy  robot platform  easily-differentiable heuristic function  random sampling  stochastic gradient optimization algorithms  decision making process  risk-aware formulations  Robots  Computational modeling  Trajectory  Navigation  Decision making  Cost function 
Abstract: In Multi-Policy Decision-Making (MPDM), many computationally-expensive forward simulations are performed in order to predict the performance of a set of candidate policies. In risk-aware formulations of MPDM, only the worst outcomes affect the decision making process, and efficiently finding these influential outcomes becomes the core challenge. Recently, stochastic gradient optimization algorithms, using a heuristic function, were shown to be significantly superior to random sampling. In this paper, we show that accurate gradients can be computed - even through a complex forward simulation - using approaches similar to those in deep networks. We show that our proposed approach finds influential outcomes more reliably, and is faster than earlier methods, allowing us to evaluate more policies while simultaneously eliminating the need to design an easily-differentiable heuristic function. We demonstrate significant performance improvements in simulation as well as on a real robot platform navigating a highly dynamic environment.


Title: Bayesian Optimization Using Domain Knowledge on the ATRIAS Biped
Key Words: Bayes methods  control engineering computing  gait analysis  learning (artificial intelligence)  legged locomotion  motion control  optimisation  Bayesian optimization  1-dimensional space  feature transformation  different walking controllers  ATRIAS bipedal robot  nonhumanoid robot morphologies  human-inspired neuromuscular controller  human walking  16-dimensional locomotion controller  bipedal locomotion  black-box data-efficient optimization scheme  data-efficient learning techniques  legged locomotion  simulation transfer  expert-designed heuristics  robotics controllers  domain knowledge  Hardware  Legged locomotion  Optimization  Kernel  Transforms  Measurement 
Abstract: Robotics controllers often consist of expert-designed heuristics, which can be hard to tune in higher dimensions. Simulation can aid in optimizing these controllers if parameters learned in simulation transfer to hardware. Unfortunately, this is often not the case in legged locomotion, necessitating learning directly on hardware. This motivates using data-efficient learning techniques like Bayesian Optimization (BO) to minimize collecting expensive data samples. BO is a black-box data-efficient optimization scheme, though its performance typically degrades in higher dimensions. We aim to overcome this problem by incorporating domain knowledge, with a focus on bipedal locomotion. In our previous work, we proposed a feature transformation that projected a 16-dimensional locomotion controller to a 1-dimensional space using knowledge of human walking. When optimizing a human-inspired neuromuscular controller in simulation, this feature transformation enhanced sample efficiency of BO over traditional BO with a Squared Exponential kernel. In this paper, we present a generalized feature transform applicable to non-humanoid robot morphologies and evaluate it on the ATRIAS bipedal robot, in both simulation and hardware. We present three different walking controllers and two are evaluated on the real robot. Our results show that this feature transform captures important aspects of walking and accelerates learning on hardware and simulation, as compared to traditional BO.


Title: Subject-Independent Data Pooling in Classification of Gait Intent Using Mechanomyography on a Transtibial Amputee
Key Words: artificial limbs  electromyography  gait analysis  learning (artificial intelligence)  medical signal processing  neurophysiology  signal classification  support vector machines  support vector machine classifier  movement delay classification  controller response  training pool  subject specific classifiers  prosthetic legs  subject-specific amputee data sets  supervised training  real-time monitoring  EMG  neuromuscular interfaces  patient activities  inertial measurement units  prosthetic measurement units  gait mode  active lower limb prosthetics  transtibial amputee  mechanomyography  gait intent  subject-independent data pooling  prosthetic control  subject-specific training  MMG gait classifier  user-specific data  pooled training data  Electromyography  Training  Legged locomotion  Muscles  Sensors  Prosthetics  Support vector machines 
Abstract: Active lower limb prosthetics rely on the detection of gait mode to direct controller response. The majority of systems require feedback from the prosthetic and/or inertial measurement units (IMUs). Reliance on movement delays classification, reducing the range of patient activities and terrain traversed. Neuromuscular interfaces using electromyography (EMG) enable real-time monitoring by registering user intent, however EMG has known robustness issues out-of-clinic that have impeded its translation. Furthermore, supervised training of gait classifiers can require large subject-specific amputee data sets which are difficult to obtain. Mechanomyography (MMG) has shown less dependence on environmental conditions than EMG yet has seen limited use in this realm. In this investigation we introduce an MMG gait classifier targeting improved control of prosthetic (robotic) legs. We compare the accuracy of subject specific classifiers to those trained using subject-independent pooling. Additionally, we quantify the effect of introducing a small amount of data from individual test subjects to the training pool. Experiments were performed on 12 participants and 5 gait modes. A support vector machine (SVM) classifier achieved 65% accuracy with subject-specific data, 92% with pooled training data, and 94% with pooled plus limited user-specific data. The results show the promise of MMG gait classifiers with increased robustness and reduced subject-specific training in prosthetic control.


Title: Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping
Key Words: image fusion  image reconstruction  image segmentation  learning (artificial intelligence)  multiview frame fusion technique  perceived 3D structure  semantic labelling task  human interaction  verbal references  location related services  multiview 3D entangled forest  semantic maps  offline reconstruction  single frames  online multiview semantic segmentation  batch approach  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Forestry  Labeling  Context modeling  Standards 
Abstract: Applications that provide location related services need to understand the environment in which humans live such that verbal references and human interaction are possible. We formulate this semantic labelling task as the problem of learning the semantic labels from the perceived 3D structure. In this contribution we propose a batch approach and a novel multi-view frame fusion technique to exploit multiple views for improving the semantic labelling results. The batch approach works offline and is the direct application of an existing single-view method to scene reconstructions with multiple views. The multi-view frame fusion works in an incremental fashion accumulating the single-view results, hence allowing the online multi-view semantic segmentation of single frames and the offline reconstruction of semantic maps. Our experiments show the superiority of the approaches based on our fusion scheme, which leads to a more accurate semantic labelling.


Title: Mark Yourself: Road Marking Segmentation via Weakly-Supervised Annotations from Multimodal Data
Key Words: feature extraction  image segmentation  object detection  object recognition  traffic engineering computing  unsupervised learning  video signal processing  visual databases  road marking segmentation  weakly-supervised annotations  multimodal data  weakly-supervised learning system  complex urban environments  monocular camera  expensive manual labelling  annotated images  deep semantic segmentation network  road markings  traffic situations  weather conditions  sensor modalities  lighting  qualitative performance  real-time road marking detection  labelling effort  Oxford RobotCar dataset  CamVid dataset  Roads  Laser radar  Cameras  Image segmentation  Real-time systems  Labeling  Semantics 
Abstract: This paper presents a weakly-supervised learning system for real-time road marking detection using images of complex urban environments obtained from a monocular camera. We avoid expensive manual labelling by exploiting additional sensor modalities to generate large quantities of annotated images in a weakly-supervised way, which are then used to train a deep semantic segmentation network. At run time, the road markings in the scene are detected in real time in a variety of traffic situations and under different lighting and weather conditions without relying on any preprocessing steps or predefined models. We achieve reliable qualitative performance on the Oxford RobotCar dataset, and demonstrate quantitatively on the CamVid dataset that exploiting these annotations significantly reduces the required labelling effort and improves performance.


Title: Semantic Labeling of Indoor Environments from 3D RGB Maps
Key Words: data mining  feature extraction  geometry  image classification  image colour analysis  image reconstruction  image sensors  indoor navigation  learning (artificial intelligence)  mobile robots  object detection  object recognition  robot vision  SLAM (robots)  stereo image processing  semantic labels assignment  rooms reconstruction  deep-learning techniques  virtual RGB views  geometric analysis  object detection  scene classification  room types  3D RGB maps  indoor environments  semantic labeling  Semantics  Labeling  Three-dimensional displays  Robots  Training  Task analysis  Solid modeling 
Abstract: We present an approach to automatically assign semantic labels to rooms reconstructed from 3D RGB maps of apartments. Evidence for the room types is generated using state-of-the-art deep-learning techniques for scene classification and object detection based on automatically generated virtual RGB views, as well as from a geometric analysis of the map's 3D structure. The evidence is merged in a conditional random field, using statistics mined from different datasets of indoor environments. We evaluate our approach qualitatively and quantitatively and compare it to related methods.


Title: SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud
Key Words: computer games  feedforward neural nets  image classification  image segmentation  learning (artificial intelligence)  object detection  optical radar  pattern clustering  Grand Theft Auto  video game  autonomous driving  road-objects  semantic segmentation  3D LiDAR point cloud  real-time road-object segmentation  recurrent CRF  realistic training data  LiDAR simulator  extra training data  3D bounding boxes  point-wise segmentation labels  CNN model  instance-level labels  point-wise label map  transformed LiDAR point cloud  convolutional neural networks  SqueezeSeg  end-to-end pipeline  point-wise classification problem  LiDAR point clouds  time 8.2 ms to 9.2 ms  time 3.0 d  Three-dimensional displays  Laser radar  Computational modeling  Pipelines  Autonomous vehicles  Semantics  Clustering algorithms 
Abstract: We address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a point-wise classification problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7±0.5 ms per frame), highly desirable for autonomous driving. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code is open-source released1. The paper is accompanied by a video2 containing a high level introduction and demonstrations of this work.


Title: Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments
Key Words: cameras  distance measurement  feature extraction  image classification  image sensors  learning (artificial intelligence)  mobile robots  motion estimation  object detection  pose estimation  robot vision  SLAM (robots)  stereo image processing  self-supervised distractor learning  robust monocular visual odometry  self-supervised approach  distractors  camera images  cluttered urban environments  per-pixel ephemerality mask  depth map  deep convolutional network  monocular visual odometry pipeline  sparse features  dense photometric matching  metric-scale VO  single camera  robust VO methods  odometry drift  egomotion estimation  moving vehicles  urban traffic  vehicle motion  ephemerality  offline multisession mapping approaches  Three-dimensional displays  Cameras  Robustness  Visual odometry  Motion estimation  Entropy  Training data 
Abstract: We present a self-supervised approach to ignoring “distractors” in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.


Title: Semantic Segmentation from Limited Training Data
Key Words: convolution  image classification  image segmentation  learning (artificial intelligence)  neural nets  object detection  object recognition  recurrent neural nets  robot vision  limited training data  robotic perception  cluttered scenes  shiny surfaces  transparent surfaces  robust perception pipeline  data acquisition  deep metric learning approach  semantic-agnostic boundary detection  pixel-wise voting  fully-supervised semantic segmentation approach  ARC 2017 dataset  Amazon Robotics Challenge 2017  dataset collection  deep convolutional neural networks  Task analysis  Image segmentation  Training  Semantics  Robots  Measurement  Three-dimensional displays 
Abstract: We present our approach for robotic perception in cluttered scenes that led to winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small objects with shiny and transparent surfaces, the biggest challenge of the 2017 competition was the introduction of unseen categories. In contrast to traditional approaches which require large collections of annotated data and many hours of training, the task here was to obtain a robust perception pipeline with only few minutes of data acquisition and training time. To that end, we present two strategies that we explored. One is a deep metric learning approach that works in three separate steps: semantic-agnostic boundary detection, patch classification and pixel-wise voting. The other is a fully-supervised semantic segmentation approach with efficient dataset collection. We conduct an extensive analysis of the two methods on our ARC 2017 dataset. Interestingly, only few examples of each class are sufficient to fine-tune even very deep convolutional neural networks for this specific task.


Title: Interactive Robot Knowledge Patching Using Augmented Reality
Key Words: augmented reality  data visualisation  decision making  human-robot interaction  knowledge representation  learning (artificial intelligence)  robot programming  Augmented Reality  Temporal And-Or graph  robot program  interactive robot teaching  AR interface  knowledge representation  comprehensive visualizations  decision making process  Microsoft HoloLens  interactive robot knowledge patching  Task analysis  Decision making  Knowledge representation  Visualization  Robot sensing systems  Grammar 
Abstract: We present a novel Augmented Reality (AR) approach, through Microsoft HoloLens, to address the challenging problems of diagnosing, teaching, and patching interpretable knowledge of a robot. A Temporal And-Or graph (T-AOG) of opening bottles is learned from human demonstration and programmed to the robot. This representation yields a hierarchical structure that captures the compositional nature of the given task, which is highly interpretable for the users. By visualizing the knowledge structure represented by a T-AOG and the decision making process by parsing the T-AOG, the user can intuitively understand what the robot knows, supervise the robot's action planner, and monitor visually latent robot states (e.g., the force exerted during interactions). Given a new task, through such comprehensive visualizations of robot's inner functioning, users can quickly identify the reasons of failures, interactively teach the robot with a new action, and patch it to the current knowledge structure. In this way, the robot is capable of solving similar but new tasks only through minor modifications provided by the users interactively. This process demonstrates the interpretability of our knowledge representation and the effectiveness of the AR interface.


Title: Learning Task-Based Instructional Policy for Excavator-Like Robots
Key Words: excavators  learning by example  mobile robots  learning from demonstration  demonstration trajectories automatic segmentation  hydraulic actuated scaled excavator robot  complex truck loading task  nongeneric policy model  mapping continuous state action trajectories  expert demonstration  task-based instructional policy  Task analysis  Robots  Trajectory  Hidden Markov models  Load modeling  Actuators  Loading 
Abstract: We explore beyond existing work in learning from demonstration by asking the question: “Can robots learn to guide?”, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct humans in executing complex task? As a solution, we propose learning of instructional policy (πI) that maps the state to an instruction for a human. To learn πI, we define action primitives that addresses the challenge of mapping continuous state action trajectories to human parse-able instructions. Action primitives are demonstrated to be very effective in automatic segmentation of demonstration trajectories into fewer repetitive and reusable segments, and a highly scalable approach in comparison to the existing state-of-the art. Finally, we construct a non-generic policy model as a generative model for instructional policies to generate instruction for an entire task. With few modifications, the proposed model is demonstrated to perform autonomous execution of complex truck loading task on hydraulic actuated scaled excavator robot. Guidance approach is tested based on a controlled group study involving 75 participants, who learn to perform the same task.


Title: 3D Human Pose Estimation in RGBD Images for Robotic Task Learning
Key Words: image colour analysis  image sensors  learning (artificial intelligence)  pose estimation  service robots  monocular 3D pose estimation  service robot  color images  robust human keypoint detectors  robotic task learning  RGBD images  3D human pose estimation  human teacher  PR2 robot  Three-dimensional displays  Two dimensional displays  Color  Pose estimation  Robot sensing systems  Task analysis 
Abstract: We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher.


Title: Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning
Key Words: learning systems  mobile robots  navigation  path planning  road vehicles  autonomous vehicles  unsignaled intersections  Deep RL  intersection handling problem  deep reinforcement learning system  occluded intersections  active sensing behaviors  Autonomous vehicles  Automobiles  Machine learning  Safety  Navigation  Learning (artificial intelligence) 
Abstract: Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of Deep Reinforcement Learning to handle intersection problems. Using recent advances in Deep RL, we are able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate and have limited ability to generalize. We then explore a system's ability to learn active sensing behaviors to enable navigating safely in the case of occlusions. Our analysis, provides insight into the intersection handling problem, the solutions learned by the network point out several shortcomings of current rule-based methods, and the failures of our current deep reinforcement learning system point to future research directions.


Title: Dynamic Occupancy Grid Prediction for Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic Labeling
Key Words: Bayes methods  convolution  feedforward neural nets  filtering theory  intelligent transportation systems  mobile robots  Monte Carlo methods  road traffic  time series  traffic engineering computing  unsupervised learning  complex interactions  dynamic occupancy grid prediction  urban autonomous driving  deep learning approach  long-term situation prediction  intelligent vehicles  complex downtown scenarios  multiple road users  motor vehicles  Bayesian filtering technique  environment representation  machine learning  deep convolutional neural network  spatially distributed velocity estimates  raw data sequence  input time series  multiple sensors  convolutional neural networks  road user interaction  pixel-wise balancing  static cells  dynamic cells  unsupervised learning character  pedestrians  bikes  distributed velocity estimation  Monte-Carlo simulation  Vehicle dynamics  Machine learning  Sensor fusion  Roads  Time series analysis  Laser radar 
Abstract: Long-term situation prediction plays a crucial role for intelligent vehicles. A major challenge still to overcome is the prediction of complex downtown scenarios with multiple road users, e.g., pedestrians, bikes, and motor vehicles, interacting with each other. This contribution tackles this challenge by combining a Bayesian filtering technique for environment representation, and machine learning as long-term predictor. More specifically, a dynamic occupancy grid map is utilized as input to a deep convolutional neural network. This yields the advantage of using spatially distributed velocity estimates from a single time step for prediction, rather than a raw data sequence, alleviating common problems dealing with input time series of multiple sensors. Furthermore, convolutional neural networks have the inherent characteristic of using context information, enabling the implicit modeling of road user interaction. Pixel-wise balancing is applied in the loss function counteracting the extreme imbalance between static and dynamic cells. One of the major advantages is the unsupervised learning character due to fully automatic label generation. The presented algorithm is trained and evaluated on multiple hours of recorded sensor data and compared to Monte-Carlo simulation. Experiments show the ability to model complex interactions.


Title: End-to-End Race Driving with Deep Reinforcement Learning
Key Words: automobiles  cameras  computer games  image colour analysis  learning (artificial intelligence)  traffic engineering computing  deep reinforcement learning  mediated perception  object recognition  scene understanding  learning strategies  RGB image  forward facing camera  car control  road structures  end-to-end race driving  reinforcement learning algorithm  legal speed limits  asynchronous actor critic framework  rally game  temperature 3.0 C  Automobiles  Training  Brakes  Games  Roads  Physics  Computer architecture 
Abstract: We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.


Title: Situation Assessment for Planning Lane Changes: Combining Recurrent Models and Prediction
Key Words: automobiles  driver information systems  intelligent transportation systems  learning (artificial intelligence)  mobile robots  prediction theory  recurrent neural nets  road safety  road traffic  road traffic control  fully autonomous cars  complex scenes  dynamic scenes  car following scenarios  situation assessment algorithm  lane changing  recurrent models  driver-assistance systems  bidirectional recurrent neural network  intelligent driver model  lane changes planning  maneuvers planning  driving situations classification  deep learning architecture  long short-term memory units  Automobiles  Planning  Machine learning  Predictive models  Prediction algorithms  Autonomous automobiles 
Abstract: One of the greatest challenges towards fully autonomous cars is the understanding of complex and dynamic scenes. Such understanding is needed for planning of maneuvers, especially those that are particularly frequent such as lane changes. While in recent years advanced driver-assistance systems have made driving safer and more comfortable, these have mostly focused on car following scenarios, and less on maneuvers involving lane changes. In this work we propose a situation assessment algorithm for classifying driving situations with respect to their suitability for lane changing. For this, we propose a deep learning architecture based on a Bidirectional Recurrent Neural Network, which uses Long Short-Term Memory units, and integrates a prediction component in the form of the Intelligent Driver Model. We prove the feasibility of our algorithm on the publicly available NGSIM datasets, where we outperform existing methods.


Title: Sampled-Point Network for Classification of Deformed Building Element Point Clouds
Key Words: disasters  feature extraction  image classification  learning (artificial intelligence)  object recognition  robot vision  object recognition  post-disaster urban areas  search-and-rescue robots  deformed building element point clouds  point network  synthetically-deformed object datasets  point sorting  point coordinates  classification network  deformed building elements  3D class recognition  point cloud input  disaster relief operations  potentially-deformed objects  unstructured environments  point cloud data  3D point cloud  physical site information  Three-dimensional displays  Strain  Object recognition  Deformable models  Machine learning  Feature extraction  Buildings 
Abstract: Search-and-rescue (SAR) robots operating in post-disaster urban areas need to accurately identify physical site information to perform navigation, mapping and manipulation tasks. This can be achieved by acquiring a 3D point cloud of the environment and performing object recognition from the point cloud data. However, this task is complicated by the unstructured environments and potentially-deformed objects encountered during disaster relief operations. Current 3D object recognition methods rely on point cloud input acquired under suitable conditions and do not consider deformations such as outlier noise, bending and truncation. This work introduces a deep learning architecture for 3D class recognition from point clouds of deformed building elements. The classification network, consisting of stacked convolution and average pooling layers applied directly to point coordinates, was trained using point clouds sampled from a database of mesh models. The proposed method achieves robustness to input variability using point sorting, resampling, and rotation normalization techniques. Experimental results on synthetically-deformed object datasets show that the proposed method outperforms the conventional deep learning methods in terms of classification accuracy and computational efficiency.


Title: Recognizing Objects in-the-Wild: Where do we Stand?
Key Words: cameras  image classification  image colour analysis  image representation  learning (artificial intelligence)  mobile robots  neural nets  object recognition  robot vision  multiview object dataset  RGB-D camera  deep convolutional networks  Web images  robotic system  autonomous agents  good visual perceptual systems  robotic vision research communities  human-populated environments  robot vision  real-life robotic data  object classification  deep representations  object recognition algorithms  real-life application  mobile robot  Task analysis  Clutter  Visualization  Mobile robots  Cameras  Robot vision systems 
Abstract: The ability to recognize objects is an essential skill for a robotic system acting in human-populated environments. Despite decades of effort from the robotic and vision research communities, robots are still missing good visual perceptual systems, preventing the use of autonomous agents for realworld applications. The progress is slowed down by the lack of a testbed able to accurately represent the world perceived by the robot in-the-wild. In order to fill this gap, we introduce a large-scale, multi-view object dataset collected with an RGB-D camera mounted on a mobile robot. The dataset embeds the challenges faced by a robot in a real-life application and provides a useful tool for validating object recognition algorithms. Besides describing the characteristics of the dataset, the paper evaluates the performance of a collection of well-established deep convolutional networks on the new dataset and analyzes the transferability of deep representations from Web images to robotic data. Despite the promising results obtained with such representations, the experiments demonstrate that object classification with real-life robotic data is far from being solved. Finally, we provide a comparative study to analyze and highlight the open challenges in robot vision, explaining the discrepancies in the performance.


Title: Omnidirectional CNN for Visual Place Recognition and Navigation
Key Words: cameras  feature extraction  feedforward neural nets  image matching  image retrieval  learning (artificial intelligence)  mobile robots  object recognition  pose estimation  robot vision  visual place recognition  place exemplars  recognition method  omnidirectional cameras  visual input  matched place exemplar  closest place exemplar  relative distance  retrieved closest place  omnidirectional view  powerful O-CNN  Omnidirectional CNN  virtual world datasets  real-world datasets  omnidirectional convolutional neural network  camera pose variation  Visualization  Navigation  Robots  Measurement  Cameras  Feature extraction  Task analysis 
Abstract: Visual place recognition is challenging, especially when only a few place exemplars are given. To mitigate the challenge, we consider place recognition method using omnidirectional cameras and propose a novel Omnidirectional Convolutional Neural Network (O-CNN) to handle severe camera pose variation. Given a visual input, the task of the O-CNN is not to retrieve the matched place exemplar, but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place. With the ability to estimate relative distance, a heuristic policy is proposed to navigate a robot to the retrieved closest place. Note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance. To train a powerful O-CNN, we build a virtual world for training on a large scale. We also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently. Finally, our experimental results confirm that our method achieves state-of-the-art accuracy and speed with both the virtual world and real-world datasets.


Title: Addressing Challenging Place Recognition Tasks Using Generative Adversarial Networks
Key Words: feature extraction  image recognition  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  visual perception  perception task  place recognition tasks  simultaneous localization and mapping  SLAM  coupled Generative Adversarial Networks  domain translation task  Task analysis  Gallium nitride  Lighting  Generators  Image recognition  Feature extraction  Visualization 
Abstract: Place recognition is an essential component of Simultaneous Localization And Mapping (SLAM). Under severe appearance change, reliable place recognition is a difficult perception task since the same place is perceptually very different in the morning, at night, or over different seasons. This work addresses place recognition as a domain translation task. Using a pair of coupled Generative Adversarial Networks (GANs), we show that it is possible to generate the appearance of one domain (such as summer) from another (such as winter) without requiring image-to-image correspondences across the domains. Mapping between domains is learned from sets of images in each domain without knowing the instance-to-instance correspondence by enforcing a cyclic consistency constraint. In the process, meaningful feature spaces are learned for each domain, the distances in which can be used for the task of place recognition. Experiments show that learned features correspond to visual similarity and can be effectively used for place recognition across seasons.


Title: A Deep Incremental Boltzmann Machine for Modeling Context in Robots
Key Words: Boltzmann machines  learning (artificial intelligence)  pattern classification  contextual model  context layer  scene classification benchmark  nonincremental models  deep incremental Boltzmann machine  robots  context modeling efforts  fixed structure  incremental deep model  Neurons  Context modeling  Hidden Markov models  Robots  Computational modeling  Adaptation models  Data models 
Abstract: Context is an essential capability for robots that are to be as adaptive as possible in challenging environments. Although there are many context modeling efforts, they assume a fixed structure and number of contexts. In this paper, we propose an incremental deep model that extends Restricted Boltzmann Machines. Our model gets one scene at a time, and gradually extends the contextual model when necessary, either by adding a new context or a new context layer to form a hierarchy. We show on a scene classification benchmark that our method converges to a good estimate of the contexts of the scenes, and performs better or on-par on several tasks compared to other incremental models or non-incremental models.


Title: Accelerating Model Learning with Inter-Robot Knowledge Transfer
Key Words: learning (artificial intelligence)  manipulator dynamics  multi-robot systems  robot programming  training transfer models  online learning  inverse dynamics model  model learning  inter-robot knowledge transfer  multirobot setting  trajectory tracking tasks  robot inverse dynamics model  tabula rasa learning  robot learning  Interbotix PhantomX Pincher arm  Kuka youBot arm  Adaptation models  Manipulator dynamics  Data models  Acceleration  Task analysis 
Abstract: Online learning of a robot's inverse dynamics model for trajectory tracking necessitates an interaction between the robot and its environment to collect training data. This is challenging for physical robots in the real world, especially for humanoids and manipulators due to their large and high dimensional state and action spaces, as a large amount of data must be collected over time. This can put the robot in danger when learning tabula rasa and can also be a time-intensive process especially in a multi-robot setting, where each robot is learning its model from scratch. We propose accelerating learning of the inverse dynamics model for trajectory tracking tasks in this multi-robot setting using knowledge transfer, where robots share and re-use data collected by preexisting robots, in order to speed up learning for new robots. We propose a scheme for collecting a sample of correspondences from the robots for training transfer models, and demonstrate, in simulations, the benefit of knowledge transfer in accelerating online learning of the inverse dynamics model between several robots, including between a low-cost Interbotix PhantomX Pincher arm, and a more expensive and relatively heavier Kuka youBot arm. We show that knowledge transfer can save up to 63% of training time of the youBot arm compared to learning from scratch, and about 58% for the lighter Pincher arm.


Title: Online Learning of a Memory for Learning Rates
Key Words: gradient methods  learning (artificial intelligence)  optimisation  pattern classification  learning rates  learning process  memory model  optimal learning rate landscape  task specific optimization  meta-learner  internal memory  optimization tasks  meta-learning algorithm speeds  learning control tasks  online learning settings  gradient behaviors  gradient-based optimizer  MNIST classification  Task analysis  Optimization  Prediction algorithms  Robots  Transforms  Computational modeling  Predictive models 
Abstract: The promise of learning to learn for robotics rests on the hope that by extracting some information about the learning process itself we can speed up subsequent similar learning tasks. Here, we introduce a computationally efficient online meta-learning algorithm that builds and optimizes a memory model of the optimal learning rate landscape from previously observed gradient behaviors. While performing task specific optimization, this memory of learning rates predicts how to scale currently observed gradients. After applying the gradient scaling our meta-learner updates its internal memory based on the observed effect its prediction had. Our meta-learner can be combined with any gradient-based optimizer, learns on the fly and can be transferred to new optimization tasks. In our evaluations we show that our meta-learning algorithm speeds up learning of MNIST classification and a variety of learning control tasks, either in batch or online learning settings.


Title: Learning Coupled Forward-Inverse Models with Combined Prediction Errors
Key Words: learning (artificial intelligence)  robots  multiple solutions  inverse space  forward models  paired forward-inverse models  multiple modules  local minima  training multiple models-that  monolithic complex network  efficient alternative  multiple simple models  complex models  unstructured environments  combined prediction errors  coupled forward-inverse models  Inverse problems  Computational modeling  Data models  Predictive models  Adaptation models  Robots  Context modeling 
Abstract: Challenging tasks in unstructured environments require robots to learn complex models. Given a large amount of information, learning multiple simple models can offer an efficient alternative to a monolithic complex network. Training multiple models-that is, learning their parameters and their responsibilities-has been shown to be prohibitively hard as optimization is prone to local minima. To efficiently learn multiple models for different contexts, we thus develop a new algorithm based on expectation maximization (EM). In contrast to comparable concepts, this algorithm trains multiple modules of paired forward-inverse models by using the prediction errors of both forward and inverse models simultaneously. In particular, we show that our method yields a substantial improvement over only considering the errors of the forward models on tasks where the inverse space contains multiple solutions.


Title: DEFO-NET: Learning Body Deformation Using Generative Adversarial Networks
Key Words: finite element analysis  image colour analysis  image reconstruction  mobile robots  robot vision  RGB-D image  finite element methods  mobile robots  single depth view  physical finite element model simulator  autonomous robots  Generative Adversarial networks  body deformation  DEFO-NET  Strain  Gallium nitride  Force  Deformable models  Robots  Training  Generators 
Abstract: Modelling the physical properties of everyday objects is a fundamental prerequisite for autonomous robots. We present a novel generative adversarial network (DEFO-NET), able to predict body deformations under external forces from a single RGB-D image. The network is based on an invertible conditional Generative Adversarial Network (IcGAN) and is trained on a collection of different objects of interest generated by a physical finite element model simulator. Defo-netinherits the generalisation properties of GANs. This means that the network is able to reconstruct the whole 3-D appearance of the object given a single depth view of the object and to generalise to unseen object configurations. Contrary to traditional finite element methods, our approach is fast enough to be used in real-time applications. We apply the network to the problem of safe and fast navigation of mobile robots carrying payloads over different obstacles and floor materials. Experimental results in real scenarios show how a robot equipped with an RGB-D camera can use the network to predict terrain deformations under different payload configurations and use this to avoid unsafe areas.


Title: Bodily Aware Soft Robots: Integration of Proprioceptive and Exteroceptive Sensors
Key Words: cameras  convolution  dexterous manipulators  mobile robots  object tracking  path planning  recurrent neural nets  sensors  bodily aware soft robots  exteroceptive sensors  proprioceptive sensors  bend sensors  visual sensor  nonlinearity  octopus-inspired arm  camera record  arm capturing  internal sensory signals  stacked convolutional autoencoder  CAE  recurrent neural network  RNN  motion  Convolution  Soft robotics  Robot sensing systems  Visualization  Recurrent neural networks 
Abstract: Being aware of our body has great importance in our everyday life. It helps us to complete difficult tasks, such as movement in a dark room or grasping a complex object. These skills are important for robots as well, however, robotic bodily awareness is still an open question, and the nonlinearity of soft robots adds even more complexity. In this paper, we address this problem and present a novel method to implement bodily awareness into a real soft robot by the integration of its exteroceptive and proprioceptive sensors. We use an octopus-inspired arm as an example where the proprioceptive representation is approximated by four bend sensors integrated into the soft body, while a camera records the movement of the arm capturing its exteroceptive representation. The internal sensory signals are mapped to the visual information using a combination of a stacked convolutional autoencoder (CAE) and a recurrent neural network (RNN). As a result, the soft robot can learn to estimate and, therefore, to imagine its motion even when its visual sensor is not available.


Title: Deep Learning a Quadrotor Dynamic Model for Multi-Step Prediction
Key Words: aircraft control  autonomous aerial vehicles  control engineering computing  helicopters  learning (artificial intelligence)  mobile robots  motion control  predictive control  recurrent neural nets  robot dynamics  robot kinematics  trajectory control  quadrotor dynamic model  motion prediction  dynamic systems  long horizons  deep learning  deep recurrent neural networks  quadrotor motion model  initial system state  motor speeds  prediction horizon  recurrent neural network state initialization  quadrotor vehicle flights  indoor flight arena  hybrid network architecture  system identification methods  robust state predictions  time 2.0 s  frequency 100.0 Hz  Mathematical model  Predictive models  Vehicle dynamics  Aerodynamics  Recurrent neural networks  Training 
Abstract: We develop a multi-step motion prediction modeling method for dynamic systems over long horizons using deep learning. Building on previous work, we propose a novel hybrid network architecture, by combining deep recurrent neural networks with a quadrotor motion model created using classic system identification methods. The proposed model takes only the initial system state and motor speeds over the prediction horizon as inputs and returns robust state predictions for up to two seconds of motion at 100 Hz. We employ recurrent neural network state initialization during training, to exploit real-world dataset collected from quadrotor vehicle flights in an indoor flight arena. Our experiments demonstrate that the proposed hybrid network model consistently outperforms both black box and rigid body dynamics predictions over single and multi-step prediction scenarios, with an order of magnitude improvements in velocity estimates in particular.


Title: Safe Learning of Quadrotor Dynamics Using Barrier Certificates
Key Words: aircraft control  autonomous aerial vehicles  control system synthesis  Gaussian processes  helicopters  learning systems  mobile robots  nonlinear control systems  probability  uncertain systems  complex dynamical systems  accurate nonlinear models  data-driven approach  Gaussian processes  learning process  barrier certificates  safe learning  learning controller  quadrotor dynamics  Safety  Control systems  Computational modeling  Gaussian processes  Adaptation models  Lyapunov methods  System dynamics 
Abstract: To effectively control complex dynamical systems, accurate nonlinear models are typically needed. However, these models are not always known. In this paper, we present a data-driven approach based on Gaussian processes that learns models of quadrotors operating in partially unknown environments. What makes this challenging is that if the learning process is not carefully controlled, the system will go unstable, i.e., the quadcopter will crash. To this end, barrier certificates are employed for safe learning. The barrier certificates establish a non-conservative forward invariant safe region, in which high probability safety guarantees are provided based on the statistics of the Gaussian Process. A learning controller is designed to efficiently explore those uncertain states and expand the barrier certified safe region based on an adaptive sampling scheme. Simulation results are provided to demonstrate the effectiveness of the proposed approach.


Title: A Framework for Sensorless Tissue Motion Tracking in Robotic Endomicroscopy Scanning
Key Words: biological tissues  biomedical optical imaging  endoscopes  image resolution  image segmentation  laser applications in medicine  medical image processing  medical robotics  optical microscopy  probe-based confocal laser endomicroscopy  robotic endomicroscopy scanning  image-quality metric  sensorless tissue motion tracking  ex vivo porcine tissue validate  autonomous endomicroscopy scanning  pCLE robotic tool  novel sensorless framework  sensorless approaches  endomicroscopy probe  robotic manipulation  tissue deformation  probe-tissue contact force  Probes  Tools  Force  Robot sensing systems  Strain 
Abstract: Recent advances in probe-based Confocal Laser Endomicroscopy (pCLE) enable real-time, in situ and in vivo tissue assessment at the micro scale. The limited field-of-view offered by pCLE necessitates the use of mosaicking to allow for accurate tissue characterization from the incoming image stream. However, mosaicking requires a series of contiguous good-quality images, which is particularly challenging because probe-tissue distance must be maintained within a very narrow working range at all times and probe-tissue contact force must be kept to a minimum so that tissue deformation is avoided. Robotic manipulation of the endomicroscopy probe has provided partial solution to these challenges, but sensorless approaches have not been thoroughly investigated up to date. This paper proposes a novel sensorless framework that uses a single non-reference image-quality metric to learn an approximation of tissue motion and subsequently track it. Moreover, a pCLE robotic tool for autonomous endomicroscopy scanning is designed and used for testing and validation purposes. Experiments on lens paper and ex vivo porcine tissue validate the philosophy of the framework.


Title: ViTac: Feature Sharing Between Vision and Tactile Sensing for Cloth Texture Recognition
Key Words: covariance analysis  feature extraction  image recognition  image texture  neural nets  touch (physiological)  tactile data  cloth textures  good recognition performance  perception performance  tactile sensing  shared representation space  feature sharing  cloth texture recognition  multimodal sensing ability  tactile images  Deep Maximum Covariance Analysis  learned features  DMCA framework  unimodal data  joint latent space  Gelsight sensor  deep neural networks  sensing modalities  Visualization  Tactile sensors  Cameras  Task analysis  Surface topography 
Abstract: Vision and touch are two of the important sensing modalities for humans and they offer complementary information for sensing the environment. Robots could also benefit from such multi-modal sensing ability. In this paper, addressing for the first time (to the best of our knowledge) texture recognition from tactile images and vision, we propose a new fusion method named Deep Maximum Covariance Analysis (DMCA) to learn a joint latent space for sharing features through vision and tactile sensing. The features of camera images and tactile data acquired from a GelSight sensor are learned by deep neural networks. But the learned features are of a high dimensionality and are redundant due to the differences between the two sensing modalities, which deteriorates the perception performance. To address this, the learned features are paired using maximum covariance analysis. Results of the algorithm on a newly collected dataset of paired visual and tactile data relating to cloth textures show that a good recognition performance of greater than 90% can be achieved by using the proposed DMCA framework. In addition, we find that the perception performance of either vision or tactile sensing can be improved by employing the shared representation space, compared to learning from unimodal data.


Title: Calibration and Analysis of Tactile Sensors as Slip Detectors
Key Words: calibration  force sensors  neural nets  slip  tactile sensors  long short-term memory neural networks  high-quality slip detection  tactile technologies  electro-mechanical resistance  sensing mechanics  tactile sensing  robust slip detectors  sensor behavior  sensory responses  spectral analysis  sensory data points  systematic data collection process  robust slip detection  tactile-based slip detection  secondary force modulation protocols  human hand  mechanical transients  tactile afferents  Force  Tactile sensors  Detectors  Robustness  Spectral analysis  tactile sensors  slip detection  neural networks  deep learning 
Abstract: The existence of tactile afferents sensitive to slip-related mechanical transients in the human hand augments the robustness of grasping through secondary force modulation protocols. Despite this knowledge and the fact that tactile-based slip detection has been researched for decades, robust slip detection is still not an out-of-the-box capability for any commercially available tactile sensor. This research seeks to bridge this gap with a comprehensive study addressing several aspects of slip detection. In particular, key developments include a systematic data collection process yielding millions of sensory data points, a spectral analysis of sensory responses providing insight into sensor behavior, and the application of Long Short-Term Memory (LSTM) neural networks to produce robust slip detectors from three commercially available sensors capable of tactile sensing. The sensing mechanics behind these sensors are all fundamentally different and leverage principles in electro-mechanical resistance, optics, and hydro-acoustics. Critically, slip detection performance of the tactile technologies is quantified through a measurement methodology that unveils the effects of data window size, sampling rate, material type, slip speed, and sensor manufacturing variability. Results indicate that the investigated commercial tactile sensors are inherently capable of high-quality slip detection.


Title: Learning Manipulation Graphs from Demonstrations Using Multimodal Sensory Signals
Key Words: dexterous manipulators  graph theory  learning (artificial intelligence)  mobile robots  tactile sensors  erroneous contact states  manipulation demonstrations  motor primitive  manipulation task  insertion tasks  learned manipulation graphs  robust manipulation executions  sensory goals  multimodal sensory signals  complex contact manipulation tasks  contact state  contact state information  Barrett arm  BioTacs  contact changes  Robot sensing systems  Task analysis  Fasteners  Trajectory  Motion segmentation  Vibrations 
Abstract: Complex contact manipulation tasks can be decomposed into sequences of motor primitives. Individual primitives often end with a distinct contact state, such as inserting a screwdriver tip into a screw head or loosening it through twisting. To achieve robust execution, the robot should be able to verify that the primitive's goal has been reached as well as disambiguate it from erroneous contact states. In this paper, we introduce and evaluate a framework to autonomously construct manipulation graphs from manipulation demonstrations. Our manipulation graphs include sequences of motor primitives for performing a manipulation task as well as corresponding contact state information. The sensory models for the contact states allow the robot to verify the goal of each motor primitive as well as detect erroneous contact changes. The proposed framework was experimentally evaluated on grasping, unscrewing, and insertion tasks on a Barrett arm and hand equipped with two BioTacs. The results of our experiments indicate that the learned manipulation graphs achieve more robust manipulation executions by confirming sensory goals as well as discovering and detecting novel failure modes.


Title: Robotizing Double-Bar Ankle-Foot Orthosis
Key Words: actuators  cables (mechanical)  gait analysis  iterative learning control  medical robotics  muscle  orthotics  patient rehabilitation  pneumatic actuators  position control  pulleys  shafts  springs (mechanical)  double-bar ankle-foot orthosis  post-stroke gait rehabilitation  double-bar AFO  rehabilitation facilities  pneumatic actuator  Bowden cable force-transmission system  modular joint system  Modular Exoskeletal Joint  MEJ  hollow shaft  AFO's pivot  Bowden cables  contraction forces  actuation scheme  Nested-cylinder Pneumatic Artificial Muscle system  PAM  ideal actuation system  exoskeletal robots  NcPAM houses  cable-tensioning spring  cable tension  cable stopper  ankle-joint trajectory tracking performances  integrated system  iterative learning control  Robots  Force  Actuators  Mechanical cables  Exoskeletons  Torque  Springs 
Abstract: This paper introduces an approach that robotizes an ankle-foot orthosis (AFO). In particular, toward post-stroke gait rehabilitation, we robotize a double-bar AFO, which is widely used in rehabilitation facilities, by newly designing a modular joint, a pneumatic actuator, and a Bowden cable force-transmission system. Our modular joint system, called the Modular Exoskeletal Joint (MEJ), has a hollow shaft for simple attachment to an AFO's pivot. We designed MEJ to compactly house an encoder that is built in a bearing in a pulley. We adopted Bowden cables to transmit contraction forces from an actuator to the MEJ. As an actuation scheme, we developed the Nested-cylinder Pneumatic Artificial Muscle (NcPAM) system. Even though PAMs are mechanically compliant and lightweight, they can still generate a large force. Therefore, they can provide an ideal actuation system for exoskeletal robots. The nested-cylinder in NcPAM houses a cable-tensioning spring to properly maintain small cable tension for passive movements and a cable stopper to connect the PAM and the cable for properly transmitting the large force generated by PAM. We show the ankle-joint trajectory tracking performances of this integrated system using iterative learning control.


Title: High-Level MLN-Based Approach for Spatial Context Disambiguation
Key Words: control engineering computing  inference mechanisms  learning (artificial intelligence)  Markov processes  mobile robots  probability  robot dynamics  sensor fusion  spatial context disambiguation  probabilistic MLN-based model  incomplete knowledge  High-level task planning  semantic spatial relations  robot dynamic  High-level MLN  MLN probabilistic reasoning  Robot sensing systems  Context modeling  Semantics  Probabilistic logic  Object recognition 
Abstract: In this paper, we propose a probabilistic MLN-based model for spatial context disambiguation. This model serves as a solution for the problem of incomplete knowledge in High-level task planning. By applying the state of the art MLN probabilistic reasoning such as MCSAT, we determine the concept class of the current spatial context of the robot and contribute by combining semantic spatial relations with observed data at different timesteps. The inherent uncertainty of robot dynamic environments makes the proposed approach suitable to deal with partial observability and sensing limitations of robots. Simulation experiments and evaluation results are presented to validate our model.


Title: Optimizing Simulations with Noise-Tolerant Structured Exploration
Key Words: digital simulation  fast Fourier transforms  finite difference methods  gradient methods  Hadamard transforms  Jacobian matrices  legged locomotion  Newton method  optimisation  rendering (computer graphics)  Walsh functions  trajectory optimizers  noisy dynamics  quasiNewton optimizer  turning policies  noise-tolerant structured exploration  blackbox optimization  parameter perturbation directions  structured orthogonal matrices  structured finite differences  continuous control tasks  agile walking learning  Fast Walsh-Hadamard Fourier Transform  FWHT FFT  drop-in noise-tolerant replacement  quadruped locomotion  deep reinforcement learning  Mujoco simulator  3D renderers  Perturbation methods  Optimization  Standards  Smoothing methods  Robots  Jacobian matrices  Task analysis 
Abstract: We propose a simple drop-in noise-tolerant replacement for the standard finite difference procedure used ubiquitously in blackbox optimization. In our approach, parameter perturbation directions are defined by a family of structured orthogonal matrices. We show that at the small cost of computing a Fast Walsh-Hadamard/Fourier Transform (FWHT/FFT), such structured finite differences consistently give higher quality approximation of gradients and Jacobians in comparison to vanilla approaches that use coordinate directions or random Gaussian perturbations. We find that trajectory optimizers like Iterative LQR and Differential Dynamic Programming require fewer iterations to solve several classic continuous control tasks when our methods are used to linearize noisy, blackbox dynamics instead of standard finite differences. By embedding structured exploration in a quasi-Newton optimizer (LBFGS), we are able to learn agile walking and turning policies for quadruped locomotion, that successfully transfer from simulation to actual hardware. We theoretically justify our methods via bounds on the quality of gradient reconstruction and provide a basis for applying them also to nonsmooth problems.


Title: Using a Memory of Motion to Efficiently Warm-Start a Nonlinear Predictive Controller
Key Words: autonomous aerial vehicles  iterative methods  learning (artificial intelligence)  nonlinear control systems  optimal control  optimisation  path planning  predictive control  sampling methods  trajectory optimisation (aerospace)  direct optimal control  control policy  optimal state-control trajectories  nonlinear predictive controller  nonlinear optimization problem  model-based methodology  control cycle  kinodynamic probabilistic roadmap  nonlinear solver  unmanned aerial vehicle  UAV  complex dynamical systems  sampling-based planning  policy learning  Computational modeling  Approximation algorithms  Optimal control  Planning  Robots  Trajectory optimization 
Abstract: Predictive control is an efficient model-based methodology to control complex dynamical systems. In general, it boils down to the resolution at each control cycle of a large nonlinear optimization problem. A critical issue is then to provide a good guess to initialize the nonlinear solver so as to speed up convergence. This is particularly important when disturbances or changes in the environment prevent the use of the trajectory computed at the previous control cycle as initial guess. In this paper, we introduce an original and very efficient solution to automatically build this initial guess. We propose to rely on off-line computation to build an approximation of the optimal trajectories, that can be used on-line to initialize the predictive controller. To that end, we combined the use of sampling-based planning, policy learning with generic representations (such as neural networks), and direct optimal control. We first propose an algorithm to simultaneously build a kinodynamic probabilistic roadmap (PRM) and approximate value function and control policy. This algorithm quickly converges toward an approximation of the optimal state-control trajectories (along with an optimal PRM). Then, we propose two methods to store the optimal trajectories and use them to initialize the predictive controller. We experimentally show that directly storing the state-control trajectories leads the predictive controller to quickly converges (2 to 5 iterations) toward the (global) optimal solution. The results are validated in simulation with an unmanned aerial vehicle (UAV) and other dynamical systems.


Title: Goal Directed Dynamics
Key Words: control engineering computing  end effectors  humanoid robots  learning (artificial intelligence)  manipulator dynamics  motion control  quadratic programming  robot dynamics  telerobotics  forward dynamics  greedy optimization  feature-based control  control policies  trajectory optimization  goal directed dynamics  general control framework  low-level optimizer  robot dynamics  dynamical system  high level command  cost function  end-effector poses  soft-constraint physics model  quadratic programming framework  teleoperation  MuJoCo simulator  Acceleration  Robots  Force  Cost function  Dynamics 
Abstract: We develop a general control framework where a low-level optimizer is built into the robot dynamics. This optimizer together with the robot constitute a goal directed dynamical system, controlled on a higher level. The high level command is a cost function. It can encode desired accelerations, end-effector poses, center of pressure, and other intuitive features that have been studied before. Unlike the currently popular quadratic programming framework, which comes with performance guarantees at the expense of modeling flexibility, the optimization problem we solve at each time step is non-convex and non-smooth. Nevertheless, by exploiting the unique properties of the soft-constraint physics model we have recently developed, we are able to design an efficient solver for goal directed dynamics. It is only two times slower than the forward dynamics solver, and is much faster than real time. The simulation results reveal that complex movements can be generated via greedy optimization of simple costs. This new computational infrastructure can facilitate teleoperation, feature-based control, deep learning of control policies, and trajectory optimization. It will become a standard feature in future releases of the MuJoCo simulator.


Title: Acceleration of Gradient-Based Path Integral Method for Efficient Optimal and Inverse Optimal Control
Key Words: convergence of numerical methods  gradient methods  learning (artificial intelligence)  optimal control  optimisation  predictive control  gradient-based path integral method  inverse optimal control  accelerated path integral method  gradient descent  iterative path integral method  optimization methods  momentum-based acceleration  momentum-based methods  Nesterov Accelerated Gradient  simulated control systems  model predictive control  path integral networks  accelerated PI-Net  reinforcement learning  Iterative methods  Acceleration  Optimal control  Convergence  Mirrors  Trajectory  Vehicle dynamics 
Abstract: This paper deals with a new accelerated path integral method, which iteratively searches optimal controls with a small number of iterations. This study is based on the recent observations that a path integral method for reinforcement learning can be interpreted as gradient descent. This observation also applies to an iterative path integral method for optimal control, which sets a convincing argument for utilizing various optimization methods for gradient descent, such as momentum-based acceleration, step-size adaptation and their combination. We introduce these types of methods to the path integral and demonstrate that momentum-based methods, like Nesterov Accelerated Gradient and Adam, can significantly improve the convergence rate to search for optimal controls in simulated control systems. We also demonstrate that the accelerated path integral could improve the performance on model predictive control for various vehicle navigation tasks. Finally, we represent this accelerated path integral method as a recurrent network, which is the accelerated version of the previously proposed path integral networks (PI-Net). We can train the accelerated PI-Net more efficiently for inverse optimal control with less RAM than the original PI-Net.


Title: Simultaneous Planning and Estimation Based on Physics Reasoning in Robot Manipulation
Key Words: manipulators  multi-robot systems  path planning  physics reasoning  robot manipulation  manipulation planning  human-robot cooperation  multi-robot cooperation  Planning  Cognition  Estimation  Force  Robot sensing systems 
Abstract: For robots to autonomously achieve manipulation tasks in various scenes, advanced operational skills such as tool use, learning from demonstration, and multi-robot/human-robot cooperation are necessary. In this research, we devise a method for robots to realize such operational skills in a unified manner by evaluating physical consistency (referred to as “physics reasoning”) based on the formulation of the manipulation statics constraints. First, we propose manipulation planning and estimation methods in which the operational feasibility and properties' likelihood are derived by physics reasoning. In addition, we propose a framework to manipulate an object with unknown physical properties by executing planning and estimation both sequentially and in parallel. We demonstrate the effectiveness of the proposed methods by performing experiments in which real humanoid robots achieve various manipulation tasks with advanced operational skills.


Title: Learning Modes of Within-Hand Manipulation
Key Words: actuators  control engineering computing  feature extraction  image classification  learning (artificial intelligence)  manipulators  motion control  robot vision  tactile sensors  learning modes  prehensile fingertip-based within-hand manipulation  tactile sensors  actuator states  visual data  supervised learning techniques  classification performance  Extra Trees  Gradient Boosting  visual features  classification rate  actuator loads  within-hand manipulation movements  hand/object system  Actuators  Visualization  Task analysis  Robot sensing systems  Grippers  Feature extraction 
Abstract: In this work, we investigate methods to detect four phenomena (modes) that occur during prehensile fingertip-based within-hand manipulation without the use of tactile sensors. By using actuator states and visual data, we aim to recognize different modes of operation such as interpreting if the hand is about to drop the object, if the object will begin to slide on the fingers, or if the system is at or near a singularity. For this purpose, we utilize supervised learning techniques, which allow us to detect the modes without the use of a mechanical model of the system. We analyze the individual roles of specific features available through both the actuator and visual data, and identify the ones that have the most significance for detecting the operation modes. Our results show classification performance of 96% (using either Extra Trees, Gradient Boosting, or SVM) when using combined actuator and visual features. Interestingly, we were able to achieve a 94% classification rate using only actuator information, and 93 % using only visual information. Overall, the classifiers identified actuator positions, actuator loads, and commanded velocities as the most important features for detecting a mode. These results have implications for enabling the control of within-hand manipulation movements utilizing a minimal amount of sensory information without a model of the hand/object system.


Title: Cost Functions to Specify Full-Body Motion and Multi-Goal Manipulation Tasks
Key Words: control engineering computing  evolutionary computation  gradient methods  humanoid robots  learning (artificial intelligence)  manipulator kinematics  motion control  operating systems (computers)  particle swarm optimisation  public domain software  robot programming  trees (mathematics)  full-body motion generation  open-source software package  inverse kinematics  arbitrary kinematic trees  evolutionary optimization  particle swarm optimization  cost functions  multigoal manipulation tasks  serial kinematic chains  dual-arm manipulation  multifinger hands  memetic algorithm  full-body motion specification  ROS  MoveIt!  Kinematics  Task analysis  Cost function  Robot kinematics  End effectors  Quaternions 
Abstract: While the problem of inverse kinematics on serial kinematic chains is well researched, solving motion tasks quickly on more complex robots remains an open problem. Examples include dual-arm manipulation, grasping with multi-finger hands, and full-body motion generation for humanoids. In this paper, we introduce an open-source software package for ROS and MoveIt! that solves inverse kinematics and motion tasks on robots with arbitrary kinematic trees. The underlying memetic algorithm integrates evolutionary optimization, particle swarm optimization, and gradient methods. The optimization respects joint limits, effectively avoids local minima, and achieves fast convergence to accurate solutions. More importantly, the overall motion goal is specified using a set of weighted sub-goals, providing great flexibility and control of secondary objectives. Several application examples demonstrate how to combine the predefined sub-goals to achieve complex motion tasks.


Title: Robot Composite Learning and the Nunchaku Flipping Challenge
Key Words: humanoid robots  human-robot interaction  learning (artificial intelligence)  mobile robots  robot dynamics  heavily case-specific engineering  ubiquitous manner  human demonstration  LfD  dynamic skills  composite learning scheme  human definition  advanced motor skills  dynamic time-critical maneuver  complex contact control  partly soft partly rigid objects  nunchaku flipping challenge  physical success  robot composite learning  robot dynamics  hyper robot motor capabilities  robot learning  Petri nets  Robot learning  Mobile robots  Compounds  Dynamics  Real-time systems 
Abstract: Advanced motor skills are essential for robots to physically coexist with humans. Much research on robot dynamics and control has achieved success on hyper robot motor capabilities, but mostly through heavily case-specific engineering. Meanwhile, in terms of robot acquiring skills in a ubiquitous manner, robot learning from human demonstration (LfD) has achieved great progress, but still has limitations handling dynamic skills and compound actions. We present a composite learning scheme which goes beyond LfD and integrates robot learning from human definition, demonstration, and evaluation. The method tackles advanced motor skills that require dynamic time-critical maneuver, complex contact control, and handling partly soft partly rigid objects. We also introduce the “nunchaku flipping challenge”, an extreme test that puts hard requirements to all these three aspects. Continued from our previous presentations, this paper introduces the latest update of the composite learning scheme and the physical success of the nunchaku flipping challenge.


Title: Iterative Learning Scheme for Dexterous In-Hand Manipulation with Stochastic Uncertainty
Key Words: dexterous manipulators  gradient methods  iterative learning control  stochastic processes  stochastic systems  uncertain systems  dexterous manipulation tasks  model-based approaches  stochastic uncertainty  iterative learning scheme  adaptive learning rate methods  dexterous in-hand manipulation  gradient descent-based iterative learning control  Uncertainty  Stochastic processes  Robots  Torque  Cost function  Noise measurement  Robustness 
Abstract: In-hand manipulation has attracted attention because of its potential for performing dexterous manipulation tasks. Few successful examples using real robotic fingers have been reported because model-based approaches have been assumed. A gradient descent-based iterative learning control is one of the typical methods for improving the control performance without the need for a precise model. However, the learning performances deteriorate greatly owing to the stochastic uncertainties, and the learning rates have to be determined manually. We propose a novel iterative learning scheme with adaptive learning rate methods for dexterous in-hand manipulation. The proposed scheme not only eliminates the need for a precise model and manual tuning of a learning rate but also is robust to stochastic uncertainties and insensitive to hyperparameters. The validity of the proposed iterative learning scheme is demonstrated through several experiments.


Title: Extrinsic Dexterity Through Active Slip Control Using Deep Predictive Models
Key Words: dexterous manipulators  end effectors  grippers  learning (artificial intelligence)  slip  tactile sensors  extrinsic dexterity  active slip control  machine learning methodology  robot dexterity  recent insights  deep learning  tactile sensor information  manipulated object  robot end-effector  deep predictive models  Grippers  Training  Robot sensing systems  Predictive models  Acceleration 
Abstract: We present a machine learning methodology for actively controlling slip, in order to increase robot dexterity. Leveraging recent insights in deep learning, we propose a Deep Predictive Model that uses tactile sensor information to reason about slip and its future influence on the manipulated object. The obtained information is then used to precisely manipulate objects within a robot end-effector using external perturbations imposed by gravity or acceleration. We show in a set of experiments that this approach can be used to increase a robot's repertoire of motor skills.


Title: Meshed Up: Learnt Error Correction in 3D Reconstructions
Key Words: error correction  feature extraction  image reconstruction  inverse problems  learning (artificial intelligence)  stereo image processing  3D reconstruction accuracy  sensors  laser reconstruction  deep network architecture  stereo image reconstruction  two dimensional inverse-depth image extraction  2D inverse-depth image extraction  RMSE  depth reconstructions  depth estimate errors  machine learning technique  learnt error correction  Image reconstruction  Feature extraction  Cameras  Three-dimensional displays  Lasers  Image color analysis  Training 
Abstract: Dense reconstructions often contain errors that prior work has so far minimised using high quality sensors and regularising the output. Nevertheless, errors still persist. This paper proposes a machine learning technique to identify errors in three dimensional (3D) meshes. Beyond simply identifying errors, our method quantifies both the magnitude and the direction of depth estimate errors when viewing the scene. This enables us to Improve the reconstruction accuracy. We train a suitably deep network architecture with two 3D meshes: a high-quality laser reconstruction, and a lower quality stereo image reconstruction. The network predicts the amount of error in the lower quality reconstruction with respect to the high-quality one, having only view the former through its input. We evaluate our approach by correcting two dimensional (2D) inverse-depth images extracted from the 3D model, and show that our method improves the quality of these depth reconstructions by up to a relative 10% RMSE.


Title: SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes
Key Words: geometry  image colour analysis  image representation  image segmentation  learning (artificial intelligence)  object detection  joint geometric  object segmentation  indoor scenes  unseen objects  nonobject surfaces  scene semantics  scene surfaces  unified energy function  hierarchical segmentation trees  RGB-D image  deep learning-based methods  SceneCut  convolutional oriented boundary network  Image segmentation  Semantics  Silicon  Object segmentation  Robots  Training  Three-dimensional displays 
Abstract: This paper presents SceneCut, a novel approach to jointly discover previously unseen objects and non-object surfaces using a single RGB-D image. SceneCut's joint reasoning over scene semantics and geometry allows a robot to detect and segment object instances in complex scenes where modern deep learning-based methods either fail to separate object instances, or fail to detect objects that were not seen during training. SceneCut automatically decomposes a scene into meaningful regions which either represent objects or scene surfaces. The decomposition is qualified by an unified energy function over objectness and geometric fitting. We show how this energy function can be optimized efficiently by utilizing hierarchical segmentation trees. Moreover, we leverage a pre-trained convolutional oriented boundary network to predict accurate boundaries from images, which are used to construct high-quality region hierarchies. We evaluate SceneCut on several different indoor environments, and the results show that SceneCut significantly outperforms all the existing methods.


Title: Label Fusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes
Key Words: image colour analysis  image fusion  image reconstruction  image representation  image segmentation  learning (artificial intelligence)  neural net architecture  object detection  object tracking  pose estimation  robot vision  video cameras  video signal processing  reconstruction techniques  ground truth label generation  labeled object instances  object pose  video scene collection  annotation pipeline  DNN architecture  RGBD image  object meshes  human assisted ICP-fitting  3D dense reconstruction  RGBD camera  pixelwise labels  specific robotic manipulation task  training data  DNN pipelines  object segmentation  deep neural network architectures  cluttered scenes  real RGBD data  label fusion  Pipelines  Three-dimensional displays  Robot sensing systems  Image segmentation  Cameras  Image reconstruction 
Abstract: Deep neural network (DNN) architectures have been shown to outperform traditional pipelines for object segmentation and pose estimation using RGBD data, but the performance of these DNN pipelines is directly tied to how representative the training data is of the true data. Hence a key requirement for employing these methods in practice is to have a large set of labeled data for your specific robotic manipulation task, a requirement that is not generally satisfied by existing datasets. In this paper we develop a pipeline to rapidly generate high quality RGBD data with pixelwise labels and object poses. We use an RGBD camera to collect video of a scene from multiple viewpoints and leverage existing reconstruction techniques to produce a 3D dense reconstruction. We label the 3D reconstruction using a human assisted ICP-fitting of object meshes. By reprojecting the results of labeling the 3D scene we can produce labels for each RGBD image of the scene. This pipeline enabled us to collect over 1,000,000 labeled object instances in just a few days. We use this dataset to answer questions related to how much training data is required, and of what quality the data must be, to achieve high performance from a DNN architecture. Our dataset and annotation pipeline are available at labelfusion.csail.mit.edu.


Title: Dropout Sampling for Robust Object Detection in Open-Set Conditions
Key Words: approximation theory  Bayes methods  image classification  inference mechanisms  learning (artificial intelligence)  mobile robots  object detection  regression analysis  robot vision  sampling methods  dropout sampling network  dropout variational inference  approximation technique  Bayesian deep learning  mobile robot  versatile campus environment  robotic vision  regression tasks  image classification  open-set conditions  robust object detection  Object detection  Uncertainty  Training  Bayes methods  Entropy  Task analysis  Robots 
Abstract: Dropout Variational Inference, or Dropout Sampling, has been recently proposed as an approximation technique for Bayesian Deep Learning and evaluated for image classification and regression tasks. This paper investigates the utility of Dropout Sampling for object detection for the first time. We demonstrate how label uncertainty can be extracted from a state-of-the-art object detection system via Dropout Sampling. We evaluate this approach on a large synthetic dataset of 30,000 images, and a real-world dataset captured by a mobile robot in a versatile campus environment. We show that this uncertainty can be utilized to increase object detection performance under the open-set conditions that are typically encountered in robotic vision. A Dropout Sampling network is shown to achieve a 12.3 % increase in recall (for the same precision score as a standard network) and a 15.1 % increase in precision (for the same recall score as the standard network).


Title: Learning Human Ergonomic Preferences for Handovers
Key Words: ergonomics  human-robot interaction  manipulators  human ergonomic preferences  handovers  robots  ergonomic human grasping configurations  ergonomic cost function  online estimation problem  in-person user study  Ergonomics  Handover  Cost function  Training  Manipulators 
Abstract: Our goal is for people to be physically comfortable when taking objects from robots. This puts a burden on the robot to hand over the object in such a way that a person can easily reach it, without needing to strain or twist their arm - a way that is conducive to ergonomic human grasping configurations. To achieve this, the robot needs to understand what makes a configuration more or less ergonomic to the person, i.e. their ergonomic cost function. In this work, we formulate learning a person's ergonomic cost as an online estimation problem. The robot can implicitly make queries to the person by handing them objects in different configurations, and gets observations in response about the way they choose to take the object. We compare the performance of both passive and active approaches for solving this problem in simulation, as well as in an in-person user study.


Title: Joining High-Level Symbolic Planning with Low-Level Motion Primitives in Adaptive HRI: Application to Dressing Assistance
Key Words: control engineering computing  footwear  human-robot interaction  learning (artificial intelligence)  mobile robots  planning (artificial intelligence)  robot programming  service robots  daily living assistance  robot motion encoding  programming  shoe-dressing scenario  robot verbosity  robot speed  safe living assistance  dressing assistance  adaptive HRI  low-level motion primitives  high-level symbolic planning  user preferences  human-robot interaction  Task analysis  Planning  Robot sensing systems  Footwear  Foot  Computational modeling 
Abstract: For a safe and successful daily living assistance, far from the highly controlled environment of a factory, robots should be able to adapt to ever-changing situations. Programming such a robot is a tedious process that requires expert knowledge. An alternative is to rely on a high-level planner, but the generic symbolic representations used are not well suited to particular robot executions. Contrarily, motion primitives encode robot motions in a way that can be easily adapted to different situations. This paper presents a combined framework that exploits the advantages of both approaches. The number of required symbolic states is reduced, as motion primitives provide “smart actions” that take the current state and cope online with variations. Symbolic actions can include interactions (e.g., ask and inform) that are difficult to demonstrate. We show that the proposed framework can adapt to the user preferences (in terms of robot speed and robot verbosity), can readjust the trajectories based on the user movements, and can handle unforeseen situations. Experiments are performed in a shoe-dressing scenario. This scenario is particularly interesting because it involves a sufficient number of actions, and the human-robot interaction requires the handling of user preferences and unexpected reactions.


Title: Training Deep Neural Networks for Visual Servoing
Key Words: feedforward neural nets  learning (artificial intelligence)  pose estimation  position control  robot vision  servomechanisms  visual servoing  6 DOF robot  deep neural network-based method  convolutional neural network  robust handling  scene-agnostic network  deep neural network training  real-time 6 DOF positioning  pose-based visual servoing control law  occlusions  lighting variations  Training  Robots  Feature extraction  Task analysis  Cameras  Voltage control  Robustness 
Abstract: We present a deep neural network-based method to perform high-precision, robust and real-time 6 DOF positioning tasks by visual servoing. A convolutional neural network is fine-tuned to estimate the relative pose between the current and desired images and a pose-based visual servoing control law is considered to reach the desired pose. The paper describes how to efficiently and automatically create a dataset used to train the network. We show that this enables the robust handling of various perturbations (occlusions and lighting variations). We then propose the training of a scene-agnostic network by feeding in both the desired and current images into a deep network. The method is validated on a 6 DOF robot.


Title: SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Control
Key Words: cameras  closed loop systems  control engineering computing  gradient methods  image colour analysis  image segmentation  industrial robots  learning (artificial intelligence)  minimisation  neural nets  pose estimation  robot dynamics  robot vision  SE3-pose-Nets  deep visuomotor control  SE3-Nets  encoder-decoder structure  pose embedding  point-wise data associations  closed-loop control  scene dynamics  structred deep dynamics models  pose error minimization  gradient-based methods  Baxter robot  Three-dimensional displays  Predictive models  Transforms  Computational modeling  Data models  Aerospace electronics  Training 
Abstract: In this work, we present an approach to deep visuomotor control using structured deep dynamics models. Our model, a variant of SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure. Unlike prior work, our model is structured: given an input scene, our network explicitly learns to segment salient parts and predict their pose embedding and motion, modeled as a change in the pose due to the applied actions. We train our model using a pair of point clouds separated by an action and show that given supervision only through point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses. We further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space, where the actions are computed by minimizing pose error using gradient-based methods, similar to traditional model-based control. We present results on controlling a Baxter robot from raw depth data in simulation and RGBD data in the real world and compare against two baseline deep networks. We also test the robustness and generalization performance of our controller under changes in camera pose, lighting, occlusion, and motion. Our method is robust, runs in real-time, achieves good prediction of scene dynamics, and outperforms baselines on multiple control runs. Video results can be found at: https://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/.


Title: Fast Object Learning and Dual-arm Coordination for Cluttered Stowing, Picking, and Packing
Key Words: grippers  humanoid robots  human-robot interaction  industrial manipulators  learning (artificial intelligence)  mobile robots  multi-robot systems  object detection  path planning  robot vision  service robots  robotic picking  cluttered bins  2017 Amazon Robotics Challenge  ARC  storage system  deep object perception pipeline  custom turntable capture system  transfer learning  robot arms  NimbRo Picking  stow-and-pick task  Task analysis  Training  Robot kinematics  Pipelines  Robot sensing systems  Semantics 
Abstract: Robotic picking from cluttered bins is a demanding task, for which Amazon Robotics holds challenges. The 2017 Amazon Robotics Challenge (ARC) required stowing items into a storage system, picking specific items, and packing them into boxes. In this paper, we describe the entry of team NimbRo Picking. Our deep object perception pipeline can be quickly and efficiently adapted to new items using a custom turntable capture system and transfer learning. It produces high-quality item segments, on which grasp poses are found. A planning component coordinates manipulation actions between two robot arms, minimizing execution time. The system has been demonstrated successfully at ARC, where our team reached second places in both the picking task and the final stow-and-pick task. We also evaluate individual components.


Title: Modeling Driver Behavior from Demonstrations in Dynamic Environments Using Spatiotemporal Lattices
Key Words: collision avoidance  driver information systems  learning (artificial intelligence)  mobile robots  road vehicles  spatiotemporal lattices  path planners  intelligent vehicles  cost function  model parameters  demonstrated driving data  Inverse Reinforcement  IRL methods  forward control problem  traditional path-planning techniques  conformal spatiotemporal state lattice  dynamic obstacles  model assessment  IRL framework  highly dynamic environments  highway tactical driving task  instrumented vehicle  driver behavior modeling  Trajectory  Lattices  Task analysis  Spatiotemporal phenomena  Vehicle dynamics  Learning (artificial intelligence)  Cost function 
Abstract: One of the most challenging tasks in the development of path planners for intelligent vehicles is the design of the cost function that models the desired behavior of the vehicle. While this task has been traditionally accomplished by hand-tuning the model parameters, recent approaches propose to learn the model automatically from demonstrated driving data using Inverse Reinforcement Learning (IRL). To determine if the model has correctly captured the demonstrated behavior, most IRL methods require obtaining a policy by solving the forward control problem repetitively. Calculating the full policy is a costly task in continuous or large domains and thus often approximated by finding a single trajectory using traditional path-planning techniques. In this work, we propose to find such a trajectory using a conformal spatiotemporal state lattice, which offers two main advantages. First, by conforming the lattice to the environment, the search is focused only on feasible motions for the robot, saving computational power. And second, by considering time as part of the state, the trajectory is optimized with respect to the motion of the dynamic obstacles in the scene. As a consequence, the resulting trajectory can be used for the model assessment. We show how the proposed IRL framework can successfully handle highly dynamic environments by modeling the highway tactical driving task from demonstrated driving data gathered with an instrumented vehicle.


Title: Multimodal Probabilistic Model-Based Planning for Human-Robot Interaction
Key Words: decision making  human-robot interaction  intelligent transportation systems  learning (artificial intelligence)  probability  highway on-ramp-off-ramps  human-robot interaction policies  multimodal probabilistic model-based planning  traffic weaving scenario  human-in-the-loop simulation  candidate future robot actions  interaction history  action distributions  direct learning  candidate robot action sequences  human responses  massively parallel sampling  real-time robot policy construction  human-human exemplars  future human actions  multimodal probability distributions  inherent multimodal uncertainty  experienced drivers  entering exiting cars  decision making  Robots  Vehicles  Predictive models  History  Cognition  Probabilistic logic  Weaving 
Abstract: This paper presents a method for constructing human-robot interaction policies in settings where multimodality, i.e., the possibility of multiple highly distinct futures, plays a critical role in decision making. We are motivated in this work by the example of traffic weaving, e.g., at highway on-ramps/off-ramps, where entering and exiting cars must swap lanes in a short distance-a challenging negotiation even for experienced drivers due to the inherent multimodal uncertainty of who will pass whom. Our approach is to learn multimodal probability distributions over future human actions from a dataset of human-human exemplars and perform real-time robot policy construction in the resulting environment model through massively parallel sampling of human responses to candidate robot action sequences. Direct learning of these distributions is made possible by recent advances in the theory of conditional variational autoencoders (CVAEs), whereby we learn action distributions simultaneously conditioned on the present interaction history, as well as candidate future robot actions in order to take into account response dynamics. We demonstrate the efficacy of this approach with a human-in-the-loop simulation of a traffic weaving scenario.


Title: Drive Video Analysis for the Detection of Traffic Near-Miss Incidents
Key Words: automobiles  driver information systems  learning (artificial intelligence)  object detection  road safety  road traffic  video signal processing  traffic near-miss incidents  self-driving cars  advanced driver assistance system equipped vehicles  dangerous traffic  normal drivers  novel traffic database  mounting driving recorders  automated systems  database instances  large-scale traffic near-miss incident database  monocular driving recorder  NIDB traffic  primary database-related improvements  near-miss scenes  near-miss detection  drive video analysis  near-miss incident  motion representation  performance level  Databases  Vehicles  Autonomous automobiles  Semantics  Advanced driver assistance systems  Public transportation  Training 
Abstract: Because of their recent introduction, self-driving cars and advanced driver assistance system (ADAS) equipped vehicles have had little opportunity to learn, the dangerous traffic (including near-miss incident) scenarios that provide normal drivers with strong motivation to drive safely. Accordingly, as a means of providing learning depth, this paper presents a novel traffic database that contains information on a large number of traffic near-miss incidents that were obtained by mounting driving recorders in more than 100 taxis over the course of a decade. The study makes the following two main contributions: (i) In order to assist automated systems in detecting near-miss incidents based on database instances, we created a large-scale traffic near-miss incident database (NIDB) that consists of video clip of dangerous events captured by monocular driving recorders. (ii) To illustrate the applicability of NIDB traffic near-miss incidents, we provide two primary database-related improvements: parameter fine-tuning using various near-miss scenes from NIDB, and foreground/background separation into motion representation. Then, using our new database in conjunction with a monocular driving recorder, we developed a near-miss recognition method that provides automated systems with a performance level that is comparable to a human-level understanding of near-miss incidents (64.5% vs. 68.4% at near-miss recognition, 61.3% vs. 78.7% at near-miss detection).


Title: Teach-and-Replay of Mobile Robot with Particle Filter on Episode
Key Words: learning (artificial intelligence)  mobile robots  particle filtering (numerical methods)  robust control  mobile robot  reinforcement learning method  task teaching  micromouse type robot  Teach-and-Replay  PFoE  particle filter on episode  Robot sensing systems  Robot kinematics  Education  Hidden Markov models  Mobile robots  Task analysis 
Abstract: A novel method for replaying behavior of a mobile robot from its memory of past experiences is presented in this paper. The method is a version of a particle filter on episode (PFoE), which applies a particle filter on the memory so as to efficiently find some similar situations with the current one. Though the original PFoE was proposed as a reinforcement learning method, we once removed the reward system from the original one so as to apply it to task teaching. In the experiment, we gave several kinds of motion to a micromouse type robot with the proposed method through a gamepad. The robot replayed the behaviors robustly with sensor feedback after several number of repetitive teaching.


Title: Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation
Key Words: feature extraction  image segmentation  learning (artificial intelligence)  manipulators  neural nets  robot vision  multitask domain adaptation  deep learning  successful grasping probability  transfer learning framework  domain-adversarial loss  candidate motor command  specified target object  instance segmentation mask  monocular RGB images  neural network  cluttered scenes  instance grasping  robotic manipulation  Grasping  Robots  Adaptation models  Data models  Feature extraction  Image segmentation  Neural networks 
Abstract: Learning-based approaches to robotic manipulation are limited by the scalability of data collection and accessibility of labels. In this paper, we present a multi-task domain adaptation framework for instance grasping in cluttered scenes by utilizing simulated robot experiments. Our neural network takes monocular RGB images and the instance segmentation mask of a specified target object as inputs, and predicts the probability of successfully grasping the specified object for each candidate motor command. The proposed transfer learning framework trains a model for instance grasping in simulation and uses a domain-adversarial loss to transfer the trained model to real robots using indiscriminate grasping data, which is available both in simulation and the real world. We evaluate our model in real-world robot experiments, comparing it with alternative model architectures as well as an indiscriminate grasping baseline.


Title: Learning Robotic Assembly from CAD
Key Words: CAD  industrial manipulators  learning (artificial intelligence)  mobile robots  path planning  production engineering computing  robotic assembly  suboptimal control  autonomous robotic assembly  industrial assembly tasks  contact-rich manipulation skills  motion planning approaches  robot controllers  reinforcement learning  robot skills  contact-rich dynamics  control policy  robot executions  locally suboptimal solutions  RL performance  CAD design files  geometric motion plan  CAD data  assembly controller  manufacturing trends  Task analysis  Planning  Robots  Trajectory  Tracking  Robotic assembly  Dynamics 
Abstract: In this work, motivated by recent manufacturing trends, we investigate autonomous robotic assembly. Industrial assembly tasks require contact-rich manipulation skills, which are challenging to acquire using classical control and motion planning approaches. Consequently, robot controllers for assembly domains are presently engineered to solve a particular task, and cannot easily handle variations in the product or environment. Reinforcement learning (RL) is a promising approach for autonomously acquiring robot skills that involve contact-rich dynamics. However, RL relies on random exploration for learning a control policy, which requires many robot executions, and often gets trapped in locally suboptimal solutions. Instead, we posit that prior knowledge, when available, can improve RL performance. We exploit the fact that in modern assembly domains, geometric information about the task is readily available via the CAD design files. We propose to leverage this prior knowledge by guiding RL along a geometric motion plan, calculated using the CAD data. We show that our approach effectively improves over traditional control approaches for tracking the motion plan, and can solve assembly tasks that require high precision, even without accurate state estimation. In addition, we propose a neural network architecture that can learn to track the motion plan, thereby generalizing the assembly controller to changes in the object positions.


Title: A General and Flexible Search Framework for Disassembly Planning
Key Words: assembly planning  design for disassembly  iterative methods  search problems  iterative motion planning  collision information  subassembly identification  preemptive scheme  exhaustive scheme  search strategies  hierarchical approach  disassembly sequence planning  parallelism  part separation techniques  Planning  Trajectory  Measurement  Data structures  Search problems  Learning systems  Containers 
Abstract: We present a new general framework for disassembly sequence planning. This framework is versatile allowing different types of search schemes (exhaustive vs. preemptive), various part separation techniques, and the ability to group parts, or not, into subassemblies to improve the solution efficiency and parallelism. This enables a truly hierarchical approach to disassembly sequence planning. We demonstrate two different search strategies using this framework that can either yield a single solution quickly or provide a spectrum of solutions from which an optimal may be selected. We also develop a method for subassembly identification based on collision information. Our results show improved performance over an iterative motion planning based method for finding a single solution and greater functionality through hierarchical planning and optimal solution search.


Title: Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration
Key Words: learning (artificial intelligence)  manipulators  recurrent neural nets  robot vision  nonprehensile manipulation  recurrent neural network  raw images  VAE-GAN-based reconstruction  autoregressive multimodal action prediction  complex manipulation tasks  towel  weight  reconstruction-based regularization  vision-based multitask manipulation  end-to-end learning  multitask learning  low-cost robotic arm  robot arm trajectories  complex picking and placing tasks  Task analysis  Robots  Feature extraction  Neural networks  Image reconstruction  Training  Visualization 
Abstract: We propose a technique for multi-task learning from demonstration that trains the controller of a low-cost robotic arm to accomplish several complex picking and placing tasks, as well as non-prehensile manipulation. The controller is a recurrent neural network using raw images as input and generating robot arm trajectories, with the parameters shared across the tasks. The controller also combines VAE-GAN-based reconstruction with autoregressive multimodal action prediction. Our results demonstrate that it is possible to learn complex manipulation tasks, such as picking up a towel, wiping an object, and depositing the towel to its previous position, entirely from raw images with direct behavior cloning. We show that weight sharing and reconstruction-based regularization substantially improve generalization and robustness, and training on multiple tasks simultaneously increases the success rate on all tasks.


Title: Learning 6-DOF Grasping Interaction via Deep Geometry-Aware 3D Representations
Key Words: convolution  dexterous manipulators  geometry  grippers  image reconstruction  image representation  intelligent robots  learning (artificial intelligence)  optimisation  recurrent neural nets  virtual reality  outcome prediction model  3D shape modeling  DGGN  analysis-by-synthesis optimization  6-DOF grasping net  virtual reality  sensory annotations  data augmentation strategy  CNN  3D occupancy grid  mental geometry-aware representation  deep geometry-aware grasping network  3D geometry prediction  grasping interaction learning  parallel jaw gripper  RGBD input  internal geometry-aware representation  Grasping  Three-dimensional displays  Shape  Geometry  Solid modeling  Two dimensional displays  Robots 
Abstract: This paper focuses on the problem of learning 6- DOF grasping with a parallel jaw gripper in simulation. Our key idea is constraining and regularizing grasping interaction learning through 3D geometry prediction. We introduce a deep geometry-aware grasping network (DGGN) that decomposes the learning into two steps. First, we learn to build mental geometry-aware representation by reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via generative 3D shape modeling. Second, we learn to predict grasping outcome with its internal geometry-aware representation. The learned outcome prediction model is used to sequentially propose grasping solutions via analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best of our knowledge, we are presenting for the first time a method to learn a 6-DOF grasping net from RGBD input; (2) We build a grasping dataset from demonstrations in virtual reality with rich sensory and interaction annotations. This dataset includes 101 everyday objects spread across 7 categories, additionally, we propose a data augmentation strategy for effective learning; (3) We demonstrate that the learned geometry-aware representation leads to about 10% relative performance improvement over the baseline CNN on grasping objects from our dataset. (4) We further demonstrate that the model generalizes to novel viewpoints and object instances.


Title: Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions
Key Words: industrial robots  interactive systems  learning (artificial intelligence)  manipulators  natural language interfaces  natural language processing  object detection  robot vision  natural language processing technologies  unconstrained spoken instructions  instruction ambiguity  physical industrial robot arm  natural instructions  object picking task  real-world objects  unconstrained spoken language instructions  spoken natural language  human instructions  comprehensive system  Task analysis  Object recognition  Natural languages  Object detection  Feature extraction  Service robots 
Abstract: Comprehension of spoken natural language is an essential skill for robots to communicate with humans effectively. However, handling unconstrained spoken instructions is challenging due to (1) complex structures and the wide variety of expressions used in spoken language, and (2) inherent ambiguity of human instructions. In this paper, we propose the first comprehensive system for controlling robots with unconstrained spoken language, which is able to effectively resolve ambiguity in spoken instructions. Specifically, we integrate deep learning-based object detection together with natural language processing technologies to handle unconstrained spoken instructions, and propose a method for robots to resolve instruction ambiguity through dialogue. Through our experiments on both a simulated environment as well as a physical industrial robot arm, we demonstrate the ability of our system to understand natural instructions from human operators effectively, and show how higher success rates of the object picking task can be achieved through an interactive clarification process.


Title: Distributed Learning for the Decentralized Control of Articulated Mobile Robots
Key Words: decentralised control  distributed control  learning systems  mobile robots  multi-agent systems  highly cluttered evaluation environments  decentralized control architectures  central pattern generators  spatially distributed portions  articulated bodies  system-level objectives  reinforcement learning  independent agents  parallel environments  meta-level agent  homogeneous decentralized control  articulated locomotion  distributed learning  asynchronous advantage actor-critic algorithm  A3C  decentralized control policies  independently controlled portion  autonomous decentralized compliant control framework  compliant control baseline  articulated mobile robots  Shape  Decentralized control  Aerospace electronics  Robot kinematics  Admittance  Hardware 
Abstract: Decentralized control architectures, such as those conventionally defined by central pattern generators, independently coordinate spatially distributed portions of articulated bodies to achieve system-level objectives. State of the art distributed algorithms for reinforcement learning employ a different but conceptually related idea; independent agents simultaneously coordinating their own behaviors in parallel environments while asynchronously updating the policy of a system-or, rather, meta-level agent. This work, to the best of the authors' knowledge, is the first to explicitly explore the potential relationship between the underlying concepts in homogeneous decentralized control for articulated locomotion and distributed learning. We present an approach that leverages the structure of the asynchronous advantage actor-critic (A3C) algorithm to provide a natural framework for learning decentralized control policies on a single platform. Our primary contribution shows an individual agent in the A3C algorithm can be defined by an independently controlled portion of the robot's body, thus enabling distributed learning on a single platform for efficient hardware implementation. To this end, we show how the system is trained offline using hardware experiments implementing an autonomous decentralized compliant control framework. Our experimental results show that the trained agent outperforms the compliant control baseline by more than 40% in terms of steady progression through a series of randomized, highly cluttered evaluation environments.


Title: Neural Task Programming: Learning to Generalize Across Hierarchical Tasks
Key Words: learning (artificial intelligence)  manipulators  neural nets  neural task programming  unseen tasks  sequential tasks  robot manipulation tasks  bottom-level programs  hierarchical neural program  finer sub-task specifications  task specification  neural program induction  few-shot learning  NTP  novel robot learning framework  hierarchical tasks  Task analysis  Programming  Robots  Sorting  Semantics  Topology  Data models 
Abstract: In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well towards unseen tasks with increasing lengths, variable topologies, and changing objectives.stanfordvl.github.io/ntp/.


Title: Proprioceptive Inference for Dual-Arm Grasping of Bulky Objects Using RoboSimian
Key Words: Bayes methods  dexterous manipulators  humanoid robots  inference mechanisms  learning (artificial intelligence)  mobile robots  position control  robot kinematics  sensors  torque measurement  nasa jet propulsion laboratorys  robosimian  JPL  supporting manipulator  cumbersome objects  data-driven Bayesian models  inferred object properties  dual-arm lifting  bulky object  dual-arm grasping  proprioceptive inference  Manipulators  Probabilistic logic  Rotation measurement  Grasping  Shape  Task analysis 
Abstract: This work demonstrates dual-arm lifting of bulky objects based on inferred object properties (center of mass (COM) location, weight, and shape) using proprioception (i.e. force torque measurements). Data-driven Bayesian models describe these quantities, which enables subsequent behaviors to depend on confidence of the learned models. Experiments were conducted using the NASA Jet Propulsion Laboratory's (JPL) RoboSimian to lift a variety of cumbersome objects ranging in mass from 7kg to 25kg. The position of a supporting second manipulator was determined using a particle set and heuristics that were derived from inferred object properties. The supporting manipulator decreased the initial manipulator's load and distributed the wrench load more equitably across each manipulator, for each bulky object. Knowledge of the objects came from pure proprioception (i.e. without reliance on vision or other exteroceptive sensors) throughout the experiments.


Title: Unsupervised Learning of Hierarchical Models for Hand-Object Interactions
Key Words: image segmentation  pose estimation  support vector machines  tactile sensors  unsupervised learning  force vectors  unsupervised manner  event labeling sequences  manipulation event segmentation  hierarchical models  hand-object interactions  contact forces  unsupervised learning approach  manipulation event parsing  low-cost easy-to-replicate tactile glove  temporal grammar model  Force  Grammar  Robot sensing systems  Task analysis  Motion segmentation  Unsupervised learning 
Abstract: Contact forces of the hand are visually unobservable, but play a crucial role in understanding hand-object interactions. In this paper, we propose an unsupervised learning approach for manipulation event segmentation and manipulation event parsing. The proposed framework incorporates hand pose kinematics and contact forces using a low-cost easy-to-replicate tactile glove. We use a temporal grammar model to capture the hierarchical structure of events, integrating extracted force vectors from the raw sensory input of poses and forces. The temporal grammar is represented as a temporal And-Or graph (T-AOG), which can be induced in an unsupervised manner. We obtain the event labeling sequences by measuring the similarity between segments using the Dynamic Time Alignment Kernel (DTAK). Experimental results show that our method achieves high accuracy in manipulation event segmentation, recognition and parsing by utilizing both pose and force data.


Title: Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping
Key Words: image colour analysis  manipulators  neurocontrollers  robot vision  deep robotic grasping  off-the-shelf simulators  ground-truth annotations  randomized simulated environments  domain adaptation methods  grasping system  raw monocular RGB images  pixel-level domain adaptation  real-world grasping performance  annotated visual grasping datasets  GraspGAN  generative adversial network  Grasping  Robots  Training  Feature extraction  Adaptation models  Cameras  Task analysis 
Abstract: Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.


Title: Reinforcement Learning for 4-Finger-Gripper Manipulation
Key Words: grippers  intelligent robots  learning (artificial intelligence)  manipulator dynamics  motion control  path planning  high-level discrete actions  Q-learning  rhythmic Dynamic Movement Primitives  4-finger-gripper manipulator  hierarchical planning  Reinforcement Learning  4-finger-gripper manipulation  hierarchical-planning approach  Robots  Trajectory  Task analysis  Planning  Heuristic algorithms  Learning (artificial intelligence)  Approximation algorithms 
Abstract: In the framework of robotics, Reinforcement Learning (RL) deals with the learning of a task by the robot itself. This paper presents a hierarchical-planning approach in which the robot learns the optimal behavior for different levels in a decoupled way. For high-level discrete actions, Q-learning was chosen, whereas for the low level we utilize Policy Improvement with Path Integrals (PI2) algorithm to learn the parameters of policies, represented by rhythmic Dynamic Movement Primitives (DMPs). The paper studies the case of a 4-finger-gripper manipulator, which performs the task of continuously spinning a ball around a desired axis. The results demonstrate the efficacy of the hierarchical planning and the increased performance of the task when PI2 is used in conjunction with rhythmic DMPs in a real environment.


Title: Single-Image Footstep Prediction for Versatile Legged Locomotion
Key Words: convolution  feedforward neural nets  image colour analysis  intelligent robots  learning (artificial intelligence)  legged locomotion  path planning  prediction theory  robot kinematics  sensors  versatile legged locomotion  robots  longterm routes  horizontal terrain  vertical terrain  onboard sensors  vantage points  strongly foreshortened images  terrain features  viewing angle  convolutional neural network method  arbitrary tilt angles  route planner  plausible plans  rock climbing gyms  walking robots  climbing robots  single image footstep prediction  distance angle  valid handhold prediction  foothold locations prediction  single RGB+D images  learning techniques  flat ground  stairs  walls  Cameras  Legged locomotion  Planning  Robot kinematics  Three-dimensional displays  Rocks 
Abstract: Walking and climbing robots need to plan longterm routes on both horizontal and vertical terrain, but onboard sensors take images from vantage points that provide strongly foreshortened images that cause the appearance of terrain features to vary greatly by distance and viewing angle. This paper presents a convolutional neural network (CNN) method for predicting valid handhold and foothold locations from single RGB+D images taken at arbitrary tilt angles. Experiments show that the method predicts holds more accurately than comparable learning techniques, and that a route planner based on these predictions generates plausible plans for flat ground, stairs, and walls in rock climbing gyms.


Title: Learning to Parse Natural Language to Grounded Reward Functions with Weak Supervision
Key Words: grammars  lambda calculus  learning (artificial intelligence)  natural language processing  robots  trees (mathematics)  parse weights  validation-driven perceptron weight updates  goal-condition learning approach  grounded reward functions  language representations  weighted linear Combinatory Categorial Grammar semantic parser  CCG lexicon  parse trees  robot behaviors  Cleanup World domain  natural language parsing  goal-state reward functions  lambda calculus  Natural languages  Semantics  Task analysis  Planning  Robots  Trajectory  Navigation 
Abstract: In order to intuitively and efficiently collaborate with humans, robots must learn to complete tasks specified using natural language. We represent natural language instructions as goal-state reward functions specified using lambda calculus. Using reward functions as language representations allows robots to plan efficiently in stochastic environments. To map sentences to such reward functions, we learn a weighted linear Combinatory Categorial Grammar (CCG) semantic parser. The parser, including both parameters and the CCG lexicon, is learned from a validation procedure that does not require execution of a planner, annotating reward functions, or labeling parse trees, unlike prior approaches. To learn a CCG lexicon and parse weights, we use coarse lexical generation and validation-driven perceptron weight updates using the approach of Artzi and Zettlemoyer [4]. We present results on the Cleanup World domain [18] to demonstrate the potential of our approach. We report an F1 score of 0.82 on a collected corpus of 23 tasks containing combinations of nested referential expressions, comparators and object properties with 2037 corresponding sentences. Our goal-condition learning approach enables an improvement of orders of magnitude in computation time over a baseline that performs planning during learning, while achieving comparable results. Further, we conduct an experiment with just 6 labeled demonstrations to show the ease of teaching a robot behaviors using our method. We show that parsing models learned from small data sets can generalize to commands not seen during training.


Title: Deep Haptic Model Predictive Control for Robot-Assisted Dressing
Key Words: assisted living  clothing  control engineering computing  end effectors  handicapped aids  haptic interfaces  human-robot interaction  learning (artificial intelligence)  optimal control  predictive control  service robots  physical human-robot interaction  people with disabilities  controller objective function  deep predictive model  prediction horizon  PR2 robot  physics-based simulation  dressing assistance  garment  deep recurrent model  nonrigid garments  physical implications  robot-assisted dressing  deep haptic model predictive control  Robot sensing systems  Haptic interfaces  End effectors  Clothing  Predictive models 
Abstract: Robot-assisted dressing offers an opportunity to benefit the lives of many people with disabilities, such as some older adults. However, robots currently lack common sense about the physical implications of their actions on people. The physical implications of dressing are complicated by non-rigid garments, which can result in a robot indirectly applying high forces to a person's body. We present a deep recurrent model that, when given a proposed action by the robot, predicts the forces a garment will apply to a person's body. We also show that a robot can provide better dressing assistance by using this model with model predictive control. The predictions made by our model only use haptic and kinematic observations from the robot's end effector, which are readily attainable. Collecting training data from real world physical human-robot interaction can be time consuming, costly, and put people at risk. Instead, we train our predictive model using data collected in an entirely self-supervised fashion from a physics-based simulation. We evaluated our approach with a PR2 robot that attempted to pull a hospital gown onto the arms of 10 human participants. With a 0.2s prediction horizon, our controller succeeded at high rates and lowered applied force while navigating the garment around a persons fist and elbow without getting caught. Shorter prediction horizons resulted in significantly reduced performance with the sleeve catching on the participants' fists and elbows, demonstrating the value of our model's predictions. These behaviors of mitigating catches emerged from our deep predictive model and the controller objective function, which primarily penalizes high forces.


Title: EmoRL: Continuous Acoustic Emotion Classification Using Deep Reinforcement Learning
Key Words: acoustic signal processing  emotion recognition  human-robot interaction  learning (artificial intelligence)  neural nets  EmoRL model  audio signal  continuous acoustic emotion classification  deep reinforcement learning  acoustically expressed emotions  deep neural network-based models  affective state evaluation  real-time communication scenario  human-robot interaction  Acoustics  Robots  Adaptation models  Logic gates  Feature extraction  Predictive models  Recurrent neural networks 
Abstract: Acoustically expressed emotions can make communication with a robot more efficient. Detecting emotions like anger could provide a clue for the robot indicating unsafe/undesired situations. Recently, several deep neural network-based models have been proposed which establish new state-of-the-art results in affective state evaluation. These models typically start processing at the end of each utterance, which not only requires a mechanism to detect the end of an utterance but also makes it difficult to use them in a real-time communication scenario, e.g. human-robot interaction. We propose the EmoRL model that triggers an emotion classification as soon as it gains enough confidence while listening to a person speaking. As a result, we minimize the need for segmenting the audio signal for classification and achieve lower latency as the audio signal is processed incrementally. The method is competitive with the accuracy of a strong baseline model, while allowing much earlier prediction.


Title: Brain-Computer Interface Meets ROS: A Robotic Approach to Mentally Drive Telepresence Robots
Key Words: brain  brain-computer interfaces  collision avoidance  control engineering computing  geriatrics  handicapped aids  learning (artificial intelligence)  medical robotics  medical signal processing  mobile robots  operating systems (computers)  patient rehabilitation  position control  robot programming  telerobotics  video streaming  noninvasive Brain-Computer Interface  Robot Operating System  telepresence robot  mobile device  human brain signals  severe physical disabilities  elderly people  BCI user  robot position control  obstacle avoidance  video streaming  Navigation  Robot sensing systems  Task analysis  Telepresence  Brain-computer interfaces 
Abstract: This paper shows and evaluates a novel approach to integrate a non-invasive Brain-Computer Interface (BCI) with the Robot Operating System (ROS) to mentally drive a telepresence robot. Controlling a mobile device by using human brain signals might improve the quality of life of people suffering from severe physical disabilities or elderly people who cannot move anymore. Thus, the BCI user can actively interact with relatives and friends located in different rooms thanks to a video streaming connection to the robot. To facilitate the control of the robot via BCI, we explore new ROS-based algorithms for navigation and obstacle avoidance in order to make the system safer and more reliable. In this regard, the robot exploits two maps of the environment, one for localization and one for navigation, and both are used as additional visual feedback for the BCI user to control the robot position. Experimental results show a decrease of the number of commands needed to complete the navigation task, suggesting a reduction user's cognitive workload. The novelty of this work is to provide a first evidence of an integration between BCI and ROS that can simplify and foster the development of software for BCI driven robotics devices.


Title: FaNeuRobot: A Framework for Robot and Prosthetics Control Using the NeuCube Spiking Neural Network Architecture and Finite Automata Theory
Key Words: brain  brain-computer interfaces  electroencephalography  finite automata  handicapped aids  learning (artificial intelligence)  medical robotics  medical signal processing  muscle  neural nets  neurophysiology  brain-machine interface  central nervous system  biomedical signal  finite automata theory  NeuCube evolving spiking neural network architecture  robust prosthetic control  anthropomorphic mechanical design  noninvasive BMI  muscle atrophy  motor control framework  anthropomorphic design  prosthetic limbs  limb amputation  prosthetics control  Prosthetics  Muscles  Electroencephalography  DC motors  Grasping  Bones  Thumb 
Abstract: Limb amputation is a global problem. Prosthetic limbs can enhance the quality of life of amputees. To this end, anthropomorphic design and intuitive manipulation are two essential requirements. This paper presents a motor control framework for prosthetic control through Brain-Machine Interface (BMI) using Finite Automata Theory, and NeuCube Evolving Spiking Neural Network (SNN) architecture. Voluntary control of prosthetics requires decoding motor commands from the Central Nervous System of the amputee. Selection of the most suitable biomedical signal depends on many parameters such as level of amputation and muscle atrophy. Non-invasive BMI's have the possibility of supporting a wider range of amputees as it extracts the motor commands from the brain. In this paper, we present a proof of concept study on whether a cognitive computational model that is inspired by the motor control of the human body through muscle synergies combined with an anthropomorphic mechanical design, can result in accurate and robust prosthetic control through a noninvasive BMI. In future, learning of a complex Finite Automata that reflects complex upper limb motor behaviours will be investigated.


Title: Human in the Loop of Robot Learning: EEG-Based Reward Signal for Target Identification and Reaching Task
Key Words: brain  control engineering computing  electroencephalography  human-robot interaction  intelligent robots  learning (artificial intelligence)  robot programming  target reaching task  assistive technologies  shared control  intelligent robotic device  electrophysiological measures  error detection  Error-related Potentials  semiautonomous system  online robot learning task  detected ErrP  robot learning loop  optimal policy learning  shared autonomy  reinforcement learning framework  Electroencephalography  Training  Graphical user interfaces  Microsoft Windows  Testing  Robot learning 
Abstract: Shared control and shared autonomy play an important role in assistive technologies, allowing the offloading of the cognitive burden required for control from the user to the intelligent robotic device. In this context, electrophysiological measures of error detection, directly measured from a person's brain activity as Error-related Potentials (ErrPs), can be exploited to provide passive adaptation of an external semi-autonomous system to the human. This concept was implemented in an online robot learning task, where user's evaluation of the robot's actions, in terms of detected ErrP, was exploited to update a reward function in a Reinforcement Learning (RL) framework. Results from both simulated and experimental studies show that the introduction of human evaluation in the robot learning loop allows for: (1) the acceleration of optimal policy learning in a target reaching task, (2) the introduction of a further degree of control in robot learning, namely identification of one among multiple targets, according to the user's will. Overall, presented results support the potential of human-robot co-adaptive and co-operative strategies to develop human-centered assistive technologies.


Title: Incremental Adversarial Domain Adaptation for Continually Changing Environments
Key Words: feature extraction  image segmentation  unsupervised learning  machine learning models  robotics applications  alignment step  feature distribution  GAN training  continuous appearance shifts  continually changing environments  incremental adversarial domain adaptation  generative adversarial network  traversable-path segmentation task  unsupervised domain adaptation  Training  Task analysis  Adaptation models  Robots  Gallium nitride  Mathematical model  Lighting 
Abstract: Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.


Title: DeepVP: Deep Learning for Vanishing Point Detection on 1 Million Street View Images
Key Words: cameras  convolution  edge detection  feedforward neural nets  image classification  learning (artificial intelligence)  object detection  algorithmic vanishing point detector  DeepVP  Google street view image dataset  camera parameters  deep learning  deep vanishing point system  CNN classification problem  inferred ground-truth vanishing points  convolutional neural network  vanishing point detection  Roads  Cameras  Google  Machine learning  Videos  Image segmentation  Computer architecture 
Abstract: We propose a novel approach to detect vanishing points in images using a convolutional neural network (CNN) trained on a newly collected Google street-view image dataset. By utilizing the camera parameters and road direction data from Google street view, we collected a total of 1,053,425 images with inferred ground-truth vanishing points, along 23 worldwide routes totaling 125,165 kilometers. We then formulate vanishing point detection as a CNN classification problem using an output layer with 225 discrete possible vanishing point locations. Experimental results show that our deep vanishing point system outperforms the state-of-the-art algorithmic vanishing point detector. We achieved 99% accuracy in recovering the horizon line and 92% in locating the vanishing point within a ±5-degree range.


Title: Deep Lidar CNN to Understand the Dynamics of Moving Vehicles
Key Words: image colour analysis  image sensors  image sequences  learning (artificial intelligence)  neural net architecture  optical radar  semantic networks  pretext tasks  test time  including distilled image information  standard image-based optical flow  novel lidar-flow feature  semantic information  image data  consecutive lidar scans  testing time  CNN architecture  external observed vehicles  observer vehicle  proprio-motion  autonomous cars  Deep Learning solutions  RGB images  semantically rich information  Autonomous Driving  perception technologies  Deep lidar CNN  Laser radar  Task analysis  Vehicle dynamics  Three-dimensional displays  Dynamics  Machine learning  Semantics 
Abstract: Perception technologies in Autonomous Driving are experiencing their golden age due to the advances in Deep Learning. Yet, most of these systems rely on the semantically rich information of RGB images. Deep Learning solutions applied to the data of other sensors typically mounted on autonomous cars (e.g. lidars or radars) are not explored much. In this paper we propose a novel solution to understand the dynamics of moving vehicles of the scene from only lidar information. The main challenge of this problem stems from the fact that we need to disambiguate the proprio-motion of the “observer” vehicle from that of the external “observed” vehicles. For this purpose, we devise a CNN architecture which at testing time is fed with pairs of consecutive lidar scans. However, in order to properly learn the parameters of this network, during training we introduce a series of so-called pretext tasks which also leverage on image data. These tasks include semantic information about vehicleness and a novel lidar-flow feature which combines standard image-based optical flow with lidar scans. We obtain very promising results and show that including distilled image information only during training, allows improving the inference results of the network at test time, even when image data is no longer used.


Title: Optimization Beyond the Convolution: Generalizing Spatial Relations with End-to-End Metric Learning
Key Words: convolution  distance learning  feedforward neural nets  gradient methods  learning (artificial intelligence)  mobile robots  optimisation  pose estimation  solid modelling  robots  arbitrary spatial relations  sizes  shapes  distance metric learning  3D point clouds  metric space  object poses  arbitrary target relation  domestic environments  convolution  gradient based optimization  neural network  geometric models  continuous spectrum  end to end metric learning  Measurement  Three-dimensional displays  Robots  Optimization  Transforms  Shape  Convolution 
Abstract: To operate intelligently in domestic environments, robots require the ability to understand arbitrary spatial relations between objects and to generalize them to objects of varying sizes and shapes. In this work, we present a novel end-to-end approach to generalize spatial relations based on distance metric learning. We train a neural network to transform 3D point clouds of objects to a metric space that captures the similarity of the depicted spatial relations, using only geometric models of the objects. Our approach employs gradient-based optimization to compute object poses in order to imitate an arbitrary target relation by reducing the distance to it under the learned metric. Our results based on simulated and real-world experiments show that the proposed method enables robots to generalize spatial relations to unknown objects over a continuous spectrum.


Title: Constructing Category-Specific Models for Monocular Object-SLAM
Key Words: cameras  feature extraction  mobile robots  object detection  pose estimation  robot vision  SLAM (robots)  category-specific models  real-time object-oriented SLAM  monocular camera  object-level models  category-level models  object deformations  discriminative object features  category models  object landmark observations  generic monocular SLAM framework  2D object features  sparse feature-based monocular SLAM  object instance retrieval  instance-independent monocular object-SLAM system  feature-based SLAM methods  time 2.0 d  time 3.0 d  Solid modeling  Simultaneous localization and mapping  Three-dimensional displays  Object oriented modeling  Pipelines  Two dimensional displays  Shape 
Abstract: We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fty, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show - to the best of our knowledge - first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.


Title: Anticipating Many Futures: Online Human Motion Prediction and Generation for Human-Robot Interaction
Key Words: human-robot interaction  image coding  image colour analysis  image motion analysis  image representation  probability  robot vision  motion patterns  kinematic cues  natural human motion  human-robot interaction  online human motion prediction  target prediction  RGB depth images  skeletal data  conditional variational autoencoder  time 300.0 ms to 500.0 ms  Trajectory  Task analysis  Robot kinematics  Predictive models  Computational modeling  Training data 
Abstract: Fluent and safe interactions of humans and robots require both partners to anticipate the others' actions. The bottleneck of most methods is the lack of an accurate model of natural human motion. In this work, we present a conditional variational autoencoder that is trained to predict a window of future human motion given a window of past frames. Using skeletal data obtained from RGB depth images, we show how this unsupervised approach can be used for online motion prediction for up to 1660 ms. Additionally, we demonstrate online target prediction within the first 300-500 ms after motion onset without the use of target specific training data. The advantage of our probabilistic approach is the possibility to draw samples of possible future motion patterns. Finally, we investigate how movements and kinematic cues are represented on the learned low dimensional manifold.


Title: Social Attention: Modeling Attention in Human Crowds
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  path planning  publicly available crowd datasets  trained attention model  Social Attention  human crowds  robots  human predictable trajectories  human trajectory prediction  Trajectory  Navigation  Predictive models  Robots  Collision avoidance  Dynamics  Task analysis 
Abstract: Robots that navigate through human crowds need to be able to plan safe, efficient, and human predictable trajectories. This is a particularly challenging problem as it requires the robot to predict future human trajectories within a crowd where everyone implicitly cooperates with each other to avoid collisions. Previous approaches to human trajectory prediction have modeled the interactions between humans as a function of proximity. However, that is not necessarily true as some people in our immediate vicinity moving in the same direction might not be as important as other people that are further away, but that might collide with us in the future. In this work, we propose Social Attention, a novel trajectory prediction model that captures the relative importance of each person when navigating in the crowd, irrespective of their proximity. We demonstrate the performance of our method against a state-of-the-art approach on two publicly available crowd datasets and analyze the trained attention model to gain a better understanding of which surrounding agents humans attend to, when navigating in a crowd.


Title: Fully Convolutional Neural Networks for Road Detection with Multiple Cues Integration
Key Words: convergence  convolution  feature extraction  feedforward neural nets  gradient methods  image colour analysis  learning (artificial intelligence)  mobile robots  optical radar  position control  convolutional neural networks  multiple cues integration  autonomous driving  deep learning  road detection algorithms  pre-trained Resnet-lOl  RGB images  CNN  feature maps extraction  Lidar scanner  position map  image gradient  convergence  KITTI benchmark  Roads  Feature extraction  Laser radar  Three-dimensional displays  Task analysis  Fuses  Network architecture 
Abstract: Road detection from images is a key task in autonomous driving. The recent advent of deep learning (and in particular, CNN or convolutional neural networks) has greatly improved the performance of road detection algorithms. In this paper, we show how to fuse multiple different cues under the same convolutional network framework. Specifically, we adopt a pre-trained Resnet-lOl to extract feature maps from RGB images; we then connect it with three extra deconvolution layers. These deconvolution layers is trained conditioning on appropriate image cues, and in our case they are a height image (i.e. elevation map obtained by e.g. Lidar scanner), image gradient, and position map. We also design two skip layers to speed up the convergence. Experiments on KITTI benchmark show competitive performance of our new networks.


Title: A Deep Learning Based Behavioral Approach to Indoor Autonomous Navigation
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  navigational behaviors  deep learning architectures  semantic abstraction  navigation tasks  navigational missions  behavioral approach  indoor autonomous navigation  semantically rich graph representation  indoor robotic navigation  semantic locations  Navigation  Semantics  Visualization  Simultaneous localization and mapping  Robustness  Measurement 
Abstract: We present a semantically rich graph representation for indoor robotic navigation. Our graph representation encodes: semantic locations such as offices or corridors as nodes, and navigational behaviors such as enter office or cross a corridor as edges. In particular, our navigational behaviors operate directly from visual inputs to produce motor controls and are implemented with deep learning architectures. This enables the robot to avoid explicit computation of its precise location or the geometry of the environment, and enables navigation at a higher level of semantic abstraction. We evaluate the effectiveness of our representation by simulating navigation tasks in a large number of virtual environments. Our results show that using a simple sets of perceptual and navigational behaviors, the proposed approach can successfully guide the way of the robot as it completes navigational missions such as going to a specific office. Furthermore, our implementation shows to be effective to control the selection and switching of behaviors.


Title: Deep Predictive Models for Collision Risk Assessment in Autonomous Driving
Key Words: Bayes methods  collision avoidance  decision making  driver information systems  image colour analysis  learning (artificial intelligence)  recurrent neural nets  risk management  video streaming  Deep Predictive Models  collision risk assessment  autonomous driving  predictive approach  assisted driving  deep predictive model  video streams  RGB images  temporal information  multi-modal information  proprioceptive state  Bayesian convolutional LSTM  decision making  Predictive models  Accidents  Stochastic processes  Uncertainty  Bayes methods  Cameras  Tensile stress 
Abstract: In this paper, we investigate a predictive approach for collision risk assessment in autonomous and assisted driving. A deep predictive model is trained to anticipate imminent accidents from traditional video streams. In particular, the model learns to identify cues in RGB images that are predictive of hazardous upcoming situations. In contrast to previous work, our approach incorporates (a) temporal information during decision making, (b) multi-modal information about the environment, as well as the proprioceptive state and steering actions of the controlled vehicle, and (c) information about the uncertainty inherent to the task. To this end, we discuss Deep Predictive Models and present an implementation using a Bayesian Convolutional LSTM. Experiments in a simple simulation environment show that the approach can learn to predict impending accidents with reasonable accuracy, especially when multiple cameras are used as input sources.


Title: End-to-End Driving Via Conditional Imitation Learning
Key Words: collision avoidance  learning systems  mobile robots  road traffic control  driving policy functions  conditional imitation learning  sensorimotor coordination  vision-based driving  robotic truck  driving policies  deep networks  high-level navigational commands  urban driving  high-level command input  condition imitation  Robot sensing systems  Task analysis  Vehicles  Cameras  Roads  Navigation 
Abstract: Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands.


Title: VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving
Key Words: data visualisation  feedforward neural nets  learning (artificial intelligence)  object detection  traffic engineering computing  video signal processing  convolutional neural network  irrelevant information  prediction decision  CNN-based systems  steering self-driving cars  visualization method  valuable debugging tool  theoretical arguments  input pixels  individual pixels  visualization tool  NVIDIA neural-network-based end-to-end learning system  autonomous driving  VisualBackProp  public road video data  layer-wise relevance propagation approach  similar visualization results  PilotNet steering decision  relevant object capture  Neurons  Visualization  Deconvolution  Tools  Biological neural networks  Roads  Data visualization 
Abstract: This paper proposes a new method, that we call VisualBackProp, for visualizing which sets of pixels of the input image contribute most to the predictions made by the convolutional neural network (CNN). The method heavily hinges on exploring the intuition that the feature maps contain less and less irrelevant information to the prediction decision when moving deeper into the network. The technique we propose is dedicated for CNN-based systems for steering self-driving cars and is therefore required to run in real-time. This makes the proposed visualization method a valuable debugging tool which can be easily used during both training and inference. We justify our approach with theoretical arguments and confirm that the proposed method identifies sets of input pixels, rather than individual pixels, that collaboratively contribute to the prediction. We utilize the proposed visualization tool in the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet. We demonstrate that VisualBackProp determines which elements in the road image most influence PilotNet's steering decision and indeed captures relevant objects on the road. The empirical evaluation furthermore shows the plausibility of the proposed approach on public road video data as well as in other applications and reveals that it compares favorably to the layer-wise relevance propagation approach, i.e. it obtains similar visualization results and achieves order of magnitude speed-ups.


Title: Predicting Ego-Vehicle Paths from Environmental Observations with a Deep Neural Network
Key Words: driver information systems  feature extraction  image motion analysis  learning (artificial intelligence)  neural nets  road vehicles  static vehicle environment  grid-based prediction  varying assistance tasks  baseline approaches  environmental observations  deep neural network  advanced driver assistance systems  predictive model  road topologies  environmental properties  path extraction  ego-vehicle path prediction  ego-vehicle motion  Predictive models  Vehicles  Sensors  Roads  Trajectory  Data models  Motion measurement 
Abstract: Advanced driver assistance systems allow for increasing user comfort and safety by sensing the environment and anticipating upcoming hazards. Often, this requires to accurately predict how situations will change. Recent approaches make simplifying assumptions on the predictive model of the Ego-Vehicle motion or assume prior knowledge, such as road topologies, to be available. However, in many urban areas this assumption is not satisfied. Furthermore, temporary changes (e.g. construction areas, vehicles parked on the street) are not considered by such models. Since many cars observe the environment with several different sensors, predictive models can benefit from them by considering environmental properties. In this work, we present an approach for an Ego-Vehicle path prediction from such sensor measurements of the static vehicle environment. Besides proposing a learned model for predicting the driver's multi-modal future path as a grid-based prediction, we derive an approach for extracting paths from it. In driver assistance systems both can be used to solve varying assistance tasks. The proposed approach is evaluated on real driving data and outperforms several baseline approaches.


Title: Learning Steering Bounds for Parallel Autonomous Systems
Key Words: Bayes methods  cameras  control engineering computing  Gaussian processes  learning (artificial intelligence)  mixture models  mobile robots  neural nets  path planning  road vehicles  robot vision  steering systems  parallel autonomous systems  deep learning  autonomous driving task  camera data input  autonomous navigation  vehicle control  continuous control probability distribution  deep neural network based algorithm  steering angles  parallel autonomy setting  driving conditions  variational Bayesian methods  steering bounds learning  end-to-end learning  steering control options  Gaussian mixture models  Autonomous vehicles  Navigation  Neural networks  Probability distribution  Decision making  Machine learning  Bayes methods 
Abstract: Deep learning has been successfully applied to “end-to-end” learning of the autonomous driving task, where a deep neural network learns to predict steering control commands from camera data input. However, the learned representations do not support higher-level decision making required for autonomous navigation, nor the uncertainty estimates required for parallel autonomy, where vehicle control is shared between human and robot. This paper tackles the problem of learning a representation to predict a continuous control probability distribution, and thus steering control options and bounds for those options, which can be used for autonomous navigation. Each mode of the distribution encodes a possible macro-action that the system could execute at that instant, and the covariances of the modes place bounds on safe steering control values. Our approach has the added advantage of being trained on unlabeled data collected from inexpensive cameras. The deep neural network based algorithm generates a probability distribution over the space of steering angles, from which we leverage Variational Bayesian methods to extract a mixture model and compute the different possible actions in the environment. A bound, which the autonomous vehicle must respect in our parallel autonomy setting, is then computed for each of these actions. We evaluate our approach on a challenging dataset containing a wide variety of driving conditions, and show that our algorithm is capable of parameterizing Gaussian Mixture Models for possible actions, and extract steering bounds with a mean error of only 2 degrees. Additionally, we demonstrate our system working on a full scale autonomous vehicle and evaluate its ability to successful handle various different parallel autonomy situations.


Title: End to End Learning of Spiking Neural Network Based on R-STDP for a Lane Keeping Vehicle
Key Words: control engineering computing  learning (artificial intelligence)  mobile robots  neural nets  road vehicles  robot vision  SLAM (robots)  spiking neural network  lane keeping vehicle  mobile applications  mobile robot applications  reward-modulated spike-timing-dependent-plasticity  reinforcement learning  Pioneer robot  lane information  robot tasks control  end to end learning approach  R-STDP  SNNs training  neuromorphic vision sensor  lateral localization accuracy  Voltage control  Task analysis  Robot sensing systems  Training  Synapses  Neurons 
Abstract: Learning-based methods have demonstrated clear advantages in controlling robot tasks, such as the information fusion abilities, strong robustness, and high accuracy. Meanwhile, the on-board systems of robots have limited computation and energy resources, which are contradictory with state-of-the-art learning approaches. They are either too lightweight to solve complex problems or too heavyweight to be used for mobile applications. On the other hand, training spiking neural networks (SNNs) with biological plausibility has great potentials of performing fast computation and energy efficiency. However, the lack of effective learning rules for SNNs impedes their wide usage in mobile robot applications. This paper addresses the problem by introducing an end to end learning approach of spiking neural networks for a lane keeping vehicle. We consider the reward-modulated spike-timing-dependent-plasticity (R-STDP) as a promising solution in training SNNs, since it combines the advantages of both reinforcement learning and the well-known STDP. We test our approach in three scenarios that a Pioneer robot is controlled to keep lanes based on an SNN. Specifically, the lane information is encoded by the event data from a neuromorphic vision sensor. The SNN is constructed using R-STDP synapses in an all-to-all fashion. We demonstrate the advantages of our approach in terms of the lateral localization accuracy by comparing with other state-of-the-art learning algorithms based on SNNs.


Title: Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image
Key Words: image colour analysis  image reconstruction  image resolution  image sampling  image segmentation  image sensors  learning (artificial intelligence)  mean square error methods  optical radar  random processes  regression analysis  SLAM (robots)  sparse matrices  prediction root-mean-square error  sparse maps  dense maps  sparse-to-dense  dense depth prediction  sparse set  depth measurements  single RGB image  depth estimation  monocular images  low-resolution depth sensor  single deep regression network  RGB-D raw data  sparse depth samples  visual simultaneous localization and mapping algorithms  plug-in module  NYU-depth-v2 indoor dataset  LiDARs  Training  Laser radar  Image reconstruction  Estimation  Prediction algorithms  Simultaneous localization and mapping 
Abstract: We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software2 and video demonstration3 are publicly available.


Title: Multi-Stage Suture Detection for Robot Assisted Anastomosis Based on Deep Learning
Key Words: blood vessels  convolution  image recognition  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  recurrent neural nets  surgery  thread centerline reconstruction  multistage suture detection  robot assisted anastomosis  deep learning  robust suture detection  suture augmentation  robotic-assisted surgery  fully convolutional neural networks  trainee suturing skill evaluation  curvilinear structure detector  Yarn  Instruction sets  Surgery  Splines (mathematics)  Image reconstruction  Robots  Task analysis 
Abstract: The technique of robust suture detection is vital in many applications including trainee suturing skill evaluation, suture augmentation in robotic-assisted surgery and suture recognition for automatic suturing. Due to the complicated environment of surgery, the detection of a suture is challenged by high deformation and frequent occlusion. In this paper, we propose a deep multi-stage framework for suture detection. The fully convolutional neural networks are firstly used to predict a gradient map which not only serves as a segmentation mask, but also provides useful structure information for the following thread centerline reconstruction. An overlapping map is also predicted to improve the quality of the gradient map in self-intersection area. Based on the gradient map, multiple segments of the thread are extracted and linked to form the whole thread using a curvilinear structure detector. Experiments on two types of threads demonstrate that the proposed method is able to detect the thread with human level performance when the thread is no occlusion or under finite self-intersection.


Title: Active Clothing Material Perception Using Tactile Sensing and Deep Learning
Key Words: clothing  control engineering computing  convolution  feedforward neural nets  image sensors  intelligent robots  learning (artificial intelligence)  mobile robots  robot vision  tactile sensors  active clothing material perception  tactile sensing  deep learning  intelligent robot  robot system  object properties  common object category  external Kinect sensor  GelSight tactile sensor  tactile data  physical properties  durability  semantic properties  clothing properties  active tactile perception system  vision-touch system  robots  varied clothing related housework  convolutional neural networks  Clothing  Tactile sensors  Shape  Grippers 
Abstract: Humans represent and discriminate the objects in the same category using their properties, and an intelligent robot should be able to do the same. In this paper, we build a robot system that can autonomously perceive the object properties through touch. We work on the common object category of clothing. The robot moves under the guidance of an external Kinect sensor, and squeezes the clothes with a GelSight tactile sensor, then it recognizes the 11 properties of the clothing according to the tactile data. Those properties include the physical properties, like thickness, fuzziness, softness and durability, and semantic properties, like wearing season and preferred washing methods. We collect a dataset of 153 varied pieces of clothes, and conduct 6616 robot exploring iterations on them. To extract the useful information from the high-dimensional sensory output, we applied Convolutional Neural Networks (CNN) on the tactile data for recognizing the clothing properties, and on the Kinect depth images for selecting exploration locations. Experiments show that using the trained neural networks, the robot can autonomously explore the unknown clothes and learn their properties. This work proposes a new framework for active tactile perception system with vision-touch system, and has potential to enable robots to help humans with varied clothing related housework.


Title: Shaping in Practice: Training Wheels to Learn Fast Hopping Directly in Hardware
Key Words: control engineering computing  learning (artificial intelligence)  legged locomotion  robust control  temporary modifications  physical hardware  robot leg  reward landscape  fast hopping  robot controllers  engineering effort  potentially unstable parameters  training wheels  video synopsis  boom learning  robustness  Legged locomotion  Training  Wheels  Hardware  Hip 
Abstract: Learning instead of designing robot controllers can greatly reduce engineering effort required, while also emphasizing robustness. Despite considerable progress in simulation, applying learning directly in hardware is still challenging, in part due to the necessity to explore potentially unstable parameters. We explore the concept of shaping the reward landscape with training wheels; temporary modifications of the physical hardware that facilitate learning. We demonstrate the concept with a robot leg mounted on a boom learning to hop fast. This proof of concept embodies typical challenges such as instability and contact, while being simple enough to empirically map out and visualize the reward landscape. Based on our results we propose three criteria for designing effective training wheels for learning in robotics. A video synopsis can be found at https://youtu.be/6iH5E3LrYh8.


Title: Speeding Up Incremental Learning Using Data Efficient Guided Exploration
Key Words: Bayes methods  generalisation (artificial intelligence)  learning (artificial intelligence)  regression analysis  search problems  global parametric model  model-free policy search agent  model selection  Bayes method  online incremental learning  data uncertainty  reinforcement learning  probabilistic models  motor primitives  data efficient guided exploration  Task analysis  Uncertainty  Covariance matrices  Adaptation models  Computational modeling  Parametric statistics  Predictive models 
Abstract: To cope with varying conditions, motor primitives (MPs) must support generalization over task parameters to avoid learning separate primitives for each situation. In this regard, deterministic and probabilistic models have been proposed for generalizing MPs to new task parameters, thus providing limited generalization. Although generalization of MPs using probabilistic models has been studied, it is not clear how such generalizable models can be learned efficiently. Reinforcement learning can be more efficient when the exploration process is tuned with data uncertainty, thus reducing unnecessary exploration in a data-efficient way. We propose an empirical Bayes method to predict uncertainty and utilize it for guiding the exploration process of an incremental learning framework. The online incremental learning framework uses a single human demonstration for constructing a database of MPs. The main ingredients of the proposed framework are a global parametric model (GPDMP) for generalizing MPs for new situations, a model-free policy search agent for optimizing the failed predicted MPs, model selection for controlling the complexity of GPDMp, and empirical Bayes for extracting the uncertainty of MPs prediction. Experiments with a ball-in-a-cup task demonstrate that the global GPDMP model generalizes significantly better than linear models and Locally Weighted Regression especially in terms of extrapolation capability. Furthermore, the model selection has successfully identified the required complexity of GPDMP even with few training samples while satisfying the Occam Razor's prinicple. Above all, the uncertainty predicted by the proposed empirical Bayes approach successfully guided the exploration process of the model-free policy search. The experiments indicated statistically significant improvement of learning speed over covariance matrix adaptation (CMA) with a significance of p=0.002.


Title: Eager and Memory-Based Non-Parametric Stochastic Search Methods for Learning Control
Key Words: learning systems  nonparametric statistics  optimisation  robots  search problems  stochastic processes  learning control  direct policy search  complex problems  nonparametric methods  robot skill learning  memory-based learner  hybrid controller  memory-based non-parametric stochastic search methods  computing schedules  robot controller parameter optimisation  Stochastic processes  Robots  Search methods  Task analysis  Entropy  Computational modeling  Kernel 
Abstract: Direct policy search has shown to be a successful method to optimize robot controller parameters. However, defining a good parametric form for the controller can be challenging for complex problems. Non-parametric methods provide a flexible alternative and are thus a promising tool in robot skill learning. In this paper, we investigate two nonparametric methods based on similar principles but utilizing differing computing schedules: an eager learner and a memory-based learner. We compare the methods experimentally on two different control problems. Furthermore, we define and evaluate a new `hybrid' controller that combines the strong points of both of these methods.


Title: Data-driven Construction of Symbolic Process Models for Reinforcement Learning
Key Words: genetic algorithms  Internet  learning (artificial intelligence)  mobile robots  pendulums  time-varying systems  time varying dynamics  controlling systems  data driven construction  online  single node genetic programming  SNGP  pendulum swing up problem  training data  accurate models  real-time experiments  simulated mobile robot  realtime robot control  analytic equations  parsimonious models  symbolic regression  acceptable policy  RL  reinforcement learning  symbolic process models  Mathematical model  Data models  Learning (artificial intelligence)  Computational modeling  Mobile robots  Genetic programming  Model learning for control  AI-based methods  symbolic regression  reinforcement learning  optimal control 
Abstract: Reinforcement learning (RL) is a suitable approach for controlling systems with unknown or time-varying dynamics. RL in principle does not require a model of the system, but before it learns an acceptable policy, it needs many unsuccessful trials, which real robots usually cannot withstand. It is well known that RL can be sped up and made safer by using models learned online. In this paper, we propose to use symbolic regression to construct compact, parsimonious models described by analytic equations, which are suitable for realtime robot control. Single node genetic programming (SNGP) is employed as a tool to automatically search for equations fitting the available data. We demonstrate the approach on two benchmark examples: a simulated mobile robot and the pendulum swing-up problem; the latter both in simulations and real-time experiments. The results show that through this approach we can find accurate models even for small batches of training data. Based on the symbolic model found, RL can control the system well.


Title: PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-Based Planning
Key Words: learning (artificial intelligence)  mobile robots  navigation  neural nets  path planning  probability  robot dynamics  robot vision  sampling methods  sampling based planner  hierarchical method  sampling based path planning  large scale topology  probabilistic roadmaps  feature based deep neural net policies  continuous state  action spaces  simulation  office environments  aerial cargo delivery  urban environments  load displacement constraints  trajectories  noisy sensor conditions  flights  training  PRM RL  long range robotic navigation tasks  point to point navigation policies  end to end differential drive indoor navigation  nontrivial robot dynamics  robot configurations  task constraints  capture robot dynamics  RL agent  reinforcement learning  Task analysis  Robot sensing systems  Indoor navigation  Aerospace electronics  Learning (artificial intelligence) 
Abstract: We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling-based path planning with reinforcement learning (RL). The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of the large-scale topology. Next, the sampling-based planners provide roadmaps which connect robot configurations that can be successfully navigated by the RL agent. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. Our results show improvement in task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an environment 63 million times larger than used in training.


Title: Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics
Key Words: control engineering computing  learning (artificial intelligence)  legged locomotion  optimisation  search problems  parameterized black-box priors  robotics  data-efficient algorithms  reinforcement learning  dynamical model  black-box optimization algorithm  model-based policy search approaches  model learning procedure  high-dimensional systems  physical hexapod robot  Black-DROPS algorithm  Robots  Data models  Mathematical model  Computational modeling  Heuristic algorithms  Analytical models  Task analysis 
Abstract: The most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms, which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. Among the few proposed approaches, the recently introduced Black-DROPS algorithm exploits a black-box optimization algorithm to achieve both high data-efficiency and good computation times when several cores are used; nevertheless, like all model-based policy search approaches, Black-DROPS does not scale to high dimensional state/action spaces. In this paper, we introduce a new model learning procedure in Black-DROPS that leverages parameterized black-box priors to (1) scale up to high-dimensional systems, and (2) be robust to large inaccuracies of the prior information. We demonstrate the effectiveness of our approach with the “pendubot” swing-up task in simulation and with a physical hexapod robot (48D state space, 18D action space) that has to walk forward as fast as possible. The results show that our new algorithm is more data-efficient than previous model-based policy search algorithms (with and without priors) and that it can allow a physical 6-legged robot to learn new gaits in only 16 to 30 seconds of interaction time.


Title: Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation
Key Words: learning (artificial intelligence)  mobile robots  path planning  robot vision  double Q-learning  self-supervised deep reinforcement learning  self-supervised training  model-based methods  value-based model-free methods  learning-based methods  planning method  internal map  robot navigation  generalized computation graph  Computational modeling  Navigation  Learning (artificial intelligence)  Robots  Task analysis  Prediction algorithms  Planning 
Abstract: Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg.


Title: Bayesian Scale Estimation for Monocular SLAM Based on Generic Object Detection for Correcting Scale Drift
Key Words: Bayes methods  learning (artificial intelligence)  object detection  robot vision  SLAM (robots)  monocular SLAM system  Bayesian framework  deep-learning based generic object detector  detection region  scale drift  monocular systems  Bayesian scale estimation  generic object detection  local scale correction  object class detection  KITTI dataset  quantitative evaluations  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Trajectory  Bayes methods  Object detection  Image reconstruction 
Abstract: We propose a novel real-time algorithm for estimating the local scale correction of a monocular SLAM system, to obtain a correctly scaled version of the 3D map and of the camera trajectory. Within a Bayesian framework, it integrates observations from a deep-learning based generic object detector and landmarks from the map whose projection lie inside a detection region, to produce scale correction estimates from single frames. For each observation, a prior distribution on the height of the detected object class is used to define the observation's likelihood. Due to the scale drift inherent to monocular SLAM systems, we also incorporate a rough model on the dynamics of scale drift. Quantitative evaluations are presented on the KITTI dataset, and compared with different approaches. The results show a superior performance of our proposal in terms of relative translational error when compared to other monocular systems based on object detection.


Title: Trajectory-Optimized Sensing for Active Search of Tissue Abnormalities in Robotic Surgery
Key Words: end effectors  Gaussian processes  knowledge acquisition  learning (artificial intelligence)  medical robotics  mobile robots  position control  robot kinematics  surgery  trajectory optimisation (aerospace)  tumours  uncertain systems  tumors  Gaussian processes  stiffness distribution  palpation path  acquisition function  active learning algorithm  incorporate uncertainties  robot position  sensor measurements  robot-kinematics  trajectory-optimized sensing  tissue abnormalities  da Vinci research kit  insertable robotic effector platform  robotic surgery  6-DoF industrial arm  dVRK  IREP  Trajectory  Robot sensing systems  Optimization  Uncertainty  Bayes methods  Tumors 
Abstract: In this work, we develop an approach for guiding robots to automatically localize and find the shapes of tumors and other stiff inclusions present in the anatomy. Our approach uses Gaussian processes to model the stiffness distribution and active learning to direct the palpation path of the robot. The palpation paths are chosen such that they maximize an acquisition function provided by an active learning algorithm. Our approach provides the flexibility to avoid obstacles in the robot's path, incorporate uncertainties in robot position and sensor measurements, include prior information about location of stiff inclusions while respecting the robot-kinematics. To the best of our knowledge this is the first work in literature that considers all the above conditions while localizing tumors. The proposed framework is evaluated via simulation and experimentation on three different robot platforms: 6-DoF industrial arm, da Vinci Research Kit (dVRK), and the Insertable Robotic Effector Platform (IREP). Results show that our approach can accurately estimate the locations and boundaries of the stiff inclusions while reducing exploration time.


Title: A Method for Online Optimization of Lower Limb Assistive Devices with High Dimensional Parameter Spaces
Key Words: gait analysis  handicapped aids  medical robotics  advanced prosthesis controls  control parameters  optimization method  offline portion  intact subject gait data  neuromuscular control policy  ankle prosthesis  high-dimensional parameter spaces  parameter selection process  offline optimization procedure  dueling bandits problem  assistive lower-limb devices  control policies  lower limb assistive devices  online optimization  Knee  Prosthetics  Optimization  Torque  Neuromuscular  Trajectory 
Abstract: We propose a method for optimizing control policies for assistive lower-limb devices. The method frames parameter selection as a dueling bandits problem in which a user indicates his or her qualitative preferences between pairs of parameter sets chosen from a library. We generate the library through an offline optimization procedure that seeks to reproduce the varied gaits of healthy human subjects. By separating the parameter selection process into online and offline portions, the method can handle high-dimensional parameter spaces and produces policies that can generalize to different gait scenarios such as speed variation. We evaluate the method on five subjects walking on a powered knee and ankle prosthesis governed by a neuromuscular control policy that has 43 parameters. We find the five subjects preferred four different parameter sets from the library and that the resulting optima resemble intact subject gait data. This result suggests the offline portion of the optimization method indeed produces control parameters that can adapt to different gaits. Moreover, we find that for three out of the four parameter sets we tested, the procedure also generates parameters that improve the ability of the prosthesis to adapt to increasing gait speed by increasing ankle net work production. The results encourage further research and exploration in clinical settings toward advanced prosthesis controls that employ online learning.


Title: Endo-VMFuseNet: A Deep Visual-Magnetic Sensor Fusion Approach for Endoscopic Capsule Robots
Key Words: endoscopes  learning (artificial intelligence)  magnetic sensors  medical robotics  sensor fusion  sensor fusion techniques  endo-VMFuseNet  asymmetric sensor data  asynchronous sensor data  deep learning  active medical robots  passive capsule endoscopes  medical device companies  endoscopic capsule robots  deep visual-magnetic sensor fusion approach  Robot sensing systems  Magnetic separation  Magnetic levitation  Sensor fusion  Magnetic cores  Magnetic resonance imaging 
Abstract: In the last decade, researchers and medical device companies have made major advances towards transforming passive capsule endoscopes into active medical robots. One of the major challenges is to endow capsule robots with accurate perception of the environment inside the human body, which will provide necessary information and enable improved medical procedures. We extend the success of deep learning approaches from various research fields to the problem of sensor fusion for endoscopic capsule robots in the case of asynchronous and asymmetric sensor data without any need of calibration between sensors. The results performed on real pig stomach datasets show that our method achieves high precision for both translational and rotational movements and contains various advantages over traditional sensor fusion techniques.


Title: EndoSensorFusion: Particle Filtering-Based Multi-Sensory Data Fusion with Switching State-Space Model for Endoscopic Capsule Robots
Key Words: distance measurement  endoscopes  learning (artificial intelligence)  medical robotics  particle filtering (numerical methods)  pose estimation  recurrent neural nets  robot vision  sensor fusion  multisensor fusion  endoscopy robots  endoscopic capsule robot trajectories  recurrent neural network  nonlinear kinematic model  sensor reliability  online estimation  particle filter  gastrointestinal tract  therapeutic technology  switching state-space model  particle filtering-based multisensory data fusion  Robot sensing systems  Switches  Kalman filters  Proposals  Endoscopes  Magnetic resonance imaging 
Abstract: A reliable, real time, multi-sensor fusion functionality is crucial for localization of actively controlled capsule endoscopy robots, which are an emerging, minimally invasive diagnostic and therapeutic technology for the gastrointestinal (GI) tract. In this study, we propose a novel multi-sensor fusion approach based on a particle filter that incorporates an online estimation of sensor reliability and a non-linear kinematic model learned by a recurrent neural network. Our method sequentially estimates the true robot pose from noisy pose observations delivered by multiple sensors. We experimentally test the method using 5 degree-of-freedom (5-DoF) absolute pose measurement by a magnetic localization system and a 6-DoF relative pose measurement by visual odometry. In addition, the proposed method is capable of detecting and handling sensor failures by ignoring corrupted data, providing the robustness expected of a medical device. Detailed analyses and evaluations are presented using ex vivo experiments on a porcine stomach model, proving that our system achieves high translational and rotational accuracies for different types of endoscopic capsule robot trajectories.


Title: Real-Time Learning of Efficient Lift Generation on a Dynamically Scaled Flapping Wing Using Policy Search
Key Words: aerodynamics  aerospace components  aerospace robotics  learning (artificial intelligence)  search problems  efficient lift generation  policy search algorithm  real-time robotic learning problem  dynamically scaled flapping robotic wing  degrees-of-freedom  mineral oil  Reynolds number  optimal wing pitching amplitude  stroke-pitch phase difference  aerodynamic efficiency  quasisteady aerodynamic mechanism  wing rotation  stroke reversal  unsteady lift generation mechanisms  stroke amplitude range  Trajectory  Heuristic algorithms  Servomotors  Real-time systems  Aerodynamics  Robot sensing systems 
Abstract: In this work, we present a successful application of a policy search algorithm to a real-time robotic learning problem, where the goal is to maximize the efficiency of lift generation on a dynamically scaled flapping robotic wing. The robotic wing has two degrees-of-freedom, i.e., stroke and pitch, and operates in a tank filled with mineral oil. For all experiments, the Reynolds number is maintained constant at 1000, where learning is performed for different prescribed stroke amplitudes to find the optimal wing pitching amplitude and the stroke-pitch phase difference that maximize the power loading (PL) of lift generation, a measure of aerodynamic efficiency. For the investigated stroke amplitude range (30°-90°), the efficiency is observed to increase with the stroke amplitude and the lift is mainly generated through the delayed stall, a quasi-steady aerodynamic mechanism. Furthermore, the wing rotation becomes more asymmetric with respect to stroke reversal as the stroke amplitude decreases, indicating an increased use of unsteady lift generation mechanisms at lower stroke amplitudes.


Title: Realtime Planning for High-DOF Deformable Bodies Using Two-Stage Learning
Key Words: collision avoidance  finite element analysis  learning (artificial intelligence)  mobile robots  motion control  neurocontrollers  path planning  position control  realtime planning  high-DOF deformable bodies  arbitrarily-shaped volumetric deformable bodies  complex environments  high-dimensional configuration spaces  dynamics constraints  two-stage learning method  multitask controller  dynamic movement primitives  neural-network controller  DMP task  finite element method  contact invariant optimization  gradient-based method  two-stage learning algorithm  trained DMP controller  different navigation tasks  learned motion planner  walking deformable robots  obstacle avoidance  Deep Q-Learning  Planning  Deformable models  Robots  Finite element analysis  Strain  Computational modeling  Task analysis 
Abstract: We present a method for planning the motion of arbitrarily-shaped volumetric deformable bodies or robots through complex environments. Such robots have very high-dimensional configuration spaces and we compute trajectories that satisfy the dynamics constraints using a two-stage learning method. First, we train a multitask controller parameterized using dynamic movement primitives (DMP), which encodes various locomotion or movement skills. Next, we train a neural-network controller to select the DMP task to navigate the robot through environments while avoiding obstacles. By combining the finite element method (FEM), model reduction, and contact invariant optimization (CIO), the DMP controller's parameters can be optimized efficiently using a gradient-based method, while the neural-network's parameters are optimized using Deep Q-Learning (DQL). This two-stage learning algorithm also allows us to reuse the trained DMP controller for different navigation tasks, such as moving through different environmental types and to different goal positions. Our results show that the learned motion planner can navigate swimming and walking deformable robots with thousands of DOFs at realtime.


Title: Dex-Net 3.0: Computing Robust Vacuum Suction Grasp Targets in Point Clouds Using a New Analytic Model and Deep Learning
Key Words: convolution  dexterous manipulators  end effectors  feedforward neural nets  grippers  industrial manipulators  learning (artificial intelligence)  manipulator dynamics  deep learning  vacuum-based end effectors  multifinger grippers  suction cup  external wrenches  pneumatic suction gripper  point clouds  grasp quality convolutional neural network  robust vacuum suction grasp targets  gravity wrench  parallel-jaw grippers  object pose  material properties  GQ-CNN  ABB YuMi  adversarial  Three-dimensional displays  Robustness  Robots  Analytical models  Seals  Computational modeling  Planning 
Abstract: Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98%, 82%, and 58% respectively, improving to 81% in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net.


Title: Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation
Key Words: control engineering computing  human-robot interaction  learning by example  manipulators  neural nets  robot programming  robot vision  telerobotics  virtual reality  virtual reality teleoperation  robot skill acquisition  raw pixels  consumer-grade Virtual Reality headsets  hand tracking hardware  deep neural network policies  manipulation tasks  deep imitation learning  PR2 robot  RGB-D images  Robots  Task analysis  Neural networks  Three-dimensional displays  Head  Visualization  Grippers 
Abstract: Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.


Title: Accelerated Testing and Evaluation of Autonomous Vehicles via Imitation Learning
Key Words: collision avoidance  learning (artificial intelligence)  life testing  mobile robots  neurocontrollers  regression analysis  autonomous vehicle  imitation learning  surrogate agents  test scenario generation  performance modes  deep neural networks  imitator surrogates  mission performance  simulation-based testing  on-line imitation  complex mission  target vehicle  behavioral modes  dataset aggregation  collision avoidance  Testing  Training  Autonomous vehicles  Trajectory  Adaptation models  History 
Abstract: In this paper, we investigate the use of surrogate agents to accelerate test scenario generation for autonomous vehicles. Our goal is to train the surrogate to replicate the true performance modes of the system. We create these surrogates by utilizing imitation learning with deep neural networks. By using imitator surrogates in place of the true agent, we are capable of predicting mission performance more quickly, gaining greater throughput for simulation-based testing. We demonstrate that using on-line imitation learning with Dataset Aggregation (DAgger) can not only correctly encode a policy that executes a complex mission, but can also encode multiple different behavioral modes. To improve performance for the target vehicle and mission, we manipulate the training set during each iteration to remove samples which do not contribute to the final policy. We call this approach Quantile-DAgger (Q-DAgger) and demonstrate its ability to replicate the behaviors of an autonomous vehicle in a collision avoidance scenario.


Title: Feature-Based Transfer Learning for Robotic Push Manipulation
Key Words: CAD  learning (artificial intelligence)  manipulators  contact models  motion models  point cloud object model  CAD model  contact-based predictors  robotic push manipulation  feature-based transfer learning  Robots  Three-dimensional displays  Predictive models  Solid modeling  Kernel  Probability density function  Training 
Abstract: This paper presents a data-efficient approach to learning transferable forward models for robotic push manipulation. Our approach extends our previous work on contact-based predictors by leveraging information on the pushed object's local surface features. We test the hypothesis that, by conditioning predictions on local surface features, we can achieve generalisation across objects of different shapes. In doing so, we do not require a CAD model of the object but rather rely on a point cloud object model (PCOM). Our approach involves learning motion models that are specific to contact models. Contact models encode the contacts seen during training time and allow generating similar contacts at prediction time. Predicting on familiar ground reduces the motion models' sample complexity while using local contact information for prediction increases their transferability. In extensive experiments in simulation, our approach is capable of transfer learning for various test objects, outperforming a baseline predictor. We support those results with a proof of concept on a real robot.


Title: Inducing Probabilistic Context-Free Grammars for the Sequencing of Movement Primitives
Key Words: Bayes methods  context-free grammars  formal languages  grammars  humanoid robots  learning (artificial intelligence)  Markov processes  Monte Carlo methods  motion control  probability  robot dynamics  robots  rule-based nature  formal grammars  complex robot policies  composing primitives  modern robotics  inducing probabilistic context-free grammars  simple movement primitives  complex sequences  degree-of-freedom lightweight robotic arm  Markov Chain Monte Carlo optimization  grammar space  robot movement primitives  physical nature  yet unsolved challenge  complicated challenge  way robot policies  hierarchical concept  recursively structured tasks  hierarchically tasks  Grammar  Robots  Task analysis  Probabilistic logic  Sequential analysis  Markov processes  Optimization 
Abstract: Movement Primitives are a well studied and widely applied concept in modern robotics. Composing primitives out of an existing library, however, has shown to be a challenging problem. We propose the use of probabilistic context-free grammars to sequence a series of primitives to generate complex robot policies from a given library of primitives. The rule-based nature of formal grammars allows an intuitive encoding of hierarchically and recursively structured tasks. This hierarchical concept strongly connects with the way robot policies can be learned, organized, and re-used. However, the induction of context-free grammars has proven to be a complicated and yet unsolved challenge. In this work, we exploit the physical nature of robot movement primitives to restrict and efficiently search the grammar space. The grammar is learned applying a Markov Chain Monte Carlo optimization over the posteriors of the grammars given the observations. The proposal distribution is defined as a mixture over the probabilities of the operators connecting the search space. Restrictions to these operators guarantee continuous sequences while reducing the grammar space. We validate our method on a redundant 7 degree-of-freedom lightweight robotic arm on tasks that require the generation of complex sequences consisting of simple movement primitives.


Title: Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations
Key Words: image colour analysis  learning (artificial intelligence)  neural nets  object detection  robots  convolutional pose machines  human-readable plans learning  domain randomization  Baxter robot  image space  synthetic images  perception network  program execution  program generation  human-readable program  synthetically trained neural networks  world space  Task analysis  Training  Neural networks  Robot sensing systems  Robustness  Stacking 
Abstract: We present a system to infer and execute a human-readable program from a real-world demonstration. The system consists of a series of neural networks to perform perception, program generation, and program execution. Leveraging convolutional pose machines, the perception network reliably detects the bounding cuboids of objects in real images even when severely occluded, after training only on synthetic images using domain randomization. To increase the applicability of the perception network to new scenarios, the network is formulated to predict in image space rather than in world space. Additional networks detect relationships between objects, generate plans, and determine actions to reproduce a real-world demonstration. The networks are trained entirely in simulation, and the system is tested in the real world on the pick-and-place problem of stacking colored cubes using a Baxter robot.


Title: Generalized Task-Parameterized Skill Learning
Key Words: Gaussian processes  humanoid robots  human-robot interaction  learning (artificial intelligence)  manipulators  mixture models  motion control  robot programming  generalized task-parameterized skill learning  human skills  task-parameterized Gaussian mixture model  TP-GMM  human-robot collaboration  dual-arm manipulation  learning framework  task parameters  robot joint limits  task-parameterized learning  learned skills  real robotic systems  task constraints  learning perspective  Programming by demonstration  Task analysis  Trajectory  Robot kinematics  Optimization  Feature extraction  Robot sensing systems 
Abstract: Programming by demonstration has recently gained much attention due to its user-friendly and natural way to transfer human skills to robots. In order to facilitate the learning of multiple demonstrations and meanwhile generalize to new situations, a task-parameterized Gaussian mixture model (TP-GMM) has been recently developed. This model has achieved reliable performance in areas such as human-robot collaboration and dual-arm manipulation. However, the crucial task frames and associated parameters in this learning framework are often set by the human teacher, which renders three problems that have not been addressed yet: (i) task frames are treated equally, without considering their individual importance, (ii) task parameters are defined without taking into account additional task constraints, such as robot joint limits and motion smoothness, and (iii) a fixed number of task frames are pre-defined regardless of whether some of them may be redundant or even irrelevant for the task at hand. In this paper, we generalize the task-parameterized learning by addressing the aforementioned problems. Moreover, we provide a novel learning perspective which allows the robot to refine and adapt previously learned skills in a low dimensional space. Several examples are studied in both simulated and real robotic systems, showing the applicability of our approach.


Title: Teaching Human Teachers to Teach Robot Learners
Key Words: automatic programming  computer aided instruction  control engineering computing  data visualisation  feedback  human-robot interaction  robot programming  robots  teaching  robot learners generalisable skills  demonstration data sets  ambiguous demonstrations  teaching phase  interactive teaching process  robust teaching process  human teachers  heuristic rules  robot learners teaching  undemonstrated states  visual feedback  programming by demonstration  PbD  feedback visualisation  Task analysis  Education  Trajectory  Robot sensing systems  Visualization  Service robots 
Abstract: Using Programming by Demonstration to teach robot learners generalisable skills relies on having effective human teachers. This paper aims to address two problems commonly observed in demonstration data sets that arise due to poor teaching strategies; undemonstrated states and ambiguous demonstrations. Overcoming these issues through the use of visual feedback and simple heuristic rules is investigated as a potential way of guiding novice users to more effectively teach robot learners to generalise a task. The proposed method intends to offer the user a more transparent understanding of the robot learner's model state during the teaching phase, to create a more interactive and robust teaching process. Results from a single-factor, three-phase repeated measures study with n=30 participants, comparing the proposed feedback and heuristic rules set against an unguided condition, show a statistically significant (F(2,58)=7.952,p=0.001) improvement of user teaching efficiency of approximately 180% when using the proposed feedback visualisation.


Title: Deep Trail-Following Robotic Guide Dog in Pedestrian Environments for People who are Blind and Visually Impaired - Learning from Virtual and Real Worlds
Key Words: biomimetics  control engineering computing  convolution  feedforward neural nets  handicapped aids  learning (artificial intelligence)  mobile robots  robotic guide dog  interclass trail variations  deep convolutional neural network  virtual worlds  man-made trails  pedestrian environments  contact feedback  tactile trails  autonomous trail-following  virtual real-world environments  visually impaired  Dogs  Cameras  Robot vision systems  Navigation  Mobile robots 
Abstract: Navigation in pedestrian environments is critical to enabling independent mobility for the blind and visually impaired (BVI) in their daily lives. White canes have been commonly used to obtain contact feedback for following walls, curbs, or man-made trails, whereas guide dogs can assist in avoiding physical contact with obstacles or other pedestrians. However, the infrastructures of tactile trails or guide dogs are expensive to maintain. Inspired by the autonomous lane following of self-driving cars, we wished to combine the capabilities of existing navigation solutions for BVI users. We proposed an autonomous, trail-following robotic guide dog that would be robust to variances of background textures, illuminations, and interclass trail variations. A deep convolutional neural network (CNN) is trained from both the virtual and realworld environments. Our work included major contributions: 1) conducting experiments to verify that the performance of our models trained in virtual worlds was comparable to that of models trained in the real world; 2) conducting user studies with 10 blind users to verify that the proposed robotic guide dog could effectively assist them in reliably following man-made trails.


Title: MergeNet: A Deep Net Architecture for Small Obstacle Discovery
Key Words: image colour analysis  learning (artificial intelligence)  neural net architecture  object tracking  traffic engineering computing  lost and found dataset  complementary features  RGBD input  high level features  low level features  weight-sharing  multistage training procedure  annotation process  autonomous driving  on-road scenes  novel network architecture  small obstacle discovery  deep net architecture  MergeNet  Roads  Image segmentation  Strips  Semantics  Training  Autonomous vehicles  Task analysis 
Abstract: We present here, a novel network architecture called MergeNet for discovering small obstacles for on-road scenes in the context of autonomous driving. The basis of the architecture rests on the central consideration of training with less amount of data since the physical setup and the annotation process for small obstacles is hard to scale. For making effective use of the limited data, we propose a multi-stage training procedure involving weight-sharing, separate learning of low and high level features from the RGBD input and a refining stage which learns to fuse the obtained complementary features. The model is trained and evaluated on the Lost and Found dataset and is able to achieve state-of-art results with just 135 images in comparison to the 1000 images used by the previous benchmark. Additionally, we also compare our results with recent methods trained on 6000 images and show that our method achieves comparable performance with only 1000 training samples.


Title: Deep Encoder-Decoder Networks for Mapping Raw Images to Dynamic Movement Primitives
Key Words: backpropagation  handwriting recognition  handwritten character recognition  neural nets  dynamic movement primitives  cost functions  raw image mapping  backpropagation  MNIST database  deep encoder-decoder network  associated movement trajectories  perception-action couplings  encoder-decoder networks  calculated movements  handwriting movements  Trajectory  Differential equations  Neural networks  Cost function  Training  Robot kinematics 
Abstract: In this paper we propose a new approach for learning perception-action couplings. We show that by collecting a suitable set of raw images and the associated movement trajectories, a deep encoder-decoder network can be trained that takes raw images as input and outputs the corresponding dynamic movement primitives. We propose suitable cost functions for training the network and describe how to calculate their gradients to enable effective training by back-propagation. We tested the proposed approach both on a synthetic dataset and on a widely used MNIST database to generate handwriting movements from raw images of digits. The calculated movements were also applied for digit writing with a real robot.


Title: What is (Missing or Wrong) in the Scene? A Hybrid Deep Boltzmann Machine for Contextualized Scene Modeling
Key Words: Boltzmann machines  image classification  learning (artificial intelligence)  restricted Boltzmann machines  hybrid deep Boltzmann machine  scene classification dataset  baseline models  different objects  visible nodes  BM  hybrid Boltzmann Machine  contextualized scene modeling  scene reasoning tasks  Training  Task analysis  Robots  Computational modeling  Context modeling  Estimation  Cognition 
Abstract: Scene models allow robots to reason about what is in the scene, what else should be in it, and what should not be in it. In this paper, we propose a hybrid Boltzmann Machine (BM) for scene modeling where relations between objects are integrated. To be able to do that, we extend BM to include tri-way edges between visible (object) nodes and make the network to share the relations across different objects. We evaluate our method against several baseline models (Deep Boltzmann Machines, and Restricted Boltzmann Machines) on a scene classification dataset, and show that it performs better in several scene reasoning tasks.


Title: AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
Key Words: image classification  learning (artificial intelligence)  object detection  multiple object detection  AffordanceNet  object localization  object classification  affordance label  robust resizing strategy  deconvolutional layer sequence  real-time robotic applications  testing environments  end-to-end architecture  multitask loss function  affordance mask  RGB images  object affordance detection  end-to-end deep learning approach  Feature extraction  Robots  Computer architecture  Object detection  Training  Image segmentation  Machine learning 
Abstract: We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net.


Title: ContextualNet: Exploiting Contextual Information Using LSTMs to Improve Image-Based Localization
Key Words: convolution  feedforward neural nets  learning (artificial intelligence)  pose estimation  SLAM (robots)  CNN-LSTM model  pose estimation  single monocular image  Convolutional Neural Networks  image-based localization  ContextualNet  image content  indoor office space  Feature extraction  Cameras  Context modeling  Computer vision  Logic gates  Neural networks 
Abstract: Convolutional Neural Networks (CNN) have successfully been utilized for localization using a single monocular image [1]. Most of the work to date has either focused on reducing the dimensionality of data for better learning of parameters during training or on developing different variations of CNN models to improve pose estimation. Many of the best performing works solely consider the content in a single image, while the context from historical images is ignored. In this paper, we propose a combined CNN-LSTM which is capable of incorporating contextual information from historical images to better estimate the current pose. Experimental results achieved using a dataset collected in an indoor office space improved the overall system results to 0.8 m & 2.5° at the third quartile of the cumulative distribution as compared with 1.5 m & 3.0° achieved by PoseNet [1]. Furthermore, we demonstrate how the temporal information exploited by the CNN-LSTM model assists in localizing the robot in situations where image content does not have sufficient features.


Title: Learning Human-Aware Path Planning with Fully Convolutional Networks
Key Words: convolution  feedforward neural nets  learning (artificial intelligence)  mobile robots  navigation  path planning  random processes  trees (mathematics)  robot social navigation  Fully Convolutional Neural Networks  mobile robots  human-aware path planning learning  optimal Rapidly-exploring Random Tree planner  robot navigation  classification problem  Robots  Navigation  Task analysis  Trajectory  Cost function  Feature extraction 
Abstract: This work presents an approach to learn path planning for robot social navigation by demonstration. We make use of Fully Convolutional Neural Networks (FCNs) to learn from expert's path demonstrations a map that marks a feasible path to the goal as a classification problem. The use of FCNs allows us to overcome the problem of manually designing/identifying the cost-map and relevant features for the task of robot navigation. The method makes use of optimal Rapidly-exploring Random Tree planner (RRT*) to overcome eventual errors in the path prediction; the FCNs prediction is used as cost-map and also to partially bias the sampling of the configuration space, leading the planner to behave similarly to the learned expert behavior. The approach is evaluated in experiments with real trajectories and compared with Inverse Reinforcement Learning algorithms that use RRT* as underlying planner.


Title: Pedestrian Prediction by Planning Using Deep Neural Networks
Key Words: collision avoidance  convolution  learning (artificial intelligence)  mobile robots  neural nets  pedestrians  traffic engineering computing  monolithic neural network  inverse reinforcement learning  pedestrian prediction  deep neural networks  collision avoidance  autonomous vehicles  goal-directed planning  mixture density function  motion prediction  convolutional network  traffic participant prediction  trajectories  Planning  Network topology  Topology  Convolution  Learning (artificial intelligence)  Trajectory  Prediction algorithms 
Abstract: Accurate traffic participant prediction is the prerequisite for collision avoidance of autonomous vehicles. In this work, we propose to predict pedestrians using goal-directed planning. For this, we infer a mixture density function for possible destinations. We use these destinations as the goal states of a planning stage that performs motion prediction based on common behavior patterns. The patterns are learned by a fully convolutional network operating on maps of the environment. We show that this entire system can be modeled as one monolithic neural network and trained via inverse reinforcement learning. Experimental validation on real world data shows the system's ability to predict both, destinations and trajectories accurately.


Title: Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction
Key Words: feature selection  human-robot interaction  image motion analysis  learning (artificial intelligence)  mobile robots  pose estimation  recurrent neural nets  stochastic processes  human robot cooperation scenario  prediction model  action prediction dataset  human motion data  human-robot cooperation  recurrent neural network approach  multiple action sequences prediction  assistive applications  nonverbal cues  neural networks  human action prediction problem  continuous spaces  discrete spaces  encoder-decoder recurrent neural network topology  discrete action prediction problem  action sequences  feature selection  stochastic reward  Predictive models  Decoding  Hidden Markov models  Recurrent neural networks  Robot kinematics  Trajectory 
Abstract: Close human-robot cooperation is a key enabler for new developments in advanced manufacturing and assistive applications. Close cooperation require robots that can predict human actions and intent, understanding human non-verbal cues. Recent approaches based on neural networks have led to encouraging results in the human action prediction problem both in continuous and discrete spaces. Our approach extends the research in this direction. Our contributions are three-fold. First, we validate the use of gaze and body pose cues as a means of predicting human action through a feature selection method. Next, we address two shortcomings of existing literature: predicting multiple and variable-length action sequences. This is achieved by applying an encoder-decoder recurrent neural network topology in the discrete action prediction problem. In addition, we theoretically demonstrate the importance of predicting multiple action sequences as a means of estimating the stochastic reward in a human robot cooperation scenario. Finally, we show the ability to effectively train the prediction model on an action prediction dataset, involving human motion data, and explore the influence of the model's parameters on its performance.


Title: Text2Action: Generative Adversarial Synthesis from Language to Action
Key Words: recurrent neural nets  robots  text analysis  video signal processing  sequence to sequence model  Baxter robot  virtual agent  generative adversarial synthesis  Text2action  MSR-Video-to-Text  action decoder RNN  text encoder recurrent neural network  generative network  SEQ2SEQ  sequence model  generative adversarial network  human behavior  sentence  human action sequence  generative model  Decoding  Gallium nitride  Hidden Markov models  Generative adversarial networks  Robots  Recurrent neural networks  Generators 
Abstract: In this paper, we propose a generative model which learns the relationship between language and human action in order to generate a human action sequence given a sentence describing human behavior. The proposed generative model is a generative adversarial network (GAN), which is based on the sequence to sequence (SEQ2SEQ) model. Using the proposed generative network, we can synthesize various actions for a robot or a virtual agent using a text encoder recurrent neural network (RNN) and an action decoder RNN. The proposed generative network is trained from 29,770 pairs of actions and sentence annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video dataset. We demonstrate that the network can generate human-like actions which can be transferred to a Baxter robot, such that the robot performs an action based on a provided sentence. Results show that the proposed generative network correctly models the relationship between language and action and can generate a diverse set of actions from the same sentence.


Title: A Data-driven Model for Interaction-Aware Pedestrian Motion Prediction in Object Cluttered Environments
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  motion estimation  navigation  neural nets  pedestrians  human motion behavior  prediction accuracy  data-driven model  interaction-aware pedestrian motion prediction  object cluttered environments  interaction-aware motion prediction approach  human navigation behavior  Long-Short Term Memory neural networks  static obstacles  trajectory forecasting  polar angle space  Predictive models  Robots  Trajectory  Adaptation models  Navigation  Planning  Neural networks 
Abstract: This paper reports on a data-driven, interaction-aware motion prediction approach for pedestrians in environments cluttered with static obstacles. When navigating in such workspaces shared with humans, robots need accurate motion predictions of the surrounding pedestrians. Human navigation behavior is mostly influenced by their surrounding pedestrians and by the static obstacles in their vicinity. In this paper we introduce a new model based on Long-Short Term Memory (LSTM) neural networks, which is able to learn human motion behavior from demonstrated data. To the best of our knowledge, this is the first approach using LSTMs, that incorporates both static obstacles and surrounding pedestrians for trajectory forecasting. As part of the model, we introduce a new way of encoding surrounding pedestrians based on a 1d-grid in polar angle space. We evaluate the benefit of interaction-aware motion prediction and the added value of incorporating static obstacles on both simulation and real-world datasets by comparing with state-of-the-art approaches. The results show, that our new approach outperforms the other approaches while being very computationally efficient and that taking into account static obstacles for motion predictions significantly improves the prediction accuracy, especially in cluttered environments.


Title: Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data
Key Words: feature extraction  feedforward neural nets  gesture recognition  image classification  image motion analysis  learning (artificial intelligence)  motion recognition  spatiotemporal features  dense occupancy grids  3D point cloud data  end-to-end spatiotemporal gesture learning approach  dynamic gestures  spatiotemporal learning  point cloud data augmentation  3D convolutional neural network  gestures sample data  Three-dimensional displays  Robot sensing systems  Feature extraction  Solid modeling  Training data  Spatiotemporal phenomena  Hidden Markov models 
Abstract: In this paper, we demonstrate an end-to-end spatiotemporal gesture learning approach for 3D point cloud data using a new gestures dataset of point clouds acquired from a 3D sensor. Nine classes of gestures were learned from gestures sample data. We mapped point cloud data into dense occupancy grids, then time steps of the occupancy grids are used as inputs into a 3D convolutional neural network which learns the spatiotemporal features in the data without explicit modeling of gesture dynamics. We also introduced a 3D region of interest jittering approach for point cloud data augmentation. This resulted in an increased classification accuracy of up to 10% when the augmented data is added to the original training data. The developed model is able to classify gestures from the dataset with 84.44% accuracy. We propose that point cloud data will be a more viable data type for scene understanding and motion recognition, as 3D sensors become ubiquitous in years to come.


Title: 3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data
Key Words: cameras  codecs  image annotation  image coding  learning (artificial intelligence)  mobile robots  object detection  pedestrians  pose estimation  robot vision  service robots  SLAM (robots)  long-term temporal information  sequence-to-sequence LSTM encoder-decoder  on-the-fly prediction  global coordinate system  T-Pose-LSTM model  human trajectory prediction  long-term mobile robot deployments  3DOF pedestrian trajectory prediction learned  Long-Term autonomous mobile robot deployment data  autonomous mobile service robots  monocular camera images  range-finder sensors  3DOF pedestrian trajectory prediction approach  temporal 3DOF-pose long-short-term memory  robust human detection  Trajectory  Cameras  Robot kinematics  Robot vision systems  Two dimensional displays  Mobile robots 
Abstract: This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.


Title: Fusing Object Context to Detect Functional Area for Cognitive Robots
Key Words: feature extraction  image classification  image fusion  image recognition  learning (artificial intelligence)  object detection  object context  deep learning  object detection dataset  current object detection framework  functional area detection  functionality-related feature  object-related  potential image regions  deep-model-based classifier  functional area image dataset  area detection problem  image recognition  cognitive robot  Feature extraction  Object detection  Robots  Task analysis  Machine learning  Proposals  Image recognition 
Abstract: A cognitive robot usually needs to perform multiple tasks in practice and needs to locate the desired area for each task. Since deep learning has achieved substantial progress in image recognition, to solve this area detection problem, it is straightforward to label a functional area (affordance) image dataset and apply a well-trained deep-model-based classifier on all the potential image regions. However, annotating the functional area is time consuming and the requirement of large amount of training data limits the application scope. We observe that the functional area are usually related to the surrounding object context. In this work, we propose to use the existing object detection dataset and employ the object context as effective prior to improve the performance without additional annotated data. In particular, we formulate a two-stream network that fuses the object-related and functionality-related feature for functional area detection. The whole system is formulated in an end-to-end manner and easy to implement with current object detection framework. Experiments demonstrate that the proposed network outperforms current method by almost 20% in terms of precision and recall.


Title: When Regression Meets Manifold Learning for Object Recognition and Pose Estimation
Key Words: convolution  feedforward neural nets  image matching  nearest neighbour methods  object recognition  pose estimation  regression analysis  manifold learning  pose regression  NN descriptor matching  manifold descriptor learning  multitask learning framework  nearest neighbor search  convolutional neural networks  pose estimation  object recognition  Pose estimation  Manifolds  Task analysis  Training  Robustness  Object recognition  Three-dimensional displays 
Abstract: In this work, we propose a method for object recognition and pose estimation from depth images using convolutional neural networks. Previous methods addressing this problem rely on manifold learning to learn low dimensional viewpoint descriptors and employ them in a nearest neighbor search on an estimated descriptor space. In comparison we create an efficient multi-task learning framework combining manifold descriptor learning and pose regression. By combining the strengths of manifold learning using triplet loss and pose regression, we could either estimate the pose directly reducing the complexity compared to NN search, or use the learned descriptor for the NN descriptor matching. By in depth experimental evaluation of the novel loss function we observed that the view descriptors learned by the network are much more discriminative resulting in almost 30% increase regarding relative pose accuracy compared to related works. On the other hand, regarding directly regressed poses we obtained important improvement compared to simple pose regression. By leveraging the advantages of both manifold learning and regression tasks, we are able to improve the current state-of-the-art for object recognition and pose retrieval.


Title: A Deep Learning-Based Stalk Grasping Pipeline
Key Words: crops  image segmentation  industrial manipulators  learning (artificial intelligence)  mobile robots  robot vision  stalk segmentation  grasp point generation pipeline  high stalk density  lighting variation  custom-built ground robot  end-to-end system  average grasping accuracy  Generative Adversarial Network  in-situ sorghum stalk detection  online pipeline  deep learning-based high throughput  labor-intensive phenotyping processes  robotic solutions  plant attributes  precise measurements  fast measurements  deep learning-based stalk grasping pipeline  pixel-wise stalk detection  grasp point detection  stalk detection F1 score  Robots  Pipelines  Image segmentation  Generators  Three-dimensional displays  Gallium nitride  Cameras 
Abstract: The need for fast and precise measurements of plant attributes makes robotic solutions an ideal replacement for labor-intensive phenotyping processes. In this work we present a deep learning-based high throughput, online pipeline for in-situ sorghum stalk detection and grasping. We use a variation of Generative Adversarial Network (GAN) for stalk segmentation trained on a relatively small number of images followed by a grasp point generation pipeline. The presented pipeline is robust to field challenges such as occlusions, high stalk density and lighting variation, and was deployed on a custom-built ground robot. We tested our end-to-end system in a field of Sorghum bicolor in South Carolina, USA, achieving an average grasping accuracy of 74.13% and a stalk detection F1 score of 0.90. Grasp point detection for plant manipulation takes an average of 0.98 seconds, and pixel-wise stalk detection takes 0.2 seconds per image.


Title: Reinforcement Learning of Depth Stabilization with a Micro Diving Agent
Key Words: embedded systems  learning (artificial intelligence)  microrobots  multi-agent systems  robot programming  underwater vehicles  model-based value-function RL algorithm  micro underwater agents  underwater robotics  underwater depth stabilization  light embedded systems  control tasks  microdiving agent  reinforcement learning  Computational modeling  Learning (artificial intelligence)  Task analysis  Robot kinematics  Heuristic algorithms  Force 
Abstract: Reinforcement learning (RL) allows robots to solve control tasks through interaction with their environment. In this paper we study a model-based value-function RL approach, which is suitable for computationally limited robots and light embedded systems. We develop a diving agent, which uses the RL algorithm for underwater depth stabilization. Simulations and experiments with the micro diving agent demonstrate its ability to learn the depth stabilization task.


Title: Gaussian Process Adaptive Sampling Using the Cross-Entropy Method for Environmental Sensing and Monitoring
Key Words: bathymetry  entropy  Gaussian processes  learning (artificial intelligence)  mobile robots  optimisation  path planning  sampling methods  single ROI  deepest region  coastal bathymetry mapping mission validate  efficient sampling strategy  latest sensory measurements  sampling density  CE trajectory optimization  higher spatial variability  exhibit extreme sensory measurements  exploring learning  initial stage  path planning  GP-UCB  GP upper confidence  receding-horizon Cross-Entropy trajectory optimization  environmental sensing  cross-entropy method  Gaussian process adaptive sampling  Robot sensing systems  Adaptation models  Optimization  Predictive models  Trajectory  Uncertainty 
Abstract: In this paper, we focus on adaptive sampling on a Gaussian Processes (GP) using the receding-horizon Cross-Entropy (CE) trajectory optimization. Specifically, we employ the GP upper confidence bound (GP-UCB) as the optimization criteria to adaptively plan sampling paths that balance the exploitation-exploration trade-off. Path planning at the initial stage focuses on exploring and learning a model of the environment, and later, on exploiting the learned model to focus sampling around regions that exhibit extreme sensory measurements and much higher spatial variability, denoted as the Region of Interest (ROI). The integration of the CE trajectory optimization allows the sampling density to be dynamically adjusted based on the latest sensory measurements, thus providing an efficient sampling strategy for sensing and localizing the ROI. We demonstrate the effectiveness of the proposed method in exploring simulated scalar fields with single or multiple ROIs. Field experiments with an Unmanned Surface Vehicle (USV) in a coastal bathymetry mapping mission validate the approach's capability in quickly exploring and mapping the given area, and then focusing and increasing the sampling density around the deepest region, as a surrogate for e.g. the extremal concentration of a pollutant in the environment.


Title: OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World
Key Words: control engineering computing  decision making  learning (artificial intelligence)  manipulators  neural nets  optimisation  decision-making problems  reinforcement learning architecture  OptLayer  neural network  closest actions  safe actions  robot reaching tasks  practical constrained optimization  deep reinforcement learning techniques  Robot kinematics  Neural networks  Task analysis  Optimization  Learning (artificial intelligence)  Training 
Abstract: While deep reinforcement learning techniques have recently produced considerable achievements on many decision-making problems, their use in robotics has largely been limited to simulated worlds or restricted motions, since unconstrained trial-and-error interactions in the real world can have undesirable consequences for the robot or its environment. To overcome such limitations, we propose a novel reinforcement learning architecture, OptLayer, that takes as inputs possibly unsafe actions predicted by a neural network and outputs the closest actions that satisfy chosen constraints. While learning control policies often requires carefully crafted rewards and penalties while exploring the range of possible actions, OptLayer ensures that only safe actions are actually executed and unsafe predictions are penalized during training. We demonstrate the effectiveness of our approach on robot reaching tasks, both simulated and in the real world.


Title: Composable Deep Reinforcement Learning for Robotic Manipulation
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  composable deep reinforcement  model-free deep reinforcement learning  simulated robotic manipulation  model-free methods  real-world robotic tasks  maximum entropy policies  soft Q-learning  real-world robotic manipulation  Entropy  Robots  Learning (artificial intelligence)  Neural networks  Machine learning  Task analysis  Training 
Abstract: Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.


Title: Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning
Key Words: collision avoidance  decentralised control  gradient methods  learning (artificial intelligence)  mobile robots  multi-robot systems  multiscenario multistage training framework  optimal policy  policy gradient  reinforcement learning algorithm  learned sensor-level collision avoidance policy  final learned policy  collision-free paths  large-scale robot system  deep reinforcement learning  safe collision avoidance policy  efficient collision avoidance policy  optimally decentralized multirobot collision avoidance  agent-level feature extraction  decentralized methods  maps raw sensor measurements  multirobot systems  decentralized sensor-level collision avoidance policy  local collision-free action  distributed multirobot collision avoidance systems  Collision avoidance  Robot sensing systems  Robot kinematics  Navigation  Robustness  Training 
Abstract: Developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths without observing other robots' states and intents. While other distributed multi-robot collision avoidance systems exist, they often require extracting agent-level features to plan a local collision-free action, which can be computationally prohibitive and not robust. More importantly, in practice the performance of these methods are much lower than their centralized counterparts. We present a decentralized sensor-level collision avoidance policy for multi-robot systems, which directly maps raw sensor measurements to an agent's steering commands in terms of movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots on rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. We validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient, collision-free paths for a large-scale robot system. We also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period, including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots. Videos are available at https://sites.google.com/view/drlmaca.


Title: Tensegrity Robot Locomotion Under Limited Sensory Inputs via Deep Reinforcement Learning
Key Words: learning systems  mobile robots  motion control  neurocontrollers  nonlinear dynamical systems  search problems  state-space methods  nonlinear dynamics  high-dimensional state space  robotic systems  space exploration  tensegrity robot locomotion  deep reinforcement learning algorithms  policy learning process  locomotion control policies  neural network policies  mirror descent guided policy search  end-to-end locomotion policies  tensegrity robotics  Robot sensing systems  Neural networks  Aerospace electronics  Hardware  NASA  Training 
Abstract: Tensegrity robots are composed of rigid rods connected by elastic cables, and their unique light-weight yet compliant structure makes them an appealing choice for space exploration. However, locomotion control for these robotic systems remains difficult due to their nonlinear dynamics and high-dimensional state space. We demonstrate that in the domain of tensegrity robotics, it is possible to efficiently learn end-to-end locomotion policies using mirror descent guided policy search (MDGPS) even with limited sensory inputs. We compare learned neural network policies with other locomotion control policies in various testing environments; and results show that neural network policies consistently outperform others. We also shed light to the policy learning process by analyzing different choices of observation inputs to the robot. Moreover these findings motivate exploration of deep reinforcement learning algorithms in the domain of tensegrity robotics. We show preliminary results with one such locomotion example on discontinuous rough terrains.


Title: Applying Asynchronous Deep Classification Networks and Gaming Reinforcement Learning-Based Motion Planners to Mobile Robots
Key Words: control engineering computing  game theory  learning (artificial intelligence)  mobile robots  neural nets  path planning  pattern classification  gaming reinforcement learning-based motion planner  mobile robotic platform  deep classifier  asynchronous deep classification network  visual recognition  motion planning  deep learning-based algorithms  TT2-bot  embedded neural networks  Mobile robots  Learning (artificial intelligence)  Machine learning  Neural networks  Sensors  Training 
Abstract: In this paper, we propose a new methodology to embed deep learning-based algorithms in both visual recognition and motion planning for general mobile robotic platforms. A framework for an asynchronous deep classification network is introduced to integrate heavy deep classification networks into a mobile robot with no loss of system bandwidth. Moreover, a gaming reinforcement learning-based motion planner, a novel and convenient embodiment of reinforcement learning, is introduced for simple implementation and high applicability. The proposed approaches are implemented and evaluated on a developed robot, TT2-bot. The evaluation was based on a mission devised for a qualitative evaluation of the general purposes and performances of a mobile robotic platform. The robot was required to recognize targets with a deep classifier and plan the path effectively using a deep motion planner. As a result, the robot verified that the proposed approaches successfully integrate deep learning technologies on the stand-alone mobile robot. The embedded neural networks for recognition and path planning were critical components for the robot.


Title: Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  three-term control  DRL network  standard Deep Deterministic Policy Gradient network  training wheels  deep reinforcement learning  robotic applications  robot applications  Assisted Reinforcement Learning  PID controller  local planning  navigation problems  simple control law  Training  Navigation  Acceleration  Robot kinematics  Machine learning  Task analysis 
Abstract: Deep Reinforcement Learning (DRL) has been applied successfully to many robotic applications. However, the large number of trials needed for training is a key issue. Most of existing techniques developed to improve training efficiency (e.g. imitation) target on general tasks rather than being tailored for robot applications, which have their specific context to benefit from. We propose a novel framework, Assisted Reinforcement Learning, where a classical controller (e.g. a PID controller) is used as an alternative, switchable policy to speed up training of DRL for local planning and navigation problems. The core idea is that the simple control law allows the robot to rapidly learn sensible primitives, like driving in a straight line, instead of random exploration. As the actor network becomes more advanced, it can then take over to perform more complex actions, like obstacle avoidance. Eventually, the simple controller can be discarded entirely. We show that not only does this technique train faster, it also is less sensitive to the structure of the DRL network and consistently outperforms a standard Deep Deterministic Policy Gradient network. We demonstrate the results in both simulation and real-world experiments.


Title: Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods
Key Words: learning (artificial intelligence)  Monte Carlo methods  neural nets  robot vision  deep neural network models  off-policy correction  vision-based robotic grasping  off-policy methods  deep reinforcement learning algorithms  off-policy learning  Monte Carlo methods  Grasping  Robots  Task analysis  Benchmark testing  Monte Carlo methods  Machine learning  Training 
Abstract: In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms1.


Title: Overcoming Exploration in Reinforcement Learning with Demonstrations
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  reinforcement learning  reward function  task horizon  RL methods  exploration problem  multistep robotics tasks  robot arm  deep deterministic policy gradients  hindsight experience replay  simulated robotics tasks  Task analysis  Robots  Learning (artificial intelligence)  Stacking  Training  Mathematical model  Games 
Abstract: Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.


Title: Adaptive Sampling and Online Learning in Multi-Robot Sensor Coverage with Mixture of Gaussian Processes
Key Words: Gaussian processes  learning (artificial intelligence)  multi-robot systems  optimisation  online learning  multirobot sensor coverage  online environmental sampling  multirobot coverage control  environmental phenomenon  robot team  Gaussian Process  locally learned Gaussian Processes  collective model learning  simultaneous adaptive sampling  density function  sensing performance optimization  Robot sensing systems  Adaptation models  Density functional theory  Temperature distribution  Data models 
Abstract: We consider the problem of online environmental sampling and modeling for multi-robot sensor coverage, where a team of robots spread out over the workspace in order to optimize the overall sensing performance. In contrast to most existing works on multi-robot coverage control that assume prior knowledge of the distribution of environmental phenomenon, also known as density function, we relax this assumption and enable the robot team to efficiently learn the model of the unknown density function Online using adaptive sampling and non-parametric inference such as Gaussian Process (GP). To capture significantly different components of the environmental phenomenon, we propose a new approach with mixture of locally learned Gaussian Processes for collective model learning and an information-theoretic criterion for simultaneous adaptive sampling in multi-robot coverage. Our approach demonstrates a better generalization of the environment modeling and thus the improved performance of coverage without assuming the density function is known a priori. We demonstrate the effectiveness of our algorithm via simulations of information gathering from indoor static sensors.


Title: Distance-Based Multi-Robot Coordination on Pocket Drones
Key Words: learning (artificial intelligence)  particle filtering (numerical methods)  recurrent neural nets  remotely operated vehicles  Deep Q-Learning Network  recurrent network  UWB-distance information  neural networks  distance-based multirobot coordination  pocket drones  MicroAerial Vehicles  recurrent neural network  Drones  Robot kinematics  Recurrent neural networks  Hardware  Robot sensing systems  Distance measurement 
Abstract: We present a fully realised system illustrating decentralised coordination on Micro Aerial Vehicles (MAV) or pocket drones, based on distance information. This entails the development of an ultra light hardware solution to determine the distances between the drones and also the development of a model to learn good control policies. The model we present is a combination of a recurrent neural network and a Deep Q-Learning Network (DQN). The recurrent network provides bearing information to the DQN. The DQN itself is responsible for choosing movement actions to avoid collisions and to reach a desired position. Overall we are able provide a complete system which is capable of letting multiple drones navigate in a confined space only based on UWB-distance information and velocity input. We tackle the problem of neural networks and real world sensor noise, by combining the network with a particle filter and show that the combination outperforms the traditional particle filter in terms of converge speed and robustness. A video is available at: https://youtu.be/yj6QqhOzpok.


Title: Human-in-the-Loop Mixed-Initiative Control Under Temporal Tasks
Key Words: control engineering computing  human-robot interaction  learning (artificial intelligence)  mobile robots  motion control  path planning  planning (artificial intelligence)  temporal logic  plan adaptation scheme  short-term tasks  iterative inverse reinforcement learning algorithm  human preference  plan synthesis  human-in-the-loop simulations  mixed-initiative control  motion control  task planning problem  mobile robots  high-level tasks  Linear Temporal Logic  hard constraints  soft constraints  robot autonomy  additive terms  contingent task assignments  online coordination scheme  mixed-initiative continuous controller  temporal tasks  human initiatives  Task analysis  Safety  Robot kinematics  Automata  Planning  Navigation 
Abstract: This paper considers the motion control and task planning problem of mobile robots under complex high-level tasks and human initiatives. The assigned task is specified as Linear Temporal Logic (LTL) formulas that consist of hard and soft constraints. The human initiative influences the robot autonomy in two explicit ways: with additive terms in the continuous controller and with contingent task assignments. We propose an online coordination scheme that encapsulates (i) a mixed-initiative continuous controller that ensures all-time safety despite of possible human errors, (ii) a plan adaptation scheme that accommodates new features discovered in the workspace and short-term tasks assigned by the operator during run time, and (iii) an iterative inverse reinforcement learning (IRL) algorithm that allows the robot to asymptotically learn the human preference on the parameters during the plan synthesis. The results are demonstrated by both realistic human-in-the-loop simulations and experiments.


Title: Learning to Race Through Coordinate Descent Bayesian Optimisation
Key Words: automobiles  Bayes methods  Hilbert spaces  mobile robots  optimisation  robot dynamics  vehicle dynamics  dynamical model  robot  car racing simulation  descent Bayesian optimisation  race track  kernel Hilbert space  Bayesian optimisation  Optimization  Robot kinematics  Bayes methods  Search problems  Kernel  Linear programming 
Abstract: In the automation of many kinds of processes, the observable outcome can often be described as the combined effect of an entire sequence of actions, or controls, applied throughout the process execution. In these cases, strategies to optimise control policies for individual stages of the process are not applicable, and instead the whole policy needs to be optimised at once. On the other hand, the cost to evaluate the policy's performance might also be high, being desirable that a solution can be found with as few interactions as possible with the real system. We consider the problem of optimising control policies to allow a robot to complete a given race track within a minimum amount of time. We assume that the robot has no prior information about the track or its own dynamical model, just an initial valid driving example. Localisation is only applied to monitor the robot and to provide an indication of its position along the track's centre axis. With that in mind, we propose a method for finding a policy that minimises the time per lap while keeping the vehicle on the track using a Bayesian optimisation (BO) approach over a reproducing kernel Hilbert space. We apply an algorithm to search more efficiently over high-dimensional policy-parameter spaces with BO, by iterating over each dimension individually, in a sequential coordinate descent-like scheme. Experiments demonstrate the performance of the algorithm against other methods in a simulated car racing environment.


Title: Towards Emergence of Tool Use in Robots: Automatic Tool Recognition and Use Without Prior Tool Learning
Key Words: cognition  object recognition  robot vision  automatic tool recognition  human dexterity  skill transfer  robots cognition  robots capabilities  tools embodiment  object recognition  Tools  Task analysis  Kinematics  Automobiles  Robot sensing systems  Dynamics 
Abstract: Humans are adept at tool use. We can intuitively and immediately improvise and use unknown objects in our environment as tools, to assist us in performing tasks. In this study, we provide similar cognition and capabilities to robots. Neuroscientific studies on tool use have suggested that human dexterity with tools is enabled by the embodiment of the tools, which in effect, allows humans to immediately transfer prior skills acquired without tools, onto tasks requiring tool use. Here, utilizing the theoretical results from our investigations on embodiment and tool use in humans over the last years, we propose a concept and algorithm to enable similar skill transfer by robots. Our algorithm enables a robot that has had no prior learning with tools, to automatically recognize an object (seen for the first time) in its environment as a potential tool for an otherwise unattainable task, and use the tool to perform the task thereafter.


Title: Put-in-Box Task Generated from Multiple Discrete Tasks by aHumanoid Robot Using Deep Learning
Key Words: feature extraction  humanoid robots  learning (artificial intelligence)  manipulators  recurrent neural nets  multiple discrete tasks  deep learning  deep neural networks  robot manipulation model  DNNs  long sequential dynamic tasks  multiple short sequential tasks  multiple timescale recurrent neural network  MTRNN  initial motion steps  final motion steps  initial image input  subtask  put-in-box task  Task analysis  Robots  Feature extraction  Switches  Neurons  Training  Convolution 
Abstract: For robots to have a wide range of applications, they must be able to execute numerous tasks. However, recent studies into robot manipulation using deep neural networks (DNN) have primarily focused on single tasks. Therefore, we investigate a robot manipulation model that uses DNNs and can execute long sequential dynamic tasks by performing multiple short sequential tasks at appropriate times. To generate compound tasks, we propose a model comprising two DNNs: a convolutional autoencoder that extracts image features and a multiple timescale recurrent neural network (MTRNN) to generate motion. The internal state of the MTRNN is constrained to have similar values at the initial and final motion steps; thus, motions can be differentiated based on the initial image input. As an example compound task, we demonstrate that the robot can generate a “Put-In-Box” task that is divided into three subtasks: open the box, grasp the object and put it into the box, and close the box. The subtasks were trained as discrete tasks, and the connections between each subtask were not trained. With the proposed model, the robot could perform the Put-In-Box task by switching among subtasks and could skip or repeat subtasks depending on the situation.


Title: CASSL: Curriculum Accelerated Self-Supervised Learning
Key Words: grippers  learning (artificial intelligence)  sensitivity analysis  adaptive-underactuated multifingered gripper  curriculum accelerated self-supervised learning  variance-based global sensitivity analysis  control parameters  control dimensions  training data  CASSL orders  higher-dimensional action  map visual information  clever sampling strategy  data collection efforts  higher-dimensional control  low-dimensional action  self-supervised learning approaches  complete end-to-end learning  staged curriculum learning  CASSL framework  Aerospace electronics  Grasping  Training  Task analysis  Robots  Sensitivity analysis 
Abstract: Recent self-supervised learning approaches focus on using a few thousand data points to learn policies for high-level, low-dimensional action spaces. However, scaling this framework for higher-dimensional control requires either scaling up the data collection efforts or using a clever sampling strategy for training. We present a novel approach - Curriculum Accelerated Self-Supervised Learning (CASSL) - to train policies that map visual information to high-level, higher-dimensional action spaces. CASSL orders the sampling of training data based on control dimensions: the learning and sampling are focused on few control parameters before other parameters. The right curriculum for learning is suggested by variance-based global sensitivity analysis of the control space. We apply our CASSL framework to learning how to grasp using an adaptive, underactuated multi-fingered gripper, a challenging system to control. Our experimental results indicate that CASSL provides significant improvement and generalization compared to baseline methods such as staged curriculum learning (8% increase) and complete end-to-end learning with random exploration (14% improvement) tested on a set of novel objects.


Title: Learning to Control Redundant Musculoskeletal Systems with Neural Networks and SQP: Exploiting Muscle Properties
Key Words: biomechanics  biomimetics  bone  humanoid robots  learning (artificial intelligence)  muscle  neural nets  nonlinear control systems  physiological models  quadratic programming  machine learning approaches  muscle stimulations  high actuator redundancy  learned forward model  neural network  biomimetic muscle-driven robot show  nonlinearity  biomechanical musculoskeletal systems  quadratic programming  SQP  Muscles  Joints  Robots  Biological system modeling  Torque  Biomechanics 
Abstract: Modeling biomechanical musculoskeletal systems reveals that the mapping from muscle stimulations to movement dynamics is highly nonlinear and complex, which makes it difficult to control those systems with classical techniques. In this work, we not only investigate whether machine learning approaches are capable of learning a controller for such systems. We are especially interested in the question if the structure of the musculoskeletal apparatus exhibits properties that are favorable for the learning task. In particular, we consider learning a control policy from target positions to muscle stimulations. To account for the high actuator redundancy of biomechanical systems, our approach uses a learned forward model represented by a neural network and sequential quadratic programming to obtain the control policy, which also enables us to alternate the co-contraction level and hence allows to change the stiffness of the system and to include optimality criteria like small muscle stimulations. Experiments on both a simulated musculoskeletal model of a human arm and a real biomimetic muscle-driven robot show that our approach is able to learn an accurate controller despite high redundancy and nonlinearity, while retaining sample efficiency.


Title: Q-CP: Learning Action Values for Cooperative Planning
Key Words: iterative methods  learning (artificial intelligence)  learning systems  mobile robots  Monte Carlo methods  multi-robot systems  navigation  path planning  state-space methods  stochastic games  tree searching  uncertain systems  multirobot systems  manifold applications  unstructured scenarios  state dimensionality  model-based reinforcement learning algorithm  Q-learning  curse-of-dimensionality  cooperation scenario  mobile robots  robot behaviors  uncertainties  state space exploration  action values learning  stochastic cooperative games  cooperative navigation problem  cooperative planning  Monte-Carlo tree search iterations  general-sum games  KUKA YouBots  robot hand-overs  coordination task  Robot kinematics  Games  Monte Carlo methods  Task analysis  Planning  Stochastic processes 
Abstract: Research on multi-robot systems has demonstrated promising results in manifold applications and domains. Still, efficiently learning an effective robot behaviors is very difficult, due to unstructured scenarios, high uncertainties, and large state dimensionality (e.g, hyper-redundant and groups of robot). To alleviate this problem, we present Q-CP a cooperative model-based reinforcement learning algorithm, which exploits action values to both (1) guide the exploration of the state space and (2) generate effective policies. Specifically, we exploit Q-learning to attack the curse-of-dimensionality in the iterations of a Monte-Carlo Tree Search. We implement and evaluate Q-CP on different stochastic cooperative (general-sum) games: (1) a simple cooperative navigation problem among 3 robots, (2) a cooperation scenario between a pair of KUKA YouBots performing hand-overs, and (3) a coordination task between two mobile robots entering a door. The obtained results show the effectiveness of Q- CP in the chosen applications, where action values drive the exploration and reduce the computational demand of the planning process while achieving good performance.


Title: Robotic Assistance-as-Needed for Enhanced Visuomotor Learning in Surgical Robotics Training: An Experimental Study
Key Words: learning (artificial intelligence)  medical robotics  surgery  telerobotics  visuomotor learning  complex visuomotor training  da Vinci Research Kit surgical console  surgical teleoperated robots  surgical practice  hands-on training  surgical robotics training  Task analysis  Training  Robot kinematics  Wires  Tools  Surgery 
Abstract: Hands-on training is an indispensable part of surgical practice. As the tools used in the operating room become more intricate, the demand for efficient training methods increases. This work proposes a robotic assistance-as-needed method for training with surgical teleoperated robots. The method adapts the intensity of the assistance according to the trainee's current and past performance while gradually increasing the level of control of the trainee as the training progresses. The work includes an experiment comprising 160 acquisition sessions from 16 novice subjects performing a bimanual teleoperated exercise with a da Vinci Research Kit surgical console. Results capture the subtleties in the task's learning curve with and without robotic assistance and hint at the potential of robotic assistance for complex visuomotor training. Although robotic assistance for motor learning has received mixed results that range from beneficial to detrimental effects, this study shows such assistance may increase the rate of learning of certain skills in complex motor tasks.


Title: Fast and Reliable Autonomous Surgical Debridement with Cable-Driven Robots Using a Two-Phase Calibration Procedure
Key Words: biological tissues  calibration  cameras  diseases  edge detection  end effectors  endoscopes  medical robotics  neural nets  position control  robot vision  surgery  telerobotics  fragment phantoms  cable-driven robots  diseased tissue fragments  da Vinci Research Kit  cable-driven systems  two-phase coarse-to-fine calibration method  red calibration marker  end effector  open-loop trajectories  camera pixels  internal robot end-effector configurations  robotic surgical assistants  deep neural network  end-effector position  random forest  two-phase calibration procedure  surgical debridement  fine transformation bias  residual compensation bias  coarse transformation bias  time 7.3 s to 15.8 s  size 4.55 mm  size 2.14 mm  size 1.08 mm  Calibration  Cameras  Grippers  Robot kinematics  Robot vision systems  Tools 
Abstract: Automating precision subtasks such as debridement (removing dead or diseased tissue fragments) with Robotic Surgical Assistants (RSAs) such as the da Vinci Research Kit (dVRK) is challenging due to inherent nOnlinearities in cable-driven systems. We propose and evaluate a novel two-phase coarse-to-fine calibration method. In Phase I (coarse), we place a red calibration marker on the end effector and let it randomly move through a set of open-loop trajectories to obtain a large sample set of camera pixels and internal robot end-effector configurations. This coarse data is then used to train a Deep Neural Network (DNN) to learn the coarse transformation bias. In Phase II (fine), the bias from Phase I is applied to move the end -effector toward a small set of specific target points on a printed sheet. For each target, a human operator manually adjusts the end -effector position by direct contact (not through teleoperation) and the residual compensation bias is recorded. This fine data is then used to train a Random Forest (RF) to learn the fine transformation bias. Subsequent experiments suggest that without calibration, position errors average 4.55mm. Phase I can reduce average error to 2.14mm and the combination of Phase I and Phase II can reduces average error to 1.08mm. We apply these results to debridement of raisins and pumpkin seeds as fragment phantoms. Using an endoscopic stereo camera with standard edge detection, experiments with 120 trials achieved average success rates of 94.5 %, exceeding prior results with much larger fragments (89.4%) and achieving a speedup of 2.1x, decreasing time per fragment from 15.8 seconds to 7.3 seconds. Source code, data, and videos are available at https://sites.google.com/view/calib-icra/.


Title: Machine Learning for Placement-Insensitive Inertial Motion Capture
Key Words: calibration  image motion analysis  image sensors  multilayer perceptrons  sensor positions  body segments  standard deviation  calibration values  rotation matrices  inertial motion-capture systems  latency errors  motion data  sensor-displacement patterns  multilayer perceptrons  rotational transformations  kinematic algorithms  sensor movement  performance degradation  Euler angles  placement-insensitive inertial motion capture  machine learning  joint angles  sensor data  time 3.0 hour  Tracking  Robot sensing systems  Motion segmentation  Machine learning  Calibration  Kinematics  Neural networks 
Abstract: Although existing inertial motion-capture systems work reasonably well (≤10° error in Euler angles), their accuracy suffers when sensor positions change relative to the associated body segments (±60° mean error and 120° standard deviation). We attribute this performance degradation to undermined calibration values, sensor movement latency and displacement offsets. The latter specifically leads to incongruent rotation matrices in kinematic algorithms that rely on rotational transformations. To overcome these limitations, we propose to employ machine-learning techniques. In particular, we use multi-layer perceptrons to learn sensor-displacement patterns based on 3 hours of motion data collected from 12 test subjects in the lab over 215 trials. Furthermore, to compensate for calibration and latency errors, we directly process sensor data with deep neural networks and estimate the joint angles. Based on these approaches, we demonstrate up to 69% reduction in tracking errors.


Title: Grasping of Unknown Objects Using Deep Convolutional Neural Networks Based on Depth Images
Key Words: dexterous manipulators  end effectors  feedforward neural nets  grippers  humanoid robots  learning (artificial intelligence)  pose estimation  rendering (computer graphics)  robot vision  Deep Convolutional Neural Networks  training input  high-quality grasps  analytical grasp planners  rendered depth images  training objects  deep learning techniques  robotic grasping  approach directions  grasping setup  big data grasping database  qualitative grasping experiments  humanoid robot ARMAR-III  unknown objects  data-driven  deep learning approach  Grasping  Robots  Training  Data models  Databases  Feature extraction  Machine learning 
Abstract: We present a data-driven, bottom-up, deep learning approach to robotic grasping of unknown objects using Deep Convolutional Neural Networks (DCNNs). The approach uses depth images of the scene as its sole input for synthesis of a single-grasp solution during execution, adequately portraying the robot's visual perception during exploration of a scene. The training input consists of precomputed high-quality grasps, generated by analytical grasp planners, accompanied with rendered depth images of the training objects. In contrast to previous work on applying deep learning techniques to robotic grasping, our approach is able to handle full end-effector poses and therefore approach directions other than the view direction of the camera. Furthermore, the approach is not limited to a certain grasping setup (e. g. parallel jaw gripper) by design. We evaluate the method regarding its force-closure performance in simulation using the KIT and YCB object model datasets as well as a big data grasping database. We demonstrate the performance of our approach in qualitative grasping experiments on the humanoid robot ARMAR-III.


Title: Active Reward Learning from Critiques
Key Words: Bayes methods  control engineering computing  learning (artificial intelligence)  query processing  robot programming  critiques  active reward Learning  programming robots  active Bayesian inverse reinforcement learning  trajectory queries  labeling process  active learning  Trajectory  Robots  Learning (artificial intelligence)  Bayes methods  Entropy  Task analysis  Uncertainty 
Abstract: Learning from demonstration algorithms, such as Inverse Reinforcement Learning, aim to provide a natural mechanism for programming robots, but can often require a prohibitive number of demonstrations to capture important subtleties of a task. Rather than requesting additional demonstrations blindly, active learning methods leverage uncertainty to query the user for action labels at states with high expected information gain. However, this approach can still require a large number of labels to adequately reduce uncertainty and may also be unintuitive, as users are not accustomed to determining optimal actions in a single out-of-context state. To address these shortcomings, we propose a novel trajectory-based active Bayesian inverse reinforcement learning algorithm that (1) queries the user for critiques of automatically generated trajectories, rather than asking for demonstrations or action labels, (2) utilizes trajectory segmentation to expedite the critique / labeling process, and (3) predicts the user's critiques to generate the most highly informative trajectory queries. We evaluated our algorithm in simulated domains, finding it to compare favorably to prior work and a randomized baseline.


Title: Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling
Key Words: estimation theory  learning (artificial intelligence)  learning systems  measurement uncertainty  mobile robots  Monte Carlo methods  sampling methods  uncertainty handling  uncertainty estimation method utilizing  Monte Carlo sampling  robotics applications  autonomous driving  epistemic uncertainties  aleatoric uncertainties  uncertainty acquisition  demonstration method  sampling-free variance modeling  mixture density network  uncertainty-aware learning  Uncertainty  Predictive models  Noise measurement  Data models  Training  Estimation  Measurement uncertainty 
Abstract: In this paper, we propose an uncertainty-aware learning from demonstration method by presenting a novel uncertainty estimation method utilizing a mixture density network appropriate for modeling complex and noisy human behaviors. The proposed uncertainty acquisition can be done with a single forward path without Monte Carlo sampling and is suitable for real-time robotics applications. Then, we show that it can be decomposed into explained variance and unexplained variance where the connections between aleatoric and epistemic uncertainties are addressed. The properties of the proposed uncertainty measure are analyzed through three different synthetic examples, absence of data, heavy measurement noise, and composition of functions scenarios. We show that each case can be distinguished using the proposed uncertainty measure and presented an uncertainty-aware learning from demonstration method for autonomous driving using this property. The proposed uncertainty-aware learning from demonstration method outperforms other compared methods in terms of safety using a complex real-world driving dataset.


Title: Human-Driven Feature Selection for a Robotic Agent Learning Classification Tasks from Demonstration
Key Words: feature extraction  learning (artificial intelligence)  pattern classification  robots  LfD scenarios  human feature selection  robot learner  informative features  multiclass classification task  computational feature selection  human selected features  informative task features  general-purpose robot  learning computation  robotic agent learning classification tasks  human-driven feature selection  Task analysis  Feature extraction  Robots  Training  Training data  Object recognition  Support vector machines 
Abstract: The state features available to a robot define the variables on which the learning computation depends. However, little prior work considers feature selection in the context of deploying a general-purpose robot able to learn new tasks. In this work, we explore human-driven feature selection in which a robotic agent can identify useful features with the aid of a human user, by extracting information from users about which features are most informative for discriminating between classes of objects needed for a given task (e.g. sorting groceries). The research questions examine (a) whether a domain expert is able to identify a subset of informative task features, (b) whether human selected features will enable the agent to classify unseen examples as accurately as using computational feature selection, and (c) if the interaction strategy used to elicit the information from the user impacts the quality of resulting feature selection. Toward that end, we conducted a user study with 30 participants on campus, given a multi-class classification task and one of five different approaches for conveying information about informative features to a robot learner. Our findings show that when features are semantically interpretable, human feature selection is effective in LfD scenarios because it is able to outperform computational methods when there is limited training data, yet still remains on-par with computational methods as the training sample size increases.


Title: Deep Auxiliary Learning for Visual Localization and Odometry
Key Words: distance measurement  feature extraction  learning (artificial intelligence)  neural nets  pose estimation  video signal processing  state-of-the-art SIFT-based approaches  deep learning technique  multitask learning  Geometric Consistency Loss  visual odometry estimation  global localization  parameter sharing  multitask model  consecutive monocular images  VLocNet  convolutional neural networks  action execution  robot  visual localization  Task analysis  Visual odometry  Estimation  Visualization  Training  Robustness 
Abstract: Localization is an indispensable component of a robot's autonomy stack that enables it to determine where it is in the environment, essentially making it a precursor for any action execution or planning. Although convolutional neural networks have shown promising results for visual localization, they are still grossly outperformed by state-of-the-art local feature-based techniques. In this work, we propose VLocNet, a new convolutional neural network architecture for 6-DoF global pose regression and odometry estimation from consecutive monocular images. Our multitask model incorporates hard parameter sharing, thus being compact and enabling real-time inference, in addition to being end-to-end trainable. We propose a novel loss function that utilizes auxiliary learning to leverage relative pose information during training, thereby constraining the search space to obtain consistent pose estimates. We evaluate our proposed VLocNet on indoor as well as outdoor datasets and show that even our single task model exceeds the performance of state-of-the-art deep architectures for global localization, while achieving competitive performance for visual odometry estimation. Furthermore, we present extensive experimental evaluations utilizing our proposed Geometric Consistency Loss that show the effectiveness of multitask learning and demonstrate that our model is the first deep learning technique to be on par with, and in some cases outperforms state-of-the-art SIFT-based approaches.


Title: Generalizing Informed Sampling for Asymptotically-Optimal Sampling-Based Kinodynamic Planning via Markov Chain Monte Carlo
Key Words: approximation theory  convergence of numerical methods  estimation theory  learning (artificial intelligence)  Markov processes  Monte Carlo methods  optimisation  path planning  random processes  sampling methods  state-space methods  trees (mathematics)  asymptotically-optimal motion planners  subsequent samples  motion-planning problem  Euclidean space  nonEuclidean state spaces  dimensional state space  planning algorithm  sub-level-set  Monte Carlo sampling methods  high-quality solutions  high-dimensional problems  Markov chain Monte Carlo  informed set  generalizing informed sampling  asymptotically-optimal sampling-based kinodynamic planning  hierarchical rejection sampling  Trajectory  Planning  Monte Carlo methods  Markov processes  Cost function  Heuristic algorithms 
Abstract: Asymptotically-optimal motion planners such as RRT* have been shown to incrementally approximate the shortest path between start and goal states. Once an initial solution is found, their performance can be dramatically improved by restricting subsequent samples to regions of the state space that can potentially improve the current solution. When the motion-planning problem lies in a Euclidean space, this region Xinf, called the informed set, can be sampled directly. However, when planning with differential constraints in non-Euclidean state spaces, no analytic solutions exists to sampling Xinf directly. State-of-the-art approaches to sampling Xinf in such domains such as Hierarchical Rejection Sampling (HRS) may still be slow in high -dimensional state space. This may cause the planning algorithm to spend most of its time trying to produces samples in Xinf rather than explore it. In this paper, we suggest an alternative approach to produce samples in the informed set Xinf for a wide range of settings. Our main insight is to recast this problem as one of sampling uniformly within the sub-level-set of an implicit non-convex function. This recasting enables us to apply Monte Carlo sampling methods, used very effectively in the Machine Learning and Optimization communities, to solve our problem. We show for a wide range of scenarios that using our sampler can accelerate the convergence rate to high-quality solutions in high-dimensional problems.


Title: Dancing PRM*: Simultaneous Planning of Sampling and Optimization with Configuration Free Space Approximation
Key Words: approximation theory  optimisation  path planning  grid-based approaches  optimization-based planner  resolution-complete factors  spatial information  empirical information  learned information  optimization-based local planner  asymptotic optimal planners  simultaneous planning  configuration free space approximation  optimal motion planning  sampling-based planner  Dancing PRM  Planning  Approximation algorithms  Trajectory  Optimization  Robots  Probabilistic logic  Linear programming 
Abstract: A recent trend in optimal motion planning has broadened the research area toward the hybridization of sampling, optimization and grid-based approaches. We can expect that synergy from such integrations leads to overall performance improvement, but seamless integration and generalization is still an open problem. In this paper, we suggest a hybrid motion planning algorithm utilizing a sampling-based and optimization-based planner while simultaneously approximating a configuration free space. Unlike conventional optimization-based approaches, the proposed algorithm does not depend on a priori information or resolution-complete factors, e.g., a distance field. Ours instead learns spatial information on the fly by exploiting empirical information during the execution, and decentralizes the information over the constructed graph for efficient access. With the help of the learned information, our optimization-based local planner exploits the local area to identify the connectivity of configuration free space without depending on the precomputed domain knowledge. To show the novelty of proposed algorithm, we evaluate it against other asymptotic optimal planners in both synthetic and complex benchmarks with varying degrees of freedom. We also discuss the performance improvement, properties and limitations we have observed.


Title: Learning Sampling Distributions for Robot Motion Planning
Key Words: collision avoidance  mobile robots  sampling methods  robot motion planning  sampling-based motion planning  collision-avoidance  variational autoencoder  bias sampling  Planning  Robots  Probabilistic logic  Manifolds  Collision avoidance  Feature extraction  Acceleration 
Abstract: A defining feature of sampling-based motion planning is the reliance on an implicit representation of the state space, which is enabled by a set of probing samples. Traditionally, these samples are drawn either probabilistically or deterministically to uniformly cover the state space. Yet, the motion of many robotic systems is often restricted to “small” regions of the state space, due to e.g. differential constraints or collision-avoidance constraints. To accelerate the planning process, it is thus desirable to devise non-uniform sampling strategies that favor sampling in those regions where an optimal solution might lie. This paper proposes a methodology for nonuniform sampling, whereby a sampling distribution is learned from demonstrations, and then used to bias sampling. The sampling distribution is computed through a conditional variational autoencoder, allowing sample generation from the latent space conditioned on the specific planning problem. This methodology is general, can be used in combination with any sampling-based planner, and can effectively exploit the underlying structure of a planning problem while maintaining the theoretical guarantees of sampling-based approaches. Specifically, on several planning problems, the proposed methodology is shown to effectively learn representations for the relevant regions of the state space, resulting in an order of magnitude improvement in terms of success rate and convergence to the optimal cost.


Title: Deep Object-Centric Representations for Generalizable Robot Learning
Key Words: intelligent robots  learning (artificial intelligence)  manipulators  visual perception  robotic manipulation  generalizable robot learning  object-centric representations  reinforcement learning  object-level attentional mechanism  perception system  semantic feature space  Task analysis  Visualization  Semantics  Trajectory  Computer vision  Standards 
Abstract: Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose using an object-centric prior and a semantic feature space for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine relevant objects from a few trajectories or demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent attention locates possible objects in the scene, and a task-specific attention identifies which objects are predictive of the trajectories. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse relevant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.


Title: Generative One-Shot Learning (GOL): A Semi-Parametric Approach to One-Shot Learning in Autonomous Vision
Key Words: computer vision  learning (artificial intelligence)  mobile robots  neural nets  object recognition  Pareto optimisation  GOL  input single one-shot objects  environment perception  autonomous vision  semiparametric approach  deep neural networks  visual perception  driving environment  training perceptions systems  generative framework  highly autonomous driving systems  generative one-shot learning  HAD systems  Pareto optimal solutions  object detection algorithms  Pareto optimization  Training  Autonomous vehicles  Generators  Linear programming  Probability density function 
Abstract: Highly Autonomous Driving (HAD) systems rely on deep neural networks for the visual perception of the driving environment. Such networks are train on large manually annotated databases. In this work, a semi-parametric approach to one-shot learning is proposed, with the aim of bypassing the manual annotation step required for training perceptions systems used in autonomous driving. The proposed generative framework, coined Generative One-Shot Learning (GOL), takes as input single one-shot objects, or generic patterns, and a small set of so-called regularization samples used to drive the generative process. New synthetic data is generated as Pareto optimal solutions from one-shot objects using a set of generalization functions built into a generalization generator. GOL has been evaluated on environment perception challenges encountered in autonomous vision.


Title: Adaptive Deep Learning Through Visual Domain Localization
Key Words: generalisation (artificial intelligence)  humanoid robots  human-robot interaction  image classification  learning (artificial intelligence)  robot vision  robot vision  domain shift  end-to-end deep domain adaptation architecture  target domain  training time  human-robot interactions  adaptive deep  visual domain localization  commercial robot  illumination conditions  domain adaptation methods  robotics applications  computer vision  generalization issue  iCub World database  Visualization  Training  Adaptive systems  Adaptation models  Machine learning  Service robots 
Abstract: A commercial robot, trained by its manufacturer to recognize a predefined number and type of objects, might be used in many settings, that will in general differ in their illumination conditions, background, type and degree of clutter, and so on. Recent computer vision works tackle this generalization issue through domain adaptation methods, assuming as source the visual domain where the system is trained and as target the domain of deployment. All approaches assume to have access to images from all classes of the target during training, an unrealistic condition in robotics applications. We address this issue proposing an algorithm that takes into account the specific needs of robot vision. Our intuition is that the nature of the domain shift experienced mostly in robotics is local. We exploit this through the learning of maps that spatially ground the domain and quantify the degree of shift, embedded into an end-to-end deep domain adaptation architecture. By explicitly localizing the roots of the domain shift we significantly reduce the number of parameters of the architecture to tune, we gain the flexibility necessary to deal with subset of categories in the target domain at training time, and we provide a clear feedback on the rationale behind any classification decision, which can be exploited in human-robot interactions. Experiments on two different settings of the iCub World database confirm the suitability of our method for robot vision.


Title: Towards Understanding Object-Directed Actions: A Generative Model for Grounding Syntactic Categories of Speech Through Visual Perception
Key Words: Bayes methods  cognition  hidden Markov models  human-robot interaction  image segmentation  learning (artificial intelligence)  manipulators  object detection  robot vision  object-directed actions  human arm joints  manipulating objects  segmented objects  successful human-robot collaboration  high-level cognitive functions  human language  human actions  Hidden Markov models  Grounding  Tagging  Three-dimensional displays  Robots  Computational modeling  Probabilistic logic 
Abstract: Creating successful human-robot collaboration requires robots to have high-level cognitive functions that could allow them to understand human language and actions in space. To meet this target, an elusive challenge that we address in this paper is to understand object-directed actions through grounding language based on visual cues representing the dynamics of human actions on objects, object characteristics (color and geometry), and spatial relationships between objects in a tabletop scene. The proposed probabilistic framework investigates unsupervised Part-of-Speech (POS) tagging to determine syntactic categories of words so as to infer grammatical structure of language. The dynamics of object-directed actions are characterized through the locations of the human arm joints - modeled on a Hidden Markov Model (HMM) - while manipulating objects, in addition to those of objects represented in 3D point clouds. These corresponding point clouds to segmented objects encode geometric features and spatial semantics of referents and landmarks in the environment. The proposed Bayesian learning model is successfully evaluated through interaction experiments between a human user and Toyota HSR robot in space.


Title: GeneSIS-Rt: Generating Synthetic Images for Training Secondary Real-World Tasks
Key Words: collision avoidance  image segmentation  learning (artificial intelligence)  synthetic images  synthetic data  domain-specific learning tasks  leverage recent progress  image-to-image translation  simulated images  realistic training data  real-world images  GeneSIS-Rtameliorates  GeneSIS-Rtto  high-accuracy predictions  raw simulated data  GeneSIS-RT images  mission-critical tasks  secondary real-world task training  cluttered environment  reactive obstacle avoidance  semantic segmentation  Training  Task analysis  Semantics  Image segmentation  Collision avoidance  Gallium nitride  Training data 
Abstract: We propose a novel approach for generating high-quality, synthetic data for domain-specific learning tasks, for which training data may not be readily available. We leverage recent progress in image-to-image translation to bridge the gap between simulated and real images, allowing us to generate realistic training data for real-world tasks using only unlabeled real-world images and a simulation. GeneSIS-Rtameliorates the burden of having to collect labeled real-world images and is a promising candidate for generating high-quality, domain-specific, synthetic data. To show the effectiveness of using GeneSIS-Rtto create training data, we study two tasks: semantic segmentation and reactive obstacle avoidance. We demonstrate that learning algorithms trained using data generated by GeneSIS-RT make high-accuracy predictions and outperform systems trained on raw simulated data alone, and as well or better than those trained on real data. Finally, we use our data to train a quadcopter to fly 60 meters at speeds up to 3.4 m/s through a cluttered environment, demonstrating that our GeneSIS-RT images can be used to learn to perform mission-critical tasks.


Title: Faster R-CNN with Classifier Fusion for Small Fruit Detection
Key Words: convolution  crops  feedforward neural nets  image classification  image fusion  object detection  probability  recurrent neural nets  robot vision  multiple classifiers  classifier correlation  small fruit detection  Faster R-CNN network  multiple classifier fusion  objectness classification  probabilities  agricultural robots  Proposals  Correlation  Feature extraction  Image segmentation  Machine learning  Robots  Adaptation models 
Abstract: The-state-of-the-art of fruit detection with Faster R-CNN shows lack of detection advantage on small fruits. One of reasons is only single level features is used for localization of proposal candidates. In this paper, we propose to incorporate a multiple classifier fusion strategy into a Faster R-CNN network for small fruit detection. We utilize features from three different levels to learn three classifiers for objectness classification in the stage of proposal localization. Probabilities from classifiers are combined by a simple convolutional layer to generate final objectness classification for proposal candidates. In order to keep diversity of multiple classifiers, a novel loss term of classifier correlation is introduced into original loss function. Experimental results show that our model is feasible for detecting small fruits.


Title: Robust Human Following by Deep Bayesian Trajectory Prediction for Home Service Robots
Key Words: Bayes methods  collision avoidance  human-robot interaction  learning (artificial intelligence)  mobile robots  object detection  predictive control  robot vision  robust control  service robots  trajectory control  variational techniques  RoboCup@Home 2017 Social Standard Platform League  robust functions  home service robots  service-oriented robots  human assistance  commercial service robot  RGB-D camera  deep learning methods  variational Bayesian techniques  deep learning modules  dynamic home environment  deep Bayesian trajectory prediction method  collision avoidance  robust human following  smooth person following capability  human cooperation  robustness  target detection  robot following ability  Robot kinematics  Trajectory  Collision avoidance  Robustness  Robot sensing systems  Bayes methods 
Abstract: The capability of following a person is crucial in service-oriented robots for human assistance and cooperation. Though a vast variety of following systems exist, they lack robustness against dynamic changes of the environment and relocating to continue following a lost target. Here we present a robust human following system that has the extendability to commercial service robot platforms having a RGB-D camera. The proposed framework integrates deep learning methods for perception and variational Bayesian techniques for trajectory prediction. Deep learning modules enable robots to accompany a person by detecting the target, learning the target and following while avoiding collision within the dynamic home environment. The variational Bayesian techniques robustly predict the trajectory of the target by empowering the following ability of the robot when target is lost. We experimentally demonstrate the capability of the deep Bayesian trajectory prediction method on real-time usage, following abilities, collision avoidance and trajectory prediction of the system. The proposed system was deployed at the RoboCup@Home 2017 Social Standard Platform League and successfully demonstrated its robust functions and smooth person following capability resulting in winning the 1st place.


Title: A Nonparametric Motion Flow Model for Human Robot Cooperation
Key Words: Gaussian processes  human-robot interaction  image motion analysis  image representation  image sequences  learning (artificial intelligence)  optimisation  nonparametric motion flow model  human robot cooperation method  partial trajectory information  target trajectories  learned motion description  underlying reward function  interacting trajectories  variance functions  temporal properties  spatial properties  motion flow similarity measure  motion trajectory  Trajectory  Motion measurement  Kernel  Computational modeling  Robot sensing systems  Task analysis 
Abstract: In this paper, we present a novel nonparametric motion flow model that effectively describes a motion trajectory of a human and its application to human robot cooperation. To this end, motion flow similarity measure which considers both spatial and temporal properties of a trajectory is proposed by utilizing the mean and variance functions of a Gaussian process. We also present a human robot cooperation method using the proposed motion flow model. Given a set of interacting trajectories of two workers, the underlying reward function of cooperating behaviors is optimized by using the learned motion description as an input to the reward function where a stochastic trajectory optimization method is used to control a robot. The presented human robot cooperation method is compared with the state-of-the-art algorithm, which utilizes a mixture of interaction primitives (MIP), in terms of the RMS error between generated and target trajectories. While the proposed method shows comparable performance with the MIP when the full observation of human demonstrations is given, it shows superior performance when partial trajectory information is given.


Title: Learning by Demonstration and Adaptation of Finishing Operations Using Virtual Mechanism Approach
Key Words: force sensors  grinding  grinding machines  industrial robots  iterative learning control  polishing  polishing machines  robot dynamics  robot kinematics  surface finishing  finishing operations  virtual mechanism approach  passive digitizer  optimal robot execution  serial kinematic chain  augmented system  polishing tools  grinding tool  iterative learning controller  Robot kinematics  Task analysis  Tools  Service robots  Trajectory  Quaternions 
Abstract: In this paper we propose a new approach for efficient programming of grinding and polishing operation. In the proposed system, the initial policy is performed by a skilled operator and recorded with a passive digitizer. The demonstrated policy comprises both position and force data. The optimal robot execution of the task is provided by applying a virtual mechanism approach, which models the polishing/grinding tool as a serial kinematic chain. By joining the robot and the virtual mechanism in an augmented system, additional degrees of freedom are obtained and redundancy resolution can be applied to optimize the demonstrated motion. Another benefit of the proposed approach is that the same policy can be transferred to different combination of robots and grinding/polishing tools without any modification of the captured motion. The proposed approach requires known contact point between the treated object and the polishing/grinding tool. We propose a novel approach for accurate estimation of this point using data obtained from the force-torque sensor. Finally, the demonstrated path is refined to compensate for inaccurate calibration and different dynamics of a robot and the human demonstrator using iterative learning controller. The proposed method was verified in a real industrial environment.


Title: Hybrid Probabilistic Trajectory Optimization Using Null-Space Exploration
Key Words: humanoid robots  learning systems  manipulator kinematics  probability  trajectory control  joint space  motion constraints  probabilistic formulation  dynamic movement primitives  probabilistic treatment  trajectory constraints  hybrid space learning  motion smoothness  robot null-space  hybrid probabilistic trajectory optimization  null-space exploration  Cartesian space  learning from demonstration  Jacobian-based inverse kinematics  Probabilistic logic  Task analysis  Robot kinematics  Acceleration  Trajectory optimization 
Abstract: In the context of learning from demonstration, human examples are usually imitated in either Cartesian or joint space. However, this treatment might result in undesired movement trajectories in either space. This is particularly important for motion skills such as striking, which typically imposes motion constraints in both spaces. In order to address this issue, we consider a probabilistic formulation of dynamic movement primitives, and apply it to adapt trajectories in Cartesian and joint spaces simultaneously. The probabilistic treatment allows the robot to capture the variability of multiple demonstrations and facilitates the mixture of trajectory constraints from both spaces. In addition to this proposed hybrid space learning, the robot often needs to consider additional constraints such as motion smoothness and joint limits. On the basis of Jacobian-based inverse kinematics, we propose to exploit robot null-space so as to unify trajectory constraints from Cartesian and joint spaces while satisfying additional constraints. Evaluations of hand-shaking and striking tasks carried out with a humanoid robot demonstrate the applicability of our approach.


Title: UnDeepVO: Monocular Visual Odometry Through Unsupervised Deep Learning
Key Words: cameras  feature extraction  learning (artificial intelligence)  neural nets  pose estimation  stereo image processing  unsupervised learning  monocular visual odometry system  monocular camera  deep neural networks  unsupervised deep learning scheme  UnDeepVo  Training  Cameras  Machine learning  Three-dimensional displays  Estimation  Image sequences  Visual odometry 
Abstract: We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVo:one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVoby using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. 1. The experiments on KITTI dataset show our UnDeepVO achieves good performance in terms of pose accuracy.


Title: Verifying Controllers Against Adversarial Examples with Bayesian Optimization
Key Words: Bayes methods  Gaussian processes  learning (artificial intelligence)  optimisation  robots  safety  complex safety specifications  complex controllers  Bayesian optimization  adversarial examples  coherent optimization framework  Gaussian Process prior  individual functions  reinforcement learning  reward functions  smooth functions  complex boolean combinations  adversarial counter examples  safety constraints  Bayesian Optimization  active-testing framework  safety-critical applications  Safety  Uncertainty  Robots  Testing  Trajectory  Optimization  Bayes methods 
Abstract: Recent successes in reinforcement learning have lead to the development of complex controllers for realworld robots. As these robots are deployed in safety-critical applications and interact with humans, it becomes critical to ensure safety in order to avoid causing harm. A first step in this direction is to test the controllers in simulation. To be able to do this, we need to capture what we mean by safety and then efficiently search the space of all behaviors to see if they are safe. In this paper, we present an active-testing framework based on Bayesian Optimization. We specify safety constraints using logic and exploit structure in the problem in order to test the system for adversarial counter examples that violate the safety specifications. These specifications are defined as complex boolean combinations of smooth functions on the trajectories and, unlike reward functions in reinforcement learning, are expressive and impose hard constraints on the system. In our framework, we exploit regularity assumptions on individual functions in form of a Gaussian Process (GP) prior. We combine these into a coherent optimization framework using problem structure. The resulting algorithm is able to provably verify complex safety specifications or alternatively find counter examples. Experimental results show that the proposed method is able to find adversarial examples quickly.


Title: Learning-Based Model Predictive Control Under Signal Temporal Logic Specifications
Key Words: control system synthesis  Gaussian processes  learning (artificial intelligence)  predictive control  regression analysis  temporal logic  learning-based model predictive control method  differential constraints  dynamical systems  control strategy synthesis method  signal temporal logic specifications  specific rules  model predictive control procedure  learned margin  signal temporal logic formula  designed controller  traditional control scheme  Predictive control  Robustness  Collision avoidance  Task analysis  Gaussian processes  Service robots 
Abstract: This paper presents a control strategy synthesis method for dynamical systems with differential constraints while satisfying a set of given rules in consideration of their importances. A special attention is given to situations where all rules cannot be met in order to fulfill a given task. Such dilemmas compel us to make a decision on the degree of satisfaction of each rule including which rule should be maintained or not. In this work, we propose a learning-based model predictive control method in order to solve this problem, where a key insight is to combine a learning method and traditional control scheme so that the designed controller behaves close to human experts. A rule is represented as a signal temporal logic (STL) formula. A robustness slackness, a margin to the satisfaction of the rule, is learned from expert's demonstrations using Gaussian process regression. The learned margin is used in a model predictive control procedure, which helps to decide how much to obey each rule, even ignoring specific rules. In track driving simulation, we show that the proposed method generates human-like behavior and efficiently handles dilemmas as human teachers do.


Title: A Robust Robot Design for Item Picking
Key Words: calibration  cameras  grippers  industrial manipulators  learning (artificial intelligence)  path planning  robot vision  service robots  feature-based comparison  gripper system  grasping strategy  robust performance  target items  robot system  dual 6 degrees of freedom industrial arms  error recovery strategies  fixed calibrated frame  multiple stereo cameras  vision system  custom-designed top-open extendable shelf  calibrated table  fixed bases  module designs  component selection  motion planning  system requirements  Amazon Robotics Challenge  reliable system  stable system  item picking  robust robot design  Cameras  Manipulators  Task analysis  Planning  Service robots  Robot vision systems 
Abstract: In order to build a stable and reliable system for the Amazon Robotics Challenge we went through a detailed study of the performance and system requirements based on the rules and our past experience of the challenge. The challenge was to build a robot that integrates grasping, vision, motion planning, among others, to be able to pick items from a shelf to specific order boxes. This paper presents the development process including component selection, module designs, and deployment. The resulting robot system has dual 6 degrees of freedom industrial arms mounted on fixed bases, which in turn are mounted on a calibrated table. The robot works with a custom-designed top-open extendable shelf. The vision system uses multiple stereo cameras mounted on a fixed calibrated frame. Feature-based comparison and machine-learning based matching are used to identify and determine item pose. The gripper system uses suction cup and the grasping strategy is pick from the top. Error recovery strategies were also implemented to ensure robust performance. During the competition, the robot was able to pick all target items with the shortest amount of time.


Title: Pick and Place Without Geometric Object Models
Key Words: geometry  learning (artificial intelligence)  manipulators  geometric object models  robotic pick  deep reinforcement learning problem  deep RL  robotic manipulation frame  low level states  pick-place  regrasping problems  exact geometry  sensor perception  Shape  History  Task analysis  Robot sensing systems  Three-dimensional displays  Geometry 
Abstract: We propose a novel formulation of robotic pick and place as a deep reinforcement learning (RL) problem. Whereas most deep RL approaches to robotic manipulation frame the problem in terms of low level states and actions, we propose a more abstract formulation. In this formulation, actions are target reach poses for the hand and states are a history of such reaches. We show this approach can solve a challenging class of pick-place and regrasping problems where the exact geometry of the objects to be handled is unknown. The only information our method requires is: 1) the sensor perception available to the robot at test time; 2) prior knowledge of the general class of objects for which the system was trained. We evaluate our method using objects belonging to two different categories, mugs and bottles, both in simulation and on real hardware. Results show a major improvement relative to a shape primitives baseline.


Title: Semantic Robot Programming for Goal-Directed Manipulation in Cluttered Scenes
Key Words: graph theory  image colour analysis  learning (artificial intelligence)  manipulators  path planning  pose estimation  robot programming  robot vision  object geometries  Semantic Robot Programming  task planning  motion planning  Discriminatively-Informed Generative Estimation of Scenes and Transforms  DIGEST method  RGBD images  goal-directed manipulation  cluttered scene dataset  Michigan Progress Fetch robot  object poses  robot manipulator  SRP  semantic mapping  Task analysis  Semantics  Robot programming  Estimation  Planning  Detectors 
Abstract: We present the Semantic Robot Programming (SRP) paradigm as a convergence of robot programming by demonstration and semantic mapping. In SRP, a user can directly program a robot manipulator by demonstrating a snapshot of their intended goal scene in workspace. The robot then parses this goal as a scene graph comprised of object poses and inter-object relations, assuming known object geometries. Task and motion planning is then used to realize the user's goal from an arbitrary initial scene configuration. Even when faced with different initial scene configurations, SRP enables the robot to seamlessly adapt to reach the user's demonstrated goal. For scene perception, we propose the Discriminatively-Informed Generative Estimation of Scenes and Transforms (DIGEST) method to infer the initial and goal states of the world from RGBD images. The efficacy of SRP with DIGEST perception is demonstrated for the task of tray-setting with a Michigan Progress Fetch robot. Scene perception and task execution are evaluated with a public household occlusion dataset and our cluttered scene dataset.


Title: Cross-Domain Transfer in Reinforcement Learning Using Target Apprentice
Key Words: generalisation (artificial intelligence)  learning (artificial intelligence)  target apprentice learning  cross-domain transfer  reinforcement learning  cross-domain tasks  target task learning  target domain  policy augmentation  Task analysis  Adaptation models  Automobiles  Manifolds  Bicycles  Learning (artificial intelligence)  Complexity theory 
Abstract: In this paper, we present a new approach to transfer in Reinforcement Learning (RL) for cross-domain tasks. Unlike, available transfer approaches, where target task learning is accelerated through initialized learning from source, we propose to adapt and reuse the optimal source policy directly in the related domains. We show the optimal policy from a related source task can be near optimal in target domain provided an adaptive policy accounts for the model error between target and the projected source. A significant advantage of the proposed policy augmentation is in generalizing the policies across related domains without having to re-Iearn the new tasks. We demonstrate that, this architecture leads to better sample efficiency in the transfer, reducing sample complexity of target task learning to target apprentice learning.


Title: Intent-Aware Multi-Agent Reinforcement Learning
Key Words: aerospace robotics  control engineering computing  decision theory  function approximation  learning (artificial intelligence)  Markov processes  multi-agent systems  planning (artificial intelligence)  robot dynamics  low-level planning algorithms  intent-aware multiagent reinforcement learning  learning algorithm  planning process  partially observable Markov decision process  linear function approximation  intent-aware multiagent planning  aerial robots  human interaction  dynamic process  POMDP  Planning  Prediction algorithms  Automata  Vehicles  History  Computational modeling 
Abstract: This paper proposes an intent-aware multi-agent planning framework as well as a learning algorithm. Under this framework, an agent plans in the goal space to maximize the expected utility. The planning process takes the belief of other agents' intents into consideration. Instead of formulating the learning problem as a partially observable Markov decision process (POMDP), we propose a simple but effective linear function approximation of the utility function. It is based on the observation that for humans, other people's intents will pose an influence on our utility for a goal. The proposed framework has several major advantages: i) it is computationally feasible and guaranteed to converge. ii) It can easily integrate existing intent prediction and low-level planning algorithms. iii) It does not suffer from sparse feedbacks in the action space. We experiment our algorithm in a real-world problem that is non-episodic, and the number of agents and goals can vary over time. Our algorithm is trained in a scene in which aerial robots and humans interact, and tested in a novel scene with a different environment. Experimental results show that our algorithm achieves the best performance and human-like behaviors emerge during the dynamic process.


Title: Improving Model-Based Balance Controllers Using Reinforcement Learning and Adaptive Sampling
Key Words: humanoid robots  learning (artificial intelligence)  optimal control  sampling methods  control signals  adaptive sampling  model-based balance controllers  humanoid model  in-place balancing  training disturbances  standard reinforcement learning formulations  deep reinforcement learning techniques  control policy  RoA  model-based optimal controller  learning framework  full-body actions  nonplanar pushes  full-body dynamics  optimal control theory  Perturbation methods  Hip  Adaptation models  Robots  Training  Lips  Learning (artificial intelligence) 
Abstract: Balance control to recover from a wide range of disturbances is an important skill for humanoid robots. Traditionally, researchers have often designed a balance controller by applying optimal control theory on a simplified model that abstracts the full-body dynamics. However, the resulting controller may not be able to recover from unexpected scenarios such as non-planar pushes, or fail to exploit full-body actions such as balancing with arm movements. This paper presents a learning framework for enhancing the performance of a model-based optimal controller by expanding the region of attraction (RoA). We train a control policy that generates additional control signals on top of the model-based controller using deep reinforcement learning techniques. Instead of relying on standard reinforcement learning formulations, we explicitly model the region of attraction and continuously adjust it during the training. By drawing the training disturbances at the boundary of the RoA, we can effectively expand the RoA while avoiding local minima. We test our learning framework for in-place balancing as well as balancing with stepping on a humanoid model in simulation.


Title: Deep Reinforcement Learning Supervised Autonomous Exploration in Office Environments
Key Words: decision making  learning (artificial intelligence)  mobile robots  path planning  supervised autonomous exploration  office environments  exploration region selection  autonomous robot exploration task  greedy methods  long-term planning  deep reinforcement learning  exploration knowledge  office blueprints  DRL model  next-best-view selection approach  structural integrity measurement  office maps  decision making process  Planning  Optimization  Prediction algorithms  Task analysis  Predictive models  Computer architecture  Uncertainty 
Abstract: Exploration region selection is an essential decision making process in autonomous robot exploration task. While a majority of greedy methods are proposed to deal with this problem, few efforts are made to investigate the importance of predicting long-term planning. In this paper, we present an algorithm that utilizes deep reinforcement learning (DRL) to learn exploration knowledge over office blueprints, which enables the agent to predict a long-term visiting order for unexplored subregions. On the basis of this algorithm, we propose an exploration architecture that integrates a DRL model, a next-best-view (NBV) selection approach and a structural integrity measurement to further improve the exploration performance. At the end of this paper, we evaluate the proposed architecture against other methods on several new office maps, showing that the agent can efficiently explore uncertain regions with a shorter path and smarter behaviors.


Title: Bayesian Optimization with Automatic Prior Selection for Data-Efficient Direct Policy Search
Key Words: Bayes methods  control engineering computing  learning (artificial intelligence)  legged locomotion  manipulators  maximum likelihood estimation  motion control  optimisation  search problems  Most Likely Expected Improvement  5DOF planar arm  6-legged robot  transfer learning task  acquisition function  data-efficient direct policy search  automatic prior selection  Bayesian optimization  Optimization  Bayes methods  Legged locomotion  Task analysis  Predictive models  Computational modeling 
Abstract: One of the most interesting features of Bayesian optimization for direct policy search is that it can leverage priors (e.g., from simulation or from previous tasks) to accelerate learning on a robot. In this paper, we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation. We tackle this problem by introducing a novel acquisition function, called Most Likely Expected Improvement (MLEI), that combines the likelihood of the priors and the expected improvement. We evaluate this new acquisition function on a transfer learning task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has to learn to walk on flat ground and on stairs, with priors corresponding to different stairs and different kinds of damages. Our results show that MLEI effectively identifies and exploits the priors, even when there is no obvious match between the current situations and the priors.


Title: Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning
Key Words: computational complexity  learning (artificial intelligence)  neural nets  predictive control  model-free learning  model-free fine-tuning  model-free deep reinforcement learning algorithms  model-based algorithms  model predictive control  model-based reinforcement learning algorithm  complex locomotion tasks  deep neural network dynamics models  model-free learner  model-based approaches  model-free methods  sample complexity  model-based deep reinforcement learning  robotic skills  MPC  plausible gaits  stable gaits  Task analysis  Predictive models  Neural networks  Data models  Heuristic algorithms  Machine learning  Complexity theory 
Abstract: Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that neural network dynamics models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits that accomplish various complex locomotion tasks. We further propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5× on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf.


Title: Adapting Parameterized Motions Using Iterative Learning and Online Collision Detection
Key Words: Bayes methods  collision avoidance  Gaussian processes  intelligent robots  learning (artificial intelligence)  optimisation  robotic assembly  servomotors  signal classification  iterative learning  online collision detection  robust robot system  uncertainty-tolerant motions  Gaussian Process learning  Bayesian Optimization  robot motor currents  assembly process  medium-sized productions  parameterized motions  Collision avoidance  Robot sensing systems  Robustness  Robotic assembly  Trajectory 
Abstract: Achieving both the flexibility and robustness required to advance the use of robotics in small and medium-sized productions is an essential but difficult task. A fundamental problem is making the robot run blindly without additional sensors while still being robust to uncertainties and variations in the assembly processes. In this paper, we address the use of parameterized motions suitable for blind execution and robust to uncertainties in the assembly process. Collisions and incorrect assemblies are detected based on robot motor currents while motion parameters are updated based on Bayesian Optimization utilizing Gaussian Process learning. This allows for motion parameters to be optimized using real world trials which incorporate all uncertainties inherent in the assembly process without requiring advanced robot and sensor setups. The result is a simple and straightforward system which helps the user automatically find robust and uncertainty-tolerant motions. We present experiments for an assembly case showing both detection and learning in the real world and how these combine to a robust robot system.


Title: Learning Robust Policies for Object Manipulation with Robot Swarms
Key Words: Hilbert spaces  learning systems  mobile robots  multi-robot systems  robotic assembly  robust control  search problems  swarm size  robust policies learning  Hilbert space embeddings  policy search methods  low-level object movement policy  high-level assembly plan  assembly process  policy search method  autonomous object assembly  swarm robotics  robot swarms  object manipulation  Robot sensing systems  Task analysis  Light sources  Robustness  Kernel  Trajectory 
Abstract: Swarm robotics investigates how a large population of robots with simple actuation and limited sensors can collectively solve complex tasks. One particular interesting application with robot swarms is autonomous object assembly. Such tasks have been solved successfully with robot swarms that are controlled by a human operator using a light source. In this paper, we present a method to solve such assembly tasks autonomously based on policy search methods. We split the assembly process in two subtasks: generating a high-level assembly plan and learning a low-level object movement policy. The assembly policy plans the trajectories for each object and the object movement policy controls the trajectory execution. Learning the object movement policy is challenging as it depends on the complex state of the swarm which consists of an individual state for each agent. To approach this problem, we introduce a representation of the swarm which is based on Hilbert space embeddings of distributions. This representation is invariant to the number of agents in the swarm as well as to the allocation of an agent to its position in the swarm. These invariances make the learned policy robust to changes in the swarm and also reduce the search space for the policy search method significantly. We show that the resulting system is able to solve assembly tasks with varying object shapes in multiple simulation scenarios and evaluate the robustness of our representation to changes in the swarm size. Furthermore, we demonstrate that the policies learned in simulation are robust enough to be transferred to real robots.


Title: Slip Detection with Combined Tactile and Visual Information
Key Words: feature extraction  force control  grippers  image capture  image sequences  learning (artificial intelligence)  manipulators  neural nets  pattern classification  robot vision  tactile sensors  robot arm  grasping positions  slip detection  visual information  robotic manipulation  deep neural network  GelSight tactile sensor  grasping forces  DNN training  grasp stability  tactile information  gripper  camera-based tactile sensor  image sequences  image capture  Grippers  Grasping  Cameras  Tactile sensors  Force 
Abstract: Slip detection plays a vital role in robotic manipulation and it has long been a challenging problem in the robotic community. In this paper, we propose a new method based on deep neural network (DNN) to detect slip. The training data is acquired by a GelSight tactile sensor and a camera mounted on a gripper when we use a robot arm to grasp and lift 94 daily objects with different grasping forces and grasping positions. The DNN is trained to classify whether a slip occurred or not. To evaluate the performance of the DNN, we test 10 unseen objects in 152 grasps. A detection accuracy as high as 88.03 % is achieved. It is anticipated that the accuracy can be further improved with a larger dataset. This method is beneficial for robots to make stable grasps, which can be widely applied to automatic force control, grasping strategy selection and fine manipulation.


Title: Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning
Key Words: decision theory  graph theory  learning (artificial intelligence)  Markov processes  mobile robots  navigation  path planning  probability  search problems  control problem  single follower robot  visual contact  moving target  plausible predictions  predictive models  discrete hypotheses  combinatorial search  physical space  model target behavior  learned navigation reward function  semantic terrain features  search methods  predictive pursuit algorithm  multiple satellite maps  simulation scenarios  inverse reinforcement learning  long term behavior  short term behavior  planning pursuit paths  locations  graph representation  latent destination  position  POMDP solvers  domain specific knowledge  model based probabilistic pursuit  Navigation  Planning  Trajectory  Visualization  Entropy  Predictive models  Learning (artificial intelligence) 
Abstract: We address the integrated prediction, planning, and control problem that enables a single follower robot (the photographer) to quickly re-establish visual contact with a moving target (the subject) that has escaped the follower's field of view. We deal with this scenario, which reactive controllers are typically ill-equipped to handle, by making plausible predictions about the long- and short-term behavior of the target, and planning pursuit paths that will maximize the chance of seeing the target again. At the core of our pursuit method is the use of predictive models of target behavior, which help narrow down the set of possible future locations of the target to a few discrete hypotheses, as well as the use of combinatorial search in physical space to check those hypotheses efficiently. We model target behavior in terms of a learned navigation reward function, using Inverse Reinforcement Learning, based on semantic terrain features of satellite maps. Our pursuit algorithm continuously predicts the latent destination of the target and its position in the future, and relies on efficient graph representation and search methods in order to navigate to locations at which the target is most likely to be seen at an anticipated time. We perform extensive evaluation of our predictive pursuit algorithm over multiple satellite maps, thousands of simulation scenarios, against state-of-the art MDP and POMDP solvers. We show that our method significantly outperforms them by exploiting domain-specific knowledge, while being able to run in real-time.


