TY  - CONF
TI  - Enhanced Tele-interaction in Unknown Environments Using Semi-Autonomous Motion and Impedance Regulation Principles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5813
EP  - 5820
AU  - L. Muratore
AU  - A. Laurenzi
AU  - E. M. Hoffman
AU  - L. Baccelliere
AU  - N. Kashiri
AU  - D. G. Caldwell
AU  - N. G. Tsagarakis
PY  - 2018
KW  - collision avoidance
KW  - human-robot interaction
KW  - mobile robots
KW  - telerobotics
KW  - human intervention
KW  - tele-interaction
KW  - shared-autonomy Tele-Interaction control approach
KW  - impendance colliding
KW  - impedance setting
KW  - physical interactions
KW  - slave robot
KW  - human pilot
KW  - shared-autonomy control principles
KW  - autonomous impedance regulation
KW  - autonomous manner
KW  - physical constraints
KW  - robot platform
KW  - interaction forces
KW  - physical obstacles
KW  - remote robot
KW  - impedance modulators
KW  - autonomous motion
KW  - motion commands
KW  - remote workspace
KW  - human operator
KW  - remote environment
KW  - hazardous environments
KW  - robotics teleoperation
KW  - impedance regulation principles
KW  - Robots
KW  - Impedance
KW  - Task analysis
KW  - Collision avoidance
KW  - Payloads
KW  - Trajectory
KW  - Correlation
DO  - 10.1109/ICRA.2018.8460559
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Robotics teleoperation has been extensively studied and considered in the past in several task scenarios where direct human intervention is not possible due to the hazardous environments. In such applications, both communication degradation and reduced perception of the remote environment are practical issues that can challenge the human operator while controlling the robot and attempting to physically interact within the remote workspace. To address this challenge, we introduce a novel shared-autonomy Tele-Interaction control approach that blends the motion commands from the pilot (master side) with locally (slave side) executed autonomous motion and impedance modulators. This enables a remote robot to handle and autonomously avoid physical obstacles during manoeuvring, reduce interaction forces during contacts, and finally accommodate different payload conditions while at the same time operating with a “default” low impedance setting. We implemented and experimentally validated the proposed method both on simulation and on a real robot platform called CENTAURO. A series of tasks, such as maneuvering through the physical constraints of the remote environment in an autonomous manner, pushing and lifting heavy objects with autonomous impedance regulation and colliding with the rigid geometry of the remote environment were executed. The obtained results demonstrate the effectiveness of the shared-autonomy control principles that eventually aim to reduce the level of attention and stress of human pilot while manoeuvring the slave robot, and at the same time to enhance the robustness of the robot during physical interactions even if accidentally occurred.
ER  - 

TY  - CONF
TI  - Intuitive Hand Teleoperation by Novice Operators Using a Continuous Teleoperation Subspace
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5821
EP  - 5827
AU  - C. Meeker
AU  - T. Rasmussen
AU  - M. Ciocarlie
PY  - 2018
KW  - dexterous manipulators
KW  - motion control
KW  - telerobotics
KW  - intuitive control method
KW  - pose spaces
KW  - low-dimensional teleoperation subspace
KW  - continuous teleoperation subspace
KW  - nonanthropomorphic robot
KW  - teleoperation subspaces
KW  - teleoperation subspace mapping
KW  - intuitive hand teleoperation
KW  - novice operators
KW  - human-in-the-loop manipulation
KW  - autonomous grasping
KW  - input device
KW  - teleoperation methods
KW  - Aerospace electronics
KW  - Grasping
KW  - Kinematics
KW  - Task analysis
KW  - Teleoperators
KW  - Shape
DO  - 10.1109/ICRA.2018.8460506
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Human-in-the-loop manipulation is useful in when autonomous grasping is not able to deal sufficiently well with corner cases or cannot operate fast enough. Using the teleoperator's hand as an input device can provide an intuitive control method but requires mapping between pose spaces which may not be similar. We propose a low-dimensional and continuous teleoperation subspace which can be used as an intermediary for mapping between different hand pose spaces. We present an algorithm to project between pose space and teleoperation subspace. We use a non-anthropomorphic robot to experimentally prove that it is possible for teleoperation subspaces to effectively and intuitively enable teleoperation. In experiments, novice users completed pick and place tasks significantly faster using teleoperation subspace mapping than they did using state of the art teleoperation methods.
ER  - 

TY  - CONF
TI  - Avoiding Human-Robot Collisions Using Haptic Communication
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5828
EP  - 5834
AU  - Y. Che
AU  - C. T. Sun
AU  - A. M. Okamura
PY  - 2018
KW  - collision avoidance
KW  - haptic interfaces
KW  - human-robot interaction
KW  - mobile robots
KW  - service robots
KW  - telerobotics
KW  - collision scenario
KW  - haptic communication channel
KW  - human movement
KW  - human-robot collisions
KW  - autonomous navigation
KW  - populated environments
KW  - mobile robots
KW  - human-robot communication
KW  - navigation tasks
KW  - human users
KW  - wearable haptic interface
KW  - distinct haptic cues
KW  - vibration amplitudes
KW  - single human-single robot orthogonal encounter scenario
KW  - Collision avoidance
KW  - Haptic interfaces
KW  - Robot kinematics
KW  - Navigation
KW  - Legged locomotion
KW  - Predictive models
DO  - 10.1109/ICRA.2018.8460946
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Fully autonomous navigation in populated environments is still a challenging problem for mobile robots. This paper explores the idea of using active human-robot communication to facilitate navigation tasks. We propose to convey a robot's intent to human users via a wearable haptic interface. The interface can display distinct haptic cues by modulating vibration amplitudes and patterns. We applied the concept to a single human/single robot orthogonal encounter scenario, where one of the two parties has to yield the right of way to avoid collision. Under certain conditions, the robot's intent (to yield to the human or not) is revealed to the human via the haptic interface prior to the interaction. We conducted an experiment with 10 users, in which the robot was teleoperated as a substitute for autonomy. Results show that, when given priority, users become more risk-accepting and use different strategies to navigate the collision scenario than when the robot takes priority or there is no haptic communication channel. In addition, we propose a social-force based model to predict human movement during navigation. The effect of communication can be explained as a shift in the user's safety buffer and expectation of the robot's future velocity.
ER  - 

TY  - CONF
TI  - High Speed Whole Body Dynamic Motion Experiment with Real Time Master-Slave Humanoid Robot System
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5835
EP  - 5841
AU  - Y. Ishiguro
AU  - K. Kojima
AU  - F. Sugai
AU  - S. Nozawa
AU  - Y. Kakiuchi
AU  - K. Okada
AU  - M. Inaba
PY  - 2018
KW  - humanoid robots
KW  - motion control
KW  - robot dynamics
KW  - smoothing methods
KW  - stability
KW  - telerobotics
KW  - master-slave operations
KW  - foot landing delay prediction
KW  - trajectory smoothing method
KW  - master-slave tennis swing experiment
KW  - high kick motion experiment
KW  - life-sized humanoid robot JAXON
KW  - high speed whole body dynamic motion experiment
KW  - online real time whole body master-slave control
KW  - dynamic whole body master-slave experiment
KW  - flexible master-slave operation
KW  - real time master-slave humanoid robot system
KW  - Foot
KW  - Master-slave
KW  - Humanoid robots
KW  - Interpolation
KW  - Dynamics
KW  - Real-time systems
DO  - 10.1109/ICRA.2018.8461207
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we propose novel methods suitable for online real time whole body master-slave control with real life-sized humanoid robot. We conducted some dynamic whole body master-slave experiment with life-sized humanoid robot, and we achieved speedier and flexible master-slave operation compared to conventional study. Conventionally, master-slave operations with humanoid robots were available with only the upper body of the humanoid robot, and the COM movement was limited to be static. In our previous study, we introduced LIP model based restrictions to ensure the balance stability. In this study, we extend the safety restrictions by introducing foot landing delay prediction and trajectory smoothing method suitable for real robot. We conducted master-slave tennis swing experiment and high kick motion experiment with life-sized humanoid robot “JAXON”, and we evaluated the effectiveness of our proposed methods and system.
ER  - 

TY  - CONF
TI  - Deep Trail-Following Robotic Guide Dog in Pedestrian Environments for People who are Blind and Visually Impaired - Learning from Virtual and Real Worlds
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5849
EP  - 5855
AU  - T. Chuang
AU  - N. Lin
AU  - J. Chen
AU  - C. Hung
AU  - Y. Huang
AU  - C. Teng
AU  - H. Huang
AU  - L. Yu
AU  - L. Giarré
AU  - H. Wang
PY  - 2018
KW  - biomimetics
KW  - control engineering computing
KW  - convolution
KW  - feedforward neural nets
KW  - handicapped aids
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robotic guide dog
KW  - interclass trail variations
KW  - deep convolutional neural network
KW  - virtual worlds
KW  - man-made trails
KW  - pedestrian environments
KW  - contact feedback
KW  - tactile trails
KW  - autonomous trail-following
KW  - virtual real-world environments
KW  - visually impaired
KW  - Dogs
KW  - Cameras
KW  - Robot vision systems
KW  - Navigation
KW  - Mobile robots
DO  - 10.1109/ICRA.2018.8460994
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Navigation in pedestrian environments is critical to enabling independent mobility for the blind and visually impaired (BVI) in their daily lives. White canes have been commonly used to obtain contact feedback for following walls, curbs, or man-made trails, whereas guide dogs can assist in avoiding physical contact with obstacles or other pedestrians. However, the infrastructures of tactile trails or guide dogs are expensive to maintain. Inspired by the autonomous lane following of self-driving cars, we wished to combine the capabilities of existing navigation solutions for BVI users. We proposed an autonomous, trail-following robotic guide dog that would be robust to variances of background textures, illuminations, and interclass trail variations. A deep convolutional neural network (CNN) is trained from both the virtual and realworld environments. Our work included major contributions: 1) conducting experiments to verify that the performance of our models trained in virtual worlds was comparable to that of models trained in the real world; 2) conducting user studies with 10 blind users to verify that the proposed robotic guide dog could effectively assist them in reliably following man-made trails.
ER  - 

TY  - CONF
TI  - MergeNet: A Deep Net Architecture for Small Obstacle Discovery
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5856
EP  - 5862
AU  - K. Gupta
AU  - S. A. Javed
AU  - V. Gandhi
AU  - K. M. Krishna
PY  - 2018
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - object tracking
KW  - traffic engineering computing
KW  - lost and found dataset
KW  - complementary features
KW  - RGBD input
KW  - high level features
KW  - low level features
KW  - weight-sharing
KW  - multistage training procedure
KW  - annotation process
KW  - autonomous driving
KW  - on-road scenes
KW  - novel network architecture
KW  - small obstacle discovery
KW  - deep net architecture
KW  - MergeNet
KW  - Roads
KW  - Image segmentation
KW  - Strips
KW  - Semantics
KW  - Training
KW  - Autonomous vehicles
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461065
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present here, a novel network architecture called MergeNet for discovering small obstacles for on-road scenes in the context of autonomous driving. The basis of the architecture rests on the central consideration of training with less amount of data since the physical setup and the annotation process for small obstacles is hard to scale. For making effective use of the limited data, we propose a multi-stage training procedure involving weight-sharing, separate learning of low and high level features from the RGBD input and a refining stage which learns to fuse the obtained complementary features. The model is trained and evaluated on the Lost and Found dataset and is able to achieve state-of-art results with just 135 images in comparison to the 1000 images used by the previous benchmark. Additionally, we also compare our results with recent methods trained on 6000 images and show that our method achieves comparable performance with only 1000 training samples.
ER  - 

TY  - CONF
TI  - Deep Encoder-Decoder Networks for Mapping Raw Images to Dynamic Movement Primitives
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5863
EP  - 5868
AU  - R. Pahič
AU  - A. Gams
AU  - A. Ude
AU  - J. Morimoto
PY  - 2018
KW  - backpropagation
KW  - handwriting recognition
KW  - handwritten character recognition
KW  - neural nets
KW  - dynamic movement primitives
KW  - cost functions
KW  - raw image mapping
KW  - backpropagation
KW  - MNIST database
KW  - deep encoder-decoder network
KW  - associated movement trajectories
KW  - perception-action couplings
KW  - encoder-decoder networks
KW  - calculated movements
KW  - handwriting movements
KW  - Trajectory
KW  - Differential equations
KW  - Neural networks
KW  - Cost function
KW  - Training
KW  - Robot kinematics
DO  - 10.1109/ICRA.2018.8460954
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper we propose a new approach for learning perception-action couplings. We show that by collecting a suitable set of raw images and the associated movement trajectories, a deep encoder-decoder network can be trained that takes raw images as input and outputs the corresponding dynamic movement primitives. We propose suitable cost functions for training the network and describe how to calculate their gradients to enable effective training by back-propagation. We tested the proposed approach both on a synthetic dataset and on a widely used MNIST database to generate handwriting movements from raw images of digits. The calculated movements were also applied for digit writing with a real robot.
ER  - 

TY  - CONF
TI  - What is (Missing or Wrong) in the Scene? A Hybrid Deep Boltzmann Machine for Contextualized Scene Modeling
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5869
EP  - 5874
AU  - I. Bozcan
AU  - Y. Oymak
AU  - İ. Z. Alemdar
AU  - S. Kalkan
PY  - 2018
KW  - Boltzmann machines
KW  - image classification
KW  - learning (artificial intelligence)
KW  - restricted Boltzmann machines
KW  - hybrid deep Boltzmann machine
KW  - scene classification dataset
KW  - baseline models
KW  - different objects
KW  - visible nodes
KW  - BM
KW  - hybrid Boltzmann Machine
KW  - contextualized scene modeling
KW  - scene reasoning tasks
KW  - Training
KW  - Task analysis
KW  - Robots
KW  - Computational modeling
KW  - Context modeling
KW  - Estimation
KW  - Cognition
DO  - 10.1109/ICRA.2018.8460828
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Scene models allow robots to reason about what is in the scene, what else should be in it, and what should not be in it. In this paper, we propose a hybrid Boltzmann Machine (BM) for scene modeling where relations between objects are integrated. To be able to do that, we extend BM to include tri-way edges between visible (object) nodes and make the network to share the relations across different objects. We evaluate our method against several baseline models (Deep Boltzmann Machines, and Restricted Boltzmann Machines) on a scene classification dataset, and show that it performs better in several scene reasoning tasks.
ER  - 

TY  - CONF
TI  - Scene Recognition and Object Detection in a Unified Convolutional Neural Network on a Mobile Manipulator
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5875
EP  - 5881
AU  - H. Sun
AU  - Z. Meng
AU  - P. Y. Tao
AU  - M. H. Ang
PY  - 2018
KW  - control engineering computing
KW  - convolution
KW  - feature extraction
KW  - feedforward neural nets
KW  - image classification
KW  - image colour analysis
KW  - manipulators
KW  - mobile robots
KW  - object detection
KW  - object recognition
KW  - operating systems (computers)
KW  - robot programming
KW  - robot vision
KW  - scene classification
KW  - unified architecture
KW  - global scene features
KW  - regional object features
KW  - object recognition
KW  - continuous robot beliefs
KW  - robotics applications
KW  - Robot Operating System
KW  - mobile manipulator
KW  - object detection
KW  - object locations
KW  - network predictions
KW  - SUN RGBD dataset
KW  - 3D space
KW  - unified convolutional neural network
KW  - Proposals
KW  - Robots
KW  - Object detection
KW  - Object recognition
KW  - Three-dimensional displays
KW  - Semantics
KW  - Feature extraction
DO  - 10.1109/ICRA.2018.8460535
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Environment understanding, object detection and recognition are crucial skills for robots operating in the real world. In this paper, we propose a Convolutional Neural Network with multi-task objectives: object detection and scene classification in one unified architecture. The proposed network reasons globally about an image to understand the scene, hypothesize object locations, and encodes global scene features with regional object features to improve object recognition. We evaluate our network on the standard SUN RGBD dataset. Experiments show that our approach outperforms state-of-the-arts. Network predictions are further transformed into continuous robot beliefs to ensure temporal coherence and extended to 3D space for robotics applications. We embed the whole framework in Robot Operating System, and evaluate its performance on a real robot for semantic mapping and grasp detection.
ER  - 

TY  - CONF
TI  - AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5882
EP  - 5889
AU  - T. Do
AU  - A. Nguyen
AU  - I. Reid
PY  - 2018
KW  - image classification
KW  - learning (artificial intelligence)
KW  - object detection
KW  - multiple object detection
KW  - AffordanceNet
KW  - object localization
KW  - object classification
KW  - affordance label
KW  - robust resizing strategy
KW  - deconvolutional layer sequence
KW  - real-time robotic applications
KW  - testing environments
KW  - end-to-end architecture
KW  - multitask loss function
KW  - affordance mask
KW  - RGB images
KW  - object affordance detection
KW  - end-to-end deep learning approach
KW  - Feature extraction
KW  - Robots
KW  - Computer architecture
KW  - Object detection
KW  - Training
KW  - Image segmentation
KW  - Machine learning
DO  - 10.1109/ICRA.2018.8460902
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net.
ER  - 

TY  - CONF
TI  - ContextualNet: Exploiting Contextual Information Using LSTMs to Improve Image-Based Localization
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5890
EP  - 5896
AU  - M. Patel
AU  - B. Emery
AU  - Y. Chen
PY  - 2018
KW  - convolution
KW  - feedforward neural nets
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - SLAM (robots)
KW  - CNN-LSTM model
KW  - pose estimation
KW  - single monocular image
KW  - Convolutional Neural Networks
KW  - image-based localization
KW  - ContextualNet
KW  - image content
KW  - indoor office space
KW  - Feature extraction
KW  - Cameras
KW  - Context modeling
KW  - Computer vision
KW  - Logic gates
KW  - Neural networks
DO  - 10.1109/ICRA.2018.8461124
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Convolutional Neural Networks (CNN) have successfully been utilized for localization using a single monocular image [1]. Most of the work to date has either focused on reducing the dimensionality of data for better learning of parameters during training or on developing different variations of CNN models to improve pose estimation. Many of the best performing works solely consider the content in a single image, while the context from historical images is ignored. In this paper, we propose a combined CNN-LSTM which is capable of incorporating contextual information from historical images to better estimate the current pose. Experimental results achieved using a dataset collected in an indoor office space improved the overall system results to 0.8 m & 2.5° at the third quartile of the cumulative distribution as compared with 1.5 m & 3.0° achieved by PoseNet [1]. Furthermore, we demonstrate how the temporal information exploited by the CNN-LSTM model assists in localizing the robot in situations where image content does not have sufficient features.
ER  - 

TY  - CONF
TI  - Learning Human-Aware Path Planning with Fully Convolutional Networks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5897
EP  - 5902
AU  - N. Pérez-Higueras
AU  - F. Caballero
AU  - L. Merino
PY  - 2018
KW  - convolution
KW  - feedforward neural nets
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - random processes
KW  - trees (mathematics)
KW  - robot social navigation
KW  - Fully Convolutional Neural Networks
KW  - mobile robots
KW  - human-aware path planning learning
KW  - optimal Rapidly-exploring Random Tree planner
KW  - robot navigation
KW  - classification problem
KW  - Robots
KW  - Navigation
KW  - Task analysis
KW  - Trajectory
KW  - Cost function
KW  - Feature extraction
DO  - 10.1109/ICRA.2018.8460851
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This work presents an approach to learn path planning for robot social navigation by demonstration. We make use of Fully Convolutional Neural Networks (FCNs) to learn from expert's path demonstrations a map that marks a feasible path to the goal as a classification problem. The use of FCNs allows us to overcome the problem of manually designing/identifying the cost-map and relevant features for the task of robot navigation. The method makes use of optimal Rapidly-exploring Random Tree planner (RRT*) to overcome eventual errors in the path prediction; the FCNs prediction is used as cost-map and also to partially bias the sampling of the configuration space, leading the planner to behave similarly to the learned expert behavior. The approach is evaluated in experiments with real trajectories and compared with Inverse Reinforcement Learning algorithms that use RRT* as underlying planner.
ER  - 

TY  - CONF
TI  - Pedestrian Prediction by Planning Using Deep Neural Networks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5903
EP  - 5908
AU  - E. Rehder
AU  - F. Wirth
AU  - M. Lauer
AU  - C. Stiller
PY  - 2018
KW  - collision avoidance
KW  - convolution
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - pedestrians
KW  - traffic engineering computing
KW  - monolithic neural network
KW  - inverse reinforcement learning
KW  - pedestrian prediction
KW  - deep neural networks
KW  - collision avoidance
KW  - autonomous vehicles
KW  - goal-directed planning
KW  - mixture density function
KW  - motion prediction
KW  - convolutional network
KW  - traffic participant prediction
KW  - trajectories
KW  - Planning
KW  - Network topology
KW  - Topology
KW  - Convolution
KW  - Learning (artificial intelligence)
KW  - Trajectory
KW  - Prediction algorithms
DO  - 10.1109/ICRA.2018.8460203
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Accurate traffic participant prediction is the prerequisite for collision avoidance of autonomous vehicles. In this work, we propose to predict pedestrians using goal-directed planning. For this, we infer a mixture density function for possible destinations. We use these destinations as the goal states of a planning stage that performs motion prediction based on common behavior patterns. The patterns are learned by a fully convolutional network operating on maps of the environment. We show that this entire system can be modeled as one monolithic neural network and trained via inverse reinforcement learning. Experimental validation on real world data shows the system's ability to predict both, destinations and trajectories accurately.
ER  - 

TY  - CONF
TI  - Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5909
EP  - 5914
AU  - P. Schydlo
AU  - M. Rakovic
AU  - L. Jamone
AU  - J. Santos-Victor
PY  - 2018
KW  - feature selection
KW  - human-robot interaction
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - pose estimation
KW  - recurrent neural nets
KW  - stochastic processes
KW  - human robot cooperation scenario
KW  - prediction model
KW  - action prediction dataset
KW  - human motion data
KW  - human-robot cooperation
KW  - recurrent neural network approach
KW  - multiple action sequences prediction
KW  - assistive applications
KW  - nonverbal cues
KW  - neural networks
KW  - human action prediction problem
KW  - continuous spaces
KW  - discrete spaces
KW  - encoder-decoder recurrent neural network topology
KW  - discrete action prediction problem
KW  - action sequences
KW  - feature selection
KW  - stochastic reward
KW  - Predictive models
KW  - Decoding
KW  - Hidden Markov models
KW  - Recurrent neural networks
KW  - Robot kinematics
KW  - Trajectory
DO  - 10.1109/ICRA.2018.8460924
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Close human-robot cooperation is a key enabler for new developments in advanced manufacturing and assistive applications. Close cooperation require robots that can predict human actions and intent, understanding human non-verbal cues. Recent approaches based on neural networks have led to encouraging results in the human action prediction problem both in continuous and discrete spaces. Our approach extends the research in this direction. Our contributions are three-fold. First, we validate the use of gaze and body pose cues as a means of predicting human action through a feature selection method. Next, we address two shortcomings of existing literature: predicting multiple and variable-length action sequences. This is achieved by applying an encoder-decoder recurrent neural network topology in the discrete action prediction problem. In addition, we theoretically demonstrate the importance of predicting multiple action sequences as a means of estimating the stochastic reward in a human robot cooperation scenario. Finally, we show the ability to effectively train the prediction model on an action prediction dataset, involving human motion data, and explore the influence of the model's parameters on its performance.
ER  - 

TY  - CONF
TI  - Text2Action: Generative Adversarial Synthesis from Language to Action
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5915
EP  - 5920
AU  - H. Ahn
AU  - T. Ha
AU  - Y. Choi
AU  - H. Yoo
AU  - S. Oh
PY  - 2018
KW  - recurrent neural nets
KW  - robots
KW  - text analysis
KW  - video signal processing
KW  - sequence to sequence model
KW  - Baxter robot
KW  - virtual agent
KW  - generative adversarial synthesis
KW  - Text2action
KW  - MSR-Video-to-Text
KW  - action decoder RNN
KW  - text encoder recurrent neural network
KW  - generative network
KW  - SEQ2SEQ
KW  - sequence model
KW  - generative adversarial network
KW  - human behavior
KW  - sentence
KW  - human action sequence
KW  - generative model
KW  - Decoding
KW  - Gallium nitride
KW  - Hidden Markov models
KW  - Generative adversarial networks
KW  - Robots
KW  - Recurrent neural networks
KW  - Generators
DO  - 10.1109/ICRA.2018.8460608
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we propose a generative model which learns the relationship between language and human action in order to generate a human action sequence given a sentence describing human behavior. The proposed generative model is a generative adversarial network (GAN), which is based on the sequence to sequence (SEQ2SEQ) model. Using the proposed generative network, we can synthesize various actions for a robot or a virtual agent using a text encoder recurrent neural network (RNN) and an action decoder RNN. The proposed generative network is trained from 29,770 pairs of actions and sentence annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video dataset. We demonstrate that the network can generate human-like actions which can be transferred to a Baxter robot, such that the robot performs an action based on a provided sentence. Results show that the proposed generative network correctly models the relationship between language and action and can generate a diverse set of actions from the same sentence.
ER  - 

TY  - CONF
TI  - A Data-driven Model for Interaction-Aware Pedestrian Motion Prediction in Object Cluttered Environments
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5921
EP  - 5928
AU  - M. Pfeiffer
AU  - G. Paolo
AU  - H. Sommer
AU  - J. Nieto
AU  - R. Siegwart
AU  - C. Cadena
PY  - 2018
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion estimation
KW  - navigation
KW  - neural nets
KW  - pedestrians
KW  - human motion behavior
KW  - prediction accuracy
KW  - data-driven model
KW  - interaction-aware pedestrian motion prediction
KW  - object cluttered environments
KW  - interaction-aware motion prediction approach
KW  - human navigation behavior
KW  - Long-Short Term Memory neural networks
KW  - static obstacles
KW  - trajectory forecasting
KW  - polar angle space
KW  - Predictive models
KW  - Robots
KW  - Trajectory
KW  - Adaptation models
KW  - Navigation
KW  - Planning
KW  - Neural networks
DO  - 10.1109/ICRA.2018.8461157
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper reports on a data-driven, interaction-aware motion prediction approach for pedestrians in environments cluttered with static obstacles. When navigating in such workspaces shared with humans, robots need accurate motion predictions of the surrounding pedestrians. Human navigation behavior is mostly influenced by their surrounding pedestrians and by the static obstacles in their vicinity. In this paper we introduce a new model based on Long-Short Term Memory (LSTM) neural networks, which is able to learn human motion behavior from demonstrated data. To the best of our knowledge, this is the first approach using LSTMs, that incorporates both static obstacles and surrounding pedestrians for trajectory forecasting. As part of the model, we introduce a new way of encoding surrounding pedestrians based on a 1d-grid in polar angle space. We evaluate the benefit of interaction-aware motion prediction and the added value of incorporating static obstacles on both simulation and real-world datasets by comparing with state-of-the-art approaches. The results show, that our new approach outperforms the other approaches while being very computationally efficient and that taking into account static obstacles for motion predictions significantly improves the prediction accuracy, especially in cluttered environments.
ER  - 

TY  - CONF
TI  - Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5929
EP  - 5934
AU  - J. Owoyemi
AU  - K. Hashimoto
PY  - 2018
KW  - feature extraction
KW  - feedforward neural nets
KW  - gesture recognition
KW  - image classification
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - motion recognition
KW  - spatiotemporal features
KW  - dense occupancy grids
KW  - 3D point cloud data
KW  - end-to-end spatiotemporal gesture learning approach
KW  - dynamic gestures
KW  - spatiotemporal learning
KW  - point cloud data augmentation
KW  - 3D convolutional neural network
KW  - gestures sample data
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Feature extraction
KW  - Solid modeling
KW  - Training data
KW  - Spatiotemporal phenomena
KW  - Hidden Markov models
DO  - 10.1109/ICRA.2018.8460910
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we demonstrate an end-to-end spatiotemporal gesture learning approach for 3D point cloud data using a new gestures dataset of point clouds acquired from a 3D sensor. Nine classes of gestures were learned from gestures sample data. We mapped point cloud data into dense occupancy grids, then time steps of the occupancy grids are used as inputs into a 3D convolutional neural network which learns the spatiotemporal features in the data without explicit modeling of gesture dynamics. We also introduced a 3D region of interest jittering approach for point cloud data augmentation. This resulted in an increased classification accuracy of up to 10% when the augmented data is added to the original training data. The developed model is able to classify gestures from the dataset with 84.44% accuracy. We propose that point cloud data will be a more viable data type for scene understanding and motion recognition, as 3D sensors become ubiquitous in years to come.
ER  - 

TY  - CONF
TI  - Functional Object-Oriented Network: Construction & Expansion
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5935
EP  - 5941
AU  - D. Paulius
AU  - A. B. Jelodar
AU  - Y. Sun
PY  - 2018
KW  - knowledge representation
KW  - object-oriented methods
KW  - manipulation sequences
KW  - knowledge retrieval algorithm
KW  - object categories
KW  - functional units
KW  - object similarity
KW  - video sources
KW  - knowledge spanning
KW  - object-motion affordances
KW  - structured knowledge representation
KW  - functional object-oriented network
KW  - Task analysis
KW  - Robots
KW  - Merging
KW  - Sun
KW  - Knowledge representation
KW  - Mirrors
KW  - Neurons
DO  - 10.1109/ICRA.2018.8460200
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We build upon the functional object-oriented network (FOON), a structured knowledge representation which is constructed from observations of human activities and manipulations. A FOON can be used for representing object-motion affordances. Knowledge retrieval through graph search allows us to obtain novel manipulation sequences using knowledge spanning across many video sources, hence the novelty in our approach. However, we are limited to the sources collected. To further improve the performance of knowledge retrieval as a follow up to our previous work, we discuss generalizing knowledge to be applied to objects which are similar to what we have in FOON without manually annotating new sources of knowledge. We discuss two means of generalization: 1) expanding our network through the use of object similarity to create new functional units from those we already have, and 2) compressing the functional units by object categories rather than specific objects. We discuss experiments which compare the performance of our knowledge retrieval algorithm with both expansion and compression by categories.
ER  - 

TY  - CONF
TI  - 3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5942
EP  - 5948
AU  - L. Sun
AU  - Z. Yan
AU  - S. M. Mellado
AU  - M. Hanheide
AU  - T. Duckett
PY  - 2018
KW  - cameras
KW  - codecs
KW  - image annotation
KW  - image coding
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - pedestrians
KW  - pose estimation
KW  - robot vision
KW  - service robots
KW  - SLAM (robots)
KW  - long-term temporal information
KW  - sequence-to-sequence LSTM encoder-decoder
KW  - on-the-fly prediction
KW  - global coordinate system
KW  - T-Pose-LSTM model
KW  - human trajectory prediction
KW  - long-term mobile robot deployments
KW  - 3DOF pedestrian trajectory prediction learned
KW  - Long-Term autonomous mobile robot deployment data
KW  - autonomous mobile service robots
KW  - monocular camera images
KW  - range-finder sensors
KW  - 3DOF pedestrian trajectory prediction approach
KW  - temporal 3DOF-pose long-short-term memory
KW  - robust human detection
KW  - Trajectory
KW  - Cameras
KW  - Robot kinematics
KW  - Robot vision systems
KW  - Two dimensional displays
KW  - Mobile robots
DO  - 10.1109/ICRA.2018.8461228
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.
ER  - 

TY  - CONF
TI  - Conditional Compatibility Branch and Bound for Feature Cloud Matching
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5965
EP  - 5970
AU  - X. Shen
AU  - M. H. Ang
AU  - D. Rus
PY  - 2018
KW  - computational complexity
KW  - image matching
KW  - SLAM (robots)
KW  - statistical distributions
KW  - tree searching
KW  - chi-square test
KW  - gating threshold
KW  - joint compatibility branch and bound
KW  - conditional compatibility branch and bound
KW  - feature cloud matching
KW  - incremental posterior joint compatibility
KW  - IPJC
KW  - FastJCBB
KW  - global optimal data association
KW  - Joint Compatibility test
KW  - JC test based search algorithm
KW  - CC test
KW  - conditional probability distribution
KW  - feature pairing
KW  - Conditional Compatibility test
KW  - Probabilistic logic
KW  - Robots
KW  - Measurement errors
KW  - Computational complexity
KW  - Probability distribution
KW  - Integrated circuits
KW  - Feature Cloud Matching
KW  - Scan Matching
KW  - Data Association
KW  - Conditional Compatibility Test
DO  - 10.1109/ICRA.2018.8460711
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we consider the problem of data association in feature cloud matching. While Joint Compatibility (JC) test is a widely adopted technique for searching the global optimal data association, it becomes less restrictive as more features are well matched. The early well-matched features contribute little to total matching cost while the gating threshold increases in the chi-square test, which allows the acceptance of bad feature pairings in the last step. In this paper, we propose the Conditional Compatibility (CC) test, which is not only more restrictive than JC test, but also probabilistically sound. The proposed test of a new feature pairing is based on the conditional probability distribution of feature locations given the early pairings. CC test can be added into any JC test based search algorithm, such as Joint Compatibility Branch and Bound (JCBB), Incremental Posterior Joint Compatibility (IPJC) and FastJCBB, without increasing much computational complexity. The more restrictive criterion of accepting a feature pairing, not only helps to reject bad associations, but also bounds the search space, which substantially improves the search efficiency. The real matching experiments justify that our algorithm produces better feature cloud matching results in a more efficient manner.
ER  - 

TY  - CONF
TI  - Assigning Visual Words to Places for Loop Closure Detection
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5979
EP  - 5985
AU  - K. A. Tsintotas
AU  - L. Bampis
AU  - A. Gasteratos
PY  - 2018
KW  - image matching
KW  - image recognition
KW  - image representation
KW  - image segmentation
KW  - mobile robots
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - simultaneous localization and mapping
KW  - image stream
KW  - image match
KW  - dynamic segmentation
KW  - nearest neighbor voting scheme
KW  - image descriptors
KW  - query time
KW  - on-line clustering algorithm
KW  - visual vocabulary construction
KW  - robotic applications
KW  - LCD
KW  - place recognition
KW  - loop closure detection
KW  - visual words
KW  - Visualization
KW  - Liquid crystal displays
KW  - Robots
KW  - Databases
KW  - Pipelines
KW  - Feature extraction
KW  - Vocabulary
DO  - 10.1109/ICRA.2018.8461146
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Place recognition of pre-visited areas, widely known as Loop Closure Detection (LCD), constitutes one of the most important components in robotic applications, where the robot needs to estimate its pose while navigating through the field (e.g., simultaneous localization and mapping). In this paper, we present a novel approach for LCD based on the assignment of Visual Words (VWs) to particular places of the traversed path. The system operates in real time and does not require any pre-training procedure, such as visual vocabulary construction or descriptor-space dimensionality reduction. A place is defined through a dynamic segmentation of the incoming image stream and is assigned with VWs through the usage of an on-line clustering algorithm. At query time, image descriptors are converted into VWs on the map accumulating votes to the corresponding places. By means of a probability function, the mechanism is capable of identifying a loop closing candidate place. A nearest neighbor voting scheme on the descriptors' space allows the system to select the most appropriate image match at the chosen place. Geometrical and temporal consistency checks are applied on the proposed loop closing pair increasing the system's performance. Evaluation took place on several publicly available and challenging datasets offering high precision and recall scores as compared to other state-of-the-art approaches.
ER  - 

TY  - CONF
TI  - Dijkstra Model for Stereo-Vision Based Road Detection: A Non-Parametric Method
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5986
EP  - 5993
AU  - Y. Zhang
AU  - J. Yang
AU  - J. Ponce
AU  - H. Kong
PY  - 2018
KW  - computer vision
KW  - graph theory
KW  - image segmentation
KW  - object detection
KW  - roads
KW  - stereo image processing
KW  - traffic engineering computing
KW  - road detection
KW  - nonparametric method
KW  - improved v-disparity map
KW  - vanishing point
KW  - road region
KW  - horizon information
KW  - source node
KW  - weighted graph
KW  - left stereo-image
KW  - adjacency relationships
KW  - adjacent pixels
KW  - disparity information
KW  - road borders
KW  - Dijkstra algorithm
KW  - Dijkstra model
KW  - stereo vision
KW  - gray-scale information
KW  - image pairs
KW  - road scenes
KW  - weighted-sampling RANSAC-like method
KW  - KITTI dataset
KW  - Roads
KW  - Robustness
KW  - Three-dimensional displays
KW  - Stereo vision
KW  - Splines (mathematics)
KW  - Cameras
KW  - Image edge detection
DO  - 10.1109/ICRA.2018.8461071
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper proposes a new method for detecting a road from a stereo pair of images. First, the horizon is accurately estimated by a robust, weighted-sampling RANSAC-like method in the improved v-disparity map. The vanishing point of the road region is located using both the horizon information and road flatness constraints. Then it is used as the source node of a weighted graph formed by the pixels of the left stereo-image and their adjacency relationships. The weight of each edge measures the inconsistency of adjacent pixels, and is computed using both the gray-scale and disparity information. Detecting road borders is thus reduced to finding two shortest paths from the source node to the bottom row of the image by the Dijkstra algorithm. The proposed method has been tested on 2621 image pairs of different road scenes from the KITTI dataset. Our experiments demonstrate that this training free approach detects horizon, vanishing point, and road region accurately and robustly, and compares favorably with the state of the art on the KITTI benchmark.
ER  - 

TY  - CONF
TI  - Asynchronous Multi-Sensor Fusion for 3D Mapping and Localization
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5994
EP  - 5999
AU  - P. Geneva
AU  - K. Eckenhoff
AU  - G. Huang
PY  - 2018
KW  - graph theory
KW  - intelligent transportation systems
KW  - optimisation
KW  - pose estimation
KW  - sensor fusion
KW  - stereo image processing
KW  - 3D localization
KW  - factor graph-based optimization
KW  - autonomous driving
KW  - 3D pose measurement
KW  - asynchronous multisensor fusion
KW  - asynchronous-measurement alignment
KW  - graph nodes
KW  - out-of-sequence measurement alignment
KW  - multiple navigation sensors
KW  - modular sensor-fusion system
KW  - autonomous vehicles
KW  - 3D mapping
KW  - asynchronous sensors
KW  - multiple heterogeneous sensors
KW  - Sensors
KW  - Three-dimensional displays
KW  - Optimization
KW  - Atmospheric measurements
KW  - Particle measurements
KW  - Frequency measurement
KW  - Laser radar
DO  - 10.1109/ICRA.2018.8460204
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we address the problem of optimally fusing multiple heterogeneous and asynchronous sensors for use in 3D mapping and localization of autonomous vehicles. To this end, based on the factor graph-based optimization framework, we design a modular sensor-fusion system that allows for efficient and accurate incorporation of multiple navigation sensors operating at different sampling rates. In particular, we develop a general method of out-of-sequence (asynchronous) measurement alignment to incorporate heterogeneous sensors into a factor graph for mapping and localization in 3D, without requiring the addition of new graph nodes, thus allowing the graph to have an overall reduced complexity. The proposed sensor-fusion system is validated on a real-world experimental dataset, in which the asynchronous-measurement alignment is shown to have an improved performance when compared to a naive approach without alignment.
ER  - 

TY  - CONF
TI  - Localization Under Topological Uncertainty for Lane Identification of Autonomous Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6000
EP  - 6005
AU  - S. B. Nashed
AU  - D. M. Ilstrup
AU  - J. Biswas
PY  - 2018
KW  - hidden Markov models
KW  - mobile robots
KW  - position control
KW  - remotely operated vehicles
KW  - road traffic control
KW  - topology
KW  - VSM-HMM
KW  - topological uncertainty
KW  - lane membership
KW  - topological localization process
KW  - topological structure estimation
KW  - AV lane estimation
KW  - lane identification
KW  - autonomous vehicles
KW  - topological location
KW  - decision-making
KW  - public roads
KW  - variable structure multiple hidden Markov model
KW  - metric location
KW  - Earth mover distance
KW  - Hidden Markov models
KW  - Computational modeling
KW  - Topology
KW  - Roads
KW  - Uncertainty
KW  - Measurement
KW  - Vehicle dynamics
DO  - 10.1109/ICRA.2018.8461185
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Autonomous vehicles (AVs) require accurate metric and topological location estimates for safe, effective navigation and decision-making. Although many high-definition (HD) roadmaps exist, they are not always accurate since public roads are dynamic, shaped unpredictably by both human activity and nature. Thus, AVs must be able to handle situations in which the topology specified by the map does not agree with reality. We present the Variable Structure Multiple Hidden Markov Model (VSM-HMM) as a framework for localizing in the presence of topological uncertainty, and demonstrate its effectiveness on an AV where lane membership is modeled as a topological localization process. VSM-HMMs use a dynamic set of HMMs to simultaneously reason about location within a set of most likely current topologies and therefore may also be applied to topological structure estimation as well as AV lane estimation. In addition, we present an extension to the Earth Mover's Distance which allows uncertainty to be taken into account when computing the distance between belief distributions on simplices of arbitrary relative sizes.
ER  - 

TY  - CONF
TI  - Stabilizing Traffic with Autonomous Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6012
EP  - 6018
AU  - C. Wu
AU  - A. M. Bayen
AU  - A. Mehta
PY  - 2018
KW  - frequency-domain analysis
KW  - intelligent transportation systems
KW  - linear systems
KW  - mobile robots
KW  - nonlinear programming
KW  - optimal control
KW  - road safety
KW  - road traffic control
KW  - road vehicles
KW  - stability
KW  - autonomously controlled vehicles
KW  - autonomous vehicles
KW  - human-driven vehicles
KW  - traffic stabilization
KW  - safer roads
KW  - energy savings
KW  - single-lane system stabilization
KW  - linear string stability
KW  - optimality conditions
KW  - frequency-domain analysis
KW  - nonlinear optimization problem
KW  - safety constraint
KW  - optimal linear controller
KW  - traffic conditions
KW  - human driver behavior
KW  - Autonomous vehicles
KW  - Vehicle dynamics
KW  - Stability criteria
KW  - Optimization
KW  - Mathematical model
DO  - 10.1109/ICRA.2018.8460567
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Autonomous vehicles promise safer roads, energy savings, and more efficient use of existing infrastructure, among many other benefits. Although the effect of autonomous vehicles has been studied in the limits (near-zero or full penetration), the transition range requires new formulations, mathematical modeling, and control analysis. In this article, we study the ability of small numbers of autonomous vehicles to stabilize a single-lane system of human-driven vehicles. We formalize the problem in terms of linear string stability, derive optimality conditions from frequency-domain analysis, and pose the resulting nonlinear optimization problem. In particular, we introduce two conditions which simultaneously stabilize traffic while imposing a safety constraint on the autonomous vehicle and limiting degradation of performance. With this optimal linear controller in a system with typical human driver behavior, we can numerically determine that only a 6% uniform penetration of autonomously controlled vehicles (i.e. one per string of up to 16 human-driven vehicles) is necessary to stabilize traffic across all traffic conditions.
ER  - 

TY  - CONF
TI  - Data-Driven Model Predictive Control of Autonomous Mobility-on-Demand Systems
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6019
EP  - 6025
AU  - R. Iglesias
AU  - F. Rossi
AU  - K. Wang
AU  - D. Hallac
AU  - J. Leskovec
AU  - M. Pavone
PY  - 2018
KW  - demand forecasting
KW  - intelligent transportation systems
KW  - predictive control
KW  - recurrent neural nets
KW  - road traffic control
KW  - end-to-end performance
KW  - customer demand
KW  - data-driven Model Predictive Control
KW  - LSTM neural network
KW  - travel demand
KW  - Autonomous Mobility-on-Demand systems control
KW  - DiDi Chuxing
KW  - MPC algorithm
KW  - transportation system
KW  - optimal rebalancing strategy
KW  - AMoD system
KW  - Prediction algorithms
KW  - Predictive control
KW  - Transportation
KW  - Pricing
KW  - Control systems
KW  - Steady-state
KW  - Real-time systems
DO  - 10.1109/ICRA.2018.8460966
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The goal of this paper is to present an end-to-end, data-driven framework to control Autonomous Mobility-on-Demand systems (AMoD, i.e. fleets of self-driving vehicles). We first model the AMoD system using a time-expanded network, and present a formulation that computes the optimal rebalancing strategy (i.e., preemptive repositioning) and the minimum feasible fleet size for a given travel demand. Then, we adapt this formulation to devise a Model Predictive Control (MPC) algorithm that leverages short-term demand forecasts based on historical data to compute rebalancing strategies. Using simulations based on real customer data from DiDi Chuxing, we test the end-to-end performance of this controller with a state-of-the-art LSTM neural network to predict customer demand: we show that this approach scales very well for large systems (indeed, the computational complexity of the MPC algorithm does not depend on the number of customers and of vehicles in the system) and outperforms state-of-the-art rebalancing strategies by reducing the mean customer wait time by up to to 89.6 %.
ER  - 

TY  - CONF
TI  - VALUE: Large Scale Voting-Based Automatic Labelling for Urban Environments
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6033
EP  - 6038
AU  - G. Dabisias
AU  - E. Ruffaldi
AU  - H. Grimmett
AU  - P. Ondruska
PY  - 2018
KW  - distributed processing
KW  - image reconstruction
KW  - stereo image processing
KW  - traffic engineering computing
KW  - VALUE
KW  - automatic localisation
KW  - static 3D objects
KW  - New York City
KW  - urban environments
KW  - voting-based automatic labelling
KW  - distributed voting schema
KW  - traffic lights
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Urban areas
KW  - Cameras
KW  - Robustness
KW  - Noise measurement
KW  - Semantics
DO  - 10.1109/ICRA.2018.8460196
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a simple and robust method for the automatic localisation of static 3D objects in large-scale urban environments. By exploiting the potential to merge a large volume of noisy but accurately localised 2D image data, we achieve superior performance in terms of both robustness and accuracy of the recovered 3D information. The method is based on a simple distributed voting schema which can be fully distributed and parallelised to scale to large-scale scenarios. To evaluate the method we collected city-scale data sets from New York City and San Francisco consisting of almost 400k images spanning the area of 40 km2 and used it to accurately recover the 3D positions of traffic lights. We demonstrate a robust performance and also show that the solution improves in quality over time as the amount of data increases.
ER  - 

TY  - CONF
TI  - Automated Process for Incorporating Drivable Path into Real-Time Semantic Segmentation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6039
EP  - 6044
AU  - W. Zhou
AU  - S. Worrall
AU  - A. Zyner
AU  - E. Nebot
PY  - 2018
KW  - cameras
KW  - image segmentation
KW  - mobile robots
KW  - object detection
KW  - path planning
KW  - road traffic
KW  - road vehicles
KW  - robot vision
KW  - path prediction model
KW  - camera sensors
KW  - autonomous vehicle systems
KW  - vision systems
KW  - real-time semantic segmentation
KW  - intelligent vehicles
KW  - odometry
KW  - monocular camera
KW  - car-width drivable lane
KW  - path proposal category
KW  - intelligent vehicle system
KW  - drivable path information
KW  - human operation
KW  - clear lane markings
KW  - urban roads
KW  - Semantics
KW  - Cameras
KW  - Roads
KW  - Image segmentation
KW  - Sensors
KW  - Proposals
KW  - Trajectory
DO  - 10.1109/ICRA.2018.8460486
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Vision systems are widely used in autonomous vehicle systems due to the rich information that camera sensors provide of the surrounding environment. This paper presents an automatic algorithm to obtain the drivable path of a vehicle operating in urban roads with or without clear lane markings. The developed system projects trajectories obtained during human operation of the vehicle and utilizes these to generate automatic labels for training a semantic based path prediction model. The system segments an urban scenario into 13 categories including vehicles, pedestrian, undrivable road, other categories relevant to urban roads, and a new class for a path proposal. The drivable path information is essential particularly in unstructured scenarios, and is critical for an intelligent vehicle system to make sound driving decisions. The path proposal category is a car-width drivable lane estimated to be safe to drive for the vehicle under consideration. The data collection, model training and inference process requires only images from a monocular camera and odometry from a low-cost IMU combined with a wheel encoder. The algorithm has been successfully demonstrated on the Sydney University campus, which is a challenging environment without clear road markings. The algorithm was demonstrated to run in real-time, proving its applicability for intelligent vehicles.
ER  - 

TY  - CONF
TI  - Precise Ego-Motion Estimation with Millimeter-Wave Radar Under Diverse and Challenging Conditions
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6045
EP  - 6052
AU  - S. H. Cen
AU  - P. Newman
PY  - 2018
KW  - CW radar
KW  - distance measurement
KW  - feature extraction
KW  - FM radar
KW  - Global Positioning System
KW  - image matching
KW  - millimetre wave radar
KW  - motion estimation
KW  - precise ego-motion estimation
KW  - millimeter-wave radar
KW  - cameras
KW  - lidars
KW  - proprioceptive sensors
KW  - radars
KW  - long-range objects
KW  - mobile autonomous systems
KW  - frequency-modulated continuous-wave scanning radar
KW  - GPS/INS
KW  - radar odometry
KW  - Sensors
KW  - Feature extraction
KW  - Radar imaging
KW  - Azimuth
KW  - Motion estimation
KW  - Robustness
DO  - 10.1109/ICRA.2018.8460687
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In contrast to cameras, lidars, GPS, and proprioceptive sensors, radars are affordable and efficient systems that operate well under variable weather and lighting conditions, require no external infrastructure, and detect long-range objects. In this paper, we present a reliable and accurate radar-only motion estimation algorithm for mobile autonomous systems. Using a frequency-modulated continuous-wave (FMCW) scanning radar, we first extract landmarks with an algorithm that accounts for unwanted effects in radar returns. To estimate relative motion, we then perform scan matching by greedily adding point correspondences based on unary descriptors and pairwise compatibility scores. Our radar odometry results are robust under a variety of conditions, including those under which visual odometry and GPS/INS fail.
ER  - 

TY  - CONF
TI  - SeDAR - Semantic Detection and Ranging: Humans can Localise without LiDAR, can Robots?
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6053
EP  - 6060
AU  - O. Mendez
AU  - S. Hadfield
AU  - N. Pugeault
AU  - R. Bowden
PY  - 2018
KW  - feature extraction
KW  - image colour analysis
KW  - image matching
KW  - image representation
KW  - mobile robots
KW  - pose estimation
KW  - robot vision
KW  - vision-based approaches
KW  - high level semantic cues
KW  - floorplan
KW  - global localisation approach
KW  - range measurements
KW  - robotic scan-matching algorithms
KW  - SeDAR
KW  - semantic detection and ranging
KW  - pose estimation
KW  - 2D geometry
KW  - 2D representation
KW  - discriminative landmarks
KW  - RGB images
KW  - Semantics
KW  - Robot sensing systems
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Robustness
DO  - 10.1109/ICRA.2018.8461074
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - How does a person work out their location using a floorplan? It is probably safe to say that we do not explicitly measure depths to every visible surface and try to match them against different pose estimates in the floorplan. And yet, this is exactly how most robotic scan-matching algorithms operate. Similarly, we do not extrude the 2D geometry present in the floorplan into 3D and try to align it to the real-world. And yet, this is how most vision-based approaches localise. Humans do the exact opposite. Instead of depth, we use high level semantic cues. Instead of extruding the floorplan up into the third dimension, we collapse the 3D world into a 2D representation. Evidence of this is that many of the floorplans we use in everyday life are not accurate, opting instead for high levels of discriminative landmarks. In this work, we use this insight to present a global localisation approach that relies solely on the semantic labels present in the floorplan and extracted from RGB images. While our approach is able to use range measurements if available, we demonstrate that they are unnecessary as we can achieve results comparable to state-of-the-art without them.
ER  - 

TY  - CONF
TI  - Design of a Novel 3-DoF Leg with Series and Parallel Compliant Actuation for Energy Efficient Articulated Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6068
EP  - 6075
AU  - W. Roozing
AU  - Z. Ren
AU  - N. G. Tsagarakis
PY  - 2018
KW  - actuators
KW  - control system synthesis
KW  - elasticity
KW  - legged locomotion
KW  - motion control
KW  - similar mass
KW  - mass distribution
KW  - biarticulated actuation configuration
KW  - mechanical design
KW  - actuation configuration principles
KW  - 3-DoF leg
KW  - parallel compliant actuation
KW  - series-elastic main actuators
KW  - leg design
KW  - energy efficient articulated robots
KW  - Legged locomotion
KW  - Actuators
KW  - Knee
KW  - Pulleys
KW  - Torque
KW  - Energy storage
DO  - 10.1109/ICRA.2018.8460493
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This work presents the development of a 3-DoF leg with series and parallel compliant actuation. Series-elastic main actuators are combined with parallel high efficiency energy storage branches, to substantially improve energy efficiency. The leg design is semi-anthropomorphic, with similar mass and mass distribution to the human limb, and includes a biarticulated actuation configuration. The parallel branches are driven by secondary motors and their design parameters are optimised. The mechanical design of the prototype leg is presented, introducing details of the actuation configuration principles employed. Preliminary experimental data are presented, in which a baseline series-elastic-only configuration is compared with configurations with mono- and biarticulated parallel branches, respectively. The results effectively demonstrate the concept's potential, showing improvements of 53% and 60% in electrical power consumption while the leg is executing loaded cyclic motion profiles.
ER  - 

TY  - CONF
TI  - Design of a Serial-Parallel Hybrid Leg for a Humanoid Robot
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6076
EP  - 6081
AU  - K. G. Gim
AU  - J. Kim
AU  - K. Yamane
PY  - 2018
KW  - end effectors
KW  - gait analysis
KW  - humanoid robots
KW  - legged locomotion
KW  - manipulator kinematics
KW  - medical robotics
KW  - motion control
KW  - position control
KW  - trajectory tracking
KW  - gait trajectory
KW  - bar-linkage mechanism
KW  - serial mechanism
KW  - twin 3 DOF serial chains
KW  - parallel mechanisms
KW  - serial mechanisms
KW  - 6 DOF leg mechanism
KW  - humanoid robot
KW  - serial-parallel Hybrid Leg
KW  - inverse kinematics
KW  - forward kinematics
KW  - conventional serial leg
KW  - commercial robot
KW  - kinematic specification
KW  - hardware prototype
KW  - knee pitch rotation
KW  - Legged locomotion
KW  - Kinematics
KW  - Couplings
KW  - Prototypes
KW  - Hip
KW  - Hardware
DO  - 10.1109/ICRA.2018.8460733
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a 6 DOF leg mechanism for a humanoid robot. The proposed Hybrid Leg is designed to combine serial and parallel mechanisms and consists of a pair of twin 3 DOF serial chains in parallel. A 5-bar-linkage mechanism is implemented to the serial mechanism to generate 2 DOF motion regarding hip and knee pitch rotation. The hardware prototype is designed by matching the kinematic specification of a commercial robot's leg to compare the proposed mechanism with a conventional serial leg. We derive the analytical expressions of its forward and inverse kinematics. End-effector workspaces are shown with plots and inverse dynamics analysis of Hybrid Leg and serial leg with a given walking gait trajectory is presented. Hardware experiment is conducted with a prototype to verify the simulated workspace and trajectory tracking performance.
ER  - 

TY  - CONF
TI  - Self-Engaging Spined Gripper with Dynamic Penetration and Release for Steep Jumps
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6082
EP  - 6089
AU  - J. S. Lee
AU  - M. Plecnik
AU  - J. Yang
AU  - R. S. Fearing
PY  - 2018
KW  - actuators
KW  - adhesion
KW  - aerospace robotics
KW  - bone
KW  - gait analysis
KW  - grippers
KW  - impact (mechanical)
KW  - legged locomotion
KW  - robot kinematics
KW  - substrates
KW  - dynamic penetration
KW  - steep jumps
KW  - high impact forces
KW  - low duty cycles
KW  - monopedal jumping robots
KW  - slipping foot
KW  - dynamic jumping robot
KW  - surface approach speed
KW  - cycle time
KW  - penetrable substrate
KW  - gripper mechanism
KW  - robot Salto
KW  - angled spines
KW  - penetrable inclines
KW  - holding angles
KW  - leg crouch-extension
KW  - actuators
KW  - static adhesion
KW  - ceiling
KW  - self engaging spined gripper
KW  - kinematics
KW  - Grippers
KW  - Pins
KW  - Robot kinematics
KW  - Legged locomotion
KW  - Couplings
KW  - Substrates
DO  - 10.1109/ICRA.2018.8460933
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Due to high impact forces and low duty cycles, monopedal jumping robots are particularly susceptible to failure from a slipping foot. Spines provide a solution to reduce slip, but there has been little research on how to effectively engage them into a surface with a dynamic jumping robot. Previous robots utilizing spines operate in different regimes of surface approach speed and cycle time. For a penetrable substrate, spines must be directed into the surface at suitable holding angles, then extracted before the foot leaves the ground. We accomplished this by designing a gripper mechanism for the robot Salto that pushes in angled spines along their length and is kinematically constrained to engage/disengage with leg crouch/extension. The resulting mechanism introduces no new actuators, enables jumping on penetrable inclines up to 60°, and enables static adhesion to hold 7.5 times the robot's weight from a ceiling.
ER  - 

TY  - CONF
TI  - Design, Modeling, and Analysis of Inductive Resonant Coupling Wireless Power Transfer for Micro Aerial Vehicles (MAVs)
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6104
EP  - 6109
AU  - G. M. Plaizier
AU  - E. Andersen
AU  - B. Truong
AU  - X. He
AU  - S. Roundy
AU  - K. K. Leang
PY  - 2018
KW  - battery powered vehicles
KW  - coils
KW  - inductive power transmission
KW  - WPT system
KW  - transmit coil
KW  - WPT circuit design
KW  - power-transfer model
KW  - two-coil system
KW  - WPT circuitry
KW  - wirelessly powered MAV
KW  - inductive resonant coupling
KW  - microaerial vehicle
KW  - power transfer system
KW  - Batteries
KW  - Magnetic resonance
KW  - Integrated circuit modeling
KW  - Geometry
KW  - Analytical models
KW  - Couplings
KW  - Wireless power transfer
DO  - 10.1109/ICRA.2018.8461162
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents the design, modeling, analysis, and experimental validation of an inductive resonant wireless power transfer (WPT) system to power a micro aerial vehicle (MAV). Using WPT, in general, enables longer flight times, virtually eliminates the need for batteries, and minimizes down time for recharging or replacing batteries. The proposed WPT system consists of a transmit coil, which can either be fixed to ground or placed on a mobile platform, and a receive coil carried by the MAV. The details of the WPT circuit design are presented. A power-transfer model is developed for the two-coil system, where the model is used to select suitable coil geometries to maximize the power received by the MAV for hovering. Analysis, simulation, and experimental results are presented to demonstrate the effectiveness of the WPT circuitry. Finally, a wirelessly powered MAV that hovers above the transmit coil is demonstrated in a laboratory setting.
ER  - 

TY  - CONF
TI  - Simplified Quasi-Steady Aeromechanic Model for Flapping-Wing Robots with Passively Rotating Hinges
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6110
EP  - 6115
AU  - Z. Li
AU  - S. Suntharasantic
AU  - P. Chirarattananon
PY  - 2018
KW  - aerodynamics
KW  - aerospace components
KW  - approximation theory
KW  - bending strength
KW  - drag
KW  - hinges
KW  - Navier-Stokes equations
KW  - robot kinematics
KW  - torque
KW  - model predictions
KW  - flapping-wing robots
KW  - flexural passive wing hinges
KW  - aerodynamic forces
KW  - balanced torque
KW  - quasisteady aeromechanic model
KW  - rotating hinge
KW  - mechanical complexity
KW  - drag
KW  - stroke-averaged forces
KW  - Aerodynamics
KW  - Torque
KW  - Robots
KW  - Fasteners
KW  - Predictive models
KW  - Drag
KW  - Force
DO  - 10.1109/ICRA.2018.8461020
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - At millimeter and centimeter scales, flapping-wing robots often employ flexural passive wing hinges to eliminate extra actuation and mechanical complexity. In this paper, we propose a modified quasi-steady model for predicting aerodynamic forces from a flapping wing with a passively rotating hinge. The model is based on a simplifying assumption of balanced torque (aerodynamic torque equals to the restoring torque from the hinge). The resulting lift and drag can then be accurately predicted by the modified quasi-steady model without direct knowledge of the angle of attack of the wing. Approximate expression of stroke-averaged forces are also derived. We performed flapping experiments on a centimeter-scale device and the measured lifts show good agreement with the model predictions.
ER  - 

TY  - CONF
TI  - Surface Edge Explorer (see): Planning Next Best Views Directly from 3D Observations
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6116
EP  - 6123
AU  - R. Border
AU  - J. D. Gammell
AU  - P. Newman
PY  - 2018
KW  - computational geometry
KW  - feature extraction
KW  - image reconstruction
KW  - mesh generation
KW  - solid modelling
KW  - Surface Edge Explorer
KW  - Best View planning
KW  - NBV planning approaches
KW  - voxel grids
KW  - triangulated meshes
KW  - surface geometry
KW  - high-resolution models
KW  - Surface representations
KW  - multiple survey stages
KW  - scene-model-free NBV planning approach
KW  - density representation
KW  - current measurements
KW  - observed surface boundaries
KW  - surface coverage
KW  - evaluated state-of-the-art volumetric approaches
KW  - Next Best views
KW  - time 3.0 d
KW  - Planning
KW  - Computational modeling
KW  - Density measurement
KW  - Geometry
KW  - Surface treatment
KW  - Three-dimensional displays
DO  - 10.1109/ICRA.2018.8461098
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Surveying 3D scenes is a common task in robotics. Systems can do so autonomously by iteratively obtaining measurements. This process of planning observations to improve the model of a scene is called Next Best View (NBV) planning. NBV planning approaches often use either volumetric (e.g., voxel grids) or surface (e.g., triangulated meshes) representations. Volumetric approaches generalise well between scenes as they do not depend on surface geometry but do not scale to high-resolution models of large scenes. Surface representations can obtain high-resolution models at any scale but often require tuning of unintuitive parameters or multiple survey stages. This paper presents a scene-model-free NBV planning approach with a density representation. The Surface Edge Explorer (SEE) uses the density of current measurements to detect and explore observed surface boundaries. This approach is shown experimentally to provide better surface coverage in lower computation time than the evaluated state-of-the-art volumetric approaches while moving equivalent distances.
ER  - 

TY  - CONF
TI  - Active Image-Based Modeling with a Toy Drone
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6124
EP  - 6131
AU  - R. Huang
AU  - D. Zou
AU  - R. Vaughan
AU  - P. Tan
PY  - 2018
KW  - autonomous aerial vehicles
KW  - data acquisition
KW  - image reconstruction
KW  - image sensors
KW  - solid modelling
KW  - stereo image processing
KW  - iterative linear method
KW  - multiview stereo problem
KW  - online model reconstruction
KW  - toy unmanned aerial vehicle
KW  - data acquisition
KW  - toy drone
KW  - image-based modeling techniques
KW  - photo-realistic 3D models
KW  - multi-view stereo algorithm
KW  - active image-based modeling
KW  - Three-dimensional displays
KW  - Image reconstruction
KW  - Solid modeling
KW  - Unmanned aerial vehicles
KW  - Planning
KW  - Pipelines
KW  - Cameras
DO  - 10.1109/ICRA.2018.8460673
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Image-based modeling techniques [1]-[3] can now generate photo-realistic 3D models from images. But it is up to users to provide high quality images with good coverage and view overlap, which makes the data capturing process tedious and time consuming. We seek to automate data capturing for image-based modeling. The core of our system is an iterative linear method to solve the multi-view stereo (MVS) problem quickly and plan the Next-Best-View (NBV) effectively. Our fast MVS algorithm enables online model reconstruction and quality assessment to determine the NBVs on the fly. We test our system with a toy unmanned aerial vehicle (UAV) in simulated, indoor and outdoor experiments. Results show that our system improves the efficiency of data acquisition and ensures the completeness of the final model.
ER  - 

TY  - CONF
TI  - Fusing Object Context to Detect Functional Area for Cognitive Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6132
EP  - 6139
AU  - H. Cheng
AU  - J. Cai
AU  - Q. Liu
AU  - Z. Zhang
AU  - K. Yang
AU  - C. C. Loy
AU  - L. Lin
PY  - 2018
KW  - feature extraction
KW  - image classification
KW  - image fusion
KW  - image recognition
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object context
KW  - deep learning
KW  - object detection dataset
KW  - current object detection framework
KW  - functional area detection
KW  - functionality-related feature
KW  - object-related
KW  - potential image regions
KW  - deep-model-based classifier
KW  - functional area image dataset
KW  - area detection problem
KW  - image recognition
KW  - cognitive robot
KW  - Feature extraction
KW  - Object detection
KW  - Robots
KW  - Task analysis
KW  - Machine learning
KW  - Proposals
KW  - Image recognition
DO  - 10.1109/ICRA.2018.8460590
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - A cognitive robot usually needs to perform multiple tasks in practice and needs to locate the desired area for each task. Since deep learning has achieved substantial progress in image recognition, to solve this area detection problem, it is straightforward to label a functional area (affordance) image dataset and apply a well-trained deep-model-based classifier on all the potential image regions. However, annotating the functional area is time consuming and the requirement of large amount of training data limits the application scope. We observe that the functional area are usually related to the surrounding object context. In this work, we propose to use the existing object detection dataset and employ the object context as effective prior to improve the performance without additional annotated data. In particular, we formulate a two-stream network that fuses the object-related and functionality-related feature for functional area detection. The whole system is formulated in an end-to-end manner and easy to implement with current object detection framework. Experiments demonstrate that the proposed network outperforms current method by almost 20% in terms of precision and recall.
ER  - 

TY  - CONF
TI  - When Regression Meets Manifold Learning for Object Recognition and Pose Estimation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6140
EP  - 6146
AU  - M. Bui
AU  - S. Zakharov
AU  - S. Albarqouni
AU  - S. Ilic
AU  - N. Navab
PY  - 2018
KW  - convolution
KW  - feedforward neural nets
KW  - image matching
KW  - nearest neighbour methods
KW  - object recognition
KW  - pose estimation
KW  - regression analysis
KW  - manifold learning
KW  - pose regression
KW  - NN descriptor matching
KW  - manifold descriptor learning
KW  - multitask learning framework
KW  - nearest neighbor search
KW  - convolutional neural networks
KW  - pose estimation
KW  - object recognition
KW  - Pose estimation
KW  - Manifolds
KW  - Task analysis
KW  - Training
KW  - Robustness
KW  - Object recognition
KW  - Three-dimensional displays
DO  - 10.1109/ICRA.2018.8460654
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this work, we propose a method for object recognition and pose estimation from depth images using convolutional neural networks. Previous methods addressing this problem rely on manifold learning to learn low dimensional viewpoint descriptors and employ them in a nearest neighbor search on an estimated descriptor space. In comparison we create an efficient multi-task learning framework combining manifold descriptor learning and pose regression. By combining the strengths of manifold learning using triplet loss and pose regression, we could either estimate the pose directly reducing the complexity compared to NN search, or use the learned descriptor for the NN descriptor matching. By in depth experimental evaluation of the novel loss function we observed that the view descriptors learned by the network are much more discriminative resulting in almost 30% increase regarding relative pose accuracy compared to related works. On the other hand, regarding directly regressed poses we obtained important improvement compared to simple pose regression. By leveraging the advantages of both manifold learning and regression tasks, we are able to improve the current state-of-the-art for object recognition and pose retrieval.
ER  - 

TY  - CONF
TI  - Ultra-Fast Multi-Scale Shape Estimation of Light Transport Matrix for Complex Light Reflection Objects
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6147
EP  - 6152
AU  - N. Chiba
AU  - K. Hashimoto
PY  - 2018
KW  - cameras
KW  - image reconstruction
KW  - image resolution
KW  - light reflection
KW  - optical projectors
KW  - shape measurement
KW  - sparse matrices
KW  - multiple light paths
KW  - complex light reflection objects
KW  - Light Transport Matrix estimation
KW  - LT Matrix estimation
KW  - high resolution measurement
KW  - sparse matrix representation
KW  - ultra-fast multiscale shape estimation
KW  - target objects
KW  - specular reflection
KW  - light path
KW  - memory efficiency
KW  - 256 × 256 resolution projector
KW  - camera system
KW  - 3D measurement methods
KW  - Cameras
KW  - Three-dimensional displays
KW  - Estimation
KW  - Sparse matrices
KW  - Shape
KW  - Shape measurement
KW  - Sensors
DO  - 10.1109/ICRA.2018.8460892
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - 3D measurement of target objects characterized by specular reflection or subsurface scatterings cannot be measured by traditional 3D measurement methods because these targets have multiple light paths that make it difficult to determine the unique surface. We define these objects as complex light reflection objects. In this case, 3D measurement methods based on Light Transport (LT) Matrix estimation may be a solution to measure these complex light reflection objects, because LT Matrix captures every light path, and we can identify all 3D points on the target shape by using LT Matrix. However, these methods either provide low resolution results, or they are too slow for use in robot vision in practice. In this paper, we suppress the computational cost of LT Matrix estimation by dividing LT Matrix estimation into multi-scale. The proposed method reduces the number of candidate combinations between camera pixels and projector pixels greatly by using the information given by low resolution observations. The proposed algorithm allows high resolution measurement of the LT Matrix very efficiently. Furthermore, careful implementation of our method by using a sparse matrix representation achieves memory efficiency. We evaluated our method by measuring 3D points for a 256 × 256 resolution projector and camera system, which is an LT matrix 4096 times larger than that developed in our previous study [1] and 100 times faster than our naïve implementation of [2].
ER  - 

TY  - CONF
TI  - A Deep Learning-Based Stalk Grasping Pipeline
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6161
EP  - 6167
AU  - T. Parhar
AU  - H. Baweja
AU  - M. Jenkins
AU  - G. Kantor
PY  - 2018
KW  - crops
KW  - image segmentation
KW  - industrial manipulators
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - stalk segmentation
KW  - grasp point generation pipeline
KW  - high stalk density
KW  - lighting variation
KW  - custom-built ground robot
KW  - end-to-end system
KW  - average grasping accuracy
KW  - Generative Adversarial Network
KW  - in-situ sorghum stalk detection
KW  - online pipeline
KW  - deep learning-based high throughput
KW  - labor-intensive phenotyping processes
KW  - robotic solutions
KW  - plant attributes
KW  - precise measurements
KW  - fast measurements
KW  - deep learning-based stalk grasping pipeline
KW  - pixel-wise stalk detection
KW  - grasp point detection
KW  - stalk detection F1 score
KW  - Robots
KW  - Pipelines
KW  - Image segmentation
KW  - Generators
KW  - Three-dimensional displays
KW  - Gallium nitride
KW  - Cameras
DO  - 10.1109/ICRA.2018.8460597
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The need for fast and precise measurements of plant attributes makes robotic solutions an ideal replacement for labor-intensive phenotyping processes. In this work we present a deep learning-based high throughput, online pipeline for in-situ sorghum stalk detection and grasping. We use a variation of Generative Adversarial Network (GAN) for stalk segmentation trained on a relatively small number of images followed by a grasp point generation pipeline. The presented pipeline is robust to field challenges such as occlusions, high stalk density and lighting variation, and was deployed on a custom-built ground robot. We tested our end-to-end system in a field of Sorghum bicolor in South Carolina, USA, achieving an average grasping accuracy of 74.13% and a stalk detection F1 score of 0.90. Grasp point detection for plant manipulation takes an average of 0.98 seconds, and pixel-wise stalk detection takes 0.2 seconds per image.
ER  - 

TY  - CONF
TI  - Configuration of Perception Systems via Planning Over Factor Graphs
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6168
EP  - 6174
AU  - V. Dietrich
AU  - B. Kast
AU  - P. Schmitt
AU  - S. Albrecht
AU  - M. Fiegert
AU  - W. Feiten
AU  - M. Beetz
PY  - 2018
KW  - assembling
KW  - graph theory
KW  - planning (artificial intelligence)
KW  - production engineering computing
KW  - sensor fusion
KW  - sensors
KW  - perception systems
KW  - factor graph
KW  - automated systems
KW  - data processing algorithms
KW  - sensor noise
KW  - calibration errors
KW  - planning problem
KW  - probabilistic graphical models
KW  - configuration space
KW  - perceptions systems
KW  - perception steps
KW  - sensor data fusion
KW  - industrial assembly
KW  - Task analysis
KW  - Planning
KW  - Robot sensing systems
KW  - Uncertainty
KW  - Semantics
KW  - Cameras
KW  - Probabilistic logic
DO  - 10.1109/ICRA.2018.8460955
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Sensor guided, automated systems require the composition of various sensors and data processing algorithms to obtain relevant information for performing their task. Many applications have additional requirements such as a certain accuracy, which has to be achieved despite sensor noise and calibration errors. In this paper we model the configuration of perception systems as a planning problem over probabilistic graphical models. We work on a subset of the full configuration space of perceptions systems, specifically the used sensors, data processing algorithms and view poses. Based on a semantic description of the goal, available sensors and data processing algorithms, our system plans perception steps and sensor data fusion autonomously. The planner operates by constructing a factor graph until the accuracy requirements of tasks are fulfilled or unobtainable with the available action set. We validate our approach in an industrial assembly scenario.
ER  - 

TY  - CONF
TI  - Intelligent Shipwreck Search Using Autonomous Underwater Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6175
EP  - 6182
AU  - J. Rutledge
AU  - W. Yuan
AU  - J. Wu
AU  - S. Freed
AU  - A. Lewis
AU  - Z. Wood
AU  - T. Gambin
AU  - C. Clark
PY  - 2018
KW  - archaeology
KW  - autonomous underwater vehicles
KW  - control engineering computing
KW  - image processing
KW  - intelligent control
KW  - marine control
KW  - mobile robots
KW  - path planning
KW  - sonar
KW  - archaeological survey
KW  - intelligent shipwreck search
KW  - autonomous underwater vehicles
KW  - autonomous robot system
KW  - multistep pipeline
KW  - high altitude scan
KW  - low-resolution side scan sonar data
KW  - image processing software
KW  - AUV path planner
KW  - archaeological sites
KW  - underwater archaeological sites
KW  - ranking algorithm
KW  - Sonar
KW  - Proposals
KW  - Clustering algorithms
KW  - Pipelines
KW  - Feature extraction
KW  - Software
DO  - 10.1109/ICRA.2018.8460548
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents an autonomous robot system that is designed to autonomously search for and geo-localize potential underwater archaeological sites. The system, based on Autonomous Underwater Vehicles, invokes a multi-step pipeline. First, the AUV constructs a high altitude scan over a large area to collect low-resolution side scan sonar data. Second, image processing software is employed to automatically detect and identify potential sites of interest. Third, a ranking algorithm assigns importance scores to each site. Fourth, an AUV path planner is used to plan a time-limited path that visits sites with a high importance at a low altitude to acquire high-resolution sonar data. Last, the AUV is deployed to follow this path. This system was implemented and evaluated during an archaeological survey located along the coast of Malta. These experiments demonstrated that the system is able to identify valuable archaeological sites accurately and efficiently in a large previously unsurveyed area. Also, the planned missions led to the discovery of a historical plane wreck whose location was previously unknown.
ER  - 

TY  - CONF
TI  - A Robust Model Predictive Control Approach for Autonomous Underwater Vehicles Operating in a Constrained Workspace
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6183
EP  - 6188
AU  - S. Heshmati-alamdari
AU  - G. C. Karras
AU  - P. Marantos
AU  - K. J. Kyriakopoulos
PY  - 2018
KW  - autonomous underwater vehicles
KW  - collision avoidance
KW  - mobile robots
KW  - nonlinear control systems
KW  - predictive control
KW  - robust control
KW  - robust Model Predictive Control approach
KW  - constrained workspace
KW  - underwater robotic vehicles
KW  - static obstacles
KW  - workspace boundary
KW  - thruster saturation
KW  - vehicle velocity
KW  - control design
KW  - ocean currents
KW  - control inputs
KW  - way-point tracking mission
KW  - control strategy
KW  - constrained test tank
KW  - Nonlinear Model Predictive Control scheme
KW  - way points
KW  - underwater robotic vehicle
KW  - Oceans
KW  - Robots
KW  - Underwater vehicles
KW  - Mathematical model
KW  - Vehicle dynamics
KW  - Energy consumption
KW  - Computational modeling
DO  - 10.1109/ICRA.2018.8460918
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a novel Nonlinear Model Predictive Control (NMPC) scheme for underwater robotic vehicles operating in a constrained workspace including static obstacles. The purpose of the controller is to guide the vehicle towards specific way points. Various limitations such as: obstacles, workspace boundary, thruster saturation and predefined desired upper bound of the vehicle velocity are captured as state and input constraints and are guaranteed during the control design. The proposed scheme incorporates the full dynamics of the vehicle in which the ocean currents are also involved. Hence, the control inputs calculated by the proposed scheme are formulated in a way that the vehicle will exploit the ocean currents, when these are in favor of the way-point tracking mission which results in reduced energy consumption by the thrusters. The performance of the proposed control strategy is experimentally verified using a 4 Degrees of Freedom (DoF) underwater robotic vehicle inside a constrained test tank with obstacles.
ER  - 

TY  - CONF
TI  - Design, Modeling, and Nonlinear Model Predictive Tracking Control of a Novel Autonomous Surface Vehicle
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6189
EP  - 6196
AU  - W. Wang
AU  - L. A. Mateos
AU  - S. Park
AU  - P. Leoni
AU  - B. Gheneti
AU  - F. Duarte
AU  - C. Ratti
AU  - D. Rus
PY  - 2018
KW  - autonomous underwater vehicles
KW  - boats
KW  - Global Positioning System
KW  - hydrodynamics
KW  - indoor environment
KW  - matrix algebra
KW  - mobile robots
KW  - motion control
KW  - nonlinear control systems
KW  - predictive control
KW  - tracking
KW  - trajectory control
KW  - nonlinear model predictive tracking control
KW  - autonomous surface vehicle
KW  - autonomous robotic boat
KW  - indoor environments
KW  - outdoor environments
KW  - cross type four-thruster configuration
KW  - robot prototype
KW  - nonlinear dynamic model
KW  - NMPC algorithm
KW  - surface swarm robotics testbeds
KW  - trajectory tracking
KW  - holonomic motions
KW  - fiberglass
KW  - centripetal matrix
KW  - Coriolis
KW  - hydrodynamic
KW  - damping
KW  - GPS modules
KW  - inertial measurement unit
KW  - IMU
KW  - swimming pool
KW  - natural river
KW  - code generation strategy
KW  - Boats
KW  - Vehicle dynamics
KW  - Symmetric matrices
KW  - Global Positioning System
KW  - Heuristic algorithms
KW  - Robot kinematics
DO  - 10.1109/ICRA.2018.8460632
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we present the design, modeling, and real-time nonlinear model predictive control (NMPC) of an autonomous robotic boat. The robot is easy to manufacture, highly maneuverable, and capable of accurate trajectory tracking in both indoor and outdoor environments. In particular, a cross type four-thruster configuration is proposed for the robotic boat to produce efficient holonomic motions. The robot prototype is rapidly 3D-printed and then sealed by adhering several layers of fiberglass. To achieve accurate tracking control, we formulate an NMPC strategy for the four-control-input boat with control input constraints, where the nonlinear dynamic model includes a Coriolis and centripetal matrix, the hydrodynamic added mass, and damping. By integrating “GPS” modules and an inertial measurement unit (IMU) into the robot, we demonstrate accurate trajectory tracking of the robotic boat along preplanned paths in both a swimming pool and a natural river. Furthermore, the code generation strategy employed in our paper yields a two order of magnitude improvement in the run time of the NMPC algorithm compared to similar systems. The robot is designed to form the basis for surface swarm robotics testbeds, on which collective algorithms for surface transportation and self-assembly of dynamic floating infrastructures can be assessed.
ER  - 

TY  - CONF
TI  - Reinforcement Learning of Depth Stabilization with a Micro Diving Agent
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6197
EP  - 6203
AU  - G. Brinkmann
AU  - W. M. Bessa
AU  - D. Duecker
AU  - E. Kreuzer
AU  - E. Solowjow
PY  - 2018
KW  - embedded systems
KW  - learning (artificial intelligence)
KW  - microrobots
KW  - multi-agent systems
KW  - robot programming
KW  - underwater vehicles
KW  - model-based value-function RL algorithm
KW  - micro underwater agents
KW  - underwater robotics
KW  - underwater depth stabilization
KW  - light embedded systems
KW  - control tasks
KW  - microdiving agent
KW  - reinforcement learning
KW  - Computational modeling
KW  - Learning (artificial intelligence)
KW  - Task analysis
KW  - Robot kinematics
KW  - Heuristic algorithms
KW  - Force
DO  - 10.1109/ICRA.2018.8461137
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Reinforcement learning (RL) allows robots to solve control tasks through interaction with their environment. In this paper we study a model-based value-function RL approach, which is suitable for computationally limited robots and light embedded systems. We develop a diving agent, which uses the RL algorithm for underwater depth stabilization. Simulations and experiments with the micro diving agent demonstrate its ability to learn the depth stabilization task.
ER  - 

TY  - CONF
TI  - Real-Time Underwater 3D Reconstruction Using Global Context and Active Labeling
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6204
EP  - 6211
AU  - R. DeBortoli
AU  - A. Nicolai
AU  - F. Li
AU  - G. A. Hollinger
PY  - 2018
KW  - cameras
KW  - feature extraction
KW  - feedforward neural nets
KW  - image classification
KW  - image reconstruction
KW  - image resolution
KW  - image sensors
KW  - sonar imaging
KW  - underwater vehicles
KW  - underwater environments
KW  - sonar images
KW  - low-resolution imagery
KW  - standard cameras
KW  - automatic feature extractors
KW  - sonar imagery
KW  - environment reconstructions
KW  - high data capture rates
KW  - standard imaging sonars
KW  - high-quality frames
KW  - feature annotation
KW  - real-time reconstruction capability
KW  - underwater vehicle
KW  - time underwater 3D reconstruction
KW  - real-time 3D reconstruction
KW  - convolutional neural network
KW  - Image reconstruction
KW  - Feature extraction
KW  - Real-time systems
KW  - Three-dimensional displays
KW  - Imaging
KW  - Sonar measurements
DO  - 10.1109/ICRA.2018.8461148
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this work we develop a novel framework that enables the real-time 3D reconstruction of underwater environments using features from 2D sonar images. Due to noisy and low-resolution imagery as compared with standard cameras, automatic feature extractors for sonar images are not reliable in many scenarios. Thus, a human often needs to hand-select features in sonar imagery for environment reconstructions. Given the high data capture rates of standard imaging sonars (on the order of 20Hz), hand-annotating the features in every frame cannot be done in real-time. To address this we use a Convolutional Neural Network (CNN) that analyzes incoming imagery in real-time and proposes only a small subset of high-quality frames to the user for feature annotation. We demonstrate that our approach provides real-time reconstruction capability without loss in classification performance on datasets captured onboard our underwater vehicle while operating in a variety of environments.
ER  - 

TY  - CONF
TI  - Dynamic Reconfiguration of Mission Parameters in Underwater Human-Robot Collaboration
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6212
EP  - 6219
AU  - M. J. Islam
AU  - M. Ho
AU  - J. Sattar
PY  - 2018
KW  - feedforward neural nets
KW  - finite state machines
KW  - gesture recognition
KW  - human-robot interaction
KW  - mobile robots
KW  - hand gestures
KW  - hand gesture recognition
KW  - gesture-to-instruction mapping
KW  - finite-state machine
KW  - convolutional neural network
KW  - human-robot collaborative tasks
KW  - autonomous underwater robots
KW  - parameter reconfiguration method
KW  - real-time programming
KW  - underwater human-robot collaboration
KW  - mission parameters
KW  - dynamic reconfiguration
KW  - Robustness
KW  - Task analysis
KW  - Visualization
KW  - Robot sensing systems
KW  - Unmanned underwater vehicles
KW  - Gesture recognition
DO  - 10.1109/ICRA.2018.8461197
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a real-time programming and parameter reconfiguration method for autonomous underwater robots in human-robot collaborative tasks. Using a set of intuitive and meaningful hand gestures, we develop a syntactically simple framework that is computationally more efficient than a complex, grammar-based approach. In the proposed framework, a convolutional neural network is trained to provide accurate hand gesture recognition; subsequently, a finite-state machine- based deterministic model performs efficient gesture-to-instruction mapping and further improves robustness of the interaction scheme. The key aspect of this framework is that it can be easily adopted by divers for communicating simple instructions to underwater robots without using artificial tags such as fiducial markers or requiring memorization of a potentially complex set of language rules. Extensive experiments are performed both on field-trial data and through simulation, which demonstrate the robustness, efficiency, and portability of this framework in a number of different scenarios. Finally, a user interaction study is presented that illustrates the gain in the ease of use of our proposed interaction framework compared to the existing methods for the underwater domain.
ER  - 

TY  - CONF
TI  - Gaussian Process Adaptive Sampling Using the Cross-Entropy Method for Environmental Sensing and Monitoring
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6220
EP  - 6227
AU  - Y. T. Tan
AU  - A. Kunapareddy
AU  - M. Kobilarov
PY  - 2018
KW  - bathymetry
KW  - entropy
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - sampling methods
KW  - single ROI
KW  - deepest region
KW  - coastal bathymetry mapping mission validate
KW  - efficient sampling strategy
KW  - latest sensory measurements
KW  - sampling density
KW  - CE trajectory optimization
KW  - higher spatial variability
KW  - exhibit extreme sensory measurements
KW  - exploring learning
KW  - initial stage
KW  - path planning
KW  - GP-UCB
KW  - GP upper confidence
KW  - receding-horizon Cross-Entropy trajectory optimization
KW  - environmental sensing
KW  - cross-entropy method
KW  - Gaussian process adaptive sampling
KW  - Robot sensing systems
KW  - Adaptation models
KW  - Optimization
KW  - Predictive models
KW  - Trajectory
KW  - Uncertainty
DO  - 10.1109/ICRA.2018.8460821
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we focus on adaptive sampling on a Gaussian Processes (GP) using the receding-horizon Cross-Entropy (CE) trajectory optimization. Specifically, we employ the GP upper confidence bound (GP-UCB) as the optimization criteria to adaptively plan sampling paths that balance the exploitation-exploration trade-off. Path planning at the initial stage focuses on exploring and learning a model of the environment, and later, on exploiting the learned model to focus sampling around regions that exhibit extreme sensory measurements and much higher spatial variability, denoted as the Region of Interest (ROI). The integration of the CE trajectory optimization allows the sampling density to be dynamically adjusted based on the latest sensory measurements, thus providing an efficient sampling strategy for sensing and localizing the ROI. We demonstrate the effectiveness of the proposed method in exploring simulated scalar fields with single or multiple ROIs. Field experiments with an Unmanned Surface Vehicle (USV) in a coastal bathymetry mapping mission validate the approach's capability in quickly exploring and mapping the given area, and then focusing and increasing the sampling density around the deepest region, as a surrogate for e.g. the extremal concentration of a pollutant in the environment.
ER  - 

TY  - CONF
TI  - OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6236
EP  - 6243
AU  - T. Pham
AU  - G. De Magistris
AU  - R. Tachibana
PY  - 2018
KW  - control engineering computing
KW  - decision making
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - optimisation
KW  - decision-making problems
KW  - reinforcement learning architecture
KW  - OptLayer
KW  - neural network
KW  - closest actions
KW  - safe actions
KW  - robot reaching tasks
KW  - practical constrained optimization
KW  - deep reinforcement learning techniques
KW  - Robot kinematics
KW  - Neural networks
KW  - Task analysis
KW  - Optimization
KW  - Learning (artificial intelligence)
KW  - Training
DO  - 10.1109/ICRA.2018.8460547
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - While deep reinforcement learning techniques have recently produced considerable achievements on many decision-making problems, their use in robotics has largely been limited to simulated worlds or restricted motions, since unconstrained trial-and-error interactions in the real world can have undesirable consequences for the robot or its environment. To overcome such limitations, we propose a novel reinforcement learning architecture, OptLayer, that takes as inputs possibly unsafe actions predicted by a neural network and outputs the closest actions that satisfy chosen constraints. While learning control policies often requires carefully crafted rewards and penalties while exploring the range of possible actions, OptLayer ensures that only safe actions are actually executed and unsafe predictions are penalized during training. We demonstrate the effectiveness of our approach on robot reaching tasks, both simulated and in the real world.
ER  - 

TY  - CONF
TI  - Composable Deep Reinforcement Learning for Robotic Manipulation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6244
EP  - 6251
AU  - T. Haarnoja
AU  - V. Pong
AU  - A. Zhou
AU  - M. Dalal
AU  - P. Abbeel
AU  - S. Levine
PY  - 2018
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - composable deep reinforcement
KW  - model-free deep reinforcement learning
KW  - simulated robotic manipulation
KW  - model-free methods
KW  - real-world robotic tasks
KW  - maximum entropy policies
KW  - soft Q-learning
KW  - real-world robotic manipulation
KW  - Entropy
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Neural networks
KW  - Machine learning
KW  - Task analysis
KW  - Training
DO  - 10.1109/ICRA.2018.8460756
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.
ER  - 

TY  - CONF
TI  - Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6252
EP  - 6259
AU  - P. Long
AU  - T. Fan
AU  - X. Liao
AU  - W. Liu
AU  - H. Zhang
AU  - J. Pan
PY  - 2018
KW  - collision avoidance
KW  - decentralised control
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-robot systems
KW  - multiscenario multistage training framework
KW  - optimal policy
KW  - policy gradient
KW  - reinforcement learning algorithm
KW  - learned sensor-level collision avoidance policy
KW  - final learned policy
KW  - collision-free paths
KW  - large-scale robot system
KW  - deep reinforcement learning
KW  - safe collision avoidance policy
KW  - efficient collision avoidance policy
KW  - optimally decentralized multirobot collision avoidance
KW  - agent-level feature extraction
KW  - decentralized methods
KW  - maps raw sensor measurements
KW  - multirobot systems
KW  - decentralized sensor-level collision avoidance policy
KW  - local collision-free action
KW  - distributed multirobot collision avoidance systems
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Navigation
KW  - Robustness
KW  - Training
DO  - 10.1109/ICRA.2018.8461113
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths without observing other robots' states and intents. While other distributed multi-robot collision avoidance systems exist, they often require extracting agent-level features to plan a local collision-free action, which can be computationally prohibitive and not robust. More importantly, in practice the performance of these methods are much lower than their centralized counterparts. We present a decentralized sensor-level collision avoidance policy for multi-robot systems, which directly maps raw sensor measurements to an agent's steering commands in terms of movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots on rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. We validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient, collision-free paths for a large-scale robot system. We also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period, including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots. Videos are available at https://sites.google.com/view/drlmaca.
ER  - 

TY  - CONF
TI  - Tensegrity Robot Locomotion Under Limited Sensory Inputs via Deep Reinforcement Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6260
EP  - 6267
AU  - J. Luo
AU  - R. Edmunds
AU  - F. Rice
AU  - A. M. Agogino
PY  - 2018
KW  - learning systems
KW  - mobile robots
KW  - motion control
KW  - neurocontrollers
KW  - nonlinear dynamical systems
KW  - search problems
KW  - state-space methods
KW  - nonlinear dynamics
KW  - high-dimensional state space
KW  - robotic systems
KW  - space exploration
KW  - tensegrity robot locomotion
KW  - deep reinforcement learning algorithms
KW  - policy learning process
KW  - locomotion control policies
KW  - neural network policies
KW  - mirror descent guided policy search
KW  - end-to-end locomotion policies
KW  - tensegrity robotics
KW  - Robot sensing systems
KW  - Neural networks
KW  - Aerospace electronics
KW  - Hardware
KW  - NASA
KW  - Training
DO  - 10.1109/ICRA.2018.8463144
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Tensegrity robots are composed of rigid rods connected by elastic cables, and their unique light-weight yet compliant structure makes them an appealing choice for space exploration. However, locomotion control for these robotic systems remains difficult due to their nonlinear dynamics and high-dimensional state space. We demonstrate that in the domain of tensegrity robotics, it is possible to efficiently learn end-to-end locomotion policies using mirror descent guided policy search (MDGPS) even with limited sensory inputs. We compare learned neural network policies with other locomotion control policies in various testing environments; and results show that neural network policies consistently outperform others. We also shed light to the policy learning process by analyzing different choices of observation inputs to the robot. Moreover these findings motivate exploration of deep reinforcement learning algorithms in the domain of tensegrity robotics. We show preliminary results with one such locomotion example on discontinuous rough terrains.
ER  - 

TY  - CONF
TI  - Applying Asynchronous Deep Classification Networks and Gaming Reinforcement Learning-Based Motion Planners to Mobile Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6268
EP  - 6275
AU  - G. Ryou
AU  - Y. Sim
AU  - S. H. Yeon
AU  - S. Seok
PY  - 2018
KW  - control engineering computing
KW  - game theory
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - path planning
KW  - pattern classification
KW  - gaming reinforcement learning-based motion planner
KW  - mobile robotic platform
KW  - deep classifier
KW  - asynchronous deep classification network
KW  - visual recognition
KW  - motion planning
KW  - deep learning-based algorithms
KW  - TT2-bot
KW  - embedded neural networks
KW  - Mobile robots
KW  - Learning (artificial intelligence)
KW  - Machine learning
KW  - Neural networks
KW  - Sensors
KW  - Training
DO  - 10.1109/ICRA.2018.8460798
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we propose a new methodology to embed deep learning-based algorithms in both visual recognition and motion planning for general mobile robotic platforms. A framework for an asynchronous deep classification network is introduced to integrate heavy deep classification networks into a mobile robot with no loss of system bandwidth. Moreover, a gaming reinforcement learning-based motion planner, a novel and convenient embodiment of reinforcement learning, is introduced for simple implementation and high applicability. The proposed approaches are implemented and evaluated on a developed robot, TT2-bot. The evaluation was based on a mission devised for a qualitative evaluation of the general purposes and performances of a mobile robotic platform. The robot was required to recognize targets with a deep classifier and plan the path effectively using a deep motion planner. As a result, the robot verified that the proposed approaches successfully integrate deep learning technologies on the stand-alone mobile robot. The embedded neural networks for recognition and path planning were critical components for the robot.
ER  - 

TY  - CONF
TI  - Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6276
EP  - 6283
AU  - L. Xie
AU  - S. Wang
AU  - S. Rosa
AU  - A. Markham
AU  - N. Trigoni
PY  - 2018
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - three-term control
KW  - DRL network
KW  - standard Deep Deterministic Policy Gradient network
KW  - training wheels
KW  - deep reinforcement learning
KW  - robotic applications
KW  - robot applications
KW  - Assisted Reinforcement Learning
KW  - PID controller
KW  - local planning
KW  - navigation problems
KW  - simple control law
KW  - Training
KW  - Navigation
KW  - Acceleration
KW  - Robot kinematics
KW  - Machine learning
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461203
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Deep Reinforcement Learning (DRL) has been applied successfully to many robotic applications. However, the large number of trials needed for training is a key issue. Most of existing techniques developed to improve training efficiency (e.g. imitation) target on general tasks rather than being tailored for robot applications, which have their specific context to benefit from. We propose a novel framework, Assisted Reinforcement Learning, where a classical controller (e.g. a PID controller) is used as an alternative, switchable policy to speed up training of DRL for local planning and navigation problems. The core idea is that the simple control law allows the robot to rapidly learn sensible primitives, like driving in a straight line, instead of random exploration. As the actor network becomes more advanced, it can then take over to perform more complex actions, like obstacle avoidance. Eventually, the simple controller can be discarded entirely. We show that not only does this technique train faster, it also is less sensitive to the structure of the DRL network and consistently outperforms a standard Deep Deterministic Policy Gradient network. We demonstrate the results in both simulation and real-world experiments.
ER  - 

TY  - CONF
TI  - Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6284
EP  - 6291
AU  - D. Quillen
AU  - E. Jang
AU  - O. Nachum
AU  - C. Finn
AU  - J. Ibarz
AU  - S. Levine
PY  - 2018
KW  - learning (artificial intelligence)
KW  - Monte Carlo methods
KW  - neural nets
KW  - robot vision
KW  - deep neural network models
KW  - off-policy correction
KW  - vision-based robotic grasping
KW  - off-policy methods
KW  - deep reinforcement learning algorithms
KW  - off-policy learning
KW  - Monte Carlo methods
KW  - Grasping
KW  - Robots
KW  - Task analysis
KW  - Benchmark testing
KW  - Monte Carlo methods
KW  - Machine learning
KW  - Training
DO  - 10.1109/ICRA.2018.8461039
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms1.
ER  - 

TY  - CONF
TI  - Overcoming Exploration in Reinforcement Learning with Demonstrations
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6292
EP  - 6299
AU  - A. Nair
AU  - B. McGrew
AU  - M. Andrychowicz
AU  - W. Zaremba
AU  - P. Abbeel
PY  - 2018
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - reinforcement learning
KW  - reward function
KW  - task horizon
KW  - RL methods
KW  - exploration problem
KW  - multistep robotics tasks
KW  - robot arm
KW  - deep deterministic policy gradients
KW  - hindsight experience replay
KW  - simulated robotics tasks
KW  - Task analysis
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Stacking
KW  - Training
KW  - Mathematical model
KW  - Games
DO  - 10.1109/ICRA.2018.8463162
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.
ER  - 

TY  - CONF
TI  - Fast Image-Based Geometric Change Detection Given a 3D Model
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6308
EP  - 6315
AU  - E. Palazzolo
AU  - C. Stachniss
PY  - 2018
KW  - image reconstruction
KW  - image sequences
KW  - object detection
KW  - solid modelling
KW  - stereo image processing
KW  - fast image-based geometric change detection
KW  - multiple images
KW  - self-recorded image sequences
KW  - robotic applications
KW  - 3D model
KW  - 3D location
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Computational modeling
KW  - Cameras
KW  - Robots
KW  - Two dimensional displays
KW  - Uncertainty
DO  - 10.1109/ICRA.2018.8461019
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - 3D models of the environment are used in numerous robotic applications and should reflect the current state of the world. In this paper, we address the problem of quickly finding structural changes between the current state of the world and a given 3D model using a small number of images. Our approach finds inconsistencies between pairs of images by re-projecting an image onto another one by passing through the given 3D model. This process leads to ambiguities, which we resolve by combining multiple images such that the 3D location of the change can be estimated. A focus of our approach is that it can be executed fast enough to allow the operation on a mobile system. We implemented our approach in C++ and released it as open source software. We tested it on existing datasets as well as on self-recorded image sequences and 3D models, which we publicly share. Our experiments show that our method quickly finds changes in the geometry of a scene.
ER  - 

TY  - CONF
TI  - Multimodal 2D Image to 3D Model Registration via a Mutual Alignment of Sparse and Dense Visual Features
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6316
EP  - 6322
AU  - N. Crombez
AU  - R. Seulin
AU  - O. Morel
AU  - D. Fofi
AU  - C. Demonceaux
PY  - 2018
KW  - geometry
KW  - image registration
KW  - solid modelling
KW  - multimodal 2D image
KW  - 3D model registration
KW  - dense visual features
KW  - 2D/3D registration methods
KW  - geometric features
KW  - feature type
KW  - hybrid registration framework
KW  - visual sensors
KW  - 3D model
KW  - Mutual Alignment
KW  - geometric visual features
KW  - sparse visual features
KW  - 2D/3D alignment
KW  - Three-dimensional displays
KW  - Cameras
KW  - Solid modeling
KW  - Visualization
KW  - Feature extraction
KW  - Two dimensional displays
KW  - Sensors
DO  - 10.1109/ICRA.2018.8461092
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Many fields of application could benefit from an accurate registration of measurements of different modalities over a known 3D model. However, aligning a 2D image to a 3D model is a challenging task and is even more complex when the two have a different modality. Most of the 2D/3D registration methods are based on either geometric or dense visual features. Both have their own advantages and their own drawbacks. We propose, in this paper, to mutually exploit the advantages of one feature type to reduce the drawbacks of the other one. For this, an hybrid registration framework has been designed to mutually align geometrical and dense visual features in order to obtain an accurate final 2D/3D alignment. We evaluate and compare the proposed registration method on real data acquired by a robot equipped with several visual sensors. The results highlights the robustness of the method and its ability to produce wide convergence domain and a high registration accuracy.
ER  - 

TY  - CONF
TI  - An Efficient Volumetric Mesh Representation for Real-Time Scene Reconstruction Using Spatial Hashing
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6323
EP  - 6330
AU  - W. Dong
AU  - J. Shi
AU  - W. Tang
AU  - X. Wang
AU  - H. Zha
PY  - 2018
KW  - computational geometry
KW  - data structures
KW  - image reconstruction
KW  - image representation
KW  - mesh generation
KW  - sensor fusion
KW  - solid modelling
KW  - real-time scene reconstruction
KW  - spatial hashing
KW  - flexible data structures
KW  - 3D data fusion
KW  - mesh refinement
KW  - mesh quality
KW  - mesh memory consumption
KW  - volumetric mesh representation
KW  - Hamming distance
KW  - 3D reconstruction
KW  - Three-dimensional displays
KW  - Real-time systems
KW  - Data integration
KW  - Mesh generation
KW  - Rendering (computer graphics)
KW  - Data structures
KW  - Sensors
DO  - 10.1109/ICRA.2018.8463157
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Mesh plays an indispensable role in dense realtime reconstruction essential in robotics. Efforts have been made to maintain flexible data structures for 3D data fusion, yet an efficient incremental framework specifically designed for online mesh storage and manipulation is missing. We propose a novel framework to compactly generate, update, and refine mesh for scene reconstruction upon a volumetric representation. Maintaining a spatial-hashed field of cubes, we distribute vertices with continuous value on discrete edges that support O(1) vertex accessing and forbid memory redundancy. By introducing Hamming distance in mesh refinement, we further improve the mesh quality regarding the triangle type consistency with a low cost. Lock-based and lock-free operations were applied to avoid thread conflicts in GPU parallel computation. Experiments demonstrate that the mesh memory consumption is significantly reduced while the running speed is kept in the online reconstruction process.
ER  - 

TY  - CONF
TI  - Mapping with Dynamic-Object Probabilities Calculated from Single 3D Range Scans
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6331
EP  - 6336
AU  - P. Ruchti
AU  - W. Burgard
PY  - 2018
KW  - control engineering computing
KW  - laser ranging
KW  - mobile robots
KW  - navigation
KW  - neural nets
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - KITTI dataset
KW  - mapping process
KW  - mapping module
KW  - dynamic object
KW  - pointwise probability
KW  - neural network
KW  - laser range data
KW  - 3D grid map
KW  - navigation functions
KW  - robot perceptions
KW  - dynamic environments
KW  - safe navigation
KW  - robust navigation
KW  - autonomous robotic systems
KW  - single 3D range scans
KW  - dynamic-object probabilities
KW  - time 3.0 d
KW  - Three-dimensional displays
KW  - Cameras
KW  - Neural networks
KW  - Measurement by laser beam
KW  - Lasers
KW  - Laser beams
KW  - Image segmentation
DO  - 10.1109/ICRA.2018.8463149
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Various autonomous robotic systems require maps for robust and safe navigation. Particularly when robots are employed in dynamic environments, accurate knowledge about which components of the robot perceptions belong to dynamic and static aspects in the environment can greatly improve navigation functions. In this paper we propose a novel method for building 3D grid maps using laser range data in dynamic environments. Our approach uses a neural network to estimate the pointwise probability of a point belonging to a dynamic object. The output from our network is fed to the mapping module for building a 3D grid map containing only static parts of the environment. We present experimental results obtained by training our neural network using the KITTI dataset and evaluating it in a mapping process using our own dataset. In extensive experiments, we show that maps generated using the proposed probability about dynamic objects increases the accuracy of the resulting maps.
ER  - 

TY  - CONF
TI  - A Survey of Voxel Interpolation Methods and an Evaluation of Their Impact on Volumetric Map-Based Visual Odometry
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3637
EP  - 3643
AU  - D. R. Canelhas
AU  - T. Stoyanov
AU  - A. J. Lilienthal
PY  - 2018
KW  - cameras
KW  - computational geometry
KW  - distance measurement
KW  - image reconstruction
KW  - interpolation
KW  - medical image processing
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - camera trajectories
KW  - trilinear interpolation method
KW  - depth-camera pose tracking
KW  - performance degradation
KW  - truncated signed distance field
KW  - voxel-based map representations
KW  - geometric interpolation methods
KW  - intermediate options
KW  - nearest neighbors
KW  - voxel volumes
KW  - volumetric map-based visual odometry
KW  - voxel interpolation methods
KW  - Interpolation
KW  - Memory management
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Extrapolation
KW  - Two dimensional displays
KW  - Image resolution
DO  - 10.1109/ICRA.2018.8461227
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Voxel volumes are simple to implement and lend themselves to many of the tools and algorithms available for 2D images. However, the additional dimension of voxels may be costly to manage in memory when mapping large spaces at high resolutions. While lowering the resolution and using interpolation is common work-around, in the literature we often find that authors either use trilinear interpolation or nearest neighbors and rarely any of the intermediate options. This paper presents a survey of geometric interpolation methods for voxel-based map representations. In particular we study the truncated signed distance field (TSDF) and the impact of using fewer than 8 samples to perform interpolation within a depth-camera pose tracking and mapping scenario. We find that lowering the number of samples fetched to perform the interpolation results in performance similar to the commonly used trilinear interpolation method, but leads to higher frame-rates. We also report that lower bit-depth generally leads to performance degradation, though not as much as may be expected, with voxels containing as few as 3 bits sometimes resulting in adequate estimation of camera trajectories.
ER  - 

TY  - CONF
TI  - Complex Urban LiDAR Data Set
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6344
EP  - 6351
AU  - J. Jeong
AU  - Y. Cho
AU  - Y. Shin
AU  - H. Roh
AU  - A. Kim
PY  - 2018
KW  - graph theory
KW  - mobile robots
KW  - optical radar
KW  - pose estimation
KW  - radar computing
KW  - SLAM (robots)
KW  - complex urban environments
KW  - light detection and ranging data set
KW  - fiber optic gyro
KW  - inertial measurement unit
KW  - Global Positioning System
KW  - vehicle pose estimation
KW  - graph simultaneous location and mapping algorithm
KW  - graph SLAM algorithm
KW  - Robot Operating System environment
KW  - raw sensor data
KW  - 2D LiDAR
KW  - 16-ray 3D LiDARs
KW  - LiDAR sensors
KW  - three-dimensional LiDAR
KW  - building complexes
KW  - high-rise buildings
KW  - complex urban LiDAR data set
KW  - frequency 100.0 Hz
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Global Positioning System
KW  - Two dimensional displays
KW  - Sensor systems
KW  - Urban areas
DO  - 10.1109/ICRA.2018.8460834
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a Light Detection and Ranging (LiDAR) data set that targets complex urban environments. Urban environments with high-rise buildings and congested traffic pose a significant challenge for many robotics applications. The presented data set is unique in the sense it is able to capture the genuine features of an urban environment (e.g. metropolitan areas, large building complexes and underground parking lots). Data of two-dimensional (2D) and three-dimensional (3D) LiDAR, which are typical types of LiDAR sensors, are provided in the data set. The two 16-ray 3D LiDARs are tilted on both sides for maximal coverage. One 2D LiDAR faces backward while the other faces forwards to collect data of roads and buildings, respectively. Raw sensor data from Fiber Optic Gyro (FOG), Inertial Measurement Unit (IMU), and the Global Positioning System (GPS) are presented in a file format for vehicle pose estimation. The pose information of the vehicle estimated at 100 Hz is also presented after applying the graph simultaneous localization and mapping (SLAM) algorithm. For the convenience of development, the file player and data viewer in Robot Operating System (ROS) environment were also released via the web page. The full data sets are available at: http://irap.kaist.ac.kr/dataset. In this website, 3D preview of each data set is provided using WebGL.
ER  - 

TY  - CONF
TI  - Live Structural Modeling Using RGB-D SLAM
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6352
EP  - 6358
AU  - N. Olivier
AU  - H. Uchiyama
AU  - M. Mishima
AU  - D. Thomas
AU  - R. Taniguchi
AU  - R. Roberto
AU  - J. P. Lima
AU  - V. Teichrieb
PY  - 2018
KW  - image colour analysis
KW  - image fusion
KW  - image texture
KW  - robot vision
KW  - SLAM (robots)
KW  - solid modelling
KW  - live structural modeling
KW  - dense point cloud
KW  - shape map
KW  - single point cloud
KW  - metric primitive modeling
KW  - RGB-D SLAM
KW  - primitive shape localization
KW  - shape fusion
KW  - Shape
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Computational modeling
KW  - History
KW  - Estimation
DO  - 10.1109/ICRA.2018.8460973
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a method for localizing primitive shapes in a dense point cloud computed by the RGB-D SLAM system. To stably generate a shape map containing only primitive shapes, the primitive shape is incrementally modeled by fusing the shapes estimated at previous frames in the SLAM, so that an accurate shape can be finally generated. Specifically, the history of the fusing process is used to avoid the influence of error accumulation in the SLAM. The point cloud of the shape is then updated by fusing the points in all the previous frames into a single point cloud. In the experimental results, we show that metric primitive modeling in texture-less and unprepared environments can be achieved online.
ER  - 

TY  - CONF
TI  - Adaptive Sampling and Online Learning in Multi-Robot Sensor Coverage with Mixture of Gaussian Processes
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6359
EP  - 6364
AU  - W. Luo
AU  - K. Sycara
PY  - 2018
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - optimisation
KW  - online learning
KW  - multirobot sensor coverage
KW  - online environmental sampling
KW  - multirobot coverage control
KW  - environmental phenomenon
KW  - robot team
KW  - Gaussian Process
KW  - locally learned Gaussian Processes
KW  - collective model learning
KW  - simultaneous adaptive sampling
KW  - density function
KW  - sensing performance optimization
KW  - Robot sensing systems
KW  - Adaptation models
KW  - Density functional theory
KW  - Temperature distribution
KW  - Data models
DO  - 10.1109/ICRA.2018.8460473
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We consider the problem of online environmental sampling and modeling for multi-robot sensor coverage, where a team of robots spread out over the workspace in order to optimize the overall sensing performance. In contrast to most existing works on multi-robot coverage control that assume prior knowledge of the distribution of environmental phenomenon, also known as density function, we relax this assumption and enable the robot team to efficiently learn the model of the unknown density function Online using adaptive sampling and non-parametric inference such as Gaussian Process (GP). To capture significantly different components of the environmental phenomenon, we propose a new approach with mixture of locally learned Gaussian Processes for collective model learning and an information-theoretic criterion for simultaneous adaptive sampling in multi-robot coverage. Our approach demonstrates a better generalization of the environment modeling and thus the improved performance of coverage without assuming the density function is known a priori. We demonstrate the effectiveness of our algorithm via simulations of information gathering from indoor static sensors.
ER  - 

TY  - CONF
TI  - Coordinated Dense Aerial Traffic with Self-Driving Drones
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6365
EP  - 6372
AU  - B. Balázs
AU  - G. Vásárhelyi
PY  - 2018
KW  - air traffic control
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - decentralised control
KW  - multi-robot systems
KW  - coordinated dense aerial traffic
KW  - general air traffic control solution
KW  - decentralized air traffic control solution
KW  - package-delivery scenarios
KW  - intelligent collective collision avoidance
KW  - motion planning
KW  - jam-free optimal traffic flow
KW  - force-based distributed multirobot control model
KW  - behaviour-driven velocity alignment
KW  - self-organized queueing
KW  - conflict-avoiding self-driving
KW  - Drones
KW  - Mathematical model
KW  - Atmospheric modeling
KW  - Oscillators
KW  - Acceleration
KW  - Task analysis
KW  - Roads
DO  - 10.1109/ICRA.2018.8461073
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper we present a general, decentralized air traffic control solution using autonomous drones. We challenge some of the most difficult dense traffic situations, namely, crosswalk and package-delivery scenarios, where intelligent collective collision avoidance and motion planning is essential for a jam-free optimal traffic flow. We build up a force-based distributed multi-robot control model using a tunable selection of interaction terms: anisotropic repulsion, behaviour-driven velocity alignment, self-organized queueing and conflict-avoiding self-driving. We optimize the model with evolution in a realistic simulation framework and demonstrate its applicability with 30 autonomous drones in a coordinated outdoor flight within a densely packed virtual arena.
ER  - 

TY  - CONF
TI  - Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6373
EP  - 6380
AU  - T. N. Hoang
AU  - Y. Xiao
AU  - K. Sivakumar
AU  - C. Amato
AU  - J. P. Howl
PY  - 2018
KW  - decision making
KW  - multi-agent systems
KW  - multi-robot systems
KW  - planning (artificial intelligence)
KW  - multirobot
KW  - small-scale synchronous decision-making scenarios
KW  - asynchronous agents
KW  - multiple strategic adversaries
KW  - adversary strategies
KW  - optimized stratagems
KW  - unified policy
KW  - near-optimality
KW  - optimal adversarial policy switching
KW  - decentralized asynchronous multiagent systems
KW  - communication capabilities
KW  - Switches
KW  - Planning
KW  - Task analysis
KW  - Robot kinematics
KW  - Probabilistic logic
DO  - 10.1109/ICRA.2018.8460485
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - A key challenge in multi-robot and multi-agent systems is generating solutions that are robust to other self-interested or even adversarial parties who actively try to prevent the agents from achieving their goals. The practicality of existing works addressing this challenge is limited to only small-scale synchronous decision-making scenarios or a single agent planning its best response against a single adversary with fixed, procedurally characterized strategies. In contrast this paper considers a more realistic class of problems where a team of asynchronous agents with limited observation and communication capabilities need to compete against multiple strategic adversaries with changing strategies. This problem necessitates agents that can coordinate to detect changes in adversary strategies and plan the best response accordingly. Our approach first optimizes a set of stratagems that represent these best responses. These optimized stratagems are then integrated into a unified policy that can detect and respond when the adversaries change their strategies. The near-optimality of the proposed framework is established theoretically as well as demonstrated empirically in simulation and hardware.
ER  - 

TY  - CONF
TI  - Data Ferrying with Swarming UAS in Tactical Defence Networks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6381
EP  - 6388
AU  - R. Hunjet
AU  - B. Fraser
AU  - T. Stevens
AU  - L. Hodges
AU  - K. Mayen
AU  - J. C. Barca
AU  - M. Cochrane
AU  - R. Cannizzaro
AU  - J. L. Palmer
PY  - 2018
KW  - mobile robots
KW  - multi-robot systems
KW  - harsh communication environments
KW  - UAS
KW  - human-swarm interaction
KW  - data dissemination capabilities
KW  - indoor flight facilities
KW  - physical swarm robotic platforms
KW  - radio-frequency communications
KW  - swarm members
KW  - tactical defence networks
KW  - Emulation
KW  - Convergence
KW  - Australia
KW  - Radio frequency
KW  - Data models
KW  - Robot kinematics
DO  - 10.1109/ICRA.2018.8463151
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper we categorise swarming into four classes, depending on the manner in which swarm members communicate. We identify two of these classes as ready candidates for the provision of communications within tactical defence networks in which radio-frequency communications are highly contested or denied. We demonstrate the feasibility of a swarm-robotics approach to data ferrying from both of these classes using simulation, emulation, and physical swarm robotic platforms within indoor flight facilities. The results show strong alignment between data dissemination capabilities of the simulated and physical systems; we envisage these techniques providing communications between not only troops, but also other swarm robotic platforms, thereby enabling swarm robotics applications and human-swarm interaction within harsh communications environments.
ER  - 

TY  - CONF
TI  - Distance-Based Multi-Robot Coordination on Pocket Drones
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6389
EP  - 6394
AU  - B. Broecker
AU  - K. Tuyls
AU  - J. Butterworth
PY  - 2018
KW  - learning (artificial intelligence)
KW  - particle filtering (numerical methods)
KW  - recurrent neural nets
KW  - remotely operated vehicles
KW  - Deep Q-Learning Network
KW  - recurrent network
KW  - UWB-distance information
KW  - neural networks
KW  - distance-based multirobot coordination
KW  - pocket drones
KW  - MicroAerial Vehicles
KW  - recurrent neural network
KW  - Drones
KW  - Robot kinematics
KW  - Recurrent neural networks
KW  - Hardware
KW  - Robot sensing systems
KW  - Distance measurement
DO  - 10.1109/ICRA.2018.8461176
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - We present a fully realised system illustrating decentralised coordination on Micro Aerial Vehicles (MAV) or pocket drones, based on distance information. This entails the development of an ultra light hardware solution to determine the distances between the drones and also the development of a model to learn good control policies. The model we present is a combination of a recurrent neural network and a Deep Q-Learning Network (DQN). The recurrent network provides bearing information to the DQN. The DQN itself is responsible for choosing movement actions to avoid collisions and to reach a desired position. Overall we are able provide a complete system which is capable of letting multiple drones navigate in a confined space only based on UWB-distance information and velocity input. We tackle the problem of neural networks and real world sensor noise, by combining the network with a particle filter and show that the combination outperforms the traditional particle filter in terms of converge speed and robustness. A video is available at: https://youtu.be/yj6QqhOzpok.
ER  - 

TY  - CONF
TI  - Human-in-the-Loop Mixed-Initiative Control Under Temporal Tasks
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6395
EP  - 6400
AU  - M. Guo
AU  - S. Andersson
AU  - D. V. Dimarogonas
PY  - 2018
KW  - control engineering computing
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - planning (artificial intelligence)
KW  - temporal logic
KW  - plan adaptation scheme
KW  - short-term tasks
KW  - iterative inverse reinforcement learning algorithm
KW  - human preference
KW  - plan synthesis
KW  - human-in-the-loop simulations
KW  - mixed-initiative control
KW  - motion control
KW  - task planning problem
KW  - mobile robots
KW  - high-level tasks
KW  - Linear Temporal Logic
KW  - hard constraints
KW  - soft constraints
KW  - robot autonomy
KW  - additive terms
KW  - contingent task assignments
KW  - online coordination scheme
KW  - mixed-initiative continuous controller
KW  - temporal tasks
KW  - human initiatives
KW  - Task analysis
KW  - Safety
KW  - Robot kinematics
KW  - Automata
KW  - Planning
KW  - Navigation
DO  - 10.1109/ICRA.2018.8460793
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper considers the motion control and task planning problem of mobile robots under complex high-level tasks and human initiatives. The assigned task is specified as Linear Temporal Logic (LTL) formulas that consist of hard and soft constraints. The human initiative influences the robot autonomy in two explicit ways: with additive terms in the continuous controller and with contingent task assignments. We propose an online coordination scheme that encapsulates (i) a mixed-initiative continuous controller that ensures all-time safety despite of possible human errors, (ii) a plan adaptation scheme that accommodates new features discovered in the workspace and short-term tasks assigned by the operator during run time, and (iii) an iterative inverse reinforcement learning (IRL) algorithm that allows the robot to asymptotically learn the human preference on the parameters during the plan synthesis. The results are demonstrated by both realistic human-in-the-loop simulations and experiments.
ER  - 

TY  - CONF
TI  - Cooperative Adaptive Control for Cloud-Based Robotics
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6401
EP  - 6408
AU  - P. M. Wensing
AU  - J. Slotine
PY  - 2018
KW  - adaptive control
KW  - cloud computing
KW  - control engineering computing
KW  - control system synthesis
KW  - decentralised control
KW  - Lyapunov methods
KW  - manipulators
KW  - multi-robot systems
KW  - adaptive control
KW  - robot manipulators
KW  - synchronous centralized update laws
KW  - parameter convergence
KW  - time-varying network topologies
KW  - nonidealized networked conditions
KW  - planar manipulator
KW  - cloud-based robotics
KW  - inertial parameters
KW  - collective sufficient richness notion
KW  - decentralized update laws
KW  - Adaptive control
KW  - Manipulators
KW  - Convergence
KW  - Robot sensing systems
KW  - Cloud computing
KW  - Trajectory
DO  - 10.1109/ICRA.2018.8460856
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper studies collaboration through the cloud in the context of cooperative adaptive control for robot manipulators. We first consider the case of multiple robots manipulating a common object through synchronous centralized update laws to identify unknown inertial parameters. Through this development, we introduce a notion of Collective Sufficient Richness, wherein parameter convergence can be enabled through teamwork in the group. The introduction of this property and the analysis of stable adaptive controllers that benefit from it constitute the main new contributions of this work. Building on this original example, we then consider decentralized update laws, time-varying network topologies, and the influence of communication delays on this process. Perhaps surprisingly, these nonidealized networked conditions inherit the same benefits of convergence being determined through collective effects for the group. Simple simulations of a planar manipulator identifying an unknown load are provided to illustrate the central idea and benefits of Collective Sufficient Richness.
ER  - 

TY  - CONF
TI  - Optimized Environment Exploration for Autonomous Underwater Vehicles
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6409
EP  - 6416
AU  - E. Vidal
AU  - J. D. Hernández
AU  - K. Istenic
AU  - M. Carreras
PY  - 2018
KW  - autonomous underwater vehicles
KW  - image reconstruction
KW  - mobile robots
KW  - path planning
KW  - quadtrees
KW  - tree data structures
KW  - viewpoint generation process
KW  - consistent maps
KW  - noisy sonar data
KW  - optimized environment exploration
KW  - autonomous underwater
KW  - autonomous robotic environment exploration
KW  - underwater domain
KW  - noisy acoustic sensors
KW  - high localization error
KW  - control disturbances
KW  - robotic exploration algorithm
KW  - underwater vehicles
KW  - view planning
KW  - path planning algorithms
KW  - exploration approach
KW  - quadtree data structure
KW  - relevant queries
KW  - natural environments
KW  - underwater maps
KW  - map representation
KW  - optical coverage
KW  - time 3.0 d
KW  - Cameras
KW  - Sonar
KW  - Planning
KW  - Robot sensing systems
KW  - Inspection
KW  - Three-dimensional displays
DO  - 10.1109/ICRA.2018.8460919
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Achieving full autonomous robotic environment exploration in the underwater domain is very challenging, mainly due to noisy acoustic sensors, high localization error, control disturbances of the water and lack of accurate underwater maps. In this work we present a robotic exploration algorithm for underwater vehicles that does not rely on prior information about the environment. Our method has been greatly influenced by many robotic exploration, view planning and path planning algorithms. The proposed method constitutes a significant improvement over our previous work [1]: Firstly, we refine our exploration approach to improve robustness; Secondly, we propose an alternative map representation based on the quadtree data structure that allows different relevant queries to be performed efficiently, reducing the computational cost of the viewpoint generation process; Thirdly, we present an algorithm that is capable of generating consistent maps even when noisy sonar data is used. The aforementioned contributions have increased the reliability of the algorithm, allowing new real experiments performed in artificial structures but also in more challenging natural environments, from which we provide a 3D reconstruction to show that with this algorithm full optical coverage is obtained.
ER  - 

TY  - CONF
TI  - Pilot Surveys for Adaptive Informative Sampling
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6417
EP  - 6424
AU  - S. Kemna
AU  - O. Kroemer
AU  - G. S. Sukhatme
PY  - 2018
KW  - aerospace control
KW  - environmental factors
KW  - Gaussian processes
KW  - mobile robots
KW  - path planning
KW  - regression analysis
KW  - sampling methods
KW  - sampling trajectory
KW  - Gaussian Process regression
KW  - pilot surveys
KW  - adaptive informative sampling
KW  - GP hyperparameter estimation
KW  - path planning decisions
KW  - environmental field modeling
KW  - informative samples
KW  - adaptive sampling techniques
KW  - GP regression
KW  - Adaptation models
KW  - Data models
KW  - Kernel
KW  - Robots
KW  - Gaussian processes
KW  - Estimation
KW  - Optimization
DO  - 10.1109/ICRA.2018.8460488
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Adaptive sampling has been shown to be an effective method for modeling environmental fields, such as algae concentrations in the ocean. In adaptive sampling, a robot adapts its sampling trajectory based on data that it is collecting. This data is often aggregated into models, using techniques such as Gaussian Process (G P) regression. The (hyper-)parameters for these models need to be manually set or, ideally, estimated from data. For GP regression, hyperparameters are typically estimated using prior data. This paper addresses the case where initial hyperparameters need to be estimated, but no prior data is available. Without prior data or accurately pre-defined hyperparameters, adaptive sampling techniques may fail, because there is no good model to base path planning decisions on. One method of gathering data is to perform a pilot survey. This survey needs to select informative samples for initiating the model, but without having a model to determine where best to sample. In this work, we evaluate four pilot surveys, which use a softmax function on the distance between waypoints and previously sampled data for waypoint selection. Simulation results show that pilot surveys that maximize waypoint spread over randomization lead to more stable estimation of GP hyperparameters, and create accurate models more quickly.
ER  - 

TY  - CONF
TI  - Gaze-Assisted Adaptive Motion Scaling Optimization Using Graded and Preference Based Bayesian Approaches
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6425
EP  - 6430
AU  - G. Gras
AU  - C. A. Seneci
AU  - P. Giataganas
AU  - G. Yang
PY  - 2018
KW  - adaptive control
KW  - Bayes methods
KW  - human-robot interaction
KW  - medical robotics
KW  - optimisation
KW  - surgery
KW  - telerobotics
KW  - gaze-assisted adaptive motion scaling optimization
KW  - Bayesian approaches
KW  - master-slave surgical systems
KW  - slave robot
KW  - gaze-assisted intention recognition scheme
KW  - Bayesian approach
KW  - human-robot interface
KW  - Bayesian optimization methods
KW  - Robots
KW  - Bayes methods
KW  - Instruments
KW  - Linear programming
KW  - Optimization methods
KW  - Master-slave
DO  - 10.1109/ICRA.2018.8460588
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - A key component to the success of master-slave surgical systems is the quality of the master interface used to relay the surgeon's instructions to the slave robot. In previous work the authors developed a gaze-assisted intention recognition scheme, allowing the system to dynamically adapt the motion scaling based on where the user is trying to reach. This allowed users to perform tasks significantly more quickly and with less need for clutching. However, the system possessed a number of core parameters that were manually optimized, potentially providing a non-optimal solution depending on the user. This paper presents a Bayesian approach to the problem of optimizing a human-robot interface in a user-specific manner. Two Bayesian optimization methods are studied: one in which users are asked to grade robot behavior for a given set of parameters, and one where only preference relative to other parameter sets is expressed. The performance of these optimizations is evaluated in a blind comparison user study, demonstrating that the optimized parameters are preferred to the manually optimized ones in over 90 % of cases after only 12 test samples. These parameters are further shown to perform at least as well as the manually optimized ones in all cases, and showing statistically significant improvement in the case of the graded optimization.
ER  - 

TY  - CONF
TI  - Learning to Race Through Coordinate Descent Bayesian Optimisation
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6431
EP  - 6438
AU  - R. Oliveira
AU  - F. H. M. Rocha
AU  - L. Ott
AU  - V. Guizilini
AU  - F. Ramos
AU  - V. Grassi
PY  - 2018
KW  - automobiles
KW  - Bayes methods
KW  - Hilbert spaces
KW  - mobile robots
KW  - optimisation
KW  - robot dynamics
KW  - vehicle dynamics
KW  - dynamical model
KW  - robot
KW  - car racing simulation
KW  - descent Bayesian optimisation
KW  - race track
KW  - kernel Hilbert space
KW  - Bayesian optimisation
KW  - Optimization
KW  - Robot kinematics
KW  - Bayes methods
KW  - Search problems
KW  - Kernel
KW  - Linear programming
DO  - 10.1109/ICRA.2018.8460735
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In the automation of many kinds of processes, the observable outcome can often be described as the combined effect of an entire sequence of actions, or controls, applied throughout the process execution. In these cases, strategies to optimise control policies for individual stages of the process are not applicable, and instead the whole policy needs to be optimised at once. On the other hand, the cost to evaluate the policy's performance might also be high, being desirable that a solution can be found with as few interactions as possible with the real system. We consider the problem of optimising control policies to allow a robot to complete a given race track within a minimum amount of time. We assume that the robot has no prior information about the track or its own dynamical model, just an initial valid driving example. Localisation is only applied to monitor the robot and to provide an indication of its position along the track's centre axis. With that in mind, we propose a method for finding a policy that minimises the time per lap while keeping the vehicle on the track using a Bayesian optimisation (BO) approach over a reproducing kernel Hilbert space. We apply an algorithm to search more efficiently over high-dimensional policy-parameter spaces with BO, by iterating over each dimension individually, in a sequential coordinate descent-like scheme. Experiments demonstrate the performance of the algorithm against other methods in a simulated car racing environment.
ER  - 

TY  - CONF
TI  - Towards Emergence of Tool Use in Robots: Automatic Tool Recognition and Use Without Prior Tool Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6439
EP  - 6446
AU  - K. P. Tee
AU  - J. Li
AU  - L. T. Pang Chen
AU  - K. W. Wan
AU  - G. Ganesh
PY  - 2018
KW  - cognition
KW  - object recognition
KW  - robot vision
KW  - automatic tool recognition
KW  - human dexterity
KW  - skill transfer
KW  - robots cognition
KW  - robots capabilities
KW  - tools embodiment
KW  - object recognition
KW  - Tools
KW  - Task analysis
KW  - Kinematics
KW  - Automobiles
KW  - Robot sensing systems
KW  - Dynamics
DO  - 10.1109/ICRA.2018.8460987
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Humans are adept at tool use. We can intuitively and immediately improvise and use unknown objects in our environment as tools, to assist us in performing tasks. In this study, we provide similar cognition and capabilities to robots. Neuroscientific studies on tool use have suggested that human dexterity with tools is enabled by the embodiment of the tools, which in effect, allows humans to immediately transfer prior skills acquired without tools, onto tasks requiring tool use. Here, utilizing the theoretical results from our investigations on embodiment and tool use in humans over the last years, we propose a concept and algorithm to enable similar skill transfer by robots. Our algorithm enables a robot that has had no prior learning with tools, to automatically recognize an object (seen for the first time) in its environment as a potential tool for an otherwise unattainable task, and use the tool to perform the task thereafter.
ER  - 

TY  - CONF
TI  - Put-in-Box Task Generated from Multiple Discrete Tasks by aHumanoid Robot Using Deep Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6447
EP  - 6452
AU  - K. Kase
AU  - K. Suzuki
AU  - P. Yang
AU  - H. Mori
AU  - T. Ogata
PY  - 2018
KW  - feature extraction
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - recurrent neural nets
KW  - multiple discrete tasks
KW  - deep learning
KW  - deep neural networks
KW  - robot manipulation model
KW  - DNNs
KW  - long sequential dynamic tasks
KW  - multiple short sequential tasks
KW  - multiple timescale recurrent neural network
KW  - MTRNN
KW  - initial motion steps
KW  - final motion steps
KW  - initial image input
KW  - subtask
KW  - put-in-box task
KW  - Task analysis
KW  - Robots
KW  - Feature extraction
KW  - Switches
KW  - Neurons
KW  - Training
KW  - Convolution
DO  - 10.1109/ICRA.2018.8460623
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - For robots to have a wide range of applications, they must be able to execute numerous tasks. However, recent studies into robot manipulation using deep neural networks (DNN) have primarily focused on single tasks. Therefore, we investigate a robot manipulation model that uses DNNs and can execute long sequential dynamic tasks by performing multiple short sequential tasks at appropriate times. To generate compound tasks, we propose a model comprising two DNNs: a convolutional autoencoder that extracts image features and a multiple timescale recurrent neural network (MTRNN) to generate motion. The internal state of the MTRNN is constrained to have similar values at the initial and final motion steps; thus, motions can be differentiated based on the initial image input. As an example compound task, we demonstrate that the robot can generate a “Put-In-Box” task that is divided into three subtasks: open the box, grasp the object and put it into the box, and close the box. The subtasks were trained as discrete tasks, and the connections between each subtask were not trained. With the proposed model, the robot could perform the Put-In-Box task by switching among subtasks and could skip or repeat subtasks depending on the situation.
ER  - 

TY  - CONF
TI  - CASSL: Curriculum Accelerated Self-Supervised Learning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6453
EP  - 6460
AU  - A. Murali
AU  - L. Pinto
AU  - D. Gandhi
AU  - A. Gupta
PY  - 2018
KW  - grippers
KW  - learning (artificial intelligence)
KW  - sensitivity analysis
KW  - adaptive-underactuated multifingered gripper
KW  - curriculum accelerated self-supervised learning
KW  - variance-based global sensitivity analysis
KW  - control parameters
KW  - control dimensions
KW  - training data
KW  - CASSL orders
KW  - higher-dimensional action
KW  - map visual information
KW  - clever sampling strategy
KW  - data collection efforts
KW  - higher-dimensional control
KW  - low-dimensional action
KW  - self-supervised learning approaches
KW  - complete end-to-end learning
KW  - staged curriculum learning
KW  - CASSL framework
KW  - Aerospace electronics
KW  - Grasping
KW  - Training
KW  - Task analysis
KW  - Robots
KW  - Sensitivity analysis
DO  - 10.1109/ICRA.2018.8463147
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Recent self-supervised learning approaches focus on using a few thousand data points to learn policies for high-level, low-dimensional action spaces. However, scaling this framework for higher-dimensional control requires either scaling up the data collection efforts or using a clever sampling strategy for training. We present a novel approach - Curriculum Accelerated Self-Supervised Learning (CASSL) - to train policies that map visual information to high-level, higher-dimensional action spaces. CASSL orders the sampling of training data based on control dimensions: the learning and sampling are focused on few control parameters before other parameters. The right curriculum for learning is suggested by variance-based global sensitivity analysis of the control space. We apply our CASSL framework to learning how to grasp using an adaptive, underactuated multi-fingered gripper, a challenging system to control. Our experimental results indicate that CASSL provides significant improvement and generalization compared to baseline methods such as staged curriculum learning (8% increase) and complete end-to-end learning with random exploration (14% improvement) tested on a set of novel objects.
ER  - 

TY  - CONF
TI  - Learning to Control Redundant Musculoskeletal Systems with Neural Networks and SQP: Exploiting Muscle Properties
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6461
EP  - 6468
AU  - D. Driess
AU  - H. Zimmermann
AU  - S. Wolfen
AU  - D. Suissa
AU  - D. Haeufle
AU  - D. Hennes
AU  - M. Toussaint
AU  - S. Schmitt
PY  - 2018
KW  - biomechanics
KW  - biomimetics
KW  - bone
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - muscle
KW  - neural nets
KW  - nonlinear control systems
KW  - physiological models
KW  - quadratic programming
KW  - machine learning approaches
KW  - muscle stimulations
KW  - high actuator redundancy
KW  - learned forward model
KW  - neural network
KW  - biomimetic muscle-driven robot show
KW  - nonlinearity
KW  - biomechanical musculoskeletal systems
KW  - quadratic programming
KW  - SQP
KW  - Muscles
KW  - Joints
KW  - Robots
KW  - Biological system modeling
KW  - Torque
KW  - Biomechanics
DO  - 10.1109/ICRA.2018.8463160
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Modeling biomechanical musculoskeletal systems reveals that the mapping from muscle stimulations to movement dynamics is highly nonlinear and complex, which makes it difficult to control those systems with classical techniques. In this work, we not only investigate whether machine learning approaches are capable of learning a controller for such systems. We are especially interested in the question if the structure of the musculoskeletal apparatus exhibits properties that are favorable for the learning task. In particular, we consider learning a control policy from target positions to muscle stimulations. To account for the high actuator redundancy of biomechanical systems, our approach uses a learned forward model represented by a neural network and sequential quadratic programming to obtain the control policy, which also enables us to alternate the co-contraction level and hence allows to change the stiffness of the system and to include optimality criteria like small muscle stimulations. Experiments on both a simulated musculoskeletal model of a human arm and a real biomimetic muscle-driven robot show that our approach is able to learn an accurate controller despite high redundancy and nonlinearity, while retaining sample efficiency.
ER  - 

TY  - CONF
TI  - Q-CP: Learning Action Values for Cooperative Planning
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6469
EP  - 6475
AU  - F. Riccio
AU  - R. Capobianco
AU  - D. Nardi
PY  - 2018
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mobile robots
KW  - Monte Carlo methods
KW  - multi-robot systems
KW  - navigation
KW  - path planning
KW  - state-space methods
KW  - stochastic games
KW  - tree searching
KW  - uncertain systems
KW  - multirobot systems
KW  - manifold applications
KW  - unstructured scenarios
KW  - state dimensionality
KW  - model-based reinforcement learning algorithm
KW  - Q-learning
KW  - curse-of-dimensionality
KW  - cooperation scenario
KW  - mobile robots
KW  - robot behaviors
KW  - uncertainties
KW  - state space exploration
KW  - action values learning
KW  - stochastic cooperative games
KW  - cooperative navigation problem
KW  - cooperative planning
KW  - Monte-Carlo tree search iterations
KW  - general-sum games
KW  - KUKA YouBots
KW  - robot hand-overs
KW  - coordination task
KW  - Robot kinematics
KW  - Games
KW  - Monte Carlo methods
KW  - Task analysis
KW  - Planning
KW  - Stochastic processes
DO  - 10.1109/ICRA.2018.8460180
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Research on multi-robot systems has demonstrated promising results in manifold applications and domains. Still, efficiently learning an effective robot behaviors is very difficult, due to unstructured scenarios, high uncertainties, and large state dimensionality (e.g, hyper-redundant and groups of robot). To alleviate this problem, we present Q-CP a cooperative model-based reinforcement learning algorithm, which exploits action values to both (1) guide the exploration of the state space and (2) generate effective policies. Specifically, we exploit Q-learning to attack the curse-of-dimensionality in the iterations of a Monte-Carlo Tree Search. We implement and evaluate Q-CP on different stochastic cooperative (general-sum) games: (1) a simple cooperative navigation problem among 3 robots, (2) a cooperation scenario between a pair of KUKA YouBots performing hand-overs, and (3) a coordination task between two mobile robots entering a door. The obtained results show the effectiveness of Q- CP in the chosen applications, where action values drive the exploration and reduce the computational demand of the planning process while achieving good performance.
ER  - 

TY  - CONF
TI  - Long-Term Visual Localization Using Semantically Segmented Images
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6484
EP  - 6490
AU  - E. Stenborg
AU  - C. Toft
AU  - L. Hammarstrand
PY  - 2018
KW  - feature extraction
KW  - image segmentation
KW  - particle filtering (numerical methods)
KW  - transforms
KW  - particle filter based semantic localization solution
KW  - SIFT-features
KW  - vehicle localization
KW  - semantically labeled 3D point maps
KW  - autonomous vehicles
KW  - long-term visual navigation
KW  - robust cross-seasonal localization
KW  - semantically segmented images
KW  - long-term visual localization
KW  - image segmenter
KW  - hand-crafted feature descriptors
KW  - Semantics
KW  - Cameras
KW  - Roads
KW  - Image segmentation
KW  - Visualization
KW  - Robustness
KW  - Feature extraction
DO  - 10.1109/ICRA.2018.8463150
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.), Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.
ER  - 

TY  - CONF
TI  - Robust Localization of Mobile Robots Considering Reliability of LiDAR Measurements
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6491
EP  - 6496
AU  - J. Kim
AU  - W. Chung
PY  - 2018
KW  - mobile robots
KW  - optical radar
KW  - optical sensors
KW  - pose estimation
KW  - reliability
KW  - robot vision
KW  - mobile robot
KW  - LiDAR measurements
KW  - localization errors
KW  - LiDAR sensor-based localization
KW  - optical sensors
KW  - robust localization
KW  - range measurements
KW  - reliability
KW  - Light Detection and Ranging sensor
KW  - pose estimation
KW  - Reliability
KW  - Laser radar
KW  - Robot sensing systems
KW  - Optical sensors
KW  - Glass
KW  - Adaptive optics
KW  - Optical variables measurement
DO  - 10.1109/ICRA.2018.8460648
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this study, we propose a novel Light Detection and Ranging (LiDAR) sensor-based localization method for localization of a mobile robot. In localization using the LiDAR sensor, localization errors occur when real range measurements differ from reference distances computed from a map. This study focuses on three factors that cause differences between real range measurements and reference distances. The first factor corresponds to optical characteristics of the LiDAR sensor for objects such as glass walls and mirrors. The second factor corresponds to occlusions by dynamic obstacles. The third factor corresponds to static changes in the environment. In practical applications, three factors often simultaneously occur. Although there have been many previous works, robust localization to overcome these three difficulties is still a challenging problem. This study proposes a novel robust localization scheme that exploits only reliable range measurements. A LiDAR sensor-based localization scheme can be successfully executed by utilizing only a few reliable range measurements. Therefore, the computation of reliability plays a significant role. The computation of reliability is divided into two steps. The first step considers characteristics of optical sensors. The second step mainly deals with the effects of obstacles. The observation likelihood model exploits computed reliability for pose estimation. The proposed scheme was successfully verified through various experiments under challenging situations.
ER  - 

TY  - CONF
TI  - Sparse Gaussian Processes on Matrix Lie Groups: A Unified Framework for Optimizing Continuous-Time Trajectories
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6497
EP  - 6504
AU  - J. Dong
AU  - M. Mukadam
AU  - B. Boots
AU  - F. Dellaert
PY  - 2018
KW  - continuous time systems
KW  - Gaussian processes
KW  - Lie groups
KW  - matrix algebra
KW  - mobile robots
KW  - motion control
KW  - optimisation
KW  - path planning
KW  - regression analysis
KW  - state estimation
KW  - trajectory control
KW  - nonparametric representation
KW  - trajectory distributions
KW  - sparse GP regression
KW  - robot state
KW  - locally linear GPs
KW  - state estimation
KW  - motion planning tasks
KW  - sparse Gaussian processes
KW  - continuous-time trajectories
KW  - trajectory optimization
KW  - matrix Lie groups
KW  - robot motion reasoning
KW  - Trajectory
KW  - Simultaneous localization and mapping
KW  - Estimation
KW  - Planning
KW  - Sparse matrices
KW  - Gaussian processes
DO  - 10.1109/ICRA.2018.8461077
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Continuous-time trajectories are useful for reasoning about robot motion in a wide range of tasks. Sparse Gaussian processes (GPs) can be used as a non-parametric representation for trajectory distributions that enables fast trajectory optimization by sparse GP regression. However, most previous approaches that utilize sparse GPs for trajectory optimization are limited by the fact that the robot state is represented in vector space. In this paper, we first extend previous works to consider the state on general matrix Lie groups by applying a constant-velocity prior and defining locally linear GPs. Next, we discuss how sparse GPs on Lie groups provide a unified continuous-time framework for trajectory optimization for solving a number of robotics problems including state estimation and motion planning. Finally, we demonstrate and evaluate our approach on several different estimation and motion planning tasks with both synthetic and real-world experiments.
ER  - 

TY  - CONF
TI  - Complexity Analysis and Efficient Measurement Selection Primitives for High-Rate Graph SLAM
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6505
EP  - 6512
AU  - K. M. Frey
AU  - T. J. Steiner
AU  - J. P. How
PY  - 2018
KW  - computational complexity
KW  - graph theory
KW  - iterative methods
KW  - Newton method
KW  - optimisation
KW  - SLAM (robots)
KW  - complexity analysis
KW  - globally-efficient structure
KW  - favorable global structures
KW  - Gauss-Newton iteration
KW  - factorization step
KW  - primary computational bottleneck
KW  - graph structure
KW  - existing analytic gap
KW  - quantitative metric called elimination complexity
KW  - significant computation reductions
KW  - measurement decimation
KW  - simple heuristics
KW  - aggressive pruning
KW  - significant computational savings
KW  - structurally-naïve techniques
KW  - global level
KW  - edge count
KW  - SLAM graph
KW  - graph-based SLAM
KW  - high-rate graph
KW  - efficient measurement selection primitives
KW  - Simultaneous localization and mapping
KW  - Complexity theory
KW  - Optimization
KW  - Sparse matrices
KW  - Extraterrestrial measurements
KW  - Linear algebra
DO  - 10.1109/ICRA.2018.8460708
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Sparsity has been widely recognized as crucial for efficient optimization in graph-based SLAM. Because the sparsity and structure of the SLAM graph reflect the set of incorporated measurements, many methods for sparsification have been proposed in hopes of reducing computation. These methods often focus narrowly on reducing edge count without regard for structure at a global level. Such structurally-naïve techniques can fail to produce significant computational savings, even after aggressive pruning. In contrast, simple heuristics such as measurement decimation and keyframing are known empirically to produce significant computation reductions. To demonstrate why, we propose a quantitative metric called elimination complexity (EC) that bridges the existing analytic gap between graph structure and computation. EC quantifies the complexity of the primary computational bottleneck: the factorization step of a Gauss-Newton iteration. Using this metric, we show rigorously that decimation and keyframing impose favorable global structures and therefore achieve computation reductions on the order of r2/9 and r3, respectively, where r is the pruning rate. We additionally present numerical results showing EC provides a good approximation of computation in both batch and incremental (iSAM2) optimization and demonstrate that pruning methods promoting globally-efficient structure outperform those that do not.
ER  - 

TY  - CONF
TI  - Dense Planar-Inertial SLAM with Structural Constraints
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6521
EP  - 6528
AU  - M. Hsiao
AU  - E. Westman
AU  - M. Kaess
PY  - 2018
KW  - distance measurement
KW  - image reconstruction
KW  - least squares approximations
KW  - optimisation
KW  - robot vision
KW  - SLAM (robots)
KW  - dense visual odometry estimation
KW  - planar measurements
KW  - SLAM framework
KW  - IMU biases
KW  - planar landmarks
KW  - incremental smoothing
KW  - Bayes Tree
KW  - IMU data
KW  - visual information
KW  - modeling planes
KW  - IMU states
KW  - reconstruction results
KW  - SLAM algorithms
KW  - structural constraints
KW  - DPI-SLAM system
KW  - planar-inertial SLAM system
KW  - novel dense planar-inertial SLAM
KW  - dense 3D models
KW  - indoor environments
KW  - hand-held RGB-D sensor
KW  - inertial measurement unit
KW  - preinte-grated IMU measurements
KW  - factor graph
KW  - incremental mapping
KW  - probabilistic global optimization
KW  - Simultaneous localization and mapping
KW  - Optimization
KW  - Three-dimensional displays
KW  - Real-time systems
KW  - Estimation
KW  - Visualization
DO  - 10.1109/ICRA.2018.8461094
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this work, we develop a novel dense planar-inertial SLAM (DPI-SLAM) system to reconstruct dense 3D models of large indoor environments using a hand-held RGB-D sensor and an inertial measurement unit (IMU). The preinte-grated IMU measurements are loosely-coupled with the dense visual odometry (VO) estimation and tightly-coupled with the planar measurements in a full SLAM framework. The poses, velocities, and IMU biases are optimized together with the planar landmarks in a global factor graph using incremental smoothing and mapping with the Bayes Tree (iSAM2). With odometry estimation using both RGB-D and IMU data, our system can keep track of the poses of the sensors even without sufficient planes or visual information (e.g. textureless walls) temporarily. Modeling planes and IMU states in the fully probabilistic global optimization reduces the drift that distorts the reconstruction results of other SLAM algorithms. Moreover, structural constraints between nearby planes (e.g. right angles) are added into the DPI-SLAM system, which further recovers the drift and distortion. We test our DPI-SLAM on large indoor datasets and demonstrate its state-of-the-art performance as the first planar-inertial SLAM system.
ER  - 

TY  - CONF
TI  - Radiation Source Localization in GPS-Denied Environments Using Aerial Robots
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6537
EP  - 6544
AU  - F. Mascarich
AU  - T. Wilson
AU  - C. Papachristos
AU  - K. Alexis
PY  - 2018
KW  - gamma-ray detection
KW  - Global Positioning System
KW  - mobile robots
KW  - photomultipliers
KW  - radioactive sources
KW  - radioactivity measurement
KW  - solid scintillation detectors
KW  - radiation measurements
KW  - radioactive source localization
KW  - radiation mapping
KW  - thallium-doped cesium iodide scintillator
KW  - indoor GPS-denied environments
KW  - Cesium-137 radiation source
KW  - GPS-denied localization
KW  - visual-inertial localization
KW  - autonomous nuclear radiation source localization
KW  - aerial robot
KW  - Scintillators
KW  - Calibration
KW  - Robot sensing systems
KW  - Detectors
KW  - Unmanned aerial vehicles
KW  - Radiation detectors
DO  - 10.1109/ICRA.2018.8460760
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper details the system and methods developed to enable autonomous nuclear radiation source localization and mapping using aerial robots in GPS-denied environments. A Thallium-doped Cesium Iodide (CsI(Tl)) scintillator and a Silicon Photomultiplier are combined with custom-built electronics for counting and spectroscopy, and the provided radiation measurements are pose-annotated using visual-inertial localization enabling autonomous operation in GPS-denied environments. Provided this capability, a strategy for radioactive source localization, as well as active source search path planning was developed. The proposed method is motivated and accounts for the limited endurance of the vehicle, which entails a very small amount of dwell points, and the fact that GPS-denied localization implies varying uncertainty of the robot's position estimate. The complete system is evaluated in multiple experimental studies using a small aerial robot and a Cesium-137 radiation source. As shown, accurate radioactive source localization is achieved, enabling efficient radiation mapping of indoor GPS-denied environments.
ER  - 

TY  - CONF
TI  - LineDrone Technology: Landing an Unmanned Aerial Vehicle on a Power Line
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6545
EP  - 6552
AU  - F. Mirallès
AU  - P. Hamelin
AU  - G. Lambert
AU  - S. Lavoie
AU  - N. Pouliot
AU  - M. Montfrond
AU  - S. Montambault
PY  - 2018
KW  - aircraft landing guidance
KW  - autonomous aerial vehicles
KW  - cameras
KW  - electrical maintenance
KW  - helicopters
KW  - inspection
KW  - optical radar
KW  - power overhead lines
KW  - remotely operated vehicles
KW  - robot vision
KW  - sensor fusion
KW  - LineDrone Technology
KW  - unmanned aerial vehicle landing
KW  - semiautomatic landing
KW  - vehicle onboard vision system
KW  - monocular camera
KW  - lidar
KW  - nondestructive testing
KW  - multirotor unmanned aerial vehicle capable
KW  - power transmission lines
KW  - landing assistance
KW  - Cameras
KW  - Payloads
KW  - Unmanned aerial vehicles
KW  - Inspection
KW  - Laser radar
KW  - Task analysis
KW  - Machine vision
DO  - 10.1109/ICRA.2018.8461250
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents the design of a multirotor unmanned aerial vehicle (UAV) capable of landing semiautomatically on a power line while carrying a payload. The vehicle then rolls along the line to perform an inspection. Special attention is given to the vehicle's onboard vision system, which consists of a monocular camera and LiDAR used together to compute the pose of the vehicle relative to the power line. Landing assistance is provided to the pilot by a position-based visual controller that aligns and keeps the vehicle centered along the power line. The pilot remains in control of vertical and longitudinal movement during descent. The proposed approach was tested on a full-scale test line and shows promise for future applications of high value to the electric industry such as non-destructive testing of power transmission lines.
ER  - 

TY  - CONF
TI  - Direction Controlled Descent of Samara Autorotating Wings (SAW) with N-Wings * Research supported by the SUTD-MIT International Design Centre (IDC) and by the Temasek Laboratories Defence Innovation Research Programme (DIRP) IGDSP15020141.
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6553
EP  - 6559
AU  - S. K. Hla Win
AU  - T. H. Goh
AU  - J. E. Low
AU  - D. S. Bin Shaiful
AU  - L. T. Soe Win
AU  - G. S. Soh
AU  - S. Foong
PY  - 2018
KW  - aerospace components
KW  - gyroscopes
KW  - mechanical stability
KW  - numerical analysis
KW  - position control
KW  - wind tunnels
KW  - direction controlled descent
KW  - spinning wing
KW  - direction controllability
KW  - control schemes
KW  - conical spiral autorotation trajectory
KW  - gyroscopic stability
KW  - maple trees
KW  - translational motion
KW  - numerical simulations
KW  - multiwing SAW prototype
KW  - wind-tunnel
KW  - samara autorotating wings
KW  - ball joint
KW  - Surface acoustic waves
KW  - Blades
KW  - Prototypes
KW  - Rotors
KW  - Mathematical model
KW  - Solid modeling
KW  - Stability analysis
DO  - 10.1109/ICRA.2018.8463145
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - The seeds of Maple trees (Samara) use autorotation as a unique mechanism to disperse their seeds. By exploiting gyroscopic stability of a spinning wing, the Samara is able to cover large horizontal distance despite having no form of propulsion. We applied and adapted this natural ability in our novel concept, the Samara Autorotating Wings (SAW), and extended its stability and direction controllability by generalizing the mechanism to incorporate designs with more than 1 wing. By conceiving cyclic control, the translational motion of autorotation is regulated. A nonlinear model of SAW with n wings is derived and control schemes developed to control the translational position during autorotation. Numerical simulations were performed to investigate the performance of the multi-wing SAW prototypes to track a conical spiral autorotation trajectory. Direct experiments were conducted in a vertical wind-tunnel through a special ball joint that allows z-axis translation and all three rotational degrees of freedom. Finally, free-fall drop tests are used to verify the directional controllability and performance of SAW.
ER  - 

TY  - CONF
TI  - Pseudo-bearing Measurements for Improved Localization of Radio Sources with Multirotor UAVs
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6560
EP  - 6565
AU  - L. Dressel
AU  - M. J. Kochenderfer
PY  - 2018
KW  - autonomous aerial vehicles
KW  - directive antennas
KW  - helicopters
KW  - mobile radio
KW  - mobile robots
KW  - omnidirectional antennas
KW  - radionavigation
KW  - pseudobearing measurements
KW  - radio frequency sources
KW  - RF source
KW  - directional antenna
KW  - omnidirectional antenna
KW  - antenna theory
KW  - ground tests
KW  - multirotor UAVs
KW  - radio sources localization
KW  - bearing-like measurements
KW  - unmanned aerial vehicles
KW  - Antenna measurements
KW  - Directional antennas
KW  - Gain
KW  - Radio frequency
KW  - Extraterrestrial measurements
KW  - Rotation measurement
DO  - 10.1109/ICRA.2018.8460734
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Localizing radio frequency (RF) sources is an important application for unmanned aerial vehicles (UAVs), Localization is often carried out by estimating bearing to an RF source, which can be achieved by rotating a directional antenna in place. Multirotor UAVs are well-suited for this sensing modality because they can efficiently rotate in place. However, a full rotation from a single location is needed to account for scale factors affecting the directional antenna's measurements. Although easy to perform, these rotations tend to be slow and delay localization. In this paper, we equip a multirotor UAV with a directional antenna and an omnidirectional antenna. The omnidirectional antenna serves to normalize measurements made by the directional antenna, yielding “pseudo-bearing” measurements. These bearing-like measurements are less informative than bearing measurements but do not require a full rotation, leading to more measurements and faster localization. We validate the normalization with antenna theory and ground tests. Claims of improved localization are validated with simulations and flight tests on a multirotor UAV. Our setup significantly reduces localization time compared to a multirotor UAV equipped with only a directional antenna.
ER  - 

TY  - CONF
TI  - Onboard State Dependent LQR for Agile Quadrotors
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6566
EP  - 6572
AU  - P. Foehn
AU  - D. Scaramuzza
PY  - 2018
KW  - aircraft control
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - helicopters
KW  - linear quadratic control
KW  - mobile robots
KW  - state estimation
KW  - time-varying systems
KW  - trajectory control
KW  - onboard state dependent LQR
KW  - agile quadrotors
KW  - quadrotor control
KW  - multiple cascaded subproblems
KW  - rotational dynamics
KW  - translational dynamics
KW  - cascaded attitude controller
KW  - attitude dynamics
KW  - robustness
KW  - LQR controller
KW  - rotational states
KW  - translational states
KW  - time-varying system dynamics
KW  - control parameters
KW  - linearization
KW  - Vehicle dynamics
KW  - Acceleration
KW  - Trajectory
KW  - Quaternions
KW  - Attitude control
KW  - Visualization
KW  - Regulators
DO  - 10.1109/ICRA.2018.8460885
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - State-of-the-art approaches in quadrotor control split the problem into multiple cascaded subproblems, exploiting the different time scales of the rotational and translational dynamics. They calculate a desired acceleration as input for a cascaded attitude controller but omit the attitude dynamics. These approaches use limits on the desired acceleration to maintain feasibility and robustness through the control cascade. We propose an implementation of an LQR controller, which: (I) is linearized depending on the quadrotor's state; (II) unifies the control of rotational and translational states; (III) handles time-varying system dynamics and control parameters. Our implementation is efficient enough to compute the full linearization and solution of the LQR at a minimum of 10 Hz on the vehicle using a common ARM processor. We show four successful experiments: (I) controlling at hover state with large disturbances; (II) tracking along a trajectory; (III) tracking along an infeasible trajectory; (IV) tracking along a trajectory with disturbances. All the experiments were done using only onboard visual inertial state estimation and LQR computation. To the best of our knowledge, this is the first implementation and evaluation of a state-dependent LQR capable of onboard computation while providing this amount of versatility and performance.
ER  - 

TY  - CONF
TI  - Autonomous Fixed-Wing Aerobatics: From Theory to Flight
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6573
EP  - 6580
AU  - E. Bulka
AU  - M. Nahon
PY  - 2018
KW  - aerospace components
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - Matlab
KW  - microcontrollers
KW  - mobile robots
KW  - position control
KW  - autonomous fixed-wing aerobatics
KW  - unmanned aerial vehicles
KW  - conventional fixed-wing aircraft
KW  - hardware-in-the-loop simulator
KW  - Pixhawk microcontroller
KW  - Xplane physics engine
KW  - HIL simulator
KW  - flight platform
KW  - agile fixed-wing UAV
KW  - rotorcraft
KW  - orientation time histories
KW  - position time histories
KW  - Matlab-Simulink high-fidelity simulation
KW  - Aircraft
KW  - Aerodynamics
KW  - Atmospheric modeling
KW  - Control systems
KW  - Mathematical model
KW  - Aerospace control
KW  - Propellers
DO  - 10.1109/ICRA.2018.8460610
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Unmanned aerial vehicles (UAVs) are increasingly being proposed for a wide range of applications. A promising new class of these vehicles, known as agile fixed-wing UAV s, is intended to bridge the gap between conventional fixed-wing aircraft, which can cover long distances efficiently, and rotorcraft, which are typically very maneuverable. This paper addresses the implementation of a controller for agile UAVs, beginning with a hardware-in-the-loop (HIL) simulator, followed by testing on a real platform, both implemented on the Pixhawk microcontroller. We replace the Xplane physics engine used in the standard Pixhawk HIL with our own in-house Matlab/Simulink high-fidelity simulation of an agile UA V. The HIL simulator is found to provide substantial advantages in the transition from pure simulation to experimental testing. Once the controller is integrated into the flight platform, flight tests are conducted, and the results of those tests are compared to those from the HIL simulation and those obtained from the pure simulation environment, for maneuvers including hover, aggressive turnaround, knife-edge, and rolling Harrier. The desired position and orientation time histories were successfully tracked with the proposed implementation, demonstrating the impressive autonomous maneuverability that can be achieved by this type of aircraft.
ER  - 

TY  - CONF
TI  - Adaptive Attitude Control for a Tail-Sitter UAV with Single Thrust-Vectored Propeller
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6581
EP  - 6586
AU  - W. Wang
AU  - J. Zhu
AU  - M. Kuang
AU  - X. Zhu
PY  - 2018
KW  - adaptive control
KW  - aerodynamics
KW  - aircraft control
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - least squares approximations
KW  - Lyapunov methods
KW  - propellers
KW  - stability
KW  - unified controller
KW  - attitude dynamics model
KW  - flight regimes
KW  - Lyapunov stability theory
KW  - control challenges
KW  - rotary wing UAVs
KW  - fixed wing
KW  - tail-sitter unmanned aerial vehicles
KW  - tail-sitter UAV
KW  - adaptive attitude control
KW  - control scheme
KW  - adaptive controller
KW  - quaternion attitude description
KW  - full-regime aerodynamics model
KW  - thrust vectoring model
KW  - single thrust-vectored propeller
KW  - cumbersome controller switchings
KW  - transition flights
KW  - Propellers
KW  - Aerodynamics
KW  - Atmospheric modeling
KW  - Attitude control
KW  - Aircraft
KW  - Mathematical model
KW  - Quaternions
DO  - 10.1109/ICRA.2018.8463158
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Tail-sitter unmanned aerial vehicles (UAVs) have gained extensive popularity in recent years due to their inherent advantages of both fixed wing and rotary wing UAVs. However, these advantages are accompanied with control challenges because of two different flight regimes and drastically changing dynamics during transition flights. This paper focuses on the design of a unified controller free from cumbersome controller switchings and applicable in all attitude range for a tail-sitter with single thrust-vectored propeller. To achieve this, both thrust vectoring model and full-regime aerodynamics model are built first, after which a complete attitude dynamics model of the tail-sitter is established utilizing the quaternion attitude description to avoid the singularity problem. An adaptive controller is then derived based on a simplified model using the Lyapunov stability theory with unknown system parameters identified online by forgetting factor recursive least square (FF-RLS) method. Flight experiments are conducted to demonstrate the feasibility and effectiveness of the proposed control scheme.
ER  - 

TY  - CONF
TI  - Online Aerodynamic Model Identification on Small Fixed-Wing UAVs with Uncertain Flight Data
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6587
EP  - 6592
AU  - P. Vaiopoulos
AU  - G. Zogopoulos-Papaliakos
AU  - K. J. Kyriakopoulos
PY  - 2018
KW  - aerodynamics
KW  - aerospace components
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - least squares approximations
KW  - Monte Carlo methods
KW  - real-time systems
KW  - sensors
KW  - wind tunnels
KW  - small fixed-wing unmanned aerial vehicles
KW  - total least squares estimation
KW  - ordinary least squares method
KW  - low-cost sensor system
KW  - insufficient system excitation
KW  - variable forgetting factor
KW  - Monte Carlo approach
KW  - compound aerodynamic variables
KW  - uncertainty estimation
KW  - real-time schemes
KW  - wind-tunnel experiments
KW  - real-time estimation
KW  - uncertain flight data
KW  - online aerodynamic model identification
KW  - Aerodynamics
KW  - Atmospheric modeling
KW  - Estimation
KW  - Uncertainty
KW  - Real-time systems
KW  - Aircraft
KW  - Adaptation models
DO  - 10.1109/ICRA.2018.8460585
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper focuses on real-time estimation of the aerodynamic model parameters of small-scale fixed wing Unmanned Aerial Vehicles (UAVs) without the aid of wind-tunnel experiments, using exclusively flight data. The key tool of the following analysis centers around the principles of Total Least Squares estimation. Contrary to Ordinary Least Squares, this method accounts for errors in both explanatory data and variables to-be-explained. This is a highly desirable property for UAVs equipped with low-cost sensor systems. The proposed implementation combines both batch and real-time schemes, while deals efficiently with the problem of Insufficient System Excitation. Online adaptation to model changes is performed by applying a Variable Forgetting Factor to the estimation data. Finally, a Monte Carlo approach is developed for uncertainty estimation regarding compound aerodynamic variables.
ER  - 

TY  - CONF
TI  - Towards X-Ray Medical Imaging with Robots in the Open: Safety Without Compromising Performances
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6604
EP  - 6610
AU  - L. Joseph
AU  - V. Padois
AU  - G. Morel
PY  - 2018
KW  - collision avoidance
KW  - control engineering computing
KW  - end effectors
KW  - linear quadratic control
KW  - manipulator dynamics
KW  - medical image processing
KW  - medical robotics
KW  - mobile robots
KW  - motion control
KW  - robot vision
KW  - control solution
KW  - robotic manipulator
KW  - generic safe controller
KW  - Linear Quadratic Problem formulation
KW  - unified energetic formulation
KW  - kinetic energy
KW  - redundant Kuka LWR4+ robot
KW  - X-ray medical imaging
KW  - end-effector
KW  - Robots
KW  - Task analysis
KW  - Torque
KW  - Safety
KW  - Collision avoidance
KW  - X-ray imaging
KW  - Aerospace electronics
DO  - 10.1109/ICRA.2018.8460794
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, a control solution featuring an energetic constraint is developed to improve the safety of a robotic manipulator sharing its workspace with humans. This general control structure, exploits a generic safe controller that ensures the respect of multiple constraints thanks to a Linear Quadratic Problem formulation. With a unified energetic formulation, the controller allows to explicitly limit both the kinetic energy when moving and the wrench applied to the environment in case of contact with an unexpected obstacle. This control approach is experimented on a redundant Kuka LWR4+ robot which end-effector shall precisely point toward a given location while following a trajectory.
ER  - 

TY  - CONF
TI  - Safety-Enhanced Human-Robot Interaction Control of Redundant Robot for Teleoperated Minimally Invasive Surgery
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6611
EP  - 6616
AU  - H. Su
AU  - J. Sandoval
AU  - M. Makhdoomi
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2018
KW  - adaptive control
KW  - fuzzy control
KW  - human-robot interaction
KW  - manipulators
KW  - medical robotics
KW  - surgery
KW  - telerobotics
KW  - robot manipulator
KW  - human-robot interaction
KW  - teleoperated minimally invasive surgery
KW  - surgical task execution
KW  - virtual surgical tasks
KW  - compliant null space motion
KW  - tele-operated MIS tasks
KW  - implemented impedance control
KW  - safety-enhanced compliant behavior
KW  - teleoperation control
KW  - redundant robot
KW  - safety-enhanced human-robot interaction control
KW  - Task analysis
KW  - Null space
KW  - Manipulators
KW  - Human-robot interaction
KW  - Torque
KW  - Surgery
DO  - 10.1109/ICRA.2018.8463148
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - In this paper, a teleoperation control of a 7-DoF robot manipulator for Minimally Invasive Surgery (MIS), which guarantees a safety-enhanced compliant behavior in the null space, is described. The redundancy of the manipulator is exploited to provide a flexible workspace for nurses or other staff (assisting physicians, patient support). The issue with safety and accurate surgical task execution may arise in the presence of human-robot interaction. Based on the implemented impedance control of tele-operated MIS tasks, a safety enhanced constraint is applied on the compliant null space motion. At the same time, the control approach integrates an adaptive fuzzy compensator to guarantee the accuracy of the surgical tasks during the uncertain human-robot interaction. The performance of the proposed algorithm is verified with virtual surgical tasks. The results showed that the compliant null space motion is constrained in a safe area, and also that the accuracy of tool tip is improved, providing a flexible and safe collaborative behavior in the null space for human-robot interaction during surgical tasks.
ER  - 

TY  - CONF
TI  - Three-Dimensional Surgical Needle Localization and Tracking Using Stereo Endoscopic Image Streams
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6617
EP  - 6624
AU  - O. Özgüner
AU  - R. Hao
AU  - R. C. Jackson
AU  - T. Shkurti
AU  - W. Newman
AU  - M. C. Cavusoglu
PY  - 2018
KW  - Bayes methods
KW  - biomedical optical imaging
KW  - endoscopes
KW  - medical image processing
KW  - medical robotics
KW  - rendering (computer graphics)
KW  - robot kinematics
KW  - 3D surgical needle localization
KW  - stereoendoscopic image streams
KW  - robot kinematics
KW  - computer vision techniques
KW  - da Vinci® Surgical Robotic System
KW  - stereo endoscopic camera images
KW  - three-dimensional tracking
KW  - da Vinci ® robot endoscope
KW  - Needles
KW  - Robots
KW  - Task analysis
KW  - Surgery
KW  - Image segmentation
KW  - Bayes methods
KW  - Atmospheric measurements
DO  - 10.1109/ICRA.2018.8460867
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents algorithms for three-dimensional tracking of surgical needles using the stereo endoscopic camera images obtained from the da Vinci® Surgical Robotic System. The proposed method employs Bayesian state estimation, computer vision techniques, and robot kinematics. A virtual needle rendering procedure is implemented to create simulated images of the surgical needle under the da Vinci ® robot endoscope, which makes it possible to measure the similarity between the rendered needle image and the real needle. A particle filter algorithm using the mentioned techniques is then used for tracking the surgical needle. The performance of the tracking is experimentally evaluated using an actual da Vinci® surgical robotic system and quantitatively validated in a ROS/Gazebo simulation thereof.
ER  - 

TY  - CONF
TI  - Robotic Assistance-as-Needed for Enhanced Visuomotor Learning in Surgical Robotics Training: An Experimental Study
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6631
EP  - 6636
AU  - N. Enayati
AU  - A. M. Okamura
AU  - A. Mariani
AU  - E. Pellegrini
AU  - M. M. Coad
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2018
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - surgery
KW  - telerobotics
KW  - visuomotor learning
KW  - complex visuomotor training
KW  - da Vinci Research Kit surgical console
KW  - surgical teleoperated robots
KW  - surgical practice
KW  - hands-on training
KW  - surgical robotics training
KW  - Task analysis
KW  - Training
KW  - Robot kinematics
KW  - Wires
KW  - Tools
KW  - Surgery
DO  - 10.1109/ICRA.2018.8463168
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Hands-on training is an indispensable part of surgical practice. As the tools used in the operating room become more intricate, the demand for efficient training methods increases. This work proposes a robotic assistance-as-needed method for training with surgical teleoperated robots. The method adapts the intensity of the assistance according to the trainee's current and past performance while gradually increasing the level of control of the trainee as the training progresses. The work includes an experiment comprising 160 acquisition sessions from 16 novice subjects performing a bimanual teleoperated exercise with a da Vinci Research Kit surgical console. Results capture the subtleties in the task's learning curve with and without robotic assistance and hint at the potential of robotic assistance for complex visuomotor training. Although robotic assistance for motor learning has received mixed results that range from beneficial to detrimental effects, this study shows such assistance may increase the rate of learning of certain skills in complex motor tasks.
ER  - 

TY  - CONF
TI  - Semi-Autonomous Laparoscopic Robotic Electro-Surgery with a Novel 3D Endoscope
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6637
EP  - 6644
AU  - H. N. D. Le
AU  - J. D. Opfermann
AU  - M. Kam
AU  - S. Raghunathan
AU  - H. Saeidi
AU  - S. Leonard
AU  - J. U. Kang
AU  - A. Krieger
PY  - 2018
KW  - biomedical optical imaging
KW  - endoscopes
KW  - kidney
KW  - medical image processing
KW  - medical robotics
KW  - surgery
KW  - 3D endoscope
KW  - robotic surgical system
KW  - cutting depth
KW  - freedom electro-surgical tool
KW  - robotic system
KW  - imaging system
KW  - laparoscopic camera
KW  - open loop control scheme
KW  - porcine cadaver kidney
KW  - robotic laparoscopic surgery system
KW  - semiautonomous laparoscopic robotic electro-surgery
KW  - Robots
KW  - Imaging
KW  - Surgery
KW  - Three-dimensional displays
KW  - Laparoscopes
KW  - Kidney
KW  - Task analysis
DO  - 10.1109/ICRA.2018.8461060
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper reports a robotic laparoscopic surgery system performing electro-surgery on porcine cadaver kidney, and evaluates its accuracy in an open loop control scheme to conduct targeting and cutting tasks guided by a novel 3D endoscope. We describe the design and integration of the novel laparoscopic imaging system that is capable of reconstructing the surgical field using structured light. A targeting task is first performed to determine the average positioning error of the system as guided by the laparoscopic camera. The imaging system is then used to reconstruct the surface of a porcine cadaver kidney, and generate a cutting trajectory with consistent depth. The paper concludes by using the robotic system in open loop control to cut this trajectory using a multi degree of freedom electro-surgical tool. It is demonstrated that for a cutting depth of 3 mm, the robotic surgical system follows the trajectory with an average depth of 2.44 mm and standard deviation of 0.34 mm. The average positional accuracy of the system was 2.74±0.99 mm.
ER  - 

TY  - CONF
TI  - An Ultrasonic Bone Cutting Tool for the da Vinci Research Kit
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6645
EP  - 6650
AU  - A. Gordon
AU  - T. Looi
AU  - J. Drake
AU  - C. R. Forrest
PY  - 2018
KW  - biomedical transducers
KW  - biomedical ultrasonics
KW  - bone
KW  - finite element analysis
KW  - genetic algorithms
KW  - medical robotics
KW  - surgery
KW  - ultrasonic transducers
KW  - finite element software
KW  - cutting phantom
KW  - research kit system
KW  - ultrasonic system
KW  - multiobjective genetic algorithm
KW  - ultrasonic transducer
KW  - minimally invasive ultrasonic bone cutting tool
KW  - da Vinci research kit
KW  - Acoustics
KW  - Impedance
KW  - Finite element analysis
KW  - Transducers
KW  - Bones
KW  - Surgery
KW  - Cutting tools
DO  - 10.1109/ICRA.2018.8460797
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - This paper presents a minimally invasive ultrasonic bone cutting tool designed for the da Vinci® research kit (dVRK). An ultrasonic transducer is modelled using finite element software, and correlated with testing results using an impedance analyzer. A multi-objective genetic algorithm is then used to design and analyze the remaining components of the ultrasonic system, in order to maximize system performance. The system is fabricated and mounted to the da Vinci® research kit system and tested on a cutting phantom.
ER  - 

TY  - CONF
TI  - Fast and Reliable Autonomous Surgical Debridement with Cable-Driven Robots Using a Two-Phase Calibration Procedure
T2  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6651
EP  - 6658
AU  - D. Seita
AU  - S. Krishnan
AU  - R. Fox
AU  - S. McKinley
AU  - J. Canny
AU  - K. Goldberg
PY  - 2018
KW  - biological tissues
KW  - calibration
KW  - cameras
KW  - diseases
KW  - edge detection
KW  - end effectors
KW  - endoscopes
KW  - medical robotics
KW  - neural nets
KW  - position control
KW  - robot vision
KW  - surgery
KW  - telerobotics
KW  - fragment phantoms
KW  - cable-driven robots
KW  - diseased tissue fragments
KW  - da Vinci Research Kit
KW  - cable-driven systems
KW  - two-phase coarse-to-fine calibration method
KW  - red calibration marker
KW  - end effector
KW  - open-loop trajectories
KW  - camera pixels
KW  - internal robot end-effector configurations
KW  - robotic surgical assistants
KW  - deep neural network
KW  - end-effector position
KW  - random forest
KW  - two-phase calibration procedure
KW  - surgical debridement
KW  - fine transformation bias
KW  - residual compensation bias
KW  - coarse transformation bias
KW  - time 7.3 s to 15.8 s
KW  - size 4.55 mm
KW  - size 2.14 mm
KW  - size 1.08 mm
KW  - Calibration
KW  - Cameras
KW  - Grippers
KW  - Robot kinematics
KW  - Robot vision systems
KW  - Tools
DO  - 10.1109/ICRA.2018.8460583
JO  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 21-25 May 2018
AB  - Automating precision subtasks such as debridement (removing dead or diseased tissue fragments) with Robotic Surgical Assistants (RSAs) such as the da Vinci Research Kit (dVRK) is challenging due to inherent nOnlinearities in cable-driven systems. We propose and evaluate a novel two-phase coarse-to-fine calibration method. In Phase I (coarse), we place a red calibration marker on the end effector and let it randomly move through a set of open-loop trajectories to obtain a large sample set of camera pixels and internal robot end-effector configurations. This coarse data is then used to train a Deep Neural Network (DNN) to learn the coarse transformation bias. In Phase II (fine), the bias from Phase I is applied to move the end -effector toward a small set of specific target points on a printed sheet. For each target, a human operator manually adjusts the end -effector position by direct contact (not through teleoperation) and the residual compensation bias is recorded. This fine data is then used to train a Random Forest (RF) to learn the fine transformation bias. Subsequent experiments suggest that without calibration, position errors average 4.55mm. Phase I can reduce average error to 2.14mm and the combination of Phase I and Phase II can reduces average error to 1.08mm. We apply these results to debridement of raisins and pumpkin seeds as fragment phantoms. Using an endoscopic stereo camera with standard edge detection, experiments with 120 trials achieved average success rates of 94.5 %, exceeding prior results with much larger fragments (89.4%) and achieving a speedup of 2.1x, decreasing time per fragment from 15.8 seconds to 7.3 seconds. Source code, data, and videos are available at https://sites.google.com/view/calib-icra/.
ER  - 


