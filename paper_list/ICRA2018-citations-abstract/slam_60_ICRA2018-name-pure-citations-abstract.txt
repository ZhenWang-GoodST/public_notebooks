total paper: 60
Title: iMag: Accurate and Rapidly Deployable Inertial Magneto-Inductive Localisation
Key Words: Global Positioning System  sensor placement  SLAM (robots)  wireless sensor networks  inertial magneto-inductive localisation  short-term construction work  iMag  robust simultaneous localisation  inertial measurement units  Transmitters  Robustness  Simultaneous localization and mapping  Magnetic resonance imaging  Distortion  Trajectory  Magneto-inductive device  Inertial measurements  Localisation  SLAM 
Abstract: Localisation is of importance for many applications. Our motivating scenarios are short-term construction work and emergency rescue. Not only is accuracy necessary, these scenarios also require rapid setup and robustness to environmental conditions. These requirements preclude the use of many traditional methods e.g. vision-based, laser-based, Ultra-wide band (UWB) and Global Positioning System (GPS)-based localisation systems. To solve these challenges, we introduce iMag, an accurate and rapidly deployable inertial magneto-inductive (MI) localisation system. It localises monitored workers using a single MI transmitter and inertial measurement units with minimal setup effort. However, MI location estimates can be distorted and ambiguous. To solve this problem, we suggest a novel method to use MI devices for sensing environmental distortions, and use these to correctly close inertial loops. By applying robust simultaneous localisation and mapping (SLAM), our proposed localisation method achieves excellent tracking accuracy, and can improve performance significantly compared with only using an inertial measurement unit (IMU) and MI device for localisation.


Title: Learning-Based Image Enhancement for Visual Odometry in Challenging HDR Environments
Key Words: convolution  distance measurement  feedforward neural nets  image enhancement  image representation  image sequences  learning (artificial intelligence)  object tracking  robot vision  SLAM (robots)  visual odometry  high dynamic range environments  interest points  bold assumptions  brightness constancy  deep learning perspective  deep neural network  long short term memory  deep networks  VO framework  convolutional neural network  image enhancement  illumination conditions  HDR environments  Robustness  Cameras  Brightness  Lighting  Training  Decoding  Estimation 
Abstract: One of the main open challenges in visual odometry (VO) is the robustness to difficult illumination conditions or high dynamic range (HDR) environments. The main difficulties in these situations come from both the limitations of the sensors and the inability to perform a successful tracking of interest points because of the bold assumptions in VO, such as brightness constancy. We address this problem from a deep learning perspective, for which we first fine-tune a deep neural network with the purpose of obtaining enhanced representations of the sequences for VO. Then, we demonstrate how the insertion of long short term memory allows us to obtain temporally consistent sequences, as the estimation depends on previous states. However, the use of very deep networks enlarges the computational burden of the VO framework; therefore, we also propose a convolutional neural network of reduced size capable of performing faster. Finally, we validate the enhanced representations by evaluating the sequences produced by the two architectures in several state-of-art VO algorithms, such as ORB-SLAM and DSO.


Title: Fusion of Stereo and Still Monocular Depth Estimates in a Self-Supervised Learning Context
Key Words: feedforward neural nets  image colour analysis  intelligent robots  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  monocular depth estimates  autonomous robots  self-supervised learning setup  stereo vision depth  convolutional neural network  fusion method  CNN estimates  autonomous navigation  depth estimation  self-supervised learning  Estimation  Stereo vision  Robot sensing systems  Cameras  Training  Self-supervised learning  monocular depth estimation  stereo vision  convolutional neural networks 
Abstract: We study how autonomous robots can learn by themselves to improve their depth estimation capability. In particular, we investigate a self-supervised learning setup in which stereo vision depth estimates serve as targets for a convolutional neural network (CNN) that transforms a single still image to a dense depth map. After training, the stereo and mono estimates are fused with a novel fusion method that preserves high confidence stereo estimates, while leveraging the CNN estimates in the low-confidence regions. The main contribution of the article is that it is shown that the fused estimates lead to a higher performance than the stereo vision estimates alone. Experiments are performed on the KITTI dataset, and on board of a Parrot SLAMDunk, showing that even rather limited CNNs can help provide stereo vision equipped robots with more reliable depth maps for autonomous navigation.


Title: Robust Visual Localization for Hopping Rovers on Small Bodies
Key Words: cameras  planetary rovers  pose estimation  robot vision  SLAM (robots)  space vehicles  visual SLAM algorithms  ORB-SLAM2  orbiting primary spacecraft  onboard visual simultaneous localization and mapping  visual SLAM implementation  wide field of view camera  off-nadir camera pointing angles  narrow FOV camera  orbiting spacecraft  visual appearance  high-contrast shadows  hopping rover  illumination angles  Solar System bodies  collaborative visual localization method  robust visual localization  time 1.0 hour to 12.0 hour  Cameras  Space vehicles  Visualization  Simultaneous localization and mapping  Optimization  Lighting  Solar system 
Abstract: We present a collaborative visual localization method for rovers designed to hop and tumble across the surface of small Solar System bodies, such as comets and asteroids. In a two-phase approach, an orbiting primary spacecraft first maps the surface of a body by capturing images from various poses and illumination angles; these images are processed to create a prior map of 3D landmarks. In the second phase, a hopping rover is deployed to the surface where it uses a camera to relocalize to the prior map and to perform onboard visual simultaneous localization and mapping (SLAM). Small bodies present several unique challenges to existing visual SLAM algorithms, such as high-contrast shadows that move quickly over the surface due to the short (e.g. 1-12 hour) rotational periods, and large changes in visual appearance between orbit and the surface, where image scale varies by many orders of magnitude (kilometers to centimeters). In this work, we describe how to augment ORB-SLAM2-a state of the art visual SLAM implementation-to handle large variations in illumination by fusing prior images with varying illumination angles. We demonstrate how a hopping rover can use a wide field of view (FOV) camera to relocalize to prior maps captured by an orbiting spacecraft with a narrow FOV camera, and how the growth of pose and scale errors can be bounded by periodic loop closures during large hops. The proposed method is evaluated with sequences of images captured around a mock asteroid; it is shown to be robust to varying illumination angles, scene scale changes, and off-nadir camera pointing angles.


Title: Bounding Drift in Cooperative Localisation Through the Sharing of Local Loop Closures
Key Words: graph theory  mobile robots  robot vision  SLAM (robots)  direct intervehicle observations  fuses single vehicle SLAM  cooperative localisation  data association  map data  local subgraphs  shared states  localisation accuracy  bounding drift  local loop closures  robotic scenarios  data consistency  bandwidth limitations  single vehicle visual SLAM framework  Information matrix  Simultaneous localization and mapping  Bandwidth  Jacobian matrices  Visualization  Message systems 
Abstract: Handling loop closures and intervehicle observations in cooperative robotic scenarios remains a challenging problem due to data consistency, bandwidth limitations and increased computation requirements. This paper develops a general cooperative localisation and single vehicle Visual SLAM framework that includes direct intervehicle observations and pose to pose loop closures on each vehicle with states shared as required. This fuses single vehicle SLAM with cooperative localisation and avoids data association of map data across limited communication networks. The base problem is developed as a factor graph with each vehicle solving local subgraphs that are split based on intervehicle observations. We modify the order of variable elimination in subgraphs through manipulation of the square-root of the Information matrix to extract updates that include the historic states involved in the loop closures and do not require transmission of other states not involved in the measurement or retransmission of previously shared states. We demonstrate the effect on localisation accuracy and bandwidth using data captured from a set of five robots observing each other and landmarks compared to both single vehicle SLAM, pure cooperative localisation and a centralised solution.


Title: Predicting Alignment Risk to Prevent Localization Failure
Key Words: feature extraction  image registration  SLAM (robots)  cluttered man-made environments  geometric constraints  spatial overlap  failed alignment  point cloud content  laser-based localization failure  geometric features  point cloud registration  alignment risk  Three-dimensional displays  Cloud computing  Robot sensing systems  Measurement  Iterative closest point algorithm  Octrees 
Abstract: During localization and mapping the success of point cloud registration can be compromised when there is an absence of geometric features or constraints in corridors or across doorways, or when the volumes scanned only partly overlap, due to occlusions or constrictions between subsequent observations. This work proposes a strategy to predict and prevent laser-based localization failure. Our solution relies on explicit analysis of the point cloud content prior to registration. A model predicting the risk of a failed alignment is learned by analysing the degree of spatial overlap between two input point clouds and the geometric constraints available within the region of overlap. We define a novel measure of alignability for these constraints. The method is evaluated against three real-world datasets and compared to baseline approaches. The experiments demonstrate how our approach can help improve the reliability of laser-based localization during exploration of unknown and cluttered man-made environments.


Title: Relocalization, Global Optimization and Map Merging for Monocular Visual-Inertial SLAM
Key Words: cameras  distance measurement  graph theory  inertial navigation  mobile robots  optimisation  path planning  pose estimation  robot vision  SLAM (robots)  relocalization  global optimization  monocular visual-inertial SLAM  visual-inertial system  low-cost inertial measurement unit  state estimation  visual-inertial odometry  absolute pose estimation  visual-inertial SLAM system  global pose graph optimization  map merging ability  map reuse  pose graph optimization  Cameras  Optimization  Visualization  Feature extraction  Microsoft Windows  Simultaneous localization and mapping  Real-time systems 
Abstract: The monocular visual-inertial system (VINS), which consists one camera and one low-cost inertial measurement unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate visual-inertial odometry is prone to drift and cannot provide absolute pose estimation. Leveraging history information to relocalize and correct drift has become a hot topic. In this paper, we propose a monocular visual-inertial SLAM system, which can relocalize camera and get the absolute pose in a previous-built map. Then 4-DOF pose graph optimization is performed to correct drifts and achieve global consistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual drifted direction in the visual-inertial system. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. Current map and previous map can be merged together by the global pose graph optimization. We validate the accuracy of our system on public datasets and compare against other state-of-the-art algorithms. We also evaluate the map merging ability of our system in the large-scale outdoor environment. The source code of map reuse is integrated into our public code, VINS-Monol11https://github.com/HKUST-Aerial-Robotics/VINS-Mono.


Title: Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM
Key Words: image reconstruction  mobile robots  optical radar  optimisation  probability  radar imaging  robot vision  sensor fusion  SLAM (robots)  dense map-centric continuous-time SLAM  CT-SLAM  computational complexity  surfel fusion  global batch trajectory optimization  probabilistic surface element fusion  map deformation  global trajectory optimization  Continuous-Time SLAM  global batch optimization  multimodal sensor fusion  continuous-time trajectory representation  elastic LiDAR fusion  Laser radar  Trajectory optimization  Simultaneous localization and mapping  Strain  Interpolation 
Abstract: The concept of continuous-time trajectory representation has brought increased accuracy and efficiency to multi-modal sensor fusion in modern SLAM. However, regardless of these advantages, its offline property caused by the requirement of global batch optimization is critically hindering its relevance for real-time and life-long applications. In this paper, we present a dense map-centric SLAM method based on a continuous-time trajectory to cope with this problem. The proposed system locally functions in a similar fashion to conventional Continuous-Time SLAM (CT-SLAM). However, it removes the need for global trajectory optimization by introducing map deformation. The computational complexity of the proposed approach for loop closure does not depend on the operation time, but only on the size of the space it explored before the loop closure. It is therefore more suitable for long term operation compared to the conventional CT-SLAM. Furthermore, the proposed method reduces uncertainty in the reconstructed dense map by using probabilistic surface element (surfel) fusion. We demonstrate that the proposed method produces globally consistent maps without global batch trajectory optimization, and effectively reduces LiDAR noise by surfel fusion.


Title: GOMSF: Graph-Optimization Based Multi-Sensor Fusion for robust UAV Pose estimation
Key Words: autonomous aerial vehicles  distance measurement  graph theory  Kalman filters  mobile robots  nonlinear filters  optimisation  pose estimation  robot vision  sensor fusion  SLAM (robots)  GOMSF  proprioceptive measurements  exteroceptive measurements  navigation algorithms  agile mobile robots  Unmanned Aerial Vehicles  UAV pose estimation  graph optimization based multisensor fusion  6 Degree-of-Freedom visual-inertial odometry poses  extended Kalman filter  Robot sensing systems  Robot kinematics  Optimization  Pose estimation  Global Positioning System  Time measurement 
Abstract: Achieving accurate, high-rate pose estimates from proprioceptive and/or exteroceptive measurements is the first step in the development of navigation algorithms for agile mobile robots such as Unmanned Aerial Vehicles (UAVs). In this paper, we propose a decoupled Graph-Optimization based Multi-Sensor Fusion approach (GOMSF) that combines generic 6 Degree-of-Freedom (DoF) visual-inertial odometry poses and 3 DoF globally referenced positions to infer the global 6 DoF pose of the robot in real-time. Our approach casts the fusion as a real-time alignment problem between the local base frame of the visual-inertial odometry and the global base frame. The alignment transformation that relates these coordinate systems is continuously updated by optimizing a sliding window pose graph containing the most recent robot's states. We evaluate the presented pose estimation method on both simulated data and large outdoor experiments using a small UAV that is capable to run our system onboard. Results are compared against different state-of-the-art sensor fusion frameworks, revealing that the proposed approach is substantially more accurate than other decoupled fusion strategies. We also demonstrate comparable results in relation with a finely tuned Extended Kalman Filter that fuses visual, inertial and GPS measurements in a coupled way and show that our approach is generic enough to deal with different input sources in a straightforward manner. Video - https//youtu.be/GIZNSZ2soL8.


Title: Robot Navigation in Complex Workspaces Using Harmonic Maps
Key Words: geometry  mobile robots  motion control  path planning  robot vision  SLAM (robots)  complex workspaces  harmonic maps  Artificial Potential Fields  autonomous robot navigation control schemes  local minima  APF based control scheme  goal configuration  multiply connected compact 2D workspaces  Harmonic analysis  Navigation  Robot kinematics  Convergence  Tuning  Trajectory 
Abstract: Artificial Potential Fields (APFs) constitute an intuitive tool for designing autonomous robot navigation control schemes, though they generally suffer from the existence of local minima which may trap the robot away from its desired configuration, an issue usually addressed by appropriate offline “tuning” of the potential field's parameters. On the other side, most APF based approaches rely on a diffeomorphism to sphere worlds to handle realistic scenarios, which may be either costly to compute (e.g., conformal mappings) or requires some sort of preconditioning of the workspace (e.g., decomposition of complex geometries to simple elementary components). In this work, we first propose a constructive procedure to map multiply connected compact 2D workspaces to one or more punctured disks based on harmonic maps. Subsequently, we design an APF based control scheme along with an adaptive law for its parameters that requires no offline tuning to guarantee safe convergence to its goal configuration. Finally, an extensive simulation study is conducted to demonstrate the efficacy of the proposed control scheme.


Title: Semantic Labeling of Indoor Environments from 3D RGB Maps
Key Words: data mining  feature extraction  geometry  image classification  image colour analysis  image reconstruction  image sensors  indoor navigation  learning (artificial intelligence)  mobile robots  object detection  object recognition  robot vision  SLAM (robots)  stereo image processing  semantic labels assignment  rooms reconstruction  deep-learning techniques  virtual RGB views  geometric analysis  object detection  scene classification  room types  3D RGB maps  indoor environments  semantic labeling  Semantics  Labeling  Three-dimensional displays  Robots  Training  Task analysis  Solid modeling 
Abstract: We present an approach to automatically assign semantic labels to rooms reconstructed from 3D RGB maps of apartments. Evidence for the room types is generated using state-of-the-art deep-learning techniques for scene classification and object detection based on automatically generated virtual RGB views, as well as from a geometric analysis of the map's 3D structure. The evidence is merged in a conditional random field, using statistics mined from different datasets of indoor environments. We evaluate our approach qualitatively and quantitatively and compare it to related methods.


Title: Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments
Key Words: cameras  distance measurement  feature extraction  image classification  image sensors  learning (artificial intelligence)  mobile robots  motion estimation  object detection  pose estimation  robot vision  SLAM (robots)  stereo image processing  self-supervised distractor learning  robust monocular visual odometry  self-supervised approach  distractors  camera images  cluttered urban environments  per-pixel ephemerality mask  depth map  deep convolutional network  monocular visual odometry pipeline  sparse features  dense photometric matching  metric-scale VO  single camera  robust VO methods  odometry drift  egomotion estimation  moving vehicles  urban traffic  vehicle motion  ephemerality  offline multisession mapping approaches  Three-dimensional displays  Cameras  Robustness  Visual odometry  Motion estimation  Entropy  Training data 
Abstract: We present a self-supervised approach to ignoring “distractors” in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.


Title: Semantic Mapping with Omnidirectional Vision
Key Words: cameras  distortion  image classification  image fusion  image segmentation  image sensors  mobile robots  path planning  robot vision  SLAM (robots)  omnidirectional vision  omnidirectional images  robust segmentation  occupancy grid maps  inverse sensor model  nonlinear distortions  omnidirectional camera mirror  place category classifier  range-based occupancy grid  dense semantic map  bird eye view  visual semantic mapping framework  robot local free space  Semantics  Sensors  Cameras  Robots  Buildings  Mirrors  Visualization 
Abstract: This paper presents a purely visual semantic mapping framework using omnidirectional images. The approach rests upon the robust segmentation of the robot's local free space, replacing conventional range sensors for the generation of occupancy grid maps. The perceptions are mapped into a bird's eye view allowing an inverse sensor model directly by removing the non-linear distortions of the omnidirectional camera mirror. The system relies on a place category classifier to label the navigation relevant categories: room, corridor, doorway, and open room. Each place class maintains a separated grid map that are fused with the range-based occupancy grid for building a dense semantic map.


Title: Design of an Autonomous Racecar: Perception, State Estimation and System Integration
Key Words: mobile robots  road vehicles  state estimation  modular redundant sub-systems  lateral accelerations  longitudinal accelerations  flüela driverless  onboard sensing  Formula Student Driverless competition  system integration  autonomous racecar  Automobiles  State estimation  Robot sensing systems  Current measurement  Laser radar  Wheels  Global Positioning System 
Abstract: This paper introduces jlüela driverless: the first autonomous racecar to win a Formula Student Driverless competition. In this competition, among other challenges, an autonomous racecar is tasked to complete 10 laps of a previously unknown racetrack as fast as possible and using only onboard sensing and computing. The key components of flüela's design are its modular redundant sub-systems that allow robust performance despite challenging perceptual conditions or partial system failures. The paper presents the integration of key components of our autonomous racecar, i.e., system design, EKF-based state estimation, LiDAR-based perception, and particle filter-based SLAM. We perform an extensive experimental evaluation on real-world data, demonstrating the system's effectiveness by outperforming the next-best ranking team by almost half the time required to finish a lap. The autonomous racecar reaches lateral and longitudinal accelerations comparable to those achieved by experienced human drivers.


Title: Line-Based Global Localization of a Spherical Camera in Manhattan Worlds
Key Words: cameras  feature extraction  gradient methods  Hough transforms  SLAM (robots)  3D line map  complicated six degrees of freedom search  2D line information  line-based global localization  spherical Hough representation  6 DoF localization process  Manhattan world assumption  3D-2D line correspondences  spherical-gradient filtering  spherical image  indoor environment  camera position  global environmental information  spherical camera  indoor localization  indoor spaces  Cameras  Three-dimensional displays  Image edge detection  Robustness  Robot vision systems  Solid modeling  Estimation 
Abstract: Localization is an important task for mobile service robots in indoor spaces. In this research, we propose a novel technique for indoor localization using a spherical camera. Spherical cameras can obtain a complete view of the surroundings allowing the use of global environmental information. We take advantage of this in order to estimate camera position and the orientation with respect to a known 3D line map of an indoor environment, using a single image. We robustly extract 2D line information from the spherical image via spherical-gradient filtering and match it to 3D line information in the line map. Our method requires no information about the 3D-2D line correspondences. In order to avoid a complicated six degrees of freedom (6 DoF) search for position and orientation, we use a Manhattan world assumption to decompose the line information in the image. The 6 DoF localization process is divided into two phases. First, we estimate the orientation by extracting the three principle directions from the image. Then, the position is estimated by robustly matching the distribution of lines between the image and the 3D model via a spherical Hough representation. This decoupled search can robustly localize a spherical camera using a single image, as we demonstrate experimentally.


Title: Addressing Challenging Place Recognition Tasks Using Generative Adversarial Networks
Key Words: feature extraction  image recognition  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  visual perception  perception task  place recognition tasks  simultaneous localization and mapping  SLAM  coupled Generative Adversarial Networks  domain translation task  Task analysis  Gallium nitride  Lighting  Generators  Image recognition  Feature extraction  Visualization 
Abstract: Place recognition is an essential component of Simultaneous Localization And Mapping (SLAM). Under severe appearance change, reliable place recognition is a difficult perception task since the same place is perceptually very different in the morning, at night, or over different seasons. This work addresses place recognition as a domain translation task. Using a pair of coupled Generative Adversarial Networks (GANs), we show that it is possible to generate the appearance of one domain (such as summer) from another (such as winter) without requiring image-to-image correspondences across the domains. Mapping between domains is learned from sets of images in each domain without knowing the instance-to-instance correspondence by enforcing a cyclic consistency constraint. In the process, meaningful feature spaces are learned for each domain, the distances in which can be used for the task of place recognition. Experiments show that learned features correspond to visual similarity and can be effectively used for place recognition across seasons.


Title: Data-Efficient Decentralized Visual SLAM
Key Words: cameras  data mining  graph theory  image sensors  multi-robot systems  optimisation  pose estimation  robot vision  SLAM (robots)  decentralized visual SLAM system  decentralized SLAM components  data-efficient decentralized visual SLAM  pose-graph optimization method  data association scales  robot count  data transfers  robots  map data  visual SLAM systems exchange  versatile cameras  lightweight cameras  cheap cameras  multirobot applications  mapping  Supplementary Material Data  Simultaneous localization and mapping  Visualization  Optimization  Pose estimation  Trajectory  Bandwidth 
Abstract: Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code. Supplementary Material Data and code are at: https://github.com/uzh-rpg/dslam_open.


Title: A Linear Least Square Initialization Method for 3D Pose Graph Optimization Problem
Key Words: approximation theory  computer vision  graph theory  image reconstruction  iterative methods  least squares approximations  optimisation  pose estimation  SLAM (robots)  important optimization problem  machine vision applications  3D SLAM  graph corresponds  PGO problem  relative noisy observation  PGO solvers  state-of-the-art initialization methods  low noise problems  measurement noise  iterative methods  high noise problems  PGO optimization problem  iterative least-squares method  linear least square initialization method  pose graph optimization  least-square problem  Robots  Integrated circuits  Cost function  Iterative methods  Three-dimensional displays  Estimation  Pose Graph Optimization  Least square  3D SLAM  Initialization method 
Abstract: Pose Graph Optimization (PGO) is an important optimization problem arising in robotics and machine vision applications like 3D reconstruction and 3D SLAM. Each node of pose graph corresponds to an orientation and a location. The PGO problem finds orientations and locations of the nodes from relative noisy observation between nodes. Recent investigations show that well-known iterative PGO solvers need good initialization to converge to good solutions. However, we observed that state-of-the-art initialization methods obtain good initialization only in low noise problems, and they fail in challenging problems having more measurement noise. Consequently, iterative methods may converge to bad solutions in high noise problems. In this paper, a new method for obtaining orientations in the PGO optimization problem is presented. Like other well-known methods the initial locations are obtained from the result of a least-squares problem. The proposed method iteratively approximates the problem around current estimation and converts it to a least-squares problem. Therefore, the method can be seen as an iterative least-squares method which is computationally efficient. Simulation results show that the proposed initialization method helps the most well-known iterative solver to obtain better optima and significantly outperform other solvers in some cases.


Title: IMLS-SLAM: Scan-to-Model Matching Based on 3D Data
Key Words: collision avoidance  mobile robots  optical radar  remotely operated vehicles  road traffic control  robot vision  SLAM (robots)  stereo image processing  robotics community  stereo cameras  depth sensors  Velodyne LiDAR  autonomous driving  low-drift SLAM algorithm  3D LiDAR data  scan-to-model matching framework  specific sampling strategy  LiDAR scans  Velodyne HDL32  Velodyne HDL64  global drift  IMLS-SLAM  3D data  simultaneous localization and mapping  localized LiDAR sweeps  IMLS surface representation  implicit moving least squares  size 4.0 km  size 16.0 m  time 10.0 year  Three-dimensional displays  Laser radar  Simultaneous localization and mapping  Two dimensional displays  Iterative closest point algorithm  Observability 
Abstract: The Simultaneous Localization And Mapping (SLAM) problem has been well studied in the robotics community, especially using mono, stereo cameras or depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the last 10 years to be very useful to perceive the environment in autonomous driving, but few methods exist that directly use these 3D data for odometry. We present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method relies on a scan-to-model matching framework. We first have a specific sampling strategy based on the LiDAR scans. We then define our model as the previous localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface representation. We show experiments with the Velodyne HDL32 with only 0.40% drift over a 4 km acquisition without any loop closure (i.e., 16 m drift after 4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and ranked among the best methods (against mono, stereo and LiDAR methods) with a global drift of only 0.69%.


Title: ApriISAM: Real-Time Smoothing and Mapping
Key Words: error analysis  matrix decomposition  mobile robots  SLAM (robots)  sparse matrices  fixed computational budget  dynamic variable reordering algorithm  ApriISAM  real-time smoothing  online robots  incremental SLAM algorithms  batch algorithms  absolute error  incremental Cholesky factorizations  marginalization order  iSAM  re-linearize  Simultaneous localization and mapping  Heuristic algorithms  Smoothing methods  Sparse matrices  Clustering algorithms  Real-time systems 
Abstract: For online robots, incremental SLAM algorithms offer huge potential computational savings over batch algorithms. The dominant incremental algorithms are iSAM and iSAM2 which offer radically different approaches to computing incremental updates, balancing issues like 1) the need to re-linearize, 2) changes in the desirable variable marginalization order, and 3) the underlying conceptual approach (i.e. the “matrix” story versus the “factor graph” story). In this paper, we propose a new incremental algorithm that computes solutions with lower absolute error and generally provides lower error solutions for a fixed computational budget than either iSAM or iSAM2. Key to AprilSAM's performance are a new dynamic variable reordering algorithm for fast incremental Cholesky factorizations, a method for reducing the work involved in backsubstitutions, and a new algorithm for deciding between incremental and batch updates.


Title: Fast Nonlinear Approximation of Pose Graph Node Marginalization
Key Words: approximation theory  graph theory  mobile robots  optimisation  path planning  pose estimation  SLAM (robots)  pose graph node marginalization  longterm localization  longterm mapping  longterm navigation  pose graph structure  absolute-to relative-pose spaces  pose-composition approach scaled version  approximate subgraph  fast nonlinear approximation method  Topology  Jacobian matrices  Covariance matrices  Gaussian distribution  Simultaneous localization and mapping  Approximation methods 
Abstract: We present a fast nonlinear approximation method for marginalizing out nodes on pose graphs for longterm simultaneous localization, mapping, and navigation. Our approximation preserves the pose graph structure to leverage the rich literature of pose graphs and optimization schemes. By re-parameterizing from absolute-to relative-pose spaces, our method does not suffer from the choice of linearization points as in previous works. We then join our approximation process with a scaled version of the recently-demoted pose-composition approach. Our approach eschews the expenses of many state-of-the-art convex optimization schemes through our efficient and simple O(N2) implementation for a given known topology of the approximate subgraph. We demonstrate its speed and near optimality in practice by comparing against state-of-the-art techniques on popular datasets.


Title: A Monocular SLAM System Leveraging Structural Regularity in Manhattan World
Key Words: cameras  feature extraction  mobile robots  optimisation  pose estimation  robot vision  SLAM (robots)  rotation optimization strategy  parallelism  global binding method  absolute rotation  relative rotation  translation optimization strategy leveraging coplanarity  coplanar features  relative translation  optimal absolute translation  3D line optimization strategy  structural line segments  structural features  structural feature-based optimization module  3D map  structural regularity  optimization strategies  monocular SLAM systems  Manhattan World  camera poses  Three-dimensional displays  Cameras  Optimization  Parallel processing  Simultaneous localization and mapping  Robustness  Estimation 
Abstract: The structural features in Manhattan world encode useful geometric information of parallelism, orthogonality and/or coplanarity in the scene. By fully exploiting these structural features, we propose a novel monocular SLAM system which provides accurate estimation of camera poses and 3D map. The foremost contribution of the proposed system is a structural feature-based optimization module which contains three novel optimization strategies. First, a rotation optimization strategy using the parallelism and orthogonality of 3D lines is presented. We propose a global binding method to compute an accurate estimation of the absolute rotation of the camera. Then we propose an approach for calculating the relative rotation to further refine the absolute rotation. Second, a translation optimization strategy leveraging coplanarity is proposed. Coplanar features are effectively identified, and we leverage them by a unified model handling both points and lines to calculate the relative translation, and then the optimal absolute translation. Third, a 3D line optimization strategy utilizing parallelism, orthogonality and coplanarity simultaneously is proposed to obtain an accurate 3D map consisting of structural line segments with low computational complexity. Experiments in man-made environments have demonstrated that the proposed system outperforms existing state-of-the-art monocular SLAM systems in terms of accuracy and robustness.


Title: Pairwise Consistent Measurement Set Maximization for Robust Multi-Robot Map Merging
Key Words: expectation-maximisation algorithm  graph theory  mobile robots  multi-robot systems  optimisation  robot vision  SLAM (robots)  PCM  robust multirobot map  robust selection  robust SLAM methods  multirobot case  simultaneous localization and mapping  pairwise consistency set maximization  pairwise consistent measurement set maximization  odometry backbone  Simultaneous localization and mapping  Robot kinematics  Phase change materials  Trajectory  Robustness  Merging 
Abstract: This paper reports on a method for robust selection of inter-map loop closures in multi-robot simultaneous localization and mapping (SLAM). Existing robust SLAM methods assume a good initialization or an “odometry backbone” to classify inlier and outlier loop closures. In the multi-robot case, these assumptions do not always hold. This paper presents an algorithm called Pairwise Consistency Maximization (PCM) that estimates the largest pairwise internally consistent set of measurements. Finding the largest pairwise internally consistent set can be transformed into an instance of the maximum clique problem from graph theory, and by leveraging the associated literature it can be solved in realtime. This paper evaluates how well PCM approximates the combinatorial gold standard using simulated data. It also evaluates the performance of PCM on synthetic and real-world data sets in comparison with DCS, SCGP, and RANSAC, and shows that PCM significantly outperforms these methods.


Title: Map-Aware Particle Filter for Localization
Key Words: distance measurement  Global Positioning System  optical radar  particle filtering (numerical methods)  SLAM (robots)  map-aware particle filter  2D LiDAR localization  GPS localization  map information  localization sensors  particle filter framework  map-matching  prior occupancy grid  vehicle localization  Trajectory  Roads  Atmospheric measurements  Particle measurements  Sensors  Particle filters  Two dimensional displays 
Abstract: This work presents a method to improve vehicle localization by using the information from a prior occupancy grid to bound the possible poses. The method, named Map-Aware Particle Filter, uses a nonlinear approach to map-matching that can be integrated into a particle filter framework for localization. Each particle is re-weighted based on the validity of its current position in the map. In addition, we buffer the trajectory followed by the vehicle and then append it to each particle's pose. We then quantify the overlap between the trajectory and the map's free space. This serves as a measure of each particle's validity given the trajectory and the shape of the map. We evaluated the method by performing experiments with different types of localization sensors: First, (i) we significantly reduced the drift inherent to dead reckoning. By only using wheel odometry and map information we achieved loop closure over a distance of approximately 3 km. We also (ii) increased the accuracy of GPS localization. Finally, (iii) we fused a fragile 2D LiDAR localization with the map information. The resulting system had a higher robustness and managed to close the loop in an outdated map where it had failed before.


Title: Feature-Based SLAM for Imaging Sonar with Under-Constrained Landmarks
Key Words: feature extraction  image reconstruction  image sensors  reliability  SLAM (robots)  sonar imaging  sonar imaging  point landmark identification  feature-point extraction  general-purpose method  planar scene assumption  underwater feature-based SLAM  under-constrained landmarks  Feature extraction  Imaging  Sonar measurements  Simultaneous localization and mapping  Three-dimensional displays  Image reconstruction 
Abstract: Recent algorithms have demonstrated the feasibility of underwater feature-based SLAM using imaging sonar. But previous methods have either relied on manual feature extraction and correspondence or used prior knowledge of the scene, such as the planar scene assumption. Our proposed system provides a general-purpose method for feature-point extraction and correspondence in arbitrary scenes. Additionally, we develop a method of identifying point landmarks that are likely to be well-constrained and reliably reconstructed. Finally, we demonstrate that while under-constrained landmarks cannot be accurately reconstructed themselves, they can still be used to constrain and correct the sensor motion. These advances represent a large step towards general-purpose, feature-based SLAM with imaging sonar.


Title: SLAMBench2: Multi-Objective Head-to-Head Benchmarking for Visual SLAM
Key Words: augmented reality  autonomous aerial vehicles  mobile computing  mobile robots  navigation  robot vision  SLAM (robots)  visual SLAM  augmented reality systems  nonfunctional requirements  mobile phone-based AR application  tight energy budget  UAV navigation system  SLAMBench2  benchmarking framework  open source  close source  performance metrics  ORB-SLAM2  publicly-available software framework  SLAM applications  SLAM systems  SLAM algorithms  multiobjective head-to-head benchmarking  functional requirements  Simultaneous localization and mapping  Measurement  Trajectory  Benchmark testing  User interfaces  C++ languages 
Abstract: SLAM is becoming a key component of robotics and augmented reality (AR) systems. While a large number of SLAM algorithms have been presented, there has been little effort to unify the interface of such algorithms, or to perform a holistic comparison of their capabilities. This is a problem since different SLAM applications can have different functional and non-functional requirements. For example, a mobile phone-based AR application has a tight energy budget, while a UAV navigation system usually requires high accuracy. SLAMBench2 is a benchmarking framework to evaluate existing and future SLAM systems, both open and close source, over an extensible list of datasets, while using a comparable and clearly specified list of performance metrics. A wide variety of existing SLAM algorithms and datasets is supported, e.g. ElasticFusion, InfiniTAM, ORB-SLAM2, OKVIS, and integrating new ones is straightforward and clearly specified by the framework. SLAMBench2 is a publicly-available software framework which represents a starting point for quantitative, comparable and val-idatable experimental research to investigate trade-offs across SLAM systems.


Title: Don't Look Back: Robustifying Place Categorization for Viewpoint- and Condition-Invariant Place Recognition
Key Words: cameras  feature extraction  image matching  image representation  image sequences  mobile robots  neural nets  object recognition  path planning  robot vision  SLAM (robots)  semantics-aware higher-order layers  deep neural networks  pure appearance-based techniques  place categorization  place-centric characteristics  condition-invariant place recognition  rear view mirror  semantic visual understanding  visual places  Semantics  Visualization  Robustness  Databases  Image recognition  Cameras  Neural networks 
Abstract: When a human drives a car along a road for the first time, they later recognize where they are on the return journey typically without needing to look in their rear view mirror or turn around to look back, despite significant viewpoint and appearance change. Such navigation capabilities are typically attributed to our semantic visual understanding of the environment [1] beyond geometry to recognizing the types of places we are passing through such as “passing a shop on the left” or “moving through a forested area”. Humans are in effect using place categorization [2] to perform specific place recognition even when the viewpoint is 180 degrees reversed. Recent advances in deep neural networks have enabled high performance semantic understanding of visual places and scenes, opening up the possibility of emulating what humans do. In this work, we develop a novel methodology for using the semantics-aware higher-order layers of deep neural networks for recognizing specific places from within a reference database. To further improve the robustness to appearance change, we develop a descriptor normalization scheme that builds on the success of normalization schemes for pure appearance-based techniques such as SeqSLAM [3]. Using two different datasets - one road-based, one pedestrian-based, we evaluate the performance of the system in performing place recognition on reverse traversals of a route with a limited field of view camera and no turn-back-and-Iook behaviours, and compare to existing state-of-the-art techniques and vanilla off-the-shelf features. The results demonstrate significant improvements over the existing state of the art, especially for extreme perceptual challenges that involve both great viewpoint change and environmental appearance change. We also provide experimental analyses of the contributions of the various system components: the use of spatio-temporal sequences, place categorization and place-centric characteristics as opposed to object-centric semantics.


Title: Towards Globally Consistent Visual-Inertial Collaborative SLAM
Key Words: autonomous aerial vehicles  mobile robots  path planning  robot vision  SLAM (robots)  globally consistent tracking  autonomous robot navigation  monocular-inertial odometry  vision-based perception  metric scale estimation  benchmarking datasets  UAVs  monocular-inertial sensor suite  unmanned aerial vehicles  visual-inertial collaborative SLAM  drift correction  Simultaneous localization and mapping  Collaboration  Unmanned aerial vehicles  Optimization  Measurement  Trajectory 
Abstract: Motivated by the need for globally consistent tracking and mapping before autonomous robot navigation becomes realistically feasible, this paper presents a novel backend to monocular-inertial odometry. As some of the most challenging platforms for vision-based perception, we evaluate the performance of our system using Unmanned Aerial Vehicles (UAV s). Our experimental validation demonstrates that the proposed approach achieves drift correction and metric scale estimation from a single UAV on benchmarking datasets. Furthermore, the generality of our approach is demonstrated to achieve globally consistent maps built in a collaborative manner from two UAVs, each equipped with a monocular-inertial sensor suite, showing the possible gains opened by collaboration amongst robots to perform SLAM. Video - https://youtu.be/wbX36HBu2Eg.


Title: Topomap: Topological Mapping and Navigation Based on Visual SLAM Maps
Key Words: mobile robots  navigation  path planning  robot vision  SLAM (robots)  three-dimensional topological map  noisy sparse point cloud  convex free-space clusters  global planning  mobile robotic platform  Topomap  visual SLAM  visual robot navigation  navigation task  sparse feature-based map  path planning algorithms  visual simultaneous localization and mapping system  Navigation  Visualization  Simultaneous localization and mapping  Path planning  Three-dimensional displays  Planning 
Abstract: Visual robot navigation within large-scale, semistructured environments deals with various challenges such as computation intensive path planning algorithms or insufficient knowledge about traversable spaces. Moreover, many state-of-the-art navigation approaches only operate locally instead of gaining a more conceptual understanding of the planning objective. This limits the complexity of tasks a robot can accomplish and makes it harder to deal with uncertainties that are present in the context of real-time robotics applications. In this work, we present Topomap, a framework which simplifies the navigation task by providing a map to the robot which is tailored for path planning use. This novel approach transforms a sparse feature-based map from a visual Simultaneous Localization And Mapping (SLAM) system into a three-dimensional topological map. This is done in two steps. First, we extract occupancy information directly from the noisy sparse point cloud. Then, we create a set of convex free-space clusters, which are the vertices of the topological map. We show that this representation improves the efficiency of global planning, and we provide a complete derivation of our algorithm. Planning experiments on real world datasets demonstrate that we achieve similar performance as RRT* with significantly lower computation times and storage requirements. Finally, we test our algorithm on a mobile robotic platform to prove its advantages.


Title: PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor Fusion and Hardware Co-Design
Key Words: cameras  image fusion  robot vision  SLAM (robots)  stereo image processing  synchronisation  embedded simultaneous localization and mapping algorithm  multi-core processor  public visual-inertial datasets  PerceptIn Robotics Vision System  Hardware Co-Design  advanced visual-inertial SLAM System  state-of-the-art visual-inertial algorithms  additional sensor modalities  inertial measurements  visual measurements  flexible sensor fusion approach  PIRVS software features  precise hardware synchronization  global-shutter stereo camera  PIRVS hardware  visual-inertial computing hardware  Simultaneous localization and mapping  Hardware  Cameras  Feature extraction  Synchronization  Visualization 
Abstract: In this paper, we present the PerceptIn Robotics Vision System (PIRVS), a visual-inertial computing hardware with embedded simultaneous localization and mapping (SLAM) algorithm. The PIRVS hardware is equipped with a multi-core processor, a global-shutter stereo camera, and an IMU with precise hardware synchronization. The PIRVS software features a flexible sensor fusion approach to not only tightly integrate visual measurements with inertial measurements and also to loosely couple with additional sensor modalities. It runs in real-time on both PC and the PIRVS hardware. We perform a thorough evaluation of the proposed system using multiple public visual-inertial datasets. Experimental results demonstrate that our system reaches comparable accuracy of state-of-the-art visual-inertial algorithms on PC, while being more efficient on the PIRVS hardware.


Title: ProSLAM: Graph SLAM from a Programmer's Perspective
Key Words: C++ language  data structures  graph theory  public domain software  robot vision  SLAM (robots)  stereo image processing  Graph SLAM  data structures  C++ programming language  standard libraries  lightweight open-source stereo visual SLAM system  programmer  ProSLAM  algorithmic aspects  mathematical aspects  highly modular system  Simultaneous localization and mapping  Visualization  Three-dimensional displays  Cameras  Data structures  Benchmark testing 
Abstract: In this paper we present ProSLAM, a lightweight open-source stereo visual SLAM system designed with simplicity in mind. This work stems from the experience gathered by the authors while teaching SLAM and aims at providing a highly modular system that can be easily implemented and understood. Rather than focusing on the well known mathematical aspects of stereo visual SLAM, we highlight the data structures and the algorithmic aspects required to realize such a system. We implemented ProSLAM using the C++ programming language in combination with a minimal set of standard libraries. The results of a thorough validation performed on several standard benchmark datasets show that ProSLAM achieves precision comparable to state-of-the-art approaches, while requiring substantially less computation.


Title: Talk Resource-Efficiently to Me: Optimal Communication Planning for Distributed Loop Closure Detection
Key Words: mobile robots  multi-robot systems  robot vision  SLAM (robots)  cooperative simultaneous localization and mapping  inter-robot loop closures  general resource-efficiency communication planning  sensory data sharing  distributed loop closure detection  optimal communication planning  CSLAM  Robot sensing systems  Distributed databases  Planning  Trajectory  Visualization  Metadata 
Abstract: Due to the distributed nature of cooperative simultaneous localization and mapping (CSLAM), detecting inter-robot loop closures necessitates sharing sensory data with other robots. A naïve approach to data sharing can easily lead to a waste of mission-critical resources. This paper investigates the logistical aspects of CSLAM. Particularly, we present a general resource-efficient communication planning framework that takes into account both the total amount of exchanged data and the induced division of labor between the participating robots. Compared to other state-of-the-art approaches, our framework is able to verify the same set of potential inter-robot loop closures while exchanging considerably less data and influencing the induced workloads. We develop a fast algorithm for finding globally optimal communication policies, and present theoretical analysis to characterize the necessary and sufficient conditions under which simpler strategies are optimal. The proposed framework is extensively evaluated with data from the KITTI odometry benchmark datasets.


Title: StaticFusion: Background Reconstruction for Dense RGB-D SLAM in Dynamic Environments
Key Words: cameras  image colour analysis  image filtering  image motion analysis  image reconstruction  image segmentation  image sensors  image sequences  motion estimation  object detection  object tracking  pose estimation  probability  robot vision  SLAM (robots)  frame-to-model alignment  3D model estimation  outlier filtering techniques  moving object detection  camera pose tracking  probabilistic static-dynamic segmentation  background structure reconstruction  dynamic scenes  static environments  dynamic sequences  static sequences  camera motion estimation  weighted dense RGB-D fusion  current RGB-D image pair  implicit robust penalisers  background structure  robust dense RGB-D SLAM  visual SLAM  dynamic environments  Cameras  Robustness  Image segmentation  Motion segmentation  Dynamics  Three-dimensional displays  Image reconstruction 
Abstract: Dynamic environments are challenging for visual SLAM as moving objects can impair camera pose tracking and cause corruptions to be integrated into the map. In this paper, we propose a method for robust dense RGB-D SLAM in dynamic environments which detects moving objects and simultaneously reconstructs the background structure. While most methods employ implicit robust penalisers or outlier filtering techniques in order to handle moving objects, our approach is to simultaneously estimate the camera motion as well as a probabilistic static/dynamic segmentation of the current RGB-D image pair. This segmentation is then used for weighted dense RGB-D fusion to estimate a 3D model of only the static parts of the environment. By leveraging the 3D model for frame-to-model alignment, as well as static/dynamic segmentation, camera motion estimation has reduced overall drift - as well as being more robust to the presence of dynamics in the scene. Demonstrations are presented which compare the proposed method to related state-of-the-art approaches using both static and dynamic sequences. The proposed method achieves similar performance in static environments and improved accuracy and robustness in dynamic scenes.


Title: Constructing Category-Specific Models for Monocular Object-SLAM
Key Words: cameras  feature extraction  mobile robots  object detection  pose estimation  robot vision  SLAM (robots)  category-specific models  real-time object-oriented SLAM  monocular camera  object-level models  category-level models  object deformations  discriminative object features  category models  object landmark observations  generic monocular SLAM framework  2D object features  sparse feature-based monocular SLAM  object instance retrieval  instance-independent monocular object-SLAM system  feature-based SLAM methods  time 2.0 d  time 3.0 d  Solid modeling  Simultaneous localization and mapping  Three-dimensional displays  Object oriented modeling  Pipelines  Two dimensional displays  Shape 
Abstract: We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fty, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show - to the best of our knowledge - first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.


Title: End to End Learning of Spiking Neural Network Based on R-STDP for a Lane Keeping Vehicle
Key Words: control engineering computing  learning (artificial intelligence)  mobile robots  neural nets  road vehicles  robot vision  SLAM (robots)  spiking neural network  lane keeping vehicle  mobile applications  mobile robot applications  reward-modulated spike-timing-dependent-plasticity  reinforcement learning  Pioneer robot  lane information  robot tasks control  end to end learning approach  R-STDP  SNNs training  neuromorphic vision sensor  lateral localization accuracy  Voltage control  Task analysis  Robot sensing systems  Training  Synapses  Neurons 
Abstract: Learning-based methods have demonstrated clear advantages in controlling robot tasks, such as the information fusion abilities, strong robustness, and high accuracy. Meanwhile, the on-board systems of robots have limited computation and energy resources, which are contradictory with state-of-the-art learning approaches. They are either too lightweight to solve complex problems or too heavyweight to be used for mobile applications. On the other hand, training spiking neural networks (SNNs) with biological plausibility has great potentials of performing fast computation and energy efficiency. However, the lack of effective learning rules for SNNs impedes their wide usage in mobile robot applications. This paper addresses the problem by introducing an end to end learning approach of spiking neural networks for a lane keeping vehicle. We consider the reward-modulated spike-timing-dependent-plasticity (R-STDP) as a promising solution in training SNNs, since it combines the advantages of both reinforcement learning and the well-known STDP. We test our approach in three scenarios that a Pioneer robot is controlled to keep lanes based on an SNN. Specifically, the lane information is encoded by the event data from a neuromorphic vision sensor. The SNN is constructed using R-STDP synapses in an all-to-all fashion. We demonstrate the advantages of our approach in terms of the lateral localization accuracy by comparing with other state-of-the-art learning algorithms based on SNNs.


Title: Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image
Key Words: image colour analysis  image reconstruction  image resolution  image sampling  image segmentation  image sensors  learning (artificial intelligence)  mean square error methods  optical radar  random processes  regression analysis  SLAM (robots)  sparse matrices  prediction root-mean-square error  sparse maps  dense maps  sparse-to-dense  dense depth prediction  sparse set  depth measurements  single RGB image  depth estimation  monocular images  low-resolution depth sensor  single deep regression network  RGB-D raw data  sparse depth samples  visual simultaneous localization and mapping algorithms  plug-in module  NYU-depth-v2 indoor dataset  LiDARs  Training  Laser radar  Image reconstruction  Estimation  Prediction algorithms  Simultaneous localization and mapping 
Abstract: We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software2 and video demonstration3 are publicly available.


Title: Just-in-Time Reconstruction: Inpainting Sparse Maps Using Single View Depth Predictors as Priors
Key Words: convolution  feature extraction  image colour analysis  image fusion  image reconstruction  image sensors  iterative methods  neural nets  pose estimation  recurrent neural nets  robot vision  SLAM (robots)  stereo image processing  CRF model  RGB image  confidence-based fusion  realtime inpainting  convolutional neural networks  CNN  ORB-SLAM  Kinect  conditional depth error distributions  pixel-wise confidence weights  input depth map  fused depth map  virtual depth sensor  single-view depth prediction network  sparse sensor  monocular visual SLAM system  fully dense depth map  realtime image-guided inpainting  just-in-time reconstruction  single view depth predictors  scale-invariant depth error  outlier input depth  LIDAR depth maps  arbitrary scale  sparse map  Image reconstruction  Simultaneous localization and mapping  Visualization  Three-dimensional displays  Real-time systems  Uncertainty 
Abstract: We present “just-in-time reconstruction” as realtime image-guided inpainting of a map with arbitrary scale and sparsity to generate a fully dense depth map for the image. In particular, our goal is to inpaint a sparse map - obtained from either a monocular visual SLAM system or a sparse sensor - using a single-view depth prediction network as a virtual depth sensor. We adopt a fairly standard approach to data fusion, to produce a fused depth map by performing inference over a novel fully-connected Conditional Random Field (CRF) which is parameterized by the input depth maps and their pixel-wise confidence weights. Crucially, we obtain the confidence weights that parameterize the CRF model in a data-dependent manner via Convolutional Neural Networks (CNNs) which are trained to model the conditional depth error distributions given each source of input depth map and the associated RGB image. Our CRF model penalises absolute depth error in its nodes and pairwise scale-invariant depth error in its edges, and the confidence-based fusion minimizes the impact of outlier input depth values on the fused result. We demonstrate the flexibility of our method by real-time inpainting of ORB-SLAM, Kinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary scale and varied amount of irregular sparsity.


Title: Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping
Key Words: continuous time systems  entropy  graph theory  image registration  image resolution  laser ranging  optical radar  SLAM (robots)  solid modelling  stereo image processing  laser-range scanners  high data rate  3D laser scanner  surfel-based registration  recursive state estimation  multiresolution maps  continuous-time SLAM  3D lidar-based online mapping  online simultaneous localization and mapping  Three-dimensional displays  Measurement by laser beam  Optimization  Trajectory  Laser modes  Simultaneous localization and mapping 
Abstract: Modern 3D laser-range scanners have a high data rate, making online simultaneous localization and mapping (SLAM) computationally challenging. Recursive state estimation techniques are efficient but commit to a state estimate immediately after a new scan is made, which may lead to misalignments of measurements. We present a 3D SLAM approach that allows for refining alignments during online mapping. Our method is based on efficient local mapping and a hierarchical optimization back-end. Measurements of a 3D laser scanner are aggregated in local multiresolution maps by means of surfel-based registration. The local maps are used in a multi-level graph for allocentric mapping and localization. In order to incorporate corrections when refining the alignment, the individual 3D scans in the local map are modeled as a sub-graph and graph optimization is performed to account for drift and misalignments in the local maps. Furthermore, in each sub-graph, a continuous-time representation of the sensor trajectory allows to correct measurements between scan poses. We evaluate our approach in multiple experiments by showing qualitative results. Furthermore, we quantify the map quality by an entropy-based measure.


Title: Direct Line Guidance Odometry
Key Words: distance measurement  feature extraction  robot vision  SLAM (robots)  direct line guidance odometry  pixel intensities  line-based features  point-based direct monocular visual odometry method  visual odometry algorithms  feature extraction  keypoint selection  Feature extraction  IP networks  Cameras  Optimization  Visual odometry  Simultaneous localization and mapping  Computational efficiency 
Abstract: Modern visual odometry algorithms utilize sparse point-based features for tracking due to their low computational cost. Current state-of-the-art methods are split between indirect methods that process features extracted from the image, and indirect methods that deal directly on pixel intensities. In recent years, line-based features have been used in SLAM and have shown an increase in performance albeit with an increase in computational cost. In this paper, we propose an extension to a point-based direct monocular visual odometry method. Here we that uses lines to guide keypoint selection rather than acting as features. Points on a line are treated as stronger keypoints than those in other parts of the image, steering point-selection away from less distinctive points and thereby increasing efficiency. By combining intensity and geometry information from a set of points on a line, accuracy may also be increased.


Title: Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System
Key Words: cameras  image matching  motion estimation  motion measurement  optical radar  optical sensors  optical tracking  optical windows  portable instruments  SLAM (robots)  sparse depth information  motion estimation  pose-graph SLAM  KITTI odometry benchmark datasets  direct visual SLAM  monocular camera  light detection and ranging  portable camera-LiDAR mapping system  direct visual simultaneous localization and mapping  sliding window-based tracking method  depth-integrated frame matching  feature-based visual LiDAR mapping  sensors  Cameras  Laser radar  Three-dimensional displays  Visualization  Simultaneous localization and mapping  Optimization 
Abstract: This paper describes a framework for direct visual simultaneous localization and mapping (SLAM) combining a monocular camera with sparse depth information from Light Detection and Ranging (LiDAR). To ensure realtime performance while maintaining high accuracy in motion estimation, we present (i) a sliding window-based tracking method, (ii) strict pose marginalization for accurate pose-graph SLAM and (iii) depth-integrated frame matching for large-scale mapping. Unlike conventional feature-based visual and LiDAR mapping, the proposed approach is direct, eliminating the visual feature in the objective function. We evaluated results using our portable camera-LiDAR system as well as KITTI odometry benchmark datasets. The experimental results prove that the characteristics of two complementary sensors are very effective in improving real-time performance and accuracy. Via validation, we achieved low drift error of 0.98 % in the KITTI benchmark including various environments such as a highway and residential areas.


Title: Bayesian Scale Estimation for Monocular SLAM Based on Generic Object Detection for Correcting Scale Drift
Key Words: Bayes methods  learning (artificial intelligence)  object detection  robot vision  SLAM (robots)  monocular SLAM system  Bayesian framework  deep-learning based generic object detector  detection region  scale drift  monocular systems  Bayesian scale estimation  generic object detection  local scale correction  object class detection  KITTI dataset  quantitative evaluations  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Trajectory  Bayes methods  Object detection  Image reconstruction 
Abstract: We propose a novel real-time algorithm for estimating the local scale correction of a monocular SLAM system, to obtain a correctly scaled version of the 3D map and of the camera trajectory. Within a Bayesian framework, it integrates observations from a deep-learning based generic object detector and landmarks from the map whose projection lie inside a detection region, to produce scale correction estimates from single frames. For each observation, a prior distribution on the height of the detected object class is used to define the observation's likelihood. Due to the scale drift inherent to monocular SLAM systems, we also incorporate a rough model on the dynamics of scale drift. Quantitative evaluations are presented on the KITTI dataset, and compared with different approaches. The results show a superior performance of our proposal in terms of relative translational error when compared to other monocular systems based on object detection.


Title: Efficient Active SLAM Based on Submap Joining, Graph Topology and Convex Optimization
Key Words: computational complexity  convex programming  least squares approximations  minimisation  mobile robots  path planning  predictive control  quadratic programming  robot vision  SLAM (robots)  graph topology  active SLAM problem  robot trajectory  area coverage task  model predictive control framework  uncertainty minimization MPC problem  graphical structure  2D feature-based SLAM  variable substitutions  convex optimization method  MPC framework  sequential quadratic programming method  linear SLAM  submap joining approach  planning  simultaneous localization and mapping  nonconvex constrained least-squares problem  Optimized production technology  Simultaneous localization and mapping  Uncertainty  Task analysis  Robot kinematics 
Abstract: The active SLAM problem considered in this paper aims to plan a robot trajectory for simultaneous localization and mapping (SLAM) as well as for an area coverage task with robot pose uncertainty. Based on a model predictive control (MPC) framework, these two problems are solved respectively by different methods. For the uncertainty minimization MPC problem, based on the graphical structure of the 2D feature-based SLAM, a non-convex constrained least-squares problem is presented to approximate the original problem. Then, using variable substitutions, it is further transformed into a convex problem, and then solved by a convex optimization method. For the coverage task considering robot pose uncertainty, it is formulated and solved by the MPC framework and the sequential quadratic programming (SQP) method. In the whole process, considering the computation complexity, we use linear SLAM, which is a submap joining approach, to reduce the time for planning and estimation. Finally, various simulations are presented to validate the effectiveness of the proposed approach.


Title: 2D SLAM Correction Prediction in Large Scale Urban Environments
Key Words: image representation  mobile robots  multilayer perceptrons  pose estimation  robot vision  SLAM (robots)  autonomous mobile robots  large scale urban environments  simultaneous location and mapping  hybrid correction module  likelihood distributions  2D likelihood SLAM approaches  successive estimated poses  Ensemble Multilayer Perceptron model  SLAM estimations  systematic errors  sensor measurement errors  SLAM map representation  observation model  motion model  probabilistic formulation  Simultaneous localization and mapping  Two dimensional displays  Estimation  Neural networks  Predictive models  Kalman filters 
Abstract: Simultaneous Localization And Mapping (SLAM) is one of the major bricks needed to build truly autonomous mobile robots. The probabilistic formulation of SLAM is based on two models: the motion model and the observation model. In practice, these models, together with the SLAM map representation, do not model perfectly the robot's real dynamics, the sensor measurement errors and the environment. Consequently, systematic errors affect SLAM estimations. In this paper, we propose two approaches to predict corrections to be applied to SLAM estimations. Both are based on the Ensemble Multilayer Perceptron model. The first approach uses successive estimated poses to predict the errors, with no assumptions on the underlying SLAM process or sensor used. The second method is specific to 2D likelihood SLAM approaches, thus, the likelihood distributions are used to predict the corrections, making this second approach independent of the sensor used. We also build a hybrid correction module based on successive estimated poses and the likelihood distributions. The validity of both approaches is evaluated through two experiments using different evaluation metrics and sensor configurations.


Title: Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition
Key Words: mobile robots  object recognition  robot vision  sensor fusion  SLAM (robots)  omnidirectional multisensory perception fusion  long-term place recognition  long-term autonomy  omnidirectional sensors  omnidirectional observation  multidirectional place recognition  omnidirectional multisensory data  appearance variations  Simultaneous Localization and Mapping  Feature extraction  Sensor phenomena and characterization  Simultaneous localization and mapping  Optimization 
Abstract: Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.


Title: Online Initialization and Automatic Camera-IMU Extrinsic Calibration for Monocular Visual-Inertial SLAM
Key Words: accelerometers  calibration  cameras  gyroscopes  inertial navigation  iterative methods  mobile robots  optimisation  robot vision  SLAM (robots)  extrinsic orientation  extrinsic translation  accelerometer bias  camera-IMU extrinsic parameters  initial values  visual scale  initialization stage  mechanical configuration  sensor suite changes  online initialization method  translation calibration  initialization procedure  gyroscope bias  monocular visual-inertial SLAM techniques  gyroscope  gravitational magnitude  Gyroscopes  Cameras  Quaternions  Accelerometers  Calibration  Simultaneous localization and mapping  Gravity 
Abstract: Most of the existing monocular visual-inertial SLAM techniques assume that the camera-IMU extrinsic parameters are known, therefore these methods merely estimate the initial values of velocity, visual scale, gravity, biases of gyroscope and accelerometer in the initialization stage. However, it's usually a professional work to carefully calibrate the extrinsic parameters, and it is required to repeat this work once the mechanical configuration of the sensor suite changes slightly. To tackle this problem, we propose an online initialization method to automatically estimate the initial values and the extrinsic parameters without knowing the mechanical configuration. The biases of gyroscope and accelerometer are considered in our method, and a convergence criteria for both orientation and translation calibration is introduced to identify the convergence and to terminate the initialization procedure. In the three processes of our method, an iterative strategy is firstly introduced to iteratively estimate the gyroscope bias and the extrinsic orientation. Secondly, the scale factor, gravity, and extrinsic translation are approximately estimated without considering the accelerometer bias. Finally, these values are further optimized by a refinement algorithm in which the accelerometer bias and the gravitational magnitude are taken into account. Extensive experimental results show that our method achieves competitive accuracy compared with the state-of-the-art with less calculation.


Title: Sonar Visual Inertial SLAM of Underwater Structures
Key Words: oceanographic techniques  SLAM (robots)  sonar  underwater sound  underwater vehicles  underwater structures  acoustic range data  sonar visual inertial SLAM  visual-inertial state estimation package  resource management  marine archaeology  underwater acoustic sensor  underwater cave  underwater wrecks  underwater domain  Sonar  Cameras  Visualization  Sonar navigation  Simultaneous localization and mapping  Underwater structures 
Abstract: This paper presents an extension to a state of the art Visual-Inertial state estimation package (OKVIS) in order to accommodate data from an underwater acoustic sensor. Mapping underwater structures is important in several fields, such as marine archaeology, search and rescue, resource management, hydrogeology, and speleology. Collecting the data, however, is a challenging, dangerous, and exhausting task. The underwater domain presents unique challenges in the quality of the visual data available; as such, augmenting the exteroceptive sensing with acoustic range data results in improved reconstructions of the underwater structures. Experimental results from underwater wrecks, an underwater cave, and a submerged bus demonstrate the performance of our approach.


Title: ContextualNet: Exploiting Contextual Information Using LSTMs to Improve Image-Based Localization
Key Words: convolution  feedforward neural nets  learning (artificial intelligence)  pose estimation  SLAM (robots)  CNN-LSTM model  pose estimation  single monocular image  Convolutional Neural Networks  image-based localization  ContextualNet  image content  indoor office space  Feature extraction  Cameras  Context modeling  Computer vision  Logic gates  Neural networks 
Abstract: Convolutional Neural Networks (CNN) have successfully been utilized for localization using a single monocular image [1]. Most of the work to date has either focused on reducing the dimensionality of data for better learning of parameters during training or on developing different variations of CNN models to improve pose estimation. Many of the best performing works solely consider the content in a single image, while the context from historical images is ignored. In this paper, we propose a combined CNN-LSTM which is capable of incorporating contextual information from historical images to better estimate the current pose. Experimental results achieved using a dataset collected in an indoor office space improved the overall system results to 0.8 m & 2.5° at the third quartile of the cumulative distribution as compared with 1.5 m & 3.0° achieved by PoseNet [1]. Furthermore, we demonstrate how the temporal information exploited by the CNN-LSTM model assists in localizing the robot in situations where image content does not have sufficient features.


Title: 3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data
Key Words: cameras  codecs  image annotation  image coding  learning (artificial intelligence)  mobile robots  object detection  pedestrians  pose estimation  robot vision  service robots  SLAM (robots)  long-term temporal information  sequence-to-sequence LSTM encoder-decoder  on-the-fly prediction  global coordinate system  T-Pose-LSTM model  human trajectory prediction  long-term mobile robot deployments  3DOF pedestrian trajectory prediction learned  Long-Term autonomous mobile robot deployment data  autonomous mobile service robots  monocular camera images  range-finder sensors  3DOF pedestrian trajectory prediction approach  temporal 3DOF-pose long-short-term memory  robust human detection  Trajectory  Cameras  Robot kinematics  Robot vision systems  Two dimensional displays  Mobile robots 
Abstract: This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.


Title: Conditional Compatibility Branch and Bound for Feature Cloud Matching
Key Words: computational complexity  image matching  SLAM (robots)  statistical distributions  tree searching  chi-square test  gating threshold  joint compatibility branch and bound  conditional compatibility branch and bound  feature cloud matching  incremental posterior joint compatibility  IPJC  FastJCBB  global optimal data association  Joint Compatibility test  JC test based search algorithm  CC test  conditional probability distribution  feature pairing  Conditional Compatibility test  Probabilistic logic  Robots  Measurement errors  Computational complexity  Probability distribution  Integrated circuits  Feature Cloud Matching  Scan Matching  Data Association  Conditional Compatibility Test 
Abstract: In this paper, we consider the problem of data association in feature cloud matching. While Joint Compatibility (JC) test is a widely adopted technique for searching the global optimal data association, it becomes less restrictive as more features are well matched. The early well-matched features contribute little to total matching cost while the gating threshold increases in the chi-square test, which allows the acceptance of bad feature pairings in the last step. In this paper, we propose the Conditional Compatibility (CC) test, which is not only more restrictive than JC test, but also probabilistically sound. The proposed test of a new feature pairing is based on the conditional probability distribution of feature locations given the early pairings. CC test can be added into any JC test based search algorithm, such as Joint Compatibility Branch and Bound (JCBB), Incremental Posterior Joint Compatibility (IPJC) and FastJCBB, without increasing much computational complexity. The more restrictive criterion of accepting a feature pairing, not only helps to reject bad associations, but also bounds the search space, which substantially improves the search efficiency. The real matching experiments justify that our algorithm produces better feature cloud matching results in a more efficient manner.


Title: Assigning Visual Words to Places for Loop Closure Detection
Key Words: image matching  image recognition  image representation  image segmentation  mobile robots  probability  robot vision  SLAM (robots)  simultaneous localization and mapping  image stream  image match  dynamic segmentation  nearest neighbor voting scheme  image descriptors  query time  on-line clustering algorithm  visual vocabulary construction  robotic applications  LCD  place recognition  loop closure detection  visual words  Visualization  Liquid crystal displays  Robots  Databases  Pipelines  Feature extraction  Vocabulary 
Abstract: Place recognition of pre-visited areas, widely known as Loop Closure Detection (LCD), constitutes one of the most important components in robotic applications, where the robot needs to estimate its pose while navigating through the field (e.g., simultaneous localization and mapping). In this paper, we present a novel approach for LCD based on the assignment of Visual Words (VWs) to particular places of the traversed path. The system operates in real time and does not require any pre-training procedure, such as visual vocabulary construction or descriptor-space dimensionality reduction. A place is defined through a dynamic segmentation of the incoming image stream and is assigned with VWs through the usage of an on-line clustering algorithm. At query time, image descriptors are converted into VWs on the map accumulating votes to the corresponding places. By means of a probability function, the mechanism is capable of identifying a loop closing candidate place. A nearest neighbor voting scheme on the descriptors' space allows the system to select the most appropriate image match at the chosen place. Geometrical and temporal consistency checks are applied on the proposed loop closing pair increasing the system's performance. Evaluation took place on several publicly available and challenging datasets offering high precision and recall scores as compared to other state-of-the-art approaches.


Title: Mapping with Dynamic-Object Probabilities Calculated from Single 3D Range Scans
Key Words: control engineering computing  laser ranging  mobile robots  navigation  neural nets  probability  robot vision  SLAM (robots)  KITTI dataset  mapping process  mapping module  dynamic object  pointwise probability  neural network  laser range data  3D grid map  navigation functions  robot perceptions  dynamic environments  safe navigation  robust navigation  autonomous robotic systems  single 3D range scans  dynamic-object probabilities  time 3.0 d  Three-dimensional displays  Cameras  Neural networks  Measurement by laser beam  Lasers  Laser beams  Image segmentation 
Abstract: Various autonomous robotic systems require maps for robust and safe navigation. Particularly when robots are employed in dynamic environments, accurate knowledge about which components of the robot perceptions belong to dynamic and static aspects in the environment can greatly improve navigation functions. In this paper we propose a novel method for building 3D grid maps using laser range data in dynamic environments. Our approach uses a neural network to estimate the pointwise probability of a point belonging to a dynamic object. The output from our network is fed to the mapping module for building a 3D grid map containing only static parts of the environment. We present experimental results obtained by training our neural network using the KITTI dataset and evaluating it in a mapping process using our own dataset. In extensive experiments, we show that maps generated using the proposed probability about dynamic objects increases the accuracy of the resulting maps.


Title: A Survey of Voxel Interpolation Methods and an Evaluation of Their Impact on Volumetric Map-Based Visual Odometry
Key Words: cameras  computational geometry  distance measurement  image reconstruction  interpolation  medical image processing  pose estimation  robot vision  SLAM (robots)  camera trajectories  trilinear interpolation method  depth-camera pose tracking  performance degradation  truncated signed distance field  voxel-based map representations  geometric interpolation methods  intermediate options  nearest neighbors  voxel volumes  volumetric map-based visual odometry  voxel interpolation methods  Interpolation  Memory management  Three-dimensional displays  Pose estimation  Extrapolation  Two dimensional displays  Image resolution 
Abstract: Voxel volumes are simple to implement and lend themselves to many of the tools and algorithms available for 2D images. However, the additional dimension of voxels may be costly to manage in memory when mapping large spaces at high resolutions. While lowering the resolution and using interpolation is common work-around, in the literature we often find that authors either use trilinear interpolation or nearest neighbors and rarely any of the intermediate options. This paper presents a survey of geometric interpolation methods for voxel-based map representations. In particular we study the truncated signed distance field (TSDF) and the impact of using fewer than 8 samples to perform interpolation within a depth-camera pose tracking and mapping scenario. We find that lowering the number of samples fetched to perform the interpolation results in performance similar to the commonly used trilinear interpolation method, but leads to higher frame-rates. We also report that lower bit-depth generally leads to performance degradation, though not as much as may be expected, with voxels containing as few as 3 bits sometimes resulting in adequate estimation of camera trajectories.


Title: Complex Urban LiDAR Data Set
Key Words: graph theory  mobile robots  optical radar  pose estimation  radar computing  SLAM (robots)  complex urban environments  light detection and ranging data set  fiber optic gyro  inertial measurement unit  Global Positioning System  vehicle pose estimation  graph simultaneous location and mapping algorithm  graph SLAM algorithm  Robot Operating System environment  raw sensor data  2D LiDAR  16-ray 3D LiDARs  LiDAR sensors  three-dimensional LiDAR  building complexes  high-rise buildings  complex urban LiDAR data set  frequency 100.0 Hz  Laser radar  Three-dimensional displays  Global Positioning System  Two dimensional displays  Sensor systems  Urban areas 
Abstract: This paper presents a Light Detection and Ranging (LiDAR) data set that targets complex urban environments. Urban environments with high-rise buildings and congested traffic pose a significant challenge for many robotics applications. The presented data set is unique in the sense it is able to capture the genuine features of an urban environment (e.g. metropolitan areas, large building complexes and underground parking lots). Data of two-dimensional (2D) and three-dimensional (3D) LiDAR, which are typical types of LiDAR sensors, are provided in the data set. The two 16-ray 3D LiDARs are tilted on both sides for maximal coverage. One 2D LiDAR faces backward while the other faces forwards to collect data of roads and buildings, respectively. Raw sensor data from Fiber Optic Gyro (FOG), Inertial Measurement Unit (IMU), and the Global Positioning System (GPS) are presented in a file format for vehicle pose estimation. The pose information of the vehicle estimated at 100 Hz is also presented after applying the graph simultaneous localization and mapping (SLAM) algorithm. For the convenience of development, the file player and data viewer in Robot Operating System (ROS) environment were also released via the web page. The full data sets are available at: http://irap.kaist.ac.kr/dataset. In this website, 3D preview of each data set is provided using WebGL.


Title: Live Structural Modeling Using RGB-D SLAM
Key Words: image colour analysis  image fusion  image texture  robot vision  SLAM (robots)  solid modelling  live structural modeling  dense point cloud  shape map  single point cloud  metric primitive modeling  RGB-D SLAM  primitive shape localization  shape fusion  Shape  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Computational modeling  History  Estimation 
Abstract: This paper presents a method for localizing primitive shapes in a dense point cloud computed by the RGB-D SLAM system. To stably generate a shape map containing only primitive shapes, the primitive shape is incrementally modeled by fusing the shapes estimated at previous frames in the SLAM, so that an accurate shape can be finally generated. Specifically, the history of the fusing process is used to avoid the influence of error accumulation in the SLAM. The point cloud of the shape is then updated by fusing the points in all the previous frames into a single point cloud. In the experimental results, we show that metric primitive modeling in texture-less and unprepared environments can be achieved online.


Title: Complexity Analysis and Efficient Measurement Selection Primitives for High-Rate Graph SLAM
Key Words: computational complexity  graph theory  iterative methods  Newton method  optimisation  SLAM (robots)  complexity analysis  globally-efficient structure  favorable global structures  Gauss-Newton iteration  factorization step  primary computational bottleneck  graph structure  existing analytic gap  quantitative metric called elimination complexity  significant computation reductions  measurement decimation  simple heuristics  aggressive pruning  significant computational savings  structurally-naïve techniques  global level  edge count  SLAM graph  graph-based SLAM  high-rate graph  efficient measurement selection primitives  Simultaneous localization and mapping  Complexity theory  Optimization  Sparse matrices  Extraterrestrial measurements  Linear algebra 
Abstract: Sparsity has been widely recognized as crucial for efficient optimization in graph-based SLAM. Because the sparsity and structure of the SLAM graph reflect the set of incorporated measurements, many methods for sparsification have been proposed in hopes of reducing computation. These methods often focus narrowly on reducing edge count without regard for structure at a global level. Such structurally-naïve techniques can fail to produce significant computational savings, even after aggressive pruning. In contrast, simple heuristics such as measurement decimation and keyframing are known empirically to produce significant computation reductions. To demonstrate why, we propose a quantitative metric called elimination complexity (EC) that bridges the existing analytic gap between graph structure and computation. EC quantifies the complexity of the primary computational bottleneck: the factorization step of a Gauss-Newton iteration. Using this metric, we show rigorously that decimation and keyframing impose favorable global structures and therefore achieve computation reductions on the order of r2/9 and r3, respectively, where r is the pruning rate. We additionally present numerical results showing EC provides a good approximation of computation in both batch and incremental (iSAM2) optimization and demonstrate that pruning methods promoting globally-efficient structure outperform those that do not.


Title: Dense Planar-Inertial SLAM with Structural Constraints
Key Words: distance measurement  image reconstruction  least squares approximations  optimisation  robot vision  SLAM (robots)  dense visual odometry estimation  planar measurements  SLAM framework  IMU biases  planar landmarks  incremental smoothing  Bayes Tree  IMU data  visual information  modeling planes  IMU states  reconstruction results  SLAM algorithms  structural constraints  DPI-SLAM system  planar-inertial SLAM system  novel dense planar-inertial SLAM  dense 3D models  indoor environments  hand-held RGB-D sensor  inertial measurement unit  preinte-grated IMU measurements  factor graph  incremental mapping  probabilistic global optimization  Simultaneous localization and mapping  Optimization  Three-dimensional displays  Real-time systems  Estimation  Visualization 
Abstract: In this work, we develop a novel dense planar-inertial SLAM (DPI-SLAM) system to reconstruct dense 3D models of large indoor environments using a hand-held RGB-D sensor and an inertial measurement unit (IMU). The preinte-grated IMU measurements are loosely-coupled with the dense visual odometry (VO) estimation and tightly-coupled with the planar measurements in a full SLAM framework. The poses, velocities, and IMU biases are optimized together with the planar landmarks in a global factor graph using incremental smoothing and mapping with the Bayes Tree (iSAM2). With odometry estimation using both RGB-D and IMU data, our system can keep track of the poses of the sensors even without sufficient planes or visual information (e.g. textureless walls) temporarily. Modeling planes and IMU states in the fully probabilistic global optimization reduces the drift that distorts the reconstruction results of other SLAM algorithms. Moreover, structural constraints between nearby planes (e.g. right angles) are added into the DPI-SLAM system, which further recovers the drift and distortion. We test our DPI-SLAM on large indoor datasets and demonstrate its state-of-the-art performance as the first planar-inertial SLAM system.


Title: Ruling the Control Authority of a Service Robot Based on Information Precision
Key Words: geriatrics  handicapped aids  mobile robots  path planning  position control  service robots  SLAM (robots)  active sensing system  control law  senior user guidance  path following problem  landmarks  actuator control  accurate localisation  exact localisation  robotic walking assistant  information precision  service robot  control authority  design strategy  massive data collection  SLAM approaches  Robot sensing systems  Estimation error  Uncertainty  Probabilistic logic  Service robots  Automobiles 
Abstract: We consider the problem of guiding a senior user along a path using a robotic walking assistant. This is a particular type of path following problem, for which most of the solutions available in the literature require an exact localisation of the robot in the environment. An accurate localisation is obtained either with a heavy infrastructure (e.g., an active sensing system deployed in the environment or deploying landmarks in known positions) or using SLAM approaches with a massive data collection. Our key observation is that the intervention of the system (and a good level of accuracy) is only required in proximity of difficult decision points, while we can rely on the user in an environment where the only possibility is just to maintain a course (e.g., a corridor). The direct implication is that we can instrument the environment with a heavy infrastructure only in certain areas. This design strategy has to be complemented by an adequate control law that shifts the authority (i.e., the control of the actuators) between the robot and the user according to the accuracy of the information available to the robot. Such a control law is exactly the contribution of this paper.


Title: Feature-constrained Active Visual SLAM for Mobile Robot Navigation
Key Words: collision avoidance  mobile robots  navigation  path planning  robot vision  SLAM (robots)  sensory constraints  iterative motion planning framework  collision avoidance  online mapping  associated map points  distance-optimal path planner  data-driven approach  continuous identification  feature-based Visual Simultaneous Localization  vision-based navigation  failure avoidance  mobile robot navigation  feature-constrained active Visual SLAM  Cameras  Navigation  Collision avoidance  Simultaneous localization and mapping  Planning 
Abstract: This paper focuses on tracking failure avoidance during vision-based navigation to a desired goal in unknown environments. While using feature-based Visual Simultaneous Localization and Mapping (VSLAM), continuous identification and association of map points are required during motion. Thus, we discuss a motion planning framework that takes into account sensory constraints for a reliable navigation. We use information available in the SLAM and propose a data-driven approach to predict the number of map points associated in a given pose. Then, a distance-optimal path planner utilizes the model to constrain paths such that the number of associated map points in each pose is above a threshold. We also include an online mapping of the environment for collision avoidance. Overall, we propose an iterative motion planning framework that enables real-time replanning after the acquisition of more information. Experiments in two environments demonstrate the performance of the proposed framework.


Title: Selection and Compression of Local Binary Features for Remote Visual SLAM
Key Words: feature extraction  feature selection  mobile robots  multi-robot systems  robot vision  SLAM (robots)  feature selection stage  remote visual SLAM  autonomous robotics  collaborative SLAM approaches  multiple robots  feature coding scheme  simultaneous localization and mapping  visual sensors  embedded devices  local binary features extraction  centralized powerful processing node  Visualization  Encoding  Simultaneous localization and mapping  Feature extraction  Task analysis  Image coding 
Abstract: In the field of autonomous robotics, Simultaneous Localization and Mapping (SLAM) is still a challenging problem. With cheap visual sensors attracting more and more attention, various solutions to the SLAM problem using visual cues have been proposed. However, current visual SLAM systems are still computationally demanding, especially on embedded devices. In addition, collaborative SLAM approaches emerge using visual information acquired from multiple robots simultaneously to build a joint map. In order to address both challenges, we present an approach for remote visual SLAM where local binary features are extracted at the robot, compressed and sent over a network to a centralized powerful processing node running the visual SLAM algorithm. To this end, we propose a new feature coding scheme including a feature selection stage which ensures that only relevant information is transmitted. We demonstrate the effectiveness of our approach on well-known datasets. With the proposed approach, it is possible to build an accurate map while limiting the data rate to 75 kbits/frame.


Title: Realtime State Estimation with Tactile and Visual Sensing. Application to Planar Manipulation
Key Words: end effectors  feedback  object detection  pose estimation  position control  robot vision  SLAM (robots)  state estimation  touch (physiological)  tactile input  visual input  incremental smoothing  visual sensing  contact sensing  end-effector  realtime state estimation  robust object state estimation  visual sensor  visual feedback  object shapes  object manipulation  incremental smoothing and mapping  iSAM  planar manipulation  object poses estimation  Robot sensing systems  Visualization  Cost function  State estimation  Cameras 
Abstract: Accurate and robust object state estimation enables successful object manipulation. Visual sensing is widely used to estimate object poses. However, in a cluttered scene or in a tight workspace, the robot's end-effector often occludes the object from the visual sensor. The robot then loses visual feedback and must fall back on open-loop execution. In this paper, we integrate both tactile and visual input using a framework for solving the SLAM problem, incremental smoothing and mapping (iSAM), to provide a fast and flexible solution. Visual sensing provides global pose information but is noisy in general, whereas contact sensing is local, but its measurements are more accurate relative to the end-effector. By combining them, we aim to exploit their advantages and overcome their limitations. We explore the technique in the context of a pusher-slider system. We adapt iSAM's measurement cost and motion cost to the pushing scenario, and use an instrumented setup to evaluate the estimation quality with different object shapes, on different surface materials, and under different contact modes.


