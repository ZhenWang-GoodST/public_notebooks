total paper: 146
Title: Ultra-Wideband Radar for Robust Inspection Drone in Underground Coal Mines
Key Words: coal  dust  inspection  mining  mining industry  personnel  safety  ultra wideband radar  walls  ultra-wideband radar  robust inspection drone  underground coal mine  human workers  autonomous inspection drone  coal dust  robust sensing solution  safety risk  Coal mining  Ultra wideband radar  Robustness  Drones  Coal  Sensors 
Abstract: Coal mines pose a high safety risk for human workers. An autonomous inspection drone would enable a coal mine operation to reduce this risk by minimizing the time spent by workers inside the mine. This inspection drone must be highly robust to the harsh and dangerous environment of an underground coal mine, with high levels of coal dust and humidity that can obstruct many conventional sensing methods. For high functionality, the drone must sense and avoid potential obstacles and as well as inspect and map the mining wall face. The objective of this paper is to present ultra-wideband (UWB) radar as a robust sensing solution to this challenging environment and validate its performance experimentally in the typical coal mine environment, both statically and dynamically.


Title: iMag: Accurate and Rapidly Deployable Inertial Magneto-Inductive Localisation
Key Words: Global Positioning System  sensor placement  SLAM (robots)  wireless sensor networks  inertial magneto-inductive localisation  short-term construction work  iMag  robust simultaneous localisation  inertial measurement units  Transmitters  Robustness  Simultaneous localization and mapping  Magnetic resonance imaging  Distortion  Trajectory  Magneto-inductive device  Inertial measurements  Localisation  SLAM 
Abstract: Localisation is of importance for many applications. Our motivating scenarios are short-term construction work and emergency rescue. Not only is accuracy necessary, these scenarios also require rapid setup and robustness to environmental conditions. These requirements preclude the use of many traditional methods e.g. vision-based, laser-based, Ultra-wide band (UWB) and Global Positioning System (GPS)-based localisation systems. To solve these challenges, we introduce iMag, an accurate and rapidly deployable inertial magneto-inductive (MI) localisation system. It localises monitored workers using a single MI transmitter and inertial measurement units with minimal setup effort. However, MI location estimates can be distorted and ambiguous. To solve this problem, we suggest a novel method to use MI devices for sensing environmental distortions, and use these to correctly close inertial loops. By applying robust simultaneous localisation and mapping (SLAM), our proposed localisation method achieves excellent tracking accuracy, and can improve performance significantly compared with only using an inertial measurement unit (IMU) and MI device for localisation.


Title: Manipulating Highly Deformable Materials Using a Visual Feedback Dictionary
Key Words: feature extraction  manipulators  mobile robots  robot vision  visual servoing  complex physical properties  autonomous robotic manipulation systems  visual feedback dictionary-based method  deformable objects  visual servoing  RGB sensor stream  deformable model features  histogram features  high-level representations  deformable material  manipulation data  robotic end-effectors  complex manipulation tasks  human-robot manipulation tasks  material characteristics  deformable materials  Visualization  Dictionaries  Feature extraction  Task analysis  Visual servoing  Deformable models 
Abstract: The complex physical properties of highly deformable materials such as clothes pose significant challenges for autonomous robotic manipulation systems. We present a novel visual feedback dictionary-based method for manipulating deformable objects towards a desired configuration. Our approach is based on visual servoing and we use an efficient technique to extract key features from the RGB sensor stream in the form of a histogram of deformable model features. These histogram features serve as high-level representations of the state of the deformable material. Next, we collect manipulation data and use a visual feedback dictionary that maps the velocity in the high-dimensional feature space to the velocity of the robotic end-effectors for manipulation. We have evaluated our approach on a set of complex manipulation tasks and human-robot manipulation tasks on different cloth pieces with varying material characteristics.


Title: Online Safe Trajectory Generation for Quadrotors Using Fast Marching Method and Bernstein Basis Polynomial
Key Words: autonomous aerial vehicles  clutter  computational geometry  convex programming  helicopters  indoor environment  mobile robots  path planning  polynomials  search problems  state estimation  trajectory control  online safe trajectory generation  Bernstein basis polynomial  onboard state estimation  velocity field  Euclidean signed distance field  time allocation  flight corridor  piecewise Bézier curves  outdoor environments  convex programs  autonomous navigation  marching-based path searching method  light-weight quadrotor platform  online quadrotor motion planning  cluttered indoor environments  ESDF  open-source package  Resource management  Planning  Trajectory optimization  Safety  Autonomous robots 
Abstract: In this paper, we propose a framework for online quadrotor motion planning for autonomous navigation in unknown environments. Based on the onboard state estimation and environment perception, we adopt a fast marching-based path searching method to find a path on a velocity field induced by the Euclidean signed distance field (ESDF) of the map, to achieve better time allocation. We generate a flight corridor for the quadrotor to travel through by inflating the path against the environment. We represent the trajectory as piecewise Bézier curves by using Bernstein polynomial basis and formulate the trajectory generation problem as typical convex programs. By using Bézier curves, we are able to bound positions and higher order dynamics of the trajectory entirely within safe regions. The proposed motion planning method is integrated into a customized light-weight quadrotor platform and is validated by presenting fully autonomous navigation in unknown cluttered indoor and outdoor environments. We also release our code for trajectory generation as an open-source package.


Title: Multilayered Kinodynamics Simulation for Detailed Whole-Body Motion Generation and Analysis
Key Words: biomedical measurement  fracture  gait analysis  injuries  kinematics  Monte Carlo methods  whole-body motions  whole-body motion generation  vertical contact force  human motion mechanisms  whole-body human model  MLKD Sim  multilayered kinodynamics simulation  Dynamics  Computational modeling  Force  Legged locomotion  Kinematics  Data models  Analytical models 
Abstract: This study generates and analyzes unsafe human motions that cannot be measured experimentally in laboratories with dynamic consistency. Detailed whole-body motions are generated by a multilayered kinodynamics simulation (MLKD Sim) that uses a detailed digital whole-body human model and a simple motion-representation model that parametrically represents human motion mechanisms. First, we develop the simple motion-representation model that represents human motion and contact force data that are experimentally measured in a laboratory, and we identify this model's parameters based on these experimental data. Forward dynamics computation of this motion-representation model with changing model and/or environmental parameters simulates motion modification as well as a contact force with dynamic consistency. Finally, the mapping function from the motion-representation model's motion to the detailed motion identified from the experimental data is used to reconstruct the detailed whole-body motion. MLKD Sim reconstructs a vertical contact force with average error of 2.18E+02 N, center of mass trajectory with average error of 3.31E-02 m, ankle joint angle with average error of 1.11E-01 rad (2.95E+00%), and ankle joint torque with average error of 6.13E+01 Nm (1.93E+01%). Unsafe motion simulation results show that the physical load on the hip, knee, and ankle joints increases by 9.23E+01%, 5.42E+02%, and 1.45E+02% respectively with 0.5-m level difference in a running surface. These results imply that when sprinting in an unknown environment, we need to protect, in order, the knee ankle, and hip joints. This study conducts detailed dynamics and kinematics analysis of unsafe human motions that cannot be measured experimentally in laboratories to prevent injuries, falls, and fatigue, and these results should find applications in the fields of medicine and welfare.


Title: Deep Forward and Inverse Perceptual Models for Tracking and Prediction
Key Words: deconvolution  feedforward neural nets  image sensors  Kalman filters  learning (artificial intelligence)  mobile robots  nonlinear filters  state estimation  high-dimensional images  deconvolutional methods  robotic system  robot trajectories  convolutional neural network model  photo-realistic images  image generation  video frames  perceptual model  robotics  inverse models  inverse perceptual models  Predictive models  Inverse problems  Robot sensing systems  Kinematics  Training 
Abstract: We consider the problems of learning forward models that map state to high-dimensional images and inverse models that map high-dimensional images to state in robotics. Specifically, we present a perceptual model for generating video frames from state with deep networks, and provide a framework for its use in tracking and prediction tasks. We show that our proposed model greatly outperforms standard deconvolutional methods and GANs for image generation, producing clear, photo-realistic images. We also develop a convolutional neural network model for state estimation and compare the result to an Extended Kalman Filter to estimate robot trajectories. We validate all models on a real robotic system.


Title: Local Behavior-Based Navigation in Rough Off-Road Scenarios Based on Vehicle Kinematics
Key Words: mobile robots  off-road vehicles  path planning  robot dynamics  robot kinematics  trajectory control  elevation grid map  robot orientation  behavior-based control paradigm  rough terrains  traversability  occupancy maps  on-road local navigation approaches  trajectory candidates  behavior-based local navigation approach  vehicle kinematics  rough off-road scenarios  Wheels  Navigation  Trajectory  Robot sensing systems  Axles  Three-dimensional displays  robotics  off-road navigation  behavior-based control  tentacles 
Abstract: This paper describes a novel behavior-based local navigation approach for rough off-road scenarios. Trajectory candidates are generated based on vehicle kinematics and dynamics as well as the desired global trajectory. In contrast to on-road local navigation approaches, the work at hand proposes the use of a shiftable elevation grid map instead of occupancy maps since traversability in rough terrains does not only depend on location, but also on the robot's orientation. The traversability is evaluated by determining tire contact points with the terrain to take various different safety and efficiency aspects like underbody collisions and rollover risk into account. By exploiting the behavior-based control paradigm, the navigation approach can be easily extended and its robustness is shown in experimental evaluations using an Unimog U5023.


Title: Dynamic Simulation of Planetary Rovers with Terrain Property Mapping * Research supported by National Natural Science Foundation of China (Grant No. 61370033), National Basic Research Program of China (Grant No. 2013CB035502), Foundation for Innovative Research Groups of the Natural Science Foundation of China (Grant No. 51521003), Foundation of Chinese State Key Laboratory of Robotics and Systems (Grant No. SKLRS201501B, SKLRS20164B), Harbin Applied Technology Project of Research and Development (2015RQQXJ081), and the “111 Project” (Grant No. B07018).
Key Words: cartography  control engineering computing  geometry  Mars  mobile robots  planetary rovers  soil  wheels  digital elevation map  terrain physical properties  terrain pressure-sinkage property  contact model  soil  terramechanics model  shearing property  friction angle  Mars exploration  complex terrains  terrain property mapping  planetary rovers  dynamic simulation  three-wheel-rover  Soil  Mathematical model  Wheels  Stress  Computational modeling  Shearing  Force 
Abstract: Simulation of planetary rovers moving on complex terrains is critical for Mars exploration. Equivalent stiffness is proposed and used to characterize the pressure-sinkage property of terrain, while friction angle to characterize the shearing property. Terramechanics model for calculating forces between rigid wheel and soil is proved to be the same with that contact model for calculating forces between rigid wheel and rock. A Digital Elevation Map with Physical Properties is developed and applied to simulate terrain physical properties along with its geometry information. The established methods are validated using simulation and experimental tests with a three-wheel-rover.


Title: Learning Place-and-Time-Dependent Binary Descriptors for Long-Term Visual Localization
Key Words: computer vision  natural scenes  pose estimation  pose change  place-and-time-dependent binary descriptor  GRIEF evolution algorithm  correspondence generation  single-experience Visual Teach and Repeat system  localization failures  natural scene changes  vision-based navigation  long-term visual localization  extreme illumination changes  binary descriptors  single descriptor scheme  adaptive descriptor  descriptor generation  long-term seasonal variations  short-term illumination changes  Lighting  Visualization  Latches  Navigation  Robustness  Measurement  Evolutionary computation 
Abstract: Vision-based navigation is extremely susceptible to natural scene changes. This can result in localization failures in less than a few hours after map creation. To combat short-term illumination changes as well as long-term seasonal variations, we propose using a place-and-time-dependent binary descriptor that adapts to different scenarios in an online fashion. This is achieved by extending the GRIEF [6] evolution algorithm in two ways: correspondence generation using a known pose change and the inclusion of LATCH triplets in addition to BRIEF comparisons for descriptor generation. We show the adaptive descriptor outperforms a single descriptor scheme for localization within a single-experience Visual Teach and Repeat (VT&R) system while maintaining the efficiency of binary descriptors. By adapting the description function to different environmental conditions, it allows the system to operate for a longer period before a new experience is required. In the presence of extreme illumination changes from day to night, we obtain 40% more inlier matches compared to SURF. In the case of seasonal variations, a 70% increase is demonstrated. The increased correspondences result in more localizable sections along the paths, amounting to a 25% and 150% increase in the lighting and seasonal cases, respectively.


Title: Cubic Range Error Model for Stereo Vision with Illuminators
Key Words: cameras  image sensors  robot vision  stereo image processing  telecommunication scheduling  cubic range error model  stereo vision  low-cost depth sensors  stereo camera setup  robotics  augmented reality  map generation  sensor scheduling policy  multisensor setup  range error models  uncertainty estimates  range measurements  integrated illuminators  off-the-shelf structured light stereo system  Cameras  Robot sensing systems  Uncertainty  Lighting  Geometry 
Abstract: Use of low-cost depth sensors, such as a stereo camera setup with illuminators, is of particular interest for numerous applications ranging from robotics and transportation to mixed and augmented reality. The ability to quantify noise is crucial for these applications, e.g., when the sensor is used for map generation or to develop a sensor scheduling policy in a multi-sensor setup. Range error models provide uncertainty estimates and help weigh the data correctly in instances where range measurements are taken from different vantage points or with different sensors. Such a model is derived in this work. We show that the range error for stereo systems with integrated illuminators is cubic and validate the proposed model experimentally with an off-the-shelf structured light stereo system. The experiments confirm the validity of the model and simplify the application of this type of sensor in robotics.


Title: Fusion of Stereo and Still Monocular Depth Estimates in a Self-Supervised Learning Context
Key Words: feedforward neural nets  image colour analysis  intelligent robots  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  monocular depth estimates  autonomous robots  self-supervised learning setup  stereo vision depth  convolutional neural network  fusion method  CNN estimates  autonomous navigation  depth estimation  self-supervised learning  Estimation  Stereo vision  Robot sensing systems  Cameras  Training  Self-supervised learning  monocular depth estimation  stereo vision  convolutional neural networks 
Abstract: We study how autonomous robots can learn by themselves to improve their depth estimation capability. In particular, we investigate a self-supervised learning setup in which stereo vision depth estimates serve as targets for a convolutional neural network (CNN) that transforms a single still image to a dense depth map. After training, the stereo and mono estimates are fused with a novel fusion method that preserves high confidence stereo estimates, while leveraging the CNN estimates in the low-confidence regions. The main contribution of the article is that it is shown that the fused estimates lead to a higher performance than the stereo vision estimates alone. Experiments are performed on the KITTI dataset, and on board of a Parrot SLAMDunk, showing that even rather limited CNNs can help provide stereo vision equipped robots with more reliable depth maps for autonomous navigation.


Title: Robust Visual Localization for Hopping Rovers on Small Bodies
Key Words: cameras  planetary rovers  pose estimation  robot vision  SLAM (robots)  space vehicles  visual SLAM algorithms  ORB-SLAM2  orbiting primary spacecraft  onboard visual simultaneous localization and mapping  visual SLAM implementation  wide field of view camera  off-nadir camera pointing angles  narrow FOV camera  orbiting spacecraft  visual appearance  high-contrast shadows  hopping rover  illumination angles  Solar System bodies  collaborative visual localization method  robust visual localization  time 1.0 hour to 12.0 hour  Cameras  Space vehicles  Visualization  Simultaneous localization and mapping  Optimization  Lighting  Solar system 
Abstract: We present a collaborative visual localization method for rovers designed to hop and tumble across the surface of small Solar System bodies, such as comets and asteroids. In a two-phase approach, an orbiting primary spacecraft first maps the surface of a body by capturing images from various poses and illumination angles; these images are processed to create a prior map of 3D landmarks. In the second phase, a hopping rover is deployed to the surface where it uses a camera to relocalize to the prior map and to perform onboard visual simultaneous localization and mapping (SLAM). Small bodies present several unique challenges to existing visual SLAM algorithms, such as high-contrast shadows that move quickly over the surface due to the short (e.g. 1-12 hour) rotational periods, and large changes in visual appearance between orbit and the surface, where image scale varies by many orders of magnitude (kilometers to centimeters). In this work, we describe how to augment ORB-SLAM2-a state of the art visual SLAM implementation-to handle large variations in illumination by fusing prior images with varying illumination angles. We demonstrate how a hopping rover can use a wide field of view (FOV) camera to relocalize to prior maps captured by an orbiting spacecraft with a narrow FOV camera, and how the growth of pose and scale errors can be bounded by periodic loop closures during large hops. The proposed method is evaluated with sequences of images captured around a mock asteroid; it is shown to be robust to varying illumination angles, scene scale changes, and off-nadir camera pointing angles.


Title: Bounding Drift in Cooperative Localisation Through the Sharing of Local Loop Closures
Key Words: graph theory  mobile robots  robot vision  SLAM (robots)  direct intervehicle observations  fuses single vehicle SLAM  cooperative localisation  data association  map data  local subgraphs  shared states  localisation accuracy  bounding drift  local loop closures  robotic scenarios  data consistency  bandwidth limitations  single vehicle visual SLAM framework  Information matrix  Simultaneous localization and mapping  Bandwidth  Jacobian matrices  Visualization  Message systems 
Abstract: Handling loop closures and intervehicle observations in cooperative robotic scenarios remains a challenging problem due to data consistency, bandwidth limitations and increased computation requirements. This paper develops a general cooperative localisation and single vehicle Visual SLAM framework that includes direct intervehicle observations and pose to pose loop closures on each vehicle with states shared as required. This fuses single vehicle SLAM with cooperative localisation and avoids data association of map data across limited communication networks. The base problem is developed as a factor graph with each vehicle solving local subgraphs that are split based on intervehicle observations. We modify the order of variable elimination in subgraphs through manipulation of the square-root of the Information matrix to extract updates that include the historic states involved in the loop closures and do not require transmission of other states not involved in the measurement or retransmission of previously shared states. We demonstrate the effect on localisation accuracy and bandwidth using data captured from a set of five robots observing each other and landmarks compared to both single vehicle SLAM, pure cooperative localisation and a centralised solution.


Title: Detection and Resolution of Motion Conflict in Visual Inertial Odometry
Key Words: distance measurement  inertial navigation  motion estimation  motion conflict detection  motion estimation  motion conflict resolution  visual-inertial odometry  Motion Conflict aware Visual Inertial Odometry  Visualization  Cameras  Estimation  Robustness  Simultaneous localization and mapping  Hidden Markov models 
Abstract: In this paper, we present a novel method to detect and resolve motion conflicts in visual-inertial odometry. Recently, it has been common to integrate an IMU sensor with visual odometry in order to improve localization accuracy and robustness. However, when a disagreement between the two sensor modalities occurs, the localization accuracy reduces drastically and leads to irreversible errors. In such conditions, multiple motion estimates based on the set of observations used are possible. This creates a conflict (motion conflict) in determining which observations to use for accurate ego-motion estimation. Therefore, we present a method to detect motion conflicts based on per-frame positional estimate discrepancy and per-landmark reprojection errors. Additionally, we also present a method to resolve motion conflicts by eliminating inconsistent IMU and landmark measurements. Finally, we implement Motion Conflict aware Visual Inertial Odometry (MC-VIO) by combining both detection and resolution of motion conflicts. We perform quantitative and qualitative evaluation of MC-VIO on visually and inertially challenging datasets. Experimental results indicate that the MC-VIO algorithm reduces the increase in absolute trajectory error by 80% and the relative pose error by 60% for scenes with motion conflict, in comparison to the state-of-the-art reference VIO algorithm.


Title: Predicting Alignment Risk to Prevent Localization Failure
Key Words: feature extraction  image registration  SLAM (robots)  cluttered man-made environments  geometric constraints  spatial overlap  failed alignment  point cloud content  laser-based localization failure  geometric features  point cloud registration  alignment risk  Three-dimensional displays  Cloud computing  Robot sensing systems  Measurement  Iterative closest point algorithm  Octrees 
Abstract: During localization and mapping the success of point cloud registration can be compromised when there is an absence of geometric features or constraints in corridors or across doorways, or when the volumes scanned only partly overlap, due to occlusions or constrictions between subsequent observations. This work proposes a strategy to predict and prevent laser-based localization failure. Our solution relies on explicit analysis of the point cloud content prior to registration. A model predicting the risk of a failed alignment is learned by analysing the degree of spatial overlap between two input point clouds and the geometric constraints available within the region of overlap. We define a novel measure of alignability for these constraints. The method is evaluated against three real-world datasets and compared to baseline approaches. The experiments demonstrate how our approach can help improve the reliability of laser-based localization during exploration of unknown and cluttered man-made environments.


Title: Adversarial Training for Adverse Conditions: Robust Metric Localisation Using Appearance Transfer
Key Words: feature extraction  image filtering  image recognition  adversarial training  adverse conditions  robust metric localisation  appearance transfer  visual place recognition  invertable generator  image transforming filter  feature-matching  dense descriptor maps  output synthetic images  input RGB image  generated images  multiple traversals  reliable localisation  Generators  Detectors  Measurement  Feature extraction  Computer architecture  Training  Pipelines 
Abstract: We present a method of improving visual place recognition and metric localisation under very strong appearance change. We learn an invertable generator that can transform the conditions of images, e.g. from day to night, summer to winter etc. This image transforming filter is explicitly designed to aid and abet feature-matching using a new loss based on SURF detector and dense descriptor maps. A network is trained to output synthetic images optimised for feature matching given only an input RGB image, and these generated images are used to localize the robot against a previously built map using traditional sparse matching approaches. We benchmark our results using multiple traversals of the Oxford RobotCar Dataset over a year-long period, using one traversal as a map and the other to localise. We show that this method significantly improves place recognition and localisation under changing and adverse conditions, while reducing the number of mapping runs needed to successfully achieve reliable localisation.


Title: Learning Sensor Feedback Models from Demonstrations via Phase-Modulated Neural Networks
Key Words: adaptive control  feedback  humanoid robots  learning (artificial intelligence)  mobile robots  path planning  radial basis function networks  tactile sensors  phase-modulated neural networks  feedback model maps  motion plan adaptation  radial basis function network structure  tactile sensor traces  data-driven framework  anthropomorphic robot  Robot sensing systems  Adaptation models  Task analysis  Mathematical model  Neural networks  Quaternions 
Abstract: In order to robustly execute a task under environmental uncertainty, a robot needs to be able to reactively adapt to changes arising in its environment. The environment changes are usually reflected in deviation from expected sensory traces. These deviations in sensory traces can be used to drive the motion adaptation, and for this purpose, a feedback model is required. The feedback model maps the deviations in sensory traces to the motion plan adaptation. In this paper, we develop a general data-driven framework for learning a feedback model from demonstrations. We utilize a variant of a radial basis function network structure -with movement phases as kernel centers- which can generally be applied to represent any feedback models for movement primitives. To demonstrate the effectiveness of our framework, we test it on the task of scraping on a tilt board. In this task, we are learning a reactive policy in the form of orientation adaptation, based on deviations of tactile sensor traces. As a proof of concept of our method, we provide evaluations on an anthropomorphic robot.


Title: Relocalization, Global Optimization and Map Merging for Monocular Visual-Inertial SLAM
Key Words: cameras  distance measurement  graph theory  inertial navigation  mobile robots  optimisation  path planning  pose estimation  robot vision  SLAM (robots)  relocalization  global optimization  monocular visual-inertial SLAM  visual-inertial system  low-cost inertial measurement unit  state estimation  visual-inertial odometry  absolute pose estimation  visual-inertial SLAM system  global pose graph optimization  map merging ability  map reuse  pose graph optimization  Cameras  Optimization  Visualization  Feature extraction  Microsoft Windows  Simultaneous localization and mapping  Real-time systems 
Abstract: The monocular visual-inertial system (VINS), which consists one camera and one low-cost inertial measurement unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate visual-inertial odometry is prone to drift and cannot provide absolute pose estimation. Leveraging history information to relocalize and correct drift has become a hot topic. In this paper, we propose a monocular visual-inertial SLAM system, which can relocalize camera and get the absolute pose in a previous-built map. Then 4-DOF pose graph optimization is performed to correct drifts and achieve global consistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual drifted direction in the visual-inertial system. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. Current map and previous map can be merged together by the global pose graph optimization. We validate the accuracy of our system on public datasets and compare against other state-of-the-art algorithms. We also evaluate the map merging ability of our system in the large-scale outdoor environment. The source code of map reuse is integrated into our public code, VINS-Monol11https://github.com/HKUST-Aerial-Robotics/VINS-Mono.


Title: Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM
Key Words: image reconstruction  mobile robots  optical radar  optimisation  probability  radar imaging  robot vision  sensor fusion  SLAM (robots)  dense map-centric continuous-time SLAM  CT-SLAM  computational complexity  surfel fusion  global batch trajectory optimization  probabilistic surface element fusion  map deformation  global trajectory optimization  Continuous-Time SLAM  global batch optimization  multimodal sensor fusion  continuous-time trajectory representation  elastic LiDAR fusion  Laser radar  Trajectory optimization  Simultaneous localization and mapping  Strain  Interpolation 
Abstract: The concept of continuous-time trajectory representation has brought increased accuracy and efficiency to multi-modal sensor fusion in modern SLAM. However, regardless of these advantages, its offline property caused by the requirement of global batch optimization is critically hindering its relevance for real-time and life-long applications. In this paper, we present a dense map-centric SLAM method based on a continuous-time trajectory to cope with this problem. The proposed system locally functions in a similar fashion to conventional Continuous-Time SLAM (CT-SLAM). However, it removes the need for global trajectory optimization by introducing map deformation. The computational complexity of the proposed approach for loop closure does not depend on the operation time, but only on the size of the space it explored before the loop closure. It is therefore more suitable for long term operation compared to the conventional CT-SLAM. Furthermore, the proposed method reduces uncertainty in the reconstructed dense map by using probabilistic surface element (surfel) fusion. We demonstrate that the proposed method produces globally consistent maps without global batch trajectory optimization, and effectively reduces LiDAR noise by surfel fusion.


Title: Vehicle Detection, Tracking and Behavior Analysis in Urban Driving Environments Using Road Context
Key Words: driver information systems  learning (artificial intelligence)  object detection  object tracking  optical radar  real-time systems  road traffic  road vehicles  sensor fusion  traffic engineering computing  2D Lidar  deep learning  vehicle tracking  Lidar sensor fusion  global map coordinate system  track management  data association  high precision range estimation  monocular camera  robust fusion system  tracking system  real-time vehicle detection  road context  urban driving environments  behavior analysis  Roads  Laser radar  Sensor fusion  Robot sensing systems  Vehicle detection  Estimation  Autonomous vehicles 
Abstract: We present a real-time vehicle detection and tracking system to accomplish the complex task of driving behavior analysis in urban environments. We propose a robust fusion system that combines a monocular camera and a 2D Lidar. This system takes advantage of three key components: robust vehicle detection using deep learning techniques, high precision range estimation from Lidar, and road context from the prior map knowledge. The camera and Lidar sensor fusion, data association and track management are all performed in the global map coordinate system by taking into account the sensors' characteristics. Lastly, behavior reasoning is performed by examining the tracked vehicle states in the lane coordinate system in which the road context is encoded. We validated our approach by tracking a leading vehicle while it performed usual urban driving behaviors such as lane keeping, stop-and-go at intersections, lane changing, overtaking and turning. The leading vehicle was tracked consistently throughout the 2.3 km route and its behavior was classified reliably.


Title: Encoder-Camera-Ground Penetrating Radar Tri-Sensor Mapping for Surface and Subsurface Transportation Infrastructure Inspection
Key Words: automatic optical inspection  cameras  graph theory  ground penetrating radar  image fusion  image reconstruction  optimisation  pose estimation  structural engineering computing  transportation  GPR  wheel encoder  sensing suite  data collection scheme  ALs  types data streams  camera images  data fusion  sensory data  sensor fusion approach  encoder-camera-ground penetrating radar tri-sensor mapping  subsurface transportation infrastructure inspection  algorithmic development  multiple sensors  multimodal mapping  Ground penetrating radar  Cameras  Inspection  Synchronization  Robot sensing systems  Three-dimensional displays 
Abstract: We report system and algorithmic development for a sensing suite comprising multiple sensors for both surface and subsurface transportation infrastructure inspection focusing on multi-modal mapping for inspection. The sensing suite contains a camera, a ground penetrating radar (GPR), and a wheel encoder. We design the sensing suite and propose a data collection scheme using customized artificial landmarks (ALs). We use ALs to synchronize two types data streams: camera images that are temporally evenly-spaced and GPR/encoder data that are spatially evenly-spaced. We also employ pose graph optimization with synchronization as penalty functions to further refine synchronization and perform data fusion for 3D reconstruction. We have implemented the system and tested it in physical experiments. The results show that our system successfully fuses three sensory data and product metric 3D reconstruction. The sensor fusion approach reduces the end-to-end distance error from 7.45cm to 3.10cm.


Title: A Geometric and Unified Approach for Modeling Soft-Rigid Multi-Body Systems with Lumped and Distributed Degrees of Freedom
Key Words: elasticity  geometry  inverse problems  Newton method  robot dynamics  discrete Cosserat approach  soft-body dynamics  geometric theory  recursive Newton-Euler algorithm  forward dynamic problems  soft robots  soft-rigid multibody systems  inverse problems  linear complexity  Kinematics  Fasteners  Strain  Heuristic algorithms  Algebra  Soft robotics 
Abstract: In this paper, a geometric and unified model of soft-rigid multi-body systems is presented, based on a discrete Cosserat approach of the soft-body dynamics. The model is in fact a generalization to soft and hybrid systems of the geometric theory of rigid robotics characterized by the exponential map. A generalization of the recursive Newton-Euler algorithm is also presented, able to solve inverse and forward dynamic problems with linear O(N) complexity. The proposed model provides several improvements with respect to the existing flexible multi-body models, which make it particularly suitable to study the dynamics of modern soft robots as shown for a multi-body system inspired by motile bacteria.


Title: Indoor Coverage Path Planning: Survey, Implementation, Analysis
Key Words: mobile robots  path planning  service robots  trajectory control  indoor coverage path planning  robot trajectories  mobile cleaning robots  lawn mowing robots  harvesting machines  Path planning  Cleaning  Robot sensing systems  Planning  Traveling salesman problems 
Abstract: Coverage Path Planning (CPP) describes the process of generating robot trajectories that fully cover an area or volume. Applications are, amongst many others, mobile cleaning robots, lawn mowing robots or harvesting machines in agriculture. Many approaches and facets of this problem have been discussed in literature but despite the availability of several surveys on the topic there is little work on quantitative assessment and comparison of different coverage path planning algorithms. This paper analyzes six popular off-line coverage path planning methods, applicable to previously recorded maps, in the setting of indoor coverage path planning on room-sized units. The implemented algorithms are thoroughly compared on a large dataset of over 550 rooms with and without furniture.


Title: Robot Navigation in Complex Workspaces Using Harmonic Maps
Key Words: geometry  mobile robots  motion control  path planning  robot vision  SLAM (robots)  complex workspaces  harmonic maps  Artificial Potential Fields  autonomous robot navigation control schemes  local minima  APF based control scheme  goal configuration  multiply connected compact 2D workspaces  Harmonic analysis  Navigation  Robot kinematics  Convergence  Tuning  Trajectory 
Abstract: Artificial Potential Fields (APFs) constitute an intuitive tool for designing autonomous robot navigation control schemes, though they generally suffer from the existence of local minima which may trap the robot away from its desired configuration, an issue usually addressed by appropriate offline “tuning” of the potential field's parameters. On the other side, most APF based approaches rely on a diffeomorphism to sphere worlds to handle realistic scenarios, which may be either costly to compute (e.g., conformal mappings) or requires some sort of preconditioning of the workspace (e.g., decomposition of complex geometries to simple elementary components). In this work, we first propose a constructive procedure to map multiply connected compact 2D workspaces to one or more punctured disks based on harmonic maps. Subsequently, we design an APF based control scheme along with an adaptive law for its parameters that requires no offline tuning to guarantee safe convergence to its goal configuration. Finally, an extensive simulation study is conducted to demonstrate the efficacy of the proposed control scheme.


Title: Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping
Key Words: image fusion  image reconstruction  image segmentation  learning (artificial intelligence)  multiview frame fusion technique  perceived 3D structure  semantic labelling task  human interaction  verbal references  location related services  multiview 3D entangled forest  semantic maps  offline reconstruction  single frames  online multiview semantic segmentation  batch approach  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Forestry  Labeling  Context modeling  Standards 
Abstract: Applications that provide location related services need to understand the environment in which humans live such that verbal references and human interaction are possible. We formulate this semantic labelling task as the problem of learning the semantic labels from the perceived 3D structure. In this contribution we propose a batch approach and a novel multi-view frame fusion technique to exploit multiple views for improving the semantic labelling results. The batch approach works offline and is the direct application of an existing single-view method to scene reconstructions with multiple views. The multi-view frame fusion works in an incremental fashion accumulating the single-view results, hence allowing the online multi-view semantic segmentation of single frames and the offline reconstruction of semantic maps. Our experiments show the superiority of the approaches based on our fusion scheme, which leads to a more accurate semantic labelling.


Title: Semantic Labeling of Indoor Environments from 3D RGB Maps
Key Words: data mining  feature extraction  geometry  image classification  image colour analysis  image reconstruction  image sensors  indoor navigation  learning (artificial intelligence)  mobile robots  object detection  object recognition  robot vision  SLAM (robots)  stereo image processing  semantic labels assignment  rooms reconstruction  deep-learning techniques  virtual RGB views  geometric analysis  object detection  scene classification  room types  3D RGB maps  indoor environments  semantic labeling  Semantics  Labeling  Three-dimensional displays  Robots  Training  Task analysis  Solid modeling 
Abstract: We present an approach to automatically assign semantic labels to rooms reconstructed from 3D RGB maps of apartments. Evidence for the room types is generated using state-of-the-art deep-learning techniques for scene classification and object detection based on automatically generated virtual RGB views, as well as from a geometric analysis of the map's 3D structure. The evidence is merged in a conditional random field, using statistics mined from different datasets of indoor environments. We evaluate our approach qualitatively and quantitatively and compare it to related methods.


Title: SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud
Key Words: computer games  feedforward neural nets  image classification  image segmentation  learning (artificial intelligence)  object detection  optical radar  pattern clustering  Grand Theft Auto  video game  autonomous driving  road-objects  semantic segmentation  3D LiDAR point cloud  real-time road-object segmentation  recurrent CRF  realistic training data  LiDAR simulator  extra training data  3D bounding boxes  point-wise segmentation labels  CNN model  instance-level labels  point-wise label map  transformed LiDAR point cloud  convolutional neural networks  SqueezeSeg  end-to-end pipeline  point-wise classification problem  LiDAR point clouds  time 8.2 ms to 9.2 ms  time 3.0 d  Three-dimensional displays  Laser radar  Computational modeling  Pipelines  Autonomous vehicles  Semantics  Clustering algorithms 
Abstract: We address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a point-wise classification problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7±0.5 ms per frame), highly desirable for autonomous driving. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code is open-source released1. The paper is accompanied by a video2 containing a high level introduction and demonstrations of this work.


Title: Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments
Key Words: cameras  distance measurement  feature extraction  image classification  image sensors  learning (artificial intelligence)  mobile robots  motion estimation  object detection  pose estimation  robot vision  SLAM (robots)  stereo image processing  self-supervised distractor learning  robust monocular visual odometry  self-supervised approach  distractors  camera images  cluttered urban environments  per-pixel ephemerality mask  depth map  deep convolutional network  monocular visual odometry pipeline  sparse features  dense photometric matching  metric-scale VO  single camera  robust VO methods  odometry drift  egomotion estimation  moving vehicles  urban traffic  vehicle motion  ephemerality  offline multisession mapping approaches  Three-dimensional displays  Cameras  Robustness  Visual odometry  Motion estimation  Entropy  Training data 
Abstract: We present a self-supervised approach to ignoring “distractors” in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.


Title: Semantic Mapping with Omnidirectional Vision
Key Words: cameras  distortion  image classification  image fusion  image segmentation  image sensors  mobile robots  path planning  robot vision  SLAM (robots)  omnidirectional vision  omnidirectional images  robust segmentation  occupancy grid maps  inverse sensor model  nonlinear distortions  omnidirectional camera mirror  place category classifier  range-based occupancy grid  dense semantic map  bird eye view  visual semantic mapping framework  robot local free space  Semantics  Sensors  Cameras  Robots  Buildings  Mirrors  Visualization 
Abstract: This paper presents a purely visual semantic mapping framework using omnidirectional images. The approach rests upon the robust segmentation of the robot's local free space, replacing conventional range sensors for the generation of occupancy grid maps. The perceptions are mapped into a bird's eye view allowing an inverse sensor model directly by removing the non-linear distortions of the omnidirectional camera mirror. The system relies on a place category classifier to label the navigation relevant categories: room, corridor, doorway, and open room. Each place class maintains a separated grid map that are fused with the range-based occupancy grid for building a dense semantic map.


Title: Learning Task-Based Instructional Policy for Excavator-Like Robots
Key Words: excavators  learning by example  mobile robots  learning from demonstration  demonstration trajectories automatic segmentation  hydraulic actuated scaled excavator robot  complex truck loading task  nongeneric policy model  mapping continuous state action trajectories  expert demonstration  task-based instructional policy  Task analysis  Robots  Trajectory  Hidden Markov models  Load modeling  Actuators  Loading 
Abstract: We explore beyond existing work in learning from demonstration by asking the question: “Can robots learn to guide?”, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct humans in executing complex task? As a solution, we propose learning of instructional policy (πI) that maps the state to an instruction for a human. To learn πI, we define action primitives that addresses the challenge of mapping continuous state action trajectories to human parse-able instructions. Action primitives are demonstrated to be very effective in automatic segmentation of demonstration trajectories into fewer repetitive and reusable segments, and a highly scalable approach in comparison to the existing state-of-the art. Finally, we construct a non-generic policy model as a generative model for instructional policies to generate instruction for an entire task. With few modifications, the proposed model is demonstrated to perform autonomous execution of complex truck loading task on hydraulic actuated scaled excavator robot. Guidance approach is tested based on a controlled group study involving 75 participants, who learn to perform the same task.


Title: Pedestrian Feature Generation in Fish-Eye Images via Adversary
Key Words: driver information systems  feature extraction  image classification  object detection  pedestrians  FSTN  pedestrian detection  pedestrian feature generation  advanced driver assistance systems  ADAS  training  fish eye spatial transformer network  fish eye images  feature maps  robust  deformation  ETH  KITTI pedestrian datasets  adversarial network  fish eye pedestrian detectors  Detectors  Training  Feature extraction  Mathematical model  Cameras  Proposals  Convolution 
Abstract: Pedestrian detection in fish-eye images is always an important problem in advanced driver assistance systems (ADAS). In conventional methods, pedestrian detectors will be trained using fish-eye images. But it is hard to collect and label enough fish-eye images manually. Therefore, a new strategy for training fish-eye pedestrian detectors using images from normal pedestrian datasets is proposed in this work. Concretely, Fish-eye Spatial Transformer Network (FSTN) is designed to generate pedestrian features in fish-eye images. FSTN aims to simulate distorted pedestrian features on the feature maps. Then the entire network is trained via adversary. FSTN is trained to generate examples which are difficult for pedestrian detectors to classify. So that the detectors are more robust to the deformation. FSTN can be embedded into state-of-the-art detectors easily. And the entire pedestrian detector, where the FSTN embedded, can be trained end to end via adversary. Moreover, experiments on ETH and KITTI pedestrian datasets show the slight accuracy improvement of pedestrian detection in fish-eye images using adversarial network compared with conventional methods.


Title: Autonomous Vehicle Navigation in Rural Environments Without Detailed Prior Maps
Key Words: collision avoidance  least squares approximations  mobile robots  recursive filters  robot vision  sensors  prior maps  positive societal impact  autonomous vehicle navigating  recursive filtering approach  navigate road networks  least-squares residual approach  vehicle frame  sensor-based perception system  global navigation  sparse topological maps  mapless driving framework  autonomous navigation  autonomous driving technology  transmit detailed maps  urban areas  rural environments  autonomous vehicle navigation  Roads  Navigation  Autonomous vehicles  Trajectory  Robot sensing systems  Reliability 
Abstract: State-of-the-art autonomous driving systems rely heavily on detailed and highly accurate prior maps. However, outside of small urban areas, it is very challenging to build, store, and transmit detailed maps since the spatial scales are so large. Furthermore, maintaining detailed maps of large rural areas can be impracticable due to the rapid rate at which these environments can change. This is a significant limitation for the widespread applicability of autonomous driving technology, which has the potential for an incredibly positive societal impact. In this paper, we address the problem of autonomous navigation in rural environments through a novel mapless driving framework that combines sparse topological maps for global navigation with a sensor-based perception system for local navigation. First, a local navigation goal within the sensor view of the vehicle is chosen as a waypoint leading towards the global goal. Next, the local perception system generates a feasible trajectory in the vehicle frame to reach the waypoint while abiding by the rules of the road for the segment being traversed. These trajectories are updated to remain in the local frame using the vehicle's odometry and the associated uncertainty based on the least-squares residual and a recursive filtering approach, which allows the vehicle to navigate road networks reliably, and at high speed, without detailed prior maps. We demonstrate the performance of the system on a full-scale autonomous vehicle navigating in a challenging rural environment and benchmark the system on a large amount of collected data.


Title: Dynamic Occupancy Grid Prediction for Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic Labeling
Key Words: Bayes methods  convolution  feedforward neural nets  filtering theory  intelligent transportation systems  mobile robots  Monte Carlo methods  road traffic  time series  traffic engineering computing  unsupervised learning  complex interactions  dynamic occupancy grid prediction  urban autonomous driving  deep learning approach  long-term situation prediction  intelligent vehicles  complex downtown scenarios  multiple road users  motor vehicles  Bayesian filtering technique  environment representation  machine learning  deep convolutional neural network  spatially distributed velocity estimates  raw data sequence  input time series  multiple sensors  convolutional neural networks  road user interaction  pixel-wise balancing  static cells  dynamic cells  unsupervised learning character  pedestrians  bikes  distributed velocity estimation  Monte-Carlo simulation  Vehicle dynamics  Machine learning  Sensor fusion  Roads  Time series analysis  Laser radar 
Abstract: Long-term situation prediction plays a crucial role for intelligent vehicles. A major challenge still to overcome is the prediction of complex downtown scenarios with multiple road users, e.g., pedestrians, bikes, and motor vehicles, interacting with each other. This contribution tackles this challenge by combining a Bayesian filtering technique for environment representation, and machine learning as long-term predictor. More specifically, a dynamic occupancy grid map is utilized as input to a deep convolutional neural network. This yields the advantage of using spatially distributed velocity estimates from a single time step for prediction, rather than a raw data sequence, alleviating common problems dealing with input time series of multiple sensors. Furthermore, convolutional neural networks have the inherent characteristic of using context information, enabling the implicit modeling of road user interaction. Pixel-wise balancing is applied in the loss function counteracting the extreme imbalance between static and dynamic cells. One of the major advantages is the unsupervised learning character due to fully automatic label generation. The presented algorithm is trained and evaluated on multiple hours of recorded sensor data and compared to Monte-Carlo simulation. Experiments show the ability to model complex interactions.


Title: Sampled-Point Network for Classification of Deformed Building Element Point Clouds
Key Words: disasters  feature extraction  image classification  learning (artificial intelligence)  object recognition  robot vision  object recognition  post-disaster urban areas  search-and-rescue robots  deformed building element point clouds  point network  synthetically-deformed object datasets  point sorting  point coordinates  classification network  deformed building elements  3D class recognition  point cloud input  disaster relief operations  potentially-deformed objects  unstructured environments  point cloud data  3D point cloud  physical site information  Three-dimensional displays  Strain  Object recognition  Deformable models  Machine learning  Feature extraction  Buildings 
Abstract: Search-and-rescue (SAR) robots operating in post-disaster urban areas need to accurately identify physical site information to perform navigation, mapping and manipulation tasks. This can be achieved by acquiring a 3D point cloud of the environment and performing object recognition from the point cloud data. However, this task is complicated by the unstructured environments and potentially-deformed objects encountered during disaster relief operations. Current 3D object recognition methods rely on point cloud input acquired under suitable conditions and do not consider deformations such as outlier noise, bending and truncation. This work introduces a deep learning architecture for 3D class recognition from point clouds of deformed building elements. The classification network, consisting of stacked convolution and average pooling layers applied directly to point coordinates, was trained using point clouds sampled from a database of mesh models. The proposed method achieves robustness to input variability using point sorting, resampling, and rotation normalization techniques. Experimental results on synthetically-deformed object datasets show that the proposed method outperforms the conventional deep learning methods in terms of classification accuracy and computational efficiency.


Title: Real-Time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs
Key Words: agricultural machinery  agrochemicals  convolution  crops  feedforward neural nets  image colour analysis  image segmentation  industrial robots  robot vision  precision agriculture robots leveraging background knowledge  precision farming robots  CNN-based semantic segmentation  crop fields  sugar beet plants  RGB data  vegetation indexes  real-time semantic segmentation  trigger weeding actions  agricultural robot operator  Agriculture  Vegetation mapping  Semantics  Real-time systems  Soil  Indexes  Task analysis 
Abstract: Precision farming robots, which target to reduce the amount of herbicides that need to be brought out in the fields, must have the ability to identify crops and weeds in real time to trigger weeding actions. In this paper, we address the problem of CNN-based semantic segmentation of crop fields separating sugar beet plants, weeds, and background solely based on RGB data. We propose a CNN that exploits existing vegetation indexes and provides a classification in real time. Furthermore, it can be effectively re-trained to so far unseen fields with a comparably small amount of training data. We implemented and thoroughly evaluated our system on a real agricultural robot operating in different fields in Germany and Switzerland. The results show that our system generalizes well, can operate at around 20 Hz, and is suitable for online operation in the fields.


Title: Line-Based Global Localization of a Spherical Camera in Manhattan Worlds
Key Words: cameras  feature extraction  gradient methods  Hough transforms  SLAM (robots)  3D line map  complicated six degrees of freedom search  2D line information  line-based global localization  spherical Hough representation  6 DoF localization process  Manhattan world assumption  3D-2D line correspondences  spherical-gradient filtering  spherical image  indoor environment  camera position  global environmental information  spherical camera  indoor localization  indoor spaces  Cameras  Three-dimensional displays  Image edge detection  Robustness  Robot vision systems  Solid modeling  Estimation 
Abstract: Localization is an important task for mobile service robots in indoor spaces. In this research, we propose a novel technique for indoor localization using a spherical camera. Spherical cameras can obtain a complete view of the surroundings allowing the use of global environmental information. We take advantage of this in order to estimate camera position and the orientation with respect to a known 3D line map of an indoor environment, using a single image. We robustly extract 2D line information from the spherical image via spherical-gradient filtering and match it to 3D line information in the line map. Our method requires no information about the 3D-2D line correspondences. In order to avoid a complicated six degrees of freedom (6 DoF) search for position and orientation, we use a Manhattan world assumption to decompose the line information in the image. The 6 DoF localization process is divided into two phases. First, we estimate the orientation by extracting the three principle directions from the image. Then, the position is estimated by robustly matching the distribution of lines between the image and the 3D model via a spherical Hough representation. This decoupled search can robustly localize a spherical camera using a single image, as we demonstrate experimentally.


Title: Addressing Challenging Place Recognition Tasks Using Generative Adversarial Networks
Key Words: feature extraction  image recognition  learning (artificial intelligence)  mobile robots  robot vision  SLAM (robots)  visual perception  perception task  place recognition tasks  simultaneous localization and mapping  SLAM  coupled Generative Adversarial Networks  domain translation task  Task analysis  Gallium nitride  Lighting  Generators  Image recognition  Feature extraction  Visualization 
Abstract: Place recognition is an essential component of Simultaneous Localization And Mapping (SLAM). Under severe appearance change, reliable place recognition is a difficult perception task since the same place is perceptually very different in the morning, at night, or over different seasons. This work addresses place recognition as a domain translation task. Using a pair of coupled Generative Adversarial Networks (GANs), we show that it is possible to generate the appearance of one domain (such as summer) from another (such as winter) without requiring image-to-image correspondences across the domains. Mapping between domains is learned from sets of images in each domain without knowing the instance-to-instance correspondence by enforcing a cyclic consistency constraint. In the process, meaningful feature spaces are learned for each domain, the distances in which can be used for the task of place recognition. Experiments show that learned features correspond to visual similarity and can be effectively used for place recognition across seasons.


Title: Multi-Agent Time-Based Decision-Making for the Search and Action Problem
Key Words: computational complexity  control engineering computing  decision making  multi-agent systems  multi-robot systems  probability  rescue robots  search-and-rescue  task allocation  probabilistic reasoning  Gazebo-based environmenT  multiagent time-based decision-making  Mohamed Bin Zayed International Robotics Challenge  near-optimal decisions  agent action  allocated budget  time constraints  decentralized multiagent decision-making framework  computational complexity  task selection  missions present several challenges  robotic applications  action problem  Task analysis  Search problems  Decision making  Planning  Time factors  Robots  Probabilistic logic 
Abstract: Many robotic applications, such as search-and-rescue, require multiple agents to search for and perform actions on targets. However, such missions present several challenges, including cooperative exploration, task selection and allocation, time limitations, and computational complexity. To address this, we propose a decentralized multi-agent decision-making framework for the search and action problem with time constraints. The main idea is to treat time as an allocated budget in a setting where each agent action incurs a time cost and yields a certain reward. Our approach leverages probabilistic reasoning to make near-optimal decisions leading to maximized reward. We evaluate our method in the search, pick, and place scenario of the Mohamed Bin Zayed International Robotics Challenge (MBZIRC), by using a probability density map and reward prediction function to assess actions. Extensive simulations show that our algorithm outperforms benchmark strategies, and we demonstrate system integration in a Gazebo-based environment, validating the framework's readiness for field application.


Title: Robust Environmental Mapping by Mobile Sensor Networks
Key Words: Bayes methods  computational geometry  environmental factors  fires  mobile radio  mobile robots  wireless sensor networks  environmental mapping  ground truth distribution  Voronoi diagram  ad-hoc communication  human safety  satisfactory convergence  autonomous agents  mapping tasks  terrain elevation  physical quantities  forest fires  hazardous chemical leakages  spatial map  mobile sensor networks  decentralized manner  disjoint regions  hardware failures  short-range sensors  mobile robots  environmental parameters  robust spatial mapping  Bayesian approach  Robot sensing systems  Robustness  Mutual information  Computational modeling  Mobile robots  Task analysis 
Abstract: Constructing a spatial map of environmental parameters is a crucial step to preventing hazardous chemical leakages, forest fires, or while estimating a spatially distributed physical quantities such as terrain elevation. Although prior methods can do such mapping tasks efficiently via dispatching a group of autonomous agents, they are unable to ensure satisfactory convergence to the underlying ground truth distribution in a decentralized manner when any of the agents fail. Since the types of agents utilized to perform such mapping are typically inexpensive and prone to failure, this results in poor overall mapping performance in real-world applications, which can in certain cases endanger human safety. This paper presents a Bayesian approach for robust spatial mapping of environmental parameters by deploying a group of mobile robots capable of ad-hoc communication equipped with short-range sensors in the presence of hardware failures. Our approach first utilizes a variant of the Voronoi diagram to partition the region to be mapped into disjoint regions that are each associated with at least one robot. These robots are then deployed in a decentralized manner to maximize the likelihood that at least one robot detects every target in their associated region despite a non-zero probability of failure. A suite of simulation results is presented to demonstrate the effectiveness and robustness of the proposed method when compared to existing techniques.


Title: Bodily Aware Soft Robots: Integration of Proprioceptive and Exteroceptive Sensors
Key Words: cameras  convolution  dexterous manipulators  mobile robots  object tracking  path planning  recurrent neural nets  sensors  bodily aware soft robots  exteroceptive sensors  proprioceptive sensors  bend sensors  visual sensor  nonlinearity  octopus-inspired arm  camera record  arm capturing  internal sensory signals  stacked convolutional autoencoder  CAE  recurrent neural network  RNN  motion  Convolution  Soft robotics  Robot sensing systems  Visualization  Recurrent neural networks 
Abstract: Being aware of our body has great importance in our everyday life. It helps us to complete difficult tasks, such as movement in a dark room or grasping a complex object. These skills are important for robots as well, however, robotic bodily awareness is still an open question, and the nonlinearity of soft robots adds even more complexity. In this paper, we address this problem and present a novel method to implement bodily awareness into a real soft robot by the integration of its exteroceptive and proprioceptive sensors. We use an octopus-inspired arm as an example where the proprioceptive representation is approximated by four bend sensors integrated into the soft body, while a camera records the movement of the arm capturing its exteroceptive representation. The internal sensory signals are mapped to the visual information using a combination of a stacked convolutional autoencoder (CAE) and a recurrent neural network (RNN). As a result, the soft robot can learn to estimate and, therefore, to imagine its motion even when its visual sensor is not available.


Title: Data-Efficient Decentralized Visual SLAM
Key Words: cameras  data mining  graph theory  image sensors  multi-robot systems  optimisation  pose estimation  robot vision  SLAM (robots)  decentralized visual SLAM system  decentralized SLAM components  data-efficient decentralized visual SLAM  pose-graph optimization method  data association scales  robot count  data transfers  robots  map data  visual SLAM systems exchange  versatile cameras  lightweight cameras  cheap cameras  multirobot applications  mapping  Supplementary Material Data  Simultaneous localization and mapping  Visualization  Optimization  Pose estimation  Trajectory  Bandwidth 
Abstract: Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code. Supplementary Material Data and code are at: https://github.com/uzh-rpg/dslam_open.


Title: IMLS-SLAM: Scan-to-Model Matching Based on 3D Data
Key Words: collision avoidance  mobile robots  optical radar  remotely operated vehicles  road traffic control  robot vision  SLAM (robots)  stereo image processing  robotics community  stereo cameras  depth sensors  Velodyne LiDAR  autonomous driving  low-drift SLAM algorithm  3D LiDAR data  scan-to-model matching framework  specific sampling strategy  LiDAR scans  Velodyne HDL32  Velodyne HDL64  global drift  IMLS-SLAM  3D data  simultaneous localization and mapping  localized LiDAR sweeps  IMLS surface representation  implicit moving least squares  size 4.0 km  size 16.0 m  time 10.0 year  Three-dimensional displays  Laser radar  Simultaneous localization and mapping  Two dimensional displays  Iterative closest point algorithm  Observability 
Abstract: The Simultaneous Localization And Mapping (SLAM) problem has been well studied in the robotics community, especially using mono, stereo cameras or depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the last 10 years to be very useful to perceive the environment in autonomous driving, but few methods exist that directly use these 3D data for odometry. We present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method relies on a scan-to-model matching framework. We first have a specific sampling strategy based on the LiDAR scans. We then define our model as the previous localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface representation. We show experiments with the Velodyne HDL32 with only 0.40% drift over a 4 km acquisition without any loop closure (i.e., 16 m drift after 4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and ranked among the best methods (against mono, stereo and LiDAR methods) with a global drift of only 0.69%.


Title: ApriISAM: Real-Time Smoothing and Mapping
Key Words: error analysis  matrix decomposition  mobile robots  SLAM (robots)  sparse matrices  fixed computational budget  dynamic variable reordering algorithm  ApriISAM  real-time smoothing  online robots  incremental SLAM algorithms  batch algorithms  absolute error  incremental Cholesky factorizations  marginalization order  iSAM  re-linearize  Simultaneous localization and mapping  Heuristic algorithms  Smoothing methods  Sparse matrices  Clustering algorithms  Real-time systems 
Abstract: For online robots, incremental SLAM algorithms offer huge potential computational savings over batch algorithms. The dominant incremental algorithms are iSAM and iSAM2 which offer radically different approaches to computing incremental updates, balancing issues like 1) the need to re-linearize, 2) changes in the desirable variable marginalization order, and 3) the underlying conceptual approach (i.e. the “matrix” story versus the “factor graph” story). In this paper, we propose a new incremental algorithm that computes solutions with lower absolute error and generally provides lower error solutions for a fixed computational budget than either iSAM or iSAM2. Key to AprilSAM's performance are a new dynamic variable reordering algorithm for fast incremental Cholesky factorizations, a method for reducing the work involved in backsubstitutions, and a new algorithm for deciding between incremental and batch updates.


Title: Fast Nonlinear Approximation of Pose Graph Node Marginalization
Key Words: approximation theory  graph theory  mobile robots  optimisation  path planning  pose estimation  SLAM (robots)  pose graph node marginalization  longterm localization  longterm mapping  longterm navigation  pose graph structure  absolute-to relative-pose spaces  pose-composition approach scaled version  approximate subgraph  fast nonlinear approximation method  Topology  Jacobian matrices  Covariance matrices  Gaussian distribution  Simultaneous localization and mapping  Approximation methods 
Abstract: We present a fast nonlinear approximation method for marginalizing out nodes on pose graphs for longterm simultaneous localization, mapping, and navigation. Our approximation preserves the pose graph structure to leverage the rich literature of pose graphs and optimization schemes. By re-parameterizing from absolute-to relative-pose spaces, our method does not suffer from the choice of linearization points as in previous works. We then join our approximation process with a scaled version of the recently-demoted pose-composition approach. Our approach eschews the expenses of many state-of-the-art convex optimization schemes through our efficient and simple O(N2) implementation for a given known topology of the approximate subgraph. We demonstrate its speed and near optimality in practice by comparing against state-of-the-art techniques on popular datasets.


Title: A Monocular SLAM System Leveraging Structural Regularity in Manhattan World
Key Words: cameras  feature extraction  mobile robots  optimisation  pose estimation  robot vision  SLAM (robots)  rotation optimization strategy  parallelism  global binding method  absolute rotation  relative rotation  translation optimization strategy leveraging coplanarity  coplanar features  relative translation  optimal absolute translation  3D line optimization strategy  structural line segments  structural features  structural feature-based optimization module  3D map  structural regularity  optimization strategies  monocular SLAM systems  Manhattan World  camera poses  Three-dimensional displays  Cameras  Optimization  Parallel processing  Simultaneous localization and mapping  Robustness  Estimation 
Abstract: The structural features in Manhattan world encode useful geometric information of parallelism, orthogonality and/or coplanarity in the scene. By fully exploiting these structural features, we propose a novel monocular SLAM system which provides accurate estimation of camera poses and 3D map. The foremost contribution of the proposed system is a structural feature-based optimization module which contains three novel optimization strategies. First, a rotation optimization strategy using the parallelism and orthogonality of 3D lines is presented. We propose a global binding method to compute an accurate estimation of the absolute rotation of the camera. Then we propose an approach for calculating the relative rotation to further refine the absolute rotation. Second, a translation optimization strategy leveraging coplanarity is proposed. Coplanar features are effectively identified, and we leverage them by a unified model handling both points and lines to calculate the relative translation, and then the optimal absolute translation. Third, a 3D line optimization strategy utilizing parallelism, orthogonality and coplanarity simultaneously is proposed to obtain an accurate 3D map consisting of structural line segments with low computational complexity. Experiments in man-made environments have demonstrated that the proposed system outperforms existing state-of-the-art monocular SLAM systems in terms of accuracy and robustness.


Title: Visual Saliency-Aware Receding Horizon Autonomous Exploration with Application to Aerial Robotics
Key Words: mobile robots  optimisation  path planning  robot vision  trees (mathematics)  visual saliency-aware receding horizon autonomous exploration  reobserving salient regions  environment exploration rate  robot endurance  random tree  two-step optimization paradigm  salient objects  path planner  visual attention  aerial robotics  Visualization  Robot sensing systems  Planning  Computational modeling  Path planning 
Abstract: This paper presents a novel strategy for autonomous visual saliency-aware receding horizon exploration of unknown environments using aerial robots. Through a model of visual attention, incrementally built maps are annotated regarding the visual importance and saliency of different objects and entities in the environment. Provided this information, a path planner that simultaneously optimizes for exploration of unknown space, and also directs the robot's attention to focus on the most salient objects, is developed. Following a two-step optimization paradigm, the algorithm first samples a random tree and identifies the branch maximizing for new volume to be explored. The first viewpoint of this path is then provided as a reference to the second planning step. Within that, a new tree is spanned, admissible branches arriving at the reference viewpoint while respecting a time budget dependent on the robot endurance and its environment exploration rate are found and evaluated in terms of reobserving salient regions at sufficient resolution. The best branch is then selected and executed by the robot, and the whole process is iteratively repeated. The proposed method is evaluated regarding its ability to provide increased attention toward salient objects, is verified to run onboard a small aerial robot, and is demonstrated in a set of challenging experimental studies.


Title: Perception-aware Receding Horizon Navigation for MAVs
Key Words: aircraft control  collision avoidance  mobile robots  navigation  path planning  robot vision  state estimation  perception-aware receding horizon navigation  microaerial vehicle  state estimation uncertainty  perception-aware receding horizon approach  monocular state estimation  candidate trajectories  perception quality  collision probability  receding horizon navigation framework  improved state estimation accuracy  goal-reaching task  purely-reactive navigation system  MAV  Trajectory  State estimation  Planning  Navigation  Cameras  Measurement  Task analysis 
Abstract: To reach a given destination safely and accurately, a micro aerial vehicle needs to be able to avoid obstacles and minimize its state estimation uncertainty at the same time. To achieve this goal, we propose a perception-aware receding horizon approach. In our method, a single forward-looking camera is used for state estimation and mapping. Using the information from the monocular state estimation and mapping system, we generate a library of candidate trajectories and evaluate them in terms of perception quality, collision probability, and distance to the goal. The best trajectory to execute is then selected as the one that maximizes a reward function based on these three metrics. To the best of our knowledge, this is the first work that integrates active vision within a receding horizon navigation framework for a goal reaching task. We demonstrate by simulation and real-world experiments on an actual quadrotor that our active approach leads to improved state estimation accuracy in a goal-reaching task when compared to a purely-reactive navigation system, especially in difficult scenes (e.g., weak texture).


Title: Viewpoint-Tolerant Place Recognition Combining 2D and 3D Information for UAV Navigation
Key Words: autonomous aerial vehicles  distance measurement  geometry  mobile robots  path planning  robot vision  stereo image processing  3D information  UAV navigation  Unmanned Aerial Vehicles  vision-based odometry  loop-closure detection  place recognition framework  local 3D geometry  viewpoint-tolerant place recognition  2D Information  hand-held datasets  perceptual aliasing  binary features  Simultaneous localization and mapping  Three-dimensional displays  Visualization  Vocabulary  Image recognition  Navigation 
Abstract: The booming interest in Unmanned Aerial Vehicles (UAV s) is fed by their potentially great impact, however progress is hindered by their limited perception capabilities. While vision-based odometry was shown to run successfully onboard UAV s, loop-closure detection to correct for drift or to recover from tracking failures, has so far, proven particularly challenging for UAVs. At the heart of this is the problem of viewpoint-tolerant place recognition; in stark difference to ground robots, UAVs can revisit a scene from very different viewpoints. As a result, existing approaches struggle greatly as the task at hand violates underlying assumptions in assessing scene similarity. In this paper, we propose a place recognition framework, which exploits both efficient binary features and noisy estimates of the local 3D geometry, which are anyway computed for visual-inertial odometry onboard the UAV. Attaching both an appearance and a geometry signature to each `location', the proposed approach demonstrates unprecedented recall for perfect precision as well as high quality loop-closing transformations on both flying and hand-held datasets exhibiting large viewpoint and appearance changes as well as perceptual aliasing.


Title: Flexible Stereo: Constrained, Non-Rigid, Wide-Baseline Stereo Vision for Fixed-Wing Aerial Platforms
Key Words: aerospace components  angular velocity measurement  autonomous aerial vehicles  cameras  collision avoidance  Kalman filters  nonlinear filters  pose estimation  robot vision  stereo image processing  landing maneuvers  wing model  probability density function  measured deviations  nominal relative baseline transformation  relative pose measurements  relative perspective N-point problem  inertial measurement units  highly accurate baseline transformations  flexible stereo  wide-baseline stereo vision  fixed-wing aerial platforms  computationally efficient method  visual-inertial sensor rigs  fixed-wing unmanned aerial vehicle  estimated relative poses  highly accurate depth maps  obstacle avoidance  low-altitude flights  extended Kalman filter  Cameras  Visualization  Mathematical model  Quaternions  Unmanned aerial vehicles  Real-time systems  Accelerometers 
Abstract: This paper proposes a computationally efficient method to estimate the time-varying relative pose between two visual-inertial sensor rigs mounted on the flexible wings of a fixed-wing unmanned aerial vehicle (UAV). The estimated relative poses are used to generate highly accurate depth maps in real-time and can be employed for obstacle avoidance in low-altitude flights or landing maneuvers. The approach is structured as follows: Initially, a wing model is identified by fitting a probability density function to measured deviations from the nominal relative baseline transformation. At runtime, the prior knowledge about the wing model is fused in an Extended Kalman filter (EKF) together with relative pose measurements obtained from solving a relative perspective N-point problem (PNP), and the linear accelerations and angular velocities measured by the two inertial measurement units (IMU) which are rigidly attached to the cameras. Results obtained from extensive synthetic experiments demonstrate that our proposed framework is able to estimate highly accurate baseline transformations and depth maps.


Title: Pairwise Consistent Measurement Set Maximization for Robust Multi-Robot Map Merging
Key Words: expectation-maximisation algorithm  graph theory  mobile robots  multi-robot systems  optimisation  robot vision  SLAM (robots)  PCM  robust multirobot map  robust selection  robust SLAM methods  multirobot case  simultaneous localization and mapping  pairwise consistency set maximization  pairwise consistent measurement set maximization  odometry backbone  Simultaneous localization and mapping  Robot kinematics  Phase change materials  Trajectory  Robustness  Merging 
Abstract: This paper reports on a method for robust selection of inter-map loop closures in multi-robot simultaneous localization and mapping (SLAM). Existing robust SLAM methods assume a good initialization or an “odometry backbone” to classify inlier and outlier loop closures. In the multi-robot case, these assumptions do not always hold. This paper presents an algorithm called Pairwise Consistency Maximization (PCM) that estimates the largest pairwise internally consistent set of measurements. Finding the largest pairwise internally consistent set can be transformed into an instance of the maximum clique problem from graph theory, and by leveraging the associated literature it can be solved in realtime. This paper evaluates how well PCM approximates the combinatorial gold standard using simulated data. It also evaluates the performance of PCM on synthetic and real-world data sets in comparison with DCS, SCGP, and RANSAC, and shows that PCM significantly outperforms these methods.


Title: Map-Aware Particle Filter for Localization
Key Words: distance measurement  Global Positioning System  optical radar  particle filtering (numerical methods)  SLAM (robots)  map-aware particle filter  2D LiDAR localization  GPS localization  map information  localization sensors  particle filter framework  map-matching  prior occupancy grid  vehicle localization  Trajectory  Roads  Atmospheric measurements  Particle measurements  Sensors  Particle filters  Two dimensional displays 
Abstract: This work presents a method to improve vehicle localization by using the information from a prior occupancy grid to bound the possible poses. The method, named Map-Aware Particle Filter, uses a nonlinear approach to map-matching that can be integrated into a particle filter framework for localization. Each particle is re-weighted based on the validity of its current position in the map. In addition, we buffer the trajectory followed by the vehicle and then append it to each particle's pose. We then quantify the overlap between the trajectory and the map's free space. This serves as a measure of each particle's validity given the trajectory and the shape of the map. We evaluated the method by performing experiments with different types of localization sensors: First, (i) we significantly reduced the drift inherent to dead reckoning. By only using wheel odometry and map information we achieved loop closure over a distance of approximately 3 km. We also (ii) increased the accuracy of GPS localization. Finally, (iii) we fused a fragile 2D LiDAR localization with the map information. The resulting system had a higher robustness and managed to close the loop in an outdated map where it had failed before.


Title: Using a Memory of Motion to Efficiently Warm-Start a Nonlinear Predictive Controller
Key Words: autonomous aerial vehicles  iterative methods  learning (artificial intelligence)  nonlinear control systems  optimal control  optimisation  path planning  predictive control  sampling methods  trajectory optimisation (aerospace)  direct optimal control  control policy  optimal state-control trajectories  nonlinear predictive controller  nonlinear optimization problem  model-based methodology  control cycle  kinodynamic probabilistic roadmap  nonlinear solver  unmanned aerial vehicle  UAV  complex dynamical systems  sampling-based planning  policy learning  Computational modeling  Approximation algorithms  Optimal control  Planning  Robots  Trajectory optimization 
Abstract: Predictive control is an efficient model-based methodology to control complex dynamical systems. In general, it boils down to the resolution at each control cycle of a large nonlinear optimization problem. A critical issue is then to provide a good guess to initialize the nonlinear solver so as to speed up convergence. This is particularly important when disturbances or changes in the environment prevent the use of the trajectory computed at the previous control cycle as initial guess. In this paper, we introduce an original and very efficient solution to automatically build this initial guess. We propose to rely on off-line computation to build an approximation of the optimal trajectories, that can be used on-line to initialize the predictive controller. To that end, we combined the use of sampling-based planning, policy learning with generic representations (such as neural networks), and direct optimal control. We first propose an algorithm to simultaneously build a kinodynamic probabilistic roadmap (PRM) and approximate value function and control policy. This algorithm quickly converges toward an approximation of the optimal state-control trajectories (along with an optimal PRM). Then, we propose two methods to store the optimal trajectories and use them to initialize the predictive controller. We experimentally show that directly storing the state-control trajectories leads the predictive controller to quickly converges (2 to 5 iterations) toward the (global) optimal solution. The results are validated in simulation with an unmanned aerial vehicle (UAV) and other dynamical systems.


Title: Topological Nearest-Neighbor Filtering for Sampling-Based Planners
Key Words: filtering theory  mobile robots  nearest neighbour methods  path planning  sampling methods  trees (mathematics)  topological nearest-neighbor filtering  computational techniques  locality-sensitive hashing  workspace connectivity  nearest-neighbor time  nearest-neighbor algorithm  candidate neighbor configurations  topologically relevant set  sampling-based motion planning algorithms  Nearest-neighbor finding  sampling-based planners 
Abstract: Nearest-neighbor finding is a major bottleneck for sampling-based motion planning algorithms. The cost of finding nearest neighbors grows with the size of the roadmap, leading to significant slowdowns for problems which require many configurations to find a solution. Prior work has investigated relieving this pressure with quicker computational techniques, such as kd-trees or locality-sensitive hashing. In this work, we investigate an alternative direction for expediting this process based on workspace connectivity. We present an algorithm called Topological Nearest-Neighbor Filtering, which employs a workspace decomposition to select a topologically relevant set of candidate neighbor configurations as a pre-processing step for a nearest-neighbor algorithm. We investigate the application of this filter to several varieties of RRT and demonstrate that the filter improves both nearest-neighbor time and overall planning performance.


Title: A General Pipeline for 3D Detection of Vehicles
Key Words: automobiles  convolution  feature extraction  feedforward neural nets  mobile robots  robot vision  traffic engineering computing  2D detection network  generalised car models  two-stage convolutional neural network  3D detection algorithms  general pipeline  autonomous driving  flexible pipeline  3D point cloud  model fitting algorithm  3D box detection  2D vehicle detection  Three-dimensional displays  Two dimensional displays  Automobiles  Pipelines  Solid modeling  Proposals  Laser radar 
Abstract: Autonomous driving requires 3D perception of vehicles and other objects in the in environment. Much of the current methods support 2D vehicle detection. This paper proposes a flexible pipeline to adopt any 2D detection network and fuse it with a 3D point cloud to generate 3D information with minimum changes of the 2D detection networks. To identify the 3D box, an effective model fitting algorithm is developed based on generalised car models and score maps. A two-stage convolutional neural network (CNN) is proposed to refine the detected 3D box. This pipeline is tested on the KITTI dataset using two different 2D detection networks. The 3D detection results based on these two networks are similar, demonstrating the flexibility of the proposed pipeline. The results rank second among the 3D detection algorithms, indicating its competencies in 3D detection.


Title: Fast Disparity Estimation Using Dense Networks
Key Words: convolution  feedforward neural nets  image colour analysis  stereo image processing  CNN-based methods  DenseMapNet  deep convolutional neural networks  repetitive regions  textureless regions  stereo vision  color stereo images  dense networks  disparity map  autoencoder method  Estimation  Two dimensional displays  Semantics  Image resolution  Three-dimensional displays  Cameras  Computer vision 
Abstract: Disparity estimation is a difficult problem in stereo vision because the correspondence technique fails in images with textureless and repetitive regions. Recent body of work using deep convolutional neural networks (CNN) overcomes this problem with semantics. Most CNN implementations use an autoencoder method; stereo images are encoded, merged and finally decoded to predict the disparity map. In this paper, we present a CNN implementation inspired by dense networks to reduce the number of parameters. Furthermore, our approach takes into account semantic reasoning in disparity estimation. Our proposed network, called DenseMapNet, is compact, fast and can be trained end-to-end. DenseMapNet requires 290k parameters only and runs at 30Hz or faster on color stereo images in full resolution. Experimental results show that DenseMapNet accuracy is comparable with other significantly bigger CNN-based methods.


Title: AA-ICP: Iterative Closest Point with Anderson Acceleration
Key Words: iterative methods  PCL  Point Cloud Library  fixed point problem  ICP implementations  standard Picard iteration  iterative procedure  registration  scan-matching  Anderson acceleration  iterative closest point  AA-ICP  Iterative closest point algorithm  Acceleration  History  Convergence  Three-dimensional displays  Robots  Two dimensional displays 
Abstract: Iterative Closest Point (ICP) is a widely used method for performing scan-matching and registration. Being simple and robust, this method is still computationally expensive and may be challenging to use in real-time applications with limited resources on mobile platforms. In this paper we propose a novel effective method for acceleration of ICP which does not require substantial modifications to the existing code. This method is based on an idea of Anderson acceleration which is an iterative procedure for finding a fixed point of contractive mapping. The latter is often faster than a standard Picard iteration, usually used in ICP implementations. We show that ICP, being a fixed point problem, can be significantly accelerated by this method enhanced by heuristics to improve overall robustness. We implement proposed approach into Point Cloud Library (PCL) and make it available online. Benchmarking on the real-world data fully supports our claims.


Title: Vision-Based Global Localization Using Ceiling Space Density
Key Words: cameras  mobile robots  robot vision  service robots  home environments  free space density  available blueprint information  ceiling vision  robust localization information  robotic vacuum  superior localization results  vision-based global localization  ceiling space density  service robots  homes  self-localize  man-made constructions  documented blueprint  robot localization  smart home applications  movable objects  complicated task  horizontal range-finders  effective global localization approach  Cameras  Kernel  Three-dimensional displays  Robot vision systems  Simultaneous localization and mapping 
Abstract: Service robots are becoming a reality across homes. Still, the range of applications supported by such robots remains tied to their ability to self-localize in the environment. Man-made constructions often have a documented blueprint which can be used as input information for robot localization and smart home applications. However, home environments commonly include movable objects and furniture, which can make localization a complicated task, especially for forward-facing horizontal range-finders. In this paper, we present a new and effective global localization approach for home environments which adapts the notion of free space density to a camera pointing to the ceiling. We exploit the available blueprint information, as well as evidence that ceiling vision can provide robust localization information, even in the presence of occlusions. We perform real-world experiments using a robotic vacuum cleaner equipped with an upward-facing camera in two different apartments across multiple trajectories and compare the proposed method with competing approaches. Our solution shows superior localization results using maps where neither furniture or movable objects are not modeled.


Title: 1-Actuator 3-DoF Manipulation Using a Virtual Turntable Based on Differential Friction Surface
Key Words: actuators  dexterous manipulators  end effectors  friction  manipulator dynamics  motion control  plates (structures)  position control  flat plate  manipulator  active-passive hybrid joint mechanism  surface friction property  3- DoF manipulation strategy  virtual turntable  1-actuator 3-DoF manipulation  differential friction surface  nonprehensile manipulation  single actuator  manipulation strategy  Friction  Actuators  Orbits  Vibrations  End effectors  Frequency control 
Abstract: This paper describes nonprehensile manipulation realized using the vibration of a plate. A novel manipulation strategy is proposed wherein the three degrees-of-freedom (DoF) of a part are controlled by only one actuator. First, a manipulator driven by a single actuator is introduced. The end effector of this manipulator is a flat plate. The manipulator employs an active-passive hybrid joint mechanism with nonparallel axes. Based on the sinusoidal displacement input to the actuator, the manipulator can generate the velocity of a part omnidirectionally on the plate. Next, simulation results are presented to show that the velocity map of the part varies depending upon the surface friction property of the plate. Further, the control of the rotational behavior of the part on the boundary of two areas with different friction properties by means of the input frequency is shown. Based on this control, a 3- DoF manipulation strategy using a virtual turntable is developed to realize the desired position and orientation of the part. Finally, the proposed method is demonstrated via experiments.


Title: Feature-Based SLAM for Imaging Sonar with Under-Constrained Landmarks
Key Words: feature extraction  image reconstruction  image sensors  reliability  SLAM (robots)  sonar imaging  sonar imaging  point landmark identification  feature-point extraction  general-purpose method  planar scene assumption  underwater feature-based SLAM  under-constrained landmarks  Feature extraction  Imaging  Sonar measurements  Simultaneous localization and mapping  Three-dimensional displays  Image reconstruction 
Abstract: Recent algorithms have demonstrated the feasibility of underwater feature-based SLAM using imaging sonar. But previous methods have either relied on manual feature extraction and correspondence or used prior knowledge of the scene, such as the planar scene assumption. Our proposed system provides a general-purpose method for feature-point extraction and correspondence in arbitrary scenes. Additionally, we develop a method of identifying point landmarks that are likely to be well-constrained and reliably reconstructed. Finally, we demonstrate that while under-constrained landmarks cannot be accurately reconstructed themselves, they can still be used to constrain and correct the sensor motion. These advances represent a large step towards general-purpose, feature-based SLAM with imaging sonar.


Title: SLAMBench2: Multi-Objective Head-to-Head Benchmarking for Visual SLAM
Key Words: augmented reality  autonomous aerial vehicles  mobile computing  mobile robots  navigation  robot vision  SLAM (robots)  visual SLAM  augmented reality systems  nonfunctional requirements  mobile phone-based AR application  tight energy budget  UAV navigation system  SLAMBench2  benchmarking framework  open source  close source  performance metrics  ORB-SLAM2  publicly-available software framework  SLAM applications  SLAM systems  SLAM algorithms  multiobjective head-to-head benchmarking  functional requirements  Simultaneous localization and mapping  Measurement  Trajectory  Benchmark testing  User interfaces  C++ languages 
Abstract: SLAM is becoming a key component of robotics and augmented reality (AR) systems. While a large number of SLAM algorithms have been presented, there has been little effort to unify the interface of such algorithms, or to perform a holistic comparison of their capabilities. This is a problem since different SLAM applications can have different functional and non-functional requirements. For example, a mobile phone-based AR application has a tight energy budget, while a UAV navigation system usually requires high accuracy. SLAMBench2 is a benchmarking framework to evaluate existing and future SLAM systems, both open and close source, over an extensible list of datasets, while using a comparable and clearly specified list of performance metrics. A wide variety of existing SLAM algorithms and datasets is supported, e.g. ElasticFusion, InfiniTAM, ORB-SLAM2, OKVIS, and integrating new ones is straightforward and clearly specified by the framework. SLAMBench2 is a publicly-available software framework which represents a starting point for quantitative, comparable and val-idatable experimental research to investigate trade-offs across SLAM systems.


Title: Delight: An Efficient Descriptor for Global Localisation Using LiDAR Intensities
Key Words: mobile robots  navigation  optical information processing  optical radar  path planning  robot vision  intensity information  DELIGHT  distributed histograms  chi-squared tests  two-stage solution  geometry-based verification  range information  GPS-denied areas  robot position  kidnapped robot problems  mobile robotics  place recognition  global localisation  intensity-based prior estimation  LiDAR intensities  Laser radar  Histograms  Three-dimensional displays  Simultaneous localization and mapping 
Abstract: Place recognition is a key element of mobile robotics. It can assist with the “wake-up” and “kidnapped robot” problems, where the robot position needs to be estimated without prior information. Among the different sensors that can be used for the task (e.g., camera, GPS, LiDAR), LiDAR has the advantage of operating in the dark and in GPS-denied areas. We propose a new method that uses solely the LiDAR data and that can be performed without robot motion. In contrast to other methods, our system leverages intensity information (as opposed to only range information) which is encoded into a novel descriptor of LiDAR intensities as a group of histograms, named DELIGHT. The descriptor encodes the distributed histograms of intensity of the surroundings which are compared using chi-squared tests. Our pipeline is a two-stage solution consisting of an intensity-based prior estimation and a geometry-based verification. For a map of 220k square meters, the method achieves localisation in around 3s with a success rate of 97%, illustrating the applicability of the method in real environments.


Title: Online Probabilistic Change Detection in Feature-Based Maps
Key Words: compressed sensing  feature extraction  image representation  object detection  probability  sensor fusion  terrain mapping  online data association decisions  online probabilistic change detection  sparse feature-based maps  compact representation  static map features  feature repeatability  probabilistically principled approach  sparse mapping model  Feature extraction  Simultaneous localization and mapping  Robustness  Heuristic algorithms  Probabilistic logic  Visualization 
Abstract: Sparse feature-based maps provide a compact representation of the environment that admit efficient algorithms, for example simultaneous localization and mapping. These representations typically assume a static world and therefore contain static map features. However, since the world contains dynamic elements, determining when map features no longer correspond to the environment is essential for long-term utility. This work develops a feature-based model of the environment which evolves over time through feature persistence. Moreover, we augment the state-of-the-art sparse mapping model with a correlative structure that captures spatio-temporal properties, e.g. that nearby features frequently have similar persistence. We show that such relationships, typically addressed through an ad hoc formalism focusing only on feature repeatability, are crucial to evaluate through a probabilistically principled approach. The joint posterior over feature persistence can be computed efficiently and used to improve online data association decisions for localization. The proposed algorithms are validated in numerical simulation and using publicly available data sets.


Title: CDDT: Fast Approximate 2D Ray Casting for Accelerated Localization
Key Words: data structures  mobile robots  particle filtering (numerical methods)  ray tracing  table lookup  constant time ray casting performance  particle filter algorithm  resource-constrained mobile robots  localization approach  approximate 2D ray casting  mobile robot  compressed directional distance transform  two dimensional occupancy grid maps  autonomous robots  three dimensional lookup table  frequency 40.0 Hz  Casting  Table lookup  Robot sensing systems  Transforms  Approximation algorithms  Memory management  Acceleration 
Abstract: Localization is an essential component for autonomous robots. A well-established localization approach combines ray casting with a particle filter, leading to a computationally expensive algorithm that is difficult to run on resource-constrained mobile robots. We present a novel data structure called the Compressed Directional Distance Transform for accelerating ray casting in two dimensional occupancy grid maps. Our approach allows online map updates, and near constant time ray casting performance for a fixed size map, in contrast with other methods exhibit poor worst case performance. Our experimental results show that the proposed algorithm approximates the performance characteristics of reading from a three dimensional lookup table of ray cast solutions while requiring two orders of magnitude less memory and precomputation. This results in a particle filter algorithm which can maintain 2500 particles with 61 ray casts per particle at 40Hz, using a single CPU thread onboard a mobile robot.


Title: Towards Globally Consistent Visual-Inertial Collaborative SLAM
Key Words: autonomous aerial vehicles  mobile robots  path planning  robot vision  SLAM (robots)  globally consistent tracking  autonomous robot navigation  monocular-inertial odometry  vision-based perception  metric scale estimation  benchmarking datasets  UAVs  monocular-inertial sensor suite  unmanned aerial vehicles  visual-inertial collaborative SLAM  drift correction  Simultaneous localization and mapping  Collaboration  Unmanned aerial vehicles  Optimization  Measurement  Trajectory 
Abstract: Motivated by the need for globally consistent tracking and mapping before autonomous robot navigation becomes realistically feasible, this paper presents a novel backend to monocular-inertial odometry. As some of the most challenging platforms for vision-based perception, we evaluate the performance of our system using Unmanned Aerial Vehicles (UAV s). Our experimental validation demonstrates that the proposed approach achieves drift correction and metric scale estimation from a single UAV on benchmarking datasets. Furthermore, the generality of our approach is demonstrated to achieve globally consistent maps built in a collaborative manner from two UAVs, each equipped with a monocular-inertial sensor suite, showing the possible gains opened by collaboration amongst robots to perform SLAM. Video - https://youtu.be/wbX36HBu2Eg.


Title: Topomap: Topological Mapping and Navigation Based on Visual SLAM Maps
Key Words: mobile robots  navigation  path planning  robot vision  SLAM (robots)  three-dimensional topological map  noisy sparse point cloud  convex free-space clusters  global planning  mobile robotic platform  Topomap  visual SLAM  visual robot navigation  navigation task  sparse feature-based map  path planning algorithms  visual simultaneous localization and mapping system  Navigation  Visualization  Simultaneous localization and mapping  Path planning  Three-dimensional displays  Planning 
Abstract: Visual robot navigation within large-scale, semistructured environments deals with various challenges such as computation intensive path planning algorithms or insufficient knowledge about traversable spaces. Moreover, many state-of-the-art navigation approaches only operate locally instead of gaining a more conceptual understanding of the planning objective. This limits the complexity of tasks a robot can accomplish and makes it harder to deal with uncertainties that are present in the context of real-time robotics applications. In this work, we present Topomap, a framework which simplifies the navigation task by providing a map to the robot which is tailored for path planning use. This novel approach transforms a sparse feature-based map from a visual Simultaneous Localization And Mapping (SLAM) system into a three-dimensional topological map. This is done in two steps. First, we extract occupancy information directly from the noisy sparse point cloud. Then, we create a set of convex free-space clusters, which are the vertices of the topological map. We show that this representation improves the efficiency of global planning, and we provide a complete derivation of our algorithm. Planning experiments on real world datasets demonstrate that we achieve similar performance as RRT* with significantly lower computation times and storage requirements. Finally, we test our algorithm on a mobile robotic platform to prove its advantages.


Title: PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor Fusion and Hardware Co-Design
Key Words: cameras  image fusion  robot vision  SLAM (robots)  stereo image processing  synchronisation  embedded simultaneous localization and mapping algorithm  multi-core processor  public visual-inertial datasets  PerceptIn Robotics Vision System  Hardware Co-Design  advanced visual-inertial SLAM System  state-of-the-art visual-inertial algorithms  additional sensor modalities  inertial measurements  visual measurements  flexible sensor fusion approach  PIRVS software features  precise hardware synchronization  global-shutter stereo camera  PIRVS hardware  visual-inertial computing hardware  Simultaneous localization and mapping  Hardware  Cameras  Feature extraction  Synchronization  Visualization 
Abstract: In this paper, we present the PerceptIn Robotics Vision System (PIRVS), a visual-inertial computing hardware with embedded simultaneous localization and mapping (SLAM) algorithm. The PIRVS hardware is equipped with a multi-core processor, a global-shutter stereo camera, and an IMU with precise hardware synchronization. The PIRVS software features a flexible sensor fusion approach to not only tightly integrate visual measurements with inertial measurements and also to loosely couple with additional sensor modalities. It runs in real-time on both PC and the PIRVS hardware. We perform a thorough evaluation of the proposed system using multiple public visual-inertial datasets. Experimental results demonstrate that our system reaches comparable accuracy of state-of-the-art visual-inertial algorithms on PC, while being more efficient on the PIRVS hardware.


Title: ProSLAM: Graph SLAM from a Programmer's Perspective
Key Words: C++ language  data structures  graph theory  public domain software  robot vision  SLAM (robots)  stereo image processing  Graph SLAM  data structures  C++ programming language  standard libraries  lightweight open-source stereo visual SLAM system  programmer  ProSLAM  algorithmic aspects  mathematical aspects  highly modular system  Simultaneous localization and mapping  Visualization  Three-dimensional displays  Cameras  Data structures  Benchmark testing 
Abstract: In this paper we present ProSLAM, a lightweight open-source stereo visual SLAM system designed with simplicity in mind. This work stems from the experience gathered by the authors while teaching SLAM and aims at providing a highly modular system that can be easily implemented and understood. Rather than focusing on the well known mathematical aspects of stereo visual SLAM, we highlight the data structures and the algorithmic aspects required to realize such a system. We implemented ProSLAM using the C++ programming language in combination with a minimal set of standard libraries. The results of a thorough validation performed on several standard benchmark datasets show that ProSLAM achieves precision comparable to state-of-the-art approaches, while requiring substantially less computation.


Title: Talk Resource-Efficiently to Me: Optimal Communication Planning for Distributed Loop Closure Detection
Key Words: mobile robots  multi-robot systems  robot vision  SLAM (robots)  cooperative simultaneous localization and mapping  inter-robot loop closures  general resource-efficiency communication planning  sensory data sharing  distributed loop closure detection  optimal communication planning  CSLAM  Robot sensing systems  Distributed databases  Planning  Trajectory  Visualization  Metadata 
Abstract: Due to the distributed nature of cooperative simultaneous localization and mapping (CSLAM), detecting inter-robot loop closures necessitates sharing sensory data with other robots. A naïve approach to data sharing can easily lead to a waste of mission-critical resources. This paper investigates the logistical aspects of CSLAM. Particularly, we present a general resource-efficient communication planning framework that takes into account both the total amount of exchanged data and the induced division of labor between the participating robots. Compared to other state-of-the-art approaches, our framework is able to verify the same set of potential inter-robot loop closures while exchanging considerably less data and influencing the induced workloads. We develop a fast algorithm for finding globally optimal communication policies, and present theoretical analysis to characterize the necessary and sufficient conditions under which simpler strategies are optimal. The proposed framework is extensively evaluated with data from the KITTI odometry benchmark datasets.


Title: StaticFusion: Background Reconstruction for Dense RGB-D SLAM in Dynamic Environments
Key Words: cameras  image colour analysis  image filtering  image motion analysis  image reconstruction  image segmentation  image sensors  image sequences  motion estimation  object detection  object tracking  pose estimation  probability  robot vision  SLAM (robots)  frame-to-model alignment  3D model estimation  outlier filtering techniques  moving object detection  camera pose tracking  probabilistic static-dynamic segmentation  background structure reconstruction  dynamic scenes  static environments  dynamic sequences  static sequences  camera motion estimation  weighted dense RGB-D fusion  current RGB-D image pair  implicit robust penalisers  background structure  robust dense RGB-D SLAM  visual SLAM  dynamic environments  Cameras  Robustness  Image segmentation  Motion segmentation  Dynamics  Three-dimensional displays  Image reconstruction 
Abstract: Dynamic environments are challenging for visual SLAM as moving objects can impair camera pose tracking and cause corruptions to be integrated into the map. In this paper, we propose a method for robust dense RGB-D SLAM in dynamic environments which detects moving objects and simultaneously reconstructs the background structure. While most methods employ implicit robust penalisers or outlier filtering techniques in order to handle moving objects, our approach is to simultaneously estimate the camera motion as well as a probabilistic static/dynamic segmentation of the current RGB-D image pair. This segmentation is then used for weighted dense RGB-D fusion to estimate a 3D model of only the static parts of the environment. By leveraging the 3D model for frame-to-model alignment, as well as static/dynamic segmentation, camera motion estimation has reduced overall drift - as well as being more robust to the presence of dynamics in the scene. Demonstrations are presented which compare the proposed method to related state-of-the-art approaches using both static and dynamic sequences. The proposed method achieves similar performance in static environments and improved accuracy and robustness in dynamic scenes.


Title: Vision Based Collaborative Path Planning for Micro Aerial Vehicles
Key Words: cameras  collision avoidance  covariance matrices  mobile robots  optimisation  path planning  robot vision  trees (mathematics)  microaerial vehicles  collaborative path-planning framework  localization uncertainty  two-step planning framework  visual-fidelity aerial vehicle simulator  Planning  Uncertainty  Cameras  Three-dimensional displays  Collaboration  Optimization  Path planning 
Abstract: In this paper, we present a collaborative path-planning framework for a group of micro aerial vehicles that are capable of localizing through vision. Each of the micro aerial vehicles is assumed to be equipped with a forward facing monocular camera. The vehicles initially use their captured images to build 3D maps through common features; and subsequently track these features to localize through 3D-2D correspondences. The planning algorithm, while connecting start locations to provided goal locations, also aims to reduce the localization uncertainty of the vehicles in the group. To achieve this, we develop a two-step planning framework: the first step attempts to build an improved map of the environment by solving the next-best-view problem for multiple cameras. We express this as a black-box optimization problem and solve it using the Covariance Matrix Adaption evolution strategy (CMA-ES). Once an improved map is available, the second stage of the planning framework performs belief space planning for the vehicles individually using the rapidly exploring random belief tree (RRBT) algorithm. Through the RRBT approach, the planner generates paths that ensure feature visibility while attempting to optimize path cost and reduce localization uncertainty. We validate our approach using experiments conducted in a high visual-fidelity aerial vehicle simulator, Microsoft AirSim.


Title: Semi-Dense Visual-Inertial Odometry and Mapping for Quadrotors with SWAP Constraints
Key Words: helicopters  inertial navigation  mobile robots  path planning  robot vision  state estimation  stereo image processing  semidense visual-inertial odometry  quadrotors  SWAP constraints  autonomous navigation capabilities  dense 3D maps  indoor environments  visual inertial state estimation  microaerial vehicles  size, weight, and power constraints  stereo camera  Cameras  Three-dimensional displays  Visual odometry  Optimization  Navigation  Robot vision systems 
Abstract: Micro Aerial Vehicles have the potential to assist humans in real life tasks involving applications such as smart homes, search and rescue, and architecture construction. To enhance autonomous navigation capabilities these vehicles need to be able to create dense 3D maps of the environment, while concurrently estimating their own motion. In this paper, we are particularly interested in small vehicles that can navigate cluttered indoor environments. We address the problem of visual inertial state estimation, control and 3D mapping on platforms with Size, Weight, And Power (SWAP) constraints. The proposed approach is validated through experimental results on a 250 g, 22 cm diameter quadrotor equipped only with a stereo camera and an IMU with a computationally-limited CPU showing the ability to autonomously navigate, while concurrently creating a 3D map of the environment.


Title: Obstacle-Aided Navigation of a Soft Growing Robot
Key Words: collision avoidance  mobile robots  path planning  obstacle-aided navigation  soft growing robot  obstacle avoidance  robot path planning  soft robots  intentional obstacle collisions  soft robot navigation  robot-obstacle interaction  tip-extending soft robot  obstacle interaction model  account obstacle collisions  Collision avoidance  Computational modeling  Kinematics  Pneumatic systems  Soft robotics  Navigation 
Abstract: For many types of robots, avoiding obstacles is necessary to prevent damage to the robot and environment. As a result, obstacle avoidance has historically been an important problem in robot path planning and control. Soft robots represent a paradigm shift with respect to obstacle avoidance because their low mass and compliant bodies can make collisions with obstacles inherently safe. Here we consider the benefits of intentional obstacle collisions for soft robot navigation. We develop and experimentally verify a model of robot-obstacle interaction for a tip-extending soft robot. Building on the obstacle interaction model, we develop an algorithm to determine the path of a growing robot that takes into account obstacle collisions. We find that obstacle collisions can be beneficial for open-loop navigation of growing robots because the obstacles passively steer the robot, both reducing the uncertainty of the location of the robot and directing the robot to targets that do not lie on a straight path from the starting point. Our work shows that for a robot with predictable and safe interactions with obstacles, target locations in a cluttered, mapped environment can be reached reliably by simply setting the initial trajectory. This has implications for the control and design of robots with minimal active steering.


Title: Surface-Based Exploration for Autonomous 3D Modeling
Key Words: mobile robots  path planning  solid modelling  surface reconstruction  3D models  exploration algorithm  autonomous 3D modeling  path planning problem  exploration path  low-confidence surfaces  reconstructed surfaces  volumetric model  volumetric map  mobile robot  Surface reconstruction  Computational modeling  Three-dimensional displays  Robot sensing systems  Inspection  Mobile robots  Solid modeling 
Abstract: In this study, we addressed a path planning problem of a mobile robot to construct highly accurate 3D models of an unknown environment. Most studies have focused on exploration approaches, which find the most informative viewpoint or trajectories by analyzing a volumetric map. However, the completion of a volumetric map does not necessarily describe the completion of a 3D model. A highly complicated structure sometimes cannot be represented as a volumetric model. We propose a novel exploration algorithm that considers not only a volumetric map but also reconstructed surfaces. Unlike previous approaches, we evaluate the model completeness according to the quality of the reconstructed surfaces and extract low-confidence surfaces. The surface information is used to guide the computation of the exploration path. Experimental results showed that the proposed algorithm performed better than other state-of-the-art exploration methods and especially improved the completeness and confidence of the 3D models.


Title: Learning to Parse Natural Language to Grounded Reward Functions with Weak Supervision
Key Words: grammars  lambda calculus  learning (artificial intelligence)  natural language processing  robots  trees (mathematics)  parse weights  validation-driven perceptron weight updates  goal-condition learning approach  grounded reward functions  language representations  weighted linear Combinatory Categorial Grammar semantic parser  CCG lexicon  parse trees  robot behaviors  Cleanup World domain  natural language parsing  goal-state reward functions  lambda calculus  Natural languages  Semantics  Task analysis  Planning  Robots  Trajectory  Navigation 
Abstract: In order to intuitively and efficiently collaborate with humans, robots must learn to complete tasks specified using natural language. We represent natural language instructions as goal-state reward functions specified using lambda calculus. Using reward functions as language representations allows robots to plan efficiently in stochastic environments. To map sentences to such reward functions, we learn a weighted linear Combinatory Categorial Grammar (CCG) semantic parser. The parser, including both parameters and the CCG lexicon, is learned from a validation procedure that does not require execution of a planner, annotating reward functions, or labeling parse trees, unlike prior approaches. To learn a CCG lexicon and parse weights, we use coarse lexical generation and validation-driven perceptron weight updates using the approach of Artzi and Zettlemoyer [4]. We present results on the Cleanup World domain [18] to demonstrate the potential of our approach. We report an F1 score of 0.82 on a collected corpus of 23 tasks containing combinations of nested referential expressions, comparators and object properties with 2037 corresponding sentences. Our goal-condition learning approach enables an improvement of orders of magnitude in computation time over a baseline that performs planning during learning, while achieving comparable results. Further, we conduct an experiment with just 6 labeled demonstrations to show the ease of teaching a robot behaviors using our method. We show that parsing models learned from small data sets can generalize to commands not seen during training.


Title: Brain-Computer Interface Meets ROS: A Robotic Approach to Mentally Drive Telepresence Robots
Key Words: brain  brain-computer interfaces  collision avoidance  control engineering computing  geriatrics  handicapped aids  learning (artificial intelligence)  medical robotics  medical signal processing  mobile robots  operating systems (computers)  patient rehabilitation  position control  robot programming  telerobotics  video streaming  noninvasive Brain-Computer Interface  Robot Operating System  telepresence robot  mobile device  human brain signals  severe physical disabilities  elderly people  BCI user  robot position control  obstacle avoidance  video streaming  Navigation  Robot sensing systems  Task analysis  Telepresence  Brain-computer interfaces 
Abstract: This paper shows and evaluates a novel approach to integrate a non-invasive Brain-Computer Interface (BCI) with the Robot Operating System (ROS) to mentally drive a telepresence robot. Controlling a mobile device by using human brain signals might improve the quality of life of people suffering from severe physical disabilities or elderly people who cannot move anymore. Thus, the BCI user can actively interact with relatives and friends located in different rooms thanks to a video streaming connection to the robot. To facilitate the control of the robot via BCI, we explore new ROS-based algorithms for navigation and obstacle avoidance in order to make the system safer and more reliable. In this regard, the robot exploits two maps of the environment, one for localization and one for navigation, and both are used as additional visual feedback for the BCI user to control the robot position. Experimental results show a decrease of the number of commands needed to complete the navigation task, suggesting a reduction user's cognitive workload. The novelty of this work is to provide a first evidence of an integration between BCI and ROS that can simplify and foster the development of software for BCI driven robotics devices.


Title: Constructing Category-Specific Models for Monocular Object-SLAM
Key Words: cameras  feature extraction  mobile robots  object detection  pose estimation  robot vision  SLAM (robots)  category-specific models  real-time object-oriented SLAM  monocular camera  object-level models  category-level models  object deformations  discriminative object features  category models  object landmark observations  generic monocular SLAM framework  2D object features  sparse feature-based monocular SLAM  object instance retrieval  instance-independent monocular object-SLAM system  feature-based SLAM methods  time 2.0 d  time 3.0 d  Solid modeling  Simultaneous localization and mapping  Three-dimensional displays  Object oriented modeling  Pipelines  Two dimensional displays  Shape 
Abstract: We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fty, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show - to the best of our knowledge - first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.


Title: DPDB-Net: Exploiting Dense Connections for Convolutional Encoders
Key Words: convolutional codes  decoding  image classification  image coding  image segmentation  neural net architecture  dense connections  Cam Vid dataset  Freiburg Forest dataset  convolutional encoders  multiple segmentation tasks  feature map explosion  residual network architecture  dense block  DPDB-Net  Dual-Path Dense-Block Network  encoder-decoder architectures  feature re-usage  dense networks  multiple classification tasks  feature exploration  densely connected networks  Decoding  Computer architecture  Semantics  Task analysis  Explosions  Image segmentation  Forestry 
Abstract: Densely connected networks for classification enable feature exploration and result in state-of-the-art performance on multiple classification tasks. The alternative to dense networks is the residual network which enables feature re-usage. In this work, we combine these orthogonal concepts for encoder-decoder architectures, which we call Dual-Path Dense-Block Network (DPDB-Net). We introduce a dense block which incorporates feature re-usage and new feature exploration in the encoder. Moreover, we discuss that feature re-usage by the residual network architecture leads to a feature map explosion in the decoder and, thus, is not advantageous in this part of the network. We evaluated our proposed architecture in multiple segmentation tasks and report state-of-the-art performance on the Freiburg Forest dataset and competitive results on the Cam Vid dataset.


Title: The Hands-Free Push-Cart: Autonomous Following in Front by Predicting User Trajectory Around Obstacles
Key Words: collision avoidance  mobile robots  motion control  object detection  trajectory control  autonomous mobile robot  walking user  autonomous push-carts  multimodal person detection  human-motion model  obstacle mapper  human tracker  human motion model  robot motion planner  robot motion controller  industrial entertainment applications  domestic entertainment applications  hands-free push-cart  predicting user trajectory  Robot sensing systems  Legged locomotion  Cameras  Tracking  Trajectory  Predictive models 
Abstract: This paper demonstrates an autonomous mobile robot that follows a walking user while staying ahead of them. Despite several useful applications for autonomous push-carts, this problem has received much less attention than the easier problem of following from behind. In contrast to previous work, we use multi-modal person detection and a human-motion model that considers obstacles to predict the future path of the user. We implement the system with a modular architecture of obstacle mapper, human tracker, human motion model, robot motion planner and robot motion controller. We report on the performance of the robot in real-world experiments. We believe that approaches to this largely overlooked problem could be useful in real industrial, domestic and entertainment applications in the near future.


Title: Fully Convolutional Neural Networks for Road Detection with Multiple Cues Integration
Key Words: convergence  convolution  feature extraction  feedforward neural nets  gradient methods  image colour analysis  learning (artificial intelligence)  mobile robots  optical radar  position control  convolutional neural networks  multiple cues integration  autonomous driving  deep learning  road detection algorithms  pre-trained Resnet-lOl  RGB images  CNN  feature maps extraction  Lidar scanner  position map  image gradient  convergence  KITTI benchmark  Roads  Feature extraction  Laser radar  Three-dimensional displays  Task analysis  Fuses  Network architecture 
Abstract: Road detection from images is a key task in autonomous driving. The recent advent of deep learning (and in particular, CNN or convolutional neural networks) has greatly improved the performance of road detection algorithms. In this paper, we show how to fuse multiple different cues under the same convolutional network framework. Specifically, we adopt a pre-trained Resnet-lOl to extract feature maps from RGB images; we then connect it with three extra deconvolution layers. These deconvolution layers is trained conditioning on appropriate image cues, and in our case they are a height image (i.e. elevation map obtained by e.g. Lidar scanner), image gradient, and position map. We also design two skip layers to speed up the convergence. Experiments on KITTI benchmark show competitive performance of our new networks.


Title: A Deep Learning Based Behavioral Approach to Indoor Autonomous Navigation
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  navigational behaviors  deep learning architectures  semantic abstraction  navigation tasks  navigational missions  behavioral approach  indoor autonomous navigation  semantically rich graph representation  indoor robotic navigation  semantic locations  Navigation  Semantics  Visualization  Simultaneous localization and mapping  Robustness  Measurement 
Abstract: We present a semantically rich graph representation for indoor robotic navigation. Our graph representation encodes: semantic locations such as offices or corridors as nodes, and navigational behaviors such as enter office or cross a corridor as edges. In particular, our navigational behaviors operate directly from visual inputs to produce motor controls and are implemented with deep learning architectures. This enables the robot to avoid explicit computation of its precise location or the geometry of the environment, and enables navigation at a higher level of semantic abstraction. We evaluate the effectiveness of our representation by simulating navigation tasks in a large number of virtual environments. Our results show that using a simple sets of perceptual and navigational behaviors, the proposed approach can successfully guide the way of the robot as it completes navigational missions such as going to a specific office. Furthermore, our implementation shows to be effective to control the selection and switching of behaviors.


Title: VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving
Key Words: data visualisation  feedforward neural nets  learning (artificial intelligence)  object detection  traffic engineering computing  video signal processing  convolutional neural network  irrelevant information  prediction decision  CNN-based systems  steering self-driving cars  visualization method  valuable debugging tool  theoretical arguments  input pixels  individual pixels  visualization tool  NVIDIA neural-network-based end-to-end learning system  autonomous driving  VisualBackProp  public road video data  layer-wise relevance propagation approach  similar visualization results  PilotNet steering decision  relevant object capture  Neurons  Visualization  Deconvolution  Tools  Biological neural networks  Roads  Data visualization 
Abstract: This paper proposes a new method, that we call VisualBackProp, for visualizing which sets of pixels of the input image contribute most to the predictions made by the convolutional neural network (CNN). The method heavily hinges on exploring the intuition that the feature maps contain less and less irrelevant information to the prediction decision when moving deeper into the network. The technique we propose is dedicated for CNN-based systems for steering self-driving cars and is therefore required to run in real-time. This makes the proposed visualization method a valuable debugging tool which can be easily used during both training and inference. We justify our approach with theoretical arguments and confirm that the proposed method identifies sets of input pixels, rather than individual pixels, that collaboratively contribute to the prediction. We utilize the proposed visualization tool in the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet. We demonstrate that VisualBackProp determines which elements in the road image most influence PilotNet's steering decision and indeed captures relevant objects on the road. The empirical evaluation furthermore shows the plausibility of the proposed approach on public road video data as well as in other applications and reveals that it compares favorably to the layer-wise relevance propagation approach, i.e. it obtains similar visualization results and achieves order of magnitude speed-ups.


Title: Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image
Key Words: image colour analysis  image reconstruction  image resolution  image sampling  image segmentation  image sensors  learning (artificial intelligence)  mean square error methods  optical radar  random processes  regression analysis  SLAM (robots)  sparse matrices  prediction root-mean-square error  sparse maps  dense maps  sparse-to-dense  dense depth prediction  sparse set  depth measurements  single RGB image  depth estimation  monocular images  low-resolution depth sensor  single deep regression network  RGB-D raw data  sparse depth samples  visual simultaneous localization and mapping algorithms  plug-in module  NYU-depth-v2 indoor dataset  LiDARs  Training  Laser radar  Image reconstruction  Estimation  Prediction algorithms  Simultaneous localization and mapping 
Abstract: We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software2 and video demonstration3 are publicly available.


Title: Multi-Stage Suture Detection for Robot Assisted Anastomosis Based on Deep Learning
Key Words: blood vessels  convolution  image recognition  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  recurrent neural nets  surgery  thread centerline reconstruction  multistage suture detection  robot assisted anastomosis  deep learning  robust suture detection  suture augmentation  robotic-assisted surgery  fully convolutional neural networks  trainee suturing skill evaluation  curvilinear structure detector  Yarn  Instruction sets  Surgery  Splines (mathematics)  Image reconstruction  Robots  Task analysis 
Abstract: The technique of robust suture detection is vital in many applications including trainee suturing skill evaluation, suture augmentation in robotic-assisted surgery and suture recognition for automatic suturing. Due to the complicated environment of surgery, the detection of a suture is challenged by high deformation and frequent occlusion. In this paper, we propose a deep multi-stage framework for suture detection. The fully convolutional neural networks are firstly used to predict a gradient map which not only serves as a segmentation mask, but also provides useful structure information for the following thread centerline reconstruction. An overlapping map is also predicted to improve the quality of the gradient map in self-intersection area. Based on the gradient map, multiple segments of the thread are extracted and linked to form the whole thread using a curvilinear structure detector. Experiments on two types of threads demonstrate that the proposed method is able to detect the thread with human level performance when the thread is no occlusion or under finite self-intersection.


Title: Topological Hotspot Identification for Informative Path Planning with a Marine Robot
Key Words: computational geometry  graph theory  greedy algorithms  image segmentation  marine control  mobile robots  path planning  robot vision  topological graph  greedy-coverage algorithm  informative path planning problem  topological hotspot identification  marine robot  topological map  biological hotspots  aquatic environment  Fast Marching-based Voronoi segmentation  scheduling problem  Path planning  Monitoring  Oceans  Robot sensing systems  Task analysis  Frequency modulation 
Abstract: In this work, we present a novel method for constructing a topological map of biological hotspots in an aquatic environment using a Fast Marching-based Voronoi segmentation. Using this topological map, we develop a closed form solution to the scheduling problem for any single path through the graph. Searching over the space of all paths allows us to compute a maximally informative path that traverses a subset of the hotspots, given some budget. Using a greedy-coverage algorithm we can then compute an informative path. We evaluate our method in a set of simulated trials, both with randomly generated environments and a real-world environment. In these trials, we show that our method produces a topological graph which more accurately captures features in the environment than standard thresholding techniques. Additionally, We show that our method can improve the performance of a greedy-coverage algorithm in the informative path planning problem by guiding it to different informative areas to help it escape from local maxima.


Title: Heterogeneous Multi-Robot System for Exploration and Strategic Water Sampling
Key Words: ecology  hydrological equipment  hydrological techniques  microorganisms  multi-robot systems  remotely operated vehicles  reservoirs  water quality  data-driven behavior  real geophysical data  MODIS measurements  water-sampling apparatus  water quality sensor  plankton-rich water samples  chlorophyll density  autonomous surface vehicles plan  water-sampling behavior  efficient measurement  fresh-water systems  measuring contamination levels  drinking water  physical sampling  strategic water sampling  heterogeneous multirobot system  water reservoir  explorer robot  water sampling apparatus  ASV  Pollution measurement  Geophysical measurements  Robot sensing systems  Real-time systems  Time measurement  Water pollution 
Abstract: Physical sampling of water for off-site analysis is necessary for many applications like monitoring the quality of drinking water in reservoirs, understanding marine ecosystems, and measuring contamination levels in fresh-water systems. In this paper, the focus is on algorithms for efficient measurement and sampling using a multi-robot, data-driven, water-sampling behavior, where autonomous surface vehicles plan and execute water sampling using the chlorophyll density as a cue for plankton-rich water samples. We use two Autonomous Surface Vehicles (ASVs), one equipped with a water quality sensor and the other equipped with a water-sampling apparatus. The ASV with the sensor acts as an explorer, measuring and building a spatial map of chlorophyll density in the given region of interest. The ASV equipped with the water sampling apparatus makes decisions in real time on where to sample the water based on the suggestions made by the explorer robot. We evaluate the system in the context of measuring chlorophyll distributions. We do this both in simulation based on real geophysical data from MODIS measurements, and on real robots in a water reservoir. We demonstrate the effectiveness of the proposed approach in several ways including in terms of mean error in the interpolated data as a function of distance traveled.


Title: On Geometric Models and Their Accuracy for Extrinsic Sensor Calibration
Key Words: calibration  geometry  numerical analysis  sensors  extrinsic sensor calibration methods  robotics  numerical simulation  abstract geometric model  Calibration  Cameras  Estimation  Task analysis  Simultaneous localization and mapping 
Abstract: Extrinsic sensor calibration is an important task in robotics. There are various ways to perform the calibration task, but it often remains unclear which methods are better than the others. In this paper, we provide a systematic study about the calibration accuracy of three types of calibration methods, each represented by an abstract geometric model based on the sensor configuration and the calibration setup. We discuss the advantages and disadvantages of each model and perform a rigorous study on their noise sensitivity from a geometric perspective. As a result, we can reveal and quantify the relative calibration accuracies of the three models, thus answering the question of “which model is better and why?”. Beside our analytical analysis, we also provide numerical simulation experiments that validate our findings.


Title: A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration
Key Words: image registration  image sensors  mobile robots  sensor fusion  flexible framework  general framework  explicit data association  flexible multicue photometric point cloud registration  mobile robots  mapping systems  recorded sensor data  photometric registration  multiple modalities  image data streams  pixel-wise difference  multichannel images  Three-dimensional displays  Robot sensing systems  Cameras  Iterative closest point algorithm  Minimization  Integrated circuit modeling  Laser radar 
Abstract: The ability to build maps is a key functionality for the majority of mobile robots. A central ingredient to most mapping systems is the registration or alignment of the recorded sensor data. In this paper, we present a general methodology for photometric registration that can deal with multiple different cues. We provide examples for registering RGBD as well as 3D LIDAR data. In contrast to popular point cloud registration approaches such as ICP our method does not rely on explicit data association and exploits multiple modalities such as raw range and image data streams. Color, depth, and normal information are handled in an uniform manner and the registration is obtained by minimizing the pixel-wise difference between two multi-channel images. We developed a flexible and general framework and implemented our approach inside that framework. We also released our implementation as open source C++ code. The experiments show that our approach allows for an accurate registration of the sensor data without requiring an explicit data association or model-specific adaptations to datasets or sensors. Our approach exploits the different cues in a natural and consistent way and the registration can be done at framerate for a typical range or imaging sensor.


Title: Just-in-Time Reconstruction: Inpainting Sparse Maps Using Single View Depth Predictors as Priors
Key Words: convolution  feature extraction  image colour analysis  image fusion  image reconstruction  image sensors  iterative methods  neural nets  pose estimation  recurrent neural nets  robot vision  SLAM (robots)  stereo image processing  CRF model  RGB image  confidence-based fusion  realtime inpainting  convolutional neural networks  CNN  ORB-SLAM  Kinect  conditional depth error distributions  pixel-wise confidence weights  input depth map  fused depth map  virtual depth sensor  single-view depth prediction network  sparse sensor  monocular visual SLAM system  fully dense depth map  realtime image-guided inpainting  just-in-time reconstruction  single view depth predictors  scale-invariant depth error  outlier input depth  LIDAR depth maps  arbitrary scale  sparse map  Image reconstruction  Simultaneous localization and mapping  Visualization  Three-dimensional displays  Real-time systems  Uncertainty 
Abstract: We present “just-in-time reconstruction” as realtime image-guided inpainting of a map with arbitrary scale and sparsity to generate a fully dense depth map for the image. In particular, our goal is to inpaint a sparse map - obtained from either a monocular visual SLAM system or a sparse sensor - using a single-view depth prediction network as a virtual depth sensor. We adopt a fairly standard approach to data fusion, to produce a fused depth map by performing inference over a novel fully-connected Conditional Random Field (CRF) which is parameterized by the input depth maps and their pixel-wise confidence weights. Crucially, we obtain the confidence weights that parameterize the CRF model in a data-dependent manner via Convolutional Neural Networks (CNNs) which are trained to model the conditional depth error distributions given each source of input depth map and the associated RGB image. Our CRF model penalises absolute depth error in its nodes and pairwise scale-invariant depth error in its edges, and the confidence-based fusion minimizes the impact of outlier input depth values on the fused result. We demonstrate the flexibility of our method by real-time inpainting of ORB-SLAM, Kinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary scale and varied amount of irregular sparsity.


Title: Fast Global Labelling for Depth-Map Improvement Via Architectural Priors
Key Words: cameras  image reconstruction  spatial variables measurement  cameras  planar extraction  urban environment  vision-only method  depth map estimation techniques  fast global labelling  Labeling  Estimation  Minimization  Image reconstruction  Cameras  Pipelines  Surface texture 
Abstract: Depth map estimation techniques from cameras often struggle to accurately estimate the depth of large textureless regions. In this work we present a vision-only method that accurately extracts planar priors from a viewed scene without making any assumptions of the underlying scene layout. Through a fast global labelling, these planar priors can be associated to the individual pixels leading to more complete depth-maps specifically over large, plain and planar regions that tend to dominate the urban environment. When these depth-maps are deployed to the creation of a vision only dense reconstruction over large scales, we demonstrate reconstructions that yield significantly better results in terms of coverage while still maintaining high accuracy.


Title: A Method to Segment Maps from Different Modalities Using Free Space Layout MAORIS: Map of Ripples Segmentation
Key Words: computational geometry  convolution  image segmentation  robot vision  hand-drawn sketch maps  segmentation evaluation metric  ground-truth segmentations  Voronoi-based segmentation method  DuDe segmentation method  ground truth segmentations  segment maps  free space layout MAORIS  navigation maps  semantic representations  convolution  circular kernel  ripple-like patterns  Matthews correlation coefficient  map of ripples segmentation  Image segmentation  Merging  Robot kinematics  Robot sensing systems  Measurement  Two dimensional displays 
Abstract: How to divide floor plans or navigation maps into semantic representations, such as rooms and corridors, is an important research question in fields such as human-robot interaction, place categorization, or semantic mapping. While most works focus on segmenting robot built maps, those are not the only types of map a robot, or its user, can use. We present a method for segmenting maps from different modalities, focusing on robot built maps and hand-drawn sketch maps, and show better results than state of the art for both types. Our method segments the map by doing a convolution between the distance image of the map and a circular kernel, and grouping pixels of the same value. Segmentation is done by detecting ripple-like patterns where pixel values vary quickly, and merging neighboring regions with similar values. We identify a flaw in the segmentation evaluation metric used in recent works and propose a metric based on Matthews correlation coefficient (MCC). We compare our results to ground-truth segmentations of maps from a publicly available dataset, on which we obtain a better MCC than the state of the art with 0.98 compared to 0.65 for a recent Voronoi-based segmentation method and 0.70 for the DuDe segmentation method. We also provide a dataset of sketches of an indoor environment, with two possible sets of ground truth segmentations, on which our method obtains an MCC of 0.56 against 0.28 for the Voronoi-based segmentation method and 0.30 for DuDe.


Title: Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping
Key Words: continuous time systems  entropy  graph theory  image registration  image resolution  laser ranging  optical radar  SLAM (robots)  solid modelling  stereo image processing  laser-range scanners  high data rate  3D laser scanner  surfel-based registration  recursive state estimation  multiresolution maps  continuous-time SLAM  3D lidar-based online mapping  online simultaneous localization and mapping  Three-dimensional displays  Measurement by laser beam  Optimization  Trajectory  Laser modes  Simultaneous localization and mapping 
Abstract: Modern 3D laser-range scanners have a high data rate, making online simultaneous localization and mapping (SLAM) computationally challenging. Recursive state estimation techniques are efficient but commit to a state estimate immediately after a new scan is made, which may lead to misalignments of measurements. We present a 3D SLAM approach that allows for refining alignments during online mapping. Our method is based on efficient local mapping and a hierarchical optimization back-end. Measurements of a 3D laser scanner are aggregated in local multiresolution maps by means of surfel-based registration. The local maps are used in a multi-level graph for allocentric mapping and localization. In order to incorporate corrections when refining the alignment, the individual 3D scans in the local map are modeled as a sub-graph and graph optimization is performed to account for drift and misalignments in the local maps. Furthermore, in each sub-graph, a continuous-time representation of the sensor trajectory allows to correct measurements between scan poses. We evaluate our approach in multiple experiments by showing qualitative results. Furthermore, we quantify the map quality by an entropy-based measure.


Title: Coverage Control for Wire-Traversing Robots
Key Words: gradient methods  minimisation  mobile robots  motion control  path planning  continuous constrained coverage control problem  mobile robots  COW map  Continuous Onto Wires map  constrained locational cost minimization  final projection step  Lloyd descent algorithm  planar environment  continuous motion  one-dimensional manifolds  two-dimensional motion  wire-traversing robots  Wires  Minimization  Robot sensing systems  Optimization  Motion control  Power transmission lines 
Abstract: In this paper we consider the coverage control problem for a team of wire-traversing robots. The two-dimensional motion of robots moving in a planar environment has to be projected to one-dimensional manifolds representing the wires. Starting from Lloyd's descent algorithm for coverage control, a solution that generates continuous motion of the robots on the wires is proposed. This is realized by means of a Continuous Onto Wires (COW) map: the robots' workspace is mapped onto the wires on which the motion of the robots is constrained to be. A final projection step is introduced to ensure that the configuration of the robots on the wires is a local minimizer of the constrained locational cost. An algorithm for the continuous constrained coverage control problem is proposed and it is tested both in simulation and on a team of mobile robots.


Title: Shaping in Practice: Training Wheels to Learn Fast Hopping Directly in Hardware
Key Words: control engineering computing  learning (artificial intelligence)  legged locomotion  robust control  temporary modifications  physical hardware  robot leg  reward landscape  fast hopping  robot controllers  engineering effort  potentially unstable parameters  training wheels  video synopsis  boom learning  robustness  Legged locomotion  Training  Wheels  Hardware  Hip 
Abstract: Learning instead of designing robot controllers can greatly reduce engineering effort required, while also emphasizing robustness. Despite considerable progress in simulation, applying learning directly in hardware is still challenging, in part due to the necessity to explore potentially unstable parameters. We explore the concept of shaping the reward landscape with training wheels; temporary modifications of the physical hardware that facilitate learning. We demonstrate the concept with a robot leg mounted on a boom learning to hop fast. This proof of concept embodies typical challenges such as instability and contact, while being simple enough to empirically map out and visualize the reward landscape. Based on our results we propose three criteria for designing effective training wheels for learning in robotics. A video synopsis can be found at https://youtu.be/6iH5E3LrYh8.


Title: PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-Based Planning
Key Words: learning (artificial intelligence)  mobile robots  navigation  neural nets  path planning  probability  robot dynamics  robot vision  sampling methods  sampling based planner  hierarchical method  sampling based path planning  large scale topology  probabilistic roadmaps  feature based deep neural net policies  continuous state  action spaces  simulation  office environments  aerial cargo delivery  urban environments  load displacement constraints  trajectories  noisy sensor conditions  flights  training  PRM RL  long range robotic navigation tasks  point to point navigation policies  end to end differential drive indoor navigation  nontrivial robot dynamics  robot configurations  task constraints  capture robot dynamics  RL agent  reinforcement learning  Task analysis  Robot sensing systems  Indoor navigation  Aerospace electronics  Learning (artificial intelligence) 
Abstract: We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling-based path planning with reinforcement learning (RL). The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of the large-scale topology. Next, the sampling-based planners provide roadmaps which connect robot configurations that can be successfully navigated by the RL agent. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. Our results show improvement in task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an environment 63 million times larger than used in training.


Title: Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation
Key Words: learning (artificial intelligence)  mobile robots  path planning  robot vision  double Q-learning  self-supervised deep reinforcement learning  self-supervised training  model-based methods  value-based model-free methods  learning-based methods  planning method  internal map  robot navigation  generalized computation graph  Computational modeling  Navigation  Learning (artificial intelligence)  Robots  Task analysis  Prediction algorithms  Planning 
Abstract: Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg.


Title: Direct Line Guidance Odometry
Key Words: distance measurement  feature extraction  robot vision  SLAM (robots)  direct line guidance odometry  pixel intensities  line-based features  point-based direct monocular visual odometry method  visual odometry algorithms  feature extraction  keypoint selection  Feature extraction  IP networks  Cameras  Optimization  Visual odometry  Simultaneous localization and mapping  Computational efficiency 
Abstract: Modern visual odometry algorithms utilize sparse point-based features for tracking due to their low computational cost. Current state-of-the-art methods are split between indirect methods that process features extracted from the image, and indirect methods that deal directly on pixel intensities. In recent years, line-based features have been used in SLAM and have shown an increase in performance albeit with an increase in computational cost. In this paper, we propose an extension to a point-based direct monocular visual odometry method. Here we that uses lines to guide keypoint selection rather than acting as features. Points on a line are treated as stronger keypoints than those in other parts of the image, steering point-selection away from less distinctive points and thereby increasing efficiency. By combining intensity and geometry information from a set of points on a line, accuracy may also be increased.


Title: Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System
Key Words: cameras  image matching  motion estimation  motion measurement  optical radar  optical sensors  optical tracking  optical windows  portable instruments  SLAM (robots)  sparse depth information  motion estimation  pose-graph SLAM  KITTI odometry benchmark datasets  direct visual SLAM  monocular camera  light detection and ranging  portable camera-LiDAR mapping system  direct visual simultaneous localization and mapping  sliding window-based tracking method  depth-integrated frame matching  feature-based visual LiDAR mapping  sensors  Cameras  Laser radar  Three-dimensional displays  Visualization  Simultaneous localization and mapping  Optimization 
Abstract: This paper describes a framework for direct visual simultaneous localization and mapping (SLAM) combining a monocular camera with sparse depth information from Light Detection and Ranging (LiDAR). To ensure realtime performance while maintaining high accuracy in motion estimation, we present (i) a sliding window-based tracking method, (ii) strict pose marginalization for accurate pose-graph SLAM and (iii) depth-integrated frame matching for large-scale mapping. Unlike conventional feature-based visual and LiDAR mapping, the proposed approach is direct, eliminating the visual feature in the objective function. We evaluated results using our portable camera-LiDAR system as well as KITTI odometry benchmark datasets. The experimental results prove that the characteristics of two complementary sensors are very effective in improving real-time performance and accuracy. Via validation, we achieved low drift error of 0.98 % in the KITTI benchmark including various environments such as a highway and residential areas.


Title: Bayesian Scale Estimation for Monocular SLAM Based on Generic Object Detection for Correcting Scale Drift
Key Words: Bayes methods  learning (artificial intelligence)  object detection  robot vision  SLAM (robots)  monocular SLAM system  Bayesian framework  deep-learning based generic object detector  detection region  scale drift  monocular systems  Bayesian scale estimation  generic object detection  local scale correction  object class detection  KITTI dataset  quantitative evaluations  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Trajectory  Bayes methods  Object detection  Image reconstruction 
Abstract: We propose a novel real-time algorithm for estimating the local scale correction of a monocular SLAM system, to obtain a correctly scaled version of the 3D map and of the camera trajectory. Within a Bayesian framework, it integrates observations from a deep-learning based generic object detector and landmarks from the map whose projection lie inside a detection region, to produce scale correction estimates from single frames. For each observation, a prior distribution on the height of the detected object class is used to define the observation's likelihood. Due to the scale drift inherent to monocular SLAM systems, we also incorporate a rough model on the dynamics of scale drift. Quantitative evaluations are presented on the KITTI dataset, and compared with different approaches. The results show a superior performance of our proposal in terms of relative translational error when compared to other monocular systems based on object detection.


Title: Efficient Active SLAM Based on Submap Joining, Graph Topology and Convex Optimization
Key Words: computational complexity  convex programming  least squares approximations  minimisation  mobile robots  path planning  predictive control  quadratic programming  robot vision  SLAM (robots)  graph topology  active SLAM problem  robot trajectory  area coverage task  model predictive control framework  uncertainty minimization MPC problem  graphical structure  2D feature-based SLAM  variable substitutions  convex optimization method  MPC framework  sequential quadratic programming method  linear SLAM  submap joining approach  planning  simultaneous localization and mapping  nonconvex constrained least-squares problem  Optimized production technology  Simultaneous localization and mapping  Uncertainty  Task analysis  Robot kinematics 
Abstract: The active SLAM problem considered in this paper aims to plan a robot trajectory for simultaneous localization and mapping (SLAM) as well as for an area coverage task with robot pose uncertainty. Based on a model predictive control (MPC) framework, these two problems are solved respectively by different methods. For the uncertainty minimization MPC problem, based on the graphical structure of the 2D feature-based SLAM, a non-convex constrained least-squares problem is presented to approximate the original problem. Then, using variable substitutions, it is further transformed into a convex problem, and then solved by a convex optimization method. For the coverage task considering robot pose uncertainty, it is formulated and solved by the MPC framework and the sequential quadratic programming (SQP) method. In the whole process, considering the computation complexity, we use linear SLAM, which is a submap joining approach, to reduce the time for planning and estimation. Finally, various simulations are presented to validate the effectiveness of the proposed approach.


Title: 2D SLAM Correction Prediction in Large Scale Urban Environments
Key Words: image representation  mobile robots  multilayer perceptrons  pose estimation  robot vision  SLAM (robots)  autonomous mobile robots  large scale urban environments  simultaneous location and mapping  hybrid correction module  likelihood distributions  2D likelihood SLAM approaches  successive estimated poses  Ensemble Multilayer Perceptron model  SLAM estimations  systematic errors  sensor measurement errors  SLAM map representation  observation model  motion model  probabilistic formulation  Simultaneous localization and mapping  Two dimensional displays  Estimation  Neural networks  Predictive models  Kalman filters 
Abstract: Simultaneous Localization And Mapping (SLAM) is one of the major bricks needed to build truly autonomous mobile robots. The probabilistic formulation of SLAM is based on two models: the motion model and the observation model. In practice, these models, together with the SLAM map representation, do not model perfectly the robot's real dynamics, the sensor measurement errors and the environment. Consequently, systematic errors affect SLAM estimations. In this paper, we propose two approaches to predict corrections to be applied to SLAM estimations. Both are based on the Ensemble Multilayer Perceptron model. The first approach uses successive estimated poses to predict the errors, with no assumptions on the underlying SLAM process or sensor used. The second method is specific to 2D likelihood SLAM approaches, thus, the likelihood distributions are used to predict the corrections, making this second approach independent of the sensor used. We also build a hybrid correction module based on successive estimated poses and the likelihood distributions. The validity of both approaches is evaluated through two experiments using different evaluation metrics and sensor configurations.


Title: Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition
Key Words: mobile robots  object recognition  robot vision  sensor fusion  SLAM (robots)  omnidirectional multisensory perception fusion  long-term place recognition  long-term autonomy  omnidirectional sensors  omnidirectional observation  multidirectional place recognition  omnidirectional multisensory data  appearance variations  Simultaneous Localization and Mapping  Feature extraction  Sensor phenomena and characterization  Simultaneous localization and mapping  Optimization 
Abstract: Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.


Title: Online Initialization and Automatic Camera-IMU Extrinsic Calibration for Monocular Visual-Inertial SLAM
Key Words: accelerometers  calibration  cameras  gyroscopes  inertial navigation  iterative methods  mobile robots  optimisation  robot vision  SLAM (robots)  extrinsic orientation  extrinsic translation  accelerometer bias  camera-IMU extrinsic parameters  initial values  visual scale  initialization stage  mechanical configuration  sensor suite changes  online initialization method  translation calibration  initialization procedure  gyroscope bias  monocular visual-inertial SLAM techniques  gyroscope  gravitational magnitude  Gyroscopes  Cameras  Quaternions  Accelerometers  Calibration  Simultaneous localization and mapping  Gravity 
Abstract: Most of the existing monocular visual-inertial SLAM techniques assume that the camera-IMU extrinsic parameters are known, therefore these methods merely estimate the initial values of velocity, visual scale, gravity, biases of gyroscope and accelerometer in the initialization stage. However, it's usually a professional work to carefully calibrate the extrinsic parameters, and it is required to repeat this work once the mechanical configuration of the sensor suite changes slightly. To tackle this problem, we propose an online initialization method to automatically estimate the initial values and the extrinsic parameters without knowing the mechanical configuration. The biases of gyroscope and accelerometer are considered in our method, and a convergence criteria for both orientation and translation calibration is introduced to identify the convergence and to terminate the initialization procedure. In the three processes of our method, an iterative strategy is firstly introduced to iteratively estimate the gyroscope bias and the extrinsic orientation. Secondly, the scale factor, gravity, and extrinsic translation are approximately estimated without considering the accelerometer bias. Finally, these values are further optimized by a refinement algorithm in which the accelerometer bias and the gravitational magnitude are taken into account. Extensive experimental results show that our method achieves competitive accuracy compared with the state-of-the-art with less calculation.


Title: Sonar Visual Inertial SLAM of Underwater Structures
Key Words: oceanographic techniques  SLAM (robots)  sonar  underwater sound  underwater vehicles  underwater structures  acoustic range data  sonar visual inertial SLAM  visual-inertial state estimation package  resource management  marine archaeology  underwater acoustic sensor  underwater cave  underwater wrecks  underwater domain  Sonar  Cameras  Visualization  Sonar navigation  Simultaneous localization and mapping  Underwater structures 
Abstract: This paper presents an extension to a state of the art Visual-Inertial state estimation package (OKVIS) in order to accommodate data from an underwater acoustic sensor. Mapping underwater structures is important in several fields, such as marine archaeology, search and rescue, resource management, hydrogeology, and speleology. Collecting the data, however, is a challenging, dangerous, and exhausting task. The underwater domain presents unique challenges in the quality of the visual data available; as such, augmenting the exteroceptive sensing with acoustic range data results in improved reconstructions of the underwater structures. Experimental results from underwater wrecks, an underwater cave, and a submerged bus demonstrate the performance of our approach.


Title: Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation
Key Words: control engineering computing  human-robot interaction  learning by example  manipulators  neural nets  robot programming  robot vision  telerobotics  virtual reality  virtual reality teleoperation  robot skill acquisition  raw pixels  consumer-grade Virtual Reality headsets  hand tracking hardware  deep neural network policies  manipulation tasks  deep imitation learning  PR2 robot  RGB-D images  Robots  Task analysis  Neural networks  Three-dimensional displays  Head  Visualization  Grippers 
Abstract: Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.


Title: Navigating Congested Environments with Risk Level Sets
Key Words: collision avoidance  mobile robots  multi-agent systems  road vehicles  risk level set  congested environment navigation  cluttered environment  congestion cost  occupancy risk  cost function  planning space  agent planning  autonomous vehicle driving  risk threshold  conservative behavior  aggressive behavior  Planning  Level set  Navigation  Vehicle dynamics  Autonomous vehicles  Collision avoidance  Cost function 
Abstract: In this paper, we address the problem of navigating in a cluttered environment by introducing a congestion cost that maps the density and motion of objects to an occupancy risk. We propose that an agent can choose a “risk level set” from this cost function and construct a planning space from this set. In choosing different levels of risk, the agent adjusts its interactions with the other agents. From the assumption that agents are self-preserving, we show that any agent planning within their risk level set will avoid collisions with other agents. We then present an application of planning with risk level sets in the framework of an autonomous vehicle driving along a highway. Using the risk level sets, the agent can determine safe zones when planning a sequence of lane changes. Through simulations in Matlab, we demonstrate how the choice of risk threshold manifests as aggressive or conservative behavior.


Title: Topological Multi-Robot Belief Space Planning in Unknown Environments
Key Words: Bayes methods  graph theory  multi-robot systems  path planning  topology  graph pruning  topological properties  factor graphs  topological space  embedded state space  high-dimensional state spaces  announced path approach  topological multirobot belief space planning  BSP approaches  factor graph representation  posterior beliefs  Planning  Robot kinematics  Simultaneous localization and mapping  Linear programming  Aerospace electronics 
Abstract: In this paper we introduce a novel concept, topological belief space planning (BSP), that uses topological properties of the underlying factor graph representation of future posterior beliefs to direct the search for an optimal solution. This concept deviates from state-of-the-art BSP approaches and is motivated by recent results which indicated, in the context of graph pruning, that topological properties of factor graphs dominantly determine the estimation accuracy. Topological space is also often less dimensional than the embedded state space. In particular, we show how this novel concept can be used in multi-robot belief space planning in high-dimensional state spaces to overcome drawbacks of state-of-the-art approaches: computational intractability of an exhaustive objective evaluation for all candidate path combinations from different robots and dependence on the initial guess in the announced path approach, which can lead to a local minimum of the objective function. We demonstrate our approach in a synthetic simulation.


Title: Robust Rough-Terrain Locomotion with a Quadrupedal Robot
Key Words: collision avoidance  legged locomotion  mobile robots  motion control  optimisation  path planning  pose estimation  robot dynamics  robot kinematics  terrain mapping  robust rough-terrain locomotion  natural settings  industrial settings  motion planner  perceptive rough-terrain locomotion  safe footholds  collision-free swing-leg motions  acquired terrain map  optimization approach  significant obstacles  quadrupedal robot ANYmal  locomotion planner  dynamic environments  urban settings  pose optimization approach  Legged locomotion  Robot sensing systems  Planning  Collision avoidance  Surface treatment 
Abstract: Robots working in natural, urban, and industrial settings need to be able to navigate challenging environments. In this paper, we present a motion planner for the perceptive rough-terrain locomotion with quadrupedal robots. The planner finds safe footholds along with collision-free swing-leg motions by leveraging an acquired terrain map. To this end, we present a novel pose optimization approach that enables the robot to climb over significant obstacles. We experimentally validate our approach with the quadrupedal robot ANYmal by autonomously traversing obstacles such steps, inclines, and stairs. The locomotion planner re-plans the motion at every step to cope with disturbances and dynamic environments. The robot has no prior knowledge of the scene, and all mapping, state estimation, control, and planning is performed in real-time onboard the robot.


Title: Toward Intuitive Teleoperation in Surgery: Human-Centric Evaluation of Teleoperation Algorithms for Robotic Needle Steering
Key Words: biomechanics  cognition  force feedback  haptic interfaces  medical robotics  needles  robot kinematics  steering systems  surgery  telerobotics  robotically steered needles  joint space control  Cartesian space control  hub-centered steering  user experience  user cognitive workload  muscle fatigue  human-centric metrics  human-centric evaluation  teleoperation algorithms  teleoperated systems  physiological metrics  cognitive metrics  teleoperation performance  intuitive teleoperation  robotic needle steering  kinematic metrics  teleoperation mappings  teleoperation strategies  steering control mapping  Needles  Aerospace electronics  Task analysis  Kinematics  Haptic interfaces  Measurement  Sensors 
Abstract: The effectiveness of control algorithms for teleoperated systems is typically evaluated through experimental performance measures, post-experimental user surveys, and theoretical analysis. However, none of these methods provide an objective assessment of teleoperation algorithms with respect to the real-time changes of human users during teleoperated tasks in terms of physiological, kinematic, or cognitive metrics. In this study, we recruited subjects to control robotically steered needles in a randomized experiment, using four different teleoperation mappings (joint space control, steering control, and Cartesian space control with and without force feedback). We investigated how the choice of these algorithms affect both performance and user response. Our novel steering control mapping, which mimics hub-centered steering, is significantly correlated with decreased cognitive stress and improved teleoperation performance when compared to joint space control. Overall, user experience and teleoperation performance were significantly improved with Cartesian space control, resulting in faster needle insertion, higher targeting accuracy, lower cognitive load, and smoother movements. Furthermore, while additional haptic feedback in Cartesian space provided an improved performance, it may increase user cognitive workload and muscle fatigue. These results highlight the importance of considering human-centric metrics when designing novel teleoperation strategies for complex systems.


Title: Intuitive Hand Teleoperation by Novice Operators Using a Continuous Teleoperation Subspace
Key Words: dexterous manipulators  motion control  telerobotics  intuitive control method  pose spaces  low-dimensional teleoperation subspace  continuous teleoperation subspace  nonanthropomorphic robot  teleoperation subspaces  teleoperation subspace mapping  intuitive hand teleoperation  novice operators  human-in-the-loop manipulation  autonomous grasping  input device  teleoperation methods  Aerospace electronics  Grasping  Kinematics  Task analysis  Teleoperators  Shape 
Abstract: Human-in-the-loop manipulation is useful in when autonomous grasping is not able to deal sufficiently well with corner cases or cannot operate fast enough. Using the teleoperator's hand as an input device can provide an intuitive control method but requires mapping between pose spaces which may not be similar. We propose a low-dimensional and continuous teleoperation subspace which can be used as an intermediary for mapping between different hand pose spaces. We present an algorithm to project between pose space and teleoperation subspace. We use a non-anthropomorphic robot to experimentally prove that it is possible for teleoperation subspaces to effectively and intuitively enable teleoperation. In experiments, novice users completed pick and place tasks significantly faster using teleoperation subspace mapping than they did using state of the art teleoperation methods.


Title: Deep Encoder-Decoder Networks for Mapping Raw Images to Dynamic Movement Primitives
Key Words: backpropagation  handwriting recognition  handwritten character recognition  neural nets  dynamic movement primitives  cost functions  raw image mapping  backpropagation  MNIST database  deep encoder-decoder network  associated movement trajectories  perception-action couplings  encoder-decoder networks  calculated movements  handwriting movements  Trajectory  Differential equations  Neural networks  Cost function  Training  Robot kinematics 
Abstract: In this paper we propose a new approach for learning perception-action couplings. We show that by collecting a suitable set of raw images and the associated movement trajectories, a deep encoder-decoder network can be trained that takes raw images as input and outputs the corresponding dynamic movement primitives. We propose suitable cost functions for training the network and describe how to calculate their gradients to enable effective training by back-propagation. We tested the proposed approach both on a synthetic dataset and on a widely used MNIST database to generate handwriting movements from raw images of digits. The calculated movements were also applied for digit writing with a real robot.


Title: Scene Recognition and Object Detection in a Unified Convolutional Neural Network on a Mobile Manipulator
Key Words: control engineering computing  convolution  feature extraction  feedforward neural nets  image classification  image colour analysis  manipulators  mobile robots  object detection  object recognition  operating systems (computers)  robot programming  robot vision  scene classification  unified architecture  global scene features  regional object features  object recognition  continuous robot beliefs  robotics applications  Robot Operating System  mobile manipulator  object detection  object locations  network predictions  SUN RGBD dataset  3D space  unified convolutional neural network  Proposals  Robots  Object detection  Object recognition  Three-dimensional displays  Semantics  Feature extraction 
Abstract: Environment understanding, object detection and recognition are crucial skills for robots operating in the real world. In this paper, we propose a Convolutional Neural Network with multi-task objectives: object detection and scene classification in one unified architecture. The proposed network reasons globally about an image to understand the scene, hypothesize object locations, and encodes global scene features with regional object features to improve object recognition. We evaluate our network on the standard SUN RGBD dataset. Experiments show that our approach outperforms state-of-the-arts. Network predictions are further transformed into continuous robot beliefs to ensure temporal coherence and extended to 3D space for robotics applications. We embed the whole framework in Robot Operating System, and evaluate its performance on a real robot for semantic mapping and grasp detection.


Title: Learning Human-Aware Path Planning with Fully Convolutional Networks
Key Words: convolution  feedforward neural nets  learning (artificial intelligence)  mobile robots  navigation  path planning  random processes  trees (mathematics)  robot social navigation  Fully Convolutional Neural Networks  mobile robots  human-aware path planning learning  optimal Rapidly-exploring Random Tree planner  robot navigation  classification problem  Robots  Navigation  Task analysis  Trajectory  Cost function  Feature extraction 
Abstract: This work presents an approach to learn path planning for robot social navigation by demonstration. We make use of Fully Convolutional Neural Networks (FCNs) to learn from expert's path demonstrations a map that marks a feasible path to the goal as a classification problem. The use of FCNs allows us to overcome the problem of manually designing/identifying the cost-map and relevant features for the task of robot navigation. The method makes use of optimal Rapidly-exploring Random Tree planner (RRT*) to overcome eventual errors in the path prediction; the FCNs prediction is used as cost-map and also to partially bias the sampling of the configuration space, leading the planner to behave similarly to the learned expert behavior. The approach is evaluated in experiments with real trajectories and compared with Inverse Reinforcement Learning algorithms that use RRT* as underlying planner.


Title: Pedestrian Prediction by Planning Using Deep Neural Networks
Key Words: collision avoidance  convolution  learning (artificial intelligence)  mobile robots  neural nets  pedestrians  traffic engineering computing  monolithic neural network  inverse reinforcement learning  pedestrian prediction  deep neural networks  collision avoidance  autonomous vehicles  goal-directed planning  mixture density function  motion prediction  convolutional network  traffic participant prediction  trajectories  Planning  Network topology  Topology  Convolution  Learning (artificial intelligence)  Trajectory  Prediction algorithms 
Abstract: Accurate traffic participant prediction is the prerequisite for collision avoidance of autonomous vehicles. In this work, we propose to predict pedestrians using goal-directed planning. For this, we infer a mixture density function for possible destinations. We use these destinations as the goal states of a planning stage that performs motion prediction based on common behavior patterns. The patterns are learned by a fully convolutional network operating on maps of the environment. We show that this entire system can be modeled as one monolithic neural network and trained via inverse reinforcement learning. Experimental validation on real world data shows the system's ability to predict both, destinations and trajectories accurately.


Title: Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data
Key Words: feature extraction  feedforward neural nets  gesture recognition  image classification  image motion analysis  learning (artificial intelligence)  motion recognition  spatiotemporal features  dense occupancy grids  3D point cloud data  end-to-end spatiotemporal gesture learning approach  dynamic gestures  spatiotemporal learning  point cloud data augmentation  3D convolutional neural network  gestures sample data  Three-dimensional displays  Robot sensing systems  Feature extraction  Solid modeling  Training data  Spatiotemporal phenomena  Hidden Markov models 
Abstract: In this paper, we demonstrate an end-to-end spatiotemporal gesture learning approach for 3D point cloud data using a new gestures dataset of point clouds acquired from a 3D sensor. Nine classes of gestures were learned from gestures sample data. We mapped point cloud data into dense occupancy grids, then time steps of the occupancy grids are used as inputs into a 3D convolutional neural network which learns the spatiotemporal features in the data without explicit modeling of gesture dynamics. We also introduced a 3D region of interest jittering approach for point cloud data augmentation. This resulted in an increased classification accuracy of up to 10% when the augmented data is added to the original training data. The developed model is able to classify gestures from the dataset with 84.44% accuracy. We propose that point cloud data will be a more viable data type for scene understanding and motion recognition, as 3D sensors become ubiquitous in years to come.


Title: Assigning Visual Words to Places for Loop Closure Detection
Key Words: image matching  image recognition  image representation  image segmentation  mobile robots  probability  robot vision  SLAM (robots)  simultaneous localization and mapping  image stream  image match  dynamic segmentation  nearest neighbor voting scheme  image descriptors  query time  on-line clustering algorithm  visual vocabulary construction  robotic applications  LCD  place recognition  loop closure detection  visual words  Visualization  Liquid crystal displays  Robots  Databases  Pipelines  Feature extraction  Vocabulary 
Abstract: Place recognition of pre-visited areas, widely known as Loop Closure Detection (LCD), constitutes one of the most important components in robotic applications, where the robot needs to estimate its pose while navigating through the field (e.g., simultaneous localization and mapping). In this paper, we present a novel approach for LCD based on the assignment of Visual Words (VWs) to particular places of the traversed path. The system operates in real time and does not require any pre-training procedure, such as visual vocabulary construction or descriptor-space dimensionality reduction. A place is defined through a dynamic segmentation of the incoming image stream and is assigned with VWs through the usage of an on-line clustering algorithm. At query time, image descriptors are converted into VWs on the map accumulating votes to the corresponding places. By means of a probability function, the mechanism is capable of identifying a loop closing candidate place. A nearest neighbor voting scheme on the descriptors' space allows the system to select the most appropriate image match at the chosen place. Geometrical and temporal consistency checks are applied on the proposed loop closing pair increasing the system's performance. Evaluation took place on several publicly available and challenging datasets offering high precision and recall scores as compared to other state-of-the-art approaches.


Title: Dijkstra Model for Stereo-Vision Based Road Detection: A Non-Parametric Method
Key Words: computer vision  graph theory  image segmentation  object detection  roads  stereo image processing  traffic engineering computing  road detection  nonparametric method  improved v-disparity map  vanishing point  road region  horizon information  source node  weighted graph  left stereo-image  adjacency relationships  adjacent pixels  disparity information  road borders  Dijkstra algorithm  Dijkstra model  stereo vision  gray-scale information  image pairs  road scenes  weighted-sampling RANSAC-like method  KITTI dataset  Roads  Robustness  Three-dimensional displays  Stereo vision  Splines (mathematics)  Cameras  Image edge detection 
Abstract: This paper proposes a new method for detecting a road from a stereo pair of images. First, the horizon is accurately estimated by a robust, weighted-sampling RANSAC-like method in the improved v-disparity map. The vanishing point of the road region is located using both the horizon information and road flatness constraints. Then it is used as the source node of a weighted graph formed by the pixels of the left stereo-image and their adjacency relationships. The weight of each edge measures the inconsistency of adjacent pixels, and is computed using both the gray-scale and disparity information. Detecting road borders is thus reduced to finding two shortest paths from the source node to the bottom row of the image by the Dijkstra algorithm. The proposed method has been tested on 2621 image pairs of different road scenes from the KITTI dataset. Our experiments demonstrate that this training free approach detects horizon, vanishing point, and road region accurately and robustly, and compares favorably with the state of the art on the KITTI benchmark.


Title: Asynchronous Multi-Sensor Fusion for 3D Mapping and Localization
Key Words: graph theory  intelligent transportation systems  optimisation  pose estimation  sensor fusion  stereo image processing  3D localization  factor graph-based optimization  autonomous driving  3D pose measurement  asynchronous multisensor fusion  asynchronous-measurement alignment  graph nodes  out-of-sequence measurement alignment  multiple navigation sensors  modular sensor-fusion system  autonomous vehicles  3D mapping  asynchronous sensors  multiple heterogeneous sensors  Sensors  Three-dimensional displays  Optimization  Atmospheric measurements  Particle measurements  Frequency measurement  Laser radar 
Abstract: In this paper, we address the problem of optimally fusing multiple heterogeneous and asynchronous sensors for use in 3D mapping and localization of autonomous vehicles. To this end, based on the factor graph-based optimization framework, we design a modular sensor-fusion system that allows for efficient and accurate incorporation of multiple navigation sensors operating at different sampling rates. In particular, we develop a general method of out-of-sequence (asynchronous) measurement alignment to incorporate heterogeneous sensors into a factor graph for mapping and localization in 3D, without requiring the addition of new graph nodes, thus allowing the graph to have an overall reduced complexity. The proposed sensor-fusion system is validated on a real-world experimental dataset, in which the asynchronous-measurement alignment is shown to have an improved performance when compared to a naive approach without alignment.


Title: Localization Under Topological Uncertainty for Lane Identification of Autonomous Vehicles
Key Words: hidden Markov models  mobile robots  position control  remotely operated vehicles  road traffic control  topology  VSM-HMM  topological uncertainty  lane membership  topological localization process  topological structure estimation  AV lane estimation  lane identification  autonomous vehicles  topological location  decision-making  public roads  variable structure multiple hidden Markov model  metric location  Earth mover distance  Hidden Markov models  Computational modeling  Topology  Roads  Uncertainty  Measurement  Vehicle dynamics 
Abstract: Autonomous vehicles (AVs) require accurate metric and topological location estimates for safe, effective navigation and decision-making. Although many high-definition (HD) roadmaps exist, they are not always accurate since public roads are dynamic, shaped unpredictably by both human activity and nature. Thus, AVs must be able to handle situations in which the topology specified by the map does not agree with reality. We present the Variable Structure Multiple Hidden Markov Model (VSM-HMM) as a framework for localizing in the presence of topological uncertainty, and demonstrate its effectiveness on an AV where lane membership is modeled as a topological localization process. VSM-HMMs use a dynamic set of HMMs to simultaneously reason about location within a set of most likely current topologies and therefore may also be applied to topological structure estimation as well as AV lane estimation. In addition, we present an extension to the Earth Mover's Distance which allows uncertainty to be taken into account when computing the distance between belief distributions on simplices of arbitrary relative sizes.


Title: Dynamic Reconfiguration of Mission Parameters in Underwater Human-Robot Collaboration
Key Words: feedforward neural nets  finite state machines  gesture recognition  human-robot interaction  mobile robots  hand gestures  hand gesture recognition  gesture-to-instruction mapping  finite-state machine  convolutional neural network  human-robot collaborative tasks  autonomous underwater robots  parameter reconfiguration method  real-time programming  underwater human-robot collaboration  mission parameters  dynamic reconfiguration  Robustness  Task analysis  Visualization  Robot sensing systems  Unmanned underwater vehicles  Gesture recognition 
Abstract: This paper presents a real-time programming and parameter reconfiguration method for autonomous underwater robots in human-robot collaborative tasks. Using a set of intuitive and meaningful hand gestures, we develop a syntactically simple framework that is computationally more efficient than a complex, grammar-based approach. In the proposed framework, a convolutional neural network is trained to provide accurate hand gesture recognition; subsequently, a finite-state machine- based deterministic model performs efficient gesture-to-instruction mapping and further improves robustness of the interaction scheme. The key aspect of this framework is that it can be easily adopted by divers for communicating simple instructions to underwater robots without using artificial tags such as fiducial markers or requiring memorization of a potentially complex set of language rules. Extensive experiments are performed both on field-trial data and through simulation, which demonstrate the robustness, efficiency, and portability of this framework in a number of different scenarios. Finally, a user interaction study is presented that illustrates the gain in the ease of use of our proposed interaction framework compared to the existing methods for the underwater domain.


Title: Gaussian Process Adaptive Sampling Using the Cross-Entropy Method for Environmental Sensing and Monitoring
Key Words: bathymetry  entropy  Gaussian processes  learning (artificial intelligence)  mobile robots  optimisation  path planning  sampling methods  single ROI  deepest region  coastal bathymetry mapping mission validate  efficient sampling strategy  latest sensory measurements  sampling density  CE trajectory optimization  higher spatial variability  exhibit extreme sensory measurements  exploring learning  initial stage  path planning  GP-UCB  GP upper confidence  receding-horizon Cross-Entropy trajectory optimization  environmental sensing  cross-entropy method  Gaussian process adaptive sampling  Robot sensing systems  Adaptation models  Optimization  Predictive models  Trajectory  Uncertainty 
Abstract: In this paper, we focus on adaptive sampling on a Gaussian Processes (GP) using the receding-horizon Cross-Entropy (CE) trajectory optimization. Specifically, we employ the GP upper confidence bound (GP-UCB) as the optimization criteria to adaptively plan sampling paths that balance the exploitation-exploration trade-off. Path planning at the initial stage focuses on exploring and learning a model of the environment, and later, on exploiting the learned model to focus sampling around regions that exhibit extreme sensory measurements and much higher spatial variability, denoted as the Region of Interest (ROI). The integration of the CE trajectory optimization allows the sampling density to be dynamically adjusted based on the latest sensory measurements, thus providing an efficient sampling strategy for sensing and localizing the ROI. We demonstrate the effectiveness of the proposed method in exploring simulated scalar fields with single or multiple ROIs. Field experiments with an Unmanned Surface Vehicle (USV) in a coastal bathymetry mapping mission validate the approach's capability in quickly exploring and mapping the given area, and then focusing and increasing the sampling density around the deepest region, as a surrogate for e.g. the extremal concentration of a pollutant in the environment.


Title: Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning
Key Words: collision avoidance  decentralised control  gradient methods  learning (artificial intelligence)  mobile robots  multi-robot systems  multiscenario multistage training framework  optimal policy  policy gradient  reinforcement learning algorithm  learned sensor-level collision avoidance policy  final learned policy  collision-free paths  large-scale robot system  deep reinforcement learning  safe collision avoidance policy  efficient collision avoidance policy  optimally decentralized multirobot collision avoidance  agent-level feature extraction  decentralized methods  maps raw sensor measurements  multirobot systems  decentralized sensor-level collision avoidance policy  local collision-free action  distributed multirobot collision avoidance systems  Collision avoidance  Robot sensing systems  Robot kinematics  Navigation  Robustness  Training 
Abstract: Developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths without observing other robots' states and intents. While other distributed multi-robot collision avoidance systems exist, they often require extracting agent-level features to plan a local collision-free action, which can be computationally prohibitive and not robust. More importantly, in practice the performance of these methods are much lower than their centralized counterparts. We present a decentralized sensor-level collision avoidance policy for multi-robot systems, which directly maps raw sensor measurements to an agent's steering commands in terms of movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots on rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. We validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient, collision-free paths for a large-scale robot system. We also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period, including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots. Videos are available at https://sites.google.com/view/drlmaca.


Title: Mapping with Dynamic-Object Probabilities Calculated from Single 3D Range Scans
Key Words: control engineering computing  laser ranging  mobile robots  navigation  neural nets  probability  robot vision  SLAM (robots)  KITTI dataset  mapping process  mapping module  dynamic object  pointwise probability  neural network  laser range data  3D grid map  navigation functions  robot perceptions  dynamic environments  safe navigation  robust navigation  autonomous robotic systems  single 3D range scans  dynamic-object probabilities  time 3.0 d  Three-dimensional displays  Cameras  Neural networks  Measurement by laser beam  Lasers  Laser beams  Image segmentation 
Abstract: Various autonomous robotic systems require maps for robust and safe navigation. Particularly when robots are employed in dynamic environments, accurate knowledge about which components of the robot perceptions belong to dynamic and static aspects in the environment can greatly improve navigation functions. In this paper we propose a novel method for building 3D grid maps using laser range data in dynamic environments. Our approach uses a neural network to estimate the pointwise probability of a point belonging to a dynamic object. The output from our network is fed to the mapping module for building a 3D grid map containing only static parts of the environment. We present experimental results obtained by training our neural network using the KITTI dataset and evaluating it in a mapping process using our own dataset. In extensive experiments, we show that maps generated using the proposed probability about dynamic objects increases the accuracy of the resulting maps.


Title: A Survey of Voxel Interpolation Methods and an Evaluation of Their Impact on Volumetric Map-Based Visual Odometry
Key Words: cameras  computational geometry  distance measurement  image reconstruction  interpolation  medical image processing  pose estimation  robot vision  SLAM (robots)  camera trajectories  trilinear interpolation method  depth-camera pose tracking  performance degradation  truncated signed distance field  voxel-based map representations  geometric interpolation methods  intermediate options  nearest neighbors  voxel volumes  volumetric map-based visual odometry  voxel interpolation methods  Interpolation  Memory management  Three-dimensional displays  Pose estimation  Extrapolation  Two dimensional displays  Image resolution 
Abstract: Voxel volumes are simple to implement and lend themselves to many of the tools and algorithms available for 2D images. However, the additional dimension of voxels may be costly to manage in memory when mapping large spaces at high resolutions. While lowering the resolution and using interpolation is common work-around, in the literature we often find that authors either use trilinear interpolation or nearest neighbors and rarely any of the intermediate options. This paper presents a survey of geometric interpolation methods for voxel-based map representations. In particular we study the truncated signed distance field (TSDF) and the impact of using fewer than 8 samples to perform interpolation within a depth-camera pose tracking and mapping scenario. We find that lowering the number of samples fetched to perform the interpolation results in performance similar to the commonly used trilinear interpolation method, but leads to higher frame-rates. We also report that lower bit-depth generally leads to performance degradation, though not as much as may be expected, with voxels containing as few as 3 bits sometimes resulting in adequate estimation of camera trajectories.


Title: Complex Urban LiDAR Data Set
Key Words: graph theory  mobile robots  optical radar  pose estimation  radar computing  SLAM (robots)  complex urban environments  light detection and ranging data set  fiber optic gyro  inertial measurement unit  Global Positioning System  vehicle pose estimation  graph simultaneous location and mapping algorithm  graph SLAM algorithm  Robot Operating System environment  raw sensor data  2D LiDAR  16-ray 3D LiDARs  LiDAR sensors  three-dimensional LiDAR  building complexes  high-rise buildings  complex urban LiDAR data set  frequency 100.0 Hz  Laser radar  Three-dimensional displays  Global Positioning System  Two dimensional displays  Sensor systems  Urban areas 
Abstract: This paper presents a Light Detection and Ranging (LiDAR) data set that targets complex urban environments. Urban environments with high-rise buildings and congested traffic pose a significant challenge for many robotics applications. The presented data set is unique in the sense it is able to capture the genuine features of an urban environment (e.g. metropolitan areas, large building complexes and underground parking lots). Data of two-dimensional (2D) and three-dimensional (3D) LiDAR, which are typical types of LiDAR sensors, are provided in the data set. The two 16-ray 3D LiDARs are tilted on both sides for maximal coverage. One 2D LiDAR faces backward while the other faces forwards to collect data of roads and buildings, respectively. Raw sensor data from Fiber Optic Gyro (FOG), Inertial Measurement Unit (IMU), and the Global Positioning System (GPS) are presented in a file format for vehicle pose estimation. The pose information of the vehicle estimated at 100 Hz is also presented after applying the graph simultaneous localization and mapping (SLAM) algorithm. For the convenience of development, the file player and data viewer in Robot Operating System (ROS) environment were also released via the web page. The full data sets are available at: http://irap.kaist.ac.kr/dataset. In this website, 3D preview of each data set is provided using WebGL.


Title: Live Structural Modeling Using RGB-D SLAM
Key Words: image colour analysis  image fusion  image texture  robot vision  SLAM (robots)  solid modelling  live structural modeling  dense point cloud  shape map  single point cloud  metric primitive modeling  RGB-D SLAM  primitive shape localization  shape fusion  Shape  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Computational modeling  History  Estimation 
Abstract: This paper presents a method for localizing primitive shapes in a dense point cloud computed by the RGB-D SLAM system. To stably generate a shape map containing only primitive shapes, the primitive shape is incrementally modeled by fusing the shapes estimated at previous frames in the SLAM, so that an accurate shape can be finally generated. Specifically, the history of the fusing process is used to avoid the influence of error accumulation in the SLAM. The point cloud of the shape is then updated by fusing the points in all the previous frames into a single point cloud. In the experimental results, we show that metric primitive modeling in texture-less and unprepared environments can be achieved online.


Title: Optimized Environment Exploration for Autonomous Underwater Vehicles
Key Words: autonomous underwater vehicles  image reconstruction  mobile robots  path planning  quadtrees  tree data structures  viewpoint generation process  consistent maps  noisy sonar data  optimized environment exploration  autonomous underwater  autonomous robotic environment exploration  underwater domain  noisy acoustic sensors  high localization error  control disturbances  robotic exploration algorithm  underwater vehicles  view planning  path planning algorithms  exploration approach  quadtree data structure  relevant queries  natural environments  underwater maps  map representation  optical coverage  time 3.0 d  Cameras  Sonar  Planning  Robot sensing systems  Inspection  Three-dimensional displays 
Abstract: Achieving full autonomous robotic environment exploration in the underwater domain is very challenging, mainly due to noisy acoustic sensors, high localization error, control disturbances of the water and lack of accurate underwater maps. In this work we present a robotic exploration algorithm for underwater vehicles that does not rely on prior information about the environment. Our method has been greatly influenced by many robotic exploration, view planning and path planning algorithms. The proposed method constitutes a significant improvement over our previous work [1]: Firstly, we refine our exploration approach to improve robustness; Secondly, we propose an alternative map representation based on the quadtree data structure that allows different relevant queries to be performed efficiently, reducing the computational cost of the viewpoint generation process; Thirdly, we present an algorithm that is capable of generating consistent maps even when noisy sonar data is used. The aforementioned contributions have increased the reliability of the algorithm, allowing new real experiments performed in artificial structures but also in more challenging natural environments, from which we provide a 3D reconstruction to show that with this algorithm full optical coverage is obtained.


Title: CASSL: Curriculum Accelerated Self-Supervised Learning
Key Words: grippers  learning (artificial intelligence)  sensitivity analysis  adaptive-underactuated multifingered gripper  curriculum accelerated self-supervised learning  variance-based global sensitivity analysis  control parameters  control dimensions  training data  CASSL orders  higher-dimensional action  map visual information  clever sampling strategy  data collection efforts  higher-dimensional control  low-dimensional action  self-supervised learning approaches  complete end-to-end learning  staged curriculum learning  CASSL framework  Aerospace electronics  Grasping  Training  Task analysis  Robots  Sensitivity analysis 
Abstract: Recent self-supervised learning approaches focus on using a few thousand data points to learn policies for high-level, low-dimensional action spaces. However, scaling this framework for higher-dimensional control requires either scaling up the data collection efforts or using a clever sampling strategy for training. We present a novel approach - Curriculum Accelerated Self-Supervised Learning (CASSL) - to train policies that map visual information to high-level, higher-dimensional action spaces. CASSL orders the sampling of training data based on control dimensions: the learning and sampling are focused on few control parameters before other parameters. The right curriculum for learning is suggested by variance-based global sensitivity analysis of the control space. We apply our CASSL framework to learning how to grasp using an adaptive, underactuated multi-fingered gripper, a challenging system to control. Our experimental results indicate that CASSL provides significant improvement and generalization compared to baseline methods such as staged curriculum learning (8% increase) and complete end-to-end learning with random exploration (14% improvement) tested on a set of novel objects.


Title: Learning to Control Redundant Musculoskeletal Systems with Neural Networks and SQP: Exploiting Muscle Properties
Key Words: biomechanics  biomimetics  bone  humanoid robots  learning (artificial intelligence)  muscle  neural nets  nonlinear control systems  physiological models  quadratic programming  machine learning approaches  muscle stimulations  high actuator redundancy  learned forward model  neural network  biomimetic muscle-driven robot show  nonlinearity  biomechanical musculoskeletal systems  quadratic programming  SQP  Muscles  Joints  Robots  Biological system modeling  Torque  Biomechanics 
Abstract: Modeling biomechanical musculoskeletal systems reveals that the mapping from muscle stimulations to movement dynamics is highly nonlinear and complex, which makes it difficult to control those systems with classical techniques. In this work, we not only investigate whether machine learning approaches are capable of learning a controller for such systems. We are especially interested in the question if the structure of the musculoskeletal apparatus exhibits properties that are favorable for the learning task. In particular, we consider learning a control policy from target positions to muscle stimulations. To account for the high actuator redundancy of biomechanical systems, our approach uses a learned forward model represented by a neural network and sequential quadratic programming to obtain the control policy, which also enables us to alternate the co-contraction level and hence allows to change the stiffness of the system and to include optimality criteria like small muscle stimulations. Experiments on both a simulated musculoskeletal model of a human arm and a real biomimetic muscle-driven robot show that our approach is able to learn an accurate controller despite high redundancy and nonlinearity, while retaining sample efficiency.


Title: Long-Term Visual Localization Using Semantically Segmented Images
Key Words: feature extraction  image segmentation  particle filtering (numerical methods)  transforms  particle filter based semantic localization solution  SIFT-features  vehicle localization  semantically labeled 3D point maps  autonomous vehicles  long-term visual navigation  robust cross-seasonal localization  semantically segmented images  long-term visual localization  image segmenter  hand-crafted feature descriptors  Semantics  Cameras  Roads  Image segmentation  Visualization  Robustness  Feature extraction 
Abstract: Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.), Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.


Title: Robust Localization of Mobile Robots Considering Reliability of LiDAR Measurements
Key Words: mobile robots  optical radar  optical sensors  pose estimation  reliability  robot vision  mobile robot  LiDAR measurements  localization errors  LiDAR sensor-based localization  optical sensors  robust localization  range measurements  reliability  Light Detection and Ranging sensor  pose estimation  Reliability  Laser radar  Robot sensing systems  Optical sensors  Glass  Adaptive optics  Optical variables measurement 
Abstract: In this study, we propose a novel Light Detection and Ranging (LiDAR) sensor-based localization method for localization of a mobile robot. In localization using the LiDAR sensor, localization errors occur when real range measurements differ from reference distances computed from a map. This study focuses on three factors that cause differences between real range measurements and reference distances. The first factor corresponds to optical characteristics of the LiDAR sensor for objects such as glass walls and mirrors. The second factor corresponds to occlusions by dynamic obstacles. The third factor corresponds to static changes in the environment. In practical applications, three factors often simultaneously occur. Although there have been many previous works, robust localization to overcome these three difficulties is still a challenging problem. This study proposes a novel robust localization scheme that exploits only reliable range measurements. A LiDAR sensor-based localization scheme can be successfully executed by utilizing only a few reliable range measurements. Therefore, the computation of reliability plays a significant role. The computation of reliability is divided into two steps. The first step considers characteristics of optical sensors. The second step mainly deals with the effects of obstacles. The observation likelihood model exploits computed reliability for pose estimation. The proposed scheme was successfully verified through various experiments under challenging situations.


Title: Sparse Gaussian Processes on Matrix Lie Groups: A Unified Framework for Optimizing Continuous-Time Trajectories
Key Words: continuous time systems  Gaussian processes  Lie groups  matrix algebra  mobile robots  motion control  optimisation  path planning  regression analysis  state estimation  trajectory control  nonparametric representation  trajectory distributions  sparse GP regression  robot state  locally linear GPs  state estimation  motion planning tasks  sparse Gaussian processes  continuous-time trajectories  trajectory optimization  matrix Lie groups  robot motion reasoning  Trajectory  Simultaneous localization and mapping  Estimation  Planning  Sparse matrices  Gaussian processes 
Abstract: Continuous-time trajectories are useful for reasoning about robot motion in a wide range of tasks. Sparse Gaussian processes (GPs) can be used as a non-parametric representation for trajectory distributions that enables fast trajectory optimization by sparse GP regression. However, most previous approaches that utilize sparse GPs for trajectory optimization are limited by the fact that the robot state is represented in vector space. In this paper, we first extend previous works to consider the state on general matrix Lie groups by applying a constant-velocity prior and defining locally linear GPs. Next, we discuss how sparse GPs on Lie groups provide a unified continuous-time framework for trajectory optimization for solving a number of robotics problems including state estimation and motion planning. Finally, we demonstrate and evaluate our approach on several different estimation and motion planning tasks with both synthetic and real-world experiments.


Title: Complexity Analysis and Efficient Measurement Selection Primitives for High-Rate Graph SLAM
Key Words: computational complexity  graph theory  iterative methods  Newton method  optimisation  SLAM (robots)  complexity analysis  globally-efficient structure  favorable global structures  Gauss-Newton iteration  factorization step  primary computational bottleneck  graph structure  existing analytic gap  quantitative metric called elimination complexity  significant computation reductions  measurement decimation  simple heuristics  aggressive pruning  significant computational savings  structurally-naïve techniques  global level  edge count  SLAM graph  graph-based SLAM  high-rate graph  efficient measurement selection primitives  Simultaneous localization and mapping  Complexity theory  Optimization  Sparse matrices  Extraterrestrial measurements  Linear algebra 
Abstract: Sparsity has been widely recognized as crucial for efficient optimization in graph-based SLAM. Because the sparsity and structure of the SLAM graph reflect the set of incorporated measurements, many methods for sparsification have been proposed in hopes of reducing computation. These methods often focus narrowly on reducing edge count without regard for structure at a global level. Such structurally-naïve techniques can fail to produce significant computational savings, even after aggressive pruning. In contrast, simple heuristics such as measurement decimation and keyframing are known empirically to produce significant computation reductions. To demonstrate why, we propose a quantitative metric called elimination complexity (EC) that bridges the existing analytic gap between graph structure and computation. EC quantifies the complexity of the primary computational bottleneck: the factorization step of a Gauss-Newton iteration. Using this metric, we show rigorously that decimation and keyframing impose favorable global structures and therefore achieve computation reductions on the order of r2/9 and r3, respectively, where r is the pruning rate. We additionally present numerical results showing EC provides a good approximation of computation in both batch and incremental (iSAM2) optimization and demonstrate that pruning methods promoting globally-efficient structure outperform those that do not.


Title: Dense Planar-Inertial SLAM with Structural Constraints
Key Words: distance measurement  image reconstruction  least squares approximations  optimisation  robot vision  SLAM (robots)  dense visual odometry estimation  planar measurements  SLAM framework  IMU biases  planar landmarks  incremental smoothing  Bayes Tree  IMU data  visual information  modeling planes  IMU states  reconstruction results  SLAM algorithms  structural constraints  DPI-SLAM system  planar-inertial SLAM system  novel dense planar-inertial SLAM  dense 3D models  indoor environments  hand-held RGB-D sensor  inertial measurement unit  preinte-grated IMU measurements  factor graph  incremental mapping  probabilistic global optimization  Simultaneous localization and mapping  Optimization  Three-dimensional displays  Real-time systems  Estimation  Visualization 
Abstract: In this work, we develop a novel dense planar-inertial SLAM (DPI-SLAM) system to reconstruct dense 3D models of large indoor environments using a hand-held RGB-D sensor and an inertial measurement unit (IMU). The preinte-grated IMU measurements are loosely-coupled with the dense visual odometry (VO) estimation and tightly-coupled with the planar measurements in a full SLAM framework. The poses, velocities, and IMU biases are optimized together with the planar landmarks in a global factor graph using incremental smoothing and mapping with the Bayes Tree (iSAM2). With odometry estimation using both RGB-D and IMU data, our system can keep track of the poses of the sensors even without sufficient planes or visual information (e.g. textureless walls) temporarily. Modeling planes and IMU states in the fully probabilistic global optimization reduces the drift that distorts the reconstruction results of other SLAM algorithms. Moreover, structural constraints between nearby planes (e.g. right angles) are added into the DPI-SLAM system, which further recovers the drift and distortion. We test our DPI-SLAM on large indoor datasets and demonstrate its state-of-the-art performance as the first planar-inertial SLAM system.


Title: Radiation Source Localization in GPS-Denied Environments Using Aerial Robots
Key Words: gamma-ray detection  Global Positioning System  mobile robots  photomultipliers  radioactive sources  radioactivity measurement  solid scintillation detectors  radiation measurements  radioactive source localization  radiation mapping  thallium-doped cesium iodide scintillator  indoor GPS-denied environments  Cesium-137 radiation source  GPS-denied localization  visual-inertial localization  autonomous nuclear radiation source localization  aerial robot  Scintillators  Calibration  Robot sensing systems  Detectors  Unmanned aerial vehicles  Radiation detectors 
Abstract: This paper details the system and methods developed to enable autonomous nuclear radiation source localization and mapping using aerial robots in GPS-denied environments. A Thallium-doped Cesium Iodide (CsI(Tl)) scintillator and a Silicon Photomultiplier are combined with custom-built electronics for counting and spectroscopy, and the provided radiation measurements are pose-annotated using visual-inertial localization enabling autonomous operation in GPS-denied environments. Provided this capability, a strategy for radioactive source localization, as well as active source search path planning was developed. The proposed method is motivated and accounts for the limited endurance of the vehicle, which entails a very small amount of dwell points, and the fact that GPS-denied localization implies varying uncertainty of the robot's position estimate. The complete system is evaluated in multiple experimental studies using a small aerial robot and a Cesium-137 radiation source. As shown, accurate radioactive source localization is achieved, enabling efficient radiation mapping of indoor GPS-denied environments.


Title: Direction Controlled Descent of Samara Autorotating Wings (SAW) with N-Wings * Research supported by the SUTD-MIT International Design Centre (IDC) and by the Temasek Laboratories Defence Innovation Research Programme (DIRP) IGDSP15020141.
Key Words: aerospace components  gyroscopes  mechanical stability  numerical analysis  position control  wind tunnels  direction controlled descent  spinning wing  direction controllability  control schemes  conical spiral autorotation trajectory  gyroscopic stability  maple trees  translational motion  numerical simulations  multiwing SAW prototype  wind-tunnel  samara autorotating wings  ball joint  Surface acoustic waves  Blades  Prototypes  Rotors  Mathematical model  Solid modeling  Stability analysis 
Abstract: The seeds of Maple trees (Samara) use autorotation as a unique mechanism to disperse their seeds. By exploiting gyroscopic stability of a spinning wing, the Samara is able to cover large horizontal distance despite having no form of propulsion. We applied and adapted this natural ability in our novel concept, the Samara Autorotating Wings (SAW), and extended its stability and direction controllability by generalizing the mechanism to incorporate designs with more than 1 wing. By conceiving cyclic control, the translational motion of autorotation is regulated. A nonlinear model of SAW with n wings is derived and control schemes developed to control the translational position during autorotation. Numerical simulations were performed to investigate the performance of the multi-wing SAW prototypes to track a conical spiral autorotation trajectory. Direct experiments were conducted in a vertical wind-tunnel through a special ball joint that allows z-axis translation and all three rotational degrees of freedom. Finally, free-fall drop tests are used to verify the directional controllability and performance of SAW.


Title: Collaborative 6DoF Relative Pose Estimation for Two UAVs with Overlapping Fields of View
Key Words: aerospace computing  autonomous aerial vehicles  groupware  image fusion  Kalman filters  multi-robot systems  nonlinear filters  pose estimation  robot vision  stereo image processing  monocular-inertial odometry  Extended Kalman Filter  collaborative scene estimation  monocular camera  variable-baseline stereo rig  inertial sensor  Unmanned Aerial Vehicles  collaborative robot operation  collaborative 6DoF relative pose estimation  UAV  Cameras  Simultaneous localization and mapping  Collaboration  Estimation  Unmanned aerial vehicles 
Abstract: Driven by the promise of leveraging the benefits of collaborative robot operation, this paper presents an approach to estimate the relative transformation between two small Unmanned Aerial Vehicles (UAVs), each equipped with a single camera and an inertial sensor, comprising the first step of any meaningful collaboration. Formation flying and collaborative object manipulation are some of the few tasks that the proposed work has direct applications on, while forming a variable-baseline stereo rig using two UAVs carrying a monocular camera each promises unprecedented effectiveness in collaborative scene estimation. Assuming an overlap in the UAVs' fields of view, in the proposed framework, each UAV runs monocular-inertial odometry onboard, while an Extended Kalman Filter fuses the UAVs' estimates and common image measurements to estimate the metrically scaled relative transformation between them, in realtime. Decoupling the direction of the baseline between the cameras of the two UAVs from its magnitude, this work enables consistent and robust estimation of the uncertainty of the relative pose estimation. Our evaluation on both on simulated data and benchmarking datasets consisting of real aerial data, reveals the power of the proposed methodology in a variety of scenarios. Video - https://youtu.be/AmkkaXa2601.


Title: Object-Centric Approach to Prediction and Labeling of Manipulation Tasks
Key Words: cameras  graph theory  mobile robots  object recognition  robot vision  object-centric approach  manipulation tasks  human manipulation actions  object trajectories  context specific human vocabulary  directed action graph representation  pre-computed Location Areas  offline teaching phase  graph generation  online action recognition phase  high-level reasoning  sensor observation  visual sensory input  depth camera  LA  sector-maps  SM  Service robots  Hidden Markov models  Vocabulary  Feature extraction  Knowledge based systems  Task analysis  Action Recognition  Motion analysis  Graph method  Location Area  Sector-Map  Knowledge representation 
Abstract: We propose an object-centric framework to label and predict human manipulation actions from observations of the object trajectories in 3D space. The goal is to lift the low-level sensor observation to a context specific human vocabulary. The low-level visual sensory input from a depth camera is processed into high-level descriptive action labels using a directed action graph representation. It is built based on the concepts of pre-computed Location Areas (LA), regions within a scene where an action typically occur, and Sector-Maps (SM), reference trajectories between the LAs. The framework consists of two stages, an offline teaching phase for graph generation, and an online action recognition phase that maps the current observations to the generated graph. This graph representation allows the framework to predict the most probable action from the observed motion in real-time and to adapt its structure whenever a new LA appears. Furthermore, the descriptive action labels enable not only a better exchange of information between a human and a robot but they allow also the robots to perform high-level reasoning. We present experimental results on real human manipulation actions using a system designed with this framework to show the performance of prediction and labeling that can be achieved.


Title: Adaptive Deep Learning Through Visual Domain Localization
Key Words: generalisation (artificial intelligence)  humanoid robots  human-robot interaction  image classification  learning (artificial intelligence)  robot vision  robot vision  domain shift  end-to-end deep domain adaptation architecture  target domain  training time  human-robot interactions  adaptive deep  visual domain localization  commercial robot  illumination conditions  domain adaptation methods  robotics applications  computer vision  generalization issue  iCub World database  Visualization  Training  Adaptive systems  Adaptation models  Machine learning  Service robots 
Abstract: A commercial robot, trained by its manufacturer to recognize a predefined number and type of objects, might be used in many settings, that will in general differ in their illumination conditions, background, type and degree of clutter, and so on. Recent computer vision works tackle this generalization issue through domain adaptation methods, assuming as source the visual domain where the system is trained and as target the domain of deployment. All approaches assume to have access to images from all classes of the target during training, an unrealistic condition in robotics applications. We address this issue proposing an algorithm that takes into account the specific needs of robot vision. Our intuition is that the nature of the domain shift experienced mostly in robotics is local. We exploit this through the learning of maps that spatially ground the domain and quantify the degree of shift, embedded into an end-to-end deep domain adaptation architecture. By explicitly localizing the roots of the domain shift we significantly reduce the number of parameters of the architecture to tune, we gain the flexibility necessary to deal with subset of categories in the target domain at training time, and we provide a clear feedback on the rationale behind any classification decision, which can be exploited in human-robot interactions. Experiments on two different settings of the iCub World database confirm the suitability of our method for robot vision.


Title: Feature-constrained Active Visual SLAM for Mobile Robot Navigation
Key Words: collision avoidance  mobile robots  navigation  path planning  robot vision  SLAM (robots)  sensory constraints  iterative motion planning framework  collision avoidance  online mapping  associated map points  distance-optimal path planner  data-driven approach  continuous identification  feature-based Visual Simultaneous Localization  vision-based navigation  failure avoidance  mobile robot navigation  feature-constrained active Visual SLAM  Cameras  Navigation  Collision avoidance  Simultaneous localization and mapping  Planning 
Abstract: This paper focuses on tracking failure avoidance during vision-based navigation to a desired goal in unknown environments. While using feature-based Visual Simultaneous Localization and Mapping (VSLAM), continuous identification and association of map points are required during motion. Thus, we discuss a motion planning framework that takes into account sensory constraints for a reliable navigation. We use information available in the SLAM and propose a data-driven approach to predict the number of map points associated in a given pose. Then, a distance-optimal path planner utilizes the model to constrain paths such that the number of associated map points in each pose is above a threshold. We also include an online mapping of the environment for collision avoidance. Overall, we propose an iterative motion planning framework that enables real-time replanning after the acquisition of more information. Experiments in two environments demonstrate the performance of the proposed framework.


Title: Selection and Compression of Local Binary Features for Remote Visual SLAM
Key Words: feature extraction  feature selection  mobile robots  multi-robot systems  robot vision  SLAM (robots)  feature selection stage  remote visual SLAM  autonomous robotics  collaborative SLAM approaches  multiple robots  feature coding scheme  simultaneous localization and mapping  visual sensors  embedded devices  local binary features extraction  centralized powerful processing node  Visualization  Encoding  Simultaneous localization and mapping  Feature extraction  Task analysis  Image coding 
Abstract: In the field of autonomous robotics, Simultaneous Localization and Mapping (SLAM) is still a challenging problem. With cheap visual sensors attracting more and more attention, various solutions to the SLAM problem using visual cues have been proposed. However, current visual SLAM systems are still computationally demanding, especially on embedded devices. In addition, collaborative SLAM approaches emerge using visual information acquired from multiple robots simultaneously to build a joint map. In order to address both challenges, we present an approach for remote visual SLAM where local binary features are extracted at the robot, compressed and sent over a network to a centralized powerful processing node running the visual SLAM algorithm. To this end, we propose a new feature coding scheme including a feature selection stage which ensures that only relevant information is transmitted. We demonstrate the effectiveness of our approach on well-known datasets. With the proposed approach, it is possible to build an accurate map while limiting the data rate to 75 kbits/frame.


Title: Semantic Robot Programming for Goal-Directed Manipulation in Cluttered Scenes
Key Words: graph theory  image colour analysis  learning (artificial intelligence)  manipulators  path planning  pose estimation  robot programming  robot vision  object geometries  Semantic Robot Programming  task planning  motion planning  Discriminatively-Informed Generative Estimation of Scenes and Transforms  DIGEST method  RGBD images  goal-directed manipulation  cluttered scene dataset  Michigan Progress Fetch robot  object poses  robot manipulator  SRP  semantic mapping  Task analysis  Semantics  Robot programming  Estimation  Planning  Detectors 
Abstract: We present the Semantic Robot Programming (SRP) paradigm as a convergence of robot programming by demonstration and semantic mapping. In SRP, a user can directly program a robot manipulator by demonstrating a snapshot of their intended goal scene in workspace. The robot then parses this goal as a scene graph comprised of object poses and inter-object relations, assuming known object geometries. Task and motion planning is then used to realize the user's goal from an arbitrary initial scene configuration. Even when faced with different initial scene configurations, SRP enables the robot to seamlessly adapt to reach the user's demonstrated goal. For scene perception, we propose the Discriminatively-Informed Generative Estimation of Scenes and Transforms (DIGEST) method to infer the initial and goal states of the world from RGBD images. The efficacy of SRP with DIGEST perception is demonstrated for the task of tray-setting with a Michigan Progress Fetch robot. Scene perception and task execution are evaluated with a public household occlusion dataset and our cluttered scene dataset.


Title: Robust Dense Mapping for Large-Scale Dynamic Environments
Key Words: cameras  image motion analysis  image reconstruction  image segmentation  mobile robots  motion control  object detection  path planning  pose estimation  robot vision  stereo image processing  robust dense mapping  large-scale dynamic environments  stereo-based dense mapping algorithm  large-scale dynamic urban environments  static background  high-level mobile robotic tasks  crowded environments  instance-aware semantic segmentation  sparse scene flow  visual odometry  depth maps  stereo input  map pruning technique  reconstruction accuracy  stationary objects  moving objects detection  path planning  camera poses estimation  frequency 2.5 Hz  Three-dimensional displays  Cameras  Semantics  Vehicle dynamics  Dynamics  Real-time systems  Heuristic algorithms 
Abstract: We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project Websitea.


Title: Deep Reinforcement Learning Supervised Autonomous Exploration in Office Environments
Key Words: decision making  learning (artificial intelligence)  mobile robots  path planning  supervised autonomous exploration  office environments  exploration region selection  autonomous robot exploration task  greedy methods  long-term planning  deep reinforcement learning  exploration knowledge  office blueprints  DRL model  next-best-view selection approach  structural integrity measurement  office maps  decision making process  Planning  Optimization  Prediction algorithms  Task analysis  Predictive models  Computer architecture  Uncertainty 
Abstract: Exploration region selection is an essential decision making process in autonomous robot exploration task. While a majority of greedy methods are proposed to deal with this problem, few efforts are made to investigate the importance of predicting long-term planning. In this paper, we present an algorithm that utilizes deep reinforcement learning (DRL) to learn exploration knowledge over office blueprints, which enables the agent to predict a long-term visiting order for unexplored subregions. On the basis of this algorithm, we propose an exploration architecture that integrates a DRL model, a next-best-view (NBV) selection approach and a structural integrity measurement to further improve the exploration performance. At the end of this paper, we evaluate the proposed architecture against other methods on several new office maps, showing that the agent can efficiently explore uncertain regions with a shorter path and smarter behaviors.


Title: NanoMap: Fast, Uncertainty-Aware Proximity Queries with Lazy Search Over Local 3D Data
Key Words: cartography  collision avoidance  mobile robots  navigation  sensors  NanoMap  uncertainty-aware proximity queries  lazy search  local 3D data  local 3D information  robustly plan motions  local map structure  mapping approaches  global map fusion  motion planner  pose-uncertainty-aware local 3D geometric information  noisy relative pose transforms  depth sensor measurements  minimum-uncertainty view  motion planning  fast 3D obstacle avoidance  mapping techniques  Uncertainty  Robot sensing systems  Planning  Three-dimensional displays  Collision avoidance  History  Current measurement 
Abstract: We would like robots to be able to safely navigate at high speed, efficiently use local 3D information, and robustly plan motions that consider pose uncertainty of measurements in a local map structure. This is hard to do with previously existing mapping approaches, like occupancy grids, that are focused on incrementally fusing 3D data into a common world frame. In particular, both their fragile sensitivity to state estimation errors and computational cost can be limiting. We develop an alternative framework, NanoMap, which alleviates the need for global map fusion and enables a motion planner to efficiently query pose-uncertainty-aware local 3D geometric information. The key idea of NanoMap is to store a history of noisy relative pose transforms and search over a corresponding set of depth sensor measurements for the minimum-uncertainty view of a queried point in space. This approach affords a variety of capabilities not offered by traditional mapping techniques: (a) the pose uncertainty associated with 3D data can be incorporated in motion planning, (b) poses can be updated (i.e., from loop closures) with minimal computational effort, and (c) 3D data can be fused lazily for the purpose of planning. We provide an open-source implementation of NanoMap, and analyze its capabilities and computational efficiency in simulation experiments. Finally, we demonstrate in hardware its effectiveness for fast 3D obstacle avoidance onboard a quadrotor flying up to 10 m/s.


Title: Realtime State Estimation with Tactile and Visual Sensing. Application to Planar Manipulation
Key Words: end effectors  feedback  object detection  pose estimation  position control  robot vision  SLAM (robots)  state estimation  touch (physiological)  tactile input  visual input  incremental smoothing  visual sensing  contact sensing  end-effector  realtime state estimation  robust object state estimation  visual sensor  visual feedback  object shapes  object manipulation  incremental smoothing and mapping  iSAM  planar manipulation  object poses estimation  Robot sensing systems  Visualization  Cost function  State estimation  Cameras 
Abstract: Accurate and robust object state estimation enables successful object manipulation. Visual sensing is widely used to estimate object poses. However, in a cluttered scene or in a tight workspace, the robot's end-effector often occludes the object from the visual sensor. The robot then loses visual feedback and must fall back on open-loop execution. In this paper, we integrate both tactile and visual input using a framework for solving the SLAM problem, incremental smoothing and mapping (iSAM), to provide a fast and flexible solution. Visual sensing provides global pose information but is noisy in general, whereas contact sensing is local, but its measurements are more accurate relative to the end-effector. By combining them, we aim to exploit their advantages and overcome their limitations. We explore the technique in the context of a pusher-slider system. We adapt iSAM's measurement cost and motion cost to the pushing scenario, and use an instrumented setup to evaluate the estimation quality with different object shapes, on different surface materials, and under different contact modes.


Title: Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning
Key Words: decision theory  graph theory  learning (artificial intelligence)  Markov processes  mobile robots  navigation  path planning  probability  search problems  control problem  single follower robot  visual contact  moving target  plausible predictions  predictive models  discrete hypotheses  combinatorial search  physical space  model target behavior  learned navigation reward function  semantic terrain features  search methods  predictive pursuit algorithm  multiple satellite maps  simulation scenarios  inverse reinforcement learning  long term behavior  short term behavior  planning pursuit paths  locations  graph representation  latent destination  position  POMDP solvers  domain specific knowledge  model based probabilistic pursuit  Navigation  Planning  Trajectory  Visualization  Entropy  Predictive models  Learning (artificial intelligence) 
Abstract: We address the integrated prediction, planning, and control problem that enables a single follower robot (the photographer) to quickly re-establish visual contact with a moving target (the subject) that has escaped the follower's field of view. We deal with this scenario, which reactive controllers are typically ill-equipped to handle, by making plausible predictions about the long- and short-term behavior of the target, and planning pursuit paths that will maximize the chance of seeing the target again. At the core of our pursuit method is the use of predictive models of target behavior, which help narrow down the set of possible future locations of the target to a few discrete hypotheses, as well as the use of combinatorial search in physical space to check those hypotheses efficiently. We model target behavior in terms of a learned navigation reward function, using Inverse Reinforcement Learning, based on semantic terrain features of satellite maps. Our pursuit algorithm continuously predicts the latent destination of the target and its position in the future, and relies on efficient graph representation and search methods in order to navigate to locations at which the target is most likely to be seen at an anticipated time. We perform extensive evaluation of our predictive pursuit algorithm over multiple satellite maps, thousands of simulation scenarios, against state-of-the art MDP and POMDP solvers. We show that our method significantly outperforms them by exploiting domain-specific knowledge, while being able to run in real-time.


