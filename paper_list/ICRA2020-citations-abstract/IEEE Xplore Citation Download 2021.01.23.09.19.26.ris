TY  - CONF
TI  - Dynamic Anchor Selection for Improving Object Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9477
EP  - 9483
AU  - P. Shyam
AU  - K. -J. Yoon
AU  - K. -S. Kim
PY  - 2020
KW  - computer vision
KW  - neural nets
KW  - object detection
KW  - DANet
KW  - single-stage object detectors
KW  - dynamic anchor selection
KW  - anchor boxes
KW  - object localization candidates
KW  - MS COCO dataset
KW  - Feature extraction
KW  - Detectors
KW  - Computer architecture
KW  - Task analysis
KW  - Spatial resolution
KW  - Head
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9197076
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Anchor boxes act as potential object localization candidates allow single-stage detectors to achieve real-time performance, at the cost of localization accuracy when compared to state-of-the-art two-stage detectors. Therefore, correct selection of the scale and aspect ratio associated with an anchor box is crucial for detector performance. In this work, we propose a novel architecture called DANet for improving the localization performance of single-stage object detectors, while maintaining real-time inference. The proposed network achieves this by predicting (1) the combination of aspect ratio and scale per feature map based on object density and (2) localization confidence per anchor box. We evaluate the proposed network using the benchmark dataset. On the MS COCO dataset, DANet achieves 30.9% AP at 51.8 fps using ResNet-18 and 45.3% AP at 7.4 fps using ResNeXt-101. The code and models will be available at https://github.com/PS06/AnchorNet.
ER  - 

TY  - CONF
TI  - Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9484
EP  - 9490
AU  - D. Barnes
AU  - I. Posner
PY  - 2020
KW  - distance measurement
KW  - feature extraction
KW  - image colour analysis
KW  - image sensors
KW  - image sequences
KW  - mobile robots
KW  - motion estimation
KW  - object recognition
KW  - object tracking
KW  - radar computing
KW  - robot vision
KW  - SLAM (robots)
KW  - supervised learning
KW  - predict robust keypoints
KW  - odometry estimation
KW  - metric localisation
KW  - self-supervised framework
KW  - differentiable point-based motion estimator
KW  - localisation error
KW  - Oxford Radar RobotCar Dataset
KW  - point-based radar odometry
KW  - Radar
KW  - Measurement
KW  - Task analysis
KW  - Estimation
KW  - Robot sensing systems
KW  - Computer architecture
DO  - 10.1109/ICRA40945.2020.9196835
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a self-supervised framework for learning to detect robust keypoints for odometry estimation and metric localisation in radar. By embedding a differentiable point-based motion estimator inside our architecture, we learn keypoint locations, scores and descriptors from localisation error alone. This approach avoids imposing any assumption on what makes a robust keypoint and crucially allows them to be optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from the Oxford Radar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneously solving metric loop closures. Combining these outputs, we provide a framework capable of full mapping and localisation with radar in urban environments.
ER  - 

TY  - CONF
TI  - SpAGNN: Spatially-Aware Graph Neural Networks for Relational Behavior Forecasting from Sensor Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9491
EP  - 9497
AU  - S. Casas
AU  - C. Gulino
AU  - R. Liao
AU  - R. Urtasun
PY  - 2020
KW  - belief propagation
KW  - convolutional neural nets
KW  - Gaussian processes
KW  - graph theory
KW  - intelligent transportation systems
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - message passing
KW  - object detection
KW  - probability
KW  - self-driving
KW  - probabilistic predictions
KW  - iterative actor state update
KW  - spatially-aware graph neural network
KW  - convolutional neural network
KW  - sensor data
KW  - relational behavior forecasting
KW  - SpAGNN
KW  - model uncertainty
KW  - Gaussian belief propagation
KW  - message passing
KW  - Forecasting
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Neural networks
KW  - Object detection
KW  - Predictive models
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196697
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we tackle the problem of relational behavior forecasting from sensor data. Towards this goal, we propose a novel spatially-aware graph neural network (SpAGNN) that models the interactions between agents in the scene. Specifically, we exploit a convolutional neural network to detect the actors and compute their initial states. A graph neural network then iteratively updates the actor states via a message passing process. Inspired by Gaussian belief propagation, we design the messages to be spatially-transformed parameters of the output distributions from neighboring agents. Our model is fully differentiable, thus enabling end-to-end training. Importantly, our probabilistic predictions can model uncertainty at the trajectory level. We demonstrate the effectiveness of our approach by achieving significant improvements over the state-of-the-art on two real-world self-driving datasets: ATG4D and nuScenes.
ER  - 

TY  - CONF
TI  - Any Motion Detector: Learning Class-agnostic Scene Dynamics from a Sequence of LiDAR Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9498
EP  - 9504
AU  - A. Filatov
AU  - A. Rykov
AU  - V. Murashkin
PY  - 2020
KW  - computational geometry
KW  - computer vision
KW  - feature extraction
KW  - image sequences
KW  - intelligent transportation systems
KW  - learning (artificial intelligence)
KW  - motion compensation
KW  - motion estimation
KW  - object detection
KW  - optical radar
KW  - parameter estimation
KW  - real-time systems
KW  - road safety
KW  - traffic engineering computing
KW  - LiDAR point clouds
KW  - object detection
KW  - urban environment
KW  - motion detection
KW  - 3D point cloud sequence
KW  - object categories
KW  - temporal context aggregation
KW  - class-agnostic scene dynamics
KW  - motion parameters estimation
KW  - self driving vehicle navigation safety
KW  - motion parameter estimation
KW  - real time inference
KW  - road participant motion
KW  - neural network
KW  - camera images
KW  - Three-dimensional displays
KW  - Tensile stress
KW  - Vehicle dynamics
KW  - Transforms
KW  - Laser radar
KW  - Estimation
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9196716
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Object detection and motion parameters estimation are crucial tasks for self-driving vehicle safe navigation in a complex urban environment. In this work we propose a novel real-time approach of temporal context aggregation for motion detection and motion parameters estimation based on 3D point cloud sequence. We introduce an ego-motion compensation layer to achieve real-time inference with performance comparable to a naive odometric transform of the original point cloud sequence. Not only is the proposed architecture capable of estimating the motion of common road participants like vehicles or pedestrians but also generalizes to other object categories which are not present in training data. We also conduct an in-deep analysis of different temporal context aggregation strategies such as recurrent cells and 3D convolutions. Finally, we provide comparison results of our state-of-the-art model with existing solutions on KITTI Scene Flow dataset.
ER  - 

TY  - CONF
TI  - Where and When: Event-Based Spatiotemporal Trajectory Prediction from the iCub’s Point-Of-View
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9521
EP  - 9527
AU  - M. Monforte
AU  - A. Arriandiaga
AU  - A. Glover
AU  - C. Bartolozzi
PY  - 2020
KW  - cameras
KW  - control engineering computing
KW  - human-robot interaction
KW  - image resolution
KW  - recurrent neural nets
KW  - robot vision
KW  - spatiotemporal phenomena
KW  - iCub robot
KW  - event-based spatiotemporal trajectory prediction
KW  - nonlinear trajectories
KW  - frame-based cameras
KW  - asynchronous information stream
KW  - low latency information stream
KW  - high temporal resolution
KW  - long short-term memory networks
KW  - event-cameras spatial sampling
KW  - encoder-decoder architecture
KW  - spatial trajectory points
KW  - human-to-robot handover trajectories
KW  - Trajectory
KW  - Robots
KW  - Predictive models
KW  - Cameras
KW  - Target tracking
KW  - Data models
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197373
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fast, non-linear trajectories have been shown to be more accurately visually measured, and hence predicted, when sampled spatially (that is when the target position changes) rather than temporally, i.e. at a fixed-rate as in traditional frame-based cameras. Event-cameras, with their asynchronous, low latency information stream, allow for spatial sampling with very high temporal resolution, improving the quality of the data and the accuracy of post-processing operations. This paper investigates the use of Long Short-Term Memory (LSTM) networks with event-cameras spatial sampling for trajectory prediction. We show the benefit of using an Encoder-Decoder architecture over parameterised models for regression on event-based human-to-robot handover trajectories. In particular, we exploit the temporal information associated to the events stream to predict not only the incoming spatial trajectory points, but also when these will occur in time. After having studied the proper LSTM input/output sequence length, the network performance are compared to other regression models. Then, prediction behavior and computational time are analysed for the proposed method. We carry out the experiment using an iCub robot equipped with event-cameras, addressing the problem from the robot perspective.
ER  - 

TY  - CONF
TI  - A Data-driven Planning Framework for Robotic Texture Painting on 3D Surfaces
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9528
EP  - 9534
AU  - A. S. Vempati
AU  - R. Siegwart
AU  - J. Nieto
PY  - 2020
KW  - geometry
KW  - image texture
KW  - industrial robots
KW  - mixing
KW  - painting
KW  - path planning
KW  - recurrent neural nets
KW  - robot vision
KW  - supervised learning
KW  - robotic texture painting
KW  - painting textures
KW  - surface geometry
KW  - paint mixing
KW  - self-supervised learning framework
KW  - painting process
KW  - paint simulation environment
KW  - robot executes
KW  - data-driven planning framework
KW  - paint delivery tool flow rate
KW  - 3D surfaces
KW  - recurrent neural network
KW  - RNN
KW  - Painting
KW  - Paints
KW  - Robots
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9196693
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Painting textures on 3D surfaces requires an understanding of the surface geometry, paint flow and paint mixing. This work formulates automated painting as a planning problem and proposes a solution based on a self-supervised learning framework that enables a robot to paint monochromatic non-uniform textures on 3D surfaces. We developed a method that iteratively decides the actions to take based on constant feedback of the painting process. Inspired by recent results, we formulate our solution using a recurrent neural network (RNN) to decide where and what to paint on the surface at each time instant. Specifically, the paint delivery tool's flow rate, orientation and position relative to the surface at each time instant are evaluated. This data can then be processed by a robot's planner of choice for generating a painting mission that can achieve the desired end result. We evaluate the proposed approach by providing qualitative and quantitative results of the different components. Furthermore, we validate the effectiveness of the approach for the application by providing renderings from a paint simulation environment and show how a robot executes the planned painting mission on a generic 3D surface.
ER  - 

TY  - CONF
TI  - Learned Critical Probabilistic Roadmaps for Robotic Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9535
EP  - 9541
AU  - B. Ichter
AU  - E. Schmerling
AU  - T. -W. E. Lee
AU  - A. Faust
PY  - 2020
KW  - graph theory
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - sampling methods
KW  - implicit graph representation
KW  - state space
KW  - solution trajectories
KW  - graph-theoretic techniques
KW  - hierarchical graph
KW  - uniform sampling
KW  - sampling-based motion planning
KW  - robotic motion planning
KW  - motion planning techniques
KW  - learned critical probabilistic roadmaps
KW  - critical PRM
KW  - critical probabilistic roadmaps
KW  - Planning
KW  - Robots
KW  - Trajectory
KW  - Probabilistic logic
KW  - Training
KW  - Complexity theory
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197106
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Sampling-based motion planning techniques have emerged as an efficient algorithmic paradigm for solving complex motion planning problems. These approaches use a set of probing samples to construct an implicit graph representation of the robot's state space, allowing arbitrarily accurate representations as the number of samples increases to infinity. In practice, however, solution trajectories only rely on a few critical states, often defined by structure in the state space (e.g., doorways). In this work we propose a general method to identify these critical states via graph-theoretic techniques (betweenness centrality) and learn to predict criticality from only local environment features. These states are then leveraged more heavily via global connections within a hierarchical graph, termed Critical Probabilistic Roadmaps. Critical PRMs are demonstrated to achieve up to three orders of magnitude improvement over uniform sampling, while preserving the guarantees and complexity of sampling-based motion planning. A video is available at https://youtu.be/AYoD-pGd9ms.
ER  - 

TY  - CONF
TI  - Learning Heuristic A*: Efficient Graph Search using Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9542
EP  - 9547
AU  - S. Kim
AU  - B. An
PY  - 2020
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - optimisation
KW  - path planning
KW  - search problems
KW  - path planning problem
KW  - computation load
KW  - neural network
KW  - optimal paths
KW  - optimal cost
KW  - global optimality
KW  - admissible heuristic function
KW  - efficient graph search
KW  - learning heuristic A*
KW  - LHA*
KW  - suboptimality bound
KW  - maze-like map
KW  - Heuristic algorithms
KW  - Training
KW  - Path planning
KW  - Biological neural networks
KW  - Robots
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197015
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we consider the path planning problem on a graph. To reduce computation load by efficiently exploring the graph, we model the heuristic function as a neural network, which is trained by a training set derived from optimal paths to estimate the optimal cost between a pair of vertices on the graph. As such heuristic function cannot be proved to be an admissible heuristic to guarantee the global optimality of the path, we adapt an admissible heuristic function for the terminating criteria. Thus, proposed Learning Heuristic A* (LHA*) guarantees the bounded suboptimality of the path. The performance of LHA* was demonstrated by simulations in a maze-like map and compared with the performance of weighted A* with the same suboptimality bound.
ER  - 

TY  - CONF
TI  - 3D-CNN Based Heuristic Guided Task-Space Planner for Faster Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9548
EP  - 9554
AU  - R. Terasawa
AU  - Y. Ariki
AU  - T. Narihira
AU  - T. Tsuboi
AU  - K. Nagasaka
PY  - 2020
KW  - collision avoidance
KW  - convolutional neural nets
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - sampling methods
KW  - stereo image processing
KW  - trees (mathematics)
KW  - TS-RRT
KW  - task-space rapidly-exploring random trees
KW  - 3D-CNN
KW  - heuristic guided task-space planner
KW  - fully convolutional neural networks
KW  - heuristic map
KW  - Random Trees
KW  - sampling-based planner
KW  - collision-free path
KW  - robotic manipulation
KW  - motion planning
KW  - Planning
KW  - Task analysis
KW  - Robots
KW  - Collision avoidance
KW  - Heuristic algorithms
KW  - Feature extraction
KW  - Convolution
DO  - 10.1109/ICRA40945.2020.9196883
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion planning is important in a wide variety of applications such as robotic manipulation. However, it is still challenging to reliably find a collision-free path within a reasonable time. To address the issue, this paper proposes a novel framework which combines a sampling-based planner and deep learning for faster motion planning, focusing on heuristics. The proposed method extends Task-Space Rapidly-exploring Random Trees (TS-RRT) to guide the trees with a "heuristic map" where every voxel has a cost-to-go value toward the goal. It also utilizes fully convolutional neural networks (CNNs) for producing more appropriate heuristic maps, rather than manually-designed heuristics. To verify the effectiveness of the proposed method, experiments for motion planning using a real environment and mobile manipulator are carried out. The results indicate that it outperforms the existing planners, especially in terms of the average planning time with smaller variance.
ER  - 

TY  - CONF
TI  - Learned Sampling Distributions for Efficient Planning in Hybrid Geometric and Object-Level Representations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9555
EP  - 9562
AU  - K. Liu
AU  - M. Stadler
AU  - N. Roy
PY  - 2020
KW  - geometry
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - path planning
KW  - robot vision
KW  - linear regression
KW  - efficiency planning
KW  - sampling distribution learning
KW  - sampling-based planners
KW  - object-level semantics
KW  - myopic behavior
KW  - geometric information
KW  - navigation
KW  - robotic agent
KW  - object-level representations
KW  - hybrid geometric
KW  - Navigation
KW  - Planning
KW  - Semantics
KW  - Trajectory
KW  - Mathematical model
KW  - Optimization
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196771
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We would like to enable a robotic agent to quickly and intelligently find promising trajectories through structured, unknown environments. Many approaches to navigation in unknown environments are limited to considering geometric information only, which leads to myopic behavior. In this work, we show that learning a sampling distribution that incorporates both geometric information and explicit, object-level semantics for sampling-based planners enables efficient planning at longer horizons in partially-known environments. We demonstrate that our learned planner is up to 2.7 times more likely to find a plan than the baseline, and can result in up to a 16% reduction in traversal costs as calculated by linear regression. We also show promising qualitative results on real-world data.
ER  - 

TY  - CONF
TI  - Deep Visual Heuristics: Learning Feasibility of Mixed-Integer Programs for Manipulation Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9563
EP  - 9569
AU  - D. Driess
AU  - O. Oguz
AU  - J. -S. Ha
AU  - M. Toussaint
PY  - 2020
KW  - integer programming
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neurocontrollers
KW  - path planning
KW  - robot vision
KW  - deep visual heuristics
KW  - mixed-integer program
KW  - deep neural network
KW  - visual input
KW  - robot manipulation planning
KW  - motion planning
KW  - learning algorithm
KW  - goal encoding
KW  - optimization problems
KW  - Planning
KW  - Task analysis
KW  - Robot sensing systems
KW  - Neural networks
KW  - Grasping
KW  - Search problems
DO  - 10.1109/ICRA40945.2020.9197291
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a deep neural network that predicts the feasibility of a mixed-integer program from visual input for robot manipulation planning. Integrating learning into task and motion planning is challenging, since it is unclear how the scene and goals can be encoded as input to the learning algorithm in a way that enables to generalize over a variety of tasks in environments with changing numbers of objects and goals. To achieve this, we propose to encode the scene and the target object directly in the image space.Our experiments show that our proposed network generalizes to scenes with multiple objects, although during training only two objects are present at the same time. By using the learned network as a heuristic to guide the search over the discrete variables of the mixed-integer program, the number of optimization problems that have to be solved to find a feasible solution or to detect infeasibility can greatly be reduced.
ER  - 

TY  - CONF
TI  - Fast Frontier-based Information-driven Autonomous Exploration with an MAV
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9570
EP  - 9576
AU  - A. Dai
AU  - S. Papatheodorou
AU  - N. Funk
AU  - D. Tzoumanikas
AU  - S. Leutenegger
PY  - 2020
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - entropy
KW  - microrobots
KW  - mobile robots
KW  - navigation
KW  - octrees
KW  - probability
KW  - robot vision
KW  - MAV
KW  - collision-free navigation
KW  - autonomous robots
KW  - microaerial vehicles
KW  - map entropy
KW  - occupancy probabilities
KW  - utility function
KW  - frontier extraction
KW  - frontier-based exploration
KW  - frontier voxels
KW  - map frontiers
KW  - frontier-based information-driven autonomous exploration
KW  - exploration planner
KW  - octree map representation
KW  - visual-based navigation
KW  - motion planning
KW  - octree-based occupancy mapping
KW  - sampling-based exploration
KW  - Planning
KW  - Octrees
KW  - Entropy
KW  - Robot sensing systems
KW  - Measurement
KW  - Task analysis
KW  - Aerial Systems: Perception and Autonomy
KW  - Visual-Based Navigation
DO  - 10.1109/ICRA40945.2020.9196707
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exploration and collision-free navigation through an unknown environment is a fundamental task for autonomous robots. In this paper, a novel exploration strategy for Micro Aerial Vehicles (MAVs) is presented. The goal of the exploration strategy is the reduction of map entropy regarding occupancy probabilities, which is reflected in a utility function to be maximised. We achieve fast and efficient exploration performance with tight integration between our octree-based occupancy mapping approach, frontier extraction, and motion planning-as a hybrid between frontier-based and sampling-based exploration methods. The computationally expensive frontier clustering employed in classic frontier-based exploration is avoided by exploiting the implicit grouping of frontier voxels in the underlying octree map representation. Candidate next-views are sampled from the map frontiers and are evaluated using a utility function combining map entropy and travel time, where the former is computed efficiently using sparse raycasting. These optimisations along with the targeted exploration of frontier-based methods result in a fast and computationally efficient exploration planner. The proposed method is evaluated using both simulated and real-world experiments, demonstrating clear advantages over state-of-the-art approaches.
ER  - 

TY  - CONF
TI  - Dynamic Landing of an Autonomous Quadrotor on a Moving Platform in Turbulent Wind Conditions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9577
EP  - 9583
AU  - A. Paris
AU  - B. T. Lopez
AU  - J. P. How
PY  - 2020
KW  - aircraft landing guidance
KW  - autonomous aerial vehicles
KW  - Global Positioning System
KW  - helicopters
KW  - Kalman filters
KW  - mobile robots
KW  - nonlinear filters
KW  - robot dynamics
KW  - robot vision
KW  - robust control
KW  - turbulence
KW  - variable structure systems
KW  - vehicle dynamics
KW  - dynamic landing
KW  - autonomous quadrotor
KW  - moving platform
KW  - turbulent wind conditions
KW  - autonomous landing
KW  - fast trajectory planning
KW  - wind disturbance
KW  - fully autonomous vision-based system
KW  - quadrotor-platform distance
KW  - landing trajectory
KW  - receding horizon control
KW  - boundary layer sliding controller
KW  - extended Kalman filter
KW  - GPS measurements
KW  - robust control
KW  - precise control
KW  - Trajectory
KW  - Cameras
KW  - Vehicle dynamics
KW  - Planning
KW  - Global Positioning System
KW  - Visualization
KW  - Acceleration
KW  - Unmanned aerial vehicles
KW  - autonomous vehicles
KW  - landing on a moving platform
KW  - disturbance compensation
DO  - 10.1109/ICRA40945.2020.9197081
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous landing on a moving platform presents unique challenges for multirotor vehicles, including the need to accurately localize the platform, fast trajectory planning, and precise/robust control. Previous works studied this problem but most lack explicit consideration of the wind disturbance, which typically leads to slow descents onto the platform. This work presents a fully autonomous vision-based system that addresses these limitations by tightly coupling the localization, planning, and control, thereby enabling fast and accurate landing on a moving platform. The platform's position, orientation, and velocity are estimated by an extended Kalman filter using simulated GPS measurements when the quadrotor-platform distance is large, and by a visual fiducial system when the platform is nearby. The landing trajectory is computed online using receding horizon control and is followed by a boundary layer sliding controller that provides tracking performance guarantees in the presence of unknown, but bounded, disturbances. To improve the performance, the characteristics of the turbulent conditions are accounted for in the controller. The landing trajectory is fast, direct, and does not require hovering over the platform, as is typical of most stateof-the-art approaches. Simulations and hardware experiments are presented to validate the robustness of the approach.
ER  - 

TY  - CONF
TI  - Direct NMPC for Post-Stall Motion Planning with Fixed-Wing UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9592
EP  - 9598
AU  - M. Basescu
AU  - J. Moore
PY  - 2020
KW  - aerodynamics
KW  - aerospace components
KW  - autonomous aerial vehicles
KW  - feedback
KW  - mobile robots
KW  - motion control
KW  - nonlinear control systems
KW  - path planning
KW  - position control
KW  - predictive control
KW  - vehicle dynamics
KW  - rotary-wing UAVs
KW  - nonlinear control approach
KW  - fixed-wing UAVs
KW  - full-state direct trajectory optimization
KW  - representative aircraft model
KW  - nonlinear aircraft model
KW  - fixed-wing trajectories
KW  - randomized motion planning
KW  - local-linear feedback
KW  - direct NMPC
KW  - post-stall motion planning
KW  - fixed-wing unmanned aerial vehicles
KW  - frequency 5.0 Hz
KW  - Aerodynamics
KW  - Trajectory
KW  - Atmospheric modeling
KW  - Real-time systems
KW  - Computational modeling
KW  - Planning
KW  - Splines (mathematics)
DO  - 10.1109/ICRA40945.2020.9196724
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fixed-wing unmanned aerial vehicles (UAVs) offer significant performance advantages over rotary-wing UAVs in terms of speed, endurance, and efficiency. However, these vehicles have traditionally been severely limited with regards to maneuverability. In this paper, we present a nonlinear control approach for enabling aerobatic fixed-wing UAVs to maneuver in constrained spaces. Our approach utilizes full-state direct trajectory optimization and a minimalistic, but representative, nonlinear aircraft model to plan aggressive fixed-wing trajectories in real-time at 5 Hz across high angles-of-attack. Randomized motion planning is used to avoid local minima and local-linear feedback is used to compensate for model inaccuracies between updates. We demonstrate our method in hardware and show that both local-linear feedback and re-planning are necessary for successful navigation of a complex environment in the presence of model uncertainty.
ER  - 

TY  - CONF
TI  - A Flight Envelope Determination and Protection System for Fixed-Wing UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9599
EP  - 9605
AU  - G. Zogopoulos-Papaliakos
AU  - K. J. Kyriakopoulos
PY  - 2020
KW  - aerospace components
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - convex programming
KW  - nonlinear control systems
KW  - predictive control
KW  - remotely operated vehicles
KW  - flight envelope determination
KW  - protection system
KW  - fixed-wing UAV
KW  - generic model
KW  - nonlinear numerical model
KW  - model predictive controller
KW  - flight envelope constraints
KW  - trim flight envelope
KW  - MPC
KW  - Iron
KW  - Aircraft
KW  - Numerical models
KW  - Atmospheric modeling
KW  - Approximation algorithms
KW  - Mathematical model
KW  - Aerospace control
DO  - 10.1109/ICRA40945.2020.9197433
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we present a novel, approximate, efficient algorithm for determining the Trim Flight Envelope of a fixed-wing UAV, based on a generic, nonlinear numerical model. The resulting Flight Envelope is expressed as a convex intersection of half-spaces. Subsequently, a Model Predictive Controller (MPC) is designed which takes into account the Flight Envelope constraints, to avoid Loss-of-Control. The overall system is shown to operate in real-time in a simulation environment.
ER  - 

TY  - CONF
TI  - Multi-Head Attention for Multi-Modal Joint Vehicle Motion Forecasting
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9638
EP  - 9644
AU  - J. Mercat
AU  - T. Gilles
AU  - N. El Zoghby
AU  - G. Sandou
AU  - D. Beauvois
AU  - G. P. Gil
PY  - 2020
KW  - position control
KW  - probability
KW  - road vehicles
KW  - tracking
KW  - joint forecast
KW  - multimodal probability density functions
KW  - vehicle position tracks
KW  - multimodal joint vehicle motion forecasting
KW  - maneuver definitions
KW  - road scene
KW  - long short-term memory layers
KW  - spatial grid
KW  - Forecasting
KW  - Predictive models
KW  - Roads
KW  - Uncertainty
KW  - Computer architecture
KW  - Tensile stress
KW  - Probability density function
DO  - 10.1109/ICRA40945.2020.9197340
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel vehicle motion forecasting method based on multi-head attention. It produces joint forecasts for all vehicles on a road scene as sequences of multi-modal probability density functions of their positions. Its architecture uses multi-head attention to account for interactions between all vehicles, and long short-term memory layers for encoding and forecasting. It relies solely on vehicle position tracks, does not need maneuver definitions, and does not rasterize the scene as a spatial grid. This allows it to be more versatile than similar model while combining many forecasting capabilities, namely joint forecast with interactions, uncertainty estimation, and multi-modality. The resulting prediction likelihood outperforms state-of-the-art models on the same dataset.
ER  - 

TY  - CONF
TI  - A Volumetric Albedo Framework for 3D Imaging Sonar Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9645
EP  - 9651
AU  - E. Westman
AU  - I. Gkioulekas
AU  - M. Kaess
PY  - 2020
KW  - autonomous underwater vehicles
KW  - image reconstruction
KW  - optimisation
KW  - robot vision
KW  - solid modelling
KW  - sonar
KW  - sonar imaging
KW  - stereo image processing
KW  - 3D imaging sonar reconstruction
KW  - object-level 3D underwater reconstruction
KW  - imaging sonar sensors
KW  - nonline-of-sight reconstruction
KW  - convex linear optimization problem
KW  - alternating direction method of multipliers
KW  - ADMM
KW  - sonar elevation apertures
KW  - autonomous underwater vehicles
KW  - volumetric Albedo framework
KW  - Sonar
KW  - Image reconstruction
KW  - Imaging
KW  - Three-dimensional displays
KW  - Sensors
KW  - Nonlinear optics
KW  - Surface reconstruction
DO  - 10.1109/ICRA40945.2020.9197042
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel framework for object-level 3D underwater reconstruction using imaging sonar sensors. We demonstrate that imaging sonar reconstruction is analogous to the problem of confocal non-line-of-sight (NLOS) reconstruction. Drawing upon this connection, we formulate the problem as one of solving for volumetric albedo, where the scene of interest is modeled as a directionless albedo field. After discretization, reconstruction reduces to a convex linear optimization problem, which we can augment with a variety of priors and regularization terms. We show how to solve the resulting regularized problems using the alternating direction method of multipliers (ADMM) algorithm. We demonstrate the effectiveness of the proposed approach in simulation and on real-world datasets collected in a controlled, test tank environment with several different sonar elevation apertures.
ER  - 

TY  - CONF
TI  - Map Management Approach for SLAM in Large-Scale Indoor and Outdoor Areas
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9652
EP  - 9658
AU  - S. F. G. Ehlers
AU  - M. Stuede
AU  - K. Nuelle
AU  - T. Ortmaier
PY  - 2020
KW  - image registration
KW  - iterative methods
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - SLAM (robots)
KW  - link-points
KW  - multiple indoor areas
KW  - outdoor areas
KW  - map quality
KW  - single map approaches
KW  - semantic map management approach
KW  - multiple maps
KW  - modular map structure
KW  - utilized SLAM method
KW  - laser scan data
KW  - appropriate SLAM configuration
KW  - single independent maps
KW  - appearance-based method
KW  - iterative closest point registration
KW  - point clouds
KW  - simultaneous localization and mapping configurations
KW  - Simultaneous localization and mapping
KW  - Lasers
KW  - Navigation
KW  - Feature extraction
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196997
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents a semantic map management approach for various environments by triggering multiple maps with different simultaneous localization and mapping (SLAM) configurations. A modular map structure allows to add, modify or delete maps without influencing other maps of different areas. The hierarchy level of our algorithm is above the utilized SLAM method. Evaluating laser scan data (e.g. the detection of passing a doorway) triggers a new map, automatically choosing the appropriate SLAM configuration from a manually predefined list. Single independent maps are connected by link-points, which are located in an overlapping zone of both maps, enabling global navigation over several maps. Loop- closures between maps are detected by an appearance-based method, using feature matching and iterative closest point (ICP) registration between point clouds. Based on the arrangement of maps and link-points, a topological graph is extracted for navigation purpose and tracking the global robot's position over several maps. Our approach is evaluated by mapping a university campus with multiple indoor and outdoor areas and abstracting a metrical-topological graph. It is compared to a single map running with different SLAM configurations. Our approach enhances the overall map quality compared to the single map approaches by automatically choosing predefined SLAM configurations for different environmental setups.
ER  - 

TY  - CONF
TI  - A Hierarchical Framework for Collaborative Probabilistic Semantic Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9659
EP  - 9665
AU  - Y. Yue
AU  - C. Zhao
AU  - R. Li
AU  - C. Yang
AU  - J. Zhang
AU  - M. Wen
AU  - Y. Wang
AU  - D. Wang
PY  - 2020
KW  - Bayes methods
KW  - expectation-maximisation algorithm
KW  - geometry
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - single robot semantic mapping
KW  - collaborative geometry mapping
KW  - semantic point cloud
KW  - heterogeneous sensor fusion model
KW  - collaborative robots level
KW  - 3D semantic map fusion algorithm
KW  - hierarchical collaborative probabilistic semantic mapping framework
KW  - Bayesian rule
KW  - probability
KW  - expectation-maximization
KW  - mathematical modeling
KW  - Semantics
KW  - Collaboration
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Geometry
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197261
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Performing collaborative semantic mapping is a critical challenge for cooperative robots to maintain a comprehensive contextual understanding of the surroundings. Most of the existing work either focus on single robot semantic mapping or collaborative geometry mapping. In this paper, a novel hierarchical collaborative probabilistic semantic mapping framework is proposed, where the problem is formulated in a distributed setting. The key novelty of this work is the mathematical modeling of the overall collaborative semantic mapping problem and the derivation of its probability decomposition. In the single robot level, the semantic point cloud is obtained based on heterogeneous sensor fusion model and is used to generate local semantic maps. Since the voxel correspondence is unknown in collaborative robots level, an Expectation-Maximization approach is proposed to estimate the hidden data association, where Bayesian rule is applied to perform semantic and occupancy probability update. The experimental results show the high quality global semantic map, demonstrating the accuracy and utility of 3D semantic map fusion algorithm in real missions.
ER  - 

TY  - CONF
TI  - Autonomous Navigation in Unknown Environments using Sparse Kernel-based Occupancy Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9666
EP  - 9672
AU  - T. Duong
AU  - N. Das
AU  - M. Yip
AU  - N. Atanasov
PY  - 2020
KW  - collision avoidance
KW  - image classification
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - trajectory control
KW  - decision boundary
KW  - kernel perceptron classifier
KW  - online training algorithm
KW  - piecewise-polynomial robot trajectories
KW  - autonomous navigation
KW  - Ackermann-drive robot
KW  - sparse kernel-based occupancy
KW  - real-time occupancy mapping
KW  - autonomous robot
KW  - map representation
KW  - piecewise-polynomial robot trajectory
KW  - piecewise-linear robot trajectory
KW  - collision checking algorithm
KW  - Support vector machines
KW  - Kernel
KW  - Training
KW  - Robot sensing systems
KW  - Collision avoidance
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197412
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on real-time occupancy mapping and collision checking onboard an autonomous robot navigating in an unknown environment. We propose a new map representation, in which occupied and free space are separated by the decision boundary of a kernel perceptron classifier. We develop an online training algorithm that maintains a very sparse set of support vectors to represent obstacle boundaries in configuration space. We also derive conditions that allow complete (without sampling) collision-checking for piecewise-linear and piecewise-polynomial robot trajectories. We demonstrate the effectiveness of our mapping and collision checking algorithms for autonomous navigation of an Ackermann-drive robot in unknown environments.
ER  - 

TY  - CONF
TI  - Hybrid Topological and 3D Dense Mapping through Autonomous Exploration for Large Indoor Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9673
EP  - 9679
AU  - C. Gomez
AU  - M. Fehr
AU  - A. Millane
AU  - A. C. Hernandez
AU  - J. Nieto
AU  - R. Barber
AU  - R. Siegwart
PY  - 2020
KW  - image representation
KW  - indoor navigation
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - topology
KW  - indoor environments
KW  - topological global representations
KW  - 3D dense submaps
KW  - hybrid global map
KW  - autonomous exploration
KW  - autonomous navigation
KW  - path planning
KW  - dense 3D maps
KW  - 3D dense representations
KW  - 3D dense mapping systems
KW  - hybrid topological mapping
KW  - metric 3D maps
KW  - standard CPU
KW  - Three-dimensional displays
KW  - Measurement
KW  - Robots
KW  - Semantics
KW  - Two dimensional displays
KW  - Indoor environments
KW  - Path planning
DO  - 10.1109/ICRA40945.2020.9197226
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots require a detailed understanding of the 3D structure of the environment for autonomous navigation and path planning. A popular approach is to represent the environment using metric, dense 3D maps such as 3D occupancy grids. However, in large environments the computational power required for most state-of-the-art 3D dense mapping systems is compromising precision and real-time capability. In this work, we propose a novel mapping method that is able to build and maintain 3D dense representations for large indoor environments using standard CPUs. Topological global representations and 3D dense submaps are maintained as hybrid global map. Submaps are generated for every new visited place. A place (room) is identified as an isolated part of the environment connected to other parts through transit areas (doors). This semantic partitioning of the environment allows for a more efficient mapping and path-planning. We also propose a method for autonomous exploration that directly builds the hybrid representation in real time.We validate the real-time performance of our hybrid system on simulated and real environments regarding mapping and path-planning. The improvement in execution time and memory requirements upholds the contribution of the proposed work.
ER  - 

TY  - CONF
TI  - Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9680
EP  - 9686
AU  - S. -F. Ch’ng
AU  - N. Sogi
AU  - P. Purkait
AU  - T. -J. Chin
AU  - K. Fukui
PY  - 2020
KW  - computer vision
KW  - object detection
KW  - optimisation
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - lifted algorithm
KW  - combinatorial complexity
KW  - heuristic criterion
KW  - planar pose estimation
KW  - marker-based mapping
KW  - highly ambiguous inputs
KW  - PPE ambiguities
KW  - possible marker orientation solutions
KW  - rotation averaging formulation
KW  - marker corners
KW  - computer vision
KW  - planar markers
KW  - clique constraints
KW  - robust rotation averaging
KW  - marker pose ambiguity
KW  - Cameras
KW  - Pipelines
KW  - Machine-to-machine communications
KW  - Image edge detection
KW  - Pose estimation
KW  - Simultaneous localization and mapping
KW  - Histograms
DO  - 10.1109/ICRA40945.2020.9196902
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planar markers are useful in robotics and computer vision for mapping and localisation. Given a detected marker in an image, a frequent task is to estimate the 6DOF pose of the marker relative to the camera, which is an instance of planar pose estimation (PPE). Although there are mature techniques, PPE suffers from a fundamental ambiguity problem, in that there can be more than one plausible pose solutions for a PPE instance. Especially when localisation of the marker corners is noisy, it is often difficult to disambiguate the pose solutions based on reprojection error alone. Previous methods choose between the possible solutions using a heuristic criterion, or simply ignore ambiguous markers.We propose to resolve the ambiguities by examining the consistencies of a set of markers across multiple views. Our specific contributions include a novel rotation averaging formulation that incorporates long-range dependencies between possible marker orientation solutions that arise from PPE ambiguities. We analyse the combinatorial complexity of the problem, and develop a novel lifted algorithm to effectively resolve marker pose ambiguities, without discarding any marker observations. Results on real and synthetic data show that our method is able to handle highly ambiguous inputs, and provides more accurate and/or complete marker-based mapping and localisation.
ER  - 

TY  - CONF
TI  - Anticipating the Start of User Interaction for Service Robot in the Wild
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9687
EP  - 9693
AU  - K. Ito
AU  - Q. Kong
AU  - S. Horiguchi
AU  - T. Sumiyoshi
AU  - K. Nagamatsu
PY  - 2020
KW  - cameras
KW  - face recognition
KW  - feature extraction
KW  - image colour analysis
KW  - image motion analysis
KW  - image sensors
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - service robots
KW  - user interaction
KW  - service robot
KW  - proactive service
KW  - potential visitors
KW  - visitor
KW  - early anticipation
KW  - human pose information
KW  - publicly available JPL interaction dataset
KW  - accurate anticipation performance
KW  - CNN-LSTM-based model
KW  - human verification
DO  - 10.1109/ICRA40945.2020.9196548
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A service robot is expected to provide proactive service for visitors who require its help. In contrast to passive service, e.g., providing service only after being spoken to, proactive service initiates an interaction at an early stage, e.g., talking to potential visitors who need the robot's help in advance. This paper addresses how to anticipate the start of user interaction. We propose an approach using only a single RGB camera that anticipates whether a visitor will come to the robot for interaction or just pass it by. In the proposed approach, we (i) utilize the visitor's pose information from captured images incorporating facial information, (ii) train a CNN-LSTM-based model in an end-to-end manner with an exponential loss for early anticipation, and (iii) during the training, the network branch for facial keypoints acquired as the part of the human pose information is taught to mimic the branch trained with the face image from a specialized face detector with a human verification. By virtue of (iii), at the inference, we can run our model in an embedded system processing only the pose information without an additional face detector and typical accuracy drop. We evaluated the proposed approach on our collected real world data with a real service robot and publicly available JPL interaction dataset and found that it achieved accurate anticipation performance.
ER  - 

TY  - CONF
TI  - Spin Detection in Robotic Table Tennis*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9694
EP  - 9700
AU  - J. Tebbe
AU  - L. Klamt
AU  - Y. Gao
AU  - A. Zell
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - sport
KW  - spin detection
KW  - robotic table tennis
KW  - rotation
KW  - table tennis match
KW  - human player
KW  - return stroke
KW  - high-speed camera
KW  - frame rate
KW  - circular brand logo
KW  - background difference
KW  - spin types
KW  - table tennis rally
KW  - frequency 380.0 Hz
KW  - Three-dimensional displays
KW  - Cameras
KW  - Robot vision systems
KW  - Trajectory
KW  - Quaternions
DO  - 10.1109/ICRA40945.2020.9196536
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In table tennis, the rotation (spin) of the ball plays a crucial role. A table tennis match will feature a variety of strokes. Each generates different amounts and types of spin. To develop a robot that can compete with a human player, the robot needs to detect spin, so it can plan an appropriate return stroke. In this paper we compare three methods to estimate spin. The first two approaches use a high-speed camera that captures the ball in flight at a frame rate of 380 Hz. This camera allows the movement of the circular brand logo printed on the ball to be seen. The first approach uses background difference to determine the position of the logo. In a second alternative, we train a CNN to predict the orientation of the logo. The third method evaluates the trajectory of the ball and derives the rotation from the effect of the Magnus force. This method gives the highest accuracy and is used for a demonstration. Our robot successfully copes with different spin types in a real table tennis rally against a human opponent.
ER  - 

TY  - CONF
TI  - Look, Listen, and Act: Towards Audio-Visual Embodied Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9701
EP  - 9707
AU  - C. Gan
AU  - Y. Zhang
AU  - J. Wu
AU  - B. Gong
AU  - J. B. Tenenbaum
PY  - 2020
KW  - acoustic signal processing
KW  - audio signal processing
KW  - audio-visual systems
KW  - human computer interaction
KW  - mobile agents
KW  - navigation
KW  - path planning
KW  - audio-visual embodied navigation
KW  - mobile intelligent agents
KW  - multiple sensory inputs
KW  - sound source
KW  - indoor environment
KW  - raw egocentric visual data
KW  - audio sensory data
KW  - audio signal
KW  - visual environment
KW  - visual pieces
KW  - audio pieces
KW  - visual perception mapper module
KW  - sound perception module
KW  - audio-visual observations
KW  - simulated multimodal environment
KW  - visual-audio-room dataset
KW  - Navigation
KW  - Visualization
KW  - Task analysis
KW  - Robot sensing systems
KW  - Visual perception
KW  - Acoustics
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197008
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A crucial ability of mobile intelligent agents is to integrate the evidence from multiple sensory inputs in an environment and to make a sequence of actions to reach their goals. In this paper, we attempt to approach the problem of Audio-Visual Embodied Navigation, the task of planning the shortest path from a random starting location in a scene to the sound source in an indoor environment, given only raw egocentric visual and audio sensory data. To accomplish this task, the agent is required to learn from various modalities, i.e., relating the audio signal to the visual environment. Here we describe an approach to audio-visual embodied navigation that takes advantage of both visual and audio pieces of evidence. Our solution is based on three key ideas: a visual perception mapper module that constructs its spatial memory of the environment, a sound perception module that infers the relative location of the sound source from the agent, and a dynamic path planner that plans a sequence of actions based on the audio-visual observations and the spatial memory of the environment to navigate toward the goal. Experimental results on a newly collected Visual-Audio-Room dataset using the simulated multi-modal environment demonstrate the effectiveness of our approach over several competitive baselines.
ER  - 

TY  - CONF
TI  - Autonomous Tool Construction with Gated Graph Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9708
EP  - 9714
AU  - C. Yang
AU  - X. Lan
AU  - H. Zhang
AU  - N. Zheng
PY  - 2020
KW  - convolutional neural nets
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - tools
KW  - gated graph neural network
KW  - autonomous tool construction
KW  - reference tool
KW  - robotics
KW  - GGNN
KW  - RCNN-like structure
KW  - TC-GRCNN
KW  - large-scale training data generation
KW  - large-scale testing data generation
KW  - tool construction graph RCNN
KW  - region based convolutional neural network
KW  - candidate part pairs
KW  - Tools
KW  - Data models
KW  - Robots
KW  - Task analysis
KW  - Training
KW  - Solid modeling
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197285
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous tool construction is a significant but challenging task in robotics. This task can be interpreted as when given a reference tool, selecting some available candidate parts to reconstruct it. Most of the existing works perform tool construction in the form of action part and grasp part, which is only a specific construction pattern and limits its application to some extent. In general scenarios, a tool can be constructed in various patterns with different part pairs. Therefore, whether a part pair is most suitable for constructing the tool depends not only on itself, but on other parts in the same scene. To solve this problem, we construct a Gated Graph Neural Network (GGNN) to model the relations between all part pairs, so that we can select the candidate parts in consideration of the global information. Afterwards, we embed the constructed GGNN into a RCNN-like structure to finally accomplish tool construction. The whole model will be named Tool Construction Graph RCNN (TC-GRCNN). In addition, we develop a mechanism that can generate large-scale training and testing data in simulation environments, by which we can save the time of data collection and annotation. Finally, the proposed model is deployed on the physical robot. The experiment results show that TC-GRCNN can perform well in the general scenarios of tool construction.
ER  - 

TY  - CONF
TI  - Training-Set Distillation for Real-Time UAV Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9715
EP  - 9721
AU  - F. Li
AU  - C. Fu
AU  - F. Lin
AU  - Y. Li
AU  - P. Lu
PY  - 2020
KW  - autonomous aerial vehicles
KW  - correlation methods
KW  - image filtering
KW  - image motion analysis
KW  - minimisation
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - training-set distillation
KW  - UAV object tracking
KW  - correlation filter
KW  - visual object tracking
KW  - unmanned aerial vehicle
KW  - energy minimization function
KW  - scoring process
KW  - time slot-based distillation approach
KW  - Training
KW  - Unmanned aerial vehicles
KW  - Correlation
KW  - Reliability
KW  - Real-time systems
KW  - Optimization
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197252
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set's quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accelerate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on single CPU.
ER  - 

TY  - CONF
TI  - CNN-Based Simultaneous Dehazing and Depth Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9722
EP  - 9728
AU  - B. -U. Lee
AU  - K. Lee
AU  - J. Oh
AU  - I. S. Kweon
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - correlation methods
KW  - image coding
KW  - image colour analysis
KW  - image denoising
KW  - image representation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - spatial variables measurement
KW  - single hazy RGB input
KW  - single dense encoder
KW  - encoded image representation
KW  - dehazing image depth estimation
KW  - single image depth estimation
KW  - image dehazing
KW  - computer vision
KW  - convolutional neural networks
KW  - CNN
KW  - dehazing depth estimation algorithms
KW  - traditional haze modeling
KW  - depth estimation network
KW  - fully scaled depth map
KW  - depth-transmission consistency loss
KW  - separate decoders
KW  - Decoding
KW  - Propagation losses
KW  - Estimation
KW  - Training
KW  - Image reconstruction
KW  - Task analysis
KW  - Scattering
DO  - 10.1109/ICRA40945.2020.9197358
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It is difficult for both cameras and depth sensors to obtain reliable information in hazy scenes. Therefore, image dehazing is still one of the most challenging problems to solve in computer vision and robotics. With the development of convolutional neural networks (CNNs), lots of dehazing and depth estimation algorithms using CNNs have emerged. However, very few of those try to solve these two problems at the same time. Focusing on the fact that traditional haze modeling contains depth information in its formula, we propose a CNN-based simultaneous dehazing and depth estimation network. Our network aims to estimate both a dehazed image and a fully scaled depth map from a single hazy RGB input with end-to-end training. The network contains a single dense encoder and four separate decoders; each of them shares the encoded image representation while performing individual tasks. We suggest a novel depth-transmission consistency loss in the training scheme to fully utilize the correlation between the depth information and transmission map. To demonstrate the robustness and effectiveness of our algorithm, we performed various ablation studies and compared our results to those of state-of-the-art algorithms in dehazing and single image depth estimation, both qualitatively and quantitatively. Furthermore, we show the generality of our network by applying it to some real-world examples.
ER  - 

TY  - CONF
TI  - Internet of Things (IoT)-based Collaborative Control of a Redundant Manipulator for Teleoperated Minimally Invasive Surgeries
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9737
EP  - 9742
AU  - H. Su
AU  - S. E. Ovur
AU  - Z. Li
AU  - Y. Hu
AU  - J. Li
AU  - A. Knoll
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2020
KW  - control engineering computing
KW  - end effectors
KW  - human-robot interaction
KW  - Internet of Things
KW  - medical robotics
KW  - motion control
KW  - phantoms
KW  - redundant manipulators
KW  - surgery
KW  - telerobotics
KW  - Things-based collaborative control
KW  - teleoperated Minimally Invasive surgeries
KW  - Robot-assisted Minimally Invasive Surgery scenario
KW  - hierarchical operational space formulation
KW  - 7-DoFs redundant manipulator
KW  - multiple operational tasks
KW  - motion constraint
KW  - collision avoidance
KW  - undergoing surgical operation
KW  - Internet of Robotic Things
KW  - human-robot interaction
KW  - compliant swivel motion
KW  - HTC VIVE PRO controllers
KW  - robot elbow
KW  - smooth swivel motion
KW  - KUKA LWR4+ slave robot
KW  - SIGMA 7 master manipulator
KW  - Internet of Things-based human-robot collaborative control scheme
KW  - priority levels
KW  - Collision avoidance
KW  - Manipulators
KW  - Surgery
KW  - Task analysis
KW  - Force
KW  - Tools
DO  - 10.1109/ICRA40945.2020.9197321
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, an Internet of Things-based human-robot collaborative control scheme is developed in Robot-assisted Minimally Invasive Surgery scenario. A hierarchical operational space formulation is designed to exploit the redundancies of the 7-DoFs redundant manipulator to handle multiple operational tasks based on their priority levels, such as guaranteeing a remote center of motion constraint and avoiding collision with a swivel motion without influencing the undergoing surgical operation. Furthermore, the concept of the Internet of Robotic Things is exploited to facilitate the best action of the robot in human-robot interaction. Instead of utilizing compliant swivel motion, HTC VIVE PRO controllers, used as the Internet of Things technology, is adopted to detect the collision. A virtual force is applied to the robot elbow, enabling a smooth swivel motion for human-robot interaction. The effectiveness of the proposed strategy is validated using experiments performed on a patient phantom in a lab setup environment, with a KUKA LWR4+ slave robot and a SIGMA 7 master manipulator. By comparison with previous works, the results show improved performances in terms of the accuracy of the RCM constraint and surgical tip.
ER  - 

TY  - CONF
TI  - Passive Dynamic Balancing and Walking in Actuated Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9775
EP  - 9781
AU  - J. Reher
AU  - N. Csomay-Shanklin
AU  - D. L. Christensen
AU  - B. Bristow
AU  - A. D. Ames
AU  - L. Smoot
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - mechanical stability
KW  - pendulums
KW  - robot dynamics
KW  - passive dynamic balancing
KW  - passive dynamic systems
KW  - dynamic behaviors
KW  - actuated robots
KW  - robotic assistive devices
KW  - robotic systems
KW  - passive system
KW  - passive bipedal robot
KW  - dynamically stable periodic walking gaits
KW  - passive dynamic walking
KW  - Legged locomotion
KW  - Dynamics
KW  - Nonlinear dynamical systems
KW  - Robot sensing systems
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9197400
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The control of passive dynamic systems remains a challenging problem in the field of robotics, and insights from their study can inform everything from dynamic behaviors on actuated robots to robotic assistive devices. In this work, we explore the use of flat actuated environments for realizing passive dynamic balancing and locomotion. Specifically, we utilize a novel omnidirectional actuated floor to dynamically stabilize two robotic systems. We begin with an inverted pendulum to demonstrate the ability to control a passive system through an active environment. We then consider a passive bipedal robot wherein dynamically stable periodic walking gaits are generated through an optimization that leverages the actuated floor. The end result is the ability to demonstrate passive dynamic walking experimentally through the use of actuated environments.
ER  - 

TY  - CONF
TI  - Biped Stabilization by Linear Feedback of the Variable-Height Inverted Pendulum Model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9782
EP  - 9788
AU  - S. Caron
PY  - 2020
KW  - feedback
KW  - humanoid robots
KW  - legged locomotion
KW  - nonlinear control systems
KW  - pendulums
KW  - robot dynamics
KW  - stability
KW  - stabilization
KW  - linear feedback
KW  - variable-height
KW  - pendulum model
KW  - balancing strategy
KW  - height variations
KW  - well-known ankle strategy
KW  - biped stabilizer
KW  - input feasibility
KW  - state viability constraints
KW  - resulting stabilizer
KW  - Mathematical model
KW  - Lips
KW  - Three-dimensional displays
KW  - Feedback control
KW  - Legged locomotion
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196715
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The variable-height inverted pendulum (VHIP) model enables a new balancing strategy by height variations of the center of mass, in addition to the well-known ankle strategy. We propose a biped stabilizer based on linear feedback of the VHIP that is simple to implement, coincides with the state-of-the-art for small perturbations and is able to recover from larger perturbations thanks to this new strategy. This solution is based on "best-effort" pole placement of a 4D divergent component of motion for the VHIP under input feasibility and state viability constraints. We complement it with a suitable whole-body admittance control law and test the resulting stabilizer on the HRP-4 humanoid robot.
ER  - 

TY  - CONF
TI  - Stability Criteria of Balanced and Steppable Unbalanced States for Full-Body Systems with Implications in Robotic and Human Gait
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9789
EP  - 9795
AU  - W. Z. Peng
AU  - C. Mummolo
AU  - J. H. Kim
PY  - 2020
KW  - gait analysis
KW  - humanoid robots
KW  - legged locomotion
KW  - position control
KW  - robot dynamics
KW  - stability
KW  - steppable unbalanced state boundary
KW  - full-body systems
KW  - double support contact configurations
KW  - steppable states
KW  - biped system
KW  - full-order nonlinear system dynamics
KW  - DARwIn-OP humanoid robot
KW  - balance stability boundaries
KW  - DS contact configuration
KW  - human gait
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196820
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Biped walking involves a series of transitions between single support (SS) and double support (DS) contact configurations that can include both balanced and unbalanced states. The new concept of steppability is introduced to partition the set of unbalanced states into steppable states and falling (unsteppable) states based on the ability of a biped system to respond to forward velocity perturbations by stepping. In this work, a complete system-specific analysis of the stepping process including full-order nonlinear system dynamics is presented for the DARwIn-OP humanoid robot and a human subject in the sagittal plane with respect to both balance stability and steppability. The balance stability and steppability of each system are analyzed by numerical construction of its balance stability boundaries (BSB) for the initial SS and final DS contact configuration and the steppable unbalanced state boundary (SUB). These results are presented with center of mass (COM) trajectories obtained from walking experiments to benchmark robot controller performance and analyze the variation of balance stability and steppability with COM and swing foot position along the progression of a step cycle. For each system, DS BSBs were obtained with both constrained and unconstrained arms in order to demonstrate the ability of this approach to incorporate the effects of angular momentum and system-specific characteristics such as actuation torque, velocity, and angle limits.
ER  - 

TY  - CONF
TI  - Material Handling by Humanoid Robot While Pushing Carts Using a Walking Pattern Based on Capture Point
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9796
EP  - 9801
AU  - J. C. Vaz
AU  - P. Oh
PY  - 2020
KW  - friction
KW  - humanoid robots
KW  - legged locomotion
KW  - materials handling
KW  - motion control
KW  - robot dynamics
KW  - robot self balance
KW  - zero moment point pattern
KW  - walking pattern dynamic model
KW  - arm compliance
KW  - friction compensation
KW  - capture point method
KW  - cart pushing
KW  - humanoid robot
KW  - material handling
KW  - Humanoid robots
KW  - Legged locomotion
KW  - Friction
KW  - Robot sensing systems
KW  - Wrist
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196872
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a study that evaluates the effects on the walking pattern of a full-sized humanoid robot as it pushes different carts. Furthermore, it discuss a modified Zero Moment Point (ZMP) pattern based on a capture point method, and a friction compensation method for the arms. Humanoid researchers have demonstrated that robots can perform a wide range of tasks including handling tools, climbing ladders, and patrolling rough terrain. However, when it comes to handling objects while walking, humanoids are relatively limited; it becomes more apparent when humanoids have to push a cart. Many challenges become evident under such circumstances; for example, the walking pattern will be severely affected by the external force opposed by the cart. Therefore, an appropriate walking pattern dynamic model and arm compliance are needed to mitigate external forces. This becomes crucial in order to ensure the robot's self-balance and minimize external disturbances.
ER  - 

TY  - CONF
TI  - Interconnection and Damping Assignment Passivity-Based Control for Gait Generation in Underactuated Compass-Like Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9802
EP  - 9808
AU  - P. Arpenti
AU  - F. Ruggiero
AU  - V. Lippiello
PY  - 2020
KW  - control system synthesis
KW  - damping
KW  - gait analysis
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - robot kinematics
KW  - robust control
KW  - interconnection and damping assignment passivity-based control
KW  - gait generation
KW  - compass-like biped robot
KW  - dynamic parameter
KW  - port-Hamiltonian framework
KW  - controller discretization
KW  - parametric uncertainties
KW  - Legged locomotion
KW  - Potential energy
KW  - Damping
KW  - Transmission line matrix methods
KW  - Mathematical model
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196598
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A compass-like biped robot can go down a gentle slope without the need of actuation through a proper choice of its dynamic parameter and starting from a suitable initial condition. Addition of control actions is requested to generate additional gaits and robustify the existing one. This paper designs an interconnection and damping assignment passivity-based control, rooted within the port-Hamiltonian framework, to generate further gaits with respect to state-of-the-art methodologies, enlarge the basin of attraction of existing gaits, and further robustify the system against controller discretization and parametric uncertainties. The performance of the proposed algorithm is validated through numerical simulations and comparison with existing passivity-based techniques.
ER  - 

TY  - CONF
TI  - Multi-Robot Path Deconfliction through Prioritization by Path Prospects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9809
EP  - 9815
AU  - W. Wu
AU  - S. Bhattacharya
AU  - A. Prorok
PY  - 2020
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - path prospects
KW  - prioritization rule
KW  - heterogeneous robot teams
KW  - multirobot path deconfliction
KW  - conflict-free path planning
KW  - mobile robots
KW  - conflict-free path plans
KW  - prioritization heuristics
KW  - Robot kinematics
KW  - Collision avoidance
KW  - Planning
KW  - Trajectory
KW  - Heuristic algorithms
KW  - Couplings
DO  - 10.1109/ICRA40945.2020.9196813
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work deals with the problem of planning conflict-free paths for mobile robots in cluttered environments. Since centralized, coupled planning algorithms are computationally intractable for large numbers of robots, we consider decoupled planning, in which robots plan their paths sequentially in order of priority. Choosing how to prioritize the robots is a key consideration. State-of-the-art prioritization heuristics, however, do not model the coupling between a robot's mobility and its environment. This is particularly relevant when prioritizing between robots with different degrees of mobility. In this paper, we propose a prioritization rule that can be computed online by each robot independently, and that provides consistent, conflict-free path plans. Our innovation is to formalize a robot's path prospects to reach its goal from its current location. To this end, we consider the number of homology classes of trajectories, which capture distinct prospects of paths for each robot. This measure is used as a prioritization rule, whenever any robots enter negotiation to deconflict path plans. We perform simulations with heterogeneous robot teams and compare our method to five benchmarks. Our method achieves the highest success rate, and strikes a good balance between makespan and flowtime objectives.
ER  - 

TY  - CONF
TI  - A Connectivity-Prediction Algorithm and its Application in Active Cooperative Localization for Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9824
EP  - 9830
AU  - L. Zhang
AU  - Z. Zhang
AU  - R. Siegwart
AU  - J. J. Chung
PY  - 2020
KW  - Markov processes
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - path planning
KW  - probability
KW  - infinite power series expansion theorem
KW  - finite-term approximation
KW  - computational feasibility
KW  - adverse impacts
KW  - higher order series terms
KW  - active CL
KW  - leader-follower architecture
KW  - Markov decision process
KW  - one-step planning horizon
KW  - optimal motion strategy
KW  - MDP model
KW  - connectivity-prediction algorithm
KW  - multirobot systems
KW  - future connectivity
KW  - mobile robots
KW  - range-limited communication
KW  - active motion planning
KW  - quadratic forms
KW  - random normal variables
KW  - Prediction algorithms
KW  - Robot sensing systems
KW  - Planning
KW  - Uncertainty
KW  - Computational modeling
KW  - Gaussian distribution
DO  - 10.1109/ICRA40945.2020.9197083
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method for predicting the probability of future connectivity between mobile robots with range-limited communication. In particular, we focus on its application to active motion planning for cooperative localization (CL). The probability of connection is modeled by the distribution of quadratic forms in random normal variables and is computed by the infinite power series expansion theorem. A finite-term approximation is made to realize the computational feasibility and three more modifications are designed to handle the adverse impacts introduced by the omission of the higher order series terms. On the basis of this algorithm, an active and CL problem with leader-follower architecture is then reformulated into a Markov Decision Process (MDP) with a one-step planning horizon, and the optimal motion strategy is generated by minimizing the expected cost of the MDP. Extensive simulations and comparisons are presented to show the effectiveness and efficiency of both the proposed prediction algorithm and the MDP model.
ER  - 

TY  - CONF
TI  - Behavior Mixing with Minimum Global and Subgroup Connectivity Maintenance for Large-Scale Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9845
EP  - 9851
AU  - W. Luo
AU  - S. Yi
AU  - K. Sycara
PY  - 2020
KW  - collision avoidance
KW  - distributed algorithms
KW  - mobile robots
KW  - multi-robot systems
KW  - trees (mathematics)
KW  - large-scale multirobot systems
KW  - robot team
KW  - connected communication graph
KW  - minimum inter-robot connectivity constraints
KW  - activated connectivity constraints
KW  - behavior mixing controllers
KW  - distributed minimum connectivity constraint spanning tree algorithm
KW  - provably minimum connectivity maintenance
KW  - subgroup connectivity maintenance
KW  - minimum global connectivity maintenance
KW  - collision avoidance
KW  - distributed MCCST algorithm
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Task analysis
KW  - Safety
KW  - Multi-robot systems
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197429
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many cases the multi-robot systems are desired to execute simultaneously multiple behaviors with different controllers, and sequences of behaviors in real time, which we call behavior mixing. Behavior mixing is accomplished when different subgroups of the overall robot team change their controllers to collectively achieve given tasks while maintaining connectivity within and across subgroups in one connected communication graph. In this paper, we present a provably minimum connectivity maintenance framework to ensure the subgroups and overall robot team stay connected at all times while providing the highest freedom for behavior mixing. In particular, we propose a real-time distributed Minimum Connectivity Constraint Spanning Tree (MCCST) algorithm to select the minimum inter-robot connectivity constraints preserving subgroup and global connectivity that are least likely to be violated by the original controllers. With the employed safety and connectivity barrier certificates for the activated connectivity constraints and collision avoidance, the behavior mixing controllers are thus minimally modified from the original controllers. We demonstrate the effectiveness and scalability of our approach via simulations of up to 100 robots with multiple behaviors.
ER  - 

TY  - CONF
TI  - Energy-Optimal Cooperative Manipulation via Provable Internal-Force Regulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9859
EP  - 9865
AU  - C. K. Verginis
AU  - D. V. Dimarogonas
PY  - 2020
KW  - cooperative systems
KW  - decentralised control
KW  - force control
KW  - manipulator dynamics
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - internal-force regulation
KW  - optimal cooperative robotic manipulation problem
KW  - energy resources
KW  - rigid cooperative manipulation systems
KW  - rigid grasping contacts
KW  - energy-optimal conditions
KW  - arising internal forces
KW  - inter-agent forces
KW  - closed form expression
KW  - standard inverse dynamics
KW  - force distribution
KW  - robotic agents
KW  - nonzero inter-agent internal force vector
KW  - internal force minimization
KW  - Grasping
KW  - Force
KW  - Robots
KW  - Dynamics
KW  - Acceleration
KW  - Jacobian matrices
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196696
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper considers the optimal cooperative robotic manipulation problem in terms of energy resources. In particular, we consider rigid cooperative manipulation systems, i.e., with rigid grasping contacts, and study energy-optimal conditions in the sense of minimization of the arising internal forces, which are inter-agent forces that do not contribute to object motion. Firstly, we use recent results to derive a closed form expression for the internal forces. Secondly, by using a standard inverse dynamics control protocol, we provide novel conditions on the force distribution to the robotic agents for provable internal force minimization. Moreover, we derive novel results on the provable achievement of a desired non-zero inter-agent internal force vector. Extensive simulation results in a realistic environment verify the theoretical analysis.
ER  - 

TY  - CONF
TI  - Robot Telekinesis: Application of a Unimanual and Bimanual Object Manipulation Technique to Robot Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9866
EP  - 9872
AU  - J. H. Lee
AU  - Y. Kim
AU  - S. -G. An
AU  - S. -H. Bae
PY  - 2020
KW  - end effectors
KW  - haptic interfaces
KW  - human-robot interaction
KW  - industrial manipulators
KW  - industrial robots
KW  - sensors
KW  - robot control
KW  - dangerous industrial robots
KW  - collaborative robots
KW  - teaching pendant
KW  - direct teaching
KW  - novel robot interaction technique
KW  - robot arm
KW  - unimanual hand gestures
KW  - bimanual hand gestures
KW  - robot telekinesis
KW  - bimanual object manipulation technique
KW  - unimanual object manipulation technique
KW  - production lines
KW  - Robot sensing systems
KW  - End effectors
KW  - Task analysis
KW  - Education
KW  - Service robots
KW  - Collaboration
DO  - 10.1109/ICRA40945.2020.9197517
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unlike large and dangerous industrial robots at production lines in factories that are strictly fenced off, collaborative robots are smaller and safer and can be installed adjacent to human workers and collaborate with them. However, controlling and teaching new moves to collaborative robots can be difficult and time-consuming when using existing methods, such as pressing buttons on a teaching pendant and physically grabbing and moving the robot via direct teaching. We present Robot Telekinesis, a novel robot interaction technique that lets the user remotely control the movement of the end effector of a robot arm with unimanual and bimanual hand gestures that closely resemble handling a physical object. Through formal evaluation, we show that using a teaching pendant is slow and confusing and that direct teaching is fast and intuitive but physically demanding. Robot Telekinesis is as fast and intuitive as direct teaching without the need for physical contact or physical effort.
ER  - 

TY  - CONF
TI  - A Set-Theoretic Approach to Multi-Task Execution and Prioritization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9873
EP  - 9879
AU  - G. Notomista
AU  - S. Mayya
AU  - M. Selvaggio
AU  - M. Santos
AU  - C. Secchi
PY  - 2020
KW  - optimisation
KW  - redundant manipulators
KW  - safety-critical software
KW  - set theory
KW  - task analysis
KW  - time-varying systems
KW  - constrained optimization problem
KW  - redundant robotic manipulator
KW  - set theoretic approach
KW  - multitask execution
KW  - safety critical tasks
KW  - robotic system
KW  - optimization based task execution
KW  - set based tasks
KW  - time varying priorities
KW  - multitask prioritization
KW  - control barrier functions
KW  - Task analysis
KW  - Robot kinematics
KW  - Jacobian matrices
KW  - Aerospace electronics
KW  - Manipulators
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196741
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Executing multiple tasks concurrently is important in many robotic applications. Moreover, the prioritization of tasks is essential in applications where safety-critical tasks need to precede application-related objectives, in order to protect both the robot from its surroundings and vice versa. Furthermore, the possibility of switching the priority of tasks during their execution gives the robotic system the flexibility of changing its objectives over time. In this paper, we present an optimization-based task execution and prioritization framework that lends itself to the case of time-varying priorities as well as variable number of tasks. We introduce the concept of extended set-based tasks, encode them using control barrier functions, and execute them by means of a constrained-optimization problem, which can be efficiently solved in an online fashion. Finally, we show the application of the proposed approach to the case of a redundant robotic manipulator.
ER  - 

TY  - CONF
TI  - Variable Impedance Control in Cartesian Latent Space while Avoiding Obstacles in Null Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9888
EP  - 9894
AU  - D. Parent
AU  - A. Colomé
AU  - C. Torras
PY  - 2020
KW  - collision avoidance
KW  - feedback
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulator dynamics
KW  - mobile robots
KW  - motion control
KW  - variable impedance control
KW  - cartesian latent space
KW  - null space
KW  - human-robot interaction
KW  - assistive robots
KW  - Cartesian impedance control
KW  - joint control
KW  - desired interaction
KW  - environmental feedback
KW  - robot arm
KW  - operational space
KW  - variable stiffness
KW  - precision requirements
KW  - dimensionality reduction
KW  - freedom relevant
KW  - redundant ones
KW  - task-redundant DoF
KW  - human head
KW  - Aerospace electronics
KW  - Task analysis
KW  - Trajectory
KW  - Jacobian matrices
KW  - Manipulators
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9197192
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Human-robot interaction is one of the keys of assistive robots. Robots are expected to be compliant with people but at the same time correctly perform the tasks. In such applications, Cartesian impedance control is preferred over joint control, as the desired interaction and environmental feedback can be described more naturally, and the force to be exerted by the robot can be readily adjusted.This paper addresses the problem of controlling a robot arm in the operational space with variable stiffness so as to continuously adapt the force exerted in each phase of motion according to the precision requirements. Moreover, performing dimensionality reduction we can separate the degrees of freedom (DoF) relevant for the task from the redundant ones. The stiffness of the former can be adjusted constantly to achieve the required accuracy, while task-redundant DoF can be used to achieve other goals such as avoiding obstacles by moving in the directions where accuracy is not critical. The designed method is tested teaching the robot to give water to drink to a model of human head. Our empirical results demonstrate that the robot can learn precision requirements from demonstration. Furthermore, dimensionality reduction is proved to be useful to avoid obstacles.
ER  - 

TY  - CONF
TI  - MagicHand: Context-Aware Dexterous Grasping Using an Anthropomorphic Robotic Hand
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9895
EP  - 9901
AU  - H. Li
AU  - J. Tan
AU  - H. He
PY  - 2020
KW  - dexterous manipulators
KW  - infrared spectra
KW  - object detection
KW  - robot vision
KW  - target object
KW  - grasping poses
KW  - grasp strategies
KW  - MagicHand system
KW  - context-aware dexterous grasping
KW  - robotic grasping
KW  - context-aware anthropomorphic robotic hand grasping system
KW  - NIR spectra
KW  - molecular level
KW  - RGB-D images
KW  - Grasping
KW  - Three-dimensional displays
KW  - Neurons
KW  - Cameras
KW  - Robot vision systems
KW  - Dexterous Grasping
KW  - Characteristics of Objects Recognition
KW  - NIR Spectrum
KW  - RGB-D Images
DO  - 10.1109/ICRA40945.2020.9196538
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Understanding of characteristics of objects such as fragility, rigidity, texture and dimensions facilitates and innovates robotic grasping. In this paper, we propose a context- aware anthropomorphic robotic hand (MagicHand) grasping system which is able to gather various information about its target object and generate grasping strategies based on the perceived information. In this work, NIR spectra of target objects are perceived to recognize materials on a molecular level and RGB-D images are collected to estimate dimensions of the objects. We selected six most used grasping poses and our system is able to decide the most suitable grasp strategies based on the characteristics of an object. Through multiple experiments, the performance of the MagicHand system is demonstrated.
ER  - 

TY  - CONF
TI  - Learning Pregrasp Manipulation of Objects from Ungraspable Poses
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9917
EP  - 9923
AU  - Z. Sun
AU  - K. Yuan
AU  - W. Hu
AU  - C. Yang
AU  - Z. Li
PY  - 2020
KW  - feedback
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - ungraspable poses
KW  - robotic grasping
KW  - model-free deep reinforcement learning
KW  - feedback control policies
KW  - visual information
KW  - robot arm
KW  - object-table clearance
KW  - pregrasp manipulation learning
KW  - human bimanual manipulation
KW  - Grasping
KW  - Robustness
KW  - Training
KW  - Grippers
KW  - Sensors
KW  - End effectors
DO  - 10.1109/ICRA40945.2020.9196982
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In robotic grasping, objects are often occluded in ungraspable configurations such that no feasible grasp pose can be found, e.g. large flat boxes on the table that can only be grasped once lifted. Inspired by human bimanual manipulation, e.g. one hand to lift up things and the other to grasp, we address this type of problems by introducing pregrasp manipulation - push and lift actions. We propose a model-free Deep Reinforcement Learning framework to train feedback control policies that utilize visual information and proprioceptive states of the robot to autonomously discover robust pregrasp manipulation. The robot arm learns to push the object first towards a support surface and then lift up one side of the object, creating an object-table clearance for possible grasping solutions. Furthermore, we show the robustness of the proposed learning framework in training pregrasp policies that can be directly transferred to a real robot. Lastly, we evaluate the effectiveness and generalization ability of the learned policy in real-world experiments, and demonstrate pregrasp manipulation of objects with various sizes, shapes, weights, and surface friction.
ER  - 

TY  - CONF
TI  - Picking Thin Objects by Tilt-and-Pivot Manipulation and Its Application to Bin Picking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9932
EP  - 9938
AU  - Z. Tong
AU  - T. He
AU  - C. H. Kim
AU  - Y. Hin Ng
AU  - Q. Xu
AU  - J. Seo
PY  - 2020
KW  - dexterous manipulators
KW  - end effectors
KW  - grippers
KW  - industrial manipulators
KW  - manipulator kinematics
KW  - robot vision
KW  - convex polygonal objects
KW  - bin picking scenarios
KW  - manipulation process
KW  - robotic dexterous in-hand manipulation
KW  - tilt-and-pivot manipulation
KW  - thin object picking
KW  - tilt-and-pivot kinematics
KW  - tilt-and-pivot planning
KW  - Grippers
KW  - Shape
KW  - Solids
KW  - Surface treatment
KW  - Hardware
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197493
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces the technique of tilt-and-pivot manipulation, a new method for picking thin, rigid objects lying on a flat surface through robotic dexterous in-hand manipulation. During the manipulation process, the gripper is controlled to reorient about the contact with the object such that its finger can get in the space between the object and the supporting surface, which is formed by tilting up the object, with no relative sliding motion at the contact. As a result, a pinch grasp can be obtained on the faces of the thin object with ease. We discuss issues regarding the kinematics and planning of tilt-and-pivot, effector shape design, and the overall practicality of the manipulation technique, which is general enough to be applicable to any rigid convex polygonal objects. We also present a set of experiments in a range of bin picking scenarios.
ER  - 

TY  - CONF
TI  - Attention-Guided Lightweight Network for Real-Time Segmentation of Robotic Surgical Instruments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9939
EP  - 9945
AU  - Z. -L. Ni
AU  - G. -B. Bian
AU  - Z. -G. Hou
AU  - X. -H. Zhou
AU  - X. -L. Xie
AU  - Z. Li
PY  - 2020
KW  - convolutional neural nets
KW  - edge detection
KW  - image capture
KW  - image coding
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - robot vision
KW  - surgery
KW  - robotic surgical instruments
KW  - robot-assisted surgery
KW  - deep learning models
KW  - attention-guided lightweight network
KW  - LWANet
KW  - lightweight network MobileNetV2
KW  - depthwise separable convolution
KW  - transposed convolution
KW  - surgical instrument
KW  - encoder-decoder architecture
KW  - attention fusion block
KW  - Instruments
KW  - Convolution
KW  - Semantics
KW  - Computational efficiency
KW  - Decoding
KW  - Surgery
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197425
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The real-time segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, it is still a challenging task to implement deep learning models to do real-time segmentation for surgical instruments due to their high computational costs and slow inference speed. In this paper, we propose an attention-guided lightweight network (LWANet), which can segment surgical instruments in real-time. LWANet adopts encoder-decoder architecture, where the encoder is the lightweight network MobileNetV2, and the decoder consists of depthwise separable convolution, attention fusion block, and transposed convolution. Depthwise separable convolution is used as the basic unit to construct the decoder, which can reduce the model size and computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to emphasize target regions, contributing to locating the surgical instrument. Transposed convolution is performed to upsample feature maps for acquiring refined edges. LWANet can segment surgical instruments in real-time while takes little computational costs. Based on 960x544 inputs, its inference speed can reach 39 fps with only 3.39 GFLOPs. Also, it has a small model size and the number of parameters is only 2.06 M. The proposed network is evaluated on two datasets. It achieves state-of-the- art performance 94.10% mean IOU on Cata7 and obtains a new record on EndoVis 2017 with a 4.10% increase on mean IOU.
ER  - 

TY  - CONF
TI  - Automated robotic breast ultrasound acquisition using ultrasound feedback
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9946
EP  - 9952
AU  - M. K. Welleweerd
AU  - A. G. de Groot
AU  - S. O. H. de Looijer
AU  - F. J. Siepel
AU  - S. Stramigioli
PY  - 2020
KW  - biological tissues
KW  - biomedical ultrasonics
KW  - feedback
KW  - image registration
KW  - medical image processing
KW  - medical robotics
KW  - phantoms
KW  - surgery
KW  - visual servoing
KW  - robotic 3D breast US acquisitions
KW  - US feedback
KW  - visual servoing algorithm
KW  - patient specific scans
KW  - tissue deformations
KW  - US probe
KW  - ultrasound feedback
KW  - automated robotic breast ultrasound acquisition
KW  - Breast
KW  - Probes
KW  - Trajectory
KW  - Safety
KW  - Robot kinematics
KW  - Skin
DO  - 10.1109/ICRA40945.2020.9196736
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Current challenges in automated robotic breast ultrasound (US) acquisitions include keeping acoustic coupling between the breast and the US probe, minimizing tissue deformations and safety. In this paper, we present how an autonomous 3D breast US acquisition can be performed utilizing a 7DOF robot equipped with a linear US transducer. Robotic 3D breast US acquisitions would increase the diagnostic value of the modality since they allow patient specific scans and have a high reproducibility, accuracy and efficiency. Additionally, 3D US acquisitions allow more flexibility in examining the breast and simplify registration with preoperative images like MRI. To overcome the current challenges, the robot follows a reference- based trajectory adjusted by a visual servoing algorithm. The reference trajectory is a patient specific trajectory coming from e.g. an MRI. The visual servoing algorithm commands in-plane rotations and corrects the probe contact based on confidence maps. A safety aware, intrinsically passive framework is utilised to actuate the robot. The approach is illustrated with experiments on a phantom, which show that the robot only needs minor pre-procedural information to consistently image the phantom while relying mainly on US feedback.
ER  - 

TY  - CONF
TI  - Robust and Accurate 3D Curve to Surface Registration with Tangent and Normal Vectors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9953
EP  - 9959
AU  - Z. Min
AU  - D. Zhu
AU  - M. Q. . -H. Meng
PY  - 2020
KW  - biomechanics
KW  - image reconstruction
KW  - image registration
KW  - medical image processing
KW  - surgery
KW  - image-guided surgery
KW  - pre-to-intraoperative registration
KW  - intra-operative 3D data
KW  - tangent vectors
KW  - sparse intraoperative data points
KW  - pre-operative model points
KW  - probabilistic distribution
KW  - multidimensional point
KW  - maximum likelihood problem
KW  - rigid registration
KW  - intraoperative point
KW  - mean target registration error value
KW  - size 0.6795 mm
KW  - Three-dimensional displays
KW  - Surgery
KW  - Robustness
KW  - Probes
KW  - Probabilistic logic
KW  - Robots
KW  - Biomedical imaging
DO  - 10.1109/ICRA40945.2020.9196923
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a robust and accurate approach for the rigid registration of pre-operative and intraoperative point sets in image-guided surgery (IGS). Three challenges are identified in the pre-to-intraoperative registration: the intra-operative 3D data (usually forms a 3D curve in space) (1) is often contaminated with noise and outliers; (2) usually only covers a partial region of the whole pre-operative model; (3) is usually sparse. To tackle those challenges, we utilize the tangent vectors extracted from the sparse intraoperative data points and the normal vectors extracted from the pre-operative model points. Our first contribution is to formulate a novel probabilistic distribution of the error between a pair of corresponding tangent and normal vectors. The second contribution is, based on the novel distribution, we formulate the registration of two multi-dimensional (6D) point sets as a maximum likelihood (ML) problem and solve it under the expectation maximization (EM) framework. Our last contribution is, in order to facilitate the computation process, the derivatives of the objective function with respect to desired parameters are presented. We conduct extensive experiments to demonstrate that our approach outperforms the state-of-the-art methods. Importantly, in the context of anteriro cruciate ligament (ACL) reconstruction, our method can achieve as low as 0.6795 mm mean target registration error (TRE) value with considerable noises and very limited overlapping ratios.
ER  - 

TY  - CONF
TI  - Single-Shot Pose Estimation of Surgical Robot Instruments’ Shafts from Monocular Endoscopic Images
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9960
EP  - 9966
AU  - M. Yoshimura
AU  - M. M. Marinho
AU  - K. Harada
AU  - M. Mitsuishi
PY  - 2020
KW  - collision avoidance
KW  - endoscopes
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - neural net architecture
KW  - pose estimation
KW  - robot vision
KW  - surgery
KW  - single-shot pose estimation
KW  - monocular endoscopic images
KW  - minimally invasive surgery
KW  - collision-avoidance algorithm
KW  - online estimation
KW  - monocular endoscope
KW  - art vision-based marker-less
KW  - position estimation
KW  - surgical robot instrument shafts
KW  - annotated training dataset
KW  - improved pose-estimation deep-learning architecture
KW  - Instruments
KW  - Robots
KW  - Pose estimation
KW  - Surgery
KW  - Shafts
KW  - Endoscopes
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196779
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgical robots are used to perform minimally invasive surgery and alleviate much of the burden imposed on surgeons. Our group has developed a surgical robot to aid in the removal of tumors at the base of the skull via access through the nostrils. To avoid injuring the patients, a collision-avoidance algorithm that depends on having an accurate model for the poses of the instruments' shafts is used. Given that the model's parameters can change over time owing to interactions between instruments and other disturbances, the online estimation of the poses of the instrument's shaft is essential. In this work, we propose a new method to estimate the pose of the surgical instruments' shafts using a monocular endoscope. Our method is based on the use of an automatically annotated training dataset and an improved pose-estimation deep-learning architecture. In preliminary experiments, we show that our method can surpass state of the art vision-based marker-less pose estimation techniques (providing an error decrease of 55% in position estimation, 64% in pitch, and 69% in yaw) by using artificial images.
ER  - 

TY  - CONF
TI  - End-to-End Real-time Catheter Segmentation with Optical Flow-Guided Warping during Endovascular Intervention
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9967
EP  - 9973
AU  - A. Nguyen
AU  - D. Kundrat
AU  - G. Dagnino
AU  - W. Chi
AU  - M. E. M. K. Abdelaziz
AU  - Y. Guo
AU  - Y. Ma
AU  - T. M. Y. Kwok
AU  - C. Riga
AU  - G. -Z. Yang
PY  - 2020
KW  - catheters
KW  - feature extraction
KW  - image segmentation
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - neural nets
KW  - surgery
KW  - deep learning framework
KW  - segmentation network
KW  - encoder-decoder architecture
KW  - flow network
KW  - optical flow information
KW  - frame-to-frame temporal continuity
KW  - catheter segmentation
KW  - robot-assisted endovascular intervention
KW  - flow-guided warping function
KW  - optical flow-guided warping
KW  - Catheters
KW  - Image segmentation
KW  - X-ray imaging
KW  - Real-time systems
KW  - Machine learning
KW  - Motion segmentation
DO  - 10.1109/ICRA40945.2020.9197307
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate real-time catheter segmentation is an important pre-requisite for robot-assisted endovascular intervention. Most of the existing learning-based methods for catheter segmentation and tracking are only trained on smallscale datasets or synthetic data due to the difficulties of ground-truth annotation. Furthermore, the temporal continuity in intraoperative imaging sequences is not fully utilised. In this paper, we present FW-Net, an end-to-end and real-time deep learning framework for endovascular intervention. The proposed FW-Net has three modules: a segmentation network with encoder-decoder architecture, a flow network to extract optical flow information, and a novel flow-guided warping function to learn the frame-to-frame temporal continuity. We show that by effectively learning temporal continuity, the network can successfully segment and track the catheters in real-time sequences using only raw ground-truth for training. Detailed validation results confirm that our FW-Net outperforms stateof-the-art techniques while achieving real-time performance.
ER  - 

TY  - CONF
TI  - Pathological Airway Segmentation with Cascaded Neural Networks for Bronchoscopic Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9974
EP  - 9980
AU  - H. Zhang
AU  - M. Shen
AU  - P. L. Shah
AU  - G. -Z. Yang
PY  - 2020
KW  - computerised tomography
KW  - image segmentation
KW  - lung
KW  - medical image processing
KW  - neural nets
KW  - cascaded 2D-3D model
KW  - pathological CT scans
KW  - 3D adversarial training model
KW  - novel 2D neural network
KW  - airway tree
KW  - pathological abnormalities
KW  - preoperative chest CT scans
KW  - patient-specific airway maps
KW  - peripheral airways
KW  - enhanced visualisation
KW  - 3D airway maps
KW  - robotic bronchoscopic intervention
KW  - bronchoscopic navigation
KW  - cascaded neural networks
KW  - pathological airway segmentation
KW  - Three-dimensional displays
KW  - Atmospheric modeling
KW  - Training
KW  - Two dimensional displays
KW  - Computed tomography
KW  - Image segmentation
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9196756
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic bronchoscopic intervention requires detailed 3D airway maps for both localisation and enhanced visualisation, especially at peripheral airways. Patient-specific airway maps can be generated from preoperative chest CT scans. Due to pathological abnormalities and anatomical variations, automatically delineating the airway tree with distal branches is a challenging task. In the paper, we propose a cascaded 2D+3D model that has been tailored for airway segmentation from pathological CT scans. A novel 2D neural network is developed to generate the initial predictions where the peripheral airways are refined by a 3D adversarial training model. A sampling strategy based on a sequence of morphological operations is employed for the concatenation of the 2D and 3D models. The method has been validated on 20 pathological CT scans with results demonstrating improved segmentation accuracy and consistency, especially in peripheral airways.
ER  - 

TY  - CONF
TI  - Design of 3D-printed assembly mechanisms based on special wooden joinery techniques and its application to a robotic hand
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9981
EP  - 9987
AU  - A. Katsumaru
AU  - R. Ozawa
PY  - 2020
KW  - assembling
KW  - design engineering
KW  - plastics
KW  - printers
KW  - robots
KW  - three-dimensional printing
KW  - part-joining quality
KW  - design method
KW  - robotic hand
KW  - 3D-printed assembly mechanisms
KW  - robotic systems
KW  - plastic materials
KW  - Japanese wooden joinery techniques
KW  - assembling 3D-printed parts
KW  - Gears
KW  - Robots
KW  - Pins
KW  - Shape
KW  - Three-dimensional displays
KW  - Thumb
KW  - Printers
DO  - 10.1109/ICRA40945.2020.9197475
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recently, it has become possible to easily design and fabricate robotic systems in the laboratory and at home due to the recent development of 3D printer technology. On the other hand, the strength of the plastic materials used in reasonably priced 3D printers and the accuracy of the printed parts are generally low. These problems affect the part-joining quality. Therefore, this paper describes a design method inspired by ancient Japanese wooden joinery techniques for assembling 3D-printed parts and presents the design of a robotic hand as its application. The joinery techniques use special shapes to assemble components and allow us to assemble the robotic hand without glue, screws or nails and to easily disassemble it.
ER  - 

TY  - CONF
TI  - Parallel gripper with displacement-magnification mechanism and extendable finger mechanism
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9988
EP  - 9993
AU  - J. Tanaka
AU  - A. Sugahara
PY  - 2020
KW  - force control
KW  - grippers
KW  - impact (mechanical)
KW  - gripping force
KW  - product height
KW  - impact force
KW  - stacked rack-and-pinion system
KW  - mechanism verification
KW  - parallel gripper
KW  - gripper displacement-magnification mechanism
KW  - extendable finger mechanism
KW  - size 95.0 mm
KW  - size 110.0 mm
KW  - size 214.0 mm
KW  - mass 1.36 kg
KW  - size 60.0 mm
KW  - velocity 100.0 mm/s
KW  - Grippers
KW  - Gears
KW  - Nails
KW  - Thumb
KW  - Force
KW  - Piezoelectric transducers
DO  - 10.1109/ICRA40945.2020.9196746
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a gripper displacement-magnification mechanism and an extendable finger mechanism, both of which can be attached to a commercially available parallel gripper. We then verify the operation of the mechanism in order to expand applications of the parallel gripper. The displacement-magnification mechanism has a stacked rack-and-pinion system that doubles displacement. The extendable finger mechanism has two nails that extend and contract, reducing impact force and detecting changes in product height from expansion and contraction amounts. The parallel gripper has a width of 95 mm, a depth of 110 mm, and a height of 214 mm and weighs 1.36 kg. It has an open/close stroke of 60 mm, a gripping force of 7.4 N, and an opening/closing speed of 100 mm/s or more. Further, it was confirmed that the ends and inclinations of products can be reliably detected using the extending/contracting nail. The mechanism verification confirmed that our parallel gripper achieved the desired performance and is therefore useful.
ER  - 

TY  - CONF
TI  - A Shape Memory Polymer Adhesive Gripper For Pick-and-Place Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10010
EP  - 10016
AU  - C. Son
AU  - S. Kim
PY  - 2020
KW  - adhesion
KW  - adhesives
KW  - control system synthesis
KW  - grippers
KW  - manipulators
KW  - polymers
KW  - shape memory effects
KW  - shape memory polymer adhesive gripper
KW  - smart adhesive applications
KW  - pick-and-place applications
KW  - reversible dry adhesion
KW  - gecko grippers
KW  - high adhesion strength
KW  - SMP adhesive mechanics
KW  - reversible dry adhesive properties
KW  - single surface contact grippers
KW  - SMP adhesive gripper
KW  - Switched mode power supplies
KW  - Grippers
KW  - Adhesives
KW  - Heating systems
KW  - Shape
KW  - Rough surfaces
KW  - Surface roughness
DO  - 10.1109/ICRA40945.2020.9197511
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Over the past few years, shape memory polymer (SMP) has been extensively studied in terms of its remarkable reversible dry adhesive properties and related smart adhesive applications. However, its exceptional properties have not been exploited for further opportunities such as pick-and-place applications, which would otherwise advance the robotic manipulation. This work explores the use of an SMP to design an adhesive gripper that picks and places a target solid object employing the reversible dry adhesion of an SMP. Compared with other single surface contact grippers including vacuum, electromagnetic, electroadhesion, and gecko grippers, the SMP adhesive gripper interacts with not only flat and smooth dry surfaces but also moderately rough and even wet surfaces for pick-and-place with high adhesion strength (> 2 atmospheres). In this work, associated physical mechanisms, SMP adhesive mechanics, and thermal conditions are studied. In particular, the numerical and experimental study elucidates that the optimal compositional and topological SMP design may substantially enhance its adhesion strength and reversibility, which leads to a strong grip force simultaneously with a minimized releasing force. Finally, the versatility and utility of the SMP adhesive gripper are highlighted through diverse pick-and-place demonstrations.
ER  - 

TY  - CONF
TI  - Multi-person Pose Tracking using Sequential Monte Carlo with Probabilistic Neural Pose Predictor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10024
EP  - 10030
AU  - M. Okada
AU  - S. Takenaka
AU  - T. Taniguchi
PY  - 2020
KW  - image matching
KW  - Monte Carlo methods
KW  - neural nets
KW  - pose estimation
KW  - probability
KW  - multiperson pose tracking
KW  - sequential Monte Carlo
KW  - probabilistic neural pose predictor
KW  - uncertainty-aware modeling
KW  - critical tracking errors
KW  - tracking scheme
KW  - multiple predictions
KW  - prediction errors
KW  - proposal distribution
KW  - epistemic uncertainty
KW  - heteroscedastic aleatoric uncertainty
KW  - neural modeling
KW  - MOTA score
KW  - PoseTrack2018 validation dataset
KW  - pose matching
KW  - time-sequence information
KW  - Uncertainty
KW  - Probabilistic logic
KW  - Adaptive optics
KW  - Optical imaging
KW  - Pose estimation
KW  - Optical sensors
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9196509
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It is an effective strategy for the multi-person pose tracking task in videos to employ prediction and pose matching in a frame-by-frame manner. For this type of approach, uncertainty-aware modeling is essential because precise prediction is impossible. However, previous studies have relied on only a single prediction without incorporating uncertainty, which can cause critical tracking errors if the prediction is unreliable. This paper proposes an extension to this approach with Sequential Monte Carlo (SMC). This naturally reformulates the tracking scheme to handle multiple predictions (or hypotheses) of poses, thereby mitigating the negative effect of prediction errors. An important component of SMC, i.e., a proposal distribution, is designed as a probabilistic neural pose predictor, which can propose diverse and plausible hypotheses by incorporating epistemic uncertainty and heteroscedastic aleatoric uncertainty. In addition, a recurrent architecture is introduced to our neural modeling to utilize time-sequence information of poses to manage difficult situations, such as the frequent disappearance and reappearances of poses. Compared to existing baselines, the proposed method achieves a state-of-the-art MOTA score on the PoseTrack2018 validation dataset by reducing approximately 50% of tracking errors from a state-of-the art baseline method.
ER  - 

TY  - CONF
TI  - 4D Generic Video Object Proposals
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10031
EP  - 10037
AU  - A. Ošep
AU  - P. Voigtlaender
AU  - M. Weber
AU  - J. Luiten
AU  - B. Leibe
PY  - 2020
KW  - feature extraction
KW  - image motion analysis
KW  - image segmentation
KW  - object detection
KW  - stereo image processing
KW  - video signal processing
KW  - stereo video
KW  - 4D-GVT
KW  - data-driven object instance segmentation
KW  - neural networks
KW  - spatio-temporal object proposals
KW  - 4D generic video tubes
KW  - object categories
KW  - 4D generic video object proposals
KW  - Proposals
KW  - Electron tubes
KW  - Three-dimensional displays
KW  - Image segmentation
KW  - Video sequences
KW  - Motion segmentation
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196949
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many high-level video understanding methods require input in the form of object proposals. Currently, such proposals are predominantly generated with the help of neural networks that were trained for detecting and segmenting a set of known object classes, which limits their applicability to cases where all objects of interest are represented in the training set. We propose an approach that can reliably extract spatio-temporal object proposals for both known and unknown object categories from stereo video. Our 4D Generic Video Tubes (4D-GVT) method combines motion cues, stereo data, and data-driven object instance segmentation in a probabilistic framework to compute a compact set of video-object proposals that precisely localizes object candidates and their contours in 3D space and time.
ER  - 

TY  - CONF
TI  - Simultaneous Tracking and Elasticity Parameter Estimation of Deformable Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10038
EP  - 10044
AU  - A. Sengupta
AU  - R. Lagneau
AU  - A. Krupa
AU  - E. Marchand
AU  - M. Marchal
PY  - 2020
KW  - deformation
KW  - finite element analysis
KW  - image colour analysis
KW  - manipulators
KW  - parameter estimation
KW  - robot vision
KW  - visual information
KW  - simulated object
KW  - elasticity parameter estimation
KW  - tracked object
KW  - soft objects
KW  - deformable object
KW  - simultaneous tracking
KW  - interactive finite element method simulations
KW  - RGB-D sensor
KW  - robotic manipulation
KW  - Strain
KW  - Elasticity
KW  - Estimation
KW  - Deformable models
KW  - Force measurement
KW  - Force
KW  - Force sensors
DO  - 10.1109/ICRA40945.2020.9196770
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a novel method to simultaneously track the deformation of soft objects and estimate their elasticity parameters. The tracking of the deformable object is performed by combining the visual information captured by a RGB-D sensor with interactive Finite Element Method simulations of the object. The visual information is more particularly used to distort the simulated object. In parallel, the elasticity parameter estimation minimizes the error between the tracked object and a simulated object deformed by the forces that are measured using a force sensor. Once the elasticity parameters are estimated, our tracking algorithm can be used to estimate the deformation forces applied to an object without the use of a force sensor. We validated our method on several soft objects with different shape complexities. Our evaluations show the ability of our method to estimate the elasticity parameters as well as its use to estimate the forces applied to a deformable object without any force sensor. These results open novel perspectives to better track and control deformable objects during robotic manipulations.
ER  - 

TY  - CONF
TI  - AVOT: Audio-Visual Object Tracking of Multiple Objects for Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10045
EP  - 10051
AU  - J. Wilson
AU  - M. C. Lin
PY  - 2020
KW  - audio signal processing
KW  - audio-visual systems
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - tracking
KW  - multiple objects
KW  - visually based trackers
KW  - object collisions
KW  - audio-visual object tracking neural network
KW  - tracking error
KW  - AVOT end
KW  - audio-visual inputs
KW  - visually based object detection
KW  - tracking methods
KW  - OpenCV object tracking implementations
KW  - deep learning method
KW  - audio-visual dataset
KW  - single-modality deep learning methods
KW  - audio onset
KW  - multimodal object tracking
KW  - state-of-the-art object tracking
KW  - Object tracking
KW  - Object detection
KW  - Visualization
KW  - Neural networks
KW  - Machine learning
KW  - Feature extraction
KW  - Streaming media
DO  - 10.1109/ICRA40945.2020.9197528
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Existing state-of-the-art object tracking can run into challenges when objects collide, occlude, or come close to one another. These visually based trackers may also fail to differentiate between objects with the same appearance but different materials. Existing methods may stop tracking or incorrectly start tracking another object. These failures are uneasy for trackers to recover from since they often use results from previous frames. By using audio of the impact sounds from object collisions, rolling, etc., our audio-visual object tracking (AVOT) neural network can reduce tracking error and drift. We train AVOT end to end and use audio-visual inputs over all frames. Our audio-based technique may be used in conjunction with other neural networks to augment visually based object detection and tracking methods. We evaluate its runtime frames-per-second (FPS) performance and intersection over union (IoU) performance against OpenCV object tracking implementations and a deep learning method. Our experiments, using the synthetic Sound-20K audio-visual dataset, demonstrate that AVOT outperforms single-modality deep learning methods, when there is audio from object collisions. A proposed scheduler network to switch between AVOT and other methods based on audio onset maximizes accuracy and performance over all frames in multimodal object tracking.
ER  - 

TY  - CONF
TI  - Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10052
EP  - 10058
AU  - G. Chen
AU  - S. Shen
AU  - L. Wen
AU  - S. Luo
AU  - L. Bo
PY  - 2020
KW  - agricultural robots
KW  - cameras
KW  - convolutional neural nets
KW  - distributed processing
KW  - farming
KW  - image filtering
KW  - image matching
KW  - image sensors
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - efficient pig counting
KW  - large-scale pig farming
KW  - automated pig counting method
KW  - pig movements
KW  - pig grouping houses
KW  - real-time automated pig counting system
KW  - pig detection algorithm
KW  - deformable pig shapes
KW  - pig body part
KW  - keypoints tracking
KW  - spatial-aware temporal response filtering
KW  - pig occlusion
KW  - pig overlapping
KW  - monocular fisheye camera
KW  - inspection robot
KW  - deep convolution neural network
KW  - keypoints association
KW  - efficient on-line tracking method
KW  - tracking failures
KW  - edge computing device
KW  - Cameras
KW  - Tracking
KW  - Robot vision systems
KW  - Inspection
KW  - Skeleton
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197211
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Pig counting is a crucial task for large-scale pig farming. Pigs are usually visually counted by human. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. The existing works only focused on pig counting using single image, and its level of accuracy faced challenges due to pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the needs of pig counting for large pig grouping houses. Towards addressing these challenges, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it achieved performance superior to human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformable pig shapes. This detection included a deep convolution neural network (CNN) for pig body part keypoints detection and the keypoints association method to identify individual pigs. It then employed an efficient on-line tracking method to associate pigs across image frames. Finally, pig counts were estimated by a novel spatial-aware temporal response filtering (STRF) method to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.
ER  - 

TY  - CONF
TI  - 6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10059
EP  - 10066
AU  - C. Wang
AU  - R. Martín-Martín
AU  - D. Xu
AU  - J. Lv
AU  - C. Lu
AU  - L. Fei-Fei
AU  - S. Savarese
AU  - Y. Zhu
PY  - 2020
KW  - closed loop systems
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - https://sites.google.com/view/6packtracking
KW  - physical robot
KW  - interframe motion
KW  - 3D keypoints
KW  - real time novel object instances
KW  - RGB-D data
KW  - NOCS category-level 6D pose estimation benchmark
KW  - keypoint matching
KW  - object instance
KW  - known object categories
KW  - deep learning approach
KW  - anchor-based keypoints
KW  - category-level 6D pose tracker
KW  - 6-PACK
KW  - simple vision-based closed-loop manipulation tasks
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Robustness
KW  - Robots
KW  - Real-time systems
KW  - Tracking
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196679
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
ER  - 

TY  - CONF
TI  - Designing Ferromagnetic Soft Robots (FerroSoRo) with Level-Set-Based Multiphysics Topology Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10067
EP  - 10074
AU  - J. Tian
AU  - X. Zhao
AU  - X. D. Gu
AU  - S. Chen
PY  - 2020
KW  - actuators
KW  - deformation
KW  - design engineering
KW  - elastomers
KW  - grippers
KW  - optimisation
KW  - sensitivity analysis
KW  - topology
KW  - shape sensitivity analysis
KW  - gripper
KW  - flytrap structure
KW  - material layout
KW  - innovative structures
KW  - bionic medical devices
KW  - compliant actuators
KW  - level-set-based multiphysics topology optimization method
KW  - adjoint variable method
KW  - material time derivative
KW  - sub-objective function
KW  - architect ferromagnetic soft active structures
KW  - design domain
KW  - structural topology optimization
KW  - ferromagnetic soft elastomers
KW  - external magnetic field
KW  - shift morphology
KW  - soft elastomer matrix
KW  - ferromagnetic particles
KW  - flexible electronics
KW  - soft machines
KW  - external environmental stimulus
KW  - change configurations
KW  - flexible locomotion
KW  - soft active materials
KW  - FerroSoRo
KW  - designing ferromagnetic soft robots
KW  - Soft magnetic materials
KW  - Optimization
KW  - Topology
KW  - Magnetic domains
KW  - Magnetoacoustic effects
KW  - Level set
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197457
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft active materials can generate flexible locomotion and change configurations through large deformations when subjected to an external environmental stimulus. They can be engineered to design 'soft machines' such as soft robots, compliant actuators, flexible electronics, or bionic medical devices. By embedding ferromagnetic particles into soft elastomer matrix, the ferromagnetic soft matter can generate flexible movement and shift morphology in response to the external magnetic field. By taking advantage of this physical property, soft active structures undergoing desired motions can be generated by tailoring the layouts of the ferromagnetic soft elastomers. Structural topology optimization has emerged as an attractive tool to achieve innovative structures by optimizing the material layout within a design domain, and it can be utilized to architect ferromagnetic soft active structures. In this paper, the level-set-based topology optimization method is employed to design ferromagnetic soft robots (FerroSoRo). The objective function comprises a sub-objective function for the kinematics requirement and a sub-objective function for minimum compliance. Shape sensitivity analysis is derived using the material time derivative and adjoint variable method. Three examples, including a gripper, an actuator, and a flytrap structure, are studied to demonstrate the effectiveness of the proposed framework.
ER  - 

TY  - CONF
TI  - Exoskeleton-covered soft finger with vision-based proprioception and tactile sensing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10075
EP  - 10081
AU  - Y. She
AU  - S. Q. Liu
AU  - P. Yu
AU  - E. Adelson
PY  - 2020
KW  - actuators
KW  - convolutional neural nets
KW  - dexterous manipulators
KW  - grippers
KW  - intelligent sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - tactile sensors
KW  - high-resolution proprioceptive sensing
KW  - rich tactile sensing
KW  - highly underactuated exoskeleton
KW  - robotic gripper
KW  - tactile information
KW  - proprioception CNN
KW  - human finger proprioception
KW  - proprioceptive state
KW  - peripheral environment
KW  - vision-based proprioception
KW  - rigid-body robots
KW  - soft robots
KW  - accurate proprioception
KW  - elasticity
KW  - tactile sensor
KW  - previous GelSight sensing techniques
KW  - exoskeleton-covered soft finger
KW  - size 0.77 mm
KW  - Soft robotics
KW  - Cameras
KW  - Robot vision systems
KW  - Ink
DO  - 10.1109/ICRA40945.2020.9197369
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots offer significant advantages in adaptability, safety, and dexterity compared to conventional rigid-body robots. However, it is challenging to equip soft robots with accurate proprioception and tactile sensing due to their high flexibility and elasticity. In this work, we describe the development of a vision-based proprioceptive and tactile sensor for soft robots called GelFlex, which is inspired by previous GelSight sensing techniques. More specifically, we develop a novel exoskeleton-covered soft finger with embedded cameras and deep learning methods that enable high-resolution proprioceptive sensing and rich tactile sensing. To do so, we design features along the axial direction of the finger, which enable high-resolution proprioceptive sensing, and incorporate a reflective ink coating on the surface of the finger to enable rich tactile sensing. We design a highly underactuated exoskeleton with a tendon-driven mechanism to actuate the finger. Finally, we assemble 2 of the fingers together to form a robotic gripper and successfully perform a bar stock classification task, which requires both shape and tactile information. We train neural networks for proprioception and shape (box versus cylinder) classification using data from the embedded sensors. The proprioception CNN had over 99% accuracy on our testing set (all six joint angles were within 1° of error) and had an average accumulative distance error of 0.77 mm during live testing, which is better than human finger proprioception. These proposed techniques offer soft robots the high-level ability to simultaneously perceive their proprioceptive state and peripheral environment, providing potential solutions for soft robots to solve everyday manipulation tasks. We believe the methods developed in this work can be widely applied to different designs and applications.
ER  - 

TY  - CONF
TI  - Tuning the Energy Landscape of Soft Robots for Fast and Strong Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10082
EP  - 10088
AU  - J. Sun
AU  - B. Tighe
AU  - J. Zhao
PY  - 2020
KW  - actuators
KW  - deformation
KW  - design engineering
KW  - flexible manipulators
KW  - grippers
KW  - mobile robots
KW  - pneumatic actuators
KW  - energy landscape
KW  - soft body structures
KW  - fast motion
KW  - strong motion
KW  - soft module
KW  - soft bistable module
KW  - fast robots
KW  - strong soft robots
KW  - soft gripper
KW  - soft jumping robot
KW  - soft actuator
KW  - twisted-and-coiled actuator
KW  - Soft robotics
KW  - Springs
KW  - Potential energy
KW  - Shape
KW  - Actuators
KW  - Plastics
DO  - 10.1109/ICRA40945.2020.9196737
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots demonstrate great potential compared with traditional rigid robots owing to their inherently soft body structures. Although researchers have made tremendous progress in recent years, existing soft robots are in general plagued by a main issue: slow speeds and small forces. In this work, we aim to address this issue by actively designing the energy landscape of the soft body: the total strain energy with respect to the robot's deformation. With such a strategy, a soft robot's dynamics can be tuned to have fast and strong motion. We introduce the general design principle using a soft module with two stable states that can rapidly switch from one state to the other under external forces. We characterize the required triggering (switching) force with respect to design parameters (e.g., the initial shape of the module). We then apply the soft bistable module to develop fast and strong soft robots, whose triggering forces are generated by a soft actuator - twisted-and-coiled actuator (TCA). We demonstrate a soft gripper that can hold weights more than 8 times its own weight, and a soft jumping robot that can jump more than 5 times its body height. We envision our strategies will overcome the weakness of soft robots to unleash their potential for diverse applications.
ER  - 

TY  - CONF
TI  - REBOund: Untethered Origami Jumping Robot with Controllable Jump Height
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10089
EP  - 10095
AU  - J. Carlson
AU  - J. Friedman
AU  - C. Kim
AU  - C. Sung
PY  - 2020
KW  - deformation
KW  - finite element analysis
KW  - legged locomotion
KW  - pneumatic actuators
KW  - shear modulus
KW  - springs (mechanical)
KW  - custom release mechanisms
KW  - quick compression
KW  - fold pattern
KW  - controllable jump height
KW  - jumping maneuvers
KW  - control strategies
KW  - robot body
KW  - model fold patterns
KW  - potential energy storage
KW  - parametric origami tessellation
KW  - face deformations
KW  - nonlinear spring
KW  - pseudorigid-body model
KW  - mechanical testing system
KW  - stored potential energy
KW  - reconfigurable expanding bistable origami pattern
KW  - untethered origami jumping robot
KW  - REBOund robot
DO  - 10.1109/ICRA40945.2020.9196534
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Origami robots are well-suited for jumping maneuvers because of their light weight and ability to incorporate actuation and control strategies directly into the robot body. However, existing origami robots often model fold patterns as rigidly foldable and fail to take advantage of deformation in an origami sheet for potential energy storage. In this paper, we consider a parametric origami tessellation, the Reconfigurable Expanding Bistable Origami (REBO) pattern, which leverages face deformations to act as a nonlinear spring. We present a pseudo-rigid-body model for the REBO for computing its energy stored when compressed to a given displacement and compare that model to experimental measurements taken on a mechanical testing system. This stored potential energy, when released quickly, can cause the pattern to jump. Using our model and experimental data, we design and fabricate a jumping robot, REBOund, that uses the spring-like REBO pattern as its body. Four lightweight servo motors with custom release mechanisms allow for quick compression and release of the origami pattern, allowing the fold pattern to jump over its own height even when carrying 5 times its own weight in electronics and power. We further demonstrate that small geometric changes to the pattern allow us to change the jump height without changing the actuation or control mechanism.
ER  - 

TY  - CONF
TI  - Motion Intensity Extraction Scheme for Simultaneous Recognition of Wrist/Hand Motions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10112
EP  - 10117
AU  - M. Kim
AU  - W. K. Chung
AU  - K. Kim
PY  - 2020
KW  - electromyography
KW  - gesture recognition
KW  - medical signal processing
KW  - muscle
KW  - pattern classification
KW  - sEMG signals
KW  - motion intensity feature
KW  - grasping motions
KW  - motion intensity extraction scheme
KW  - surface electromyography
KW  - muscular information representing gestures
KW  - sEMG-based motion recognition methods
KW  - Muscles
KW  - Crosstalk
KW  - Feature extraction
KW  - Wrist
KW  - Pattern recognition
KW  - Microsoft Windows
KW  - Electrodes
DO  - 10.1109/ICRA40945.2020.9197467
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surface electromyography contains muscular information representing gestures and corresponding forces. However, conventional sEMG-based motion recognition methods, such as pattern classification and regression, have intrinsic limitations due to the complex characteristics of sEMG signals. In this paper, motion intensity, a highly selective sEMG feature proportional to the level of muscle contraction, is proposed. The motion intensity feature allows proportional and simultaneous recognition of multiple degrees of freedom. The proposed method was demonstrated in terms of simultaneous recognition of wrist/hand motions. The result shows that the proposed method can successfully decompose sEMG signals into highly selective signals to target motions. In future works, the proposed method will be adapted for more subjects and to sEMG applications for practical evaluation considering various grasping motions.
ER  - 

TY  - CONF
TI  - Simultaneous Online Motion Discrimination and Evaluation of Whole-body Exercise by Synergy Probes for Home Rehabilitation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10118
EP  - 10124
AU  - F. M. Ramos
AU  - M. Hayashibe
PY  - 2020
KW  - biomechanics
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - patient rehabilitation
KW  - synergy probe
KW  - whole-body exercise
KW  - simultaneous online motion discrimination
KW  - home rehabilitation sessions
KW  - reconstructed movement
KW  - online data
KW  - training data
KW  - Task analysis
KW  - Probes
KW  - Training data
KW  - Hidden Markov models
KW  - Torso
KW  - Real-time systems
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197232
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The development of algorithms for motion discrimination in home rehabilitation sessions poses numerous challenges. Recent studies have used the concept of synergies to discriminate a set of movements. However, the discrimination depends on the correlation of the reconstructed movement with the online data, and the training data requires well-defined movements. In this paper, we introduced the concept of a synergy probe, which makes a direct comparison between synergies and online data. The system represents synergies and movements in the same space and monitors their behavior. The results indicated that conventional methods are influenced by the segmentation of training data, and even though the reconstructed movement is similar to the ground-truth, it does not provide sufficient information to evaluate the data in real time. The synergy probes were used to discriminate and evaluate the performance of natural whole-body exercises without segmentation or previous determination of movements. An analysis of the results also demonstrated the possibility to identify the strategies used by the subjects for movement. Such information aids in gaining a better insight and can prove beneficial in home rehabilitation.
ER  - 

TY  - CONF
TI  - Validation of a Forward Kinematics Based Controller for a mobile Tethered Pelvic Assist Device to Augment Pelvic Forces during Walking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10133
EP  - 10139
AU  - D. M. Stramel
AU  - S. K. Agrawal
PY  - 2020
KW  - force control
KW  - gait analysis
KW  - medical robotics
KW  - mobile robots
KW  - motion control
KW  - patient rehabilitation
KW  - robot kinematics
KW  - mobile TPAD frame
KW  - treadmill walking
KW  - motor control patterns
KW  - open loop controller
KW  - treadmill based robotic trainer
KW  - pelvic force augmentation
KW  - mobile tethered pelvic assist device
KW  - forward kinematics controller
KW  - mobile device controller
KW  - rehabilitation robotic devices
KW  - overground gait training robotic devices
KW  - Belts
KW  - Legged locomotion
KW  - Pelvis
KW  - Kinematics
KW  - Training
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196585
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For those with irregular gait, re-calibration of motor control strategies and retraining of coordination are key goals. Thoughtful external forces or resistances during repetitive tasks can reprogram motor control patterns and strategies. Prior work in our lab has utilized this theory to improve gait in various patient groups using the Tethered Pelvic Assist Device (TPAD), a treadmill-based robotic trainer. In this paper, we propose a new, portable extension of the TPAD, which relies on an open-loop, forward kinematics based controller to remove the restriction of walking in the laboratory on a treadmill, and therefore accommodates overground ambulation. To evaluate the effects of this new control scheme and the effects of the users holding the mobile TPAD frame, a dataset of walking in four conditions was collected from eight healthy individuals. When applying a constant pelvic loading force of 10% body weight, the mean ground reaction force increased by 8.2±7.7% when the individual holds the walker frame and 11.1±7.8% when no hand contact is made. The mobile TPAD was shown to still induce a targeted loading on individuals during treadmill walking. The validation of this mobile device's controller and characterization of holding the frame allow overground studies to be conducted, and now opens the door to new training paradigms for overground gait training.
ER  - 

TY  - CONF
TI  - Model Learning for Control of a Paralyzed Human Arm with Functional Electrical Stimulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10148
EP  - 10154
AU  - D. N. Wolf
AU  - Z. A. Hall
AU  - E. M. Schearer
PY  - 2020
KW  - biomechanics
KW  - biomedical electrodes
KW  - feedback
KW  - feedforward
KW  - Gaussian processes
KW  - handicapped aids
KW  - learning (artificial intelligence)
KW  - medical control systems
KW  - medical robotics
KW  - neuromuscular stimulation
KW  - neurophysiology
KW  - patient rehabilitation
KW  - regression analysis
KW  - paralyzed human arm
KW  - functional electrical stimulation
KW  - restoring reaching ability
KW  - tetraplegia
KW  - shoulder-arm complex
KW  - full-arm 3D reaching tasks
KW  - Gaussian process regression model
KW  - feedforward-feedback control structure
KW  - paralyzed upper limb
KW  - FES-driven 3D reaching controller
KW  - Muscles
KW  - Wrist
KW  - Iron
KW  - Force
KW  - Robots
KW  - Ground penetrating radar
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9196992
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Functional electrical stimulation (FES) is a promising technique for restoring reaching ability to individuals with tetraplegia. To this point, the complexities of goal-directed reaching motions and the shoulder-arm complex have prevented the realization of this potential in full-arm 3D reaching tasks. We trained a Gaussian process regression model to form the basis of a feedforward-feedback control structure capable of achieving reaching motions with a paralyzed upper limb. Over a series of 95 reaches of at least 10 cm in length, the controller achieved an average accuracy (measured by the Euclidean distance of the wrist to the final target position) of 3.8 cm and an average error along the path of 3.5 cm. This controller is the first demonstration of an accurate, complete-arm, FES-driven 3D reaching controller to be implemented with an individual with tetraplegia.
ER  - 

TY  - CONF
TI  - Transient Behavior and Predictability in Manipulating Complex Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10155
EP  - 10161
AU  - R. Nayeem
AU  - S. Bazzi
AU  - N. Hogan
AU  - D. Sternad
PY  - 2020
KW  - feedback
KW  - feedforward
KW  - haptic interfaces
KW  - human-robot interaction
KW  - manipulator dynamics
KW  - motion control
KW  - virtual reality
KW  - robot control
KW  - internal dynamics
KW  - transient behavior
KW  - predictable dynamics
KW  - virtual object
KW  - robotic manipulandum
KW  - predictable steady state
KW  - feedforward controller
KW  - inverse dynamics
KW  - haptic feedback
KW  - Transient analysis
KW  - Robots
KW  - Steady-state
KW  - Task analysis
KW  - Dynamics
KW  - Force
KW  - Haptic interfaces
DO  - 10.1109/ICRA40945.2020.9196977
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Relatively little work in human and robot control has examined the control of underactuated objects with internal dynamics, such as transporting a cup of coffee, a task that presents little problems for humans. This study examined how humans move a `cup of coffee' with a view to identify principles that may be useful for robot control. The specific focus was on how humans choose initial conditions to safely reach a steady state. We hypothesized that subjects choose initial conditions that minimized the transient duration to reach the steady state faster, as it presented more predictable dynamics. In the experiment, the cup of coffee was reduced to a 2-D cup with a sliding ball inside which was simulated in a virtual environment. Human subjects interacted with this virtual object via a robotic manipulandum that provided haptic feedback. Participants moved the cup between two targets without losing the ball; they were instructed to explore different initial conditions before initiating the continuous interaction. Results showed that subjects converged to a small set of initial conditions that decreased their transient durations and achieved a predictable steady state faster. Simulations with a simple feedforward controller and inverse dynamics calculations confirmed that these initial conditions indeed led to shorter transients and less complex interaction forces. These results may inform robot control of objects with internal dynamics where the effects of initial conditions need further investigation.
ER  - 

TY  - CONF
TI  - A Variable-Fractional Order Admittance Controller for pHRI
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10162
EP  - 10168
AU  - D. Sirintuna
AU  - Y. Aydin
AU  - O. Caldiran
AU  - O. Tokatli
AU  - V. Patoglu
AU  - C. Basdogan
PY  - 2020
KW  - augmented reality
KW  - control engineering computing
KW  - drilling
KW  - groupware
KW  - human-robot interaction
KW  - industrial robots
KW  - production engineering computing
KW  - user interfaces
KW  - pHRI
KW  - labor intensive tasks
KW  - fractional order control
KW  - fractional order variable admittance controller
KW  - integer order variable admittance controller
KW  - realistic drilling task
KW  - transparent interaction
KW  - augmented reality headset
KW  - human sensory capabilities
KW  - variable-fractional order admittance controller
KW  - automation driven manufacturing environments
KW  - collaborative robots
KW  - augmented reality interfaces
KW  - production workflow
KW  - cognitive skills
KW  - physical human robot interaction
KW  - cobots
KW  - stability
KW  - drilling depth
KW  - Admittance
KW  - Task analysis
KW  - Robot sensing systems
KW  - Collaboration
KW  - Stability criteria
DO  - 10.1109/ICRA40945.2020.9197288
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In today's automation driven manufacturing environments, emerging technologies like cobots (collaborative robots) and augmented reality interfaces can help integrating humans into the production workflow to benefit from their adaptability and cognitive skills. In such settings, humans are expected to work with robots side by side and physically interact with them. However, the trade-off between stability and transparency is a core challenge in the presence of physical human robot interaction (pHRI). While stability is of utmost importance for safety, transparency is required for fully exploiting the precision and ability of robots in handling labor intensive tasks. In this work, we propose a new variable admittance controller based on fractional order control to handle this trade-off more effectively. We compared the performance of fractional order variable admittance controller with a classical admittance controller with fixed parameters as a baseline and an integer order variable admittance controller during a realistic drilling task. Our comparisons indicate that the proposed controller led to a more transparent interaction compared to the other controllers without sacrificing the stability. We also demonstrate a use case for an augmented reality (AR) headset which can augment human sensory capabilities for reaching a certain drilling depth otherwise not possible without changing the role of the robot as the decision maker.
ER  - 

TY  - CONF
TI  - Assistive Gym: A Physics Simulation Framework for Assistive Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10169
EP  - 10176
AU  - Z. Erickson
AU  - V. Gangaram
AU  - A. Kapusta
AU  - C. K. Liu
AU  - C. C. Kemp
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - medical robotics
KW  - mobile robots
KW  - robot programming
KW  - service robots
KW  - physics simulation framework
KW  - autonomous robots
KW  - physical interaction
KW  - physics simulations
KW  - physical assistance
KW  - open source physics
KW  - assistive robots
KW  - simulated environments
KW  - robotic manipulator
KW  - assistive gym models
KW  - commercial robots
KW  - assistive robotics research
KW  - ADL
KW  - activities of daily living
KW  - Task analysis
KW  - Manipulators
KW  - Physics
KW  - Tools
KW  - Human-robot interaction
KW  - Mobile robots
DO  - 10.1109/ICRA40945.2020.9197411
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous robots have the potential to serve as versatile caregivers that improve quality of life for millions of people worldwide. Yet, conducting research in this area presents numerous challenges, including the risks of physical interaction between people and robots. Physics simulations have been used to optimize and train robots for physical assistance, but have typically focused on a single task. In this paper, we present Assistive Gym, an open source physics simulation framework for assistive robots that models multiple tasks. It includes six simulated environments in which a robotic manipulator can attempt to assist a person with activities of daily living (ADLs): itch scratching, drinking, feeding, body manipulation, dressing, and bathing. Assistive Gym models a person's physical capabilities and preferences for assistance, which are used to provide a reward function. We present baseline policies trained using reinforcement learning for four different commercial robots in the six environments. We demonstrate that modeling human motion results in better assistance and we compare the performance of different robots. Overall, we show that Assistive Gym is a promising tool for assistive robotics research.
ER  - 

TY  - CONF
TI  - Learning Whole-Body Human-Robot Haptic Interaction in Social Contexts
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10177
EP  - 10183
AU  - J. Campbell
AU  - K. Yamane
PY  - 2020
KW  - control engineering computing
KW  - force sensors
KW  - haptic interfaces
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - telerobotics
KW  - whole-body human-robot haptic interaction
KW  - learning-from-demonstration framework
KW  - human-robot social interactions
KW  - human-robot contact
KW  - LfD framework
KW  - teleoperated bimanual robot
KW  - force sensors
KW  - Robot sensing systems
KW  - Haptic interfaces
KW  - Force
KW  - Spatiotemporal phenomena
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196933
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a learning-from-demonstration (LfD) framework for teaching human-robot social interactions that involve whole-body haptic interaction, i.e. direct human-robot contact over the full robot body. The performance of existing LfD frameworks suffers in such interactions due to the high dimensionality and spatiotemporal sparsity of the demonstration data. We show that by leveraging this sparsity, we can reduce the data dimensionality without incurring a significant accuracy penalty, and introduce three strategies for doing so. By combining these techniques with an LfD framework for learning multimodal human-robot interactions, we can model the spatiotemporal relationship between the tactile and kinesthetic information during whole-body haptic interactions. Using a teleoperated bimanual robot equipped with 61 force sensors, we experimentally demonstrate that a model trained with 121 sample hugs from 4 participants generalizes well to unseen inputs and human partners.
ER  - 

TY  - CONF
TI  - Human Preferences in Using Damping to Manage Singularities During Physical Human-Robot Collaboration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10184
EP  - 10190
AU  - M. G. Carmichael
AU  - R. Khonasty
AU  - S. Aldini
AU  - D. Liu
PY  - 2020
KW  - damping
KW  - human-robot interaction
KW  - manipulator kinematics
KW  - mobile robots
KW  - kinematic singularities
KW  - damping-based strategy
KW  - human operator
KW  - human preferences
KW  - physical human-robot collaboration
KW  - robot manipulator
KW  - kinematic singular configuration
KW  - double-blind A/B pairwise comparison testing protocol
KW  - singularities handling
KW  - Damping
KW  - Manipulators
KW  - Collaboration
KW  - Jacobian matrices
KW  - Kinematics
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9197093
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When a robot manipulator approaches a kinematic singular configuration, control strategies need to be employed to ensure safe and robust operation. If this manipulator is being controlled by a human through physical human-robot collaboration, the choice of strategy for handling singularities can have a significant effect on the feelings and impressions of the user. To date the preferences of humans during physical human-robot collaboration regarding strategies for managing kinematic singularities have yet to be thoroughly explored.This work presents an empirical study of a damping-based strategy for handling singularities with regard to the preferences of the human operator. Two different parameters, damping rate and damping asymmetry, are tested using a double-blind A/B pairwise comparison testing protocol. Participants included two cohorts made up of the general public (n=51) and people working within a robotic research centre (n=18). In total 105 individual trials were performed. Results indicate a preference for a faster, asymmetric damping behavior that slows motions towards singularities whilst allowing for faster motions away.
ER  - 

TY  - CONF
TI  - MOCA-MAN: A MObile and reconfigurable Collaborative Robot Assistant for conjoined huMAN-robot actions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10191
EP  - 10197
AU  - W. Kim
AU  - P. Balatti
AU  - E. Lamon
AU  - A. Ajoudani
PY  - 2020
KW  - gesture recognition
KW  - human-robot interaction
KW  - manipulators
KW  - medical robotics
KW  - mobile robots
KW  - multi-robot systems
KW  - mobile manipulators
KW  - supernumerary limbs
KW  - reconfiguration potential
KW  - MObile Collaborative robot Assistant
KW  - supernumerary body
KW  - MOCA-MAN
KW  - hand gesture recognition system
KW  - mobile base
KW  - long distance co-carrying tasks
KW  - manipulating tools
KW  - conjoined actions
KW  - performing heavy manipulation tasks
KW  - prolonged manipulation tasks
KW  - close-proximity manipulation
KW  - mobile robot assistant
KW  - reconfigurable collaborative robot assistant
KW  - conjoined huMAN-robot actions
KW  - collaborative robotic system
KW  - Admittance
KW  - Task analysis
KW  - Collaboration
KW  - Robot sensing systems
KW  - Clamps
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9197115
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The objective of this paper is to create a new collaborative robotic system that subsumes the advantages of mobile manipulators and supernumerary limbs. By exploiting the reconfiguration potential of a MObile Collaborative robot Assistant (MOCA), we create a collaborative robot that can function autonomously, in close proximity to humans, or be physically coupled to the human counterpart as a supernumerary body (MOCA-MAN). Through an admittance interface and a hand gesture recognition system, the controller can give higher priority to the mobile base (e.g., for long distance co-carrying tasks) or the arm movements (e.g., for manipulating tools), when performing conjoined actions. The resulting system has a high potential not only to reduce waste associated with the equipment waiting and setup times, but also to mitigate the human effort when performing heavy or prolonged manipulation tasks. The performance of the proposed system, i.e., MOCA-MAN, is evaluated by multiple subjects in two different use-case scenarios, which require large mobility or close-proximity manipulation.
ER  - 

TY  - CONF
TI  - Closing the Force Loop to Enhance Transparency in Time-delayed Teleoperation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10198
EP  - 10204
AU  - R. Balachandran
AU  - J. -H. Ryu
AU  - M. Jorda
AU  - C. Ott
AU  - A. Albu-Schaeffer
PY  - 2020
KW  - delays
KW  - force control
KW  - human-robot interaction
KW  - stability
KW  - telerobotics
KW  - master-slave teleoperation system
KW  - bilateral controllers
KW  - force transparency
KW  - force loop
KW  - force control
KW  - time delayed teleoperation
KW  - KUKA lightweight robots
KW  - time domain passivity
KW  - Force
KW  - Iron
KW  - Force measurement
KW  - Robots
KW  - Stability analysis
KW  - Delays
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197420
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the present paper, we first adopt explicit force control from general robotics and embed it into teleoperation systems to enhance the transparency by reducing the effect of the perceived inertia to the human operator and simultaneously improve contact perception. To ensure stability of the proposed teleoperation system considering time-delays, we propose a sequential design procedure based on time domain passivity approach. Experimental results of master-slave teleoperation system, based on KUKA light-weight-robots, for different values of delays are presented. Comparative analysis is conducted considering two existing approaches, namely 2-channel and 4-channel architecture based bilateral controllers, and its results clearly indicate significant improvement in force transparency owing to the proposed method. The proposed system is finally validated considering a real industrial assembly scenario.
ER  - 

TY  - CONF
TI  - Evaluation of an Exoskeleton-based Bimanual Teleoperation Architecture with Independently Passivated Slave Devices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10205
EP  - 10211
AU  - F. Porcini
AU  - D. Chiaradia
AU  - S. Marcheschi
AU  - M. Solazzi
AU  - A. Frisoli
PY  - 2020
KW  - delays
KW  - motion control
KW  - stability
KW  - telerobotics
KW  - simulated time delay
KW  - control loop frequency
KW  - multiDoFs devices
KW  - TDPA
KW  - time domain passivity approach
KW  - exoskeletal master
KW  - bimanual teleoperation system
KW  - communication delay
KW  - bilateral teleoperation
KW  - haptic feedback
KW  - robotic platforms
KW  - rescue robotics
KW  - independently passivated slave devices
KW  - exoskeleton-based bimanual teleoperation architecture
KW  - Exoskeletons
KW  - Task analysis
KW  - Computer architecture
KW  - Stability analysis
KW  - Robots
KW  - Delays
KW  - Haptic interfaces
DO  - 10.1109/ICRA40945.2020.9197079
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Search and rescue robotics is becoming a relevant topic in the last years and the growing number of robotic platforms and dedicated projects is the evidence of the interest in this area. In this context, the possibility to drive a remote robot with an exoskeleton is a promising strategy to enhance dexterity, reduce operator effort and save time. However, the use of haptic feedback (bilateral teleoperation) may lead to instability in the presence of communication delay and more complex is the case of bimanual teleoperation where the two arms can exchange energy. In this work, we present a bimanual teleoperation system based on an exoskeletal master, where multi-degrees of freedom (multi-DoFs) and kinematically different devices are involved. In the implemented architecture the two slaves are managed in parallel and independently passivated using the Time Domain Passivity Approach (TDPA) extended for multi-DoFs devices. To investigate the stability of the architecture we designed two tasks highly related to real disaster scenarios: the first one was useful to verify the system behavior in case of small movements and constrained configurations, whereas the second experiment was designed to involve larger contact forces and movements. Moreover, we compared the effect of both delay and low control loop frequency on the stability of the system when TDPA was applied. From the results, it was evident that the overall system exhibited a stable behavior with the use of the TDPA, even passivating the two slaves independently, under simulated time delay and in presence of a low control loop frequency.
ER  - 

TY  - CONF
TI  - Hand-worn Haptic Interface for Drone Teleoperation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10212
EP  - 10218
AU  - M. Macchini
AU  - T. Havy
AU  - A. Weber
AU  - F. Schiano
AU  - D. Floreano
PY  - 2020
KW  - autonomous aerial vehicles
KW  - data gloves
KW  - human-robot interaction
KW  - mobile robots
KW  - motion control
KW  - robot vision
KW  - telerobotics
KW  - trajectory control
KW  - drone teleoperation
KW  - remote radio controllers
KW  - wearable interface
KW  - drone trajectory
KW  - hand motion
KW  - haptic system
KW  - robotic systems
KW  - teleoperation performance
KW  - remote controllers
KW  - human-robot interfaces
KW  - hand-worn haptic interface
KW  - data glove
KW  - line of sight
KW  - search-and-rescue missions
KW  - Haptic interfaces
KW  - Drones
KW  - Task analysis
KW  - Robot sensing systems
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9196664
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Drone teleoperation is usually accomplished using remote radio controllers, devices that can be hard to master for inexperienced users. Moreover, the limited amount of information fed back to the user about the robot's state, often limited to vision, can represent a bottleneck for operation in several conditions. In this work, we present a wearable interface for drone teleoperation and its evaluation through a user study. The two main features of the proposed system are a data glove to allow the user to control the drone trajectory by hand motion and a haptic system used to augment their awareness of the environment surrounding the robot. This interface can be employed for the operation of robotic systems in line of sight (LoS) by inexperienced operators and allows them to safely perform tasks common in inspection and search-and-rescue missions such as approaching walls and crossing narrow passages with limited visibility conditions. In addition to the design and implementation of the wearable interface, we performed a systematic study to assess the effectiveness of the system through three user studies (n = 36) to evaluate the users' learning path and their ability to perform tasks with limited visibility. We validated our ideas in both a simulated and a real-world environment. Our results demonstrate that the proposed system can improve teleoperation performance in different cases compared to standard remote controllers, making it a viable alternative to standard Human-Robot Interfaces.
ER  - 

TY  - CONF
TI  - Toward Human-like Teleoperated Robot Motion: Performance and Perception of a Choreography-inspired Method in Static and Dynamic Tasks for Rapid Pose Selection of Articulated Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10219
EP  - 10225
AU  - A. Bushman
AU  - M. Asselmeier
AU  - J. Won
AU  - A. LaViers
PY  - 2020
KW  - control engineering computing
KW  - human-robot interaction
KW  - mobile robots
KW  - service robots
KW  - telerobotics
KW  - virtual reality
KW  - remotely-operated robot
KW  - remote telepresence
KW  - Baxter robot
KW  - Xbox One controller
KW  - JBJ
KW  - limb
KW  - multiple joints
KW  - successfully completed tasks
KW  - joint-by-joint method
KW  - choreography-inspired method
KW  - performance data
KW  - static tasks
KW  - RCC method
KW  - dynamic tasks
KW  - human-likeness
KW  - robotic motion
KW  - teleoperated robot motion
KW  - rapid pose selection
KW  - articulated robots
KW  - robot choreography center
KW  - Task analysis
KW  - Training
KW  - Dynamics
KW  - Joints
KW  - Robot motion
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9196742
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In some applications, operators may want to create fluid, human-like motion on a remotely-operated robot, for example, a device used for remote telepresence. This paper examines two methods of controlling the pose of a Baxter robot via an Xbox One controller. The first method is a joint- by-joint (JBJ) method in which one joint of each limb is specified in sequence. The second method of control, named Robot Choreography Center (RCC), utilizes choreographic abstractions in order to simultaneously move multiple joints of the limb of the robot in a predictable manner. Thirty-eight users were asked to perform four tasks with each method. Success rate and duration of successfully completed tasks were used to analyze the performances of the participants. Analysis of the preferences of the users found that the joint-by-joint (JBJ) method was considered to be more precise, easier to use, safer, and more articulate, while the choreography-inspired (RCC) method of control was perceived as faster, more fluid, and more expressive. Moreover, performance data found that while both methods of control were over 80% successful for the two static tasks, the RCC method was an average of 11.85% more successful for the two more difficult, dynamic tasks. Future work will leverage this framework to investigate ideas of fluidity, expressivity, and human-likeness in robotic motion through online user studies with larger participant pools.
ER  - 

TY  - CONF
TI  - Helping Robots Learn: A Human-Robot Master-Apprentice Model Using Demonstrations via Virtual Reality Teleoperation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10226
EP  - 10233
AU  - J. DelPreto
AU  - J. I. Lipton
AU  - L. Sanneman
AU  - A. J. Fay
AU  - C. Fourie
AU  - C. Choi
AU  - D. Rus
PY  - 2020
KW  - control engineering computing
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - robot programming
KW  - telerobotics
KW  - virtual reality
KW  - grasping task
KW  - human perception
KW  - human-robot master-apprentice model
KW  - virtual reality teleoperation
KW  - artificial intelligence
KW  - self-supervised learning
KW  - Robots
KW  - Grasping
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Virtual reality
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9196754
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As artificial intelligence becomes an increasingly prevalent method of enhancing robotic capabilities, it is important to consider effective ways to train these learning pipelines and to leverage human expertise. Working towards these goals, a master-apprentice model is presented and is evaluated during a grasping task for effectiveness and human perception. The apprenticeship model augments self-supervised learning with learning by demonstration, efficiently using the human's time and expertise while facilitating future scalability to supervision of multiple robots; the human provides demonstrations via virtual reality when the robot cannot complete the task autonomously. Experimental results indicate that the robot learns a grasping task with the apprenticeship model faster than with a solely self-supervised approach and with fewer human interventions than a solely demonstration-based approach; 100% grasping success is obtained after 150 grasps with 19 demonstrations. Preliminary user studies evaluating workload, usability, and effectiveness of the system yield promising results for system scalability and deployability. They also suggest a tendency for users to overestimate the robot's skill and to generalize its capabilities, especially as learning improves.
ER  - 

TY  - CONF
TI  - A Framework for Interactive Virtual Fixture Generation for Shared Teleoperation in Unstructured Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10234
EP  - 10241
AU  - V. Pruks
AU  - J. -H. Ryu
PY  - 2020
KW  - feature selection
KW  - haptic interfaces
KW  - telecontrol
KW  - virtual reality
KW  - unstructured environments
KW  - human operator performance
KW  - remote environment
KW  - task execution
KW  - user interface
KW  - camera images
KW  - interactive selection
KW  - feature selection
KW  - interactive virtual fixture generation
KW  - shared teleoperation
KW  - 6-DOF haptic feedback
KW  - Tools
KW  - Feature extraction
KW  - Task analysis
KW  - Robots
KW  - Detectors
KW  - Three-dimensional displays
KW  - Haptic interfaces
DO  - 10.1109/ICRA40945.2020.9196579
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Virtual fixtures (VFs) improve human operator performance in teleoperation scenarios. However, the generation of VFs is challenging, especially in unstructured environments. In this work, we introduce a framework for the interactive generation of VF. The method is based on the observation that a human can easily understand just by looking at the remote environment which VF could help in task execution. We propose a user interface that detects features on camera images and permits interactive selection of the features. We demonstrate how the feature selection can be used for designing VF, providing 6-DOF haptic feedback. In order to make the proposed framework more generally applicable to a wider variety of applications, we formalize the process of virtual fixture generation (VFG) into the specification of features, geometric primitives, and constraints. The framework can be extended further by the introduction of additional components. Through the human subject study, we demonstrate the proposed framework is intuitive, easy to use while effective, especially for performing hard contact tasks.
ER  - 

TY  - CONF
TI  - Local Obstacle-Skirting Path Planning for a Fast Bi-steerable Rover using Bézier Curves
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10242
EP  - 10248
AU  - M. Fnadi
AU  - W. Du
AU  - R. Gomes da Silva
AU  - F. Plumet
AU  - F. Benamar
PY  - 2020
KW  - automatic guided vehicles
KW  - collision avoidance
KW  - curve fitting
KW  - mobile robots
KW  - navigation
KW  - off-road vehicles
KW  - predictive control
KW  - stability
KW  - steering systems
KW  - vehicle dynamics
KW  - local obstacle-skirting path planning
KW  - obstacle avoidance
KW  - off-road mobile robots
KW  - global reference path
KW  - smooth path
KW  - lateral stability
KW  - double-steering rover
KW  - online cubic Bezier curves
KW  - bi-steerable rover
KW  - constrained model predictive control
KW  - navigation
KW  - autonomous guided vehicles
KW  - Safety
KW  - Collision avoidance
KW  - Mobile robots
KW  - Lead
KW  - Path planning
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197563
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on local path planning for obstacle avoidance tasks dedicated to off-road mobile robots. This approach calculates a new local path for the vehicle using a set of cubic Bezier curves once the safety distance is not respected; otherwise, the vehicle follows the global reference path which is defined off-line. Two basic steps are used to determine this new path. Firstly, some significant points that should belong to the planned path are extracted on-line according to the obstacle's sizes and the current state of the vehicle, these points are approved as waypoints. Secondly, on-line cubic Bezier curves are computed to create a smooth path for these points such that the safety and lateral stability of the vehicle are ensured (i.e., preventing huge curvatures and wide-variation in steering angles). This path will be used as a reference to be performed by the vehicle using a constrained model predictive control. The validation of our navigation strategy is performed via numerical simulations and experiments using a fast double-steering rover.
ER  - 

TY  - CONF
TI  - Collision Avoidance with Proximity Servoing for Redundant Serial Robot Manipulators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10249
EP  - 10255
AU  - Y. Ding
AU  - U. Thomas
PY  - 2020
KW  - collision avoidance
KW  - human-robot interaction
KW  - manipulators
KW  - motion control
KW  - quadratic programming
KW  - repulsive motions
KW  - instantaneous optimal joint velocities
KW  - quadratic optimization problem
KW  - proximity sensing skins
KW  - collision avoidance
KW  - low-latency perception
KW  - proximity sensors
KW  - fastreacting motions
KW  - safe human-robot interaction
KW  - redundant serial robot manipulators
KW  - proximity servoing
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Task analysis
KW  - Manipulators
KW  - Skin
DO  - 10.1109/ICRA40945.2020.9196759
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collision avoidance is a key technology towards safe human-robot interaction, especially on-line and fastreacting motions are required. Skins with proximity sensors mounted on the robot's outer shell provide an interesting approach to occlusion-free and low-latency perception. However, collision avoidance algorithms which make extensive use of these properties for fast-reacting motions have not yet been fully investigated. We present an improved collision avoidance algorithm for proximity sensing skins by formulating a quadratic optimization problem with inequality constraints to compute instantaneous optimal joint velocities. Compared to common repulsive force methods, our algorithm confines the approach velocity to obstacles and keeps motions pointing away from obstacles unrestricted. Since with repulsive motions the robot only moves in one direction, opposite to obstacles, our approach has better exploitation of the redundancy space to maintain the task motion and gets stuck less likely in local minima. Furthermore, our method incorporates an active behaviour for avoiding obstacles and evaluates all potentially colliding obstacles for the whole arm, rather than just the single nearest obstacle. We demonstrate the effectiveness of our method with simulations and on real robot manipulators in comparison with commonly used repulsive force methods and our prior proposed approach.
ER  - 

TY  - CONF
TI  - Predicting Obstacle Footprints from 2D Occupancy Maps by Learning from Physical Interactions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10256
EP  - 10262
AU  - M. Kollmitz
AU  - D. Büscher
AU  - W. Burgard
PY  - 2020
KW  - collision avoidance
KW  - convolutional neural nets
KW  - laser ranging
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - indoor robot localization
KW  - obstacle avoidance
KW  - laser scanners
KW  - collision events
KW  - 2D occupancy maps
KW  - obstacle footprint prediction
KW  - physical interaction learning
KW  - horizontal scanning 2D laser range finders
KW  - convolutional neural network
KW  - Two dimensional displays
KW  - Collision avoidance
KW  - Image segmentation
KW  - Training
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197474
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Horizontally scanning 2D laser rangefinders are a popular approach for indoor robot localization because of the high accuracy of the sensors and the compactness of the required 2D maps. As the scanners in this configuration only provide information about one slice of the environment, the measurements typically do not capture the full extent of a large variety of obstacles, including chairs or tables. Accordingly, obstacle avoidance based on laser scanners mounted in such a fashion is likely to fail. In this paper, we propose a learning-based approach to predict collisions in 2D occupancy maps. Our approach is based on a convolutional neural network which is trained on a 2D occupancy map and collision events recorded with a bumper while the robot is navigating in its environment. As the network operates on local structures only, it can generalize to new environments. In addition, the robot can collect and integrate new collision examples after an initial training phase. Extensive experiments carried out in simulation and a realistic real-world environment confirm that our approach allows robots to learn from collision events to avoid collisions in the future.
ER  - 

TY  - CONF
TI  - Path Planning in Dynamic Environments using Generative RNNs and Monte Carlo Tree Search
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10263
EP  - 10269
AU  - S. Eiffert
AU  - H. Kong
AU  - N. Pirmarzdashti
AU  - S. Sukkarieh
PY  - 2020
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - Monte Carlo methods
KW  - recurrent neural nets
KW  - tree searching
KW  - Monte Carlo tree search
KW  - generative recurrent neural networks
KW  - integrated path
KW  - motion models
KW  - traffic
KW  - robotic path planning
KW  - dynamic environments
KW  - effective path planning
KW  - motion prediction accuracy
KW  - planned robotic actions
KW  - generative RNNs
KW  - action space
KW  - crowd dynamics
KW  - social response
KW  - learnt model
KW  - Robots
KW  - Predictive models
KW  - Path planning
KW  - Decoding
KW  - Collision avoidance
KW  - Training
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196631
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - State of the art methods for robotic path planning in dynamic environments, such as crowds or traffic, rely on hand crafted motion models for agents. These models often do not reflect interactions of agents in real world scenarios. To overcome this limitation, this paper proposes an integrated path planning framework using generative Recurrent Neural Networks within a Monte Carlo Tree Search (MCTS). This approach uses a learnt model of social response to predict crowd dynamics during planning across the action space. This extends our recent work using generative RNNs to learn the relationship between planned robotic actions and the likely response of a crowd. We show that the proposed framework can considerably improve motion prediction accuracy during interactions, allowing more effective path planning. The performance of our method is compared in simulation with existing methods for collision avoidance in a crowd of pedestrians, demonstrating the ability to control future states of nearby individuals. We also conduct preliminary real world tests to validate the effectiveness of our method.
ER  - 

TY  - CONF
TI  - Safety-Critical Rapid Aerial Exploration of Unknown Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10270
EP  - 10276
AU  - A. Singletary
AU  - T. Gurriet
AU  - P. Nilsson
AU  - A. D. Ames
PY  - 2020
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - helicopters
KW  - mobile robots
KW  - sensors
KW  - uncertain systems
KW  - high-speed flight
KW  - uncertain environments
KW  - controller level
KW  - state uncertainty
KW  - nonlinear system dynamics
KW  - high-fidelity simulation
KW  - cave environment
KW  - safety-critical rapid aerial exploration
KW  - collision avoidance
KW  - aerial vehicles
KW  - unknown environments
KW  - quadrotor
KW  - onboard sensors
KW  - Safety
KW  - Collision avoidance
KW  - Three-dimensional displays
KW  - Drones
KW  - Trajectory
KW  - Vehicle dynamics
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197416
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper details a novel approach to collision avoidance for aerial vehicles that enables high-speed flight in uncertain environments. This framework is applied at the controller level and provides safety regardless of the planner that is used. The method is shown to be robust to state uncertainty and disturbances, and is computed entirely online utilizing the full nonlinear system dynamics. The effectiveness of this method is shown in a high-fidelity simulation of a quadrotor with onboard sensors rapidly and safely exploring a cave environment utilizing a simple planner.
ER  - 

TY  - CONF
TI  - Reconfigurable Magnetic Microswarm for Thrombolysis under Ultrasound Imaging
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10285
EP  - 10291
AU  - Q. Wang
AU  - B. Wang
AU  - J. Yu
AU  - K. Schweizer
AU  - B. J. Nelson
AU  - L. Zhang
PY  - 2020
KW  - biochemistry
KW  - biological tissues
KW  - biomedical materials
KW  - biomedical ultrasonics
KW  - blood
KW  - magnetic particles
KW  - microrobots
KW  - nanomedicine
KW  - nanoparticles
KW  - patient treatment
KW  - reconfigurable magnetic microswarm
KW  - thrombolysis
KW  - ultrasound imaging
KW  - magnetic nanoparticle microswarm
KW  - tissue plasminogen activator
KW  - oscillating magnetic field
KW  - aspect ratio
KW  - out-of-plane fluid convection
KW  - lysis rate
KW  - microswarm-induced fluid convection
KW  - Ultrasonic imaging
KW  - Magnetic resonance imaging
KW  - Convection
KW  - Coagulation
KW  - Coils
KW  - Micro/nanorobot
KW  - magnetic control
KW  - collective behavior
KW  - thrombolysis
KW  - ultrasound imaging
DO  - 10.1109/ICRA40945.2020.9197432
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose thrombolysis using a magnetic nanoparticle microswarm with tissue plasminogen activator (tPA) under ultrasound imaging. The microswarm is generated in blood using an oscillating magnetic field and can be navigated with locomotion along both the long and short axis. By modulating the input field, the aspect ratio of the microswarm can be reversibly tuned, showing the ability to adapt to different confined environments. Simulation results indicate that both in-plane and out-of-plane fluid convection are induced around the microswarm, which can be further enhanced by tuning the aspect ratio of the microswarm. Under ultrasound imaging, the microswarm is navigated in a microchannel towards a blood clot and deformed to obtain optimal lysis. Experimental results show that the lysis rate reaches -0.1725 ± 0.0612 mm3/min in the 37°C blood environment under the influence of the microswarm-induced fluid convection and tPA. The lysis rate is enhanced 2.5-fold compared to that without the microswarm (-0.0681 ± 0.0263 mm3/min). Our method provides a new strategy to increase the efficiency of thrombolysis by applying microswarm-induced fluid convection, indicating that swarming micro/nanorobots have the potential to act as effective tools towards targeted therapy.
ER  - 

TY  - CONF
TI  - Improving Optical Micromanipulation with Force-Feedback Bilateral Coupling
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10292
EP  - 10298
AU  - E. Gerena
AU  - F. Legendre
AU  - Y. Vitry
AU  - S. Régnier
AU  - S. Haliyo
PY  - 2020
KW  - control engineering computing
KW  - force feedback
KW  - haptic interfaces
KW  - micromanipulators
KW  - radiation pressure
KW  - robot vision
KW  - telerobotics
KW  - haptic device
KW  - transparent force feedback
KW  - user dexterity
KW  - microsized shapes
KW  - contact forces
KW  - optical micromanipulation
KW  - force-feedback bilateral coupling
KW  - interactive approaches
KW  - visual feedback
KW  - haptic feedback teleoperation systems
KW  - optical tweezers platform
KW  - 2D image
KW  - Optical feedback
KW  - Optical sensors
KW  - Force
KW  - Haptic interfaces
KW  - Optical imaging
KW  - Three-dimensional displays
KW  - High-speed optical techniques
DO  - 10.1109/ICRA40945.2020.9197424
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Micromanipulation is challenging due to the specific physical effects governing the microworld. Interactive approaches using only visual feedback are limited to the 2D image of the microscope, and have forcibly lower bandwidth. Recently, haptic feedback teleoperation systems have been developed to try to overcome those difficulties. This paper explores the case of an optical tweezers platform coupled to an haptic device providing transparent force feedback. The impact of haptic feedback regarding user dexterity on tactile exploration tasks is studied using 3 μm microbeads and a test bench with micro sized shapes. The results reveal a consistent improvement in both users' trajectory tracking and their control of the contact forces. This also validates the experimental setup which performed reliably on 140 different trials of the evaluation.
ER  - 

TY  - CONF
TI  - Maneuver at Micro Scale: Steering by Actuation Frequency Control in Micro Bristle Robots*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10299
EP  - 10304
AU  - Z. Hao
AU  - D. Kim
AU  - A. R. Mohazab
AU  - A. Ansari
PY  - 2020
KW  - microactuators
KW  - microrobots
KW  - piezoelectric actuators
KW  - steering systems
KW  - vibrations
KW  - resonance-based steering mechanism
KW  - differential steering
KW  - on-board piezoelectric actuator
KW  - frequency-controlled locomotion
KW  - steering mechanism
KW  - microbristle robots
KW  - actuation frequency control
KW  - principal actuation frequency components
KW  - size 6.0 mm
KW  - size 400.0 mum
KW  - size 12.0 mm
KW  - size 8.0 mm
KW  - Robots
KW  - Resonant frequency
KW  - Actuators
KW  - Vibrations
KW  - Steady-state
KW  - Wires
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9196694
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel steering mechanism, which leads to frequency-controlled locomotion demonstrated for the first time in micro bristle robots. The miniaturized robots are 3D-printed, 12 mm × 8 mm × 6 mm in size, with bristle feature sizes down to 400 μm. The robots can be steered by utilizing the distinct resonance behaviors of the asymmetrical bristle sets. The left and right sets of the bristles have different diameters, and thus different stiffnesses and resonant frequencies. The unique response of each bristle side to the vertical vibrations of a single on-board piezoelectric actuator causes differential steering of the robot. The robot can be modeled as two coupled uniform bristle robots, representing the left and the right sides. At distinct frequencies, the robots can move in all four principal directions: forward, backward, left and right. Furthermore, the full 360° 2D plane can be covered by superimposing the principal actuation frequency components with desired amplitudes. In addition to miniaturized robots, the presented resonance-based steering mechanism can be applied over multiple scales and to other mechanical systems.
ER  - 

TY  - CONF
TI  - Scaling down an insect-size microrobot, HAMR-VI into HAMR-Jr
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10305
EP  - 10311
AU  - K. Jayaram
AU  - J. Shum
AU  - S. Castellanos
AU  - E. F. Helbling
AU  - R. J. Wood
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - microrobots
KW  - motion control
KW  - insect-size microrobot
KW  - mechanically dexterous legged robot
KW  - HAMR-Jr's open-loop locomotion
KW  - HAMR-VI microrobot
KW  - design process
KW  - fabrication process
KW  - independently actuated degrees of freedom
KW  - mass 320.0 mg
KW  - frequency 1.0 Hz to 200.0 Hz
KW  - size 22.5 mm
KW  - Legged locomotion
KW  - Actuators
KW  - Resonant frequency
KW  - Heat-assisted magnetic recording
KW  - Fabrication
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197436
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Here we present HAMR-Jr, a 22.5mm, 320mg quadrupedal microrobot. With eight independently actuated degrees of freedom, HAMR-Jr is, to our knowledge, the most mechanically dexterous legged robot at its scale and is capable of high-speed locomotion (13.91bodylengthss-1) at a variety of stride frequencies (1-200Hz) using multiple gaits. We achieved this using a design and fabrication process that is flexible, allowing scaling with minimum changes to our workflow. We further characterized HAMR-Jr's open-loop locomotion and compared it with the larger scale HAMR-VI microrobot to demonstrate the effectiveness of scaling laws in predicting running performance.
ER  - 

TY  - CONF
TI  - Reality as a simulation of reality: robot illusions, fundamental limits, and a physical demonstration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10327
EP  - 10334
AU  - D. A. Shell
AU  - J. M. O’Kane
PY  - 2020
KW  - collision avoidance
KW  - human-robot interaction
KW  - mobile robots
KW  - multi-robot systems
KW  - fundamental limits
KW  - physical demonstration
KW  - robot behavior
KW  - potential mismatches
KW  - convincing illusion
KW  - system simulation
KW  - simulated systems
KW  - simple multirobot experiment
KW  - robot navigating
KW  - robot illusions
KW  - Robot sensing systems
KW  - Software
KW  - Sensor systems
KW  - Mobile robots
KW  - Emulation
DO  - 10.1109/ICRA40945.2020.9196761
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider problems in which robots conspire to present a view of the world that differs from reality. The inquiry is motivated by the problem of validating robot behavior physically despite there being a discrepancy between the robots we have at hand and those we wish to study, or the environment for testing that is available versus that which is desired, or other potential mismatches in this vein. After formulating the concept of a convincing illusion, essentially a notion of system simulation that takes place in the real world, we examine the implications of this type of simulability in terms of infrastructure requirements. Time is one important resource: some robots may be able to simulate some others but, perhaps, only at a rate that is slower than real-time. This difference gives a way of relating the simulating and the simulated systems in a form that is relative. We establish some theorems, including one with the flavor of an impossibility result, and providing several examples throughout. Finally, we present data from a simple multi-robot experiment based on this theory, with a robot navigating amid an unbounded field of obstacles."Truth is beautiful, without doubt; but so are lies."-Ralph Waldo Emerson.
ER  - 

TY  - CONF
TI  - Finding Missing Skills for High-Level Behaviors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10335
EP  - 10341
AU  - A. Pacheck
AU  - S. Moarref
AU  - H. Kress-Gazit
PY  - 2020
KW  - robots
KW  - temporal logic
KW  - KUKA IIWA arm
KW  - Baxter robot
KW  - LTL specifications
KW  - correct-by-construction robot control
KW  - LTL synthesis
KW  - high-level robot tasks
KW  - linear temporal logic
KW  - Task analysis
KW  - Maintenance engineering
KW  - Games
KW  - Robot control
KW  - Manipulators
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9197223
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recently, Linear Temporal Logic (LTL) has been used as a formalism for defining high-level robot tasks, and LTL synthesis has been used to automatically create correct-by-construction robot control. The underlying premise of this approach is that the robot has a set of actions, or skills, that can be composed to achieve the high- level task. In this paper we consider LTL specifications that cannot be synthesized into robot control due to lack of appropriate skills; we present algorithms for automatically suggesting new or modified skills for the robot that will guarantee the task will be achieved. We demonstrate our approach with a physical Baxter robot and a simulated KUKA IIWA arm.
ER  - 

TY  - CONF
TI  - Near-Optimal Reactive Synthesis Incorporating Runtime Information
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10342
EP  - 10348
AU  - S. Bharadwaj
AU  - A. P. Vinod
AU  - R. Dimitrova
AU  - U. Topcu
PY  - 2020
KW  - control system synthesis
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - suboptimal control
KW  - mission specification
KW  - dynamic environment
KW  - performance metric
KW  - task-critical information
KW  - strategy synthesis
KW  - time-varying information
KW  - online re-synthesis
KW  - pre-specified representative information scenarios
KW  - performance suboptimality
KW  - runtime information
KW  - near-optimal reactive synthesis
KW  - switching mechanism
KW  - robotic motion planning
KW  - Runtime
KW  - Switches
KW  - Games
KW  - Robots
KW  - Measurement
KW  - Safety
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196581
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of optimal reactive synthesis - compute a strategy that satisfies a mission specification in a dynamic environment, and optimizes a given performance metric. We incorporate task-critical information, that is only available at runtime, into the strategy synthesis in order to improve performance. Existing approaches to utilising such time-varying information require online re-synthesis, which is not computationally feasible in real-time applications. In this paper, we presynthesize a set of strategies corresponding to candidate instantiations (pre-specified representative information scenarios). We then propose a novel switching mechanism to dynamically switch between the strategies at runtime while guaranteeing all safety and liveness goals are met. We also characterize bounds on the performance suboptimality. We demonstrate our approach on two examples - robotic motion planning where the likelihood of the position of the robot's goal is updated in real-time, and an air traffic management problem for urban air mobility.
ER  - 

TY  - CONF
TI  - Control Synthesis from Linear Temporal Logic Specifications using Model-Free Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10349
EP  - 10355
AU  - A. K. Bozkurt
AU  - Y. Wang
AU  - M. M. Zavlanos
AU  - M. Pajic
PY  - 2020
KW  - control system synthesis
KW  - decision theory
KW  - learning (artificial intelligence)
KW  - Markov processes
KW  - mobile robots
KW  - path planning
KW  - probability
KW  - temporal logic
KW  - motion planning
KW  - MDP
KW  - RL-based synthesis approach
KW  - discount factors
KW  - model-free RL algorithm
KW  - total discounted reward
KW  - optimal policy
KW  - transition probabilities
KW  - LTL formula
KW  - Markov decision process
KW  - unknown stochastic environment
KW  - control policy synthesis
KW  - reinforcement learning frame-work
KW  - model-free reinforcement learning
KW  - linear temporal logic specifications
KW  - control synthesis
KW  - Learning (artificial intelligence)
KW  - Automata
KW  - Planning
KW  - Markov processes
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196796
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a reinforcement learning (RL) frame-work to synthesize a control policy from a given linear temporal logic (LTL) specification in an unknown stochastic environment that can be modeled as a Markov Decision Process (MDP). Specifically, we learn a policy that maximizes the probability of satisfying the LTL formula without learning the transition probabilities. We introduce a novel rewarding and discounting mechanism based on the LTL formula such that (i) an optimal policy maximizing the total discounted reward effectively maximizes the probabilities of satisfying LTL objectives, and (ii) a model-free RL algorithm using these rewards and discount factors is guaranteed to converge to such a policy. Finally, we illustrate the applicability of our RL-based synthesis approach on two motion planning case studies.
ER  - 

TY  - CONF
TI  - R-Min: a Fast Collaborative Underactuated Parallel Robot for Pick-and-Place Operations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10365
EP  - 10371
AU  - G. Jeanneau
AU  - V. Bégoc
AU  - S. Briot
AU  - A. Goldsztejn
PY  - 2020
KW  - collision avoidance
KW  - end effectors
KW  - human-robot interaction
KW  - springs (mechanical)
KW  - trajectory control
KW  - parallel manipulator
KW  - pick-and-place operations
KW  - human operator
KW  - acceleration
KW  - planar five-bar mechanism
KW  - passive joints
KW  - planar seven-bar mechanism
KW  - supplementary passive leg
KW  - collaborative parallel robot
KW  - pick-and-place trajectory
KW  - R-Min
KW  - collaborative underactuated parallel robot
KW  - tension spring
KW  - end-effector
KW  - degrees of freedom
KW  - impact force
KW  - Collision avoidance
KW  - Collaboration
KW  - Prototypes
KW  - Parallel robots
KW  - Robot sensing systems
KW  - Springs
DO  - 10.1109/ICRA40945.2020.9196990
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces an intrinsically safe parallel manipulator dedicated to fast pick-and-place operations, called R-Min. It has been designed to reduce the risk of injury during a collision with a human operator, while maintaining high speed and acceleration. The proposed architecture is based on a modification of the well-known planar five-bar mechanism, where additional passive joints are introduced to the distal links in order to create a planar seven-bar mechanism with two degrees of underactuation, so that it can passively reconfigure in case of collision. A supplementary passive leg, in which a tension spring is mounted, is added between the base and the end-effector in order to constrain the additional degrees of freedom. A prototype of this new collaborative parallel robot is designed and its equilibrium configurations under several types of loadings are analyzed. Its dynamics is also studied. We analyze the impact force occurring during a collision between our prototype and the head of an operator and compare these results with those that would have been obtained with a rigid five-bar mechanism. Simulation results of impact during a standard pick-and-place trajectory of duration 0.3 s show that a regular five-bar mechanism would injure a human, while our robot would avoid the trauma.
ER  - 

TY  - CONF
TI  - High-Flexibility Locomotion and Whole-Torso Control for a Wheel-Legged Robot on Challenging Terrain*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10372
EP  - 10377
AU  - K. Xu
AU  - S. Wang
AU  - X. Wang
AU  - J. Wang
AU  - Z. Chen
AU  - D. Liu
PY  - 2020
KW  - mobile robots
KW  - motion control
KW  - robot kinematics
KW  - wheels
KW  - high-flexibility locomotion
KW  - whole-torso control
KW  - challenging terrain
KW  - six-wheel-legged robot
KW  - irregular terrain
KW  - heavy-duty work
KW  - Stewart platforms
KW  - wheels
KW  - diverse degrees
KW  - traversability
KW  - rough terrain
KW  - sand-gravel terrain
KW  - parallel suspension system
KW  - Legged locomotion
KW  - Wheels
KW  - Torso
KW  - Force
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197526
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a parallel six-wheel-legged robot that can traverse irregular terrain while carrying objectives to do heavy-duty work. This robot is equipped with six Stewart platforms as legs and tightly integrates the additional degrees of freedom introduced by the wheels. The presented control strategy with physical system used to adapt the diverse degrees of each leg to irregular terrain such that robot increases the traversability, and simultaneously to maintain the horizontal whole-torso pose. This strategy makes use of Contact Scheduler (CS) and Whole-Torso Control (WTC) to control the multiple degrees of freedom (DOF) leg for performing high-flexibility locomotion and adapting the rough terrain like actively parallel suspension system. We conducted experiments on flat, slope, soft and sand-gravel surface, which validate the proposed control method and physical system. Especially, we attempt to traverse over sand-gravel terrain with 3 people about 240kg payload.
ER  - 

TY  - CONF
TI  - The Prince’s tears, a large cable-driven parallel robot for an artistic exhibition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10378
EP  - 10383
AU  - J. -P. Merlet
AU  - Y. Papegay
AU  - A. -V. Gasc
PY  - 2020
KW  - exhibitions
KW  - mobile robots
KW  - trajectory control
KW  - cable length estimation
KW  - robot position
KW  - glass powder
KW  - cable-driven parallel robot
KW  - artistic exhibition
KW  - positioning control
KW  - CDPR geometry
KW  - exhibition place
KW  - Prince tears
KW  - time 3.0 d
KW  - time 174.0 hour
KW  - time 32.0 d
KW  - mass 1.5 ton
KW  - Meters
KW  - Length measurement
KW  - Winches
KW  - Powders
KW  - Kinematics
KW  - Robot kinematics
KW  - cable-driven parallel robot
KW  - kinematics
KW  - art
DO  - 10.1109/ICRA40945.2020.9197011
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the development and results of a large 3 d.o.f cable-driven parallel robot (CDPR) that has been extensively used between June and August 2019 for an artistic exhibition. The purpose of the exhibition was to 3D print a wall of glass powder, which will slowly collapse after the deposit of each layer. Positioning control on the assigned trajectory was an issue because of the CDPR geometry imposed by the specific configuration of the exhibition place. We describe how this problem was solved using a combination of cable length estimation based on the winch rotation measured by encoder, together with 3 on-board lidars that were used to provide a measure of the robot position. To the best of our knowledge this is the first time that such method was used for controlling a large CDPR. This CDPR has run for 174 hours since 6/18/2019, averaging a run time of 4h15mn per day. The 3D printing of the wall started on 7/18/2019 and stops on 8/31/2019. During this period the robot was used for 32 days with an average of 2h18mn run-time per day. The robot has traveled on a total distance of 4757 meters, of which 3893 meters on the assigned trajectory. During the period 76 layers have been deposited, representing a mass of 1.5 tons of glass powder.
ER  - 

TY  - CONF
TI  - Singularity analysis and reconfiguration mode of the 3-CRS parallel manipulator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10384
EP  - 10390
AU  - C. Bouzgarrou
AU  - A. Koessler
AU  - N. Bouton
PY  - 2020
KW  - manipulator kinematics
KW  - reconfiguration mode
KW  - 3-CRS parallel manipulator
KW  - original parallel mechanism
KW  - motorized cylindrical joint
KW  - parallel robotics community
KW  - dimensional synthesis
KW  - geometric approach
KW  - relative geometric configurations
KW  - singularity analysis problem
KW  - Manipulators
KW  - Kinematics
KW  - Transmission line matrix methods
KW  - Force
KW  - Geometry
KW  - Prototypes
DO  - 10.1109/ICRA40945.2020.9197337
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The 3-CRS manipulator is an original parallel mechanism having 6 degrees of freedom (DOFs) with only 3 limbs. This mechanism uses a motorized cylindrical joint per limb. This new paradigm of actuation opens research fields on new families of robots that should particularly interest the parallel robotics community. According to its dimensional synthesis, this mechanism can have remarkable kinematic properties such as a large orientation workspace or reconfiguration capabilities. In this paper, we introduce this mechanism and we study its singularities by using a geometric approach. This approach simplifies considerably singularity analysis problem by considering the relative geometric configurations of three planes defined by the distal links of the limbs. Thanks to that, a reconfiguration mode of the 3-CRS, that doubles its reachable workspace, is highlighted. This property is illustrated on a physical prototype of the robot.
ER  - 

TY  - CONF
TI  - Trajectory optimization for a class of robots belonging to Constrained Collaborative Mobile Agents (CCMA) family
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10391
EP  - 10397
AU  - N. Kumar
AU  - S. Coros
PY  - 2020
KW  - actuators
KW  - collision avoidance
KW  - end effectors
KW  - manipulator kinematics
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - position control
KW  - constrained collaborative mobile agents family
KW  - ground mobile bases
KW  - mobile robots
KW  - closed-loop kinematic chains
KW  - revolute joints
KW  - closed- loop kinematic chains
KW  - standalone trajectory optimization method
KW  - CCMA system
KW  - fixed design parameters
KW  - control policy optimization
KW  - manipulation capabilities
KW  - tracked mobile bases
KW  - Kinematics
KW  - Mobile robots
KW  - Trajectory optimization
KW  - Mobile agents
KW  - Task analysis
KW  - Parallel Robots
KW  - Optimization and Optimal Control
KW  - Multi-Robot Systems
DO  - 10.1109/ICRA40945.2020.9197048
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel class of robots belonging to Constrained Collaborative Mobile Agents (CCMA) family which consists of ground mobile bases with non-holonomic constraints. Moreover, these mobile robots are constrained by closed-loop kinematic chains consisting of revolute joints which can be either passive or actuated. We also describe a novel trajectory optimization method which is general with respect to number of mobile robots, topology of the closed- loop kinematic chains and placement of the actuators at the revolute joints. We also extend the standalone trajectory optimization method to optimize concurrently the design parameters and the control policy. We describe various CCMA system examples, in simulation, differing in design, topology, number of mobile robots and actuation space. The simulation results for standalone trajectory optimization with fixed design parameters is presented for CCMA system examples. We also show how this method can be used for tasks other than end-effector positioning such as internal collision avoidance and external obstacle avoidance. The concurrent design and control policy optimization is demonstrated, in simulations, to increase the CCMA system workspace and manipulation capabilities. Finally, the trajectory optimization method is validated in experiments through two 4-DOF prototypes consisting of 3 tracked mobile bases.
ER  - 

TY  - CONF
TI  - Development of Body Rotational Wheeled Robot and its Verification of Effectiveness
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10405
EP  - 10411
AU  - B. -S. Sim
AU  - K. -J. Kim
AU  - K. -H. Yu
PY  - 2020
KW  - collision avoidance
KW  - friction
KW  - mobile robots
KW  - traction
KW  - wheels
KW  - step-obstacle climbing
KW  - body rotational wheeled robot
KW  - scattered obstacles
KW  - driving environment
KW  - step-type obstacle
KW  - main body rotation mechanism
KW  - robot wheels
KW  - wheel-drive robot
KW  - body mass
KW  - slope traveling
KW  - downhill wheel
KW  - mechanical effect
KW  - robot platform
KW  - Mobile robots
KW  - Wheels
KW  - Force
KW  - Gears
KW  - Friction
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197047
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A wheeled robot operating on various terrains such as scattered obstacles and slopes is required to cope with and overcome the driving environment. In this paper, in order to overcome a step-type obstacle and to steadily ascend on the slope, the main body rotation mechanism, which controls the load distribution on the robot wheels was proposed for a wheel-drive robot. By rotating the center of the body mass, the friction/traction force required for climbing step obstacles can be reduced. In the case of slope traveling, the slip can be suppressed, and the traveling ability improved by controlling the load distribution excessively increased on the downhill wheel due to the attitude change of the robot's body. The mechanical effect of the proposed body rotation mechanism was analyzed. In addition, based on the design and manufacture of the robot platform, the effectiveness of the proposed mechanism was convincingly demonstrated by indoor test for step-obstacle climbing and slope-traveling.
ER  - 

TY  - CONF
TI  - Radar Sensors in Collaborative Robotics: Fast Simulation and Experimental Validation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10452
EP  - 10458
AU  - C. Stetco
AU  - B. Ubezio
AU  - S. Mühlbacher-Karrer
AU  - H. Zangl
PY  - 2020
KW  - collision avoidance
KW  - CW radar
KW  - FM radar
KW  - frequency modulation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - radar computing
KW  - road vehicle radar
KW  - sensors
KW  - collaborative robotics
KW  - radar systems
KW  - robot systems
KW  - optimization
KW  - machine learning approaches
KW  - realistic simulation models
KW  - radar sensor simulations
KW  - relative velocities
KW  - Lambertian reflectance model
KW  - reflection estimates
KW  - frequency modulated continuous wave radar
KW  - simulation environments
KW  - Radar
KW  - Robot sensing systems
KW  - Radar antennas
KW  - Chirp
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9197180
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - With the availability of small system in package realizations, radar systems become more and more attractive for a variety of applications in robotics, in particular also for collaborative robotics. As the simulation of robot systems in realistic scenarios has become an important tool, not only for design and optimization, but also e.g. for machine learning approaches, realistic simulation models are needed. In the case of radar sensor simulations, this means providing more realistic results than simple proximity sensors, e.g. in the presence of multiple objects and/or humans, objects with different relative velocities and differentiation between background and foreground movement. Due to the short wavelength in the millimeter range, we propose to utilize methods known from computer graphics (e.g. z-buffer, Lambertian reflectance model) to quickly acquire depth images and reflection estimates. This information is used to calculate an estimate of the received signal for a Frequency Modulated Continuous Wave (FMCW) radar by superposition of the corresponding signal contributions. Due to the moderate computational complexity, the approach can be used with various simulation environments such as V-Rep or Gazebo. Validity and benefits of the approach are demonstrated by means of a comparison with experimental data obtained with a radar sensor on a UR10 arm in different scenarios.
ER  - 

TY  - CONF
TI  - Transferable Task Execution from Pixels through Deep Planning Domain Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10459
EP  - 10465
AU  - K. Kase
AU  - C. Paxton
AU  - H. Mazhar
AU  - T. Ogata
AU  - D. Fox
PY  - 2020
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - robot vision
KW  - symbolic operators
KW  - manipulation tasks
KW  - transferable task execution
KW  - visual input
KW  - symbolic planning methods
KW  - partially-observable world
KW  - hierarchical model
KW  - high-level model
KW  - deep planning domain learning
KW  - symbolic world state
KW  - DPDL
KW  - STRIPS
KW  - logical predicates
KW  - low-level policy learning
KW  - photorealistic kitchen scenario
KW  - Task analysis
KW  - Planning
KW  - Robot sensing systems
KW  - Grounding
KW  - Robustness
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196597
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While robots can learn models to solve many manipulation tasks from raw visual input, they cannot usually use these models to solve new problems. On the other hand, symbolic planning methods such as STRIPS have long been able to solve new problems given only a domain definition and a symbolic goal, but these approaches often struggle on the real world robotic tasks due to the challenges of grounding these symbols from sensor data in a partially-observable world. We propose Deep Planning Domain Learning (DPDL), an approach that combines the strengths of both methods to learn a hierarchical model. DPDL learns a high-level model which predicts values for a large set of logical predicates consisting of the current symbolic world state, and separately learns a low-level policy which translates symbolic operators into executable actions on the robot. This allows us to perform complex, multistep tasks even when the robot has not been explicitly trained on them. We show our method on manipulation tasks in a photorealistic kitchen scenario.
ER  - 


