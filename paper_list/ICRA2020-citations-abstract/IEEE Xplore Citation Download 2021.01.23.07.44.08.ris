TY  - CONF
TI  - ICRA 2020 Table of Contents
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - i
EP  - lvi
PY  - 2020
DO  - 10.1109/ICRA40945.2020.9196768
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Presents the table of contents/splash page of the proceedings record.
ER  - 

TY  - CONF
TI  - Metrically-Scaled Monocular SLAM using Learned Scale Factors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 43
EP  - 50
AU  - W. N. Greene
AU  - N. Roy
PY  - 2020
KW  - cameras
KW  - graph theory
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - SLAM (robots)
KW  - geometric SLAM factor graph
KW  - SLAM systems
KW  - relative geometry
KW  - learned depth estimation approaches
KW  - learned depth predictions
KW  - image space
KW  - network architecture
KW  - coarse images
KW  - GPU acceleration
KW  - learned metric data
KW  - unary scale factors
KW  - hardware accelerators
KW  - observable epipolar geometry
KW  - monocular SLAM
KW  - learned scale factors
KW  - monocular simultaneous localization and mapping
KW  - hardware acceleration
KW  - neural network
KW  - Simultaneous localization and mapping
KW  - Feature extraction
KW  - Cameras
KW  - Loss measurement
KW  - Neural networks
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9196900
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose an efficient method for monocular simultaneous localization and mapping (SLAM) that is capable of estimating metrically-scaled motion without additional sensors or hardware acceleration by integrating metric depth predictions from a neural network into a geometric SLAM factor graph. Unlike learned end-to-end SLAM systems, ours does not ignore the relative geometry directly observable in the images. Unlike existing learned depth estimation approaches, ours leverages the insight that when used to estimate scale, learned depth predictions need only be coarse in image space. This allows us to shrink our network to the point that performing inference on a standard CPU becomes computationally tractable.We make several improvements to our network architecture and training procedure to address the lack of depth observability when using coarse images, which allows us to estimate spatially coarse, but depth-accurate predictions in only 30 ms per frame without GPU acceleration. At runtime we incorporate the learned metric data as unary scale factors in a Sim(3) pose graph. Our method is able to generate accurate, scaled poses without additional sensors, hardware accelerators, or special maneuvers and does not ignore or corrupt the observable epipolar geometry. We show compelling results on the KITTI benchmark dataset in addition to real-world experiments with a handheld camera.
ER  - 

TY  - CONF
TI  - Inertial-Only Optimization for Visual-Inertial Initialization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 51
EP  - 57
AU  - C. Campos
AU  - J. M. M. Montiel
AU  - J. D. Tardós
PY  - 2020
KW  - feature extraction
KW  - least squares approximations
KW  - maximum likelihood estimation
KW  - optimisation
KW  - SLAM (robots)
KW  - EuRoC dataset show
KW  - time visual-inertial initialization
KW  - optimal estimation problem
KW  - maximum-a-posteriori estimation
KW  - algebraic equations
KW  - ad-hoc cost functions
KW  - ORB-SLAM visual-inertial boosting
KW  - inertial-only optimization
KW  - IMU measurement uncertainty
KW  - MAP estimation
KW  - least squares
KW  - Estimation
KW  - Trajectory
KW  - Simultaneous localization and mapping
KW  - Gravity
KW  - Visualization
KW  - Optimization
KW  - Accelerometers
DO  - 10.1109/ICRA40945.2020.9197334
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We formulate for the first time visual-inertial initialization as an optimal estimation problem, in the sense of maximum-a-posteriori (MAP) estimation. This allows us to properly take into account IMU measurement uncertainty, which was neglected in previous methods that either solved sets of algebraic equations, or minimized ad-hoc cost functions using least squares. Our exhaustive initialization tests on EuRoC dataset show that our proposal largely outperforms the best methods in the literature, being able to initialize in less than 4 seconds in almost any point of the trajectory, with a scale error of 5.3% on average. This initialization has been integrated into ORB-SLAM Visual-Inertial boosting its robustness and efficiency while maintaining its excellent accuracy.
ER  - 

TY  - CONF
TI  - Hierarchical Quadtree Feature Optical Flow Tracking Based Sparse Pose-Graph Visual-Inertial SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 58
EP  - 64
AU  - H. Xie
AU  - W. Chen
AU  - J. Wang
AU  - H. Wang
PY  - 2020
KW  - computational complexity
KW  - graph theory
KW  - image sequences
KW  - optimisation
KW  - pose estimation
KW  - quadtrees
KW  - pose-graph optimization time cost
KW  - localization accuracy
KW  - sparse pose-graph visual-inertial SLAM algorithms
KW  - hierarchical quadtree feature optical flow tracking algorithm
KW  - SPVIS
KW  - high-precision pose estimation
KW  - computational complexity
KW  - VIO-VI-SLAM system
KW  - GPU
KW  - Optical flow
KW  - Optimization
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Tracking
KW  - Feature extraction
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197278
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate, robust and real-time localization under constrained-resources is a critical problem to be solved. In this paper, we present a new sparse pose-graph visual-inertial SLAM (SPVIS). Unlike the existing methods that are costly to deal with a large number of redundant features and 3D map points, which are inefficient for improving positioning accuracy, we focus on the concise visual cues for high-precision pose estimating. We propose a novel hierarchical quadtree based optical flow tracking algorithm, it achieves high accuracy and robustness within very few concise features, which is only about one fifth features of the state-of-the-art visual-inertial SLAM algorithms. Benefiting from the efficient optical flow tracking, our sparse pose-graph optimization time cost achieves bounded complexity. By selecting and optimizing the informative features in sliding window and local VIO, the computational complexity is bounded, it achieves low time cost in long-term operation. We compare with the state-of-the-art VIO/VI-SLAM systems on the challenging public datasets by the embedded platform without GPUs, the results effectively verify that the proposed method has better real-time performance and localization accuracy.
ER  - 

TY  - CONF
TI  - Keypoint Description by Descriptor Fusion Using Autoencoders
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 65
EP  - 71
AU  - Z. Dai
AU  - X. Huang
AU  - W. Chen
AU  - C. Chen
AU  - L. He
AU  - S. Wen
AU  - H. Zhang
PY  - 2020
KW  - convolutional neural nets
KW  - image fusion
KW  - image matching
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - SLAM (robots)
KW  - keypoint description
KW  - keypoint matching
KW  - computer vision
KW  - visual simultaneous localization and mapping
KW  - SLAM
KW  - matching operation
KW  - descriptor fusion model
KW  - robust keypoint descriptor
KW  - CNN-based descriptors
KW  - DFM architecture
KW  - CNN models
KW  - mean mAP
KW  - HardNet
KW  - DenseNet169
KW  - convolutional neural networks
KW  - Fuses
KW  - Lighting
KW  - Robustness
KW  - Computer vision
KW  - Simultaneous localization and mapping
KW  - Image coding
DO  - 10.1109/ICRA40945.2020.9197205
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45% and 6.53% higher than HardNet and DenseNet169, respectively.
ER  - 

TY  - CONF
TI  - Towards Noise Resilient SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 72
EP  - 79
AU  - A. Thyagharajan
AU  - O. J. Omer
AU  - D. Mandal
AU  - S. Subramoney
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image sensors
KW  - optimisation
KW  - photometry
KW  - pose estimation
KW  - SLAM (robots)
KW  - stereo image processing
KW  - RGB-D input
KW  - TUM datasets
KW  - EuRoC datasets
KW  - stereo image pairs
KW  - adaptive algorithm
KW  - error vector
KW  - outlier rejection
KW  - computational efficiency
KW  - map-point consensus
KW  - adaptive virtual camera
KW  - noise resilient SLAM
KW  - ORB-SLAM2
KW  - sparse-indirect SLAM systems
KW  - virtual camera location
KW  - axial depth error
KW  - pose optimization
KW  - consensus information
KW  - axial noise
KW  - lateral noise
KW  - depth noise components
KW  - axial components
KW  - lateral components
KW  - noise sources
KW  - scale information
KW  - SLAM frameworks
KW  - depth sensors
KW  - photometric invariance properties
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Measurement
KW  - Feature extraction
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9196745
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Sparse-indirect SLAM systems have been dominantly popular due to their computational efficiency and photometric invariance properties. Depth sensors are critical to SLAM frameworks for providing scale information to the 3D world, yet known to be plagued by a wide variety of noise sources, possessing lateral and axial components. In this work, we demonstrate the detrimental impact of these depth noise components on the performance of the state-of-the-art sparse-indirect SLAM system (ORB-SLAM2). We propose (i) Map-Point Consensus based Outlier Rejection (MC-OR) to counter lateral noise, and (ii) Adaptive Virtual Camera (AVC) to combat axial noise accurately. MC-OR utilizes consensus information between multiple sightings of the same landmark to disambiguate noisy depth and filter it out before pose optimization. In AVC, we introduce an error vector as an accurate representation of the axial depth error. We additionally propose an adaptive algorithm to find the virtual camera location for projecting the error used in the objective function of the pose optimization. Our techniques work equally well for stereo image pairs and RGB-D input directly used by sparse-indirect SLAM systems. Our methods were tested on the TUM (RGB-D) and EuRoC (stereo) datasets and we show that they outperform existing state-of-the-art ORB-SLAM2 by 2-3x, especially in sequences critically affected by depth noise.
ER  - 

TY  - CONF
TI  - LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of Perceptually-Degraded Subterranean Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 80
EP  - 86
AU  - K. Ebadi
AU  - Y. Chang
AU  - M. Palieri
AU  - A. Stephens
AU  - A. Hatteland
AU  - E. Heiden
AU  - A. Thakur
AU  - N. Funabiki
AU  - B. Morrell
AU  - S. Wood
AU  - L. Carlone
AU  - A. -a. Agha-mohammadi
PY  - 2020
KW  - distance measurement
KW  - geophysical image processing
KW  - mobile robots
KW  - multi-robot systems
KW  - optical radar
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - terrain mapping
KW  - tunnels
KW  - long corridors
KW  - salient features
KW  - spurious loop closures
KW  - repetitive appearance
KW  - stark contrast
KW  - highly-accurate 3D maps
KW  - underground extraterrestrial worlds
KW  - lidar-based multirobot SLAM system
KW  - DARPA subterranean challenge
KW  - subterranean operation
KW  - accurate lidar-based front-end
KW  - perceptually-degraded subterranean environments
KW  - complex subterranean environments
KW  - off-nominal conditions
KW  - uneven terrains
KW  - slippery terrains
KW  - large-scale autonomous mapping-positioning
KW  - simultaneous localization and mapping
KW  - unknown subterranean environment
KW  - large-scale subterranean environment
KW  - complex subterranean environment
KW  - inaccurate wheel odometry
KW  - disaster response
KW  - flexible back-end
KW  - robust back-end
KW  - tunnel circuit
KW  - Simultaneous localization and mapping
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Base stations
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197082
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Simultaneous Localization and Mapping (SLAM) in large-scale, unknown, and complex subterranean environments is a challenging problem. Sensors must operate in off-nominal conditions; uneven and slippery terrains make wheel odometry inaccurate, while long corridors without salient features make exteroceptive sensing ambiguous and prone to drift; finally, spurious loop closures that are frequent in environments with repetitive appearance, such as tunnels and mines, could result in a significant distortion of the entire map. These challenges are in stark contrast with the need to build highly-accurate 3D maps to support a wide variety of applications, ranging from disaster response to the exploration of underground extraterrestrial worlds. This paper reports on the implementation and testing of a lidar-based multi-robot SLAM system developed in the context of the DARPA Subterranean Challenge. We present a system architecture to enhance subterranean operation, including an accurate lidar-based front-end, and a flexible and robust back-end that automatically rejects outlying loop closures. We present an extensive evaluation in large-scale, challenging subterranean environments, including the results obtained in the Tunnel Circuit of the DARPA Subterranean Challenge. Finally, we discuss potential improvements, limitations of the state of the art, and future research directions.
ER  - 

TY  - CONF
TI  - BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 87
EP  - 93
AU  - A. Harakeh
AU  - M. Smart
AU  - S. L. Waslander
PY  - 2020
KW  - Bayes methods
KW  - Gaussian processes
KW  - neural nets
KW  - object detection
KW  - NonMaximum suppression components
KW  - common object detection datasets
KW  - BayesOD
KW  - minimum Gaussian uncertainty error metric
KW  - minimum Categorical uncertainty error metric
KW  - Bayesian approach
KW  - deep object detectors
KW  - deep neural networks
KW  - uncertainty measures
KW  - output predictions
KW  - detectors nonmaximum suppression stage
KW  - anchor-based object detection
KW  - uncertainty estimation approach
KW  - standard object detector inference
KW  - Uncertainty
KW  - Detectors
KW  - Neural networks
KW  - Bayes methods
KW  - Estimation
KW  - Object detection
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9196544
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When incorporating deep neural networks into robotic systems, a major challenge is the lack of uncertainty measures associated with their output predictions. Methods for uncertainty estimation in the output of deep object detectors (DNNs) have been proposed in recent works, but have had limited success due to 1) information loss at the detectors nonmaximum suppression (NMS) stage, and 2) failure to take into account the multitask, many-to-one nature of anchor-based object detection. To that end, we introduce BayesOD, an uncertainty estimation approach that reformulates the standard object detector inference and Non-Maximum suppression components from a Bayesian perspective. Experiments performed on four common object detection datasets show that BayesOD provides uncertainty estimates that are better correlated with the accuracy of detections, manifesting as a significant reduction of 9.77%-13.13% on the minimum Gaussian uncertainty error metric and a reduction of 1.63%-5.23% on the minimum Categorical uncertainty error metric. Code will be released at https://github.com/asharakeh/bayes-od-rc.
ER  - 

TY  - CONF
TI  - Learning Object Placements For Relational Instructions by Hallucinating Scene Representations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 94
EP  - 100
AU  - O. Mees
AU  - A. Emek
AU  - J. Vertens
AU  - W. Burgard
PY  - 2020
KW  - control engineering computing
KW  - convolutional neural nets
KW  - human-robot interaction
KW  - image classification
KW  - image representation
KW  - learning (artificial intelligence)
KW  - probability
KW  - robot vision
KW  - object placement learning
KW  - relational instructions
KW  - spatial relation
KW  - convolutional neural network
KW  - pixelwise object placement probability estimation
KW  - hallucinated high-level scene representation classification
KW  - pixelwise relational probabilities
KW  - human-robot experiments
KW  - single input image
KW  - learning signal
KW  - 3D models
KW  - Robots
KW  - Training
KW  - Three-dimensional displays
KW  - Natural languages
KW  - Graphical models
KW  - Distribution functions
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9197472
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots coexisting with humans in their environment and performing services for them need the ability to interact with them. One particular requirement for such robots is that they are able to understand spatial relations and can place objects in accordance with the spatial relations expressed by their user. In this work, we present a convolutional neural network for estimating pixelwise object placement probabilities for a set of spatial relations from a single input image. During training, our network receives the learning signal by classifying hallucinated high-level scene representations as an auxiliary task. Unlike previous approaches, our method does not require ground truth data for the pixelwise relational probabilities or 3D models of the objects, which significantly expands the applicability in practical applications. Our results obtained using real-world data and human-robot experiments demonstrate the effectiveness of our method in reasoning about the best way to place objects to reproduce a spatial relation. Videos of our experiments can be found at https://youtu.be/zaZkHTWFMKM.
ER  - 

TY  - CONF
TI  - FADNet: A Fast and Accurate Network for Disparity Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 101
EP  - 107
AU  - Q. Wang
AU  - S. Shi
AU  - S. Zheng
AU  - K. Zhao
AU  - X. Chu
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - image matching
KW  - stereo image processing
KW  - deep neural networks
KW  - computer vision
KW  - disparity estimation problem
KW  - stereo matching
KW  - traditional hand-crafted feature based methods
KW  - designed DNNs
KW  - computation resources
KW  - 3D convolution based networks
KW  - real-time applications
KW  - computation-efficient networks
KW  - expression capability
KW  - large-scale datasets
KW  - multiscale predictions
KW  - FADNet
KW  - multiscale weight scheduling training technique
KW  - Estimation
KW  - Convolution
KW  - Correlation
KW  - Three-dimensional displays
KW  - Training
KW  - Feature extraction
KW  - Computer architecture
DO  - 10.1109/ICRA40945.2020.9197031
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy in stereo matching than traditional hand-crafted feature based methods. On one hand, however, the designed DNNs require significant memory and computation resources to accurately predict the disparity, especially for those 3D convolution based networks, which makes it difficult for deployment in real-time applications. On the other hand, existing computation-efficient networks lack expression capability in large-scale datasets so that they cannot make an accurate prediction in many scenarios. To this end, we propose an efficient and accurate deep network for disparity estimation named FADNet with three main features: 1) It exploits efficient 2D based correlation layers with stacked blocks to preserve fast computation; 2) It combines the residual structures to make the deeper model easier to learn; 3) It contains multi-scale predictions so as to exploit a multi-scale weight scheduling training technique to improve the accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on two popular datasets, Scene Flow and KITTI 2015. Experimental results show that FADNet achieves state-of-the-art prediction accuracy, and runs at a significant order of magnitude faster speed than existing 3D models. The codes of FADNet are available at https://github.com/HKBU-HPML/FADNet.
ER  - 

TY  - CONF
TI  - Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 108
EP  - 114
AU  - S. Kuutti
AU  - S. Fallah
AU  - R. Bowden
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - adversarial agent training
KW  - learned control policies
KW  - autonomous driving
KW  - deep neural network-driven
KW  - adversarial reinforcement learning agent
KW  - autonomous vehicle problem
KW  - automated black box testing
KW  - safety-critical applications
KW  - control policy
KW  - deep neural networks
KW  - autonomous vehicles
KW  - robot navigation
KW  - robotic arm manipulation
KW  - control problems
KW  - deep learning
KW  - deep control policies
KW  - Testing
KW  - Autonomous vehicles
KW  - Training
KW  - Learning (artificial intelligence)
KW  - Machine learning
KW  - Robots
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197351
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.
ER  - 

TY  - CONF
TI  - TRASS: Time Reversal as Self-Supervision
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 115
EP  - 121
AU  - S. Nair
AU  - M. Babaeizadeh
AU  - C. Finn
AU  - S. Levine
AU  - V. Kumar
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - optimal control
KW  - predictive control
KW  - robot programming
KW  - robot vision
KW  - self-supervision technique
KW  - high level supervision
KW  - time-reversal model
KW  - self-supervised model
KW  - goal states
KW  - complex manipulation tasks
KW  - tetris-style block pairs
KW  - visual model predictive control
KW  - robot learning
KW  - RGB camera input
KW  - TRASS
KW  - Time Reversal as Self-Supervision
KW  - Task analysis
KW  - Trajectory
KW  - Robots
KW  - Transmission line measurements
KW  - Visualization
KW  - Predictive models
KW  - Grippers
DO  - 10.1109/ICRA40945.2020.9196862
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning based approaches have shown promise in producing robust policies, but require heavy supervision and large number of environment interactions, especially from visual inputs. We propose a novel self-supervision technique that uses time-reversal to provide high level supervision to reach goals. In particular, we introduce the time-reversal model (TRM), a self-supervised model which explores outward from a set of goal states and learns to predict these trajectories in reverse. This provides a high level plan towards goals, allowing us to learn complex manipulation tasks with no demonstrations or exploration at test time. We test our method on the domain of assembly, specifically the mating of tetris-style block pairs. Using our method operating atop visual model predictive control, we are able to assemble tetris blocks on a KuKa IIWA-7 using only uncalibrated RGB camera input, and generalize to unseen block pairs. Project's-page: https://sites.google.com/view/time-reversal.
ER  - 

TY  - CONF
TI  - Advanced BIT* (ABIT*): Sampling-Based Planning with Advanced Graph-Search Techniques
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 130
EP  - 136
AU  - M. P. Strub
AU  - J. D. Gammell
PY  - 2020
KW  - approximation theory
KW  - graph theory
KW  - path planning
KW  - robots
KW  - sampling methods
KW  - search problems
KW  - sampling-based approximation
KW  - Advanced BIT*
KW  - almost-surely asymptotically optimal sampling-based planners
KW  - ABIT*
KW  - unified planning paradigm
KW  - path planning problem
KW  - advanced graph-search techniques
KW  - robotics
KW  - truncated anytime graph-based searches
KW  - RRT*
KW  - single-query
KW  - Planning
KW  - Approximation algorithms
KW  - Search problems
KW  - Path planning
KW  - Robots
KW  - Acceleration
KW  - Conferences
DO  - 10.1109/ICRA40945.2020.9196580
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path planning is an active area of research essential for many applications in robotics. Popular techniques include graph-based searches and sampling-based planners. These approaches are powerful but have limitations.This paper continues work to combine their strengths and mitigate their limitations using a unified planning paradigm. It does this by viewing the path planning problem as the two subproblems of search and approximation and using advanced graph-search techniques on a sampling-based approximation.This perspective leads to Advanced BIT*. ABIT* combines truncated anytime graph-based searches, such as ATD*, with anytime almost-surely asymptotically optimal sampling-based planners, such as RRT*. This allows it to quickly find initial solutions and then converge towards the optimum in an anytime manner. ABIT* outperforms existing single-query, sampling- based planners on the tested problems in ℝ4 and ℝ8, and was demonstrated on real-world problems with NASA/JPL-Caltech.
ER  - 

TY  - CONF
TI  - Voxel-based General Voronoi Diagram for Complex Data with Application on Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 137
EP  - 143
AU  - S. Dorn
AU  - N. Wolpert
AU  - E. Schömer
PY  - 2020
KW  - assembly planning
KW  - CAD
KW  - computational geometry
KW  - data structures
KW  - graphics processing units
KW  - path planning
KW  - production engineering computing
KW  - rendering (computer graphics)
KW  - path planning
KW  - motion planning
KW  - assembly sequence planning
KW  - real-world CAD-scenarios
KW  - disassembly path
KW  - GVD
KW  - Voronoi voxel history
KW  - disassembly paths
KW  - general Voronoi diagram graph
KW  - hash table-based data structure
KW  - error-bounded wavefront propagation
KW  - error-bounded GPU render approach
KW  - voxel-based general Voronoi diagram
KW  - roadmap
KW  - representative vehicle data set
KW  - Octrees
KW  - Planning
KW  - Three-dimensional displays
KW  - Approximation algorithms
KW  - Task analysis
KW  - Runtime
DO  - 10.1109/ICRA40945.2020.9196775
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - One major challenge in Assembly Sequence Planning (ASP) for complex real-world CAD-scenarios is to find appropriate disassembly paths for all assembled parts. Such a path places demands on its length and clearance. In the past, it became apparent that planning the disassembly path based on the (approximate) General Voronoi Diagram (GVD) is a good approach to achieve these requirements. But for complex real-world data, every known solution for computing the GVD is either too slow or very memory consuming, even if only approximating the GVD.We present a new approach for computing the approximate GVD and demonstrate its practicability using a representative vehicle data set. We can calculate an approximation of the GVD within minutes and meet the accuracy requirement of some few millimeters for the subsequent path planning. This is achieved by voxelizing the surface with a common error-bounded GPU render approach. We then use an error-bounded wavefront propagation technique and combine it with a novel hash table-based data structure, the so-called Voronoi Voxel History (VVH). On top of the GVD, we present a novel approach for the creation of a General Voronoi Diagram Graph (GVDG) that leads to an extensive roadmap. For the later motion planning task this roadmap can be used to suggest appropriate disassembly paths.
ER  - 

TY  - CONF
TI  - Dynamic Movement Primitives for moving goals with temporal scaling adaptation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 144
EP  - 150
AU  - L. Koutras
AU  - Z. Doulgeri
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - human robot collaboration
KW  - motion profiles
KW  - learned kinematic pattern
KW  - KUKA LWR4+ robot
KW  - DMP
KW  - dynamic movement primitives framework
KW  - adaptive temporal scaling
KW  - static goal
KW  - temporal scaling adaptation
KW  - moving goal
KW  - Trajectory
KW  - Robots
KW  - Dynamics
KW  - Encoding
KW  - Collaboration
KW  - Adaptation models
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196765
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we propose an augmentation to the Dynamic Movement Primitives (DMP) framework which allows the system to generalize to moving goals without the use of any known or approximation model for estimating the goal's motion. We aim to maintain the demonstrated velocity levels during the execution to the moving goal, generating motion profiles appropriate for human robot collaboration. The proposed method employs a modified version of a DMP, learned by a demonstration to a static goal, with adaptive temporal scaling in order to achieve reaching of the moving goal with the learned kinematic pattern. Only the current position and velocity of the goal are required. The goal's reaching error and its derivative is proved to converge to zero via contraction analysis. The theoretical results are verified by simulations and experiments on a KUKA LWR4+ robot.
ER  - 

TY  - CONF
TI  - Navigating Discrete Difference Equation Governed WMR by Virtual Linear Leader Guided HMPC
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 151
EP  - 157
AU  - C. Huang
AU  - X. Chen
AU  - E. Tang
AU  - M. He
AU  - L. Bu
AU  - S. Qin
AU  - Y. Zeng
PY  - 2020
KW  - difference equations
KW  - mobile robots
KW  - multi-robot systems
KW  - nonlinear control systems
KW  - path planning
KW  - predictive control
KW  - reachability analysis
KW  - set theory
KW  - navigating discrete difference equation governed WMR
KW  - virtual linear leader guided HMPC
KW  - model predictive control
KW  - classical wheeled mobile robot navigation problem
KW  - hierarchical MPC
KW  - state-of-the-art MPC
KW  - WMR navigation
KW  - nonexistence
KW  - nontrivial linear system
KW  - under-approximate reachable set
KW  - VLL-MPC
KW  - HMPC structure
KW  - virtual linear system
KW  - under-approximate path
KW  - RRT*
KW  - Navigation
KW  - Linear systems
KW  - Stability analysis
KW  - Planning
KW  - Robots
KW  - Mathematical model
KW  - Nonlinear dynamical systems
DO  - 10.1109/ICRA40945.2020.9197375
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we revisit model predictive control (MPC) for the classical wheeled mobile robot (WMR) navigation problem. We prove that the reachable set based hierarchical MPC (HMPC), a state-of-the-art MPC, cannot handle WMR navigation in theory due to the non-existence of non-trivial linear system with an under-approximate reachable set of WMR. Nevertheless, we propose a virtual linear leader guided MPC (VLL-MPC) to enable HMPC structure. Different from current HMPCs, we use a virtual linear system with an under-approximate path set rather than the traditional trace set to guide the WMR. We provide a valid construction of the virtual linear leader. We prove the stability of VLL-MPC, and discuss its complexity. In the experiment, we demonstrate the advantage of VLL-MPC empirically by comparing it with NMPC, LMPC and anytime RRT* in several scenarios.
ER  - 

TY  - CONF
TI  - Aggregation and localization of simple robots in curved environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 165
EP  - 171
AU  - R. A. Moan
AU  - V. M. Baez
AU  - A. T. Becker
AU  - J. M. O’Kane
PY  - 2020
KW  - friction
KW  - microrobots
KW  - mobile robots
KW  - path planning
KW  - curved environments
KW  - extremely simple robots
KW  - biomedical applications
KW  - tiny robots moves
KW  - shared external stimulus
KW  - low-friction models
KW  - environment boundaries
KW  - Robot sensing systems
KW  - Collision avoidance
KW  - Computational modeling
KW  - Biological system modeling
KW  - Propulsion
DO  - 10.1109/ICRA40945.2020.9197198
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper is about the closely-related problems of localization and aggregation for extremely simple robots, for which the only available action is to move in a given direction as far as the geometry of the environment allows. Such problems may arise, for example, in biomedical applications, wherein a large group of tiny robots moves in response to a shared external stimulus. Specifically, we extend the prior work on these kinds of problems presenting two algorithms for localization in environments with curved (rather than polygonal) boundaries and under low-friction models of interaction with the environment boundaries. We present both simulations and physical demonstrations to validate the approach.
ER  - 

TY  - CONF
TI  - Stable Control in Climbing and Descending Flight under Upper Walls using Ceiling Effect Model based on Aerodynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 172
EP  - 178
AU  - T. Nishio
AU  - M. Zhao
AU  - F. Shi
AU  - T. Anzai
AU  - K. Kawaharazuka
AU  - K. Okada
AU  - M. Inaba
PY  - 2020
KW  - aerodynamics
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - motion control
KW  - rotors (mechanical)
KW  - stability
KW  - wakes
KW  - ceiling effect model
KW  - flight control stability
KW  - multirotor unmanned aerial vehicles
KW  - rotor thrust
KW  - vertical flight tests
KW  - in unsteady state model based controller
KW  - aerodynamics based thrust model
KW  - vertical climbing
KW  - vertical descending
KW  - wake interaction
KW  - momentum theory
KW  - Rotors
KW  - Data models
KW  - Adaptation models
KW  - Aerodynamics
KW  - Steady-state
KW  - Atmospheric modeling
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9197137
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Stable flight control under ceilings is difficult for multirotor Unmanned Aerial Vehicles (UAVs). The wake interaction between rotors and upper walls, called the "ceiling effect", causes an increase of rotor thrust. As a result of the thrust increase, multi-rotors are drawn upward abruptly and collide with ceilings. In previous work, several thrust models of the ceiling effect have been proposed for stable flight under ceilings, assuming that the airflow around rotors is in steady states. However, the airflow around rotors in vertical flight is not in steady states and each thrust model in previous work is skillfully determined based on large amounts of precise experimental data. In this paper, we introduce an aerodynamics-based thrust model and a stable control method under ceilings. This model is derived from the momentum theory and the relationship between vertical climbing/descending rates of rotors and an induced velocity. To confirm our proposed model, we collect thrust data at various vertical rates in flight. In addition, we use only onboard sensors to estimate selfstate for structural inspections. Consequently, we reveal that the proposed model is consistent with the experimental results. Based on an aerodynamic model, we need not collect large amounts of precise experimental data to realize stable flight. Furthermore, the vertical flight tests under ceilings demonstrate that our in-unsteady-state-model-based controller outperforms the conventional steady-state ones.
ER  - 

TY  - CONF
TI  - Motion Primitives-based Path Planning for Fast and Agile Exploration using Aerial Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 179
EP  - 185
AU  - M. Dharmadhikari
AU  - T. Dang
AU  - L. Solanka
AU  - J. Loje
AU  - H. Nguyen
AU  - N. Khedekar
AU  - K. Alexis
PY  - 2020
KW  - aerospace robotics
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - motion primitives-based path planning
KW  - agile exploration
KW  - aerial robots
KW  - path planning strategy
KW  - microaerial vehicles
KW  - volumetric representation
KW  - collision-tolerant flying robot
KW  - velocity 2.0 m/s
KW  - size 0.8 m
KW  - Vehicle dynamics
KW  - Collision avoidance
KW  - Path planning
KW  - Unmanned aerial vehicles
KW  - Educational robots
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196964
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel path planning strategy for fast and agile exploration using aerial robots. Tailored to the combined need for large-scale exploration of challenging and confined environments, despite the limited endurance of micro aerial vehicles, the proposed planner employs motion primitives to identify admissible paths that search the configuration space, while exploiting the dynamic flight properties of small aerial robots. Utilizing a computationally efficient volumetric representation of the environment, the planner provides fast collision-free and future-safe paths that maximize the expected exploration gain and ensure continuous fast navigation through the unknown environment. The new method is field-verified in a set of deployments relating to subterranean exploration and specifically, in both modern and abandoned underground mines in Northern Nevada utilizing a 0.55m-wide collision-tolerant flying robot exploring with a speed of up to 2m/s and navigating sections with width as small as 0.8m.
ER  - 

TY  - CONF
TI  - Unsupervised Anomaly Detection for Self-flying Delivery Drones
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 186
EP  - 192
AU  - V. Sindhwani
AU  - H. Sidahmed
AU  - K. Choromanski
AU  - B. Jones
PY  - 2020
KW  - aerodynamics
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - regression analysis
KW  - unsupervised anomaly detection
KW  - hybrid aerial vehicles
KW  - machine learning models
KW  - flight profiles
KW  - flight log measurements
KW  - sensor readings
KW  - predictive flight dynamics models
KW  - aircraft aerodynamics
KW  - self-flying delivery drones
KW  - Aerodynamics
KW  - Smoothing methods
KW  - Robustness
KW  - Training
KW  - Anomaly detection
KW  - Optimization
KW  - Aircraft
DO  - 10.1109/ICRA40945.2020.9197074
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel anomaly detection framework for a fleet of hybrid aerial vehicles executing high-speed package pickup and delivery missions. The detection is based on machine learning models of normal flight profiles, trained on millions of flight log measurements of control inputs and sensor readings. We develop a new scalable algorithm for robust regression which can simultaneously fit predictive flight dynamics models while identifying and discarding abnormal flight missions from the training set. The resulting unsupervised estimator has a very high breakdown point and can withstand massive contamination of training data to uncover what normal flight patterns look like, without requiring any form of prior knowledge of aircraft aerodynamics or manual labeling of anomalies upfront. Across many different anomaly types, spanning simple 3sigma statistical thresholds to turbulence and other equipment anomalies, our models achieve high detection rates across the board. Our method consistently outperforms alternative robust detection methods on synthetic benchmark problems. To the best of our knowledge, dynamics modeling of hybrid delivery drones for anomaly detection at the scale of 100 million measurements from 5000 real flight missions in variable flight conditions is unprecedented.
ER  - 

TY  - CONF
TI  - Keyfilter-Aware Real-Time UAV Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 193
EP  - 199
AU  - Y. Li
AU  - C. Fu
AU  - Z. Huang
AU  - Y. Zhang
AU  - J. Pan
PY  - 2020
KW  - autonomous aerial vehicles
KW  - image filtering
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - SLAM (robots)
KW  - keyframe-based simultaneous localization and mapping
KW  - keyfilter restriction
KW  - visual tracking
KW  - background distraction
KW  - filter corruption
KW  - boundary effect
KW  - unmanned aerial vehicle
KW  - correlation filter-based tracking
KW  - keyfilter-aware real-time UAV object tracking
KW  - Unmanned aerial vehicles
KW  - Correlation
KW  - Visualization
KW  - Object tracking
KW  - Frequency-domain analysis
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196943
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.
ER  - 

TY  - CONF
TI  - Aerial Regrasping: Pivoting with Transformable Multilink Aerial Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 200
EP  - 207
AU  - F. Shi
AU  - M. Zhao
AU  - M. Murooka
AU  - K. Okada
AU  - M. Inaba
PY  - 2020
KW  - aerospace robotics
KW  - dexterous manipulators
KW  - grippers
KW  - motion control
KW  - remotely operated vehicles
KW  - stability
KW  - aerial regrasping
KW  - aerial manipulator
KW  - dexterous manipulation
KW  - transformable multilink aerial robot
KW  - transformable multilink drone
KW  - grasping stability
KW  - thrust force
KW  - continous grasping force
KW  - admittance controller
KW  - impedance controller
KW  - contact aware regrasping
KW  - Task analysis
KW  - Force
KW  - Grasping
KW  - Unmanned aerial vehicles
KW  - Rotors
KW  - End effectors
DO  - 10.1109/ICRA40945.2020.9196576
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Regrasping is one of the most common and important manipulation skills used in our daily life. However, aerial regrasping has not been seriously investigated yet, since most of the aerial manipulator lacks dexterous manipulation abilities except for the basic pick-and-place. In this paper, we focus on pivoting a long box, which is one of the most classical problems among regrasping researches, using a transformable multilink aerial robot. First, we improve our previous controller by compensating for the external wrench. Second, we optimize the joints configuration of our transformable multilink drone for stable grasping form under the constraints of thrust force and joints effort. Third, we sequentially optimize the grasping force in the pivoting process. The optimization goal is to generate continous grasping force whilst maximizing the friction force in case of the downwash, which would influence the grasped object and is difficult to model. Fourth, we develop the impedance controller in joint space and admittance controller in task space. As far as we know, it is the first research to achieve extrinsic contact-aware regrasping task on aerial robots.
ER  - 

TY  - CONF
TI  - Grounding Language to Landmarks in Arbitrary Outdoor Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 208
EP  - 215
AU  - M. Berg
AU  - D. Bayazit
AU  - R. Mathew
AU  - A. Rotter-Aboyoun
AU  - E. Pavlick
AU  - S. Tellex
PY  - 2020
KW  - human-robot interaction
KW  - mobile robots
KW  - natural language processing
KW  - path planning
KW  - robot vision
KW  - natural language phrases
KW  - arbitrary outdoor environments
KW  - urban environments
KW  - natural language commands
KW  - robot control
KW  - semantic similarities
KW  - Natural languages
KW  - Semantics
KW  - Robots
KW  - Grounding
KW  - Training
KW  - Planning
KW  - Databases
DO  - 10.1109/ICRA40945.2020.9197068
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots operating in outdoor, urban environments need the ability to follow complex natural language commands which refer to never-before-seen landmarks. Existing approaches to this problem are limited because they require training a language model for the landmarks of a particular environment before a robot can understand commands referring to those landmarks. To generalize to new environments outside of the training set, we present a framework that parses references to landmarks, then assesses semantic similarities between the referring expression and landmarks in a predefined semantic map of the world, and ultimately translates natural language commands to motion plans for a drone. This framework allows the robot to ground natural language phrases to landmarks in a map when both the referring expressions to landmarks and the landmarks themselves have not been seen during training. We test our framework with a 14-person user evaluation demonstrating an end-to-end accuracy of 76.19% in an unseen environment. Subjective measures show that users find our system to have high performance and low workload. These results demonstrate our approach enables untrained users to control a robot in large unseen outdoor environments with unconstrained natural language.
ER  - 

TY  - CONF
TI  - Deep Merging: Vehicle Merging Controller Based on Deep Reinforcement Learning with Embedding Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 216
EP  - 221
AU  - I. Nishitani
AU  - H. Yang
AU  - R. Guo
AU  - S. Keshavamurthy
AU  - K. Oguchi
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - road traffic control
KW  - road vehicles
KW  - traffic engineering computing
KW  - traffic conditions
KW  - traffic flow
KW  - Deep Merging
KW  - vehicle Merging controller
KW  - embedding network
KW  - highway merging sections
KW  - lane change
KW  - vehicle controller
KW  - merging efficiency
KW  - merging section
KW  - target vehicle speed
KW  - controlled vehicle speed
KW  - deep reinforcement learning network architecture
KW  - learning efficiency
KW  - merging behavior
KW  - Merging
KW  - Machine learning
KW  - Feature extraction
KW  - Acceleration
KW  - Road transportation
KW  - Vehicle dynamics
KW  - Network architecture
DO  - 10.1109/ICRA40945.2020.9197559
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Vehicles at highway merging sections must make lane changes to join the highway. This lane change can generate congestion. To reduce congestion, vehicles should merge so as not to affect traffic flow as much as possible. In our study, we propose a vehicle controller called Deep Merging that uses deep reinforcement learning to improve the merging efficiency of vehicles while considering the impact on traffic flow. The system uses the images of a merging section as input to output the target vehicle speed. Moreover, an embedding network for estimating the controlled vehicle speed is introduced to the deep reinforcement learning network architecture to improve the learning efficiency. In order to show the effectiveness of the proposed method, the merging behavior and traffic conditions in several situations are verified by experiments using a traffic simulator. Through these experiments, it is confirmed that the proposed method enables controlled vehicles to effectively merge without adversely affecting to the traffic flow.
ER  - 

TY  - CONF
TI  - Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 222
EP  - 228
AU  - S. Chadwick
AU  - P. Newman
PY  - 2020
KW  - data analysis
KW  - object detection
KW  - radar imaging
KW  - road vehicles
KW  - supervised learning
KW  - traffic engineering computing
KW  - noisy labels
KW  - noise-aware training techniques
KW  - training data
KW  - weakly supervised vehicle detection
KW  - radar labels
KW  - object detector
KW  - image-based vehicle detection
KW  - Training
KW  - Noise measurement
KW  - Training data
KW  - Radar imaging
KW  - Labeling
KW  - Lenses
DO  - 10.1109/ICRA40945.2020.9196855
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It has been demonstrated that the performance of an object detector degrades when it is used outside the domain of the data used to train it. However, obtaining training data for a new domain can be time consuming and expensive. In this work we demonstrate how a radar can be used to generate plentiful (but noisy) training data for image-based vehicle detection. We then show that the performance of a detector trained using the noisy labels can be considerably improved through a combination of noise-aware training techniques and relabelling of the training data using a second viewpoint. In our experiments, using our proposed process improves average precision by more than 17 percentage points when training from scratch and 10 percentage points when fine-tuning a pre-trained model.
ER  - 

TY  - CONF
TI  - Robust Lane Detection with Binary Integer Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 229
EP  - 235
AU  - K. Brandes
AU  - A. Wang
AU  - R. Shah
PY  - 2020
KW  - driver information systems
KW  - integer programming
KW  - object detection
KW  - road safety
KW  - road vehicles
KW  - vehicle dynamics
KW  - false positive cone detections
KW  - binary integer optimization
KW  - competition rules
KW  - average cone spacings
KW  - minimum track width
KW  - robust lane detection
KW  - Formula Student Driverless
KW  - FSD
KW  - student teams
KW  - autonomous racecar
KW  - main dynamic event
KW  - unknown track
KW  - Automobiles
KW  - Optimization
KW  - Robustness
KW  - Meters
KW  - Roads
KW  - Real-time systems
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197098
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Formula Student Driverless (FSD) is a competition where student teams compete to build an autonomous racecar. The main dynamic event in FSD is trackdrive, where the racecar traverses an unknown track with lanes demarcated by cones. One major challenge of the event is to determine the boundaries of the lane from cones perceived online despite false positive cone detections and sharp turns. We present a binary integer optimization to address this problem by leveraging a priori knowledge from competition rules on parameters such as average cone spacings and minimum track width. In this paper, we describe our approach, and analyze its latency, accuracy, and robustness to false positive cone detections. This approach is used on-board to solve the lane detection problem during the competition in real-time.
ER  - 

TY  - CONF
TI  - A Synchronization Approach for Achieving Cooperative Adaptive Cruise Control Based Non-Stop Intersection Passing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 236
EP  - 242
AU  - Z. Liu
AU  - H. Wei
AU  - H. Hu
AU  - C. Suo
AU  - H. Wang
AU  - H. Li
AU  - Y. -H. Liu
PY  - 2020
KW  - adaptive control
KW  - control system synthesis
KW  - distributed control
KW  - Lyapunov methods
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - road traffic control
KW  - road vehicles
KW  - stability
KW  - velocity control
KW  - synchronization approach
KW  - adaptive cruise control based nonstop intersection passing
KW  - intelligent vehicles
KW  - cruise control performance
KW  - traffic congestion
KW  - increasing traffic flow capacity
KW  - CACC problem
KW  - synchronization control
KW  - spatial-temporal synchronization mechanism
KW  - vehicle platoon control
KW  - robust CACC
KW  - cross-coupling based space synchronization mechanism
KW  - distributed control algorithm
KW  - single-lane CACC
KW  - vehicle-to-vehicle communications
KW  - autonomous vehicles
KW  - desired platoon trajectory
KW  - expected inter-vehicle distance
KW  - enter-time scheduling mechanism
KW  - high-level intersection control strategy
KW  - Lyapunov-based time-domain stability analysis approach
KW  - traditional string stability based approach
KW  - CACC system
KW  - Synchronization
KW  - Stability analysis
KW  - Cruise control
KW  - Robustness
KW  - Autonomous vehicles
KW  - Motion control
KW  - Acceleration
DO  - 10.1109/ICRA40945.2020.9196991
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Cooperative adaptive cruise control (CACC) of intelligent vehicles contributes to improving cruise control performance, reducing traffic congestion, saving energy and increasing traffic flow capacity. In this paper, we resolve the CACC problem from the viewpoint of synchronization control, our main idea is to introduce the spatial-temporal synchronization mechanism into vehicle platoon control to achieve the robust CACC and to further realize the non-stop intersection control. Firstly, by introducing the cross-coupling based space synchronization mechanism, a distributed control algorithm is presented to achieve the single-lane CACC in the presence of vehicle-to-vehicle (V2V) communications, which enables autonomous vehicles to track the desired platoon trajectory while synchronizing their longitudinal velocities to keeping the expected inter-vehicle distance. Secondly, by designing the enter-time scheduling mechanism (temporal synchronization), a high-level intersection control strategy is proposed to command vehicles to form a virtual platoon to pass through the intersection without stopping. Thirdly, a Lyapunov-based time-domain stability analysis approach is presented. Compared with the traditional string stability based approach, the proposed approach guarantees the global asymptotical convergence of the proposed CACC system. Experiments in the small-scale simulated system demonstrate the effectiveness of the proposed approach.
ER  - 

TY  - CONF
TI  - Urban Driving with Conditional Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 251
EP  - 257
AU  - J. Hawke
AU  - R. Shen
AU  - C. Gurau
AU  - S. Sharma
AU  - D. Reda
AU  - N. Nikolov
AU  - P. Mazur
AU  - S. Micklethwaite
AU  - N. Griffiths
AU  - A. Shah
AU  - A. Kndall
PY  - 2020
KW  - cameras
KW  - computer vision
KW  - decision making
KW  - driver information systems
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - road traffic
KW  - road vehicles
KW  - real-world urban autonomous driving
KW  - human driving demonstrations
KW  - user-defined route
KW  - single camera view
KW  - heavily cropped frames
KW  - lateral control
KW  - longitudinal control
KW  - real-world complexities
KW  - end-to-end conditional imitation learning approach
KW  - urban routes
KW  - simple traffic
KW  - autonomous vehicle
KW  - European urban streets
KW  - urban driving
KW  - hand-crafting generalised decision-making rules
KW  - Cameras
KW  - Sensor fusion
KW  - Autonomous vehicles
KW  - Roads
KW  - Computational modeling
KW  - Aerospace electronics
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197408
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.
ER  - 

TY  - CONF
TI  - Vehicle Localization Based on Visual Lane Marking and Topological Map Matching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 258
EP  - 264
AU  - R. Asghar
AU  - M. Garzón
AU  - J. Lussereau
AU  - C. Laugier
PY  - 2020
KW  - image filtering
KW  - image fusion
KW  - image matching
KW  - iterative methods
KW  - Kalman filters
KW  - nonlinear filters
KW  - road vehicles
KW  - traffic engineering computing
KW  - visual lane marking
KW  - topological map matching
KW  - autonomous vehicle navigation
KW  - driver assistance systems
KW  - online vehicle localization
KW  - distinct map matching algorithms
KW  - visual lane tracker
KW  - map matching algorithm
KW  - grid map
KW  - iterative closest point based lane level map matching
KW  - Roads
KW  - Iterative closest point algorithm
KW  - Global Positioning System
KW  - Dead reckoning
KW  - Sensors
KW  - Visualization
KW  - Cameras
KW  - Map Relative Localization
KW  - Topological Map Matching
KW  - Lane Level Matching
KW  - Autonomous Vehicles
DO  - 10.1109/ICRA40945.2020.9197543
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate and reliable localization is crucial to autonomous vehicle navigation and driver assistance systems. This paper presents a novel approach for online vehicle localization in a digital map. Two distinct map matching algorithms are proposed: i) Iterative Closest Point (ICP) based lane level map matching is performed with visual lane tracker and grid map ii) decision-rule based approach is used to perform topological map matching. Results of both the map matching algorithms are fused together with GPS and dead reckoning using Extended Kalman Filter to estimate vehicle's pose relative to the map. The proposed approach has been validated on real life conditions on an equipped vehicle. Detailed analysis of the experimental results show improved localization using the two aforementioned map matching algorithms.
ER  - 

TY  - CONF
TI  - RISE: A Novel Indoor Visual Place Recogniser
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 265
EP  - 271
AU  - C. Sánchez-Belenguer
AU  - E. Wolfart
AU  - V. Sequeira
PY  - 2020
KW  - cameras
KW  - image retrieval
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - indoor visual place recogniser
KW  - deep learning
KW  - image retrieval
KW  - image similarity metric
KW  - 3D laser sensor
KW  - calibrated spherical camera
KW  - deep neural network
KW  - geo-referenced images
KW  - data collection stage
KW  - 3D laser measurements
KW  - spherical panoramas
KW  - indoor areas
KW  - observed pixels
KW  - image mapping
KW  - query image
KW  - RISE
KW  - indoor visual place recognition problem
DO  - 10.1109/ICRA40945.2020.9196871
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a new technique to solve the Indoor Visual Place Recognition problem from the Deep Learning perspective. It consists on an image retrieval approach supported by a novel image similarity metric. Our work uses a 3D laser sensor mounted on a backpack with a calibrated spherical camera i) to generate the data for training the deep neural network and ii) to build a database of geo-referenced images for an environment. The data collection stage is fully automatic and requires no user intervention for labelling. Thanks to the 3D laser measurements and the spherical panoramas, we can efficiently survey large indoor areas in a very short time. The underlying 3D data associated to the map allows us to define the similarity between two training images as the geometric overlap between the observed pixels. We exploit this similarity metric to effectively train a CNN that maps images into compact embeddings. The goal of the training is to ensure that the L2 distance between the embeddings associated to two images is small when they are observing the same place and large when they are observing different places. After the training, similarities between a query image and the geo-referenced images in the database are efficiently retrieved by performing a nearest neighbour search in the embeddings space.
ER  - 

TY  - CONF
TI  - Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 272
EP  - 278
AU  - J. Quenzel
AU  - R. A. Rosu
AU  - T. Läbe
AU  - C. Stachniss
AU  - S. Behnke
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - gradient methods
KW  - image matching
KW  - image registration
KW  - image sensors
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - visual SLAM systems
KW  - photometric consistency
KW  - gradient-based dissimilarity
KW  - camera pose estimation
KW  - map building
KW  - central ingredients
KW  - autonomous robots
KW  - photometric error
KW  - gradient orientation
KW  - magnitude-dependent scaling term
KW  - stereo estimation
KW  - visual odometry systems
KW  - direct image registration tasks
KW  - robust estimates
KW  - scene depth
KW  - camera trajectory
KW  - mapping capabilities
KW  - mobile robots
KW  - sensor data registration
KW  - Measurement
KW  - Robustness
KW  - Cameras
KW  - Estimation
KW  - Visual odometry
KW  - Simultaneous localization and mapping
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197483
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicate that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.
ER  - 

TY  - CONF
TI  - ICS: Incremental Constrained Smoothing for State Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 279
EP  - 285
AU  - P. Sodhi
AU  - S. Choudhury
AU  - J. G. Mangelson
AU  - M. Kaess
PY  - 2020
KW  - matrix decomposition
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - state estimation
KW  - ICS
KW  - primal-dual method
KW  - matrix factorizations
KW  - primal-dual methods
KW  - incremental factorization
KW  - matrix structure
KW  - incremental unconstrained optimization
KW  - robot state estimate
KW  - smoothing-based estimation methods
KW  - state estimation
KW  - incremental constrained smoothing
KW  - Optimization
KW  - Smoothing methods
KW  - Time measurement
KW  - Integrated circuits
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9196649
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A robot operating in the world constantly receives information about its environment in the form of new measurements at every time step. Smoothing-based estimation methods seek to optimize for the most likely robot state estimate using all measurements up till the current time step. Existing methods solve for this smoothing objective efficiently by framing the problem as that of incremental unconstrained optimization. However, in many cases observed measurements and knowledge of the environment is better modeled as hard constraints derived from real-world physics or dynamics. A key challenge is that the new optimality conditions introduced by the hard constraints break the matrix structure needed for incremental factorization in these incremental optimization methods. Our key insight is that if we leverage primal-dual methods, we can recover a matrix structure amenable to incremental factorization. We propose a framework ICS that combines a primal-dual method like the Augmented Lagrangian with an incremental Gauss Newton approach that reuses previously computed matrix factorizations. We evaluate ICS on a set of simulated and real-world problems involving equality constraints like object contact and inequality constraints like collision avoidance.
ER  - 

TY  - CONF
TI  - Drone-aided Localization in LoRa IoT Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 286
EP  - 292
AU  - V. Delafontaine
AU  - F. Schiano
AU  - G. Cocco
AU  - A. Rusu
AU  - D. Floreano
PY  - 2020
KW  - autonomous aerial vehicles
KW  - Internet of Things
KW  - wide area networks
KW  - realistic simulated scenario
KW  - fully autonomous localization system
KW  - ten-fold improvement
KW  - localization precision
KW  - fixed network
KW  - UAV
KW  - localization accuracy
KW  - LoRa IoT networks
KW  - node localization
KW  - widespread IoT communication technologies
KW  - Long Range Wide Area Network
KW  - long communication distances
KW  - drone-aided localization system
KW  - communication system
KW  - search algorithm
KW  - Internet of Things
KW  - 3D mobility
KW  - Logic gates
KW  - Drones
KW  - Estimation
KW  - Receivers
KW  - Servers
KW  - Internet of Things
KW  - Global navigation satellite system
DO  - 10.1109/ICRA40945.2020.9196869
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Besides being part of the Internet of Things (IoT), drones can play a relevant role in it as enablers. The 3D mobility of UAVs can be exploited to improve node localization in IoT networks for, e.g., search and rescue or goods localization and tracking. One of the widespread IoT communication technologies is Long Range Wide Area Network (LoRaWAN), which allows achieving long communication distances with low power. In this work, we present a drone-aided localization system for LoRa networks in which a UAV is used to improve the estimation of a node's location initially provided by the network. We characterize the relevant parameters of the communication system and use them to develop and test a search algorithm in a realistic simulated scenario. We then move to the full implementation of a real system in which a drone is seamlessly integrated into Swisscom's LoRa network. The drone coordinates with the network with a two-way exchange of information which results in an accurate and fully autonomous localization system. The results obtained in our field tests show a ten-fold improvement in localization precision with respect to the estimation provided by the fixed network. Up to our knowledge, this is the first time a UAV is successfully integrated in a LoRa network to improve its localization accuracy.
ER  - 

TY  - CONF
TI  - A fast and practical method of indoor localization for resource-constrained devices with limited sensing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 293
EP  - 299
AU  - J. Wietrzykowski
AU  - P. Skrzypczyński
PY  - 2020
KW  - collision avoidance
KW  - indoor radio
KW  - mobile robots
KW  - navigation
KW  - probability
KW  - fast method
KW  - indoor localization
KW  - resource-constrained devices
KW  - limited sensing capabilities
KW  - wearable devices
KW  - sparse WiFi
KW  - image-based measurements
KW  - dense signal maps
KW  - conditional random fields
KW  - agent positions
KW  - known floor plan
KW  - sparse absolute position estimates
KW  - motion sequences
KW  - low-quality measurements
KW  - handheld devices
KW  - mobile devices
KW  - sensory data
KW  - Wireless fidelity
KW  - Position measurement
KW  - Computational modeling
KW  - Dead reckoning
KW  - Robot sensing systems
KW  - Buildings
DO  - 10.1109/ICRA40945.2020.9197215
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We describe and experimentally demonstrate a practical method for indoor localization using measurements obtained from resource-constrained devices with limited sensing capabilities. We focus on handheld/mobile devices but the method can be useful for a variety of wearable devices. Our system works with sparse WiFi or image-based measurements, avoiding laborious site surveying for dense signal maps and runs in real-time. It uses Conditional Random Fields to infer the most probable sequence of agent positions from a known floor plan, dead reckoning and sparse absolute position estimates. Our solution leverages known topology of the environment by pre-computing allowed motion sequences of an agent, which are then used to constraint the motion inferred from the sensory data. The system is evaluated in a typical office building, demonstrating good accuracy and robustness to sparse, low-quality measurements.
ER  - 

TY  - CONF
TI  - Long-Term Robot Navigation in Indoor Environments Estimating Patterns in Traversability Changes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 300
EP  - 306
AU  - L. Nardi
AU  - C. Stachniss
PY  - 2020
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - term robot navigation
KW  - traversability changes
KW  - mobile robots
KW  - hospitals
KW  - informed decisions
KW  - probabilistic graphical model
KW  - currently unobserved locations
KW  - indoor environments
KW  - Robots
KW  - Navigation
KW  - Predictive models
KW  - Correlation
KW  - Probabilistic logic
KW  - Planning
KW  - Indoor environments
DO  - 10.1109/ICRA40945.2020.9197078
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Nowadays, mobile robots are deployed in many indoor environments such as offices or hospitals. These environments are subject to changes in the traversability that often happen following patterns. In this paper, we investigate the problem of navigating in such environments over extended periods of time by capturing and exploiting these patterns to make informed decisions for navigation. Our approach uses a probabilistic graphical model to incrementally estimate a model of the traversability changes from the robot's observations and to make predictions at currently unobserved locations. In the belief space defined by the predictions, we plan paths that trade off the risk to encounter obstacles and the information gain of visiting unknown locations. We implemented our approach and tested it in different indoor environments. The experiments suggest that, in the long run, our approach leads robots to navigate along shorter paths compared to following a greedy shortest path policy.
ER  - 

TY  - CONF
TI  - Sample-and-computation-efficient Probabilistic Model Predictive Control with Random Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 307
EP  - 313
AU  - C. -Y. Kuo
AU  - Y. Cui
AU  - T. Matsubara
PY  - 2020
KW  - approximation theory
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - predictive control
KW  - random features
KW  - Gaussian processes
KW  - reinforcement learning methods
KW  - model predictive control
KW  - MPC
KW  - computational cost
KW  - linear Gaussian model
KW  - approximated GP dynamics
KW  - state prediction
KW  - simulated robot control tasks
KW  - sample-and-computation-efficient nature
KW  - model-based RL method
KW  - analytic moment-matching scheme
KW  - Training
KW  - Predictive models
KW  - Kernel
KW  - Probabilistic logic
KW  - Computational modeling
KW  - Computational efficiency
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197449
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gaussian processes (GPs) based Reinforcement Learning (RL) methods with Model Predictive Control (MPC) have demonstrated their excellent sample efficiency. However, since the computational cost of GPs largely depends on the training sample size, learning an accurate dynamics using GPs result in low control frequency in MPC. To alleviate this trade-off and achieve a sample-and-computation-efficient nature, we propose a novel model-based RL method with MPC. Our approach employs a linear Gaussian model with randomized features using the Fastfood as an approximated GP dynamics. Then, we derive an analytic moment-matching scheme in state prediction with the model and uncertain inputs. As a result, the computational cost of the MPC in our RL method does not depend on the training sample size and can improve the control frequency over previous methods. Through experiments with simulated and real robot control tasks, the sample efficiency, as well as the computation efficiency of our model-based RL method, are demonstrated.
ER  - 

TY  - CONF
TI  - Sample-Efficient Robot Motion Learning using Gaussian Process Latent Variable Models
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 314
EP  - 320
AU  - J. A. Delgado-Guerrero
AU  - A. Colomé
AU  - C. Torras
PY  - 2020
KW  - Gaussian processes
KW  - learning systems
KW  - manipulator dynamics
KW  - motion control
KW  - search problems
KW  - sample-efficient robot motion learning
KW  - Gaussian process latent variable models
KW  - robotic manipulators
KW  - household environments
KW  - kinesthetic teaching
KW  - parametric function
KW  - movement primitive
KW  - mutual-information-weighted Gaussian process latent variable model
KW  - trial-and-error
KW  - trajectory production
KW  - task dynamics
KW  - MP parameter latent space
KW  - robot motion task
KW  - search space
KW  - surrogate model
KW  - PS algorithms
KW  - policy search reinforcement learning
KW  - Gaussian processes
KW  - Optimization
KW  - Trajectory
KW  - Task analysis
KW  - Robots
KW  - Mutual information
KW  - Kernel
DO  - 10.1109/ICRA40945.2020.9196658
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic manipulators are reaching a state where we could see them in household environments in the following decade. Nevertheless, such robots need to be easy to instruct by lay people. This is why kinesthetic teaching has become very popular in recent years, in which the robot is taught a motion that is encoded as a parametric function - usually a Movement Primitive (MP)-. This approach produces trajectories that are usually suboptimal, and the robot needs to be able to improve them through trial-and-error. Such optimization is often done with Policy Search (PS) reinforcement learning, using a given reward function. PS algorithms can be classified as model-free, where neither the environment nor the reward function are modelled, or model-based, which can use a surrogate model of the reward function and/or a model for the dynamics of the task. However, MPs can become very high-dimensional in terms of parameters, which constitute the search space, so their optimization often requires too many samples. In this paper, we assume we have a robot motion task characterized with an MP of which we cannot model the dynamics. We build a surrogate model for the reward function, that maps an MP parameter latent space (obtained through a Mutual-information-weighted Gaussian Process Latent Variable Model) into a reward. While we do not model the task dynamics, using mutual information to shrink the task space makes it more consistent with the reward and so the policy improvement is faster in terms of sample efficiency.
ER  - 

TY  - CONF
TI  - Iterative Learning based feedforward control for Transition of a Biplane-Quadrotor Tailsitter UAS
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 321
EP  - 327
AU  - N. Raj
AU  - A. Simha
AU  - M. Kothari
AU  - Abhishek
AU  - R. N. Banavar
PY  - 2020
KW  - aerodynamics
KW  - aircraft control
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - error compensation
KW  - feedforward
KW  - helicopters
KW  - iterative learning control
KW  - learning systems
KW  - neurocontrollers
KW  - pitch control (position)
KW  - polynomials
KW  - propellers
KW  - robust control
KW  - wind tunnels
KW  - iterative learning based feedforward control
KW  - biplane-quadrotor tailsitter UAS
KW  - time on-board algorithm
KW  - forward transition maneuver
KW  - repeated flight trials
KW  - pitch angle
KW  - propeller thrust
KW  - polynomials
KW  - simplified aerodynamics
KW  - optimal coefficients
KW  - terminal conditions
KW  - air speed
KW  - modeling error compensation
KW  - geometric attitude controller
KW  - flight modes
KW  - feedforward law
KW  - high-fidelity thrust model
KW  - orientation angle
KW  - neural network model
KW  - experimental flight trials
KW  - learning algorithm
KW  - maneuver control
KW  - wind tunnel data
KW  - feedforward thrust
KW  - robustness
KW  - UAS
KW  - Aerodynamics
KW  - Propellers
KW  - Wind tunnels
KW  - Atmospheric modeling
KW  - Feedforward systems
KW  - Data models
KW  - Trajectory
KW  - VTOL UAS
KW  - transition maneuver
KW  - iterative learning
DO  - 10.1109/ICRA40945.2020.9196671
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper provides a real time on-board algorithm for a biplane-quadrotor to iteratively learn a forward transition maneuver via repeated flight trials. The maneuver is controlled by regulating the pitch angle and propeller thrust according to feedforward control laws that are parameterized by polynomials. Based on a nominal model with simplified aerodynamics, the optimal coefficients of the polynomials are chosen through simulation such that the maneuver is completed with specified terminal conditions on altitude and air speed. In order to compensate for modeling errors, repeated flight trials are performed by updating the feedforward control parameters according to an iterative learning algorithm until the maneuver is perfected. A geometric attitude controller, valid for all flight modes is employed in order to track the pitch angle according to the feedforward law. Further, a high-fidelity thrust model of the propeller for varying advance-ratio and orientation angle is obtained from wind tunnel data which is captured using a neural network model. This facilitates accurate application of feedforward thrust for varying flow conditions during transition. Experimental flight trials are performed to demonstrate the robustness and rapid convergence of the proposed learning algorithm.
ER  - 

TY  - CONF
TI  - Reinforcement Learning for Adaptive Illumination with X-rays
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 328
EP  - 334
AU  - J. -R. Betterton
AU  - D. Ratner
AU  - S. Webb
AU  - M. Kochenderfer
PY  - 2020
KW  - convolutional neural nets
KW  - image resolution
KW  - image sampling
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - X-ray imaging
KW  - adaptive illumination
KW  - reinforcement learning
KW  - image sampling
KW  - image surface
KW  - convolutional neural network
KW  - rastering method
KW  - X-rays
KW  - Apertures
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Time measurement
KW  - Imaging
KW  - Image reconstruction
KW  - X-rays
DO  - 10.1109/ICRA40945.2020.9196614
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a learning algorithm for automating image sampling in scientific applications. We consider settings where images are sampled by controlling a probe beam's scanning trajectory over the image surface. We explore alternatives to obtaining images by the standard rastering method. We formulate the scanner control problem as a reinforcement learning (RL) problem and train a policy to adaptively sample only the highest value regions of the image, choosing the acquisition time and resolution for each sample position based on an observation of previous readings. We use convolutional neural network (CNN) policies to control the scanner as a way to generalize our approach to larger samples. We show simulation results for a simple policy on both synthetic data and real world data from an archaeological application.
ER  - 

TY  - CONF
TI  - Efficient Updates for Data Association with Mixtures of Gaussian Processes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 335
EP  - 341
AU  - K. M. Brian Lee
AU  - W. Martens
AU  - J. Khatkar
AU  - R. Fitch
AU  - R. Mettu
PY  - 2020
KW  - covariance matrices
KW  - Gaussian processes
KW  - mixture models
KW  - probability
KW  - robot vision
KW  - GP mixtures
KW  - data association
KW  - Gaussian processes
KW  - probabilistic approach
KW  - important estimation
KW  - classification tasks
KW  - robotics applications
KW  - GP-based methods
KW  - sparse methods
KW  - covariance matrix
KW  - orthogonal approach
KW  - GP inference
KW  - online update scheme
KW  - sparse GPs
KW  - memoisation approach
KW  - robotic vision applications
KW  - Covariance matrices
KW  - Robots
KW  - Gaussian processes
KW  - Mixture models
KW  - Sparse representation
KW  - Inference algorithms
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196734
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gaussian processes (GPs) enable a probabilistic approach to important estimation and classification tasks that arise in robotics applications. Meanwhile, most GP-based methods are often prohibitively slow, thereby posing a substantial barrier to practical applications. Existing "sparse" methods to speed up GPs seek to either make the model more sparse, or find ways to more efficiently manage a large covariance matrix. In this paper, we present an orthogonal approach that memoises (i.e. reuses) previous computations in GP inference. We demonstrate that a substantial speedup can be achieved by incorporating memoisation into applications in which GPs must be updated frequently. Moreover, we derive a novel online update scheme for sparse GPs that can be used in conjunction with our memoisation approach for a synergistic improvement in performance. Across three robotic vision applications, we demonstrate between 40-100% speed-up over the standard method for inference in GP mixtures.
ER  - 

TY  - CONF
TI  - Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot End Effector Position
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 350
EP  - 356
AU  - H. Peng
AU  - X. Yang
AU  - Y. -H. Su
AU  - B. Hannaford
PY  - 2020
KW  - biomechanics
KW  - end effectors
KW  - medical robotics
KW  - position control
KW  - surgery
KW  - telerobotics
KW  - time data driven precision Estimator
KW  - RAVEN-II surgical robot end effector position
KW  - surgical robots
KW  - cable-driven nature
KW  - kinematics calcu-lation
KW  - reported end effector position
KW  - position inaccuracy
KW  - data-driven pipeline
KW  - robot end effector position precision estimation
KW  - improved end effector position error
KW  - RMS
KW  - entire robot workspace
KW  - End effectors
KW  - Cameras
KW  - Medical robotics
KW  - Robot sensing systems
KW  - Image edge detection
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9196915
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgical robots have been introduced to operating rooms over the past few decades due to their high sensitivity, small size, and remote controllability. The cable-driven nature of many surgical robots allows the systems to be dexterous and lightweight, with diameters as low as 5mm. However, due to the slack and stretch of the cables and the backlash of the gears, inevitable uncertainties are brought into the kinematics calcu-lation [1]. Since the reported end effector position of surgical robots like RAVEN-II [2] is directly calculated using the motor encoder measurements and forward kinematics, it may contain relatively large error up to 10mm, whereas semi-autonomous functions being introduced into abdominal surgeries require position inaccuracy of at most 1mm. To resolve the problem, a cost-effective, real-time and data-driven pipeline for robot end effector position precision estimation is proposed and tested on RAVEN-II. Analysis shows an improved end effector position error of around 1mm RMS traversing through the entire robot workspace without high-resolution motion tracker. The open source code, data sets, videos, and user guide can be found at //github.com/HaonanPeng/RAVEN Neural Network Estimator.
ER  - 

TY  - CONF
TI  - Temporal Segmentation of Surgical Sub-tasks through Deep Learning with Multiple Data Sources
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 371
EP  - 377
AU  - Y. Qin
AU  - S. A. Pedram
AU  - S. Feyzabadi
AU  - M. Allan
AU  - A. J. McLeod
AU  - J. W. Burdick
AU  - M. Azizian
PY  - 2020
KW  - biomedical ultrasonics
KW  - finite state machines
KW  - learning (artificial intelligence)
KW  - medical computing
KW  - medical image processing
KW  - medical robotics
KW  - surgery
KW  - Skill Assessment Working Set
KW  - robotic intra-operative ultrasound imaging
KW  - da Vinci® Xi surgical system
KW  - superior frame-wise state estimation accuracy
KW  - temporal segmentation
KW  - deep learning
KW  - data sources
KW  - robot-assisted surgeries
KW  - finite-state machines
KW  - surgical task
KW  - temporal perception
KW  - current surgical scene
KW  - real-time estimation
KW  - task progresses
KW  - state estimation models
KW  - surgical state estimation models
KW  - State estimation
KW  - Data models
KW  - Task analysis
KW  - Feature extraction
KW  - Hidden Markov models
KW  - Robots
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196560
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many tasks in robot-assisted surgeries (RAS) can be represented by finite-state machines (FSMs), where each state represents either an action (such as picking up a needle) or an observation (such as bleeding). A crucial step towards the automation of such surgical tasks is the temporal perception of the current surgical scene, which requires a real-time estimation of the states in the FSMs. The objective of this work is to estimate the current state of the surgical task based on the actions performed or events occurred as the task progresses. We propose Fusion-KVE, a unified surgical state estimation model that incorporates multiple data sources including the Kinematics, Vision, and system Events. Additionally, we examine the strengths and weaknesses of different state estimation models in segmenting states with different representative features or levels of granularity. We evaluate our model on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), as well as a more complex dataset involving robotic intra-operative ultrasound (RIOUS) imaging, created using the da Vinci® Xi surgical system. Our model achieves a superior frame-wise state estimation accuracy up to 89.4%, which improves the state-of-the-art surgical state estimation models in both JIGSAWS suturing dataset and our RIOUS dataset.
ER  - 

TY  - CONF
TI  - Controlling Assistive Robots with Learned Latent Actions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 378
EP  - 384
AU  - D. P. Losey
AU  - K. Srinivasan
AU  - A. Mandlekar
AU  - A. Garg
AU  - D. Sadigh
PY  - 2020
KW  - control system synthesis
KW  - handicapped aids
KW  - learning systems
KW  - manipulators
KW  - telerobotics
KW  - learned latent actions
KW  - assistive robotic arms
KW  - high-dimensional robot behavior
KW  - user-friendly latent actions
KW  - low-dimensional embeddings
KW  - robotic arm
KW  - assistive eating
KW  - cooking tasks
KW  - assistive robot control
KW  - physical disabilities
KW  - teleoperation
KW  - handheld joystick
KW  - shared autonomy baselines
KW  - Robot kinematics
KW  - Task analysis
KW  - Manipulators
KW  - Aerospace electronics
KW  - Robot sensing systems
KW  - Glass
KW  - Index Terms—Physically assistive devices
KW  - cognitive humanrobot interaction
KW  - human-centered robotics
DO  - 10.1109/ICRA40945.2020.9197197
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Assistive robotic arms enable users with physical disabilities to perform everyday tasks without relying on a caregiver. Unfortunately, the very dexterity that makes these arms useful also makes them challenging to teleoperate: the robot has more degrees-of-freedom than the human can directly coordinate with a handheld joystick. Our insight is that we can make assistive robots easier for humans to control by leveraging latent actions. Latent actions provide a low-dimensional embedding of high-dimensional robot behavior: for example, one latent dimension might guide the assistive arm along a pouring motion. In this paper, we design a teleoperation algorithm for assistive robots that learns latent actions from task demonstrations. We formulate the controllability, consistency, and scaling properties that user-friendly latent actions should have, and evaluate how different low-dimensional embeddings capture these properties. Finally, we conduct two user studies on a robotic arm to compare our latent action approach to both state-of-the-art shared autonomy baselines and a teleoperation strategy currently used by assistive arms. Participants completed assistive eating and cooking tasks more efficiently when leveraging our latent actions, and also subjectively reported that latent actions made the task easier to perform. The video accompanying this paper can be found at: https://youtu.be/wjnhrzugBj4.
ER  - 

TY  - CONF
TI  - On the efficient control of series-parallel compliant articulated robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 385
EP  - 391
AU  - V. D. Amara
AU  - J. Malzahn
AU  - Z. Ren
AU  - W. Roozing
AU  - N. Tsagarakis
PY  - 2020
KW  - actuators
KW  - legged locomotion
KW  - optimisation
KW  - position control
KW  - robot dynamics
KW  - torque
KW  - torque control
KW  - series-parallel compliant articulated leg prototype
KW  - highly-efficient parallel actuation branches
KW  - torque allocation
KW  - transmission ratio
KW  - actuator hardware specifications
KW  - periodic squat motions
KW  - motion efficiency
KW  - parallel actuators
KW  - quadratic criteria
KW  - optimization based controller
KW  - redundant robots
KW  - torque distribution
KW  - series-parallel compliant articulated robots
KW  - Torque
KW  - Actuators
KW  - Joints
KW  - Tendons
KW  - Legged locomotion
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196786
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Torque distribution in redundant robots that combine the potential of asymmetric series-parallel actuated branches and multi-articulation pose a non-trivial challenge. To address the problem, this work proposes a novel optimization based controller that can accommodate various quadratic criteria to perform the torque distribution among dissimilar series and parallel actuators in order to maximize the motion efficiency. Three candidate criteria are composed and their performances are compared during periodic squat motions with a 3 degree of freedom series-parallel compliant articulated leg prototype. It is first shown that by minimizing a criterion that takes into account the actuator hardware specifications such as torque constant and transmission ratio, the gravity-driven phases can be lengthened. Thereby, this particular criterion results in slightly better performance than when adopting a strategy that maximizes the torque allocation to the higher efficiency actuators. Furthermore, valuable insights such as that the efficacy of maximum utilization of the highly-efficient parallel actuation branches decreases progressively at high frequencies were observed.
ER  - 

TY  - CONF
TI  - Preintegrated Velocity Bias Estimation to Overcome Contact Nonlinearities in Legged Robot Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 392
EP  - 398
AU  - D. Wisth
AU  - M. Camurri
AU  - M. Fallon
PY  - 2020
KW  - control nonlinearities
KW  - distance measurement
KW  - graph theory
KW  - image fusion
KW  - inertial navigation
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - robot vision
KW  - state estimation
KW  - preintegrated velocity bias estimation
KW  - contact nonlinearities
KW  - legged robot odometry
KW  - factor graph formulation
KW  - quadruped robot
KW  - slippery terrain
KW  - deformable terrain
KW  - preintegrated velocity factor
KW  - leg flexibility
KW  - leg odometry drift
KW  - IMU factors
KW  - ANYmal robot
KW  - proprioceptive state estimator
KW  - Legged locomotion
KW  - Robot sensing systems
KW  - Velocity measurement
KW  - Estimation
KW  - Kinematics
KW  - Foot
DO  - 10.1109/ICRA40945.2020.9197214
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a novel factor graph formulation to estimate the pose and velocity of a quadruped robot on slippery and deformable terrain. The factor graph introduces a preintegrated velocity factor that incorporates velocity inputs from leg odometry and also estimates related biases. From our experimentation we have seen that it is difficult to model uncertainties at the contact point such as slip or deforming terrain, as well as leg flexibility. To accommodate for these effects and to minimize leg odometry drift, we extend the robot's state vector with a bias term for this preintegrated velocity factor. The bias term can be accurately estimated thanks to the tight fusion of the preintegrated velocity factor with stereo vision and IMU factors, without which it would be unobservable. The system has been validated on several scenarios that involve dynamic motions of the ANYmal robot on loose rocks, slopes and muddy ground. We demonstrate a 26% improvement of relative pose error compared to our previous work and 52% compared to a state-of-the-art proprioceptive state estimator.
ER  - 

TY  - CONF
TI  - Optimized Foothold Planning and Posture Searching for Energy-Efficient Quadruped Locomotion over Challenging Terrains
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 399
EP  - 405
AU  - L. Chen
AU  - S. Ye
AU  - C. Sun
AU  - A. Zhang
AU  - G. Deng
AU  - T. Liao
PY  - 2020
KW  - energy conservation
KW  - legged locomotion
KW  - motion control
KW  - optimisation
KW  - search problems
KW  - torque control
KW  - posture searching
KW  - energy-efficient quadruped locomotion
KW  - energy-efficient locomotion
KW  - legged robot
KW  - quadrupedal robot
KW  - nominal stance parameters
KW  - leg torque distribution
KW  - foothold planner
KW  - standing legs
KW  - energy-saving stance posture
KW  - stairs climbing
KW  - stairs climbing
KW  - center of gravity trajectory planner
KW  - COG
KW  - foothold planning optimization
KW  - Legged locomotion
KW  - Torque
KW  - Thigh
KW  - Hip
KW  - Knee
KW  - Energy efficiency
DO  - 10.1109/ICRA40945.2020.9197135
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Energy-efficient locomotion is of primary importance for legged robot to extend operation time in practical applications. This paper presents an approach to achieve energy-efficient locomotion for a quadrupedal robot walking over challenging terrains. Firstly, we optimize the nominal stance parameters based on the analysis of leg torque distribution. Secondly, we proposed the foothold planner and the center of gravity (COG) trajectory planner working together to guide the robot to place its standing legs in an energy-saving stance posture. We have validated the effectiveness of our method on a real quadrupedal robot in experiments including autonomously walking on plain ground and climbing stairs.
ER  - 

TY  - CONF
TI  - Extracting Legged Locomotion Heuristics with Regularized Predictive Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 406
EP  - 412
AU  - G. Bledt
AU  - S. Kim
PY  - 2020
KW  - adaptive control
KW  - control system synthesis
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - optimisation
KW  - predictive control
KW  - legged locomotion heuristics
KW  - regularized predictive control
KW  - legged robots
KW  - dynamic maneuvers
KW  - difficult terrains
KW  - meaningful cost functions
KW  - high-fidelity models
KW  - timing restrictions
KW  - principled regularization heuristics
KW  - legged locomotion optimization control
KW  - cost space offline
KW  - desired commands
KW  - optimal control actions
KW  - robot states
KW  - heuristic candidates
KW  - adaptation laws
KW  - models online
KW  - powerful heuristics
KW  - approximate complex dynamics
KW  - model simplifications
KW  - parameter uncertainty
KW  - parameter tuning process
KW  - increased capabilities
KW  - newly extracted heuristics
KW  - controller structure
KW  - mini cheetah robot
KW  - Cost function
KW  - Legged locomotion
KW  - Data models
KW  - Tuning
KW  - Predictive control
DO  - 10.1109/ICRA40945.2020.9197488
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Optimization based predictive control is a powerful tool that has improved the ability of legged robots to execute dynamic maneuvers and traverse increasingly difficult terrains. However, it is often challenging and unintuitive to design meaningful cost functions and build high-fidelity models while adhering to timing restrictions. A novel framework to extract and design principled regularization heuristics for legged locomotion optimization control is presented. By allowing a simulation to fully explore the cost space offline, certain states and actions can be constrained or isolated. Data is fit with simple models relating the desired commands, optimal control actions, and robot states to identify new heuristic candidates. Basic parameter learning and adaptation laws are then applied to the models online. This method extracts simple, but powerful heuristics that can approximate complex dynamics and account for errors stemming from model simplifications and parameter uncertainty without the loss of physical intuition while generalizing the parameter tuning process. Results on the Mini Cheetah robot verify the increased capabilities due to the newly extracted heuristics without any modification to the controller structure or gains.
ER  - 

TY  - CONF
TI  - Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 413
EP  - 419
AU  - T. Li
AU  - N. Lambert
AU  - R. Calandra
AU  - F. Meier
AU  - A. Rai
PY  - 2020
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - path planning
KW  - predictive control
KW  - robot dynamics
KW  - robot kinematics
KW  - generalizable locomotion skills
KW  - hierarchical reinforcement learning
KW  - arbitrary goals
KW  - hierarchical framework
KW  - sample-efficiency
KW  - generalizability
KW  - learned locomotion skills
KW  - real-world robots
KW  - goal-oriented locomotion
KW  - diverse primitives skills
KW  - freedom robot
KW  - coarse dynamics models
KW  - primitive cycles
KW  - model predictive control framework
KW  - Daisy hexapod hardware
KW  - size 12.0 m
KW  - Hardware
KW  - Legged locomotion
KW  - Training
KW  - Task analysis
KW  - Planning
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196642
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning to locomote to arbitrary goals on hardware remains a challenging problem for reinforcement learning. In this paper, we present a hierarchical framework that improves sample-efficiency and generalizability of learned locomotion skills on real-world robots. Our approach divides the problem of goal-oriented locomotion into two sub-problems: learning diverse primitives skills, and using model-based planning to sequence these skills. We parametrize our primitives as cyclic movements, improving sample-efficiency of learning from scratch on a 18 degrees of freedom robot. Then, we learn coarse dynamics models over primitive cycles and use them in a model predictive control framework. This allows us to learn to walk to arbitrary goals up to 12m away, after about two hours of training from scratch on hardware. Our results on a Daisy hexapod hardware and simulation demonstrate the efficacy of our approach at reaching distant targets, in different environments, and with sensory noise.
ER  - 

TY  - CONF
TI  - SoRX: A Soft Pneumatic Hexapedal Robot to Traverse Rough, Steep, and Unstable Terrain
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 420
EP  - 426
AU  - Z. Liu
AU  - Z. Lu
AU  - K. Karydis
PY  - 2020
KW  - actuators
KW  - legged locomotion
KW  - motion control
KW  - pneumatic actuators
KW  - SoRX
KW  - soft pneumatic hexapedal robot
KW  - 2-degree-of-freedom soft pneumatic actuator
KW  - tripod gait
KW  - pneumatically-actuated legged robots
KW  - rough terrain
KW  - steep terrain
KW  - open-loop control
KW  - cyclic foot trajectories
KW  - legged locomotion
KW  - physical testing
KW  - Legged locomotion
KW  - Pneumatic systems
KW  - Soft robotics
KW  - Pneumatic actuators
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196731
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robotics technology creates new ways for legged robots to interact with and adapt to their environment. In this paper we develop i) a new 2-degree-of-freedom soft pneumatic actuator, and ii) a novel soft robotic hexapedal robot called SoRX that leverages the new actuators. Simulation and physical testing confirm that the proposed actuator can generate cyclic foot trajectories that are appropriate for legged locomotion. Consistent with other hexapedal robots (and animals), SoRX employs an alternating tripod gait to propel itself forward. Experiments reveal that SoRX can reach forward speeds of up to 0.44 body lengths per second, or equivalently 101 mm/s. With a size of 230 mm length, 140 mm width and 100 mm height, and weight of 650 grams, SoRX is among the fastest tethered soft pneumatically-actuated legged robots to date. The motion capabilities of SoRX are evaluated through five experiments: running, step climbing, and traversing rough terrain, steep terrain, and unstable terrain. Experimental results show that SoRX is able to operate over challenging terrains in open-loop control and by following the same alternating tripod gait across all experimental cases.
ER  - 

TY  - CONF
TI  - UBAT: On Jointly Optimizing UAV Trajectories and Placement of Battery Swap Stations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 427
EP  - 433
AU  - M. Won
PY  - 2020
KW  - ant colony optimisation
KW  - autonomous aerial vehicles
KW  - battery powered vehicles
KW  - electric vehicles
KW  - optimal number
KW  - charging stations
KW  - UBAT
KW  - ant colony optimization
KW  - UAV trajectories
KW  - battery swap stations
KW  - unmanned aerial vehicles
KW  - UAVs
KW  - flight time
KW  - charging station deployment problem
KW  - NP-hard problem
KW  - Charging stations
KW  - Batteries
KW  - Trajectory
KW  - Optimization
KW  - Sensors
KW  - Unmanned aerial vehicles
KW  - Euclidean distance
DO  - 10.1109/ICRA40945.2020.9197227
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unmanned aerial vehicles (UAVs) have been widely used in many applications. The limited flight time of UAVs, however, still remains as a major challenge. Although numerous approaches have been developed to recharge the battery of UAVs effectively, little is known about optimal methodologies to deploy charging stations. In this paper, we address the charging station deployment problem with an aim to find the optimal number and locations of charging stations such that the system performance is maximized. We show that the problem is NP-Hard and propose UBAT, a heuristic framework based on the ant colony optimization (ACO) to solve the problem. Additionally, a suite of algorithms are designed to enhance the execution time and the quality of the solutions for UBAT. Through extensive simulations, we demonstrate that UBAT effectively performs multi-objective optimization of generation of UAV trajectories and placement of charging stations that are within 8.3% and 7.3% of the true optimal solutions, respectively.
ER  - 

TY  - CONF
TI  - Efficient Multi-Agent Trajectory Planning with Feasibility Guarantee using Relative Bernstein Polynomial
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 434
EP  - 440
AU  - J. Park
AU  - J. Kim
AU  - I. Jang
AU  - H. J. Kim
PY  - 2020
KW  - collision avoidance
KW  - multi-agent systems
KW  - optimisation
KW  - polynomials
KW  - optimization-based approaches
KW  - erroneous optimization setup
KW  - infeasible collision constraints
KW  - sequential optimization method
KW  - dummy agents
KW  - relative Bernstein polynomial
KW  - nonconvex collision avoidance constraints
KW  - multiagent trajectory planning problems
KW  - obstacle-dense environments
KW  - grid-based approaches
KW  - Trajectory
KW  - Planning
KW  - Heuristic algorithms
KW  - Collision avoidance
KW  - Optimization
KW  - System recovery
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197162
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a new efficient algorithm which guarantees a solution for a class of multi-agent trajectory planning problems in obstacle-dense environments. Our algorithm combines the advantages of both grid-based and optimization-based approaches, and generates safe, dynamically feasible trajectories without suffering from an erroneous optimization setup such as imposing infeasible collision constraints. We adopt a sequential optimization method with dummy agents to improve the scalability of the algorithm, and utilize the convex hull property of Bernstein and relative Bernstein polynomial to replace non-convex collision avoidance constraints to convex ones. The proposed method can compute the trajectory for 64 agents on average 6.36 seconds with Intel Core i7-7700 @ 3.60GHz CPU and 16G RAM, and it reduces more than 50% of the objective cost compared to our previous work. We validate the proposed algorithm through simulation and flight tests.
ER  - 

TY  - CONF
TI  - Optimal Sequential Task Assignment and Path Finding for Multi-Agent Robotic Assembly Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 441
EP  - 447
AU  - K. Brown
AU  - O. Peltzer
AU  - M. A. Sehr
AU  - M. Schwager
AU  - M. J. Kochenderfer
PY  - 2020
KW  - assembly planning
KW  - collision avoidance
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - robotic assembly
KW  - nonholonomic differential-drive robots
KW  - optimal sequential task assignment
KW  - collision-free trajectories
KW  - robotic manufacturing
KW  - collision-free routing
KW  - multiagent robotic assembly planning
KW  - path finding
KW  - Robots
KW  - Task analysis
KW  - Schedules
KW  - Manufacturing
KW  - Collision avoidance
KW  - Routing
KW  - Production facilities
DO  - 10.1109/ICRA40945.2020.9197527
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study the problem of sequential task assignment and collision-free routing for large teams of robots in applications with inter-task precedence constraints (e.g., task A and task B must both be completed before task C may begin). Such problems commonly occur in assembly planning for robotic manufacturing applications, in which sub-assemblies must be completed before they can be combined to form the final product. We propose a hierarchical algorithm for computing makespan-optimal solutions to the problem. The algorithm is evaluated on a set of randomly generated problem instances where robots must transport objects between stations in a "factory" grid world environment. In addition, we demonstrate in high-fidelity simulation that the output of our algorithm can be used to generate collision-free trajectories for non-holonomic differential-drive robots.
ER  - 

TY  - CONF
TI  - Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 448
EP  - 454
AU  - R. Han
AU  - S. Chen
AU  - Q. Hao
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-robot systems
KW  - navigation
KW  - path planning
KW  - optimal paths
KW  - multiple robots
KW  - dynamics randomization
KW  - differential drive robots
KW  - dynamic environment
KW  - obstacle complexities
KW  - multirobot navigation problem
KW  - deep reinforcement learning framework
KW  - optimal target locations
KW  - DRL based framework
KW  - navigation policy
KW  - Collision avoidance
KW  - Navigation
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Training
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9197209
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The challenges of multi-robot navigation in dynamic environments lie in uncertainties in obstacle complexities, partially observation of robots, and policy implementation from simulations to the real world. This paper presents a cooperative approach to address the multi-robot navigation problem (MRNP) under dynamic environments using a deep reinforcement learning (DRL) framework, which can help multiple robots jointly achieve optimal paths despite a certain degree of obstacle complexities. The novelty of this work includes threefold: (1) developing a cooperative architecture that robots can exchange information with each other to select the optimal target locations; (2) developing a DRL based framework which can learn a navigation policy to generate the optimal paths for multiple robots; (3) developing a training mechanism based on dynamics randomization which can make the policy generalized and achieve the maximum performance in the real world. The method is tested with Gazebo simulations and 4 differential drive robots. Both simulation and experiment results validate the superior performance of the proposed method in terms of success rate and travel time when compared with the other state-of-art technologies.
ER  - 

TY  - CONF
TI  - Adaptive Directional Path Planner for Real-Time, Energy-Efficient, Robust Navigation of Mobile Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 455
EP  - 461
AU  - M. R. Nimmagadda
AU  - S. Dattawadkar
AU  - S. Muthukumar
AU  - V. Honkote
PY  - 2020
KW  - energy conservation
KW  - graph theory
KW  - mobile robots
KW  - path planning
KW  - robust control
KW  - sample based methods
KW  - sub-optimal memory-intensive
KW  - Adaptive Directional Planner algorithm
KW  - robust local path planning
KW  - autonomous navigation
KW  - form factor mobile robots
KW  - low memory footprint
KW  - robust navigation
KW  - unknown environments
KW  - complex environments
KW  - fundamental capability
KW  - robotic applications
KW  - optimal robot path planning
KW  - complex memory intensive task
KW  - adaptive directional path planner
KW  - ADP algorithm implementation
KW  - memory size 28.0 KByte
KW  - Mobile robots
KW  - Trajectory
KW  - Real-time systems
KW  - Navigation
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197417
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous navigation through unknown and complex environments is a fundamental capability that is essential in almost all robotic applications. Optimal robot path planning is critical to enable efficient navigation. Path planning is a complex, compute and memory intensive task. Traditional methods employ either graph based search methods or sample based methods to implement path planning, which are sub-optimal and compute/memory-intensive. To this end, an Adaptive Directional Planner (ADP) algorithm is devised to achieve real-time, energy-efficient, memory-optimized, robust local path planning for enabling efficient autonomous navigation of mobile robots. The ADP algorithm ensures that the paths are optimal and kinematically-feasible. Further, the proposed algorithm is tested with different challenging scenarios verifying the functionality and robustness. The ADP algorithm implementation results demonstrate 40- 60X less number of nodes and 40 - 50X less execution time compared to the standard TP-RRT schemes, without compromising on accuracy. Finally, the algorithm has also been implemented as an accelerator for non-holonomic, multi-shape, small form factor mobile robots to provide a silicon solution with high performance and low memory footprint (28KB).
ER  - 

TY  - CONF
TI  - Exploiting sparsity in robot trajectory optimization with direct collocation and geometric algorithms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 469
EP  - 475
AU  - D. Cardona-Ortiz
AU  - A. Paz
AU  - G. Arechavaleta
PY  - 2020
KW  - geometry
KW  - Lie algebras
KW  - Lie groups
KW  - optimal control
KW  - optimisation
KW  - robots
KW  - trajectory control
KW  - geometric algorithm
KW  - robot trajectory optimization
KW  - Lie group method
KW  - floating-point operations
KW  - first-order information
KW  - sparsity exploitation
KW  - numerical differentiation
KW  - analytical differentiation
KW  - Lie algebras
KW  - state equations
KW  - recursive algorithms
KW  - articulated robots
KW  - direct collocation algorithm
KW  - Robots
KW  - Heuristic algorithms
KW  - Jacobian matrices
KW  - Optimal control
KW  - System dynamics
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9196668
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a robot trajectory optimization formulation that builds upon numerical optimal control and Lie group methods. In particular, the inherent sparsity of direct collocation is carefully analyzed to dramatically reduce the number of floating-point operations to get first-order information of the problem. We describe how sparsity exploitation is employed with both numerical and analytical differentiation. Furthermore, the use of geometric algorithms based on Lie groups and their associated Lie algebras allow to analytically evaluate the state equations and their derivatives with efficient recursive algorithms. We demonstrate the scalability of the proposed formulation with three different articulated robots, such as a finger, a mobile manipulator and a humanoid composed of five, eight and more than twenty degrees of freedom, respectively. The performance of our implementation in C++ is also validated and compared against a state-of-the-art general purpose numerical optimal control solver.
ER  - 

TY  - CONF
TI  - Bi-Convex Approximation of Non-Holonomic Trajectory Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 476
EP  - 482
AU  - A. K. Singh
AU  - R. Ram Theerthala
AU  - M. Babu
AU  - U. K. R. Nair
AU  - K. Madhava Krishna
PY  - 2020
KW  - approximation theory
KW  - convex programming
KW  - minimisation
KW  - quadratic programming
KW  - robot kinematics
KW  - nonholonomic trajectory optimization
KW  - nonholonomic kinematics
KW  - nonlinearly maps control input
KW  - nonconvex
KW  - bi-convex cost
KW  - constraint functions
KW  - bi-convex part
KW  - nonholonomic behavior
KW  - nonlinear penalty
KW  - nonlinear costs
KW  - bi-convex structure
KW  - bi-convex approximation
KW  - autonomous cars
KW  - fixed-wing aerial vehicles
KW  - computational tractability
KW  - alternating minimization
KW  - sequential quadratic programming
KW  - interior-point methods
KW  - Trajectory optimization
KW  - Minimization
KW  - Computational modeling
KW  - Collision avoidance
KW  - Robots
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9197092
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous cars and fixed-wing aerial vehicles have the so-called non-holonomic kinematics which non-linearly maps control input to states. As a result, trajectory optimization with such a motion model becomes highly non-linear and non-convex. In this paper, we improve the computational tractability of non-holonomic trajectory optimization by reformulating it in terms of a set of bi-convex cost and constraint functions along with a non-linear penalty. The bi-convex part acts as a relaxation for the non-holonomic trajectory optimization while the residual of the penalty dictates how well its output obeys the non-holonomic behavior. We adopt an alternating minimization approach for solving the reformulated problem and show that it naturally leads to the replacement of the challenging non-linear penalty with a globally valid convex surrogate. Along with the common cost functions modeling goal-reaching, trajectory smoothness, etc., the proposed optimizer can also accommodate a class of non-linear costs for modeling goal-sets, while retaining the bi-convex structure. We benchmark the proposed optimizer against off-the-shelf solvers implementing sequential quadratic programming and interior-point methods and show that it produces solutions with similar or better cost as the former while significantly outperforming the latter. Furthermore, as compared to both off-the-shelf solvers, the proposed optimizer achieves more than 20x reduction in computation time.
ER  - 

TY  - CONF
TI  - Fast, Versatile, and Open-loop Stable Running Behaviors with Proprioceptive-only Sensing using Model-based Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 483
EP  - 489
AU  - W. Gao
AU  - C. Young
AU  - J. Nicholson
AU  - C. Hubicki
AU  - J. Clark
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - open loop systems
KW  - optimisation
KW  - robot dynamics
KW  - stability
KW  - state feedback
KW  - model-based trajectory optimization
KW  - direct-drive robot
KW  - direct-collocation-formulated optimization
KW  - single-legged planar robot
KW  - open-loop stable motion primitives
KW  - proprioceptive-only sensing
KW  - versatile dynamic behaviors
KW  - expensive inertial sensors
KW  - agile control
KW  - stable control
KW  - model-based optimization
KW  - open-loop stable running behaviors
KW  - Legged locomotion
KW  - Force
KW  - Optimization
KW  - Springs
KW  - Modulation
KW  - Hip
DO  - 10.1109/ICRA40945.2020.9196542
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As we build our legged robots smaller and cheaper, stable and agile control without expensive inertial sensors becomes increasingly important. We seek to enable versatile dynamic behaviors on robots with limited modes of state feedback, specifically proprioceptive-only sensing. This work uses model-based trajectory optimization methods to design open-loop stable motion primitives. We specifically design running gaits for a single-legged planar robot, and can generate motion primitives in under 3 seconds, approaching online-capable speeds. A direct-collocation-formulated optimization generated axial force profiles for the direct-drive robot to achieve desired running speed and apex height. When implemented in hardware, these trajectories produced open-loop stable running. Further, the measured running achieved the desired speed within 10% of the speed specified for the optimization in spite of having no control loop actively measuring or controlling running speed. Additionally, we examine the shape of the optimized force profile and observe features that may be applicable to open-loop stable running in general.
ER  - 

TY  - CONF
TI  - Wasserstein Distributionally Robust Motion Planning and Control with Safety Constraints Using Conditional Value-at-Risk
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 490
EP  - 496
AU  - A. Hakobyan
AU  - I. Yang
PY  - 2020
KW  - collision avoidance
KW  - decision making
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - predictive control
KW  - probability
KW  - robust control
KW  - Wasserstein distributionally robust motion planning
KW  - safety constraints
KW  - conditional value-at-risk
KW  - optimization-based decision-making tool
KW  - safe motion planning
KW  - pre-specified threshold
KW  - probability distribution
KW  - obstacles
KW  - Wasserstein ball
KW  - available empirical distribution
KW  - out-of-sample performance guarantee
KW  - risk constraint
KW  - computationally tractable method
KW  - distributionally robust model predictive control problem
KW  - distributionally robust method
KW  - Robustness
KW  - Safety
KW  - Planning
KW  - Robots
KW  - Trajectory
KW  - Probability distribution
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196857
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose an optimization-based decision-making tool for safe motion planning and control in an environment with randomly moving obstacles. The unique feature of the proposed method is that it limits the risk of unsafety by a pre-specified threshold even when the true probability distribution of the obstacles' movements deviates, within a Wasserstein ball, from an available empirical distribution. Another advantage is that it provides a probabilistic out-of-sample performance guarantee of the risk constraint. To develop a computationally tractable method for solving the distributionally robust model predictive control problem, we propose a set of reformulation procedures using (i) the Kantorovich duality principle, (ii) the extremal representation of conditional value-at-risk, and (iii) a geometric expression of the distance to the union of halfspaces. The performance and utility of this distributionally robust method are demonstrated through simulations using a 12D quadrotor model in a 3D environment.
ER  - 

TY  - CONF
TI  - Grasping Fragile Objects Using A Stress-Minimization Metric
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 517
EP  - 523
AU  - Z. Pan
AU  - X. Gao
AU  - D. Manocha
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - minimisation
KW  - stress-minimization metric
KW  - homogeneous isotopic materials
KW  - SM metric measures
KW  - fragile objects grasping
KW  - optimal grasp planning algorithms
KW  - Measurement
KW  - Planning
KW  - Force
KW  - Tensile stress
KW  - Grasping
KW  - Resilience
DO  - 10.1109/ICRA40945.2020.9196938
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a new method to generate optimal grasps for brittle and fragile objects using a novel stress- minimization (SM) metric. Our approach is designed for objects that are composed of homogeneous isotopic materials. Our SM metric measures the maximal resistible external wrenches that would not result in fractures in the target objects. In this paper, we propose methods to compute our new metric. We also use our SM metric to design optimal grasp planning algorithms. Finally, we compare the performance of our metric and conventional grasp metrics, including Q1,Q∞,QG11,QMSV,QVEW. Our experiments show that our SM metric takes into account the material characteristics and object shapes to indicate the fragile regions, where prior methods may not work well. We also show that the computational cost of our SM metric is on par with prior methods. Finally, we show that grasp planners guided by our metric can lower the probability of breaking target objects.
ER  - 

TY  - CONF
TI  - Grasp Control for Enhancing Dexterity of Parallel Grippers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 524
EP  - 530
AU  - M. Costanzo
AU  - G. De Maria
AU  - G. Lettera
AU  - C. Natale
PY  - 2020
KW  - dexterous manipulators
KW  - friction
KW  - grippers
KW  - manipulator dynamics
KW  - materials handling
KW  - motion control
KW  - path planning
KW  - tactile sensors
KW  - grasp control
KW  - enhancing dexterity
KW  - parallel grippers
KW  - robust grasp controller
KW  - slipping avoidance
KW  - model-based algorithm
KW  - modified LuGre friction model
KW  - rotational frictional sliding motions
KW  - limit surface concept
KW  - computationally efficient method
KW  - minimum grasping force
KW  - tangential loads
KW  - torsional loads
KW  - control modalities
KW  - robot motion
KW  - automatically generates robot motions
KW  - gripper commands
KW  - Force
KW  - Grippers
KW  - Friction
KW  - Dynamics
KW  - Mathematical model
KW  - Computational modeling
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196873
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A robust grasp controller for both slipping avoidance and controlled sliding is proposed based on force/tactile feedback only. The model-based algorithm exploits a modified LuGre friction model to consider rotational frictional sliding motions. The modification relies on the Limit Surface concept where a novel computationally efficient method is introduced to compute in real-time the minimum grasping force to balance tangential and torsional loads. The two control modalities are considered by the robot motion planning algorithm that automatically generates robot motions and gripper commands to solve complex manipulation tasks in a material handling application.
ER  - 

TY  - CONF
TI  - Theoretical Derivation and Realization of Adaptive Grasping Based on Rotational Incipient Slip Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 531
EP  - 537
AU  - T. Narita
AU  - S. Nagakari
AU  - W. Conus
AU  - T. Tsuboi
AU  - K. Nagasaka
PY  - 2020
KW  - adaptive control
KW  - force control
KW  - friction
KW  - grippers
KW  - manipulators
KW  - robust control
KW  - object manipulation
KW  - grasp force control algorithm
KW  - adaptive grasping
KW  - rotational incipient slip detection
KW  - robotics
KW  - incipient slip robust detection
KW  - center of gravity
KW  - friction coefficient
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196615
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Manipulating objects whose physical properties are unknown remains one of the greatest challenges in robotics. Controlling grasp force is an essential aspect of handling unknown objects without slipping or crushing them. Although extensive research has been carried out on grasp force control, unknown object manipulation is still difficult because conventional approaches assume that object properties (mass, center of gravity, friction coefficient, etc.) are known for grasp force control. One of the approaches to address this issue is incipient slip detection. However, there has been few detailed investigations of robust detection and control of incipient slip on rotational case. This study makes contributions on deriving the theoretical model of incipient slip and proposes a new algorithm to detect incipient slip. Additionally, a novel sensor configuration and a grasp force control algorithm based on the derived theoretical model are proposed. Finally, the proposed algorithm is evaluated by grasping objects with different weights and moments including a fragile pastry (éclair).
ER  - 

TY  - CONF
TI  - Grasp State Assessment of Deformable Objects Using Visual-Tactile Fusion Perception
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 538
EP  - 544
AU  - S. Cui
AU  - R. Wang
AU  - J. Wei
AU  - F. Li
AU  - S. Wang
PY  - 2020
KW  - convolutional neural nets
KW  - deformation
KW  - dexterous manipulators
KW  - force control
KW  - grippers
KW  - image classification
KW  - image fusion
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - tactile sensors
KW  - dexterity grasping
KW  - automatic force control
KW  - classification
KW  - tactile sensor
KW  - wrist camera
KW  - robotic arm
KW  - deformable objects
KW  - 3D convolution based visual tactile fusion deep neural network
KW  - adaptive grasping
KW  - extensive grasping
KW  - C3D-VTFN
KW  - sliding deformation
KW  - grasp state assessment
KW  - Visualization
KW  - Grasping
KW  - Feature extraction
KW  - Task analysis
KW  - Tactile sensors
DO  - 10.1109/ICRA40945.2020.9196787
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Humans can quickly determine the force required to grasp a deformable object to prevent its sliding or excessive deformation through vision and touch, which is still a challenging task for robots. To address this issue, we propose a novel 3D convolution-based visual-tactile fusion deep neural network (C3D-VTFN) to evaluate the grasp state of various deformable objects in this paper. Specifically, we divide the grasp states of deformable objects into three categories of sliding, appropriate and excessive. Also, a dataset for training and testing the proposed network is built by extensive grasping and lifting experiments with different widths and forces on 16 various deformable objects with a robotic arm equipped with a wrist camera and a tactile sensor. As a result, a classification accuracy as high as 99.97% is achieved. Furthermore, some delicate grasp experiments based on the proposed network are implemented in this paper. The experimental results demonstrate that the C3D-VTFN is accurate and efficient enough for grasp state assessment, which can be widely applied to automatic force control, adaptive grasping, and other visual-tactile spatiotemporal sequence learning problems.
ER  - 

TY  - CONF
TI  - Beyond Top-Grasps Through Scene Completion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 545
EP  - 551
AU  - J. Lundell
AU  - F. Verdoja
AU  - V. Kyrki
PY  - 2020
KW  - cameras
KW  - end effectors
KW  - grippers
KW  - image colour analysis
KW  - image sensors
KW  - path planning
KW  - position control
KW  - robot vision
KW  - camera images
KW  - grasp success rate
KW  - simulated images
KW  - top-grasps
KW  - scene completion
KW  - six-degree-of-freedom grasps
KW  - simulated viewpoints
KW  - generation method
KW  - fully convolutional grasp quality CNN
KW  - end-to-end grasp
KW  - Shape
KW  - Cameras
KW  - Grasping
KW  - Planning
KW  - Robot vision systems
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197320
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Current end-to-end grasp planning methods propose grasps in the order of seconds that attain high grasp success rates on a diverse set of objects, but often by constraining the workspace to top-grasps. In this work, we present a method that allows end-to-end top-grasp planning methods to generate full six-degree-of-freedom grasps using a single RGBD view as input. This is achieved by estimating the complete shape of the object to be grasped, then simulating different viewpoints of the object, passing the simulated viewpoints to an end-to-end grasp generation method, and finally executing the overall best grasp. The method was experimentally validated on a Franka Emika Panda by comparing 429 grasps generated by the state-of-the-art Fully Convolutional Grasp Quality CNN, both on simulated and real camera images. The results show statistically significant improvements in terms of grasp success rate when using simulated images over real camera images, especially when the real camera viewpoint is angled. Code and video are available at https://irobotics.aalto.fi/beyond-topgrasps-through-scene-completion/.
ER  - 

TY  - CONF
TI  - Dex-Net AR: Distributed Deep Grasp Planning Using a Commodity Cellphone and Augmented Reality App
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 552
EP  - 558
AU  - H. Zhang
AU  - J. Ichnowski
AU  - Y. Avigal
AU  - J. Gonzales
AU  - I. Stoica
AU  - K. Goldberg
PY  - 2020
KW  - Apple computers
KW  - augmented reality
KW  - computational geometry
KW  - control engineering computing
KW  - dexterous manipulators
KW  - distributed processing
KW  - image colour analysis
KW  - image sequences
KW  - mobile computing
KW  - path planning
KW  - robot vision
KW  - smart phones
KW  - distributed deep grasp planning
KW  - commodity cellphone
KW  - augmented reality app
KW  - consumer demand
KW  - mobile phone applications
KW  - AR apps
KW  - RGB image sequence
KW  - point clouds
KW  - distributed pipeline
KW  - Dex-Net AR
KW  - Dex-Net grasp planner
KW  - error estimation
KW  - robot gripper
KW  - iPhone
KW  - ARKit
KW  - Three-dimensional displays
KW  - Cameras
KW  - Planning
KW  - Sensors
KW  - Smart phones
KW  - Feature extraction
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197247
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Consumer demand for augmented reality (AR) in mobile phone applications, such as the Apple ARKit. Such applications have potential to expand access to robot grasp planning systems such as Dex-Net. AR apps use structure from motion methods to compute a point cloud from a sequence of RGB images taken by the camera as it is moved around an object. However, the resulting point clouds are often noisy due to estimation errors. We present a distributed pipeline, Dex-Net AR, that allows point clouds to be uploaded to a server in our lab, cleaned, and evaluated by Dex-Net grasp planner to generate a grasp axis that is returned and displayed as an overlay on the object. We implement Dex-Net AR using the iPhone and ARKit and compare results with those generated with high-performance depth sensors. The success rates with AR on harder adversarial objects are higher than traditional depth images. The server URL is https://sites.google.com/berkeley.edu/dex-net-ar/home.
ER  - 

TY  - CONF
TI  - OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 559
EP  - 566
AU  - C. Won
AU  - H. Seok
AU  - Z. Cui
AU  - M. Pollefeys
AU  - J. Lim
PY  - 2020
KW  - cameras
KW  - computerised instrumentation
KW  - distance measurement
KW  - image matching
KW  - image reconstruction
KW  - image sequences
KW  - neural nets
KW  - stereo image processing
KW  - omnidirectional localization
KW  - wide-baseline multicamera systems
KW  - dense mapping system
KW  - wide-baseline multiview stereo setup
KW  - ultrawide field-of-view fisheye cameras
KW  - stereo observations
KW  - light-weighted deep neural networks
KW  - loop closing module
KW  - efficient feature matching process
KW  - omnidirectional depth maps
KW  - truncated signed distance function volume
KW  - rig estimation
KW  - omnidirectional depth map estimation
KW  - VO
KW  - FOV
KW  - TSDF
KW  - OmniSLAM
KW  - Cameras
KW  - Three-dimensional displays
KW  - Estimation
KW  - Feature extraction
KW  - Visual odometry
KW  - Sensors
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196695
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360° coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.
ER  - 

TY  - CONF
TI  - What’s in my Room? Object Recognition on Indoor Panoramic Images
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 567
EP  - 573
AU  - J. Guerrero-Viu
AU  - C. Fernandez-Labrador
AU  - C. Demonceaux
AU  - J. J. Guerrero
PY  - 2020
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - natural scenes
KW  - neural nets
KW  - object detection
KW  - object recognition
KW  - solid modelling
KW  - 3D model
KW  - instance segmentation masks
KW  - equirectangular images
KW  - deep learning
KW  - semantic segmentation tasks
KW  - object detection
KW  - object recognition system
KW  - indoor scenes
KW  - indoor panoramic images
KW  - Image segmentation
KW  - Object recognition
KW  - Three-dimensional displays
KW  - Semantics
KW  - Task analysis
KW  - Layout
KW  - Distortion
DO  - 10.1109/ICRA40945.2020.9197335
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the last few years, there has been a growing interest in taking advantage of the 360° panoramic images potential, while managing the new challenges they imply. While several tasks have been improved thanks to the contextual information these images offer, object recognition in indoor scenes still remains a challenging problem that has not been deeply investigated. This paper provides an object recognition system that performs object detection and semantic segmentation tasks by using a deep learning model adapted to match the nature of equirectangular images. From these results, instance segmentation masks are recovered, refined and transformed into 3D bounding boxes that are placed into the 3D model of the room. Quantitative and qualitative results support that our method outperforms the state of the art by a large margin and show a complete understanding of the main objects in indoor scenes.
ER  - 

TY  - CONF
TI  - FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation using Monocular Fisheye Camera for Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 574
EP  - 581
AU  - V. R. Kumar
AU  - S. A. Hiremath
AU  - M. Bach
AU  - S. Milz
AU  - C. Witt
AU  - C. Pinard
AU  - S. Yogamani
AU  - P. Mäder
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image classification
KW  - image motion analysis
KW  - image sampling
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - road vehicles
KW  - robot vision
KW  - traffic engineering computing
KW  - video signal processing
KW  - autonomous driving
KW  - nonlinear distortions
KW  - complex algorithms
KW  - Euclidean distance estimation
KW  - fisheye cameras
KW  - automotive scenes
KW  - accurate depth supervision
KW  - dense depth supervision
KW  - self-supervised learning approaches
KW  - self-supervised scale-aware framework
KW  - raw monocular fisheye videos
KW  - applying rectification
KW  - piece-wise linear approximation
KW  - fisheye projection surface
KW  - re-sampling distortion
KW  - monocular methods
KW  - unseen fisheye video
KW  - self-supervised scale-aware distance estimation
KW  - monocular fisheye camera
KW  - Cameras
KW  - Estimation
KW  - Training
KW  - Euclidean distance
KW  - Image reconstruction
KW  - Three-dimensional displays
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197319
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fisheye cameras are commonly used in applications like autonomous driving and surveillance to provide a large field of view (> 180o). However, they come at the cost of strong non-linear distortions which require more complex algorithms. In this paper, we explore Euclidean distance estimation on fisheye cameras for automotive scenes. Obtaining accurate and dense depth supervision is difficult in practice, but self-supervised learning approaches show promising results and could potentially overcome the problem. We present a novel self-supervised scale-aware framework for learning Euclidean distance and ego-motion from raw monocular fisheye videos without applying rectification. While it is possible to perform piece-wise linear approximation of fisheye projection surface and apply standard rectilinear models, it has its own set of issues like re-sampling distortion and discontinuities in transition regions. To encourage further research in this area, we will release our dataset as part of the WoodScape project [1]. We further evaluated the proposed algorithm on the KITTI dataset and obtained state-of-the-art results comparable to other self-supervised monocular methods. Qualitative results on an unseen fisheye video demonstrate impressive performance1.
ER  - 

TY  - CONF
TI  - 360SD-Net: 360° Stereo Depth Estimation with Learnable Cost Volume
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 582
EP  - 588
AU  - N. -H. Wang
AU  - B. Solarte
AU  - Y. -H. Tsai
AU  - W. -C. Chiu
AU  - M. Sun
PY  - 2020
KW  - neural nets
KW  - stereo image processing
KW  - 360SD-Net
KW  - stereo depth estimation
KW  - learnable cost volume
KW  - end-to-end trainable deep neural networks
KW  - perspective images
KW  - equirectangular projection
KW  - distortion issue
KW  - learnable shifting filter
KW  - stereo datasets
KW  - camera pairs
KW  - spherical disparity
KW  - Cameras
KW  - Estimation
KW  - Three-dimensional displays
KW  - Distortion
KW  - Feature extraction
KW  - Convolution
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196975
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recently, end-to-end trainable deep neural networks have significantly improved stereo depth estimation for perspective images. However, 360° images captured under equirectangular projection cannot benefit from directly adopting existing methods due to distortion introduced (i.e., lines in 3D are not projected onto lines in 2D). To tackle this issue, we present a novel architecture specifically designed for spherical disparity using the setting of top-bottom 360° camera pairs. Moreover, we propose to mitigate the distortion issue by (1) an additional input branch capturing the position and relation of each pixel in the spherical coordinate, and (2) a cost volume built upon a learnable shifting filter. Due to the lack of 360° stereo data, we collect two 360° stereo datasets from Matterport3D and Stanford3D for training and evaluation. Extensive experiments and ablation study are provided to validate our method against existing algorithms. Finally, we show promising results on real-world environments capturing images with two consumer-level cameras. Our project page is at https://albert100121.github.io/360SD-Net-Project-Page.
ER  - 

TY  - CONF
TI  - Omnidirectional Depth Extension Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 589
EP  - 595
AU  - X. Cheng
AU  - P. Wang
AU  - Y. Zhou
AU  - C. Guan
AU  - R. Yang
PY  - 2020
KW  - calibration
KW  - cameras
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - image reconstruction
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - stereo image processing
KW  - omnidirectional depth extension networks
KW  - omnidirectional 360° camera
KW  - autonomous robots
KW  - perception ability
KW  - FoV
KW  - depth sensors
KW  - perception system
KW  - low-cost 3D sensing system
KW  - omnidirectional camera
KW  - calibrated projective depth camera
KW  - recorded omnidirectional image
KW  - omnidirectional depth extension convolutional neural network
KW  - spherical feature
KW  - feature encoding layers
KW  - deformable convolutional spatial propagation network
KW  - feature decoding layers
KW  - omnidirectional coordination
KW  - projective coordination
KW  - feature learning
KW  - estimated depths
KW  - proposed ODE-CNN
KW  - popular 360D dataset
KW  - depth error
KW  - Convolution
KW  - Estimation
KW  - Sensors
KW  - Cameras
KW  - Transforms
KW  - Kernel
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197123
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Omnidirectional 360° camera proliferates rapidly for autonomous robots since it significantly enhances the perception ability by widening the field of view (FoV). However, corresponding 360° depth sensors, which are also critical for the perception system, are still difficult or expensive to have. In this paper, we propose a low-cost 3D sensing system that combines an omnidirectional camera with a calibrated projective depth camera, where the depth from the limited FoV can be automatically extended to the rest of recorded omnidirectional image. To accurately recover the missing depths, we design an omnidirectional depth extension convolutional neural network (ODE-CNN), in which a spherical feature transform layer (SFTL) is embedded at the end of feature encoding layers, and a deformable convolutional spatial propagation network (D-CSPN) is appended at the end of feature decoding layers. The former re-samples the neighborhood of each pixel in the omnidirectional coordination to the projective coordination, which reduce the difficulty of feature learning, and the later automatically finds a proper context to well align the structures in the estimated depths via CNN w.r.t. the reference image, which significantly improves the visual quality. Finally, we demonstrate the effectiveness of proposed ODE-CNN over the popular 360D dataset, and show that ODE-CNN significantly outperforms (relatively 33% reduction in depth error) other state-of-the-art (SoTA) methods.
ER  - 

TY  - CONF
TI  - 3D Orientation Estimation and Vanishing Point Extraction from Single Panoramas Using Convolutional Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 596
EP  - 602
AU  - Y. Shi
AU  - X. Tong
AU  - J. Wen
AU  - H. Zhao
AU  - X. Ying
AU  - H. Zha
PY  - 2020
KW  - cameras
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - image classification
KW  - regression analysis
KW  - stereo image processing
KW  - convolutional neural network
KW  - 3D orientation estimation
KW  - computer vision
KW  - 3D scene understanding
KW  - single spherical panorama
KW  - labeled 3D orientation
KW  - vanishing point extraction
KW  - single panoramas
KW  - VOP60K
KW  - Google Street View
KW  - pinhole cameras
KW  - panorama geometric information
KW  - two column vector regression loss
KW  - rotation matrix
KW  - CNN architecture
KW  - classification loss
KW  - edge extractor layer
KW  - Cameras
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Google
KW  - Estimation
KW  - Data mining
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9196966
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - 3D orientation estimation is a key component of many important computer vision tasks such as autonomous navigation and 3D scene understanding. This paper presents a new CNN architecture to estimate the 3D orientation of an omnidirectional camera with respect to the world coordinate system from a single spherical panorama. To train the proposed architecture, we leverage a dataset of panoramas named VOP60K from Google Street View with labeled 3D orientation, including 50 thousand panoramas for training and 10 thousand panoramas for testing. Previous approaches usually estimate 3D orientation under pinhole cameras. However, for a panorama, due to its larger field of view, previous approaches cannot be suitable. In this paper, we propose an edge extractor layer to utilize the low-level and geometric information of panorama, an attention module to fuse different features generated by previous layers. A regression loss for two column vectors of the rotation matrix and classification loss for the position of vanishing points are added to optimize our network simultaneously. The proposed algorithm is validated on our benchmark, and experimental results clearly demonstrate that it outperforms previous methods.
ER  - 

TY  - CONF
TI  - Curvature sensing with a spherical tactile sensor using the color-interference of a marker array
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 603
EP  - 609
AU  - X. Lin
AU  - L. Willemet
AU  - A. Bailleul
AU  - M. Wiertlewski
PY  - 2020
KW  - cameras
KW  - colour
KW  - elasticity
KW  - force sensors
KW  - image colour analysis
KW  - indentation
KW  - tactile sensors
KW  - subtractive color-mixing principle
KW  - planar manufacturing process
KW  - functional features
KW  - ChromaTouch
KW  - millimeter-size indentation
KW  - elastic membrane
KW  - 3-dimensional displacement field
KW  - distributed tactile sensor
KW  - fine distributed sense
KW  - robots
KW  - fine manipulation
KW  - local shape
KW  - soft fingers
KW  - cues
KW  - marker array
KW  - color-interference
KW  - spherical tactile sensor
KW  - curvature sensing
KW  - normal sensing
KW  - contact mechanics
KW  - Hertz contact theory
KW  - curved surface
KW  - relative motion
KW  - colored patches
KW  - local 3d displacement
KW  - measurement points
KW  - spherical shape
KW  - flat functional panels
KW  - flat surface
KW  - size 40 mm
KW  - Shape
KW  - Cameras
KW  - Tactile sensors
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197050
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The only way to perceive a small object held between our fingers is to trust our sense of touch. Touch provides cues about the state of the contact even if its view is occluded by the finger. The interaction between the soft fingers and the surface reveals crucial information, such as the local shape of the object, that plays a central role in fine manipulation. In this work, we present a new spherical sensor that endows robots with a fine distributed sense of touch. This sensor is an evolution of our distributed tactile sensor that measures the dense 3-dimensional displacement field of an elastic membrane, using the subtractive color-mixing principle. We leverage a planar manufacturing process that enables the design and manufacturing of the functional features on a flat surface. The flat functional panels are then folded to create a spherical shape able to sense a wide variety of objects.The resulting 40mm-diameter spherical sensor has 77 measurement points, each of which gives an estimation of the local 3d displacement, normal and tangential to the surface. Each marker is built around 2 sets of colored patches placed at different depths. The relative motion and resulting hue of each marker, easily captured by an embedded RGB camera, provides a measurement of their 3d motion. To benchmark the sensor, we compared the measurements obtained while pressing the sensor on a curved surface with Hertz contact theory, a hallmark of contact mechanics. While the mechanics did strictly follow Hertz contact theory, using the shear and normal sensing, ChromaTouch can estimate the curvature of an object after a millimeter-size indentation of the sensor.
ER  - 

TY  - CONF
TI  - Center-of-Mass-based Robust Grasp Planning for Unknown Objects Using Tactile-Visual Sensors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 610
EP  - 617
AU  - Q. Feng
AU  - Z. Chen
AU  - J. Deng
AU  - C. Gao
AU  - J. Zhang
AU  - A. Knoll
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - path planning
KW  - robot vision
KW  - tactile sensors
KW  - Franka Emika robot arm
KW  - tactile sensors
KW  - multisensor modules
KW  - regrasp planner
KW  - slip detection
KW  - visual sensors
KW  - center-of-mass-based robust grasp planning
KW  - Grasping
KW  - Tactile sensors
KW  - Force
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196815
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - An unstable grasp pose can lead to slip, thus an unstable grasp pose can be predicted by slip detection. A regrasp is required afterwards to correct the grasp pose in order to finish the task. In this work, we propose a novel regrasp planner with multi-sensor modules to plan grasp adjustments with the feedback from a slip detector. Then a regrasp planner is trained to estimate the location of center of mass, which helps robots find an optimal grasp pose. The dataset in this work consists of 1 025 slip experiments and 1 347 regrasps collected by one pair of tactile sensors, an RGB-D camera and one Franka Emika robot arm equipped with joint force/torque sensors. We show that our algorithm can successfully detect and classify the slip for 5 unknown test objects with an accuracy of 76.88% and a regrasp planner increases the grasp success rate by 31.0% compared to the state-of-the-art vision-based grasping algorithm.
ER  - 

TY  - CONF
TI  - OmniTact: A Multi-Directional High-Resolution Touch Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 618
EP  - 624
AU  - A. Padmanabha
AU  - F. Ebert
AU  - S. Tian
AU  - R. Calandra
AU  - C. Finn
AU  - S. Levine
PY  - 2020
KW  - cameras
KW  - computer vision
KW  - convolutional neural nets
KW  - dexterous manipulators
KW  - gels
KW  - image resolution
KW  - microsensors
KW  - neurocontrollers
KW  - sensor fusion
KW  - state estimation
KW  - tactile sensors
KW  - dexterous robotic manipulation
KW  - deep convolutional neural networks
KW  - electrical connector
KW  - multidirectional high-resolution touch sensor
KW  - low-resolution signals
KW  - multidirectional high-resolution tactile sensor
KW  - robotic hands
KW  - multiple microcameras
KW  - multidirectional deformations
KW  - gel-based skin
KW  - contact state variables
KW  - image processing
KW  - computer vision methods
KW  - state estimation problem
KW  - robotic control task
KW  - OmniTact combination
KW  - Cameras
KW  - Tactile sensors
KW  - Sensitivity
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196712
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Incorporating touch as a sensing modality for robots can enable finer and more robust manipulation skills. Existing tactile sensors are either flat, have small sensitive fields or only provide low-resolution signals. In this paper, we introduce OmniTact, a multi-directional high-resolution tactile sensor. OmniTact is designed to be used as a fingertip for robotic manipulation with robotic hands, and uses multiple micro-cameras to detect multi-directional deformations of a gel-based skin. This provides a rich signal from which a variety of different contact state variables can be inferred using modern image processing and computer vision methods. We evaluate the capabilities of OmniTact on a challenging robotic control task that requires inserting an electrical connector into an outlet, as well as a state estimation problem that is representative of those typically encountered in dexterous robotic manipulation, where the goal is to infer the angle of contact of a curved finger pressing against an object. Both tasks are performed using only touch sensing and deep convolutional neural networks to process images from the sensor's cameras. We compare with a state-of-the-art tactile sensor that is only sensitive on one side, as well as a state-of-the-art multi-directional tactile sensor, and find that OmniTact's combination of high-resolution and multi-directional sensing is crucial for reliably inserting the electrical connector and allows for higher accuracy in the state estimation task. Videos and supplementary material can be found here4.
ER  - 

TY  - CONF
TI  - Highly sensitive bio-inspired sensor for fine surface exploration and characterization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 625
EP  - 631
AU  - P. Ribeiro
AU  - S. Cardoso
AU  - A. Bernardino
AU  - L. Jamone
PY  - 2020
KW  - assembling
KW  - magnetic field measurement
KW  - magnetic sensors
KW  - magnetoresistive devices
KW  - permanent magnets
KW  - signal detection
KW  - surface texture
KW  - surface topography measurement
KW  - tunnelling magnetoresistance
KW  - bioinspired sensor
KW  - fine surface exploration
KW  - texture sensing
KW  - robotics
KW  - texture topography sensor
KW  - ciliary structure
KW  - biological structure
KW  - permanent magnetization
KW  - tunneling magnetoresistance sensor
KW  - surface texture
KW  - electronic signal acquisition board
KW  - topography scanner
KW  - elastic cilia brush
KW  - TMR sensor
KW  - surface roughness
KW  - size 7.0 mum
KW  - size 9.2 mum to 213.0 mum
KW  - Robot sensing systems
KW  - Tunneling magnetoresistance
KW  - Magnetic separation
KW  - Substrates
KW  - Bridge circuits
KW  - Magnetic tunneling
DO  - 10.1109/ICRA40945.2020.9197305
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Texture sensing is one of the types of information sensed by humans through touch, and is thus of interest to robotics that this type of information can be acquired and processed. In this work we present a texture topography sensor based on a ciliary structure, a biological structure found in many organisms. The device consists of up to 9 elastic cilia with permanent magnetization assembled on top of a highly sensitive tunneling magnetoresistance (TMR) sensor, within a compact footprint of 6×6 mm2. When these cilia brush against some textured surface, their movement and vibrations give rise to a signal that can be correlated to the characteristics of the texture being measured. We also present an electronic signal acquisition board, used in this work. Various configurations of cilia sizes are tested, with the most precise being capable of differentiating different types of sandpaper from 9.2 μm to 213 μm average surface roughness with a 7 μm resolution. As a topography scanner the sensor was able to scan a 20 μm high step in a flat surface.
ER  - 

TY  - CONF
TI  - Implementing Tactile and Proximity Sensing for Crack Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 632
EP  - 637
AU  - F. Palermo
AU  - J. Konstantinova
AU  - K. Althoefer
AU  - S. Poslad
AU  - I. Farkhatdinov
PY  - 2020
KW  - crack detection
KW  - learning (artificial intelligence)
KW  - object detection
KW  - pattern classification
KW  - tactile sensors
KW  - telerobotics
KW  - video cameras
KW  - proximity sensing
KW  - remote characterisation
KW  - physical robot-environment interaction
KW  - automatic crack detection
KW  - proximity sensor
KW  - physical environment
KW  - fibre optics
KW  - cracks
KW  - bumps
KW  - undulations
KW  - machine learning
KW  - classifier
KW  - average crack detection accuracy
KW  - width classification accuracy
KW  - Kruskal-Wallis results
KW  - force data
KW  - proximity data
KW  - optical fibres
KW  - extreme environments
KW  - Robot sensing systems
KW  - Force
KW  - Surface cracks
KW  - Optics
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196936
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Remote characterisation of the environment during physical robot-environment interaction is an important task commonly accomplished in telerobotics. This paper demonstrates how tactile and proximity sensing can be efficiently used to perform automatic crack detection. A custom-designed integrated tactile and proximity sensor is implemented. It measures the deformation of its body when interacting with the physical environment and distance to the environment's objects with the help of fibre optics. This sensor was used to slide across different surfaces and the data recorded during the experiments was used to detect and classify cracks, bumps and undulations. The proposed method uses machine learning techniques (mean absolute value as feature and random forest as classifier) to detect cracks and determine their width. An average crack detection accuracy of 86.46% and width classification accuracy of 57.30% is achieved. Kruskal-Wallis results (p<; 0.001) indicate statistically significant differences among results obtained when analysing only force data, only proximity data and both force and proximity data. In contrast to previous techniques, which mainly rely on visual modality, the proposed approach based on optical fibres is suitable for operation in extreme environments, such as nuclear facilities in which nuclear radiation may damage the electronic components of video cameras.
ER  - 

TY  - CONF
TI  - Novel Proximity Sensor for Realizing Tactile Sense in Suction Cups
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 638
EP  - 643
AU  - S. Doi
AU  - H. Koga
AU  - T. Seki
AU  - Y. Okuno
PY  - 2020
KW  - capacitance measurement
KW  - capacitive sensors
KW  - deformation
KW  - mechanical variables measurement
KW  - object detection
KW  - tactile sensors
KW  - partial contact position estimation
KW  - push-in stroke detection
KW  - deformation detection
KW  - tactile sensor module
KW  - capacitive proximity sensor module
KW  - suction cup
KW  - Robot sensing systems
KW  - Capacitance
KW  - Electrodes
KW  - Capacitance measurement
KW  - Sensitivity
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196726
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a new capacitive proximity sensor that detects deformations of a suction cup as a tactile sense. We confirmed that one sensor module provides three applications for reliable picking and a simplified setup. The first application is the picking height decision. The second one is the placing height decision for detecting whether the grasped object is placed on the placement surface. These two applications are achieved by detecting the push-in stroke of the suction cup. The final application is detection of whether the suction cup is in partial contact or full contact with the object. This function can correct the picking posture as well as detect whether picking is possible before the pull-up motion. We also demonstrate that the partial contact position can be estimated in real time.
ER  - 

TY  - CONF
TI  - Constrained Filtering-based Fusion of Images, Events, and Inertial Measurements for Pose Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 644
EP  - 650
AU  - J. H. Jung
AU  - C. Gook Park
PY  - 2020
KW  - cameras
KW  - image filtering
KW  - image sensors
KW  - image sequences
KW  - pose estimation
KW  - constrained filtering-based fusion
KW  - inertial measurements
KW  - pose estimation
KW  - camera poses
KW  - bio-inspired sensor
KW  - brightness changes
KW  - independent pixels
KW  - high dynamic range
KW  - optical flow
KW  - intensity images
KW  - pixel difference
KW  - inverse scene-depth
KW  - DVS dataset
KW  - filtering-based estimator
KW  - dynamic vision sensor
KW  - Optical imaging
KW  - Optical filters
KW  - Optical sensors
KW  - Integrated optics
KW  - Optical variables measurement
KW  - Adaptive optics
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197248
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a novel filtering-based method that fuses events from a dynamic vision sensor (DVS), images, and inertial measurements to estimate camera poses. A DVS is a bio-inspired sensor that generates events triggered by brightness changes. It can cover the drawbacks of a conventional camera by virtual of its independent pixels and high dynamic range. Specifically, we focus on optical flow obtained from both a stream of events and intensity images in which the former is much like a differential quantity, whereas the latter is a pixel difference in a much longer time interval than events. This nature characteristic motivates us to model optical flow estimated from events directly, but feature tracks for images in the filter design. An inequality constraint is considered in our method since the inverse scene-depth is larger than zero by its definition. Furthermore, we evaluate our proposed method in the benchmark DVS dataset and a dataset collected by the authors. The results reveal that the presented algorithm has reduced the position error by 49.9% on average and comparable accuracy only using events when compared to the state-of-the-art filtering-based estimator.
ER  - 

TY  - CONF
TI  - Schmidt-EKF-based Visual-Inertial Moving Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 651
EP  - 657
AU  - K. Eckenhoff
AU  - P. Geneva
AU  - N. Merrill
AU  - G. Huang
PY  - 2020
KW  - image motion analysis
KW  - image representation
KW  - Kalman filters
KW  - object tracking
KW  - pose estimation
KW  - robot vision
KW  - target tracking
KW  - tracking sensor
KW  - target motion model
KW  - Schmidt-EKF-based visual-inertial moving object tracking
KW  - tightly-coupled estimation
KW  - visual-inertial localization
KW  - joint estimation system
KW  - Schmidt-Kalman Filter
KW  - ego-motion accuracy degradation
KW  - robot-centric representation
KW  - object pose tracking
KW  - Target tracking
KW  - Robot sensing systems
KW  - Estimation
KW  - Three-dimensional displays
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197352
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we investigate the effect of tightly-coupled estimation on the performance of visual-inertial localization and dynamic object pose tracking. In particular, we show that while a joint estimation system outperforms its decoupled counterpart when given a "proper" model for the target's motion, inconsistent modeling, such as choosing improper levels for the target's propagation noises, can actually lead to a degradation in ego-motion accuracy. To address the realistic scenario where a good prior knowledge of the target's motion model is not available, we design a new system based on the Schmidt-Kalman Filter (SKF), in which target measurements do not update the navigation states, however all correlations are still properly tracked. This allows for both consistent modeling of the target errors and the ability to update target estimates whenever the tracking sensor receives non-target data such as bearing measurements to static, 3D environmental features. We show in extensive simulation that this system, along with a robot-centric representation of the target, leads to robust estimation performance even in the presence of an inconsistent target motion model. Finally, the system is validated in a real-world experiment, and is shown to offer accurate localization and object pose tracking performance.
ER  - 

TY  - CONF
TI  - Learning View and Target Invariant Visual Servoing for Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 658
EP  - 664
AU  - Y. Li
AU  - J. Košecka
PY  - 2020
KW  - convolutional neural nets
KW  - feedback
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - visual servoing
KW  - deep reinforcement learning
KW  - mobile robot navigation
KW  - deep convolutional network controller
KW  - viewpoint invariant visual servoing
KW  - target invariant visual servoing
KW  - feedback control error
KW  - Visual servoing
KW  - Navigation
KW  - Visualization
KW  - Feature extraction
KW  - Task analysis
KW  - Semantics
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197136
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller to reach the desired goal. We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment [1]. The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches.
ER  - 

TY  - CONF
TI  - Tightly-Coupled Single-Anchor Ultra-wideband-Aided Monocular Visual Odometry System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 665
EP  - 671
AU  - T. H. Nguyen
AU  - T. -M. Nguyen
AU  - L. Xie
PY  - 2020
KW  - distance measurement
KW  - feature extraction
KW  - graph theory
KW  - least squares approximations
KW  - optimisation
KW  - pose estimation
KW  - position measurement
KW  - robot vision
KW  - Levenberg-Marquardt nonlinear least squares optimization scheme
KW  - scale factor
KW  - visual features
KW  - pose-graph optimization scheme
KW  - landmark reprojection errors
KW  - visual drift
KW  - monocular visual feature observations
KW  - distance measurements
KW  - ultrawideband-aided monocular visual odometry system
KW  - single-anchor monocular visual odometry system
KW  - tightly-coupled odometry framework
KW  - anchor position estimation
KW  - robot operating system
KW  - Cameras
KW  - Distance measurement
KW  - Robot sensing systems
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196794
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we propose a tightly-coupled odometry framework, which combines monocular visual feature observations with distance measurements provided by a single ultra-wideband (UWB) anchor with an initial guess for its location. Firstly, the scale factor and the anchor position in the vision frame will be simultaneously estimated using a variant of Levenberg-Marquardt non-linear least squares optimization scheme. Once the scale factor is obtained, the map of visual features is updated with the new scale. Subsequent ranging errors in a sliding window are continuously monitored and the estimation procedure will be reinitialized to refine the estimates. Lastly, range measurements and anchor position estimates are fused when needed into a pose-graph optimization scheme to minimize both the landmark reprojection errors and ranging errors, thus reducing the visual drift and improving the system robustness. The proposed method is implemented in Robot Operating System (ROS) and can function in real-time. The performance is validated on both public datasets and real-life experiments and compared with state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Scaling Local Control to Large-Scale Topological Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 672
EP  - 678
AU  - X. Meng
AU  - N. Ratliff
AU  - Y. Xiang
AU  - D. Fox
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - deep learning
KW  - robot perception
KW  - reliability issue
KW  - world images
KW  - mechanical constraints
KW  - local controller
KW  - large-scale visual topological navigation
KW  - large-scale environments
KW  - local control
KW  - large-scale topological navigation
KW  - Navigation
KW  - Trajectory
KW  - Robustness
KW  - Robot kinematics
KW  - Planning
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196644
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual topological navigation has been revitalized recently thanks to the advancement of deep learning that substantially improves robot perception. However, the scalability and reliability issue remain challenging due to the complexity and ambiguity of real world images and mechanical constraints of real robots. We present an intuitive approach to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust. Our approach achieves state-of-the-art results in trajectory following and planning in large-scale environments. It also generalizes well to real robots and new environments without retraining or finetuning.
ER  - 

TY  - CONF
TI  - Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 679
EP  - 685
AU  - X. Pan
AU  - T. Zhang
AU  - B. Ichter
AU  - A. Faust
AU  - J. Tan
AU  - S. Ha
PY  - 2020
KW  - graphical user interfaces
KW  - humanoid robots
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - cost-effective data collection
KW  - third-person demonstrations
KW  - camera perspectives
KW  - perspective-invariant state features
KW  - model-based imitation learning approach
KW  - action-labeled human demonstrations
KW  - effective policy
KW  - zero-shot imitation
KW  - legged robot visual navigation
KW  - training effective visual navigation policies
KW  - expert demonstrations
KW  - goal-driven visual navigation policy
KW  - high-quality navigation
KW  - Feature extraction
KW  - Navigation
KW  - Legged locomotion
KW  - Visualization
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196602
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Imitation learning is a popular approach for training effective visual navigation policies. However, collecting expert demonstrations for legged robots is challenging as these robots can be hard to control, move slowly, and cannot operate continuously for long periods of time. In this work, we propose a zero-shot imitation learning framework for training a goal-driven visual navigation policy on a legged robot from human demonstrations (third-person perspective), allowing for high-quality navigation and cost-effective data collection. However, imitation learning from third-person demonstrations raises unique challenges. First, these demonstrations are captured from different camera perspectives, which we address via a feature disentanglement network (FDN) that extracts perspective-invariant state features. Second, as transition dynamics vary between systems, we reconstruct missing action labels by either building an inverse model of the robot's dynamics in the feature space and applying it to the human demonstrations or developing a Graphic User Interface (GUI) to label human demonstrations. To train a navigation policy we use a model-based imitation learning approach with FDN and action-labeled human demonstrations. We show that our framework can learn an effective policy for a legged robot, Laikago, from human demonstrations in both simulated and real-world environments. Our approach is zero-shot as the robot never navigates the same paths during training as those at testing time. We justify our framework by performing a comparative study.
ER  - 

TY  - CONF
TI  - Pressure-Driven Manipulator with Variable Stiffness Structure
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 696
EP  - 702
AU  - C. Sozer
AU  - L. Paternò
AU  - G. Tortora
AU  - A. Menciassi
PY  - 2020
KW  - actuators
KW  - controllability
KW  - deformation
KW  - fibre reinforced composites
KW  - manipulators
KW  - position control
KW  - rigidity
KW  - air pressure
KW  - controllability
KW  - position dependent stiffness
KW  - manipulator stiffness
KW  - variable stiffness structure
KW  - pressure driven manipulator
KW  - soft rigid stiffness control structure
KW  - unidirectional fiber reinforced actuators
KW  - soft robot deformability
KW  - soft robot compliance
KW  - bidirectional in-plane manipulator
KW  - Manipulators
KW  - Actuators
KW  - Encapsulation
KW  - Fabrication
KW  - Force
KW  - Controllability
DO  - 10.1109/ICRA40945.2020.9197401
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The high deformability and compliance of soft robots allow safer interaction with the environment. On the other hand, these advantages bring along controllability and predictability challenges which result in loss of force and stiffness output. Such challenges should be addressed in order to improve the overall functional performance and to meet the requirements of real-scenario applications. In this paper, we present a bidirectional in-plane manipulator which consists of two unidirectional fiber-reinforced actuators (FRAs) and a hybrid soft-rigid stiffness control structure (SCS), all of them controlled by air pressure. Both controllability and predictability of the manipulator are enhanced by the hybrid soft-rigid structure. While the FRAs provide positioning and position dependent stiffness, the SCS increases the stiffness of the manipulator without position dependency. The SCS is able to increase the manipulator stiffness by 35%, 30%, and 18%, when one FRA is pressurized at 150 kPa, 75 kPa, and 0 kPa, respectively. Experiments are carried out to present the feasibility of the proposed manipulator.
ER  - 

TY  - CONF
TI  - Human Interface for Teleoperated Object Manipulation with a Soft Growing Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 726
EP  - 732
AU  - F. Stroppa
AU  - M. Luo
AU  - K. Yoshida
AU  - M. M. Coad
AU  - L. H. Blumenschein
AU  - A. M. Okamura
PY  - 2020
KW  - biomechanics
KW  - human-robot interaction
KW  - manipulators
KW  - robots
KW  - service robots
KW  - telerobotics
KW  - soft growing robot manipulator
KW  - body-movement-based interface
KW  - pick-and-place manipulation task
KW  - human-centered interface
KW  - complex manipulation tasks
KW  - teleoperated object manipulation
KW  - human interface
KW  - Robot kinematics
KW  - Task analysis
KW  - Manipulators
KW  - Soft robotics
KW  - Tracking
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197094
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft growing robots are proposed for use in applications such as complex manipulation tasks or navigation in disaster scenarios. Safe interaction and ease of production promote the usage of this technology, but soft robots can be challenging to teleoperate due to their unique degrees of freedom. In this paper, we propose a human-centered interface that allows users to teleoperate a soft growing robot for manipulation tasks using arm movements. A study was conducted to assess the intuitiveness of the interface and the performance of our soft robot, involving a pick-and-place manipulation task. The results show that users were able to complete the task 97% of the time and achieve placement errors below 2 cm on average. These results demonstrate that our body-movement-based interface is an effective method for control of a soft growing robot manipulator.
ER  - 

TY  - CONF
TI  - Modulating hip stiffness with a robotic exoskeleton immediately changes gait
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 733
EP  - 739
AU  - J. Lee
AU  - H. R. Warren
AU  - V. Agarwal
AU  - M. E. Huber
AU  - N. Hogan
PY  - 2020
KW  - gait analysis
KW  - kinematics
KW  - medical control systems
KW  - medical robotics
KW  - patient rehabilitation
KW  - modulating hip stiffness
KW  - robotic exoskeleton
KW  - healthy kinematics
KW  - critical component
KW  - assisting rehabilitating impaired locomotion
KW  - spatiotemporal gait patterns
KW  - mechanical impedance
KW  - hip joints
KW  - Samsung GEMS-H
KW  - virtual spring
KW  - short repeated exposures
KW  - spatiotemporal measures
KW  - stiffness controller
KW  - gait behavior
KW  - mechanical effect
KW  - lower-limb assistive devices
KW  - healthy gait patterns
KW  - Hip
KW  - Exoskeletons
KW  - Legged locomotion
KW  - Read only memory
KW  - Kinematics
KW  - Springs
KW  - Time measurement
DO  - 10.1109/ICRA40945.2020.9197054
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Restoring healthy kinematics is a critical component of assisting and rehabilitating impaired locomotion. Here we tested whether spatiotemporal gait patterns can be modulated by applying mechanical impedance to hip joints. Using the Samsung GEMS-H exoskeleton, we emulated a virtual spring (positive and negative) between the user's legs. We found that applying positive stiffness with the exoskeleton decreased stride time and hip range of motion for healthy subjects during treadmill walking. Conversely, the application of negative stiffness increased stride time and hip range of motion. These effects did not vary over long nor short repeated exposures to applied stiffness. In addition, minimal transient behavior was observed in spatiotemporal measures of gait when the stiffness controller transitioned between on and off states. These results suggest that changes in gait behavior induced by applying hip stiffness were purely a mechanical effect. Together, our findings indicate that applying mechanical impedance using lower-limb assistive devices may be an effective, minimally-encumbering intervention to restore healthy gait patterns.
ER  - 

TY  - CONF
TI  - Swing-Assist for Enhancing Stair Ambulation in a Primarily-Passive Knee Prosthesis
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 740
EP  - 746
AU  - J. T. Lee
AU  - M. Goldfarb
PY  - 2020
KW  - artificial limbs
KW  - gait analysis
KW  - kinematics
KW  - medical control systems
KW  - stair ascent
KW  - SCSA prosthesis prototype
KW  - unilateral transfemoral amputee
KW  - motion capture apparatus
KW  - microprocessor-controlled daily-use
KW  - passive prosthesis
KW  - SCSA knee
KW  - stair activity
KW  - swing-assist
KW  - enhancing stair ambulation
KW  - primarily-passive knee prosthesis
KW  - primarily-passive stance-controlled swing
KW  - Knee
KW  - Prosthetics
KW  - Valves
KW  - Hip
KW  - Torque
KW  - Actuators
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196974
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the design and implementation of a controller for stair ascent and descent in a primarily-passive stance-controlled swing-assist (SCSA) prosthesis. The prosthesis and controller enable users to perform both step-over and step-to stair ascent and descent. The efficacy of the controller and SCSA prosthesis prototype in providing improved stair ambulation was tested on a unilateral transfemoral amputee in experiments that employed motion capture apparatus to compare joint kinematics with the SCSA prosthesis, relative to performing the same activity with a microprocessor-controlled daily-use passive prosthesis. Results suggest that the SCSA knee significantly decreases compensatory motion during stair activity when compared to the passive prosthesis.
ER  - 

TY  - CONF
TI  - Proof-of-concept of a Pneumatic Ankle Foot Orthosis Powered by a Custom Compressor for Drop Foot Correction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 747
EP  - 753
AU  - S. J. Kim
AU  - J. Park
AU  - W. Shin
AU  - D. Y. Lee
AU  - J. Kim
PY  - 2020
KW  - compressors
KW  - gait analysis
KW  - medical control systems
KW  - orthopaedics
KW  - orthotics
KW  - pneumatic systems
KW  - valves
KW  - unilateral drop-foot patients
KW  - maximum assistive torque
KW  - unilateral active AFO
KW  - pressurized air
KW  - wearable custom compressor
KW  - assistive devices
KW  - stationary air compressors
KW  - pneumatic systems
KW  - mass distribution
KW  - pneumatic components
KW  - powered ankle foot orthosis systems
KW  - pneumatic transmission
KW  - drop foot correction
KW  - pneumatic ankle foot orthosis powered
KW  - pressure 1050.0 kPa
KW  - pressure 550.0 kPa
KW  - Foot
KW  - Torque
KW  - DC motors
KW  - Valves
KW  - Optical sensors
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196817
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Pneumatic transmission has several advantages in developing powered ankle foot orthosis (AFO) systems, such as the flexibility in placing pneumatic components for mass distribution and providing high back-drivability via simple valve control. However, pneumatic systems are generally tethered to large stationary air compressors that restrict them for being used as daily assistive devices. In this study, we improved a previously developed wearable (untethered) custom compressor that can be worn (1.5 kg) at the waist of the body and can generate adequate amount of pressurized air (maximum pressure of 1050 kPa and a flow rate of 15.1 mL/sec at 550 kPa) to power a unilateral active AFO used to assist the dorsiflexion (DF) motion of drop-foot patients. The finalized system can provide a maximum assistive torque of 10 Nm and induces an average 0.03±0.06 Nm resistive torque when free movement is provided. The system was tested for two unilateral drop-foot patients. The proposed system showed an average improvement of 13.6° of peak dorsiflexion angle during the swing phase of the gait cycle.
ER  - 

TY  - CONF
TI  - Knowledge-Guided Reinforcement Learning Control for Robotic Lower Limb Prosthesis
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 754
EP  - 760
AU  - X. Gao
AU  - J. Si
AU  - Y. Wen
AU  - M. Li
AU  - H. H. Huang
PY  - 2020
KW  - artificial limbs
KW  - biomechanics
KW  - gait analysis
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - neurophysiology
KW  - patient rehabilitation
KW  - knowledge transfer
KW  - control tuning performance
KW  - amputee subject
KW  - AB subjects
KW  - transfer knowledge
KW  - data requirements
KW  - RL controller
KW  - robotic prosthetic limb
KW  - control method
KW  - knowledge-guided Q-learning
KW  - RL agents learn
KW  - controlled prosthesis
KW  - prosthesis control
KW  - prosthesis device
KW  - trans-femoral amputees
KW  - passive prostheses
KW  - lost functions
KW  - robotic lower limb prosthesis
KW  - guided reinforcement learning control
KW  - Prosthetics
KW  - Task analysis
KW  - Impedance
KW  - Knee
KW  - Legged locomotion
KW  - Tuning
DO  - 10.1109/ICRA40945.2020.9196749
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic prostheses provide new opportunities to better restore lost functions than passive prostheses for trans-femoral amputees. But controlling a prosthesis device automatically for individual users in different task environments is an unsolved problem. Reinforcement learning (RL) is a naturally promising tool. For prosthesis control with a user in the loop, it is desirable that the controlled prosthesis can adapt to different task environments as quickly and smoothly as possible. However, most RL agents learn or relearn from scratch when the environment changes. To address this issue, we propose the knowledge-guided Q-learning (KG-QL) control method as a principled way for the problem. In this report, we collected and used data from two able-bodied (AB) subjects wearing a RL controlled robotic prosthetic limb walking on level ground. Our ultimate goal is to build an efficient RL controller with reduced time and data requirements and transfer knowledge from AB subjects to amputee subjects. Toward this goal, we demonstrate its feasibility by employing OpenSim, a well-established human locomotion simulator. Our results show the OpenSim simulated amputee subject improved control tuning performance over learning from scratch by utilizing knowledge transfer from AB subjects. Also in this paper, we will explore the possibility of information transfer from AB subjects to help tuning for the amputee subjects.
ER  - 

TY  - CONF
TI  - Development of a Twisted String Actuator-based Exoskeleton for Hip Joint Assistance in Lifting Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 761
EP  - 767
AU  - H. -S. Seong
AU  - D. -H. Kim
AU  - I. Gaponov
AU  - J. -H. Ryu
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - design engineering
KW  - electric motors
KW  - handicapped aids
KW  - patient rehabilitation
KW  - robot dynamics
KW  - robot kinematics
KW  - torque control
KW  - wearable robots
KW  - twisted string actuator-based exoskeleton
KW  - hip joint assistance
KW  - compliant cable-driven exoskeleton
KW  - injuries
KW  - vocational setting
KW  - powerful exoskeleton
KW  - inherent TSA advantages
KW  - typical torque-speed requirements
KW  - exoskeleton design
KW  - motor selection
KW  - practical exoskeleton prototype
KW  - Hip
KW  - Exoskeletons
KW  - Torque
KW  - Pulleys
KW  - Actuators
KW  - Kinematics
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197359
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a study on a compliant cable-driven exoskeleton for hip assistance in lifting tasks that is aimed at preventing low-back pain and injuries in the vocational setting. In the proposed concept, we used twisted string actuator (TSA) to design a light-weight and powerful exoskeleton that benefits from inherent TSA advantages. We have noted that nonlinear nature of twisted strings' transmission ratio (decreasing with twisting) closely matched typical torque-speed requirements for hip assistance during lifting tasks and tried to use this fact in the exoskeleton design and motor selection. Hip-joint torque and speed required to lift a 10-kg load from stoop to stand were calculated, which gave us a baseline that we used to design and manufacture a practical exoskeleton prototype. Preliminary experimental trials demonstrated that the proposed device was capable of generating required torque and speed at the hip joint while weighing under 6 kg, including battery.
ER  - 

TY  - CONF
TI  - A Novel Portable Lower Limb Exoskeleton for Gravity Compensation during Walking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 768
EP  - 773
AU  - L. Zhou
AU  - W. Chen
AU  - W. Chen
AU  - S. Bai
AU  - J. Wang
PY  - 2020
KW  - gait analysis
KW  - gears
KW  - handicapped aids
KW  - man-machine systems
KW  - medical robotics
KW  - motion control
KW  - patient rehabilitation
KW  - robot kinematics
KW  - springs (mechanical)
KW  - torque
KW  - gravity compensation
KW  - walking assistance
KW  - spring mechanisms
KW  - hip
KW  - knee joints
KW  - gravity balancing
KW  - human leg
KW  - mating gears
KW  - tension force
KW  - springs
KW  - safety
KW  - user acceptance
KW  - design principle
KW  - limb joints
KW  - single leg exoskeleton
KW  - portable passive lower limb exoskeleton
KW  - driving torques
KW  - Springs
KW  - Exoskeletons
KW  - Gravity
KW  - Legged locomotion
KW  - Gears
KW  - Potential energy
KW  - Hip
DO  - 10.1109/ICRA40945.2020.9197422
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel portable passive lower limb exoskeleton for walking assistance. The exoskeleton is designed with built-in spring mechanisms at the hip and knee joints to realize gravity balancing of the human leg. A pair of mating gears is used to convert the tension force from the built-in springs into balancing torques at hip and knee joints for overcoming the influence of gravity. Such a design makes the exoskeleton has a compact layout with small protrusion, which improves its safety and user acceptance. In this paper, the design principle of gravity balancing is described. Simulation results show a significant reduction of driving torques at the limb joints. A prototype of single leg exoskeleton has been constructed and preliminary test results show the effectiveness of the exoskeleton.
ER  - 

TY  - CONF
TI  - Steerable Burrowing Robot: Design, Modeling and Experiments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 829
EP  - 835
AU  - M. Barenboim
AU  - A. Degani
PY  - 2020
KW  - drag
KW  - impact (mechanical)
KW  - mobile robots
KW  - numerical analysis
KW  - robot dynamics
KW  - vehicle dynamics
KW  - thrusting mechanism
KW  - depth dependent model
KW  - steerable burrowing robot
KW  - vibro-impact mechanism
KW  - rotating bevel-tip head
KW  - nonholonomic model
KW  - steering mechanism
KW  - hybrid dynamics model
KW  - S-shaped trajectory
KW  - Robot kinematics
KW  - Needles
KW  - Numerical models
KW  - Solid modeling
KW  - Trajectory
KW  - Springs
DO  - 10.1109/ICRA40945.2020.9196648
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper investigates a burrowing robot that can maneuver and steer while being submerged in a granular medium. The robot locomotes using an internal vibro-impact mechanism and steers using a rotating bevel-tip head. We formulate and investigate a non-holonomic model for the steering mechanism and a hybrid dynamics model for the thrusting mechanism. We perform a numerical analysis of the dynamics of the robot's thrusting mechanism using a simplified, orientation and depth dependent model for the drag forces acting on the robot. We first show, in simulation, that by carefully tuning various control input parameters, the thrusting mechanism can drive the robot both forward and backward. We present several experiments designed to evaluate and verify the simulative results using a proof-of-concept robot. We show that different input amplitudes indeed affect the direction of motion, as suggested by the simulation. We further demonstrate the ability of the robot to perform a simple S-shaped trajectory. These experiments demonstrate the feasibility of the robot's design and fidelity of the model.
ER  - 

TY  - CONF
TI  - High Force Density Gripping with UV Activation and Sacrificial Adhesion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 836
EP  - 842
AU  - E. Lee
AU  - Z. Goddard
AU  - J. Ngotiaoco
AU  - N. Monterrosa
AU  - A. Mazumdar
PY  - 2020
KW  - adhesion
KW  - adhesives
KW  - curing
KW  - electric motors
KW  - grippers
KW  - mobile robots
KW  - plastics
KW  - prototypes
KW  - ultraviolet sources
KW  - plastic parts
KW  - force-to-weight ratio
KW  - high force density gripping
KW  - UV activation
KW  - sacrificial adhesion
KW  - light-activated chemical adhesive
KW  - ultraviolet light sensitive acrylics
KW  - rapid curing
KW  - electric motor
KW  - proof-of concept prototypes
KW  - mobile robots
KW  - size 380.0 nm
KW  - time 15.0 s to 75.0 s
KW  - Grippers
KW  - Force
KW  - Curing
KW  - Adhesives
KW  - Chemicals
KW  - Mobile robots
KW  - Mechanism Design
KW  - Manipulation
DO  - 10.1109/ICRA40945.2020.9197246
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel physical gripping framework intended for controlled, high force density attachment on a range of surfaces. Our framework utilizes a light-activated chemical adhesive to attach to surfaces. The cured adhesive is part of a "sacrificial layer," which is shed when the gripper separates from the surface. In order to control adhesive behavior we utilize ultraviolet (UV) light sensitive acrylics which are capable of rapid curing when activated with 380nm light. Once cured, zero input power is needed to hold load. Thin plastic parts can be used as the sacrificial layers, and these can be released using an electric motor. This new gripping framework including the curing load capacity, adhesive deposition, and sacrificial methods are described in detail. Two proof-of concept prototypes are designed, built, and tested. The experimental results illustrate the response time (15-75s depending on load), high holding force-to-weight ratio (10-30), and robustness to material type. Additionally, two drawbacks of this design are discussed: corruption of the gripped surface and a limited number of layers.
ER  - 

TY  - CONF
TI  - Stiffness optimization of a cable driven parallel robot for additive manufacturing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 843
EP  - 849
AU  - D. Gueners
AU  - H. Chanal
AU  - B. C. Bouzgarrou
PY  - 2020
KW  - cables (mechanical)
KW  - industrial robots
KW  - optimisation
KW  - position control
KW  - rigidity
KW  - three-dimensional printing
KW  - vibration control
KW  - stiffness optimization
KW  - cable driven parallel robot
KW  - additive manufacturing
KW  - anchor points
KW  - robot stiffness
KW  - tool path
KW  - platform rigidity
KW  - CDPR
KW  - 3D printing
KW  - vibration modes
DO  - 10.1109/ICRA40945.2020.9197368
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, the optimization of the anchor points of a cable driven parallel robot (CDPR) for 3D printing is proposed in order to maximize the rigidity. Indeed, in the context of 3D printing, robot stiffness should guarantee a high level of tool path following accuracy. The optimized platform showed a rigidity improvement in simulation, but also experimentally with a first study of vibration modes. In the same time, this study illustrates the influence of preload in cables on the platform rigidity.
ER  - 

TY  - CONF
TI  - CAMI - Analysis, Design and Realization of a Force-Compliant Variable Cam System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 850
EP  - 856
AU  - D. Mannhart
AU  - F. Dubois
AU  - K. Bodie
AU  - V. Klemm
AU  - A. Morra
AU  - M. Hutter
PY  - 2020
KW  - cams (mechanical)
KW  - compliant mechanisms
KW  - end effectors
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - path planning
KW  - trajectory control
KW  - CAMI
KW  - multilegged locomotion
KW  - continuous gait transition
KW  - end effector trajectory
KW  - end effector motions
KW  - three dimensional cam system
KW  - force compliant variable cam system
KW  - bipedal robot
KW  - Legged locomotion
KW  - Trajectory
KW  - End effectors
KW  - Couplings
KW  - Shape
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9197019
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents a novel design concept that achieves multi-legged locomotion using a three-dimensional cam system. A computational framework has been developed to analyze and dimension this cam apparatus, that can perform arbitrary end effector motions within its design constraints. The mechanism enables continuous gait transition and inherent force compliance. With only two motors, any trajectory of a continuous set of gaits can be followed. One motor is used to actuate the system and a second one to morph its movement. To illustrate a possible application of this system, a working prototype of a bipedal robot is developed and validated in hardware. It showcases a smooth velocity change by transitioning through different gaits from standing still to walking fast at 124mm/s within 2.0s, while following the given end effector trajectory with an error of only 2.47mm.
ER  - 

TY  - CONF
TI  - Using Manipulation to Enable Adaptive Ground Mobility
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 857
EP  - 863
AU  - R. Kim
AU  - A. Debate
AU  - S. Balakirsky
AU  - A. Mazumdar
PY  - 2020
KW  - adhesion
KW  - legged locomotion
KW  - manipulators
KW  - permanent magnets
KW  - propulsion
KW  - road vehicles
KW  - wheels
KW  - swappable propulsors
KW  - adhesion forces
KW  - wheeled locomotion
KW  - legged locomotion
KW  - autonomous ground vehicles
KW  - terrain
KW  - whegs
KW  - physical adaptation
KW  - multipurpose manipulators
KW  - propulsion system
KW  - adaptive ground mobility
KW  - permanent magnets
KW  - functional prototype robot
KW  - Legged locomotion
KW  - Wheels
KW  - Manipulators
KW  - Steel
KW  - Force
KW  - Mechanism design
KW  - mobile manipulation
KW  - wheeled robots
DO  - 10.1109/ICRA40945.2020.9197061
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In order to accomplish various missions, autonomous ground vehicles must operate on a wide range of terrain. While many systems such as wheels and whegs can navigate some types of terrain, none are optimal across all. This creates a need for physical adaptation. This paper presents a broad new approach to physical adaptation that relies on manipulation. Specifically, we explore how multipurpose manipulators can enable ground vehicles to dramatically modify their propulsion system in order to optimize performance across various terrain. While this approach is general and widely applicable, this work focuses on physically switching between wheels and legs. We outline the design of "swappable propulsors" that combine the powerful adhesion forces of permanent magnets with geometric features for easy detachment. We provide analysis on how the swappable propulsors can be manipulated, and use these results to create a functional prototype robot. This robot can use its manipulator to change between wheeled and legged locomotion. Our experimental results illustrate how this approach can enhance energy efficiency and versatility.
ER  - 

TY  - CONF
TI  - SNIAE-SSE Deformation Mechanism Enabled Scalable Multicopter: Design, Modeling and Flight Performance Validation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 864
EP  - 870
AU  - T. Yang
AU  - Y. Zhang
AU  - P. Li
AU  - Y. Shen
AU  - Y. Liu
AU  - H. Chen
PY  - 2020
KW  - actuators
KW  - deformation
KW  - helicopters
KW  - symmetrical deformation
KW  - synchronous deformation
KW  - multicopter system
KW  - flight missions
KW  - stable flight behavior
KW  - folding
KW  - unfolding body deformations
KW  - SNIAE-SSE deformation mechanism
KW  - flight performance validation
KW  - modeling validating
KW  - straight scissor-like elements
KW  - simple non-intersecting angulated elements
KW  - actuation capability
KW  - Strain
KW  - Rotors
KW  - Servomotors
KW  - Prototypes
KW  - Deformable models
KW  - Task analysis
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197025
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on designing, modeling and validating a novel scalable multicopter whose deformation mechanism, called SNIAE-SSE, relies on a combination of simple non-intersecting angulated elements (SNIAEs) and straight scissor-like elements (SSEs). The proposed SNIAE-SSE mechanism has the advantages of single degree-of-freedom, fast actuation capability and large deformation ratio. In this work, enabled by the SNIAE-SSE mechanism, a quadcopter prototype with symmetrical and synchronous deformation is firstly developed, which facilitates a novel and controllably scalable multicopter system for us to analyze its modeling, as well as to validate its flight performance and dynamics during the deformation in several flight missions including hover, throwing, and morphing flying through a narrow window. Experimental results demonstrate that the developed scalable multicopter can maintain its stable flight behavior even both the folding and unfolding body deformations are fast performed, which indicates an excellent capability of the scalable multicopter to rapidly adapt to complex and dynamically changed environments.
ER  - 

TY  - CONF
TI  - Cooperative Autonomy and Data Fusion for Underwater Surveillance With Networked AUVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 871
EP  - 877
AU  - G. Ferri
AU  - P. Stinco
AU  - G. De Magistris
AU  - A. Tesei
AU  - K. D. LePage
PY  - 2020
KW  - autonomous underwater vehicles
KW  - mobile robots
KW  - sensor fusion
KW  - target tracking
KW  - underwater acoustic communication
KW  - AUV cooperative strategies
KW  - data fusion
KW  - realistic underwater surveillance scenarios
KW  - networked AUVs
KW  - data sharing
KW  - robotic networks
KW  - underwater surveillance applications
KW  - autonomous underwater vehicles
KW  - CMRE Anti-Submarine Warfare network
KW  - track management module
KW  - robot autonomy software
KW  - track classification
KW  - T2T association
KW  - Target tracking
KW  - Robot kinematics
KW  - Sonar
KW  - Receivers
KW  - Signal processing algorithms
DO  - 10.1109/ICRA40945.2020.9197367
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Cooperative autonomy and data sharing can largely improve the mission performance of robotic networks in underwater surveillance applications. In this paper, we describe the cooperative autonomy used to control the Autonomous Underwater Vehicles (AUVs) acting as sonar receiver nodes in the CMRE Anti-Submarine Warfare (ASW) network. The paper focuses on a track management module that was integrated in the robot autonomy software for enabling the share of information. Track to track (T2T) associations are used for improving track classification and for creating a common tactical picture, necessary for AUV cooperative strategies. We also present a new cooperative data-driven AUV behaviour that exploits the spatial diversity of multiple robots for improving target tracking and for facilitating T2T associations. We report results with real data collected at sea that validate the approach. The reported results are one of the first examples that show the potential of cooperative autonomy and data fusion in realistic underwater surveillance scenarios characterised by limited communications.
ER  - 

TY  - CONF
TI  - Bidirectional Resonant Propulsion and Localization for AUVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 878
EP  - 884
AU  - T. W. Secord
AU  - T. R. Louwagie
PY  - 2020
KW  - autonomous underwater vehicles
KW  - diaphragms
KW  - electromagnetic actuators
KW  - marine control
KW  - mobile robots
KW  - motion control
KW  - robot vision
KW  - SLAM (robots)
KW  - electromagnetic voice coil motor
KW  - bidirectional resonant propulsion
KW  - AUV localization
KW  - thrust vectors
KW  - diaphragm pump mechanism
KW  - resonant motion
KW  - actuator design
KW  - bidirectional resonant pump
KW  - autonomous underwater vehicles
KW  - Propulsion
KW  - Resonant frequency
KW  - Strain
KW  - Standards
KW  - Damping
KW  - Reliability engineering
DO  - 10.1109/ICRA40945.2020.9197363
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Battery life, reliability, and localization are prominent challenges in the design of autonomous underwater vehicles (AUVs). This work aims to address facets of these challenges using a single system. We describe the design of a bidirectional resonant pump that uses a single electromagnetic voice coil motor (VCM) capable of rotation around a central two degree-of-freedom flexure stage axis. This actuator design produces highly efficient resonant motion that drives two orthogonally oriented diaphragms simultaneously. The operation of this diaphragm pump mechanism produces both adjustable thrust vectors at the aft surface of the AUV and a monotonic relationship between thrust vectors and operating frequency. We propose using the unique frequency to thrust relationship to enhance AUV localization capabilities. We construct a prototype and use it to experimentally demonstrate the feasibility of the directionally-tunable resonance concept.
ER  - 

TY  - CONF
TI  - Hierarchical Planning in Time-Dependent Flow Fields for Marine Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 885
EP  - 891
AU  - J. J. Heon Lee
AU  - C. Yoo
AU  - S. Anstee
AU  - R. Fitch
PY  - 2020
KW  - autonomous underwater vehicles
KW  - computational complexity
KW  - graph theory
KW  - marine vehicles
KW  - mobile robots
KW  - path planning
KW  - remotely operated vehicles
KW  - hierarchical planning
KW  - time-dependent flow fields
KW  - shortest paths
KW  - flow predictions
KW  - motion planning
KW  - slow marine robots
KW  - dynamic ocean currents
KW  - time-dependent graphs
KW  - polynomial-time algorithm
KW  - continuous trajectories
KW  - time-varying edge costs
KW  - underlying flow field
KW  - continuous algorithm
KW  - time complexity
KW  - path quality properties
KW  - autonomous marine vehicle
KW  - marine robotics
KW  - time-varying ocean predictions
KW  - East Australian Current
KW  - Planning
KW  - Robots
KW  - Vehicle dynamics
KW  - Oceans
KW  - Heuristic algorithms
KW  - Prediction algorithms
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197513
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an efficient approach for finding shortest paths in flow fields that vary as a sequence of flow predictions over time. This approach is applicable to motion planning for slow marine robots that are subject to dynamic ocean currents. Although the problem is NP-hard in general form, we incorporate recent results from the theory of finding shortest paths in time-dependent graphs to construct a polynomial-time algorithm that finds continuous trajectories in time-dependent flow fields. The algorithm has a hierarchical structure where a graph is constructed with time-varying edge costs that are derived from sets of continuous trajectories in the underlying flow field. We show that the continuous algorithm retains the time complexity and path quality properties of the discrete graph solution, and demonstrate its application to surface and underwater vehicles including a traversal along the East Australian Current with an autonomous marine vehicle. Results show that the algorithm performs efficiently in practice and can find paths that adapt to changing ocean currents. These results are significant to marine robotics because they allow for efficient use of time-varying ocean predictions for motion planning.
ER  - 

TY  - CONF
TI  - Navigation in the Presence of Obstacles for an Agile Autonomous Underwater Vehicle
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 892
EP  - 899
AU  - M. Xanthidis
AU  - N. Karapetyan
AU  - H. Damron
AU  - S. Rahman
AU  - J. Johnson
AU  - A. O’Connell
AU  - J. M. O’Kane
AU  - I. Rekleitis
PY  - 2020
KW  - autonomous underwater vehicles
KW  - collision avoidance
KW  - feature extraction
KW  - mobile robots
KW  - navigation
KW  - optimisation
KW  - robot vision
KW  - stereo image processing
KW  - fly-overs
KW  - AUV
KW  - cluttered space
KW  - navigation framework
KW  - sampling-based correction procedure
KW  - obstacles detection
KW  - real-time 3D autonomous navigation
KW  - agile autonomous underwater vehicle
KW  - Trajopt
KW  - 3D path-optimization planning
KW  - visual features detection
KW  - Planning
KW  - Three-dimensional displays
KW  - Navigation
KW  - Optimization
KW  - Robots
KW  - Trajectory
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197558
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Navigation underwater traditionally is done by keeping a safe distance from obstacles, resulting in "fly-overs" of the area of interest. Movement of an autonomous underwater vehicle (AUV) through a cluttered space, such as a shipwreck or a decorated cave, is an extremely challenging problem that has not been addressed in the past. This paper proposes a novel navigation framework utilizing an enhanced version of Trajopt for fast 3D path-optimization planning for AUVs. A sampling-based correction procedure ensures that the planning is not constrained by local minima, enabling navigation through narrow spaces. Two different modalities are proposed: planning with a known map results in efficient trajectories through cluttered spaces; operating in an unknown environment utilizes the point cloud from the visual features detected to navigate efficiently while avoiding the detected obstacles. The proposed approach is rigorously tested, both on simulation and in-pool experiments, proven to be fast enough to enable safe real-time 3D autonomous navigation for an AUV.
ER  - 


