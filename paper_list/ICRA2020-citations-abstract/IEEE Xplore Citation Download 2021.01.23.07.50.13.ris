TY  - CONF
TI  - Mechanically Programmed Miniature Origami Grippers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2872
EP  - 2878
AU  - A. Orlofsky
AU  - C. Liu
AU  - S. Kamrava
AU  - A. Vaziri
AU  - S. M. Felton
PY  - 2020
KW  - actuators
KW  - bending
KW  - grippers
KW  - miniature origami grippers
KW  - robotic gripper design
KW  - customizable grasping tasks
KW  - miniature fingers
KW  - single actuator input
KW  - grasping tasks
KW  - Grippers
KW  - Kinematics
KW  - Grasping
KW  - Actuators
KW  - Springs
KW  - Steel
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196545
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a robotic gripper design that can perform customizable grasping tasks at the millimeter scale. The design is based on the origami string, a mechanism with a single degree of freedom that can be mechanically programmed to approximate arbitrary paths in space. By using this concept, we create miniature fingers that bend at multiple joints with a single actuator input. The shape and stiffness of these fingers can be varied to fit different grasping tasks by changing the crease pattern of the string. We show that the experimental behavior of these strings follows their analytical models and that they can perform a variety of tasks including pinching, wrapping, and twisting common objects such as pencils, bottle caps, and blueberries.
ER  - 

TY  - CONF
TI  - Bio-inspired Tensegrity Fish Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2887
EP  - 2892
AU  - J. Shintake
AU  - D. Zappetti
AU  - T. Peter
AU  - Y. Ikemoto
AU  - D. Floreano
PY  - 2020
KW  - autonomous underwater vehicles
KW  - biomechanics
KW  - bone
KW  - elasticity
KW  - mobile robots
KW  - muscle
KW  - robot dynamics
KW  - servomotors
KW  - underwater vehicles
KW  - fish robots
KW  - body stiffness
KW  - fish swimming
KW  - mechanical stiffness
KW  - tensegrity-based underwater robots
KW  - tensegrity class
KW  - elastic cables
KW  - rigid body segments
KW  - body shape
KW  - tensegrity systems
KW  - fish-like robots
KW  - bio-inspired tensegrity fish robot
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196675
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method to create fish-like robots with tensegrity systems and describes a prototype modeled on the body shape of the rainbow trout with a length of 400 mm and a mass of 102 g that is driven by a waterproof servomotor. The structure of the tensegrity robot consists of rigid body segments and elastic cables that represent bone/tissue and muscles of fish, respectively. This structural configuration employing the tensegrity class 2 is much simpler than other tensegrity-based underwater robots. It also allows the tuning of the mechanical stiffness, which is often said to be an important factor in fish swimming. In our robot, the body stiffness can be tuned by changing the cross-section of the cables and their pre-stretch ratio. We characterize the robot in terms of body stiffness, swimming speed, and thrust force while varying the body stiffness i.e., the cross-section of the elastic cables. The results show that the body stiffness of the robot can be designed to approximate that of the real fish and modulate its performance characteristics. The measured swimming speed of the robot is 0.23 m/s (0.58 BL/s), which is comparable to other fish robots of the same type. Strouhal number of the robot 0.54 is close to that of the natural counterpart, suggesting that the presented method is an effective engineering approach to realize the swimming characteristics of real fish.
ER  - 

TY  - CONF
TI  - Gaussian-Dirichlet Random Fields for Inference over High Dimensional Categorical Observations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2924
EP  - 2931
AU  - J. E. S. Soucie
AU  - H. M. Sosik
AU  - Y. Girdhar
PY  - 2020
KW  - Bayes methods
KW  - Gaussian processes
KW  - image classification
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - vectors
KW  - low dimensional vector observations
KW  - Gaussian-Dirichlet random fields
KW  - high dimensional categorical observations
KW  - spatio-temporal distribution
KW  - imaging sensor
KW  - image classifier
KW  - Dirichlet distributions
KW  - Gaussian processes
KW  - high dimensional categorical measurements
KW  - taxonomic observations
KW  - informative path planning techniques
KW  - Gaussian processes
KW  - Biological system modeling
KW  - Spatiotemporal phenomena
KW  - Graphical models
KW  - Data models
KW  - Robots
KW  - Oceans
DO  - 10.1109/ICRA40945.2020.9196713
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a generative model for the spatio-temporal distribution of high dimensional categorical observations. These are commonly produced by robots equipped with an imaging sensor such as a camera, paired with an image classifier, potentially producing observations over thousands of categories. The proposed approach combines the use of Dirichlet distributions to model sparse co-occurrence relations between the observed categories using a latent variable, and Gaussian processes to model the latent variable's spatio-temporal distribution. Experiments in this paper show that the resulting model is able to efficiently and accurately approximate the temporal distribution of high dimensional categorical measurements such as taxonomic observations of microscopic organisms in the ocean, even in unobserved (held out) locations, far from other samples. This work's primary motivation is to enable deployment of informative path planning techniques over high dimensional categorical fields, which until now have been limited to scalar or low dimensional vector observations.
ER  - 

TY  - CONF
TI  - Investigation of a Multistable Tensegrity Robot applied as Tilting Locomotion System*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2932
EP  - 2938
AU  - P. Schorr
AU  - F. Schale
AU  - J. M. Otterbach
AU  - L. Zentner
AU  - K. Zimmermann
AU  - V. Böhm
PY  - 2020
KW  - actuators
KW  - bifurcation
KW  - mechanical stability
KW  - mobile robots
KW  - motion control
KW  - numerical analysis
KW  - robot dynamics
KW  - robot kinematics
KW  - vibration control
KW  - locomotion characteristics
KW  - actuation strategy
KW  - compliant tensegrity structure
KW  - multistable tensegrity robot
KW  - multiple stable equilibrium configurations
KW  - tilting locomotion system
KW  - Prototypes
KW  - Bifurcation
KW  - Robots
KW  - Mathematical model
KW  - Shape
KW  - Reliability
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196706
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper describes the development of a tilting locomotion system based on a compliant tensegrity structure with multiple stable equilibrium configurations. A tensegrity structure featuring 4 stable equilibrium states is considered. The mechanical model of the structure is presented and the according equations of motion are derived. The variation of the length of selected structural members allows to influence the prestress state and the corresponding shape of the tensegrity structure. Based on bifurcation analyses a reliable actuation strategy to control the current equilibrium state is designed. In this work, the tensegrity structure is assumed to be in contact with a horizontal plane due to gravity. The derived actuation strategy is utilized to generate tilting locomotion by successively changing the equilibrium state. Numerical simulations are evaluated considering the locomotion characteristics. In order to validate this theoretical approach a prototype is developed. Experiments regarding to the equilibrium configurations, the actuation strategy and the locomotion characteristics are evaluated using image processing tools and motion capturing. The results verify the theoretical data and confirm the working principle of the investigated tilting locomotion system. This approach represents a feasible actuation strategy to realize a reliable tilting locomotion utilizing the multistability of compliant tensegrity structures.
ER  - 

TY  - CONF
TI  - A Novel Articulated Soft Robot Capable of Variable Stiffness through Bistable Structure
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2939
EP  - 2945
AU  - Y. Zhong
AU  - R. Du
AU  - L. Wu
AU  - H. Yu
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - dexterous manipulators
KW  - elasticity
KW  - manipulator dynamics
KW  - position control
KW  - servomotors
KW  - variable stiffness
KW  - bistable structure
KW  - unstructured environments
KW  - dynamic environments
KW  - highly dissipative nature
KW  - elastic materials results
KW  - load capability
KW  - rigid joints
KW  - compliant bistable structures
KW  - bending stiffness
KW  - articulated soft robot
KW  - force transmission
KW  - position accuracy
KW  - mechanical constrain
KW  - construction method
KW  - servomotor
KW  - variable workspace
KW  - dexterous manipulation
KW  - tip load
KW  - Soft robotics
KW  - Manipulators
KW  - Force
KW  - Springs
KW  - Kinematics
KW  - Shape
KW  - Articulated Soft Robot
KW  - Variable Stiffness
KW  - Bistable Structure
KW  - Locking Function
DO  - 10.1109/ICRA40945.2020.9197479
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robot has demonstrated promise in unstructured and dynamic environments due to unique advantages, such as safe interaction, adaptiveness, easy to actuate, and easy fabrication. However, the highly dissipative nature of elastic materials results in small stiffness of soft robot which limits certain functions, such as force transmission, position accuracy, and load capability. In this paper, we present a novel articulated soft robot with variable stiffness. The robot is constructed by rigid joints and compliant bistable structures in series. Each joint can be independently locked through triggering the bistable structure to touch the mechanical constrain. Thus, the bending stiffness of the joint can be magnified which increases the stiffness of the articulated soft robot. Through this construction method, even driven by only one servomotor, the robot demonstrates variable workspace and stiffness which have the potential of dexterous manipulation and maintaining shape under tip load.
ER  - 

TY  - CONF
TI  - Modeling and Experiments on the Swallowing and Disgorging Characteristics of an Underwater Continuum Manipulator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2946
EP  - 2952
AU  - H. Wang
AU  - H. Xu
AU  - F. Yu
AU  - X. Li
AU  - C. Yang
AU  - S. Chen
AU  - J. Chen
AU  - Y. Zhang
AU  - X. Zhou
PY  - 2020
KW  - bending
KW  - elasticity
KW  - grippers
KW  - hydraulic systems
KW  - manipulator dynamics
KW  - muscle
KW  - pneumatic actuators
KW  - underwater continuum manipulator
KW  - compliant materials
KW  - McKibben water hydraulic artificial muscle
KW  - WHAM
KW  - mechanical properties
KW  - kinematics model
KW  - soft grippers
KW  - bending procedure
KW  - disgorging procedure
KW  - mouth-tongue collaborative soft robot
KW  - single-segment soft robot arm
KW  - swallowing procedure
KW  - Manipulators
KW  - Muscles
KW  - Grippers
KW  - Kinematics
KW  - Hydraulic systems
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9196780
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots apply compliant materials to perform motions and behaviors not typically achievable by rigid robots. An underwater, compliant, multi-segment continuum manipulator that can bend, swallow, disgorge is developed in this study. The manipulator is driven by McKibben water hydraulic artificial muscle (WHAM). The mechanical properties of the WHAM are tested and analyzed experimentally. The kinematics model, which concerns about the variable diameter structure of the soft grippers, are established to simulate the behaviors of the manipulator among the bending, swallowing and disgorging procedure. A mouth-tongue collaborative soft robot assembled with another single-segment soft robot arm is presented. And its functions are experimentally testified. The distinctive functions were verified according to the experimental results.
ER  - 

TY  - CONF
TI  - Salamanderbot: A soft-rigid composite continuum mobile robot to traverse complex environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2953
EP  - 2959
AU  - Y. Sun
AU  - Y. Jiang
AU  - H. Yang
AU  - L. -C. Walter
AU  - J. Santoso
AU  - E. H. Skorina
AU  - C. Onal
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - continuously deformable slender body structure
KW  - salamanderbot
KW  - cable-driven bellows-like origami module
KW  - powered wheels
KW  - origami structure
KW  - soft-rigid composite continuum mobile robot
KW  - exploration applications
KW  - mobile soft robots
KW  - Yoshimura crease pattern
KW  - velocity 303.1 mm/s
KW  - radius 79.9 mm
KW  - Mobile robots
KW  - Wheels
KW  - Gears
KW  - Manipulators
KW  - DC motors
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196790
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots are theoretically well-suited to rescue and exploration applications where their flexibility allows for the traversal of highly cluttered environments. However, most existing mobile soft robots are not fast or powerful enough to effectively traverse three dimensional environments. In this paper, we introduce a new mobile robot with a continuously deformable slender body structure, the SalamanderBot, which combines the flexibility and maneuverability of soft robots, with the speed and power of traditional mobile robots. It consists of a cable-driven bellows-like origami module based on the Yoshimura crease pattern mounted between sets of powered wheels. The origami structure allows the body to deform as necessary to adapt to complex environments and terrains, while the wheels allow the robot to reach speeds of up to 303.1 mm/s (2.05 body-length/s). Salamanderbot can climb up to 60-degree slopes and perform sharp turns with a minimum turning radius of 79.9 mm (0.54 body-length).
ER  - 

TY  - CONF
TI  - Flexure Hinge-based Biomimetic Thumb with a Rolling-Surface Metacarpal Joint
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2960
EP  - 2966
AU  - S. Pulleyking
AU  - J. Schultz
PY  - 2020
KW  - biomimetics
KW  - bone
KW  - control system synthesis
KW  - dexterous manipulators
KW  - end effectors
KW  - manipulator kinematics
KW  - motion control
KW  - position control
KW  - surgery
KW  - flexure hinge-based biomimetic thumb
KW  - rolling-surface metacarpal joint
KW  - grasping
KW  - dexterous manipulation
KW  - kinematic multiplicity
KW  - robotic hand
KW  - kinematic model
KW  - surgical techniques
KW  - motion capture data
KW  - end effector
KW  - task-space velocities
KW  - tendon excursion velocity
KW  - human thumb state contribution
KW  - data representation
KW  - effector velocity
KW  - Joints
KW  - Fasteners
KW  - Thumb
KW  - Ellipsoids
KW  - Robots
KW  - Prototypes
KW  - Ceramics
DO  - 10.1109/ICRA40945.2020.9196578
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The human thumb's state contribution to grasping and dexterous manipulation of objects is a function of the kinematic multiplicity of joints and structure of the bones, joints, and ligaments. This paper looks at the design and evaluation of a human-like thumb for use in a robotic hand, where the thumb's state contribution to grasping and dexterous manipulation is a function of a simplified kinematic model based on that of the human thumb, but also on empirical trials of surgical techniques to retain functionality while reducing the number of joints in the thumb. Motion Capture Data of the End Effector is analyzed with the measured excursion of the tendons to determine the relationship between tendon velocities and task-space velocities. After validating the procedure experimentally, a simplified metric is proposed to represent this data, and shows that our prototype is predicted to have a relatively smooth mapping between tendon excursion velocity and end effector velocity.
ER  - 

TY  - CONF
TI  - Ibex: A reconfigurable ground vehicle with adaptive terrain navigation capability
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2975
EP  - 2980
AU  - S. Raj
AU  - R. P. Manu Aatitya
AU  - S. Jack Samuel
AU  - J. V. Karthik
AU  - D. Ezhilarasi
PY  - 2020
KW  - friction
KW  - mobile robots
KW  - off-road vehicles
KW  - optimisation
KW  - remotely operated vehicles
KW  - robot dynamics
KW  - stability
KW  - vehicle dynamics
KW  - reconfigurable ground vehicle
KW  - adaptive terrain navigation capability
KW  - unmanned ground vehicle
KW  - dynamic wheelbase
KW  - adaptive thrust
KW  - friction optimization
KW  - steep slopes
KW  - slippery surfaces
KW  - surface topography
KW  - impedance-based stabilization module
KW  - mechanical oscillatory transients
KW  - Ibex
KW  - Force
KW  - Wheels
KW  - Surface topography
KW  - Surface impedance
KW  - Land vehicles
KW  - Kinematics
KW  - Drag
DO  - 10.1109/ICRA40945.2020.9196571
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a unique unmanned ground vehicle with a dynamic wheelbase and an adaptive thrust based friction optimization scheme that aids in the traversal of steep slopes and slippery surfaces. The vehicle is capable of adapting itself to the surface topography using an impedance-based stabilization module to minimize the mechanical oscillatory transients induced during its motion. A detailed analysis of its modules has been elucidated in this paper based on the vehicle parameters. The proposed methodologies have been integrated and tested on a customized prototype. Experimental validation and simulation for the proposed modules at various terrain conditions have been carried out to authenticate its performance.
ER  - 

TY  - CONF
TI  - Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2981
EP  - 2987
AU  - Y. Yue
AU  - C. Yang
AU  - J. Zhang
AU  - M. Wen
AU  - Z. Wu
AU  - H. Zhang
AU  - D. Wang
PY  - 2020
KW  - groupware
KW  - image fusion
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - sensor fusion
KW  - SLAM (robots)
KW  - dynamic collaborative mapping
KW  - multimodal environmental perception
KW  - heterogeneous sensor fusion model
KW  - local 3D maps
KW  - night rainforest
KW  - 3D map fusion missions
KW  - multimodal sensors
KW  - long-term operation
KW  - collaborative robots
KW  - dynamic environment
KW  - dynamic objects
KW  - Collaboration
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197072
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.
ER  - 

TY  - CONF
TI  - Generating Locomotion with Effective Wheel Radius Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2988
EP  - 2994
AU  - T. Hojnik
AU  - L. Pond
AU  - R. Dungavell
AU  - P. Flick
AU  - J. Roberts
PY  - 2020
KW  - mobile robots
KW  - motion control
KW  - motor drives
KW  - road vehicles
KW  - robot dynamics
KW  - vehicle dynamics
KW  - wheels
KW  - motor drives
KW  - sloped terrain
KW  - wheel rotation
KW  - plain centre hub drive
KW  - active ride height selection
KW  - wheel radius manipulation
KW  - locomotion generation
KW  - slope traversability
KW  - wheel pose control
KW  - centre of gravity manipulation
KW  - Wheels
KW  - Gravity
KW  - Acceleration
KW  - Mathematical model
KW  - Torque
KW  - Axles
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9196825
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Travel over sloped terrain is difficult as an incline changes the interaction between each wheel and the ground resulting in an unbalanced load distribution which can lead to loss of traction and instability. This paper presents a novel approach to generating wheel rotation for primary locomotion by only changing its centre of rotation, or as a complimentary locomotion source to increase versatility of a plain centre hub drive. This is done using linear actuators within a wheel to control the position of the centre hub and induce a moment on the wheel from gravity. In doing so our platform allows for active ride height selection and individual wheel pose control. We present the system with calculations outlining the theoretical properties and perform experiments to validate the concept under loading via multiple gaits to show motion on slopes, and sustained motion over extended distance. We envision applications in conjunction to assist current motor drives and increasing slope traversability by allowing body pose and centre of gravity manipulation, or as a primary locomotion system.
ER  - 

TY  - CONF
TI  - A GNC Architecture for Planetary Rovers with Autonomous Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3003
EP  - 3009
AU  - M. Azkarate
AU  - L. Gerdes
AU  - L. Joudrier
AU  - C. J. Pérez-del-Pulgar
PY  - 2020
KW  - aerospace navigation
KW  - aerospace robotics
KW  - distance measurement
KW  - Mars
KW  - mobile robots
KW  - optimal control
KW  - path planning
KW  - planetary rovers
KW  - robot vision
KW  - SLAM (robots)
KW  - trajectory control
KW  - GNC architecture
KW  - planetary rovers
KW  - autonomous navigation
KW  - Mars exploration missions
KW  - sample fetching rover
KW  - autonomous capabilities
KW  - two-level architecture
KW  - terrain
KW  - local path replanning
KW  - trajectory control
KW  - global localization
KW  - planetary exploration
KW  - planetary analog field test campaigns
KW  - guidance navigation and control architecture
KW  - hazard detection
KW  - visual odometry
KW  - adaptive SLAM algorithm
KW  - optimal path planning
KW  - Hazards
KW  - Navigation
KW  - Space vehicles
KW  - Autonomous robots
KW  - Computer architecture
KW  - Cameras
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197122
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a Guidance, Navigation, and Control (GNC) architecture for planetary rovers targeting the conditions of upcoming Mars exploration missions such as Mars 2020 and the Sample Fetching Rover (SFR). The navigation requirements of these missions demand a control architecture featuring autonomous capabilities to achieve a fast and long traverse. The proposed solution presents a two-level architecture where the efficient navigation (lower) level is always active and the full navigation (upper) level is enabled according to the difficulty of the terrain. The first level is an efficient implementation of the basic functionalities for autonomous navigation based on hazard detection, local path replanning, and trajectory control with visual odometry. The second level implements an adaptive SLAM algorithm that improves the relative localization, evaluates the traversability of the terrain ahead for a more optimal path planning, and performs global (absolute) localization that corrects the pose drift during longer traverses. The architecture provides a solution for long-range, low supervision, and fast planetary exploration. Both navigation levels have been validated on planetary analog field test campaigns.
ER  - 

TY  - CONF
TI  - Learning Face Recognition Unsupervisedly by Disentanglement and Self-Augmentation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3018
EP  - 3024
AU  - Y. -L. Lee
AU  - M. -Y. Tseng
AU  - Y. -C. Luo
AU  - D. -R. Yu
AU  - W. -C. Chiu
PY  - 2020
KW  - face recognition
KW  - feature extraction
KW  - home automation
KW  - unsupervised learning
KW  - video surveillance
KW  - triplet network
KW  - augmentation network
KW  - identity-aware features
KW  - face samples
KW  - identity-irrelevant features
KW  - home robot applications
KW  - face recognition system
KW  - smart home environment
KW  - environment-specific face recognition model
KW  - unsupervised learning
KW  - self-augmentation
KW  - healthcare application
KW  - camera position
KW  - surveillance video
KW  - identity-aware feature extraction
KW  - spatiotemporal characteristic
KW  - face image disentanglement
DO  - 10.1109/ICRA40945.2020.9197348
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As the growth of smart home, healthcare, and home robot applications, learning a face recognition system which is specific for a particular environment and capable of self-adapting to the temporal changes in appearance (e.g., caused by illumination or camera position) is nowadays an important topic. In this paper, given a video of a group of people, which simulates the surveillance video in a smart home environment, we propose a novel approach which unsuper- visedly learns a face recognition model based on two main components: (1) a triplet network that extracts identity-aware feature from face images for performing face recognition by clustering, and (2) an augmentation network that is conditioned on the identity-aware features and aims at synthesizing more face samples. Particularly, the training data for the triplet network is obtained by using the spatiotemporal characteristic of face samples within a video, while the augmentation network learns to disentangle a face image into identity-aware and identity-irrelevant features thus is able to generate new faces of the same identity but with variance in appearance. With taking the richer training data produced by augmentation network, the triplet network is further fine-tuned and achieves better performance in face recognition. Extensive experiments not only show the efficacy of our model in learning an environment- specific face recognition model unsupervisedly, but also verify its adaptability to various appearance changes.
ER  - 

TY  - CONF
TI  - PARC: A Plan and Activity Recognition Component for Assistive Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3025
EP  - 3031
AU  - J. Massardi
AU  - M. Gravel
AU  - É. Beaudry
PY  - 2020
KW  - assisted living
KW  - mobile robots
KW  - robot vision
KW  - activity recognition solutions
KW  - human-object interaction
KW  - low-level actions
KW  - goal recognition algorithm
KW  - low-cost robotics platform
KW  - assistive robots
KW  - mobile robot assistants
KW  - daily living activities
KW  - RGB-D camera
KW  - Activity recognition
KW  - Hidden Markov models
KW  - Robot sensing systems
KW  - Feature extraction
KW  - Clustering algorithms
KW  - Activity recognition
KW  - Plan recognition
KW  - Computer vision
KW  - Robotic assistant
KW  - Activities For Daily Living
KW  - Object affordance
KW  - Particle filter
DO  - 10.1109/ICRA40945.2020.9196856
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mobile robot assistants have many applications, such as helping people in their daily living activities. These robots have to detect and recognize the actions and goals of the humans they are assisting. While there are several wide-spread plan and activity recognition solutions for controlled environments with many built-in sensors, like smart-homes, there is a lack of such systems for mobile robots operating in open settings, such as an apartment. We propose a module for the recognition of activities and goals for daily living by mobile robots, in real time and for complex activities. Our approach recognizes human-object interaction using an RGB-D camera to infer low-level actions which are sent to a goal recognition algorithm. Results show that our approach is both in real time and requires little computational resources, which facilitates its deployment on a mobile and low-cost robotics platform.
ER  - 

TY  - CONF
TI  - Image-Based Place Recognition on Bucolic Environment Across Seasons From Semantic Edge Description
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3032
EP  - 3038
AU  - A. Benbihi
AU  - S. Arravechia
AU  - M. Geist
AU  - C. Pradalier
PY  - 2020
KW  - edge detection
KW  - feature extraction
KW  - image matching
KW  - image retrieval
KW  - image texture
KW  - object recognition
KW  - wavelet transforms
KW  - multiseason environment-monitoring datasets
KW  - urban scenes
KW  - image-based place recognition
KW  - bucolic environment
KW  - semantic edge description
KW  - urban environments
KW  - natural scenes
KW  - semantic content
KW  - vegetation state
KW  - bucolic scene
KW  - global image description
KW  - semantic information
KW  - topological information
KW  - matching two images
KW  - semantic edge transforms
KW  - state-of-the-art image retrieval performance
KW  - Image edge detection
KW  - Semantics
KW  - Image segmentation
KW  - Image retrieval
KW  - Feature extraction
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9197529
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Most of the research effort on image-based place recognition is designed for urban environments. In bucolic environments such as natural scenes with low texture and little semantic content, the main challenge is to handle the variations in visual appearance across time such as illumination, weather, vegetation state or viewpoints. The nature of the variations is different and this leads to a different approach to describing a bucolic scene. We introduce a global image description computed from its semantic and topological information. It is built from the wavelet transforms of the image's semantic edges. Matching two images is then equivalent to matching their semantic edge transforms. This method reaches state-of-the-art image retrieval performance on two multi-season environment-monitoring datasets: the CMU-Seasons and the Symphony Lake dataset. It also generalizes to urban scenes on which it is on par with the current baselines NetVLAD and DELF.
ER  - 

TY  - CONF
TI  - A Multilayer-Multimodal Fusion Architecture for Pattern Recognition of Natural Manipulations in Percutaneous Coronary Interventions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3039
EP  - 3045
AU  - X. -H. Zhou
AU  - X. -L. Xie
AU  - Z. -Q. Feng
AU  - Z. -G. Hou
AU  - G. -B. Bian
AU  - R. -Q. Li
AU  - Z. -L. Ni
AU  - S. -Q. Liu
AU  - Y. -J. Zhou
PY  - 2020
KW  - control engineering computing
KW  - human-robot interaction
KW  - manipulators
KW  - medical computing
KW  - medical robotics
KW  - sensor fusion
KW  - multilayer-multimodal fusion architecture
KW  - pattern recognition
KW  - natural manipulations
KW  - percutaneous coronary interventions
KW  - robotic systems
KW  - robot-assisted procedures
KW  - human-robot interfaces
KW  - guidewire manipulations
KW  - multimodal behaviors
KW  - rule-based fusion algorithms
KW  - singlelayer recognition architecture
KW  - robot-assisted PCI
KW  - HRI
KW  - X-ray radiation reduction
KW  - medical staff
KW  - Sensors
KW  - Robots
KW  - Muscles
KW  - Feature extraction
KW  - Force
KW  - Catheters
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9197111
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The increasingly-used robotic systems can provide precise delivery and reduce X-ray radiation to medical staff in percutaneous coronary interventions (PCI), but natural manipulations of interventionalists are forgone in most robot-assisted procedures. Therefore, it is necessary to explore natural manipulations to design more advanced human-robot interfaces (HRI). In this study, a multilayer-multimodal fusion architecture is proposed to recognize six typical subpatterns of guidewire manipulations in conventional PCI. The synchronously acquired multimodal behaviors from ten subjects are used as the inputs of the fusion architecture. Six classification-based and two rule-based fusion algorithms are evaluated for performance comparisons. Experimental results indicate that the multimodal fusion brings significant accuracy improvement in comparison with single-modal schemes. Furthermore, the proposed architecture can achieve the overall accuracy of 96.90%, much higher than that of a singlelayer recognition architecture (92.56%). These results have indicated the potential of the proposed method for facilitating the development of HRI for robot-assisted PCI.
ER  - 

TY  - CONF
TI  - Real-Time Graph-Based SLAM with Occupancy Normal Distributions Transforms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3106
EP  - 3111
AU  - C. Schulz
AU  - A. Zell
PY  - 2020
KW  - graph theory
KW  - least squares approximations
KW  - mobile robots
KW  - normal distribution
KW  - robot vision
KW  - SLAM (robots)
KW  - occupancy grid map
KW  - graph-based SLAM
KW  - occupancy normal distribution transforms
KW  - normal distributions transforms
KW  - simultaneous localization and mapping
KW  - mobile robotics
KW  - least squares problem
KW  - nonlinear optimizers
KW  - global NDT scan matcher
KW  - Simultaneous localization and mapping
KW  - Cost function
KW  - Google
KW  - Jacobian matrices
KW  - Gaussian distribution
DO  - 10.1109/ICRA40945.2020.9197325
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Simultaneous Localization and Mapping (SLAM) is one of the basic problems in mobile robotics. While most approaches are based on occupancy grid maps, Normal Distributions Transforms (NDT) and mixtures like Occupancy Normal Distribution Transforms (ONDT) have been shown to represent sensor measurements more accurately. In this work, we slightly re-formulate the (O)NDT matching function such that it becomes a least squares problem that can be solved with various robust numerical and analytical non-linear optimizers. Further, we propose a novel global (O)NDT scan matcher for loop closure. In our evaluation, our NDT and ONDT methods are able to outperform the occupancy grid map based ones we adopted from Google's Cartographer implementation.
ER  - 

TY  - CONF
TI  - Spatio-Temporal Non-Rigid Registration of 3D Point Clouds of Plants
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3112
EP  - 3118
AU  - N. Chebrolu
AU  - T. Läbe
AU  - C. Stachniss
PY  - 2020
KW  - agriculture
KW  - cameras
KW  - image registration
KW  - robot vision
KW  - spatio-temporal nonrigid registration
KW  - 3D point clouds
KW  - sensor data
KW  - agricultural robotics
KW  - plant science
KW  - agricultural tasks
KW  - automated temporal plant-trait analysis
KW  - plant performance monitoring
KW  - plant registration
KW  - Three-dimensional displays
KW  - Skeleton
KW  - Hidden Markov models
KW  - Strain
KW  - Topology
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9197569
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Analyzing sensor data of plants and monitoring plant performance is a central element in different agricultural robotics applications. In plant science, phenotyping refers to analyzing plant traits for monitoring growth, for describing plant properties, or characterizing the plant's overall performance. It plays a critical role in the agricultural tasks and in plant breeding. Recently, there is a rising interest in using 3D data obtained from laser scanners and 3D cameras to develop automated non-intrusive techniques for estimating plant traits. In this paper, we address the problem of registering 3D point clouds of the plants over time, which is a backbone of applications interested in tracking spatio-temporal traits of individual plants. Registering plants over time is challenging due to its changing topology, anisotropic growth, and non-rigid motion in between scans. We propose a novel approach that exploits the skeletal structure of the plant and determines correspondences over time and drives the registration process. Our approach explicitly accounts for the non-rigidity and the growth of the plant over time in the registration. We tested our approach on a challenging dataset acquired over the course of two weeks and successfully registered the 3D plant point clouds recorded with a laser scanner forming a basis for developing systems for automated temporal plant-trait analysis.
ER  - 

TY  - CONF
TI  - Uncertainty-Based Adaptive Sensor Fusion for Visual-Inertial Odometry under Various Motion Characteristics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3119
EP  - 3125
AU  - R. Nakashima
AU  - A. Seki
PY  - 2020
KW  - distance measurement
KW  - image fusion
KW  - Jacobian matrices
KW  - motion estimation
KW  - visual-inertial odometry
KW  - inertial measurement units
KW  - scale-aware estimation
KW  - sensor states
KW  - sensor motion
KW  - noninformative inertial measurements
KW  - estimation modes
KW  - motion characteristics
KW  - uncertainty-based adaptive sensor fusion
KW  - uncertainty-based sensor fusion
KW  - relative motion estimation
KW  - observability
KW  - Jacobian matrices
KW  - Estimation
KW  - Bundle adjustment
KW  - Motion measurement
KW  - Velocity measurement
KW  - Uncertainty
KW  - Sensor fusion
DO  - 10.1109/ICRA40945.2020.9197397
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose an uncertainty-based sensor fusion framework for visual-inertial odometry, which is the task of estimating relative motion using images and measurements from inertial measurement units. Visual-inertial odometry enables robust and scale-aware estimation of motion by incorporating sensor states, such as metric scale, velocity, and the direction of gravity, into the estimation. However, the observability of the states depends on sensor motion. For example, if the sensor moves in a constant velocity, scale and velocity cannot be observed from inertial measurements. Under these degenerate motions, existing methods may produce inaccurate results because they incorporate erroneous states estimated from non-informative inertial measurements. Our proposed framework is able to avoid this situation by adaptively switching estimation modes, which represents the states that should be incorporated, based on their uncertainties. These uncertainties can be obtained at a small computational cost by reusing the Jacobian matrices computed in bundle adjustment. Our approach consistently outperformed conventional sensor fusion in datasets with different motion characteristics, namely, the KITTI odometry dataset recorded by a ground vehicle and the EuRoC MAV dataset captured from a micro aerial vehicle.
ER  - 

TY  - CONF
TI  - Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3126
EP  - 3131
AU  - J. Lin
AU  - F. Zhang
PY  - 2020
KW  - distance measurement
KW  - mobile robots
KW  - optical radar
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - FoV
KW  - autonomous vehicles
KW  - autonomous navigation
KW  - path planning
KW  - LOAM algorithm
KW  - LiDAR odometry and mapping
KW  - robot pose localization
KW  - Laser radar
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Measurement by laser beam
KW  - Laser noise
KW  - Real-time systems
KW  - Spinning
DO  - 10.1109/ICRA40945.2020.9197440
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot's pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.
ER  - 

TY  - CONF
TI  - Active SLAM using 3D Submap Saliency for Underwater Volumetric Exploration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3132
EP  - 3138
AU  - S. Suresh
AU  - P. Sodhi
AU  - J. G. Mangelson
AU  - D. Wettergreen
AU  - M. Kaess
PY  - 2020
KW  - graph theory
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - underwater volumetric exploration
KW  - active SLAM framework
KW  - 3D underwater environments
KW  - multibeam sonar
KW  - integrated SLAM
KW  - volumetric free-space information
KW  - informative loop closures
KW  - navigation policy
KW  - 3D visual dictionary
KW  - submap saliency
KW  - sensor information
KW  - pose-graph SLAM formulation
KW  - global occupancy grid map
KW  - uncertainty-agnostic framework
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Uncertainty
KW  - Conferences
KW  - Automation
KW  - Sonar
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196939
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an active SLAM framework for volumetric exploration of 3D underwater environments with multibeam sonar. Recent work in integrated SLAM and planning performs localization while maintaining volumetric free-space information. However, an absence of informative loop closures can lead to imperfect maps, and therefore unsafe behavior. To solve this, we propose a navigation policy that reduces vehicle pose uncertainty by balancing between volumetric exploration and revisitation. To identify locations to revisit, we build a 3D visual dictionary from real-world sonar data and compute a metric of submap saliency. Revisit actions are chosen based on propagated pose uncertainty and sensor information gain. Loop closures are integrated as constraints in our pose-graph SLAM formulation and these deform the global occupancy grid map. We evaluate our performance in simulation and real-world experiments, and highlight the advantages over an uncertainty-agnostic framework.
ER  - 

TY  - CONF
TI  - Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3139
EP  - 3145
AU  - X. Shi
AU  - D. Li
AU  - P. Zhao
AU  - Q. Tian
AU  - Y. Tian
AU  - Q. Long
AU  - C. Zhu
AU  - J. Song
AU  - F. Qiao
AU  - L. Song
AU  - Y. Guo
AU  - Z. Wang
AU  - Y. Zhang
AU  - B. Qin
AU  - W. Yang
AU  - F. Wang
AU  - R. H. M. Chan
AU  - Q. She
PY  - 2020
KW  - mobile robots
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - service robots
KW  - SLAM (robots)
KW  - simultaneous localization and mapping
KW  - data sequences
KW  - robotic autonomy
KW  - service robots
KW  - real-world indoor scenes
KW  - OpenLORIS-Scene datasets
KW  - SLAM problems
KW  - pose estimation
KW  - Simultaneous localization and mapping
KW  - Robot kinematics
KW  - Cameras
KW  - Synchronization
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196638
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.
ER  - 

TY  - CONF
TI  - RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, & New Methods
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3146
EP  - 3152
AU  - S. Herath
AU  - H. Yan
AU  - Y. Furukawa
PY  - 2020
KW  - computer vision
KW  - image motion analysis
KW  - inertial navigation
KW  - neural nets
KW  - robust neural inertial navigation
KW  - data-driven inertial navigation research
KW  - horizontal positions
KW  - moving subject
KW  - IMU sensor data
KW  - ground-truth 3D trajectories
KW  - natural human motions
KW  - RoNIN
KW  - Inertial navigation
KW  - Three-dimensional displays
KW  - Robustness
KW  - Estimation
KW  - Trajectory
KW  - Task analysis
KW  - Gravity
DO  - 10.1109/ICRA40945.2020.9196860
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of horizontal positions and heading direction of a moving subject from a sequence of IMU sensor measurements from a phone. In contrast to existing methods, our method can handle varying phone orientations and placements.More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We share the code and data to promote further research. (http://ronin.cs.sfu.ca).
ER  - 

TY  - CONF
TI  - Segmenting 2K-Videos at 36.5 FPS with 24.3 GFLOPs: Accurate and Lightweight Realtime Semantic Segmentation Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3153
EP  - 3160
AU  - D. Oh
AU  - D. Ji
AU  - C. Jang
AU  - Y. Hyun
AU  - H. S. Bae
AU  - S. Hwang
PY  - 2020
KW  - convolutional neural nets
KW  - image classification
KW  - image segmentation
KW  - object detection
KW  - video signal processing
KW  - lightweight segementation models
KW  - 2K-Videos
KW  - high resolution videos
KW  - NfS-SegNet
KW  - computation-efficiency
KW  - encoder network
KW  - NfS-Net
KW  - simple building blocks
KW  - memory-heavy operations
KW  - depthwise convolutions
KW  - CNN architectures
KW  - asymmetric architecture
KW  - deeper encoder
KW  - uncertainty-aware knowledge distillation method
KW  - realtime semantic segmentation network
KW  - CITYSCAPE benchmark
KW  - computer speed 24.3 GFLOPS
KW  - Knowledge engineering
KW  - Computational modeling
KW  - Real-time systems
KW  - Computer architecture
KW  - Semantics
KW  - Uncertainty
KW  - Videos
DO  - 10.1109/ICRA40945.2020.9196510
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a fast and lightweight end-to-end convolutional network architecture for real-time segmentation of high resolution videos, NfS-SegNet, that can segement 2K-videos at 36.5 FPS with 24.3 GFLOPS. This speed and computation-efficiency is due to following reasons: 1) The encoder network, NfS-Net, is optimized for speed with simple building blocks without memory-heavy operations such as depthwise convolutions, and outperforms state-of-the-art lightweight CNN architectures such as SqueezeNet [2], Mo- bileNet v1 [3] & v2 [4] and ShuffleNet v1 [5] & v2 [6] on image classification with significantly higher speed. 2) The NfS- SegNet has an asymmetric architecture with deeper encoder and shallow decoder, whose design is based on our empirical finding that the decoder is the main bottleneck in computation with relatively small contribution to the final performance. 3) Our novel uncertainty-aware knowledge distillation method guides the teacher model to focus its knowledge transfer on the most difficult image regions. We validate the performance of NfS-SegNet with the CITYSCAPE [1] benchmark, on which it achieves state-of-the-art performance among lightweight segementation models in terms of both accuracy and speed.
ER  - 

TY  - CONF
TI  - Temporally Consistent Horizon Lines
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3161
EP  - 3167
AU  - F. Kluger
AU  - H. Ackermann
AU  - M. Y. Yang
AU  - B. Rosenhahn
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - image reconstruction
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - recurrent neural nets
KW  - stereo image processing
KW  - video signal processing
KW  - geometric feature
KW  - image processing
KW  - scene understanding
KW  - computer vision
KW  - autonomous vehicle navigation
KW  - driver assistance
KW  - semantic interpretation
KW  - dynamic environments
KW  - convolutional neural networks
KW  - residual convolutional LSTM
KW  - temporally consistent horizon line estimation
KW  - video sequences
KW  - 3D reconstruction
KW  - CNN architecture
KW  - adaptive loss function
KW  - Video sequences
KW  - Cameras
KW  - Three-dimensional displays
KW  - Observers
KW  - Task analysis
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197170
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The horizon line is an important geometric feature for many image processing and scene understanding tasks in computer vision. For instance, in navigation of autonomous vehicles or driver assistance, it can be used to improve 3D reconstruction as well as for semantic interpretation of dynamic environments. While both algorithms and datasets exist for single images, the problem of horizon line estimation from video sequences has not gained attention. In this paper, we show how convolutional neural networks are able to utilise the temporal consistency imposed by video sequences in order to increase the accuracy and reduce the variance of horizon line estimates. A novel CNN architecture with an improved residual convolutional LSTM is presented for temporally consistent horizon line estimation. We propose an adaptive loss function that ensures stable training as well as accurate results. Furthermore, we introduce an extension of the KITTI dataset which contains precise horizon line labels for 43699 images across 72 video sequences. A comprehensive evaluation shows that the proposed approach consistently achieves superior performance compared with existing methods.
ER  - 

TY  - CONF
TI  - Full-Scale Continuous Synthetic Sonar Data Generation with Markov Conditional Generative Adversarial Networks*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3168
EP  - 3174
AU  - M. Jegorova
AU  - A. I. Karjalainen
AU  - J. Vazquez
AU  - T. Hospedales
PY  - 2020
KW  - acoustic signal processing
KW  - autonomous underwater vehicles
KW  - data analysis
KW  - environmental factors
KW  - Markov processes
KW  - naval engineering computing
KW  - neural nets
KW  - object recognition
KW  - realistic images
KW  - sonar imaging
KW  - statistical analysis
KW  - bootstrapping ATR systems
KW  - autonomous underwater vehicles
KW  - autonomous target recognition systems
KW  - realistic synthetic sonar imagery
KW  - Markov conditional generative adversarial networks
KW  - continuous synthetic sonar data generation
KW  - Markov conditional pix2pix
KW  - environmental factors
KW  - acoustic sensors
KW  - Sonar
KW  - Training
KW  - Semantics
KW  - Data models
KW  - Gallium nitride
KW  - Training data
KW  - Markov processes
DO  - 10.1109/ICRA40945.2020.9197353
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deployment and operation of autonomous underwater vehicles is expensive and time-consuming. High-quality realistic sonar data simulation could be of benefit to multiple applications, including training of human operators for post-mission analysis, as well as tuning and validation of autonomous target recognition (ATR) systems for underwater vehicles. Producing realistic synthetic sonar imagery is a challenging problem as the model has to account for specific artefacts of real acoustic sensors, vehicle attitude, and a variety of environmental factors. We propose a novel method for generating realistic-looking sonar side-scans of full-length missions, called Markov Conditional pix2pix (MC-pix2pix). Quantitative assessment results confirm that the quality of the produced data is almost indistinguishable from real. Furthermore, we show that bootstrapping ATR systems with MC-pix2pix data can improve the performance. Synthetic data is generated 18 times faster than real acquisition speed, with full user control over the topography of the generated data.
ER  - 

TY  - CONF
TI  - Adaptively Informed Trees (AIT*): Fast Asymptotically Optimal Path Planning through Adaptive Heuristics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3191
EP  - 3198
AU  - M. P. Strub
AU  - J. D. Gammell
PY  - 2020
KW  - path planning
KW  - sampling methods
KW  - search problems
KW  - trees (mathematics)
KW  - heuristic estimates
KW  - AIT*
KW  - asymmetric bidirectional search
KW  - optimal path planning
KW  - informed sampling-based planning algorithm
KW  - adaptively informed trees
KW  - Search problems
KW  - Image edge detection
KW  - Approximation algorithms
KW  - Planning
KW  - Heuristic algorithms
KW  - Databases
KW  - Path planning
DO  - 10.1109/ICRA40945.2020.9197338
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Informed sampling-based planning algorithms exploit problem knowledge for better search performance. This knowledge is often expressed as heuristic estimates of solution cost and used to order the search. The practical improvement of this informed search depends on the accuracy of the heuristic. Selecting an appropriate heuristic is difficult. Heuristics applicable to an entire problem domain are often simple to define and inexpensive to evaluate but may not be beneficial for a specific problem instance. Heuristics specific to a problem instance are often difficult to define or expensive to evaluate but can make the search itself trivial. This paper presents Adaptively Informed Trees (AIT*), an almost-surely asymptotically optimal sampling-based planner based on BIT*. AIT* adapts its search to each problem instance by using an asymmetric bidirectional search to simultaneously estimate and exploit a problem-specific heuristic. This allows it to quickly find initial solutions and converge towards the optimum. AIT* solves the tested problems as fast as RRT-Connect while also converging towards the optimum.
ER  - 

TY  - CONF
TI  - Informing Multi-Modal Planning with Synergistic Discrete Leads
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3199
EP  - 3205
AU  - Z. Kingston
AU  - A. M. Wells
AU  - M. Moll
AU  - L. E. Kavraki
PY  - 2020
KW  - continuous systems
KW  - discrete systems
KW  - manipulators
KW  - multimodal planning
KW  - robotic manipulation problems
KW  - continuous infinity
KW  - object grasping
KW  - manipulation plan
KW  - single-mode motions
KW  - valid transitions
KW  - manipulation planners
KW  - multimodal structure
KW  - mode-specific planners
KW  - general layered planning approach
KW  - pick-and-place manipulation domain
KW  - synergistic discrete leads
KW  - specific mode transitions
KW  - useful mode transitions
KW  - bias search
KW  - Planning
KW  - Manifolds
KW  - Task analysis
KW  - Lead
KW  - Probabilistic logic
KW  - Robot motion
DO  - 10.1109/ICRA40945.2020.9197545
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic manipulation problems are inherently continuous, but typically have underlying discrete structure, e.g., whether or not an object is grasped. This means many problems are multi-modal and in particular have a continuous infinity of modes. For example, in a pick-and-place manipulation domain, every grasp and placement of an object is a mode. Usually manipulation problems require the robot to transition into different modes, e.g., going from a mode with an object placed to another mode with the object grasped. To successfully find a manipulation plan, a planner must find a sequence of valid single-mode motions as well as valid transitions between these modes. Many manipulation planners have been proposed to solve tasks with multi-modal structure. However, these methods require mode-specific planners and fail to scale to very cluttered environments or to tasks that require long sequences of transitions. This paper presents a general layered planning approach to multi-modal planning that uses a discrete "lead" to bias search towards useful mode transitions. The difficulty of achieving specific mode transitions is captured online and used to bias search towards more promising sequences of modes. We demonstrate our planner on complex scenes and show that significant performance improvements are tied to both our discrete "lead" and our continuous representation.
ER  - 

TY  - CONF
TI  - Hierarchical Coverage Path Planning in Complex 3D Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3206
EP  - 3212
AU  - C. Cao
AU  - J. Zhang
AU  - M. Travers
AU  - H. Choset
PY  - 2020
KW  - autonomous aerial vehicles
KW  - hierarchical systems
KW  - image resolution
KW  - image sampling
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - complex three-dimensional environment
KW  - nooks
KW  - crannies
KW  - coverage planning
KW  - multiresolution hierarchical framework
KW  - three-dimensional scenes
KW  - hierarchical coverage path planning
KW  - lightweight UAV
KW  - low-level sampling
KW  - complex 3D environments
KW  - Planning
KW  - Robot sensing systems
KW  - Cameras
KW  - Octrees
KW  - Three-dimensional displays
KW  - Surface treatment
DO  - 10.1109/ICRA40945.2020.9196575
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - State-of-the-art coverage planning methods perform well in simple environments but take an ineffectively long time to converge to an optimal solution in complex three-dimensional (3D) environments. As more structures are present in the same volume of workspace, these methods slow down as they spend more time searching for all of the nooks and crannies concealed in three-dimensional spaces. This work presents a method for coverage planning that employs a multi-resolution hierarchical framework to solve the problem at two different levels, producing much higher efficiency than the state-of-the-art. First, a high-level algorithm separates the environment into multiple subspaces at different resolutions and computes an order of the subspaces for traversal. Second, a low-level sampling-based algorithm solves for paths within the subspaces for detailed coverage. In experiments, we evaluate our method using real-world datasets from complex three-dimensional scenes. Our method finds paths that are constantly shorter and converges at least ten times faster than the state-of-the-art. Further, we show results of a physical experiment where a lightweight UAV follows the paths to realize the coverage.
ER  - 

TY  - CONF
TI  - Perception-aware time optimal path parameterization for quadrotors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3213
EP  - 3219
AU  - I. Spasojevic
AU  - V. Murali
AU  - S. Karaman
PY  - 2020
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - perception-aware time optimal path parameterization
KW  - quadrotors
KW  - perception-aware time optimal path parametrization
KW  - quadrotor systems
KW  - on-board navigation
KW  - estimation algorithms
KW  - planning
KW  - efficient time optimal path parametrization algorithm
KW  - quadrotor platform
KW  - vision-driven vehicles
KW  - Trajectory
KW  - Cameras
KW  - Planning
KW  - Task analysis
KW  - Aerodynamics
KW  - Heuristic algorithms
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197157
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The increasing popularity of quadrotors has given rise to a class of predominantly vision-driven vehicles. This paper addresses the problem of perception-aware time optimal path parametrization for quadrotors. Although many different choices of perceptual modalities are available, the low weight and power budgets of quadrotor systems makes a camera ideal for on-board navigation and estimation algorithms. However, this does come with a set of challenges. The limited field of view of the camera can restrict the visibility of salient regions in the environment, which dictates the necessity to consider perception and planning jointly. The main contribution of this paper is an efficient time optimal path parametrization algorithm for quadrotors with limited field of view constraints. We show in a simulation study that a state-of-the-art controller can track planned trajectories, and we validate the proposed algorithm on a quadrotor platform in experiments.
ER  - 

TY  - CONF
TI  - Generating Visibility-Aware Trajectories for Cooperative and Proactive Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3220
EP  - 3226
AU  - N. Buckman
AU  - A. Pierson
AU  - S. Karaman
AU  - D. Rus
PY  - 2020
KW  - mobile robots
KW  - motion estimation
KW  - object detection
KW  - path planning
KW  - road traffic
KW  - road vehicles
KW  - vehicles
KW  - autonomous vehicle
KW  - ego vehicle
KW  - visibility-aware planning
KW  - visibility-aware trajectories
KW  - proactive motion planning
KW  - cooperative motion planning
KW  - partially-occluded intersection
KW  - emergent behavior
KW  - Trajectory
KW  - Uncertainty
KW  - Planning
KW  - Safety
KW  - Autonomous vehicles
KW  - Splines (mathematics)
DO  - 10.1109/ICRA40945.2020.9196809
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The safety of an autonomous vehicle not only depends on its own perception of the world around it, but also on the perception and recognition from other vehicles. If an ego vehicle considers the uncertainty other vehicles have about itself, then by reducing the estimated uncertainty it can increase its safety. In this paper, we focus on how an ego vehicle plans its trajectories through the blind spots of other vehicles. We create visibility-aware planning, where the ego vehicle chooses its trajectories such that it reduces the perceived uncertainty other vehicles may have about the state of the ego vehicle. We present simulations of traffic and highway environments, where an ego vehicle must pass another vehicle, make a lane change, or traverse a partially-occluded intersection. Emergent behavior shows that when using visibility-aware planning, the ego vehicle spends less time in a blind spot, and may slow down before entering the blind spot so as to increase the likelihood other vehicles perceive the ego vehicle.
ER  - 

TY  - CONF
TI  - An obstacle-interaction planning method for navigation of actuated vine robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3227
EP  - 3233
AU  - M. Selvaggio
AU  - L. A. Ramirez
AU  - N. D. Naclerio
AU  - B. Siciliano
AU  - E. W. Hawkes
PY  - 2020
KW  - bending
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - soft robotics
KW  - reliable robot-environment interaction models
KW  - obstacle-interaction model
KW  - robot tip
KW  - obstacle-interaction planning method
KW  - actuated vine robot navigation
KW  - wrinkling deformation
KW  - bending deformation
KW  - Soft robotics
KW  - Strain
KW  - Deformable models
KW  - Planning
KW  - Kinematics
KW  - Pneumatic systems
DO  - 10.1109/ICRA40945.2020.9196587
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The field of soft robotics is grounded on the idea that, due to their inherent compliance, soft robots can safely interact with the environment. Thus, the development of effective planning and control pipelines for soft robots should incorporate reliable robot-environment interaction models. This strategy enables soft robots to effectively exploit contacts to autonomously navigate and accomplish tasks in the environment. However, for a class of soft robots, namely vine-inspired, tip-extending or "vine" robots, such interaction models and the resulting planning and control strategies do not exist. In this paper, we analyze the behavior of vine robots interacting with their environment and propose an obstacle-interaction model that characterizes the bending and wrinkling deformation induced by the environment. Starting from this, we devise a novel obstacle-interaction planning method for these robots. We show how obstacle interactions can be effectively leveraged to enlarge the set of reachable workspace for the robot tip, and verify our findings with both simulated and real experiments. Our work improves the capabilities of this new class of soft robot, helping to advance the field of soft robotics.
ER  - 

TY  - CONF
TI  - Distributed Consensus Control of Multiple UAVs in a Constrained Environment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3234
EP  - 3240
AU  - G. Wang
AU  - W. Yang
AU  - N. Zhao
AU  - Y. Ji
AU  - Y. Shen
AU  - H. Xu
AU  - P. Li
PY  - 2020
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - decentralised control
KW  - distributed control
KW  - multi-robot systems
KW  - position control
KW  - tracking
KW  - trees (mathematics)
KW  - multiple UAVs
KW  - constrained environment
KW  - consensus problem
KW  - multiple unmanned aerial vehicles
KW  - environmental constraints
KW  - general communication topology
KW  - directed spanning tree
KW  - position transformation function
KW  - dynamic reference position
KW  - yaw angle
KW  - asymmetric topology
KW  - local tracking controller
KW  - distributed consensus control
KW  - Topology
KW  - Decentralized control
KW  - Protocols
KW  - Unmanned aerial vehicles
KW  - Tracking loops
KW  - Heuristic algorithms
KW  - Attitude control
DO  - 10.1109/ICRA40945.2020.9196926
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we investigate the consensus problem of multiple unmanned aerial vehicles (UAVs) in the presence of environmental constraints under a general communication topology containing a directed spanning tree. First, based on a position transformation function, we propose a novel dynamic reference position and yaw angle for each UAV to cope with both the asymmetric topology and the constraints. Then, the backstepping-like design methodology is presented to derive a local tracking controller for each UAV such that its position and yaw angle can converge to the reference ones. The proposed protocol is distributed in the sense that, the input update of each UAV dynamically relies only on local state information from its neighborhood set and the constraints, and it does not require any additional centralized information. It is demonstrated that under the proposed protocol, all UAVs reach consensus without violation of the environmental constraints. Finally, simulation and experimental results are provided to demonstrate the performance of the protocol.
ER  - 

TY  - CONF
TI  - Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using Learned Interactions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3241
EP  - 3247
AU  - G. Shi
AU  - W. Hönig
AU  - Y. Yue
AU  - S. -J. Chung
PY  - 2020
KW  - aerodynamics
KW  - aerospace robotics
KW  - control system synthesis
KW  - decentralised control
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - neurocontrollers
KW  - nonlinear control systems
KW  - particle swarm optimisation
KW  - stability
KW  - close-proximity multirotor control
KW  - learned interactions
KW  - Neural-Swarm
KW  - nonlinear decentralized stable controller
KW  - close-proximity flight
KW  - multirotor swarms
KW  - close-proximity control
KW  - complex aerodynamic interaction effects
KW  - safety distances
KW  - nominal dynamics model
KW  - regularized permutation-invariant Deep Neural Network
KW  - high-order multivehicle interactions
KW  - larger swarm sizes
KW  - baseline nonlinear
KW  - stable nonlinear tracking controller
KW  - Vehicle dynamics
KW  - Aerodynamics
KW  - Neural networks
KW  - Rotors
KW  - Stability analysis
KW  - Training
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196800
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present Neural-Swarm, a nonlinear decentralized stable controller for close-proximity flight of multirotor swarms. Close-proximity control is challenging due to the complex aerodynamic interaction effects between multirotors, such as downwash from higher vehicles to lower ones. Conventional methods often fail to properly capture these interaction effects, resulting in controllers that must maintain large safety distances between vehicles, and thus are not capable of close-proximity flight. Our approach combines a nominal dynamics model with a regularized permutation-invariant Deep Neural Network (DNN) that accurately learns the high-order multi-vehicle interactions. We design a stable nonlinear tracking controller using the learned model. Experimental results demonstrate that the proposed controller significantly outperforms a baseline nonlinear tracking controller with up to four times smaller worst-case height tracking errors. We also empirically demonstrate the ability of our learned model to generalize to larger swarm sizes.
ER  - 

TY  - CONF
TI  - Line Coverage with Multiple Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3248
EP  - 3254
AU  - S. Agarwal
AU  - S. Akella
PY  - 2020
KW  - computational complexity
KW  - graph theory
KW  - integer programming
KW  - linear programming
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - robot tour generation
KW  - multiple robots
KW  - line coverage problem
KW  - mixed integer linear program
KW  - NP-hard
KW  - merge-embed-merge
KW  - MEM algorithm
KW  - graph simplification
KW  - graph partitioning
KW  - Roads
KW  - Task analysis
KW  - Robot sensing systems
KW  - Routing
KW  - Heuristic algorithms
KW  - Partitioning algorithms
DO  - 10.1109/ICRA40945.2020.9197292
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The line coverage problem is the coverage of linear environment features (e.g., road networks, power lines), modeled as 1D segments, by one or more robots while respecting resource constraints (e.g., battery capacity, flight time) for each of the robots. The robots incur direction dependent costs and resource demands as they traverse the edges. We treat the line coverage problem as an optimization problem, with the total cost of the tours as the objective, by formulating it as a mixed integer linear program (MILP). The line coverage problem is NP-hard and hence we develop a heuristic algorithm, Merge-Embed-Merge (MEM). We compare it against the optimal MILP approach and a baseline heuristic algorithm, Extended Path Scanning. We show the MEM algorithm is fast and suitable for real-time applications. To tackle large-scale problems, our approach performs graph simplification and graph partitioning, followed by robot tour generation for each of the partitioned subgraphs. We demonstrate our approach on a large graph with 4,658 edges and 4,504 vertices that represents an urban region of about 16 sq. km. We compare the performance of the algorithms on several small road networks and experimentally demonstrate the approach using UAVs on the UNC Charlotte campus road network.
ER  - 

TY  - CONF
TI  - Visual Coverage Maintenance for Quadcopters Using Nonsmooth Barrier Functions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3255
EP  - 3261
AU  - R. Funada
AU  - M. Santos
AU  - T. Gencho
AU  - J. Yamauchi
AU  - M. Fujita
AU  - M. Egerstedt
PY  - 2020
KW  - aircraft control
KW  - helicopters
KW  - image sensors
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - quadcopters
KW  - visual sensors
KW  - coverage holes
KW  - coverage quality
KW  - sufficient conditions
KW  - nonsmooth barrier functions
KW  - visual coverage maintenance
KW  - coverage control
KW  - necessary conditions
KW  - Visualization
KW  - Monitoring
KW  - Robot sensing systems
KW  - Switches
KW  - Space missions
DO  - 10.1109/ICRA40945.2020.9196650
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a coverage control algorithm for teams of quadcopters with downward facing visual sensors that prevents the appearance of coverage holes in-between the monitored areas while maximizing the coverage quality as much as possible. We derive necessary and sufficient conditions for preventing the appearance of holes in-between the fields of views among trios of robots. Because this condition can be expressed as logically combined constraints, control nonsmooth barrier functions are implemented to enforce it. An algorithm which extends control nonsmooth barrier functions to hybrid systems is implemented to manage the switching among barrier functions caused by the changes of the robots composing trio. The performance and validity of the proposed algorithm are evaluated in simulation as well as on a team of quadcopters.
ER  - 

TY  - CONF
TI  - Goal-Directed Occupancy Prediction for Lane-Following Actors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3270
EP  - 3276
AU  - P. Kaniarasu
AU  - G. C. Haynes
AU  - M. Marchetti-Bowick
PY  - 2020
KW  - mobile robots
KW  - motion estimation
KW  - prediction theory
KW  - road safety
KW  - road traffic
KW  - road vehicles
KW  - roads
KW  - robot vision
KW  - traffic engineering computing
KW  - complex road networks
KW  - mapped road topology
KW  - dynamic road actors
KW  - mapped lane geometry
KW  - mode collapse problem
KW  - goal-directed occupancy prediction
KW  - lane-following actors
KW  - shared roads
KW  - safe autonomous driving
KW  - possible vehicle behaviors
KW  - possible goal reasoning
KW  - local scene context multimodality
KW  - high-level action set
KW  - future spatial occupancy prediction
KW  - Roads
KW  - Predictive models
KW  - Trajectory
KW  - Topology
KW  - Geometry
KW  - Task analysis
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9197495
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Predicting the possible future behaviors of vehicles that drive on shared roads is a crucial task for safe autonomous driving. Many existing approaches to this problem strive to distill all possible vehicle behaviors into a simplified set of high-level actions. However, these action categories do not suffice to describe the full range of maneuvers possible in the complex road networks we encounter in the real world. To combat this deficiency, we propose a new method that leverages the mapped road topology to reason over possible goals and predict the future spatial occupancy of dynamic road actors. We show that our approach is able to accurately predict future occupancy that remains consistent with the mapped lane geometry and naturally captures multi-modality based on the local scene context while also not suffering from the mode collapse problem observed in prior work.
ER  - 

TY  - CONF
TI  - Intent-Aware Pedestrian Prediction for Adaptive Crowd Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3277
EP  - 3283
AU  - K. D. Katyal
AU  - G. D. Hager
AU  - C. -M. Huang
PY  - 2020
KW  - adaptive control
KW  - collision avoidance
KW  - control engineering computing
KW  - mobile robots
KW  - navigation
KW  - pedestrians
KW  - road traffic control
KW  - traffic engineering computing
KW  - personal space violation
KW  - adaptive navigation policy
KW  - pedestrian motion
KW  - intent-aware pedestrian prediction
KW  - adaptive crowd navigation
KW  - mobile robots
KW  - pedestrian rich environments
KW  - robotic assistance
KW  - pedestrian navigation
KW  - real-world pedestrian datasets
KW  - Navigation
KW  - Trajectory
KW  - Robots
KW  - Prediction algorithms
KW  - Training
KW  - Collision avoidance
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197434
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mobile robots capable of navigating seamlessly and safely in pedestrian rich environments promise to bring robotic assistance closer to our daily lives. In this paper we draw on insights of how humans move in crowded spaces to explore how to recognize pedestrian navigation intent, how to predict pedestrian motion and how a robot may adapt its navigation policy dynamically when facing unexpected human movements. Our approach is to develop algorithms that replicate this behavior. We experimentally demonstrate the effectiveness of our prediction algorithm using real-world pedestrian datasets and achieve comparable or better prediction accuracy compared to several state-of-the-art approaches. Moreover, we show that confidence of pedestrian prediction can be used to adjust the risk of a navigation policy adaptively to afford the most comfortable level as measured by the frequency of personal space violation in comparison with baselines. Furthermore, our adaptive navigation policy is able to reduce the number of collisions by 43% in the presence of novel pedestrian motion not seen during training.
ER  - 

TY  - CONF
TI  - Brno Urban Dataset - The New Data for Self-Driving Agents and Mapping Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3284
EP  - 3290
AU  - A. Ligocki
AU  - A. Jelinek
AU  - L. Zalud
PY  - 2020
KW  - cameras
KW  - Global Positioning System
KW  - infrared detectors
KW  - mobile robots
KW  - optical radar
KW  - SLAM (robots)
KW  - WUXGA cameras
KW  - 3D LiDAR
KW  - inertial measurement unit
KW  - infrared camera
KW  - differential RTK GNSS receiver
KW  - centimetre accuracy
KW  - public dataset
KW  - submillisecond precision
KW  - autonomous driving
KW  - Brno Urban dataset
KW  - self-driving agents
KW  - mapping tasks
KW  - Brno-Czech Republic
KW  - https://github.com/RoboticsBUT/Brno-Urban-Dataset
KW  - Cameras
KW  - Sensors
KW  - Global Positioning System
KW  - Global navigation satellite system
KW  - Receivers
KW  - Laser radar
KW  - Synchronization
DO  - 10.1109/ICRA40945.2020.9197277
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous driving is a dynamically growing field of research, where quality and amount of experimental data is critical. Although several rich datasets are available these days, the demands of researchers and technical possibilities are evolving. Through this paper, we bring a new dataset recorded in Brno - Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with submillisecond precision to allow wider range of applications. At the time of publishing of this paper, recordings of more than 350 km of rides in varying environment are shared at: https://github.com/RoboticsBUT/Brno-Urban-Dataset.
ER  - 

TY  - CONF
TI  - Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3291
EP  - 3297
AU  - L. Zhang
AU  - W. Ding
AU  - J. Chen
AU  - S. Shen
PY  - 2020
KW  - decision making
KW  - decision theory
KW  - Markov processes
KW  - multi-agent systems
KW  - road traffic
KW  - trees (mathematics)
KW  - dense traffic scenarios
KW  - automated vehicles
KW  - stochastic behaviors
KW  - traffic participants
KW  - perception uncertainties
KW  - partially observable Markov decision process
KW  - efficient uncertainty-aware decision-making
KW  - longitudinal behaviors
KW  - complex driving environments
KW  - automated driving
KW  - guided branching
KW  - domain-specific closed-loop policy tree structure
KW  - DCP-Tree
KW  - conditional focused branching mechanism
KW  - CFB
KW  - domain-specific expert knowledge
KW  - Planning
KW  - Decision making
KW  - Uncertainty
KW  - Semantics
KW  - Vegetation
KW  - Aerospace electronics
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9197302
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Decision-making in dense traffic scenarios is challenging for automated vehicles (AVs) due to potentially stochastic behaviors of other traffic participants and perception uncertainties (e.g., tracking noise and prediction errors, etc.). Although the partially observable Markov decision process (POMDP) provides a systematic way to incorporate these uncertainties, it quickly becomes computationally intractable when scaled to the real-world large-size problem. In this paper, we present an efficient uncertainty-aware decision-making (EUDM) framework, which generates long-term lateral and longitudinal behaviors in complex driving environments in real-time. The computation complexity is controlled to an appropriate level by two novel techniques, namely, the domain-specific closed-loop policy tree (DCP-Tree) structure and conditional focused branching (CFB) mechanism. The key idea is utilizing domain-specific expert knowledge to guide the branching in both action and intention space. The proposed framework is validated using both onboard sensing data captured by a real vehicle and an interactive multi-agent simulation platform. We also release the code of our framework to accommodate benchmarking.
ER  - 

TY  - CONF
TI  - Imitative Reinforcement Learning Fusing Vision and Pure Pursuit for Self-driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3298
EP  - 3304
AU  - M. Peng
AU  - Z. Gong
AU  - C. Sun
AU  - L. Chen
AU  - D. Cao
PY  - 2020
KW  - generalisation (artificial intelligence)
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - road vehicles
KW  - sensor fusion
KW  - steering systems
KW  - traffic engineering computing
KW  - pretrained IPP model
KW  - CARLA driving benchmark
KW  - generalization capability
KW  - pure pursuit
KW  - autonomous urban driving navigation
KW  - two-stage framework
KW  - visual information
KW  - pure-pursuit method
KW  - steering angle
KW  - imitation learning performance
KW  - driving data
KW  - reinforcement learning method
KW  - deep deterministic policy gradient
KW  - IPP-RL framework
KW  - Learning (artificial intelligence)
KW  - Meteorology
KW  - Robustness
KW  - Task analysis
KW  - Navigation
KW  - Training
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9197027
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous urban driving navigation is still an open problem and has ample room for improvement in unknown complex environments and terrible weather conditions. In this paper, we propose a two-stage framework, called IPP-RL, to handle these problems. IPP means an Imitation learning method fusing visual information with the additional steering angle calculated by Pure-Pursuit (PP) method, and RL means using Reinforcement Learning for further training. In our IPP model, the visual information captured by camera can be compensated by the calculated steering angle, thus it could perform well under bad weather conditions. However, imitation learning performance is limited by the driving data severely. Thus we use a reinforcement learning method-Deep Deterministic Policy Gradient (DDPG)-in the second stage training, which shares the learned weights from pretrained IPP model. In this way, our IPP-RL can lower the dependency of imitation learning on demonstration data and solve the problem of low exploration efficiency caused by randomly initialized weights in reinforcement learning. Moreover, we design a more reasonable reward function and use the n-step return to update the critic-network in DDPG. Our experiments on CARLA driving benchmark demonstrate that our IPP-RL is robust to lousy weather conditions and shows remarkable generalization capability in unknown environments on navigation task.
ER  - 

TY  - CONF
TI  - Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3305
EP  - 3311
AU  - A. Savkin
AU  - T. Lapotre
AU  - K. Strauss
AU  - U. Akbar
AU  - F. Tombari
PY  - 2020
KW  - augmented reality
KW  - driver information systems
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - object detection
KW  - pedestrians
KW  - road vehicles
KW  - traffic engineering computing
KW  - adversarial appearance learning
KW  - augmented Cityscapes
KW  - pedestrian recognition
KW  - autonomous driving area synthetic data
KW  - traffic scenarios
KW  - autonomous vehicle
KW  - data augmentation
KW  - Cityscapes dataset
KW  - virtual pedestrians
KW  - augmentation realism
KW  - generative network architecture
KW  - data-set lighting conditions
KW  - VRU
KW  - Solid modeling
KW  - Semantics
KW  - Three-dimensional displays
KW  - Autonomous vehicles
KW  - Training
KW  - Cameras
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197024
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.
ER  - 

TY  - CONF
TI  - ROI-cloud: A Key Region Extraction Method for LiDAR Odometry and Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3312
EP  - 3318
AU  - Z. Zhou
AU  - M. Yang
AU  - C. Wang
AU  - B. Wang
PY  - 2020
KW  - Bayes methods
KW  - distance measurement
KW  - feature extraction
KW  - image filtering
KW  - image matching
KW  - image registration
KW  - image sampling
KW  - Monte Carlo methods
KW  - optical radar
KW  - pose estimation
KW  - robot vision
KW  - ROI-cloud
KW  - key region extraction method
KW  - LiDAR odometry
KW  - LiDAR scan
KW  - on-board IMU/odometry data
KW  - Bayes filtering
KW  - Monte Carlo sampling
KW  - autonomous robot
KW  - LiDAR localization
KW  - voxelized cube set
KW  - pose estimation
KW  - point set registration
KW  - massive point cloud data
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Heuristic algorithms
KW  - Vehicle dynamics
KW  - Robots
KW  - Urban areas
DO  - 10.1109/ICRA40945.2020.9197059
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel key region extraction method of point cloud, ROI-cloud, for LiDAR odometry and localization with autonomous robots. Traditional methods process massive point cloud data in every region within the field of view. In dense urban environments, however, processing redundant and dynamic regions of point cloud is time-consuming and harmful to the results of matching algorithms. In this paper, a voxelized cube set, ROI-cloud, is proposed to solve this problem by exclusively reserving the regions of interest for better point set registration and pose estimation. 3D space is firstly voxelized into weighted cubes. The key idea is to update their weights continually and extract cubes with high importance as key regions. By extracting geometrical features of a LiDAR scan, the importance of each cube is evaluated as a new measurement. With the help of on-board IMU/odometry data as well as new measurements, the weights of cubes are updated recursively through Bayes filtering. Thus, dynamic and redundant point cloud inside cubes with low importance are discarded by means of Monte Carlo sampling. Our method is validated on various datasets, and results indicate that the ROI-cloud improves the existing method in both accuracy and speed.
ER  - 

TY  - CONF
TI  - To Learn or Not to Learn: Visual Localization from Essential Matrices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3319
EP  - 3326
AU  - Q. Zhou
AU  - T. Sattler
AU  - M. Pollefeys
AU  - L. Leal-Taixé
PY  - 2020
KW  - cameras
KW  - feature extraction
KW  - image representation
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - visual localization
KW  - scene-specific representations
KW  - relative pose estimation
KW  - feature-based approach
KW  - deep learning
KW  - camera
KW  - autonomous robots
KW  - Cameras
KW  - Visualization
KW  - Three-dimensional displays
KW  - Pipelines
KW  - Pose estimation
KW  - Image retrieval
DO  - 10.1109/ICRA40945.2020.9196607
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual localization is the problem of estimating a camera within a scene and a key technology for autonomous robots. State-of-the-art approaches for accurate visual localization use scene-specific representations, resulting in the overhead of constructing these models when applying the techniques to new scenes. Recently, learned approaches based on relative pose estimation have been proposed, carrying the promise of easily adapting to new scenes. However, they are currently significantly less accurate than state-of-the-art approaches. In this paper, we are interested in analyzing this behavior. To this end, we propose a novel framework for visual localization from relative poses. Using a classical feature-based approach within this framework, we show state-of-the-art performance. Replacing the classical approach with learned alternatives at various levels, we then identify the reasons for why deep learned approaches do not perform well. Based on our analysis, we make recommendations for future work.
ER  - 

TY  - CONF
TI  - Hierarchical Multi-Process Fusion for Visual Place Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3327
EP  - 3333
AU  - S. Hausler
AU  - M. Milford
PY  - 2020
KW  - image fusion
KW  - mobile robots
KW  - object recognition
KW  - robot vision
KW  - hierarchical multiprocess fusion
KW  - multiple complementary techniques
KW  - visual localization
KW  - multisensor fusion
KW  - varying performance characteristics
KW  - hierarchical localization system
KW  - localization hypotheses
KW  - localization performance
KW  - final localization stage
KW  - parallel fusion
KW  - visual place recognition
KW  - Databases
KW  - Visualization
KW  - Feature extraction
KW  - Robots
KW  - Pipelines
KW  - Histograms
DO  - 10.1109/ICRA40945.2020.9197360
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpointinvariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques.
ER  - 

TY  - CONF
TI  - Camera Tracking in Lighting Adaptable Maps of Indoor Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3334
EP  - 3340
AU  - T. Caselitz
AU  - M. Krawez
AU  - J. Sundram
AU  - M. Van Loock
AU  - W. Burgard
PY  - 2020
KW  - feature extraction
KW  - image colour analysis
KW  - image sequences
KW  - indoor environment
KW  - lighting
KW  - object tracking
KW  - rendering (computer graphics)
KW  - direct dense camera tracking approach
KW  - lighting adaptable map representation
KW  - lighting invariant map representations
KW  - lighting conditions
KW  - visual localization methods
KW  - indoor environments
KW  - Lighting
KW  - Cameras
KW  - Visualization
KW  - Indoor environments
KW  - Estimation
KW  - Mathematical model
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9197471
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tracking the pose of a camera is at the core of visual localization methods used in many applications. As the observations of a camera are inherently affected by lighting, it has always been a challenge for these methods to cope with varying lighting conditions. Thus far, this issue has mainly been approached with the intent to increase robustness by choosing lighting invariant map representations. In contrast, our work aims at explicitly exploiting lighting effects for camera tracking. To achieve this, we propose a lighting adaptable map representation for indoor environments that allows real-time rendering of the scene illuminated by an arbitrary subset of the lamps contained in the model. Our method for estimating the light setting from the current camera observation enables us to adapt the model according to the lighting conditions present in the scene. As a result, lighting effects like cast shadows do no longer act as disturbances that demand robustness but rather as beneficial features when matching observations against the map. We leverage these capabilities in a direct dense camera tracking approach and demonstrate its performance in realworld experiments in scenes with varying lighting conditions.
ER  - 

TY  - CONF
TI  - Fast, Compact and Highly Scalable Visual Place Recognition through Sequence-based Matching of Overloaded Representations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3341
EP  - 3348
AU  - S. Garg
AU  - M. Milford
PY  - 2020
KW  - image matching
KW  - image representation
KW  - image sequences
KW  - mobile robots
KW  - quantisation (signal)
KW  - robot vision
KW  - storage management
KW  - hashing overload approach
KW  - compact place recognition
KW  - overloaded representations
KW  - storage footprint
KW  - place recognition system
KW  - ultra-compact place representations
KW  - sublinear storage scaling
KW  - sublinear computational scaling
KW  - visual place recognition
KW  - match selections
KW  - sequence-based matching
KW  - scalar quantization-based hashing
KW  - mobile robot
KW  - Quantization (signal)
KW  - Visualization
KW  - Indexes
KW  - Robots
KW  - Principal component analysis
KW  - Benchmark testing
KW  - Image recognition
DO  - 10.1109/ICRA40945.2020.9196827
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual place recognition algorithms trade off three key characteristics: their storage footprint, their computational requirements, and their resultant performance, often expressed in terms of recall rate. Significant prior work has investigated highly compact place representations, sub-linear computational scaling and sub-linear storage scaling techniques, but have always involved a significant compromise in one or more of these regards, and have only been demonstrated on relatively small datasets. In this paper we present a novel place recognition system which enables for the first time the combination of ultra-compact place representations, near sub-linear storage scaling and extremely lightweight compute requirements. Our approach exploits the inherently sequential nature of much spatial data in the robotics domain and inverts the typical target criteria, through intentionally coarse scalar quantization-based hashing that leads to more collisions but is resolved by sequence-based matching. For the first time, we show how effective place recognition rates can be achieved on a new very large 10 million place dataset, requiring only 8 bytes of storage per place and 37K unitary operations to achieve over 50% recall for matching a sequence of 100 frames, where a conventional stateof-the-art approach both consumes 1300 times more compute and fails catastrophically. We present analysis investigating the effectiveness of our hashing overload approach under varying sizes of quantized vector length, comparison of near miss matches with the actual match selections and characterise the effect of variance re-scaling of data on quantization. Resource link: https://github.com/oravus/CoarseHash.
ER  - 

TY  - CONF
TI  - Vision-based Multi-MAV Localization with Anonymous Relative Measurements Using Coupled Probabilistic Data Association Filter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3349
EP  - 3355
AU  - T. Nguyen
AU  - K. Mohta
AU  - C. J. Taylor
AU  - V. Kumar
PY  - 2020
KW  - aerospace robotics
KW  - microrobots
KW  - mobile robots
KW  - multi-robot systems
KW  - pose estimation
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - target tracking
KW  - multiMAV system
KW  - robot team
KW  - vision based detection
KW  - distance measurements
KW  - coupled probabilistic data association filter
KW  - nonlinear measurements
KW  - visual based robot to robot detection
KW  - vision based multiMAV localization
KW  - robot localization
KW  - robot pose estimation
KW  - multiple microaerial vehicles
KW  - Robot kinematics
KW  - Robot sensing systems
KW  - Noise measurement
KW  - Probabilistic logic
KW  - Task analysis
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196793
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the localization of robots in a multi-MAV system where external infrastructure like GPS or motion capture systems may not be available. Our approach lends itself to implementation on platforms with several constraints on size, weight, and power (SWaP). Particularly, our framework fuses the onboard VIO with the anonymous, visual-based robot-to-robot detection to estimate all robot poses in one common frame, addressing three main challenges: 1) the initial configuration of the robot team is unknown, 2) the data association between each vision-based detection and robot targets is unknown, and 3) the vision-based detection yields false negatives, false positives, inaccurate, and provides noisy bearing, distance measurements of other robots. Our approach extends the Coupled Probabilistic Data Association Filter [1] to cope with nonlinear measurements. We demonstrate the superior performance of our approach over a simple VIO-based method in a simulation with the measurement models statistically modeled using the real experimental data. We also show how onboard sensing, estimation, and control can be used for formation flight.
ER  - 

TY  - CONF
TI  - MANGA: Method Agnostic Neural-policy Generalization and Adaptation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3356
EP  - 3362
AU  - H. Bharadhwaj
AU  - S. Yamaguchi
AU  - S. -i. Maeda
PY  - 2020
KW  - learning (artificial intelligence)
KW  - robot dynamics
KW  - MANGA
KW  - multiple environments
KW  - dynamics parameters
KW  - motor noise variations
KW  - policy learning
KW  - system identification
KW  - unknown environment
KW  - dynamics configurations
KW  - dynamics conditioned policies
KW  - off-policy state-transition rollouts
KW  - training method
KW  - method agnostic neural-policy generalization and adaptation
KW  - transferring policies
KW  - Training
KW  - Robots
KW  - Task analysis
KW  - Encoding
KW  - Decoding
KW  - Heuristic algorithms
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9197398
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we target the problem of transferring policies across multiple environments with different dynamics parameters and motor noise variations, by introducing a framework that decouples the processes of policy learning and system identification. Efficiently transferring learned policies to an unknown environment with changes in dynamics configurations in the presence of motor noise is very important for operating robots in the real world, and our work is a novel attempt in that direction. We introduce MANGA: Method Agnostic Neural-policy Generalization and Adaptation, that trains dynamics conditioned policies and efficiently learns to estimate the dynamics parameters of the environment given off-policy state-transition rollouts in the environment. Our scheme is agnostic to the type of training method used - both reinforcement learning (RL) and imitation learning (IL) strategies can be used. We demonstrate the effectiveness of our approach by experimenting with four different MuJoCo agents and comparing against previously proposed transfer baselines.
ER  - 

TY  - CONF
TI  - Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3363
EP  - 3370
AU  - J. Choi
AU  - C. Dance
AU  - J. -e. Kim
AU  - K. -s. Park
AU  - J. Han
AU  - J. Seo
AU  - M. Kim
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - robust control
KW  - fast adaptation
KW  - deep reinforcement learning-based navigation skills
KW  - human preference
KW  - robot navigation
KW  - robustness
KW  - maximum velocities
KW  - reward components
KW  - optimal choice
KW  - real-world service scenarios
KW  - deep RL navigation method
KW  - reward functions
KW  - Bayesian deep learning method
KW  - preference data
KW  - diverse navigation skills
KW  - deep RL navigation agents
KW  - Navigation
KW  - Collision avoidance
KW  - Bayes methods
KW  - Training
KW  - Robot kinematics
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9197159
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.
ER  - 

TY  - CONF
TI  - Variational Inference with Mixture Model Approximation for Applications in Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3395
EP  - 3401
AU  - E. Pignat
AU  - T. Lembono
AU  - S. Calinon
PY  - 2020
KW  - approximation theory
KW  - Bayes methods
KW  - control engineering computing
KW  - inference mechanisms
KW  - mixture models
KW  - robots
KW  - statistical distributions
KW  - variational inference
KW  - mixture model approximation
KW  - robot configurations
KW  - Bayesian computation
KW  - Robots
KW  - Mixture models
KW  - Kinematics
KW  - Bayes methods
KW  - Optimization
KW  - Task analysis
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197166
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose to formulate the problem of representing a distribution of robot configurations (e.g. joint angles) as that of approximating a product of experts. Our approach uses variational inference, a popular method in Bayesian computation, which has several practical advantages over sampling-based techniques. To be able to represent complex and multimodal distributions of configurations, mixture models are used as approximate distribution. We show that the problem of approximating a distribution of robot configurations while satisfying multiple objectives arises in a wide range of problems in robotics, for which the properties of the proposed approach have relevant consequences. Several applications are discussed, including learning objectives from demonstration, planning, and warm-starting inverse kinematics problems. Simulated experiments are presented with a 7-DoF Panda arm and a 28-DoF Talos humanoid.
ER  - 

TY  - CONF
TI  - Injection of a Fluorescent Microsensor into a Specific Cell by Laser Manipulation and Heating with Multiple Wavelengths of Light
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3437
EP  - 3442
AU  - H. Maruyama
AU  - H. Hashim
AU  - R. Yanagawa
AU  - F. Arai
PY  - 2020
KW  - biomedical optical imaging
KW  - cellular biophysics
KW  - dyes
KW  - fluorescence
KW  - infrared spectra
KW  - kidney
KW  - microsensors
KW  - refractive index
KW  - Rhodamine B
KW  - temperature-sensitive fluorescent dye
KW  - refractive index
KW  - polystyrene particle
KW  - cell injection
KW  - laser manipulation
KW  - multiple wavelengths
KW  - fluorescent microsensor
KW  - Microsensors
KW  - Fluorescence
KW  - Semiconductor lasers
KW  - Heating systems
KW  - Measurement by laser beam
KW  - Laser excitation
KW  - Temperature measurement
DO  - 10.1109/ICRA40945.2020.9197234
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this study, we propose the manipulation and cell injection of a fluorescent microsensor using multiple wavelengths of light. The fluorescent microsensor is made of a 1-μm polystyrene particle containing infrared (IR: 808 nm) absorbing dye and Rhodamine B. The polystyrene particle can be manipulated in water using a 1064-nm laser because the refractive index of the polystyrene is 1.6 (refractive index of water: 1.3). The IR absorbing dye absorbs 808-nm light but does not absorb the 1064-nm laser. Rhodamine B is a temperature-sensitive fluorescent dye (excitation wavelength: 488 nm, emission wavelength: 560 nm). The functions of manipulation, heating for injection, and temperature measurement are achieved by different wavelengths of 1064 nm, 808 nm, and 488 nm, respectively. The temperature increase of fluorescent microsensor with 808-nm (40 mW, 10 s) laser was approximately 15°C, and enough for injection of fluorescent microsensor. We demonstrated manipulation and injection of the microsensor into Madin-Darby canine kidney cell using 1064-nm and 808-nm lasers. These results confirmed the effectiveness of our proposed cell injection of a fluorescent microsensor using multiple wavelengths of light.
ER  - 

TY  - CONF
TI  - Passive Quadrupedal Gait Synchronization for Extra Robotic Legs Using a Dynamically Coupled Double Rimless Wheel Model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3451
EP  - 3457
AU  - D. J. Gonzalez
AU  - H. H. Asada
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - springs (mechanical)
KW  - synchronisation
KW  - wheels
KW  - extra robotic legs system
KW  - robotic augmentation
KW  - human operator
KW  - articulated robot legs
KW  - human-XRL quadruped system
KW  - rear legs
KW  - quadrupedal robots
KW  - quadrupedal locomotion
KW  - coupler design parameters
KW  - passive quadrupedal gait synchronization
KW  - dynamically coupled double rimless wheel system
KW  - Poincaré return map
KW  - numerical simulation
KW  - Legged locomotion
KW  - Wheels
KW  - Synchronization
KW  - Couplers
KW  - Robot kinematics
KW  - Human Augmentation
KW  - Supernumerary Robotic Limbs
KW  - Exoskeletons
KW  - Locomotion
KW  - Nonlinear Dynamics
DO  - 10.1109/ICRA40945.2020.9196773
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The Extra Robotic Legs (XRL) system is a robotic augmentation worn by a human operator consisting of two articulated robot legs that walk with the operator and help bear a heavy backpack payload. It is desirable for the Human-XRL quadruped system to walk with the rear legs lead the front by 25% of the gait period, minimizing the energy lost from foot impacts while maximizing balance stability. Unlike quadrupedal robots, the XRL cannot command the human's limbs to coordinate quadrupedal locomotion. Using a pair of Rimless Wheel models, it is shown that the systems coupled with a spring and damper converge to the desired 25% phase difference. A Poincaré return map was generated using numerical simulation to examine the convergence properties to different coupler design parameters, and initial conditions. The Dynamically Coupled Double Rimless Wheel system was physically realized with a spring and dashpot chosen from the theoretical results, and initial experiments indicate that the desired synchronization properties may be achieved within several steps using this set of passive components alone.
ER  - 

TY  - CONF
TI  - Optimal Fast Entrainment Waveform for Indirectly Controlled Limit Cycle Walker Against External Disturbances
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3458
EP  - 3463
AU  - L. Li
AU  - I. Tokuda
AU  - F. Asano
PY  - 2020
KW  - legged locomotion
KW  - limit cycles
KW  - numerical analysis
KW  - oscillators
KW  - robot dynamics
KW  - stability
KW  - optimal fast entrainment waveform
KW  - phase recovery
KW  - phase reduction theory
KW  - entrainment effect
KW  - limit cycle walking
KW  - indirectly controlled limit cycle walker
KW  - external disturbances
KW  - occasional perturbation
KW  - closed orbit
KW  - phase space
KW  - successive perturbation
KW  - accumulated deviation
KW  - control law
KW  - disturbed phase
KW  - wobbling mass
KW  - wobbling motion
KW  - Limit-cycles
KW  - Legged locomotion
KW  - Perturbation methods
KW  - Mathematical model
KW  - Oscillators
KW  - Convergence
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196525
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - After occasional perturbation, it is crucial to spontaneously control the limit cycle walking so that it quickly returns to its closed orbit in phase space. Otherwise, its stability can not be sufficiently guaranteed if the speed of recovery is slow while successive perturbation is applied. The accumulated deviation may eventually drive the phase outside the basin of attraction, leading to failure of the walking. In this sense, a control law that quickly recovers the disturbed phase before encountering the following perturbations is indispensable. With this consideration, here we analytically derive an optimal fast entrainment waveform that maximizes the speed of phase recovery based on phase reduction theory. Our theoretical method is numerically evaluated using a limit cycle walker, which is indirectly controlled by the oscillation of a wobbling mass via entrainment effect. The obtained waveform is used as the desired trajectory of the wobbling motion. The simulation results show that the waveform we derived achieves the best performance among all candidates. Our method helps to enhance the stability of limit cycle walking.
ER  - 

TY  - CONF
TI  - Correspondence Identification in Collaborative Robot Perception through Maximin Hypergraph Matching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3488
EP  - 3494
AU  - P. Gao
AU  - Z. Zhang
AU  - R. Guo
AU  - H. Lu
AU  - H. Zhang
PY  - 2020
KW  - concave programming
KW  - graph theory
KW  - image matching
KW  - image representation
KW  - multi-robot systems
KW  - object detection
KW  - robot vision
KW  - collaborative multirobot perception
KW  - nonconvex noncontinuous optimization problem
KW  - multirobot coordination
KW  - collaborative robot perception
KW  - hypergraph matching approach
KW  - Collaboration
KW  - Robot kinematics
KW  - Object recognition
KW  - Optimization
KW  - Robot sensing systems
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196594
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correspondence identification is an essential problem for collaborative multi-robot perception, with the objective of deciding the correspondence of objects that are observed in the field of view of each robot. In this paper, we introduce a novel maximin hypergraph matching approach that formulates correspondence identification as a hypergraph matching problem. The proposed approach incorporates both spatial relationships and appearance features of objects to improve representation capabilities. It also integrates the maximin theorem to optimize the worst-case scenario in order to address distractions caused by non-covisible objects. In addition, we design an optimization algorithm to address the formulated non-convex non-continuous optimization problem. We evaluate our approach and compare it with seven previous techniques in two application scenarios, including multi-robot coordination on real robots and connected autonomous driving in simulations. Experimental results have validated the effectiveness of our approach in identifying object correspondence from partially overlapped views in collaborative perception, and have shown that the proposed maximin hypergraph matching approach outperforms previous techniques and obtains state-of-the-art performance.
ER  - 

TY  - CONF
TI  - Distributed Multi-Target Tracking for Autonomous Vehicle Fleets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3495
EP  - 3501
AU  - O. Shorinwa
AU  - J. Yu
AU  - T. Halsted
AU  - A. Koufos
AU  - M. Schwager
PY  - 2020
KW  - cameras
KW  - Kalman filters
KW  - maximum likelihood estimation
KW  - target tracking
KW  - vehicular ad hoc networks
KW  - wireless sensor networks
KW  - Consensus Kalman Filter
KW  - fixed communication bandwidth
KW  - high fidelity urban driving simulator
KW  - autonomous cars
KW  - time-varying communication network
KW  - distributed multitarget tracking
KW  - autonomous vehicle fleets
KW  - scalable distributed target tracking algorithm
KW  - alternating direction method of multipliers
KW  - vehicle-to-vehicle network
KW  - sensing vehicle
KW  - Kalman filter-like update
KW  - centralized maximum a posteriori estimate
KW  - CARLA
KW  - on-board cameras
KW  - Sensors
KW  - Target tracking
KW  - Kalman filters
KW  - Microsoft Windows
KW  - Estimation
KW  - Trajectory
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197241
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a scalable distributed target tracking algorithm based on the alternating direction method of multipliers that is well-suited for a fleet of autonomous cars communicating over a vehicle-to-vehicle network. Each sensing vehicle communicates with its neighbors to execute iterations of a Kalman filter-like update such that each agent's estimate approximates the centralized maximum a posteriori estimate without requiring the communication of measurements. We show that our method outperforms the Consensus Kalman Filter in recovering the centralized estimate given a fixed communication bandwidth. We also demonstrate the algorithm in a high fidelity urban driving simulator (CARLA), in which 50 autonomous cars connected on a time-varying communication network track the positions and velocities of 50 target vehicles using on-board cameras.
ER  - 

TY  - CONF
TI  - Flying batteries: In-flight battery switching to increase multirotor flight time
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3510
EP  - 3516
AU  - K. P. Jain
AU  - M. W. Mueller
PY  - 2020
KW  - aerospace control
KW  - helicopters
KW  - mobile robots
KW  - secondary cells
KW  - in-flight battery switching
KW  - multirotor flight time
KW  - mid-air docking
KW  - primary battery
KW  - quadcopter flight
KW  - docking platform
KW  - flying battery
KW  - secondary battery
KW  - arbitrary switching
KW  - Batteries
KW  - Switches
KW  - Legged locomotion
KW  - Switching circuits
KW  - Aerodynamics
KW  - Connectors
KW  - Propellers
DO  - 10.1109/ICRA40945.2020.9197580
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel approach to increase the flight time of a multirotor via mid-air docking and in-flight battery switching. A main quadcopter flying using a primary battery has a docking platform attached to it. A `flying battery' - a small quadcopter carrying a secondary battery - is equipped with docking legs that can mate with the main quadcopter's platform. Connectors between the legs and the platform establish electrical contact on docking, and enable power transfer from the secondary battery to the main quadcopter. A custom-designed circuit allows arbitrary switching between the primary battery and secondary battery. We demonstrate the concept in a flight experiment involving repeated docking, battery switching, and undocking. This is shown in the video attachment. The experiment increases the flight time of the main quadcopter by a factor of 4.7× compared to solo flight, and 2.2× a theoretical limit for that given multirotor. Importantly, this increase in flight time is not associated with a large increase in overall vehicle mass or size, leaving the main quadcopter in fundamentally the same safety class.
ER  - 

TY  - CONF
TI  - Optimal Control of an Energy-Recycling Actuator for Mobile Robotics Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3559
EP  - 3565
AU  - E. Krimsky
AU  - S. H. Collins
PY  - 2020
KW  - actuators
KW  - clutches
KW  - electric motors
KW  - energy consumption
KW  - gears
KW  - integer programming
KW  - mobile robots
KW  - optimal control
KW  - power consumption
KW  - quadratic programming
KW  - springs (mechanical)
KW  - torque
KW  - actuator power consumption
KW  - mobile robot design
KW  - elastic energy
KW  - electrical energy consumption
KW  - optimal control
KW  - given actuator design
KW  - optimized actuator energy consumption
KW  - optimized gear motor
KW  - simulated energy-recycling actuator
KW  - mobile robotics applications
KW  - Springs
KW  - Actuators
KW  - Torque
KW  - Power demand
KW  - Force
KW  - Robots
KW  - Gears
KW  - Optimization and optimal control
KW  - force control
KW  - prosthetics and exoskeletons
DO  - 10.1109/ICRA40945.2020.9196870
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Actuator power consumption is a limiting factor in mobile robot design. In this paper we introduce the concept of an energy-recycling actuator, which uses an array of springs and clutches to capture and return elastic energy in parallel with an electric motor. Engaging and disengaging clutches appropriately could reduce electrical energy consumption without sacrificing controllability, but presents a challenging control problem. We formulated the optimal control objective of minimizing actuator power consumption as a mixed-integer quadratic program (MIQP) and solved for the global minimum. For a given actuator design and a wide range of simulated torque and rotation patterns, all corresponding to zero net work over one cycle, we compared optimized actuator energy consumption to that of an optimized gear motor with simple parallel elasticity. The simulated energy-recycling actuator consumed less electrical energy: 57% less on average and 80% less in the best case. These results demonstrate an effective approach to optimal control of this type of system, and suggest that energy-recycling actuators could substantially reduce power consumption in some robotics applications.
ER  - 

TY  - CONF
TI  - An NMPC Approach using Convex Inner Approximations for Online Motion Planning with Guaranteed Collision Avoidance
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3574
EP  - 3580
AU  - T. Schoels
AU  - L. Palmieri
AU  - K. O. Arras
AU  - M. Diehl
PY  - 2020
KW  - collision avoidance
KW  - continuous time systems
KW  - convex programming
KW  - mobile robots
KW  - nonlinear control systems
KW  - predictive control
KW  - trajectory control
KW  - trajectory optimization
KW  - NMPC approach
KW  - mobile robots
KW  - continuous time collision avoidance
KW  - kinodynamic feasibility
KW  - nonlinear model predictive control
KW  - convex inner approximation
KW  - online motion planning
KW  - continuous time collision free trajectories
KW  - Collision avoidance
KW  - Robots
KW  - Trajectory optimization
KW  - Planning
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197206
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Even though mobile robots have been around for decades, trajectory optimization and continuous time collision avoidance remain subject of active research. Existing methods trade off between path quality, computational complexity, and kinodynamic feasibility. This work approaches the problem using a nonlinear model predictive control (NMPC) framework, that is based on a novel convex inner approximation of the collision avoidance constraint. The proposed Convex Inner ApprOximation (CIAO) method finds kinodynamically feasible and continuous time collision free trajectories, in few iterations, typically one. For a feasible initialization, the approach is guaranteed to find a feasible solution, i.e. it preserves feasibility. Our experimental evaluation shows that CIAO outperforms state of the art baselines in terms of planning efficiency and path quality. Experiments show that it also efficiently scales to high-dimensional systems. Furthermore real-world experiments demonstrate its capability of unifying trajectory optimization and tracking for safe motion planning in dynamic environments.
ER  - 

TY  - CONF
TI  - Action Image Representation: Learning Scalable Deep Grasping Policies with Zero Real World Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3597
EP  - 3603
AU  - M. Khansari
AU  - D. Kappler
AU  - J. Luo
AU  - J. Bingham
AU  - M. Kalakrishnan
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - grippers
KW  - image colour analysis
KW  - image representation
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - vectors
KW  - Action image representation
KW  - zero real world data
KW  - end-to-end deep-grasping policy
KW  - grasp quality
KW  - object-gripper relationship
KW  - deep convolutional network
KW  - Action Image representation
KW  - color images
KW  - depth images
KW  - combined color-depth
KW  - scalable deep grasping policy learning
KW  - salient feature extraction
KW  - Grasping
KW  - Robot sensing systems
KW  - Image representation
KW  - Proposals
KW  - Grippers
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197415
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces Action Image, a new grasp proposal representation that allows learning an end-to-end deep-grasping policy. Our model achieves 84% grasp success on 172 real world objects while being trained only in simulation on 48 objects with just naive domain randomization. Similar to computer vision problems, such as object detection, Action Image builds on the idea that object features are invariant to translation in image space. Therefore, grasp quality is invariant when evaluating the object-gripper relationship; a successful grasp for an object depends on its local context, but is independent of the surrounding environment. Action Image represents a grasp proposal as an image and uses a deep convolutional network to infer grasp quality. We show that by using an Action Image representation, trained networks are able to extract local, salient features of grasping tasks that generalize across different objects and environments. We show that this representation works on a variety of inputs, including color images (RGB), depth images (D), and combined color-depth (RGB-D). Our experimental results demonstrate that networks utilizing an Action Image representation exhibit strong domain transfer between training on simulated data and inference on real-world sensor streams. Finally, our experiments show that a network trained with Action Image improves grasp success (84% vs. 53%) over a baseline model with the same structure, but using actions encoded as vectors.
ER  - 

TY  - CONF
TI  - High Accuracy and Efficiency Grasp Pose Detection Scheme with Dense Predictions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3604
EP  - 3610
AU  - H. Cheng
AU  - D. Ho
AU  - M. Q. . -H. Meng
PY  - 2020
KW  - grippers
KW  - image colour analysis
KW  - image resolution
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - pose estimation
KW  - robot vision
KW  - parallel plate gripper
KW  - neural network
KW  - pixel location
KW  - nonmaximum suppression
KW  - sampling grasps
KW  - learning-based grasp pose detection scheme
KW  - channels images
KW  - resolution RGB image
KW  - robot grasping success rate
KW  - Cornell Grasp Dataset
KW  - predicted grasps
KW  - dense predictions
KW  - dense grasp predictions
KW  - ranking procedures
KW  - clustering procedures
KW  - NMS
KW  - nonmaximum suppression strategy
KW  - detection model
KW  - generating grasp proposals
KW  - intermediate procedures
KW  - detection accuracy
KW  - fine-tuning steps
KW  - detection algorithms
KW  - Predictive models
KW  - Solid modeling
KW  - Grippers
KW  - Robots
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Image edge detection
DO  - 10.1109/ICRA40945.2020.9197333
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning-based grasp pose detection algorithms have boosted the performance of robot grasping, but they usually need manually fine-tuning steps to find the balance between detection accuracy and efficient. In this paper, we discard these intermediate procedures, like sampling grasps and generating grasp proposals, and propose an end-to-end grasp pose detection model. Our model uses the RGB image as the input and predicts the single grasp pose in each small grid of the image. Furthermore, the best grasps are found by non-maximum suppression (NMS) strategy. The clustering and ranking procedures are left for NMS while the network only generates dense grasp predictions, which keeps the network simple and efficient. To achieve dense predictions, the predicted grasps of our detection model are represented by the 6 channels images with each pixel location representing a rated grasp. To the best of our knowledge, our model is the first neural network that attaches a grasp pose in pixel level. The model achieves 96.5% accuracy which costs 14ms for prediction of a 480×360 resolution RGB image in Cornell Grasp Dataset, and 90.4% robot grasping success rate for unknown objects with a parallel plate gripper in the real environment.
ER  - 

TY  - CONF
TI  - Transferable Active Grasping and Real Embodied Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3611
EP  - 3618
AU  - X. Chen
AU  - Z. Ye
AU  - J. Sun
AU  - Y. Fan
AU  - F. Hu
AU  - C. Wang
AU  - C. Lu
PY  - 2020
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - robot vision
KW  - stereo image processing
KW  - cluttered scenes
KW  - robot vision systems
KW  - reinforcement learning framework
KW  - 3D vision architectures
KW  - RGB-D cameras
KW  - 3-stage transferable active grasping pipeline
KW  - real embodied dataset
KW  - RED
KW  - Grasping
KW  - Clutter
KW  - Cameras
KW  - Image segmentation
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197185
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Grasping in cluttered scenes is challenging for robot vision systems, as detection accuracy can be hindered by partial occlusion of objects. We adopt a reinforcement learning (RL) framework and 3D vision architectures to search for feasible viewpoints for grasping by the use of hand-mounted RGB-D cameras. To overcome the disadvantages of photo-realistic environment simulation, we propose a large-scale dataset called Real Embodied Dataset (RED), which includes full-viewpoint real samples on the upper hemisphere with amodal annotation and enables a simulator that has real visual feedback. Based on this dataset, a practical 3-stage transferable active grasping pipeline is developed, that is adaptive to unseen clutter scenes. In our pipeline, we propose a novel mask-guided reward to overcome the sparse reward issue in grasping and ensure category-irrelevant behavior. The grasping pipeline and its possible variants are evaluated with extensive experiments both in simulation and on a real-world UR-5 robotic arm.
ER  - 

TY  - CONF
TI  - PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation Algorithm from Sparse Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3619
EP  - 3625
AU  - P. Ni
AU  - W. Zhang
AU  - X. Zhu
AU  - Q. Cao
PY  - 2020
KW  - edge detection
KW  - feature extraction
KW  - image annotation
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - robot vision
KW  - PointNet++ grasping
KW  - sparse point clouds
KW  - robot manipulation
KW  - local feature extractor
KW  - deep learning
KW  - multiobject scene
KW  - multiobject dataset
KW  - grasp generation algorithm
KW  - multiobject grasp detection algorithm
KW  - Ferrari-Canny metrics
KW  - PointNet++ based network
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Grasping
KW  - Measurement
KW  - Training
KW  - Cameras
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9196740
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Grasping for novel objects is important for robot manipulation in unstructured environments. Most of current works require a grasp sampling process to obtain grasp candidates, combined with local feature extractor using deep learning. This pipeline is time-costly, expecially when grasp points are sparse such as at the edge of a bowl.In this paper, we propose an end-to-end approach to directly predict the poses, categories and scores (qualities) of all the grasps. It takes the whole sparse point clouds as the input and requires no sampling or search process. Moreover, to generate training data of multi-object scene, we propose a fast multi-object grasp detection algorithm based on Ferrari Canny metrics. A single-object dataset (79 objects from YCB object set, 23.7k grasps) and a multi-object dataset (20k point clouds with annotations and masks) are generated. A PointNet++ based network combined with multi-mask loss is introduced to deal with different training points. The whole weight size of our network is only about 11.6M, which takes about 102ms for a whole prediction process using a GeForce 840M GPU. Our experiment shows our work get 71.43% success rate and 91.60% completion rate, which performs better than current state-of-art works.
ER  - 

TY  - CONF
TI  - Clear Grasp: 3D Shape Estimation of Transparent Objects for Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3634
EP  - 3642
AU  - S. Sajjan
AU  - M. Moore
AU  - M. Pan
AU  - G. Nagaraja
AU  - J. Lee
AU  - A. Zeng
AU  - S. Song
PY  - 2020
KW  - control engineering computing
KW  - convolutional neural nets
KW  - image colour analysis
KW  - image reconstruction
KW  - image representation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - robot vision
KW  - stereo image processing
KW  - ClearGrasp
KW  - monocular depth estimation baselines
KW  - transparent objects
KW  - 3D shape estimation
KW  - 3D geometry
KW  - RGB-D image
KW  - transparent surfaces
KW  - occlusion boundaries
KW  - robotic manipulation
KW  - Three-dimensional displays
KW  - Geometry
KW  - Estimation
KW  - Solid modeling
KW  - Cameras
KW  - Image edge detection
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197518
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp - a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms' performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials: https://sites.google.com/view/cleargrasp.
ER  - 

TY  - CONF
TI  - 6D Object Pose Regression via Supervised Learning on Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3643
EP  - 3649
AU  - G. Gao
AU  - M. Lauri
AU  - Y. Wang
AU  - X. Hu
AU  - J. Zhang
AU  - S. Frintrop
PY  - 2020
KW  - convolutional neural nets
KW  - image colour analysis
KW  - image representation
KW  - object detection
KW  - pose estimation
KW  - regression analysis
KW  - supervised learning
KW  - convolutional neural networks
KW  - color information
KW  - translation regression
KW  - deep learning
KW  - rotation regression
KW  - 6D object pose regression
KW  - supervised learning
KW  - geometry-based pose refinement
KW  - axis-angle representation
KW  - geodesic loss function
KW  - quaternion representation
KW  - YCB-video dataset
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Feature extraction
KW  - Image color analysis
KW  - Supervised learning
KW  - Rotation measurement
KW  - Quaternions
DO  - 10.1109/ICRA40945.2020.9197461
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the task of estimating the 6 degrees of freedom pose of a known 3D object from depth information represented by a point cloud. Deep features learned by convolutional neural networks from color information have been the dominant features to be used for inferring object poses, while depth information receives much less attention. However, depth information contains rich geometric information of the object shape, which is important for inferring the object pose. We use depth information represented by point clouds as the input to both deep networks and geometry-based pose refinement and use separate networks for rotation and translation regression. We argue that the axis-angle representation is a suitable rotation representation for deep learning, and use a geodesic loss function for rotation regression. Ablation studies show that these design choices outperform alternatives such as the quaternion representation and L2 loss, or regressing translation and rotation with the same network. Our simple yet effective approach clearly outperforms state-of-the-art methods on the YCB-video dataset.
ER  - 

TY  - CONF
TI  - YCB-M: A Multi-Camera RGB-D Dataset for Object Recognition and 6DoF Pose Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3650
EP  - 3656
AU  - T. Grenzdörffer
AU  - M. Günther
AU  - J. Hertzberg
PY  - 2020
KW  - cameras
KW  - feature extraction
KW  - image colour analysis
KW  - image segmentation
KW  - object detection
KW  - object recognition
KW  - pose estimation
KW  - stereo image processing
KW  - 2D bounding boxes
KW  - 3D cameras
KW  - YCB-M
KW  - multicamera RGB-D dataset
KW  - estimation system
KW  - object recognition
KW  - 3D bounding boxes
KW  - ground truth 6DoF poses
KW  - YCB object
KW  - camera model
KW  - robust algorithms
KW  - estimation algorithms
KW  - 6DoF pose estimation
KW  - Cameras
KW  - Robot vision systems
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197426
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While a great variety of 3D cameras have been introduced in recent years, most publicly available datasets for object recognition and pose estimation focus on one single camera. In this work, we present a dataset of 32 scenes that have been captured by 7 different 3D cameras, totaling 49,294 frames. This allows evaluating the sensitivity of pose estimation algorithms to the specifics of the used camera and the development of more robust algorithms that are more independent of the camera model. Vice versa, our dataset enables researchers to perform a quantitative comparison of the data from several different cameras and depth sensing technologies and evaluate their algorithms before selecting a camera for their specific task. The scenes in our dataset contain 20 different objects from the common benchmark YCB object and model set [1], [2]. We provide full ground truth 6DoF poses for each object, per-pixel segmentation, 2D and 3D bounding boxes and a measure of the amount of occlusion of each object. We have also performed an initial evaluation of the cameras using our dataset on a state-of-the-art object recognition and pose estimation system [3].
ER  - 

TY  - CONF
TI  - Self-supervised 6D Object Pose Estimation for Robot Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3665
EP  - 3671
AU  - X. Deng
AU  - Y. Xiang
AU  - A. Mousavian
AU  - C. Eppner
AU  - T. Bretl
AU  - D. Fox
PY  - 2020
KW  - image segmentation
KW  - intelligent robots
KW  - manipulators
KW  - pose estimation
KW  - robot vision
KW  - supervised learning
KW  - robot manipulation
KW  - self-supervised 6D object pose estimation
KW  - self-supervised learning
KW  - object configuration
KW  - pose estimation modules
KW  - object segmentation
KW  - 6D pose estimation performance
KW  - robots skill teaching
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Cameras
KW  - Training
KW  - Manipulators
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196714
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To teach robots skills, it is crucial to obtain data with supervision. Since annotating real world data is time-consuming and expensive, enabling robots to learn in a self- supervised way is important. In this work, we introduce a robot system for self-supervised 6D object pose estimation. Starting from modules trained in simulation, our system is able to label real world images with accurate 6D object poses for self-supervised learning. In addition, the robot interacts with objects in the environment to change the object configuration by grasping or pushing objects. In this way, our system is able to continuously collect data and improve its pose estimation modules. We show that the self-supervised learning improves object segmentation and 6D pose estimation performance, and consequently enables the system to grasp objects more reliably. A video showing the experiments can be found at https://youtu.be/W1Y0Mmh1Gd8.
ER  - 

TY  - CONF
TI  - Low-cost GelSight with UV Markings: Feature Extraction of Objects Using AlexNet and Optical Flow without 3D Image Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3680
EP  - 3685
AU  - A. C. Abad
AU  - A. Ranasinghe
PY  - 2020
KW  - cameras
KW  - convolutional neural nets
KW  - feature extraction
KW  - image classification
KW  - image recognition
KW  - image reconstruction
KW  - image sequences
KW  - object recognition
KW  - UV markings
KW  - UV ink markers
KW  - low-cost GelSight sensor
KW  - nonGelsight captured images
KW  - optical flow algorithm
KW  - feature extraction
KW  - convolutional neural networks
KW  - CNN
KW  - 2D image conversion
KW  - feature recognition
KW  - Prototypes
KW  - Light emitting diodes
KW  - Feature extraction
KW  - Image recognition
KW  - Three-dimensional displays
KW  - Lighting
KW  - Coatings
DO  - 10.1109/ICRA40945.2020.9197264
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - GelSight sensor has been used to study microgeometry of objects since 2009 in tactile sensing applications. Elastomer, reflective coating, lighting, and camera were the main challenges of making a GelSight sensor within a short period. The recent addition of permanent markers to the GelSight was a new era in shear/slip studies. In our previous studies, we introduced Ultraviolet (UV) ink and UV LEDs as a new form of marker and lighting respectively. UV ink markers are invisible using ordinary LED but can be made visible using UV LED. Currently, recognition of objects or surface textures using GelSight sensor is done using fusion of camera-only images and GelSight captured images with permanent markings. Those images are fed to Convolutional Neural Networks (CNN) to classify objects. However, our novel approach in using low-cost GelSight sensor with UV markings, the 3D height map to 2D image conversion, and the additional non-Gelsight captured images for training the CNN can be eliminated. AlexNet and optical flow algorithm have been used for feature recognition of five coins without UV markings and shear/slip of the coin in GelSight with UV markings respectively. Our results on confusion matrix show that, on average coin recognition can reach 93.4% without UV markings using AlexNet. Therefore, our novel method of using GelSight with UV markings would be useful to recognize full/partial object, shear/slip, and force applied to the objects without any 3D image reconstruction.
ER  - 

TY  - CONF
TI  - Evaluation of Non-collocated Force Feedback Driven by Signal-independent Noise
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3686
EP  - 3692
AU  - Z. Chua
AU  - A. M. Okamura
AU  - D. R. Deo
PY  - 2020
KW  - brain-computer interfaces
KW  - feedback
KW  - force feedback
KW  - haptic interfaces
KW  - medical computing
KW  - neurophysiology
KW  - prosthetics
KW  - exploratory action
KW  - conventional haptic interface
KW  - iBCI-based prostheses control strategies
KW  - neural prostheses
KW  - intracortical brain computer interface
KW  - input signal
KW  - robotic prostheses
KW  - paralysis
KW  - signal-independent noise
KW  - noncollocated force feedback
KW  - signal-to-noise ratio
KW  - virtual environment
KW  - noncollocated haptic feedback
KW  - Force
KW  - Haptic interfaces
KW  - Task analysis
KW  - Noise measurement
KW  - Virtual environments
KW  - Signal to noise ratio
KW  - Probes
DO  - 10.1109/ICRA40945.2020.9197112
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Individuals living with paralysis or amputation can operate robotic prostheses using input signals based on their intent or attempt to move. Because sensory function is lost or diminished in these individuals, haptic feedback must be non-collocated. The intracortical brain computer interface (iBCI) has enabled a variety of neural prostheses for people with paralysis. An important attribute of the iBCI is that its input signal contains signal-independent noise. To understand the effects of signal-independent noise on a system with non-collocated haptic feedback and inform iBCI-based prostheses control strategies, we conducted an experiment with a conventional haptic interface as a proxy for the iBCI. Ablebodied users were tasked with locating an indentation within a virtual environment using input from their right hand. Non-collocated haptic feedback of the interaction forces in the virtual environment was augmented with noise of three different magnitudes and simultaneously rendered on users' left hands. We found increases in distance error of the guess of the indentation location, mean time per trial, mean peak absolute displacement and speed of tool movements during localization for the highest noise level compared to the other two levels. The findings suggest that users have a threshold of disturbance rejection and that they attempt to increase their signal-to-noise ratio through their exploratory actions.
ER  - 

TY  - CONF
TI  - Tactile sensing based on fingertip suction flow for submerged dexterous manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3701
EP  - 3707
AU  - P. Nadeau
AU  - M. Abbott
AU  - D. Melville
AU  - H. S. Stuart
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - motion control
KW  - neurocontrollers
KW  - recurrent neural nets
KW  - tactile sensors
KW  - saltwater corrosion
KW  - low-light conditions
KW  - robust electrical parts
KW  - mechanical parts
KW  - underwater robots
KW  - mobile manipulation tasks
KW  - suction flow mechanism
KW  - orifice occlusion
KW  - ambient pressure
KW  - tactile sensing modality
KW  - automated robotic behaviors
KW  - fingertip suction flow
KW  - submerged dexterous manipulation
KW  - robotic systems
KW  - Electron tubes
KW  - Sea measurements
KW  - Tactile sensors
KW  - Oceans
DO  - 10.1109/ICRA40945.2020.9197582
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ocean is a harsh and unstructured environment for robotic systems; high ambient pressures, saltwater corrosion and low-light conditions demand machines with robust electrical and mechanical parts that are able to sense and respond to the environment. Prior work shows that the addition of gentle suction flow to the hands of underwater robots can aid in the handling of objects during mobile manipulation tasks. The current paper explores using this suction flow mechanism as a new modality for tactile sensing; by monitoring orifice occlusion we can get a sense of how objects make contact in the hand. The electronics required for this sensor can be located remotely from the hand and the signal is insensitive to large changes in ambient pressure associated with diving depth. In this study, suction is applied to the fingertips of a two-fingered compliant gripper and suction-based tactile sensing is monitored while an object is pulled out of a pinch grasp. As a proof of concept, a recurrent neural network model was trained to predict external force trends using only the suction signals. This tactile sensing modality holds the potential to enable automated robotic behaviors or to provide operators of remotely operated vehicles with additional feedback in a robust fashion suitable for ocean deployment.
ER  - 

TY  - CONF
TI  - Highly Robust Visual Place Recognition Through Spatial Matching of CNN Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3748
EP  - 3755
AU  - L. G. Camara
AU  - C. Gäbert
AU  - L. Přeučil
PY  - 2020
KW  - convolutional neural nets
KW  - image coding
KW  - image matching
KW  - image resolution
KW  - visual databases
KW  - image resolution
KW  - VGG16 CNN architecture
KW  - matching CNN features
KW  - query image
KW  - VGG16 Convolutional Neural Network architecture
KW  - Spatial Matching Visual Place Recognition
KW  - SSM-VPR
KW  - optimal image resolutions
KW  - Visualization
KW  - Robustness
KW  - Semantics
KW  - Task analysis
KW  - Histograms
KW  - Correlation
KW  - Simultaneous localization and mapping
KW  - Visual Place Recognition
KW  - Convolutional Neural Networks
KW  - SLAM
KW  - Loop Closure
KW  - Life-long Navigation
DO  - 10.1109/ICRA40945.2020.9196967
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We revise, improve and extend the system previously introduced by us and named SSM-VPR (Semantic and Spatial Matching Visual Place Recognition), largely boosting its performance above the current state of the art. The system encodes images of places by employing the activations of different layers of a pre-trained, off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. It consists of two stages: given a query image of a place, (1) a list of candidates is selected from a database of places and (2) the candidates are geometrically compared with the query. The comparison is made by matching CNN features and, equally important, their spatial locations, selecting the best candidate as the recognized place. The performance of the system is maximized by finding optimal image resolutions during the second stage and by exploiting temporal correlation between consecutive frames in the employed datasets.
ER  - 

TY  - CONF
TI  - Online Trajectory Planning Through Combined Trajectory Optimization and Function Approximation: Application to the Exoskeleton Atalante
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3756
EP  - 3762
AU  - A. Duburcq
AU  - Y. Chevaleyre
AU  - N. Bredeche
AU  - G. Boéris
PY  - 2020
KW  - control engineering computing
KW  - function approximation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robust control
KW  - trajectory control
KW  - wearable robots
KW  - trajectory optimization
KW  - function approximation
KW  - autonomous robots
KW  - online trajectory planning
KW  - guided trajectory learning
KW  - self-balanced exoskeleton
KW  - Atalante
KW  - robust control strategies
KW  - Trajectory optimization
KW  - Function approximation
KW  - Task analysis
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196633
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous robots require online trajectory planning capability to operate in the real world. Efficient offline trajectory planning methods already exist, but are computationally demanding, preventing their use online. In this paper, we present a novel algorithm called Guided Trajectory Learning that learns a function approximation of solutions computed through trajectory optimization while ensuring accurate and reliable predictions. This function approximation is then used online to generate trajectories. This algorithm is designed to be easy to implement, and practical since it does not require massive computing power. It is readily applicable to any robotics systems and effortless to set up on real hardware since robust control strategies are usually already available. We demonstrate the computational performance of our algorithm on flat-foot walking with the self-balanced exoskeleton Atalante.
ER  - 

TY  - CONF
TI  - Act, Perceive, and Plan in Belief Space for Robot Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3763
EP  - 3769
AU  - M. Colledanchise
AU  - D. Malafronte
AU  - L. Natale
PY  - 2020
KW  - mobile robots
KW  - object recognition
KW  - path planning
KW  - belief space
KW  - robot localization
KW  - interleaved acting
KW  - planning technique
KW  - low-level geometric features
KW  - task planner
KW  - perception tasks
KW  - state spaces
KW  - Uncertainty
KW  - Planning
KW  - Task analysis
KW  - Object detection
KW  - Robot sensing systems
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9197097
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we outline an interleaved acting and planning technique to rapidly reduce the uncertainty of the estimated robot's pose by perceiving relevant information from the environment, as recognizing an object or asking someone for a direction. Generally, existing localization approaches rely on low-level geometric features such as points, lines, and planes. While these approaches provide the desired accuracy, they may require time to converge, especially with incorrect initial guesses. In our approach, a task planner computes a sequence of action and perception tasks to actively obtain relevant information from the robot's perception system. We validate our approach in large state spaces, to show how the approach scales, and in real environments, to show the applicability of our method on real robots. We prove that our approach is sound, probabilistically complete, and tractable in practical cases.
ER  - 

TY  - CONF
TI  - Decentralized Task Allocation in Multi-Agent Systems Using a Decentralized Genetic Algorithm
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3770
EP  - 3776
AU  - R. Patel
AU  - E. Rudnick-Cohen
AU  - S. Azarm
AU  - M. Otte
AU  - H. Xu
AU  - J. W. Herrmann
PY  - 2020
KW  - genetic algorithms
KW  - multi-agent systems
KW  - min-time objective
KW  - decentralized GA approach
KW  - task execution
KW  - multiagent collaborative search missions
KW  - decentralized genetic algorithm
KW  - multiagent systems
KW  - decentralized task allocation problem
KW  - decentralized evolutionary approaches
KW  - min-time performance
KW  - min-sum objective
KW  - Task analysis
KW  - Resource management
KW  - Genetic algorithms
KW  - Sociology
KW  - Statistics
KW  - Cost function
KW  - Message systems
DO  - 10.1109/ICRA40945.2020.9197314
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In multi-agent collaborative search missions, task allocation is required to determine which agents will perform which tasks. We propose a new approach for decentralized task allocation based on a decentralized genetic algorithm (GA). The approach parallelizes a genetic algorithm across the team of agents, making efficient use of their computational resources. In the proposed approach, the agents continuously search for and share better solutions during task execution. We conducted simulation experiments to compare the decentralized GA approach and several existing approaches. Two objectives were considered: a min-sum objective (minimizing the total distance traveled by all agents) and a min-time objective (minimizing the time to visit all locations of interest). The results showed that the decentralized GA approach yielded task allocations that were better on the min-time objective than those created by existing approaches and solutions that were reasonable on the min-sum objective. The decentralized GA improved min-time performance by an average of 5.6% on the larger instances. The results indicate that decentralized evolutionary approaches have a strong potential for solving the decentralized task allocation problem.
ER  - 

TY  - CONF
TI  - Fast and resilient manipulation planning for target retrieval in clutter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3777
EP  - 3783
AU  - C. Nam
AU  - J. Lee
AU  - S. Hun Cheong
AU  - B. Y. Cho
AU  - C. Kim
PY  - 2020
KW  - collision avoidance
KW  - image retrieval
KW  - manipulators
KW  - mobile robots
KW  - object detection
KW  - resilient manipulation planning
KW  - clutter
KW  - task and motion planning
KW  - robotic manipulator
KW  - target object retrieval
KW  - collision-free path
KW  - object rearrangement
KW  - TAMP framework
KW  - static environments
KW  - pick-and-place actions
KW  - baseline methods
KW  - Planning
KW  - Task analysis
KW  - Clutter
KW  - Collision avoidance
KW  - Manipulators
KW  - Search problems
DO  - 10.1109/ICRA40945.2020.9196652
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a task and motion planning (TAMP) framework for a robotic manipulator in order to retrieve a target object from clutter. We consider a configuration of objects in a confined space with a high density so no collision-free path to the target exists. The robot must relocate some objects to retrieve the target without collisions. For fast completion of object rearrangement, the robot aims to optimize the number of pick-and-place actions which often determines the efficiency of a TAMP framework.We propose a task planner incorporating motion planning to generate executable plans which aims to minimize the number of pick-and-place actions. In addition to fully known and static environments, our method can deal with uncertain and dynamic situations incurred by occluded views. Our method is shown to reduce the number of pick-and-place actions compared to baseline methods (e.g., at least 28.0% of reduction in a known static environment with 20 objects).
ER  - 

TY  - CONF
TI  - Untethered Soft Millirobot with Magnetic Actuation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3792
EP  - 3798
AU  - A. Bhattacharjee
AU  - L. W. Rogowski
AU  - X. Zhang
AU  - M. J. Kim
PY  - 2020
KW  - bending
KW  - magnetic actuators
KW  - magnetic flux
KW  - microrobots
KW  - mobile robots
KW  - motion control
KW  - permanent magnets
KW  - polymers
KW  - robot kinematics
KW  - rods (structures)
KW  - untethered soft millirobot
KW  - magnetic actuation
KW  - scalable designs
KW  - moulding technique
KW  - acrylonitrile butadiene styrene filaments
KW  - embedded permanent magnets
KW  - soft-body
KW  - soft-robots
KW  - external uniform magnetic field control system
KW  - magnetic flux densities
KW  - soft-robotic body
KW  - magnetic field strength
KW  - motion modes
KW  - magnetic field inputs
KW  - hollow rod-like structures
KW  - polydimethylsiloxane
KW  - 3D printed polylactic acid rings
KW  - pivot walking
KW  - rolling motion
KW  - tumbling motion
KW  - wiggling motion
KW  - side-tapping motion
KW  - wavy motion
KW  - deflection curve
KW  - navigation
KW  - minimally invasive in vivo applications
KW  - bending angle
KW  - Robots
KW  - Permanent magnets
KW  - Magnetic resonance imaging
KW  - Programmable logic arrays
KW  - Fabrication
KW  - Plastics
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197202
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents scalable designs and fabrication, actuation, and manipulation techniques for soft millirobots under uniform magnetic field control. The millirobots were fabricated through an economic and robust moulding technique using polydimethylsiloxane (PDMS), acrylonitrile butadiene styrene (ABS) filaments, and 3D printed polylactic acid (PLA) rings. The soft millirobots were simple hollow rod-like structures with different configurations of embedded permanent magnets inside of their soft-body or at their ends. The soft-robots were actuated using six different motion modes including: pivot walking, rolling, tumbling, side-tapping, wiggling, and wavy-motion under an external uniform magnetic field control system. The velocities of the millirobots under different motion modes were analyzed under varying magnetic flux densities (B). Moreover, deformation of the soft-robotic body in response to the magnetic field strength was measured and a deflection curve showing bending angle (φ) was produced. Soft millirobots were navigated through a maze using a combination of the available motion modes. Different arrangements of the embedded permanent magnets enabled individual soft millirobots to respond heterogeneously under the same magnetic field inputs towards performing assembly and disassembly operation as modular subunits. Overall, this soft millirobot platform shows enormous potential for minimally invasive in vivo applications.
ER  - 

TY  - CONF
TI  - Accelerated Robot Learning via Human Brain Signals
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3799
EP  - 3805
AU  - I. Akinola
AU  - Z. Wang
AU  - J. Shi
AU  - X. He
AU  - P. Lapborisuth
AU  - J. Xu
AU  - D. Watkins-Valls
AU  - P. Sajda
AU  - P. Allen
PY  - 2020
KW  - brain
KW  - electroencephalography
KW  - feedback
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - medical signal processing
KW  - accelerated robot learning
KW  - human brain signals
KW  - reinforcement learning
KW  - RL algorithms struggle
KW  - learning signal
KW  - robot learning process
KW  - error-related signal
KW  - measurable using electroencephelography
KW  - robotic agents
KW  - sparse reward settings
KW  - human observer
KW  - robot attempts
KW  - noisy error feedback signal
KW  - supervised learning
KW  - RL learning process
KW  - robotic navigation task
KW  - Task analysis
KW  - Robots
KW  - Electroencephalography
KW  - Navigation
KW  - Hafnium
KW  - Learning (artificial intelligence)
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196566
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In reinforcement learning (RL), sparse rewards are a natural way to specify the task to be learned. However, most RL algorithms struggle to learn in this setting since the learning signal is mostly zeros. In contrast, humans are good at assessing and predicting the future consequences of actions and can serve as good reward/policy shapers to accelerate the robot learning process. Previous works have shown that the human brain generates an error-related signal, measurable using electroencephelography (EEG), when the human perceives the task being done erroneously. In this work, we propose a method that uses evaluative feedback obtained from human brain signals measured via scalp EEG to accelerate RL for robotic agents in sparse reward settings. As the robot learns the task, the EEG of a human observer watching the robot attempts is recorded and decoded into noisy error feedback signal. From this feedback, we use supervised learning to obtain a policy that subsequently augments the behavior policy and guides exploration in the early stages of RL. This bootstraps the RL learning process to enable learning from sparse reward. Using a simple robotic navigation task as a test bed, we show that our method achieves a stable obstacle-avoidance policy with high success rate, outperforming learning from sparse rewards only that struggles to achieve obstacle avoidance behavior or fails to advance to the goal.
ER  - 

TY  - CONF
TI  - Muscle and Brain Activations in Cylindrical Rotary Controller Manipulation with Index Finger and Thumb
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3806
EP  - 3811
AU  - R. Okatani
AU  - T. Tsumugiwa
AU  - R. Yokogawa
AU  - M. Narusue
AU  - H. Nishimura
AU  - Y. Takeda
AU  - T. Hara
PY  - 2020
KW  - biomechanics
KW  - brain
KW  - electromyography
KW  - medical signal processing
KW  - muscle
KW  - neurophysiology
KW  - position measurement
KW  - viscosity
KW  - cylindrical rotary controller manipulation
KW  - index finger
KW  - viscosity characteristics differences
KW  - rotational manipulation
KW  - brain activations
KW  - rotary manipulation
KW  - rotary motion
KW  - viscosity characteristics
KW  - brain activity
KW  - muscles activity
KW  - muscle activity
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196520
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study aim to confirm the effect of viscosity characteristics differences on the rotational manipulation of a cylindrical rotary controller with the index finger and thumb through a quantitative analysis and evaluation of muscle and brain activations. The target motion was a rotary manipulation with the index finger and thumb of a cylindrical rotary controller with a 50 mm diameter. The rotary motion of the controller produces a click sensation at every 12 degrees in the rotation. The experimental conditions were three conditions with different viscosity characteristics related to the rotary motion of the controller. The subjects were six right-handed healthy males with a mean age of 21.7 (S. D.: 1.03) years. We analyzed the brain activity from a near- infrared spectroscopy measurement system, the muscles activity using a surface myoelectric potential measurement device, the force data at the index finger and thumb tip using two independent six-axis force/torque sensors, and the position data using a 3D position measurement device. The experimental results showed that there was no significant difference in the questionnaire survey, muscle activity, and grasping force, respectively; however, a significant difference in brain activity was observed with increased controller viscosity. Therefore, it became clear that there was a change in the brain activity when rotating the cylindrical rotary controller with the viscosity characteristics related to the rotary motion.
ER  - 

TY  - CONF
TI  - Real-Time Robot Reach-To-Grasp Movements Control Via EOG and EMG Signals Decoding
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3812
EP  - 3817
AU  - B. Specht
AU  - Z. Tayeb
AU  - E. Dean
AU  - R. Soroushmojdehi
AU  - G. Cheng
PY  - 2020
KW  - biomechanics
KW  - brain-computer interfaces
KW  - electromyography
KW  - electro-oculography
KW  - grippers
KW  - human-robot interaction
KW  - man-machine systems
KW  - medical robotics
KW  - medical signal processing
KW  - neurophysiology
KW  - signal classification
KW  - time robot reach-to-grasp movement control
KW  - grasping task
KW  - industrial robot
KW  - eye movements
KW  - electromyography signals
KW  - real-time human-robot interface system
KW  - human-controlled assistive devices
KW  - dexterous devices
KW  - EOG signals
KW  - robot arms
KW  - real-time control
KW  - HRI systems
KW  - robot control
KW  - EMG signals
KW  - real-time decoding
KW  - EMG decoding
KW  - UR-10 robot arm
KW  - Electrooculography
KW  - Electromyography
KW  - Real-time systems
KW  - Decoding
KW  - Electrodes
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9197550
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a real-time human-robot interface (HRI) system, where Electrooculography (EOG) and Electromyography (EMG) signals were decoded to perform reach-to-grasp movements. For that, five different eye movements (up, down, left, right and rest) were classified in real-time and translated into commands to steer an industrial robot (UR-10) to one of the four approximate target directions. Thereafter, EMG signals were decoded to perform the grasping task using an attached gripper to the UR-10 robot arm. The proposed system was tested offline on three different healthy subjects, and mean validation accuracy of 93.62% and 99.50% were obtained across the three subjects for EOG and EMG decoding, respectively. Furthermore, the system was successfully tested in real-time with one subject, and mean online accuracy of 91.66% and 100% were achieved for EOG and EMG decoding, respectively. Our results obtained by combining real-time decoding of EOG and EMG signals for robot control show overall the potential of this approach to develop powerful and less complex HRI systems. Overall, this work provides a proof-of-concept for successful real-time control of robot arms using EMG and EOG signals, paving the way for the development of more dexterous and human-controlled assistive devices.
ER  - 

TY  - CONF
TI  - Simultaneous Estimations of Joint Angle and Torque in Interactions with Environments using EMG
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3818
EP  - 3824
AU  - D. Kim
AU  - K. Koh
AU  - G. Oppizzi
AU  - R. Baghi
AU  - L. -C. Lo
AU  - C. Zhang
AU  - L. -Q. Zhang
PY  - 2020
KW  - biomechanics
KW  - decoding
KW  - electromyography
KW  - medical signal processing
KW  - muscle
KW  - time series
KW  - decoding method
KW  - LSTM network
KW  - decoding approach
KW  - wrist joint
KW  - long-time span
KW  - time series
KW  - core processor
KW  - short-term memory network
KW  - electromyography
KW  - decoding technique
KW  - joint angle
KW  - learning EMG signals
KW  - Electromyography
KW  - Torque
KW  - Decoding
KW  - Wrist
KW  - Logic gates
KW  - Neural networks
KW  - Kinematics
KW  - Human-machine interaction
KW  - Electromyography (EMG)
KW  - Decoding
KW  - Machine learning
KW  - Prosthesis
DO  - 10.1109/ICRA40945.2020.9197441
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We develop a decoding technique that estimates both the position and torque of a joint of the limb in interaction with an environment based on activities of the agonist-antagonist pair of muscles using electromyography in real time. The long short-term memory (LSTM) network is employed as the core processor of the proposed technique that is capable of learning time series of a long-time span with varying time lags. A validation that is conducted on the wrist joint shows that the decoding approach provides an agreement of greater than 95% in kinetics (i.e. torque) estimation and an agreement of greater than 85% in kinematics (i.e. angle) estimation, between the actual and estimated variables, during interactions with an environment. Also demonstrated is the fact that the proposed decoding method inherits the strengths of the LSTM network in terms of the capability of learning EMG signals and the corresponding responses with time dependency.
ER  - 

TY  - CONF
TI  - High-Density Electromyography Based Control of Robotic Devices: On the Execution of Dexterous Manipulation Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3825
EP  - 3831
AU  - A. Dwivedi
AU  - J. Lara
AU  - L. K. Cheng
AU  - N. Paskaranandavadivel
AU  - M. Liarokapis
PY  - 2020
KW  - biomechanics
KW  - biomedical electrodes
KW  - dexterous manipulators
KW  - electromyography
KW  - manipulators
KW  - medical robotics
KW  - medical signal processing
KW  - patient rehabilitation
KW  - prosthetics
KW  - telerobotics
KW  - EMG signals
KW  - object motion decoding
KW  - muscle importances
KW  - muscle importance results
KW  - decoded motions
KW  - fingered robotic hand
KW  - robotic devices
KW  - dexterous manipulation tasks
KW  - electromyography-based interfaces
KW  - robotics studies
KW  - teleoperation
KW  - telemanipulation applications
KW  - EMG-based control
KW  - prosthetic rehabilitation devices
KW  - assistive rehabilitation devices
KW  - robotic rehabilitation devices
KW  - grasping tasks
KW  - learning scheme
KW  - High Density Electromyography sensors
KW  - in-hand manipulation motions
KW  - object space
KW  - myoelectric activations
KW  - human forearm
KW  - hand muscles
KW  - yaw motions
KW  - geometric center
KW  - myoelectric data
KW  - high-density electromyography-based control
KW  - HD-EMG electrode arrays
KW  - Muscles
KW  - Electromyography
KW  - Task analysis
KW  - Electrodes
KW  - Robots
KW  - Decoding
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196629
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Electromyography (EMG) based interfaces have been used in various robotics studies ranging from teleoperation and telemanipulation applications to the EMG based control of prosthetic, assistive, or robotic rehabilitation devices. But most of these studies have focused on the decoding of user's motion or on the control of the robotic devices in the execution of simple tasks (e.g., grasping tasks). In this work, we present a learning scheme that employs High Density Electromyography (HD-EMG) sensors to decode a set of dexterous, in-hand manipulation motions (in the object space) based on the myoelectric activations of human forearm and hand muscles. To do that, the subjects were asked to perform roll, pitch, and yaw motions manipulating two different cubes. The first cube was designed to have a center of mass coinciding with the geometric center of the cube, while for the second cube the center of mass was shifted 14 mm to the right (off-centered design). Regarding the acquisition of the myoelectric data, custom HD-EMG electrode arrays were designed and fabricated. Using these arrays, a total of 89 EMG signals were extracted. The object motion decoding was formulated as a regression problem using the Random Forests (RF) technique and the muscle importances were studied using the inherent feature variables importance calculation procedure of the RF. The muscle importance results show that different subjects use different strategies to execute the same motions on same object when the weight is off-centered. Finally, the decoded motions were used to control a five fingered robotic hand in a proof-of-concept application.
ER  - 

TY  - CONF
TI  - Perception-Action Coupling in Usage of Telepresence Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3846
EP  - 3852
AU  - A. Valiton
AU  - Z. Li
PY  - 2020
KW  - cameras
KW  - control system synthesis
KW  - manipulators
KW  - robot vision
KW  - telecontrol
KW  - telerobotics
KW  - visual feedback
KW  - teleoperation assistance design
KW  - telepresence camera selection
KW  - standalone cameras
KW  - wearable cameras
KW  - manipulation motions
KW  - active perception control
KW  - human motor system
KW  - natural perception-action coupling
KW  - autonomous camera selection
KW  - active perception motions
KW  - coordinated manipulation
KW  - telepresence tele-action robots
KW  - telepresence cameras
KW  - robot teleoperation
KW  - telepresence system
KW  - Cameras
KW  - Task analysis
KW  - Robot vision systems
KW  - Robot kinematics
KW  - Telepresence
KW  - Teleoperators
DO  - 10.1109/ICRA40945.2020.9197578
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Telepresence tele-action robots enable human workers to reliably perform difficult tasks in remote, cluttered, and human environments. However, the effort to control coordinated manipulation and active perception motions may exhaust and intimidate novice workers. We hypothesize that such cognitive efforts would be effectively reduced if the teleoperators are provided with autonomous camera selection and control aligned with the natural perception-action coupling of the human motor system. Thus, we conducted a user study to investigate the coordination of active perception control and manipulation motions performed with visual feedback from various wearable and standalone cameras in a telepresence scenario. Our study discovered rich information about telepresence camera selection to inform telepresence system configuration and possible teleoperation assistance design for reduced cognitive effort in robot teleoperation.
ER  - 

TY  - CONF
TI  - A technical framework for human-like motion generation with autonomous anthropomorphic redundant manipulators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3853
EP  - 3859
AU  - G. Averta
AU  - D. Caporale
AU  - C. D. Santina
AU  - A. Bicchi
AU  - M. Bianchi
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulator kinematics
KW  - mechanical variables control
KW  - medical robotics
KW  - mobile robots
KW  - motion control
KW  - redundant manipulators
KW  - trajectory control
KW  - technical framework
KW  - motion generation
KW  - autonomous anthropomorphic redundant manipulators
KW  - co-bots
KW  - industrial settings
KW  - people assistance
KW  - robot motions
KW  - classic solutions
KW  - anthropomorphic movement generation
KW  - optimization procedures
KW  - neuroscientific literature
KW  - learning methods
KW  - motion variability
KW  - high dimensional datasets
KW  - human upper limb principal motion modes
KW  - functional analysis
KW  - robot trajectory optimization
KW  - redundant anthropomorphic kinematic architectures
KW  - human model
KW  - functional mode extraction
KW  - human trajectories
KW  - robotic manipulator
KW  - advanced human-robot interaction
KW  - industrial co-botics
KW  - human assistance
KW  - Kinematics
KW  - Manipulators
KW  - Trajectory
KW  - Computer architecture
KW  - Cost function
DO  - 10.1109/ICRA40945.2020.9196937
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The need for users' safety and technology accept-ability has incredibly increased with the deployment of co-bots physically interacting with humans in industrial settings, and for people assistance. A well-studied approach to meet these requirements is to ensure human-like robot motions. Classic solutions for anthropomorphic movement generation usually rely on optimization procedures, which build upon hypotheses devised from neuroscientific literature, or capitalize on learning methods. However, these approaches come with limitations, e.g. limited motion variability or the need for high dimensional datasets. In this work, we present a technique to directly embed human upper limb principal motion modes computed through functional analysis in the robot trajectory optimization. We report on the implementation with manipulators with redundant anthropomorphic kinematic architectures - although dissimilar with respect to the human model used for functional mode extraction - via Cartesian impedance control. In our experiments, we show how human trajectories mapped onto a robotic manipulator still exhibit the main characteristics of human-likeness, e.g. low jerk values. We discuss the results with respect to the state of the art, and their implications for advanced human-robot interaction in industrial co-botics and for human assistance.
ER  - 

TY  - CONF
TI  - Real-Time Adaptive Assembly Scheduling in Human-Multi-Robot Collaboration According to Human Capability*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3860
EP  - 3866
AU  - S. Zhang
AU  - Y. Chen
AU  - J. Zhang
AU  - Y. Jia
PY  - 2020
KW  - assembling
KW  - genetic algorithms
KW  - multi-agent systems
KW  - multi-robot systems
KW  - robotic assembly
KW  - scheduling
KW  - human-multirobot collaboration
KW  - human capability
KW  - optimal assembly scheduling
KW  - robot adaptation
KW  - human-single-robot interaction
KW  - human-multirobot interaction
KW  - multiagent interactions
KW  - real-time adaptive assembly scheduling approach
KW  - formulated adaptive assembly scheduling problem
KW  - human-multirobot assembly tasks
KW  - Robots
KW  - Job shop scheduling
KW  - Task analysis
KW  - Real-time systems
KW  - Schedules
KW  - Adaptive scheduling
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9196618
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Human-multi-robot collaboration is becoming more and more common in intelligent manufacturing. Optimal assembly scheduling of such systems plays a critical role in their production efficiency. Existing approaches mostly consider humans as agents with assumed or known capabilities, which leads to suboptimal performance in realistic applications where human capabilities usually change. In addition, most robot adaptation focuses on human-single-robot interaction and the adaptation in human-multi-robot interaction with changing human capability still remains challenging due to the complexity of the heterogeneous multi-agent interactions. This paper proposes a real-time adaptive assembly scheduling approach for human-multi-robot collaboration by modeling and incorporating changing human capability. A genetic algorithm is also designed to derive implementable solutions for the formulated adaptive assembly scheduling problem. The proposed approaches are validated through different simulated human-multi-robot assembly tasks and the results demonstrate the effectiveness and advantages of the proposed approaches.
ER  - 

TY  - CONF
TI  - Microscope-Guided Autonomous Clear Corneal Incision
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3867
EP  - 3873
AU  - J. Xia
AU  - S. J. Bergunder
AU  - D. Lin
AU  - Y. Yan
AU  - S. Lin
AU  - M. Ali Nasseri
AU  - M. Zhou
AU  - H. Lin
AU  - K. Huang
PY  - 2020
KW  - eye
KW  - medical computing
KW  - medical robotics
KW  - ophthalmic lenses
KW  - surgery
KW  - ex-vivo porcine eyes
KW  - microscope-guided autonomous clear corneal incision
KW  - ophthalmic microscope system
KW  - multiaxes robot
KW  - self-sealing incision
KW  - autonomous robotic system
KW  - cataract surgery
KW  - Robots
KW  - Surgery
KW  - Feature extraction
KW  - Iris
KW  - Cameras
KW  - Mirrors
KW  - Cataracts
DO  - 10.1109/ICRA40945.2020.9196645
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Clear Corneal Incision, a challenging step in cataract surgery, and important to the overall quality of the surgery. New surgeons usually spend one full year trying to perfect their incision, but even after such rigorous training deficient incisions can still occur. This paper proposes an autonomous robotic system for this self-sealing incision. A conventional ophthalmic microscope system with a monocular camera is utilized to capture the surgical scene, ascertain the robot's position, and estimate depth information. Kinematics with a remote centre of motion (RCM) is designed for a multi-axes robot to perform the incision route. The experimental results on ex-vivo porcine eyes show the autonomous Clear Corneal Incision has a stricter three-plane structure than a surgeon-made incision, which is closer to the ideal incision.
ER  - 

TY  - CONF
TI  - Asynchronous and decoupled control of the position and the stiffness of a spatial RCM tensegrity mechanism for needle manipulation*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3882
EP  - 3888
AU  - J. R. J. Realpe
AU  - G. Aiche
AU  - S. Abdelaziz
AU  - P. Poignet
PY  - 2020
KW  - manipulator kinematics
KW  - mean square error methods
KW  - medical robotics
KW  - needles
KW  - position control
KW  - surgery
KW  - asynchronous control
KW  - decoupled control
KW  - spatial RCM tensegrity mechanism
KW  - needle manipulation
KW  - spatial remote center
KW  - double parallelogram system
KW  - percutaneous needle insertion
KW  - decoupled modulation
KW  - control methodology
KW  - position tracking
KW  - needle guide
KW  - RCM tensegrity mechanism
KW  - spatial remote center of motion
KW  - Needles
KW  - Kinematics
KW  - Mathematical model
KW  - Actuators
KW  - Modulation
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197507
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a 2-DOF spatial remote center of motion (RCM) tensegrity mechanism, based on a double parallelogram system, dedicated for percutaneous needle insertion. The originality of this mechanism is its ability to be reconfigured and its capacity to perform a decoupled modulation of its stiffness in an asynchronous way. To do so, an analytical stiffness model of the robot is established, and a control methodology is proposed. A prototype of the robot is developed and assessed experimentally. The position tracking is evaluated using a 6-DOF magnetic tracker sensor showing a root mean square error less than 0.8° in both directions of the needle guide.
ER  - 

TY  - CONF
TI  - Redundancy Resolution Integrated Model Predictive Control of CDPRs: Concept, Implementation and Experiments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3889
EP  - 3895
AU  - J. C. Santos
AU  - A. Chemori
AU  - M. Gouttefarde
PY  - 2020
KW  - cables (mechanical)
KW  - predictive control
KW  - robot kinematics
KW  - robust control
KW  - trajectory control
KW  - MPC scheme
KW  - fully-constrained cable-driven parallel robots
KW  - cable tension distribution
KW  - pick-and-place task
KW  - maximum tension
KW  - tracking error minimization
KW  - redundancy resolution integrated model predictive control
KW  - CDPRs
KW  - cable tension limits
KW  - robustness
KW  - payload mass
KW  - Power cables
KW  - Robots
KW  - Real-time systems
KW  - Kinematics
KW  - Payloads
KW  - Safety
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197271
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a Model Predictive Control (MPC) strategy for fully-constrained Cable-Driven Parallel Robots. The main advantage of the proposed scheme lies in its ability to explicitly handle cable tension limits. Indeed, the cable tension distribution is performed as an integral part of the main control architecture. This characteristic significantly improves the safety of the system. Experimental results demonstrate this advantage addressing a typical pick-and-place task with two different scenarios: nominal cable tension limits and reduced maximum tension. Satisfactory tracking errors were obtained in the first scenario. In the second scenario, the desired trajectory escapes from the workspace defined by the new set of tension limits. The proposed MPC scheme is able to minimize the tracking errors without violating the tension limits. Satisfying results were also obtained regarding robustness against uncertainties on the payload mass.
ER  - 

TY  - CONF
TI  - Mechanics for Tendon Actuated Multisection Continuum Arms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3896
EP  - 3902
AU  - P. S. Gonthina
AU  - M. B. Wooten
AU  - I. S. Godage
AU  - I. D. Walker
PY  - 2020
KW  - actuators
KW  - bending
KW  - biomechanics
KW  - inspection
KW  - medical robotics
KW  - robot kinematics
KW  - tendon actuated multisection continuum arms
KW  - bending deformations
KW  - high mechanical coupling
KW  - variable length-based kinematic models
KW  - continuum arm curve parameter kinematics
KW  - robot
KW  - Tendons
KW  - Robots
KW  - Computational modeling
KW  - Strain
KW  - Numerical models
KW  - Kinematics
KW  - Deformable models
DO  - 10.1109/ICRA40945.2020.9197006
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tendon actuated multisection continuum arms have high potential for inspection applications in highly constrained spaces. They generate motion by axial and bending deformations. However, because of the high mechanical coupling between continuum sections, variable length-based kinematic models produce poor results. A new mechanics model for tendon actuated multisection continuum arms is proposed in this paper. The model combines the continuum arm curve parameter kinematics and concentric tube kinematics to correctly account for the large axial and bending deformations observed in the robot. Also, the model is computationally efficient and utilizes tendon tensions as the joint space variables thus eliminating the actuator length related problems such as slack and backlash. A recursive generalization of the model is also presented. Despite the high coupling between continuum sections, numerical results show that the model can be used for generating correct forward and inverse kinematic results. The model is then tested on a thin and long multisection continuum arm. The results show that the model can be used to successfully model the deformation.
ER  - 

TY  - CONF
TI  - Trajectory Optimization for a Six-DOF Cable-Suspended Parallel Robot with Dynamic Motions Beyond the Static Workspace
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3903
EP  - 3908
AU  - S. Xiang
AU  - H. Gao
AU  - Z. Liu
AU  - C. Gosselin
PY  - 2020
KW  - cables (mechanical)
KW  - manipulator dynamics
KW  - optimisation
KW  - path planning
KW  - position control
KW  - trajectory control
KW  - cable-suspended parallel robot
KW  - static workspace
KW  - trajectory optimization formulation
KW  - dynamic trajectories
KW  - six-degree-of-freedom
KW  - low-dimensional dynamic models
KW  - narrow feasible state space
KW  - dynamic similarity
KW  - point-mass CSPR
KW  - feasible force polyhedra
KW  - transition trajectories
KW  - highly dynamic motions
KW  - periodic trajectories
KW  - Dynamics
KW  - Planning
KW  - Robots
KW  - Trajectory optimization
KW  - Chebyshev approximation
KW  - Dynamic trajectory planning
KW  - optimization and optimal control
KW  - cable-suspended parallel robots
DO  - 10.1109/ICRA40945.2020.9196803
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a trajectory optimization formulation for planning dynamic trajectories of a six-degree-of-freedom (six-DOF) cable-suspended parallel robot (CSPR) that extend beyond the static workspace. The optimization is guided by low-dimensional dynamic models to overcome the local minima and accelerate the exploration of the narrow feasible state space. The dynamic similarity between the six-DOF CSPR and the three-DOF point-mass CSPR is discussed with the analyses of their feasible force polyhedra. Finally, the transition trajectories of a three-DOF CSPR are used as the initial guess of the translational part of the six-DOF motion. With the proposed approach, highly dynamic motions for a six-DOF CSPR are efficiently generated with multiple oscillations. The feasibility is demonstrated by point-to-point and periodic trajectories in the physics simulation.
ER  - 

TY  - CONF
TI  - An Intelligent Spraying System with Deep Learning-based Semantic Segmentation of Fruit Trees in Orchards
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3923
EP  - 3929
AU  - J. Kim
AU  - J. Seol
AU  - S. Lee
AU  - S. -W. Hong
AU  - H. I. Son
PY  - 2020
KW  - agriculture
KW  - agrochemicals
KW  - cameras
KW  - crops
KW  - image capture
KW  - image classification
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - nozzles
KW  - sensor fusion
KW  - pear orchard
KW  - deep learning-based intelligent spraying system
KW  - fruit tree detection system
KW  - SegNet model
KW  - semantic segmentation structure
KW  - deep learning model
KW  - nozzle
KW  - data fusion
KW  - RGB-D camera
KW  - image capture
KW  - pesticides
KW  - environmental safety
KW  - image classification
KW  - Spraying
KW  - Cameras
KW  - Semantics
KW  - Image segmentation
KW  - Vegetation
KW  - Decoding
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9197556
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study proposes an intelligent spraying system with semantic segmentation of fruit trees in a pear orchard. A fruit tree detection system was developed using the SegNet model, a semantic segmentation structure. The system is trained with images categorized into five distinct classes. The learned deep learning model performed with an accuracy of 83.79%. Further, we fusion depth data from an RGB-D camera to prevent the tree in the background from being detected. To operate the nozzles, each image captured from the camera is separated lengthwise into quarters and mapped to the nozzles. Then, the nozzle was opened when the area of fruit trees in each zone exceeded 20%. Two types of field experiments were performed in a pear orchard to verify the effectiveness of our system. From the results obtained, we can confirm the satisfactory performance of our deep learning-based intelligent spraying system. It is expected that the introduction of this system to actual farms will signicantly reduce the amount of pesticide used and will make the work environment safer for farmers.
ER  - 

TY  - CONF
TI  - An Efficient Planning and Control Framework for Pruning Fruit Trees
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3930
EP  - 3936
AU  - A. You
AU  - F. Sukkar
AU  - R. Fitch
AU  - M. Karkee
AU  - J. R. Davidson
PY  - 2020
KW  - agricultural products
KW  - image colour analysis
KW  - industrial manipulators
KW  - intelligent robots
KW  - robot vision
KW  - motion planning
KW  - sequence cut points
KW  - robotic pruning
KW  - control framework
KW  - pruning fruit trees
KW  - dormant pruning
KW  - fresh market tree fruit production
KW  - industrial manipulator
KW  - eye-in-hand RGB-D camera configuration
KW  - pneumatic cutter
KW  - Planning
KW  - Cameras
KW  - Three-dimensional displays
KW  - Manipulators
KW  - Vegetation
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197551
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dormant pruning is a major cost component of fresh market tree fruit production, nearly equal in scale to harvesting the fruit. However, relatively little focus has been given to the problem of pruning trees autonomously. In this paper, we introduce a robotic system consisting of an industrial manipulator, an eye-in-hand RGB-D camera configuration, and a custom pneumatic cutter. The system is capable of planning and executing a sequence of cuts while making minimal assumptions about the environment. We leverage a novel planning framework designed for high-throughput operation which builds upon previous work to reduce motion planning time and sequence cut points intelligently. In end-to-end experiments with a set of ten different branch configurations, the system achieved a high success rate in plan execution and a 1.5x speedup in throughput versus a baseline planner, representing a significant step towards the goal of practical implementation of robotic pruning.
ER  - 

TY  - CONF
TI  - Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3937
EP  - 3943
AU  - A. Binch
AU  - G. P. Das
AU  - J. Pulido Fentanes
AU  - M. Hanheide
PY  - 2020
KW  - genetic algorithms
KW  - iterative methods
KW  - mobile robots
KW  - motion control
KW  - navigation
KW  - path planning
KW  - simulated environments
KW  - genetic algorithm
KW  - resulting parameter sets
KW  - substantial performance improvements
KW  - agricultural robots
KW  - context dependant iterative parameter optimisation
KW  - robust robot navigation
KW  - autonomous mobile robotics
KW  - motion control
KW  - path planning
KW  - robust performance
KW  - robot model
KW  - parameter tuning
KW  - underlying algorithm
KW  - substantial combinatorial challenge
KW  - extensive manual tuning
KW  - navigation actions
KW  - spatial context
KW  - navigation task
KW  - respective navigation algorithms
KW  - iterative optimisation
KW  - performance metrics
KW  - Navigation
KW  - Robots
KW  - Optimization
KW  - Tuning
KW  - Robustness
KW  - Genetic algorithms
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9196550
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a "black box", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.
ER  - 

TY  - CONF
TI  - Extending Riemmanian Motion Policies to a Class of Underactuated Wheeled-Inverted-Pendulum Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3967
EP  - 3973
AU  - B. Wingo
AU  - C. -A. Cheng
AU  - M. Murtaza
AU  - M. Zafar
AU  - S. Hutchinson
PY  - 2020
KW  - collision avoidance
KW  - humanoid robots
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - pendulums
KW  - robot dynamics
KW  - wheels
KW  - RMP formalism
KW  - underacutated systems
KW  - fully-actuated subsystem
KW  - residual dynamics
KW  - manipulation tasks
KW  - 7-DoF system
KW  - second-order motion policies
KW  - RMP-based approaches
KW  - operational space control
KW  - fully state-dependent
KW  - collision avoidance bahaviors
KW  - control input
KW  - Riemmanian motion policies
KW  - underactuated wheeled-inverted-pendulum humanoid robot
KW  - Task analysis
KW  - Manifolds
KW  - Manipulator dynamics
KW  - Aerospace electronics
KW  - Humanoid robots
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9196866
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Riemannian Motion Policies (RMPs) have recently been introduced as a way to specify second-order motion policies defined on robot task spaces. RMP-based approaches have the advantage of being more general than traditional approaches based on operational space control; for example, the generalized task inertia in an RMP can be fully state-dependent, which is particularly effective in designing collision avoidance bahaviors. But until now RMPs have been applied only to fully actuated systems, i.e. systems for which each degree of freedom (DoF) can be directly actuated by a control input. In this paper, we present a method that extends the RMP formalism to a class of underacutated systems whose dynamics are amenable to a decomposition into a fully-actuated subsystem and a residual dynamics. We show the efficacy of the approach by constructing a suitable decomposition for a Wheeled-Inverted-Pendulum (WIP) humanoid robot and applying our method to derive motion policies for combined locomotion and manipulation tasks. Simulation results are presented for a 7-DoF system with one degree of underactuation.
ER  - 

TY  - CONF
TI  - Augmenting Self-Stability: Height Control of a Bernoulli Ball via Bang-Bang Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3974
EP  - 3980
AU  - T. Howison
AU  - F. Giardina
AU  - F. Iida
PY  - 2020
KW  - asymptotic stability
KW  - bang-bang control
KW  - feedback
KW  - feedforward
KW  - motion control
KW  - self-adjusting systems
KW  - uncertain systems
KW  - augmented controller
KW  - uncontrolled system
KW  - mechanical self-stability
KW  - uncertain environments
KW  - unstructured environments
KW  - explicit state observation
KW  - control law
KW  - performance metrics
KW  - minimalistic approach
KW  - feedforward bang-bang control
KW  - self-stabilizing dynamics
KW  - height control
KW  - global asymptotic stability
KW  - self-stabilising systems
KW  - Bernoulli ball
KW  - motion control
KW  - Atmospheric modeling
KW  - Dynamics
KW  - Mathematical model
KW  - Force
KW  - Asymptotic stability
KW  - Bang-bang control
DO  - 10.1109/ICRA40945.2020.9197391
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mechanical self-stability is often useful for controlling systems in uncertain and unstructured environments because it can regulate processes without explicit state observation or feedback computation. However, the performance of such systems is often not optimised, which begs the question how their dynamics can be naturally augmented by a control law to improve performance metrics. We propose a minimalistic approach to controlling mechanically self-stabilising systems by utilising model-based, feedforward bang-bang control at a global level and self-stabilizing dynamics at a local level. We demonstrate the approach in the height control problem of a sphere hovering in a vertical air jet - the so-called Bernoulli Ball. After developing a model to study the system and theoretically proving global asymptotic stability, we present the augmented controller and show how to enhance performance measures and plan behaviour. Our physical experiments show that the proposed control approach has a reduced time-to-target compared to the uncontrolled system without loss of stability (ranging from a 2.4 to 4.4 fold improvement) and that we can plan sequences of target positions at will.
ER  - 

TY  - CONF
TI  - Singularity-Free Inverse Dynamics for Underactuated Systems with a Rotating Mass
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3981
EP  - 3987
AU  - S. A. Tafrishi
AU  - M. Svinin
AU  - M. Yamamoto
PY  - 2020
KW  - manipulator dynamics
KW  - matrix algebra
KW  - motion control
KW  - nonlinear control systems
KW  - path planning
KW  - position control
KW  - singularity-free inverse dynamics
KW  - underactuated system
KW  - rotating mass
KW  - motion control
KW  - configuration singularities
KW  - configuration space
KW  - inertial coupling
KW  - small-amplitude sine wave
KW  - nonlinear dynamics
KW  - rolling system
KW  - singularity regions
KW  - coupling singularities
KW  - rolling carrier
KW  - Mathematical model
KW  - Couplings
KW  - Integrated circuits
KW  - Trajectory
KW  - Kinematics
KW  - Robots
KW  - Tensile stress
DO  - 10.1109/ICRA40945.2020.9197306
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion control of underactuated systems through the inverse dynamics contains configuration singularities. These limitations in configuration space mainly stem from the inertial coupling that passive joints/bodies create. In this study, we present a model that is free from singularity while the trajectory of the rotating mass has a small-amplitude sine wave around its circle. First, we derive the modified non-linear dynamics for a rolling system. Also, the singularity regions for this underactuated system is demonstrated. Then, the wave parameters are designed under certain conditions to remove the coupling singularities. We obtain these conditions from the positive definiteness of the inertia matrix in the inverse dynamics. Finally, the simulation results are confirmed by using a prescribed Beta function on the specified states of the rolling carrier. Because our algebraic method is integrated into the non-linear dynamics, the proposed solution has a great potential to be extended to the Lagrangian mechanics with multiple degrees-of-freedom.
ER  - 

TY  - CONF
TI  - Robust capture of unknown objects with a highly under-actuated gripper
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3996
EP  - 4002
AU  - P. E. Glick
AU  - N. Van Crey
AU  - M. T. Tolley
AU  - D. Ruffatto
PY  - 2020
KW  - actuators
KW  - adhesion
KW  - adhesives
KW  - dexterous manipulators
KW  - friction
KW  - grippers
KW  - under-actuated gripper
KW  - robotic grippers
KW  - high-friction materials
KW  - scaling forces
KW  - adhesion-controlled friction
KW  - adhesion-based grippers
KW  - high-friction interfaces
KW  - robust capture
KW  - size 65.0 cm
KW  - Grippers
KW  - Friction
KW  - Couplings
KW  - Stability analysis
KW  - Force
KW  - Torque
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197100
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Capturing large objects of unknown shape and orientation remains a challenge for most robotic grippers. We present a highly under-actuated gripper well suited for this task. Prior work shows two primary limitations to these grippers: the grip force of each link tends to decrease as the number of links increases, and the stability of an under-actuated linkage depends on the configuration of the links so grippers with many links are unlikely to be stable for arbitrary surfaces. We address these concerns by implementing two complementary methods of stabilization: using high-friction materials and scaling forces into the surface. We show that gecko-inspired adhesives provide an adhesion-controlled friction that can stabilize the gripper and improve grasp performance without the need of large normal forces. The under-actuated linkages also conform around arbitrary shapes and provide capability beyond prior adhesion-based grippers. With these high-friction interfaces, we show highly under-actuated linkages successfully grasp in many configurations without strict stability. The gripper is capable of holding over 30 N and consists of two tendon driven linkages that are each 65 cm long. This type of gripper is well suited for tasks without a predefined target geometry or orientation such as satellite servicing.
ER  - 

TY  - CONF
TI  - SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4023
EP  - 4029
AU  - P. Cai
AU  - Y. Lee
AU  - Y. Luo
AU  - D. Hsu
PY  - 2020
KW  - control engineering computing
KW  - multi-agent systems
KW  - road traffic
KW  - road vehicles
KW  - telecommunication traffic
KW  - traffic engineering computing
KW  - SUMMIT
KW  - urban driving
KW  - massive mixed traffic
KW  - unregulated urban crowd
KW  - high-speed traffic participants
KW  - high-fidelity simulator
KW  - crowd-driving algorithms
KW  - open-source OpenStreetMap map database
KW  - multiagent motion prediction model
KW  - unregulated urban traffic
KW  - heterogeneous agents
KW  - autonomous driving simulation
KW  - realistic traffic behaviors
KW  - crowd-driving settings
KW  - Roads
KW  - Robot sensing systems
KW  - Context modeling
KW  - Planning
KW  - Automobiles
KW  - Geometry
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197228
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous driving in an unregulated urban crowd is an outstanding challenge, especially, in the presence of many aggressive, high-speed traffic participants. This paper presents SUMMIT, a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physics and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control and planning, and end-to-end learning. We provide a context-aware planner together with benchmark scenarios and show that SUMMIT generates complex, realistic traffic behaviors in challenging crowd-driving settings.
ER  - 

TY  - CONF
TI  - A Model-Based Reinforcement Learning and Correction Framework for Process Control of Robotic Wire Arc Additive Manufacturing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4030
EP  - 4036
AU  - A. G. Dharmawan
AU  - Y. Xiong
AU  - S. Foong
AU  - G. Song Soh
PY  - 2020
KW  - learning (artificial intelligence)
KW  - process control
KW  - rapid prototyping (industrial)
KW  - robotic welding
KW  - three-dimensional printing
KW  - welds
KW  - wires
KW  - integrated learning-correction framework
KW  - model-based reinforcement learning
KW  - process parameters
KW  - inter-layer geometric digression
KW  - process control
KW  - robot arm
KW  - 3D metallic objects
KW  - layer by layer fashion
KW  - multilayer multibead deposition control
KW  - robotic wire arc additive manufacturing
KW  - weld beads
KW  - error stacking
KW  - material wastage reduction
KW  - MLMB print
KW  - Training
KW  - Predictive models
KW  - Process control
KW  - Adaptation models
KW  - Printing
KW  - Robots
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197222
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic Wire Arc Additive Manufacturing (WAAM) utilizes a robot arm as a motion system to build 3D metallic objects by depositing weld beads one above the other in a layer by layer fashion. A key part of this approach is the process study and control of Multi-Layer Multi-Bead (MLMB) deposition, which is very sensitive to process parameters and prone to error stacking. Despite its importance, it has been receiving less attention than its single bead counterpart in literature, probably due to the higher experimental overhead and complexity of modeling. To address these challenges, this paper proposes an integrated learning-correction framework, adapted from Model-Based Reinforcement Learning, to iteratively learn the direct effect of process parameters on MLMB print while simultaneously correct for any inter-layer geometric digression such that the final output is still satisfactory. The advantage is that this learning architecture can be used in conjunction with actual parts printing (hence, in-situ study), thus minimizing the required training time and material wastage. The proposed learning framework is implemented on an actual robotic WAAM system and experimentally evaluated.
ER  - 

TY  - CONF
TI  - Toward Optimal FDM Toolpath Planning with Monte Carlo Tree Search
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4037
EP  - 4043
AU  - C. Yoo
AU  - S. Lensgraf
AU  - R. Fitch
AU  - L. M. Clemon
AU  - R. Mettu
PY  - 2020
KW  - machine tools
KW  - Monte Carlo methods
KW  - optimisation
KW  - planning
KW  - rapid prototyping (industrial)
KW  - search problems
KW  - tree searching
KW  - 3D printing slice
KW  - dependency graph
KW  - MCTS-based algorithm
KW  - local search
KW  - toolpath quality
KW  - Monte Carlo tree search
KW  - optimal FDM toolpath planning
KW  - Planning
KW  - Monte Carlo methods
KW  - Three-dimensional printing
KW  - Solid modeling
KW  - Clustering algorithms
KW  - Printers
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9196945
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The most widely used methods for toolpath planning in 3D printing slice the input model into successive 2D layers to construct the toolpath. Unfortunately the methods can incur a substantial amount of wasted motion (i.e., the extruder is moving while not printing). In recent years we have introduced a new paradigm that characterizes the space of feasible toolpaths using a dependency graph on the input model, along with several algorithms that optimize objective functions (wasted motion or print time). A natural question that arises is, under what circumstances can we efficiently compute an optimal toolpath? In this paper, we give an algorithm for computing fused deposition modeling (FDM) toolpaths that utilizes Monte Carlo Tree Search (MCTS), a powerful generalpurpose method for navigating large search spaces that is guaranteed to converge to the optimal solution. Under reasonable assumptions on printer geometry that allow us to compress the dependency graph, our MCTS-based algorithm converges to find the optimal toolpath. We validate our algorithm on a dataset of 75 models and examine the performance on MCTS against our previous best local search-based algorithm in terms of toolpath quality. We show that a relatively short time budget for MCTS yields results on par with local search, while a larger time budget yields a 15% improvement in quality over local search. Additionally, we examine the properties of the models and MCTS executions that lead to better or worse results.
ER  - 

TY  - CONF
TI  - Optimizing performance in automation through modular robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4044
EP  - 4050
AU  - S. B. Liu
AU  - M. Althoff
PY  - 2020
KW  - flexible manufacturing systems
KW  - industrial robots
KW  - mobile robots
KW  - robot dynamics
KW  - robot kinematics
KW  - modular robots
KW  - flexible manufacturing
KW  - module composition
KW  - cycle time
KW  - energy efficiency
KW  - kinematic constraints
KW  - dynamic constraints
KW  - industrial robots
KW  - randomly generated tasks
KW  - performance metrics
KW  - modular robot
KW  - proModular.1
KW  - obstacle constraints
KW  - Cartesian space
KW  - Kinematics
KW  - Service robots
KW  - Collision avoidance
KW  - Trajectory
KW  - Task analysis
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196590
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Flexible manufacturing and automation require robots that can be adapted to changing tasks. We propose to use modular robots that are customized from given modules for a specific task. This work presents an algorithm for proposing a module composition that is optimal with respect to performance metrics such as cycle time and energy efficiency, while considering kinematic, dynamic, and obstacle constraints. Tasks are defined as trajectories in Cartesian space, as a list of poses for the robot to reach as fast as possible, or as dexterity in a desired workspace. In a simulated comparison with commercially available industrial robots, we demonstrate the superiority of our approach in randomly generated tasks with respect to the chosen performance metrics. We use our modular robot proModular.1 for the comparison.
ER  - 


