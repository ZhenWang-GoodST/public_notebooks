total paper: 176
Title: Metrically-Scaled Monocular SLAM using Learned Scale Factors
Key Words: cameras  graph theory  mobile robots  neural nets  robot vision  SLAM (robots)  geometric SLAM factor graph  SLAM systems  relative geometry  learned depth estimation approaches  learned depth predictions  image space  network architecture  coarse images  GPU acceleration  learned metric data  unary scale factors  hardware accelerators  observable epipolar geometry  monocular SLAM  learned scale factors  monocular simultaneous localization and mapping  hardware acceleration  neural network  Simultaneous localization and mapping  Feature extraction  Cameras  Loss measurement  Neural networks  Estimation 
Abstract: We propose an efficient method for monocular simultaneous localization and mapping (SLAM) that is capable of estimating metrically-scaled motion without additional sensors or hardware acceleration by integrating metric depth predictions from a neural network into a geometric SLAM factor graph. Unlike learned end-to-end SLAM systems, ours does not ignore the relative geometry directly observable in the images. Unlike existing learned depth estimation approaches, ours leverages the insight that when used to estimate scale, learned depth predictions need only be coarse in image space. This allows us to shrink our network to the point that performing inference on a standard CPU becomes computationally tractable.We make several improvements to our network architecture and training procedure to address the lack of depth observability when using coarse images, which allows us to estimate spatially coarse, but depth-accurate predictions in only 30 ms per frame without GPU acceleration. At runtime we incorporate the learned metric data as unary scale factors in a Sim(3) pose graph. Our method is able to generate accurate, scaled poses without additional sensors, hardware accelerators, or special maneuvers and does not ignore or corrupt the observable epipolar geometry. We show compelling results on the KITTI benchmark dataset in addition to real-world experiments with a handheld camera.


Title: Inertial-Only Optimization for Visual-Inertial Initialization
Key Words: feature extraction  least squares approximations  maximum likelihood estimation  optimisation  SLAM (robots)  EuRoC dataset show  time visual-inertial initialization  optimal estimation problem  maximum-a-posteriori estimation  algebraic equations  ad-hoc cost functions  ORB-SLAM visual-inertial boosting  inertial-only optimization  IMU measurement uncertainty  MAP estimation  least squares  Estimation  Trajectory  Simultaneous localization and mapping  Gravity  Visualization  Optimization  Accelerometers 
Abstract: We formulate for the first time visual-inertial initialization as an optimal estimation problem, in the sense of maximum-a-posteriori (MAP) estimation. This allows us to properly take into account IMU measurement uncertainty, which was neglected in previous methods that either solved sets of algebraic equations, or minimized ad-hoc cost functions using least squares. Our exhaustive initialization tests on EuRoC dataset show that our proposal largely outperforms the best methods in the literature, being able to initialize in less than 4 seconds in almost any point of the trajectory, with a scale error of 5.3% on average. This initialization has been integrated into ORB-SLAM Visual-Inertial boosting its robustness and efficiency while maintaining its excellent accuracy.


Title: Hierarchical Quadtree Feature Optical Flow Tracking Based Sparse Pose-Graph Visual-Inertial SLAM
Key Words: computational complexity  graph theory  image sequences  optimisation  pose estimation  quadtrees  pose-graph optimization time cost  localization accuracy  sparse pose-graph visual-inertial SLAM algorithms  hierarchical quadtree feature optical flow tracking algorithm  SPVIS  high-precision pose estimation  computational complexity  VIO-VI-SLAM system  GPU  Optical flow  Optimization  Simultaneous localization and mapping  Robustness  Tracking  Feature extraction  Visualization 
Abstract: Accurate, robust and real-time localization under constrained-resources is a critical problem to be solved. In this paper, we present a new sparse pose-graph visual-inertial SLAM (SPVIS). Unlike the existing methods that are costly to deal with a large number of redundant features and 3D map points, which are inefficient for improving positioning accuracy, we focus on the concise visual cues for high-precision pose estimating. We propose a novel hierarchical quadtree based optical flow tracking algorithm, it achieves high accuracy and robustness within very few concise features, which is only about one fifth features of the state-of-the-art visual-inertial SLAM algorithms. Benefiting from the efficient optical flow tracking, our sparse pose-graph optimization time cost achieves bounded complexity. By selecting and optimizing the informative features in sliding window and local VIO, the computational complexity is bounded, it achieves low time cost in long-term operation. We compare with the state-of-the-art VIO/VI-SLAM systems on the challenging public datasets by the embedded platform without GPUs, the results effectively verify that the proposed method has better real-time performance and localization accuracy.


Title: Keypoint Description by Descriptor Fusion Using Autoencoders
Key Words: convolutional neural nets  image fusion  image matching  learning (artificial intelligence)  robot vision  SLAM (robots)  keypoint description  keypoint matching  computer vision  visual simultaneous localization and mapping  SLAM  matching operation  descriptor fusion model  robust keypoint descriptor  CNN-based descriptors  DFM architecture  CNN models  mean mAP  HardNet  DenseNet169  convolutional neural networks  Fuses  Lighting  Robustness  Computer vision  Simultaneous localization and mapping  Image coding 
Abstract: Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45% and 6.53% higher than HardNet and DenseNet169, respectively.


Title: Towards Noise Resilient SLAM
Key Words: cameras  image colour analysis  image sensors  optimisation  photometry  pose estimation  SLAM (robots)  stereo image processing  RGB-D input  TUM datasets  EuRoC datasets  stereo image pairs  adaptive algorithm  error vector  outlier rejection  computational efficiency  map-point consensus  adaptive virtual camera  noise resilient SLAM  ORB-SLAM2  sparse-indirect SLAM systems  virtual camera location  axial depth error  pose optimization  consensus information  axial noise  lateral noise  depth noise components  axial components  lateral components  noise sources  scale information  SLAM frameworks  depth sensors  photometric invariance properties  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Measurement  Feature extraction  Optimization 
Abstract: Sparse-indirect SLAM systems have been dominantly popular due to their computational efficiency and photometric invariance properties. Depth sensors are critical to SLAM frameworks for providing scale information to the 3D world, yet known to be plagued by a wide variety of noise sources, possessing lateral and axial components. In this work, we demonstrate the detrimental impact of these depth noise components on the performance of the state-of-the-art sparse-indirect SLAM system (ORB-SLAM2). We propose (i) Map-Point Consensus based Outlier Rejection (MC-OR) to counter lateral noise, and (ii) Adaptive Virtual Camera (AVC) to combat axial noise accurately. MC-OR utilizes consensus information between multiple sightings of the same landmark to disambiguate noisy depth and filter it out before pose optimization. In AVC, we introduce an error vector as an accurate representation of the axial depth error. We additionally propose an adaptive algorithm to find the virtual camera location for projecting the error used in the objective function of the pose optimization. Our techniques work equally well for stereo image pairs and RGB-D input directly used by sparse-indirect SLAM systems. Our methods were tested on the TUM (RGB-D) and EuRoC (stereo) datasets and we show that they outperform existing state-of-the-art ORB-SLAM2 by 2-3x, especially in sequences critically affected by depth noise.


Title: LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of Perceptually-Degraded Subterranean Environments
Key Words: distance measurement  geophysical image processing  mobile robots  multi-robot systems  optical radar  robot vision  SLAM (robots)  stereo image processing  terrain mapping  tunnels  long corridors  salient features  spurious loop closures  repetitive appearance  stark contrast  highly-accurate 3D maps  underground extraterrestrial worlds  lidar-based multirobot SLAM system  DARPA subterranean challenge  subterranean operation  accurate lidar-based front-end  perceptually-degraded subterranean environments  complex subterranean environments  off-nominal conditions  uneven terrains  slippery terrains  large-scale autonomous mapping-positioning  simultaneous localization and mapping  unknown subterranean environment  large-scale subterranean environment  complex subterranean environment  inaccurate wheel odometry  disaster response  flexible back-end  robust back-end  tunnel circuit  Simultaneous localization and mapping  Laser radar  Three-dimensional displays  Base stations  Trajectory 
Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, unknown, and complex subterranean environments is a challenging problem. Sensors must operate in off-nominal conditions; uneven and slippery terrains make wheel odometry inaccurate, while long corridors without salient features make exteroceptive sensing ambiguous and prone to drift; finally, spurious loop closures that are frequent in environments with repetitive appearance, such as tunnels and mines, could result in a significant distortion of the entire map. These challenges are in stark contrast with the need to build highly-accurate 3D maps to support a wide variety of applications, ranging from disaster response to the exploration of underground extraterrestrial worlds. This paper reports on the implementation and testing of a lidar-based multi-robot SLAM system developed in the context of the DARPA Subterranean Challenge. We present a system architecture to enhance subterranean operation, including an accurate lidar-based front-end, and a flexible and robust back-end that automatically rejects outlying loop closures. We present an extensive evaluation in large-scale, challenging subterranean environments, including the results obtained in the Tunnel Circuit of the DARPA Subterranean Challenge. Finally, we discuss potential improvements, limitations of the state of the art, and future research directions.


Title: Voxel-based General Voronoi Diagram for Complex Data with Application on Motion Planning
Key Words: assembly planning  CAD  computational geometry  data structures  graphics processing units  path planning  production engineering computing  rendering (computer graphics)  path planning  motion planning  assembly sequence planning  real-world CAD-scenarios  disassembly path  GVD  Voronoi voxel history  disassembly paths  general Voronoi diagram graph  hash table-based data structure  error-bounded wavefront propagation  error-bounded GPU render approach  voxel-based general Voronoi diagram  roadmap  representative vehicle data set  Octrees  Planning  Three-dimensional displays  Approximation algorithms  Task analysis  Runtime 
Abstract: One major challenge in Assembly Sequence Planning (ASP) for complex real-world CAD-scenarios is to find appropriate disassembly paths for all assembled parts. Such a path places demands on its length and clearance. In the past, it became apparent that planning the disassembly path based on the (approximate) General Voronoi Diagram (GVD) is a good approach to achieve these requirements. But for complex real-world data, every known solution for computing the GVD is either too slow or very memory consuming, even if only approximating the GVD.We present a new approach for computing the approximate GVD and demonstrate its practicability using a representative vehicle data set. We can calculate an approximation of the GVD within minutes and meet the accuracy requirement of some few millimeters for the subsequent path planning. This is achieved by voxelizing the surface with a common error-bounded GPU render approach. We then use an error-bounded wavefront propagation technique and combine it with a novel hash table-based data structure, the so-called Voronoi Voxel History (VVH). On top of the GVD, we present a novel approach for the creation of a General Voronoi Diagram Graph (GVDG) that leads to an extensive roadmap. For the later motion planning task this roadmap can be used to suggest appropriate disassembly paths.


Title: Keyfilter-Aware Real-Time UAV Object Tracking
Key Words: autonomous aerial vehicles  image filtering  image motion analysis  learning (artificial intelligence)  mobile robots  object detection  object tracking  robot vision  SLAM (robots)  keyframe-based simultaneous localization and mapping  keyfilter restriction  visual tracking  background distraction  filter corruption  boundary effect  unmanned aerial vehicle  correlation filter-based tracking  keyfilter-aware real-time UAV object tracking  Unmanned aerial vehicles  Correlation  Visualization  Object tracking  Frequency-domain analysis  Real-time systems 
Abstract: Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.


Title: Grounding Language to Landmarks in Arbitrary Outdoor Environments
Key Words: human-robot interaction  mobile robots  natural language processing  path planning  robot vision  natural language phrases  arbitrary outdoor environments  urban environments  natural language commands  robot control  semantic similarities  Natural languages  Semantics  Robots  Grounding  Training  Planning  Databases 
Abstract: Robots operating in outdoor, urban environments need the ability to follow complex natural language commands which refer to never-before-seen landmarks. Existing approaches to this problem are limited because they require training a language model for the landmarks of a particular environment before a robot can understand commands referring to those landmarks. To generalize to new environments outside of the training set, we present a framework that parses references to landmarks, then assesses semantic similarities between the referring expression and landmarks in a predefined semantic map of the world, and ultimately translates natural language commands to motion plans for a drone. This framework allows the robot to ground natural language phrases to landmarks in a map when both the referring expressions to landmarks and the landmarks themselves have not been seen during training. We test our framework with a 14-person user evaluation demonstrating an end-to-end accuracy of 76.19% in an unseen environment. Subjective measures show that users find our system to have high performance and low workload. These results demonstrate our approach enables untrained users to control a robot in large unseen outdoor environments with unconstrained natural language.


Title: Vehicle Localization Based on Visual Lane Marking and Topological Map Matching
Key Words: image filtering  image fusion  image matching  iterative methods  Kalman filters  nonlinear filters  road vehicles  traffic engineering computing  visual lane marking  topological map matching  autonomous vehicle navigation  driver assistance systems  online vehicle localization  distinct map matching algorithms  visual lane tracker  map matching algorithm  grid map  iterative closest point based lane level map matching  Roads  Iterative closest point algorithm  Global Positioning System  Dead reckoning  Sensors  Visualization  Cameras  Map Relative Localization  Topological Map Matching  Lane Level Matching  Autonomous Vehicles 
Abstract: Accurate and reliable localization is crucial to autonomous vehicle navigation and driver assistance systems. This paper presents a novel approach for online vehicle localization in a digital map. Two distinct map matching algorithms are proposed: i) Iterative Closest Point (ICP) based lane level map matching is performed with visual lane tracker and grid map ii) decision-rule based approach is used to perform topological map matching. Results of both the map matching algorithms are fused together with GPS and dead reckoning using Extended Kalman Filter to estimate vehicle's pose relative to the map. The proposed approach has been validated on real life conditions on an equipped vehicle. Detailed analysis of the experimental results show improved localization using the two aforementioned map matching algorithms.


Title: RISE: A Novel Indoor Visual Place Recogniser
Key Words: cameras  image retrieval  image sensors  learning (artificial intelligence)  neural nets  indoor visual place recogniser  deep learning  image retrieval  image similarity metric  3D laser sensor  calibrated spherical camera  deep neural network  geo-referenced images  data collection stage  3D laser measurements  spherical panoramas  indoor areas  observed pixels  image mapping  query image  RISE  indoor visual place recognition problem 
Abstract: This paper presents a new technique to solve the Indoor Visual Place Recognition problem from the Deep Learning perspective. It consists on an image retrieval approach supported by a novel image similarity metric. Our work uses a 3D laser sensor mounted on a backpack with a calibrated spherical camera i) to generate the data for training the deep neural network and ii) to build a database of geo-referenced images for an environment. The data collection stage is fully automatic and requires no user intervention for labelling. Thanks to the 3D laser measurements and the spherical panoramas, we can efficiently survey large indoor areas in a very short time. The underlying 3D data associated to the map allows us to define the similarity between two training images as the geometric overlap between the observed pixels. We exploit this similarity metric to effectively train a CNN that maps images into compact embeddings. The goal of the training is to ensure that the L2 distance between the embeddings associated to two images is small when they are observing the same place and large when they are observing different places. After the training, similarities between a query image and the geo-referenced images in the database are efficiently retrieved by performing a nearest neighbour search in the embeddings space.


Title: Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching
Key Words: cameras  distance measurement  gradient methods  image matching  image registration  image sensors  pose estimation  robot vision  SLAM (robots)  stereo image processing  visual SLAM systems  photometric consistency  gradient-based dissimilarity  camera pose estimation  map building  central ingredients  autonomous robots  photometric error  gradient orientation  magnitude-dependent scaling term  stereo estimation  visual odometry systems  direct image registration tasks  robust estimates  scene depth  camera trajectory  mapping capabilities  mobile robots  sensor data registration  Measurement  Robustness  Cameras  Estimation  Visual odometry  Simultaneous localization and mapping  Robot vision systems 
Abstract: Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicate that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.


Title: ICS: Incremental Constrained Smoothing for State Estimation
Key Words: matrix decomposition  mobile robots  optimisation  path planning  robot vision  SLAM (robots)  state estimation  ICS  primal-dual method  matrix factorizations  primal-dual methods  incremental factorization  matrix structure  incremental unconstrained optimization  robot state estimate  smoothing-based estimation methods  state estimation  incremental constrained smoothing  Optimization  Smoothing methods  Time measurement  Integrated circuits  Simultaneous localization and mapping 
Abstract: A robot operating in the world constantly receives information about its environment in the form of new measurements at every time step. Smoothing-based estimation methods seek to optimize for the most likely robot state estimate using all measurements up till the current time step. Existing methods solve for this smoothing objective efficiently by framing the problem as that of incremental unconstrained optimization. However, in many cases observed measurements and knowledge of the environment is better modeled as hard constraints derived from real-world physics or dynamics. A key challenge is that the new optimality conditions introduced by the hard constraints break the matrix structure needed for incremental factorization in these incremental optimization methods. Our key insight is that if we leverage primal-dual methods, we can recover a matrix structure amenable to incremental factorization. We propose a framework ICS that combines a primal-dual method like the Augmented Lagrangian with an incremental Gauss Newton approach that reuses previously computed matrix factorizations. We evaluate ICS on a set of simulated and real-world problems involving equality constraints like object contact and inequality constraints like collision avoidance.


Title: A fast and practical method of indoor localization for resource-constrained devices with limited sensing
Key Words: collision avoidance  indoor radio  mobile robots  navigation  probability  fast method  indoor localization  resource-constrained devices  limited sensing capabilities  wearable devices  sparse WiFi  image-based measurements  dense signal maps  conditional random fields  agent positions  known floor plan  sparse absolute position estimates  motion sequences  low-quality measurements  handheld devices  mobile devices  sensory data  Wireless fidelity  Position measurement  Computational modeling  Dead reckoning  Robot sensing systems  Buildings 
Abstract: We describe and experimentally demonstrate a practical method for indoor localization using measurements obtained from resource-constrained devices with limited sensing capabilities. We focus on handheld/mobile devices but the method can be useful for a variety of wearable devices. Our system works with sparse WiFi or image-based measurements, avoiding laborious site surveying for dense signal maps and runs in real-time. It uses Conditional Random Fields to infer the most probable sequence of agent positions from a known floor plan, dead reckoning and sparse absolute position estimates. Our solution leverages known topology of the environment by pre-computing allowed motion sequences of an agent, which are then used to constraint the motion inferred from the sensory data. The system is evaluated in a typical office building, demonstrating good accuracy and robustness to sparse, low-quality measurements.


Title: Sample-Efficient Robot Motion Learning using Gaussian Process Latent Variable Models
Key Words: Gaussian processes  learning systems  manipulator dynamics  motion control  search problems  sample-efficient robot motion learning  Gaussian process latent variable models  robotic manipulators  household environments  kinesthetic teaching  parametric function  movement primitive  mutual-information-weighted Gaussian process latent variable model  trial-and-error  trajectory production  task dynamics  MP parameter latent space  robot motion task  search space  surrogate model  PS algorithms  policy search reinforcement learning  Gaussian processes  Optimization  Trajectory  Task analysis  Robots  Mutual information  Kernel 
Abstract: Robotic manipulators are reaching a state where we could see them in household environments in the following decade. Nevertheless, such robots need to be easy to instruct by lay people. This is why kinesthetic teaching has become very popular in recent years, in which the robot is taught a motion that is encoded as a parametric function - usually a Movement Primitive (MP)-. This approach produces trajectories that are usually suboptimal, and the robot needs to be able to improve them through trial-and-error. Such optimization is often done with Policy Search (PS) reinforcement learning, using a given reward function. PS algorithms can be classified as model-free, where neither the environment nor the reward function are modelled, or model-based, which can use a surrogate model of the reward function and/or a model for the dynamics of the task. However, MPs can become very high-dimensional in terms of parameters, which constitute the search space, so their optimization often requires too many samples. In this paper, we assume we have a robot motion task characterized with an MP of which we cannot model the dynamics. We build a surrogate model for the reward function, that maps an MP parameter latent space (obtained through a Mutual-information-weighted Gaussian Process Latent Variable Model) into a reward. While we do not model the task dynamics, using mutual information to shrink the task space makes it more consistent with the reward and so the policy improvement is faster in terms of sample efficiency.


Title: Bi-Convex Approximation of Non-Holonomic Trajectory Optimization
Key Words: approximation theory  convex programming  minimisation  quadratic programming  robot kinematics  nonholonomic trajectory optimization  nonholonomic kinematics  nonlinearly maps control input  nonconvex  bi-convex cost  constraint functions  bi-convex part  nonholonomic behavior  nonlinear penalty  nonlinear costs  bi-convex structure  bi-convex approximation  autonomous cars  fixed-wing aerial vehicles  computational tractability  alternating minimization  sequential quadratic programming  interior-point methods  Trajectory optimization  Minimization  Computational modeling  Collision avoidance  Robots  Mathematical model 
Abstract: Autonomous cars and fixed-wing aerial vehicles have the so-called non-holonomic kinematics which non-linearly maps control input to states. As a result, trajectory optimization with such a motion model becomes highly non-linear and non-convex. In this paper, we improve the computational tractability of non-holonomic trajectory optimization by reformulating it in terms of a set of bi-convex cost and constraint functions along with a non-linear penalty. The bi-convex part acts as a relaxation for the non-holonomic trajectory optimization while the residual of the penalty dictates how well its output obeys the non-holonomic behavior. We adopt an alternating minimization approach for solving the reformulated problem and show that it naturally leads to the replacement of the challenging non-linear penalty with a globally valid convex surrogate. Along with the common cost functions modeling goal-reaching, trajectory smoothness, etc., the proposed optimizer can also accommodate a class of non-linear costs for modeling goal-sets, while retaining the bi-convex structure. We benchmark the proposed optimizer against off-the-shelf solvers implementing sequential quadratic programming and interior-point methods and show that it produces solutions with similar or better cost as the former while significantly outperforming the latter. Furthermore, as compared to both off-the-shelf solvers, the proposed optimizer achieves more than 20x reduction in computation time.


Title: OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems
Key Words: cameras  computerised instrumentation  distance measurement  image matching  image reconstruction  image sequences  neural nets  stereo image processing  omnidirectional localization  wide-baseline multicamera systems  dense mapping system  wide-baseline multiview stereo setup  ultrawide field-of-view fisheye cameras  stereo observations  light-weighted deep neural networks  loop closing module  efficient feature matching process  omnidirectional depth maps  truncated signed distance function volume  rig estimation  omnidirectional depth map estimation  VO  FOV  TSDF  OmniSLAM  Cameras  Three-dimensional displays  Estimation  Feature extraction  Visual odometry  Sensors  Trajectory 
Abstract: In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360Â° coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.


Title: Tightly-Coupled Single-Anchor Ultra-wideband-Aided Monocular Visual Odometry System
Key Words: distance measurement  feature extraction  graph theory  least squares approximations  optimisation  pose estimation  position measurement  robot vision  Levenberg-Marquardt nonlinear least squares optimization scheme  scale factor  visual features  pose-graph optimization scheme  landmark reprojection errors  visual drift  monocular visual feature observations  distance measurements  ultrawideband-aided monocular visual odometry system  single-anchor monocular visual odometry system  tightly-coupled odometry framework  anchor position estimation  robot operating system  Cameras  Distance measurement  Robot sensing systems  Visualization 
Abstract: In this work, we propose a tightly-coupled odometry framework, which combines monocular visual feature observations with distance measurements provided by a single ultra-wideband (UWB) anchor with an initial guess for its location. Firstly, the scale factor and the anchor position in the vision frame will be simultaneously estimated using a variant of Levenberg-Marquardt non-linear least squares optimization scheme. Once the scale factor is obtained, the map of visual features is updated with the new scale. Subsequent ranging errors in a sliding window are continuously monitored and the estimation procedure will be reinitialized to refine the estimates. Lastly, range measurements and anchor position estimates are fused when needed into a pose-graph optimization scheme to minimize both the landmark reprojection errors and ranging errors, thus reducing the visual drift and improving the system robustness. The proposed method is implemented in Robot Operating System (ROS) and can function in real-time. The performance is validated on both public datasets and real-life experiments and compared with state-of-the-art methods.


Title: Navigation in the Presence of Obstacles for an Agile Autonomous Underwater Vehicle
Key Words: autonomous underwater vehicles  collision avoidance  feature extraction  mobile robots  navigation  optimisation  robot vision  stereo image processing  fly-overs  AUV  cluttered space  navigation framework  sampling-based correction procedure  obstacles detection  real-time 3D autonomous navigation  agile autonomous underwater vehicle  Trajopt  3D path-optimization planning  visual features detection  Planning  Three-dimensional displays  Navigation  Optimization  Robots  Trajectory  Cameras 
Abstract: Navigation underwater traditionally is done by keeping a safe distance from obstacles, resulting in "fly-overs" of the area of interest. Movement of an autonomous underwater vehicle (AUV) through a cluttered space, such as a shipwreck or a decorated cave, is an extremely challenging problem that has not been addressed in the past. This paper proposes a novel navigation framework utilizing an enhanced version of Trajopt for fast 3D path-optimization planning for AUVs. A sampling-based correction procedure ensures that the planning is not constrained by local minima, enabling navigation through narrow spaces. Two different modalities are proposed: planning with a known map results in efficient trajectories through cluttered spaces; operating in an unknown environment utilizes the point cloud from the visual features detected to navigate efficiently while avoiding the detected obstacles. The proposed approach is rigorously tested, both on simulation and in-pool experiments, proven to be fast enough to enable safe real-time 3D autonomous navigation for an AUV.


Title: Test Your SLAM! The SubT-Tunnel dataset and metric for mapping
Key Words: mobile robots  public domain software  robot vision  SLAM (robots)  SLAM  open source tools  robotic mapping algorithms  DARPA Subterranean challenge  SubT-Tunnel dataset  subterranean mine rescue dataset  Simultaneous localization and mapping  Cameras  Measurement  Laser radar  Robot vision systems 
Abstract: This paper presents an approach and introduces new open-source tools that can be used to evaluate robotic mapping algorithms. Also described is an extensive subterranean mine rescue dataset based upon the DARPA Subterranean (SubT) challenge including professionally surveyed ground truth. Finally, some commonly available approaches are evaluated using this metric.


Title: Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships
Key Words: graph theory  image matching  mobile robots  robot vision  SLAM (robots)  robotics applications  simultaneously localization and mapping  spatial relationship similarities  spatial cues  visual cues  old landmarks  long-term environment changes  landmark information  integrate landmark appearances  worst-case graph matching  place recognition performance  long-term place recognition  worst appearance similarity  similar appearances  worst-case scenario  graph matching problem  visual appearances  angular spatial relationships  graph representation  Visualization  Simultaneous localization and mapping  Robustness  Strain  Image recognition  Tensile stress 
Abstract: Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.


Title: Linear RGB-D SLAM for Atlanta World
Key Words: cameras  image colour analysis  Kalman filters  mobile robots  object detection  object tracking  pose estimation  SLAM (robots)  Manhattan world assumption  orthogonal directions  Atlanta world  vertical direction  horizontal directions  SLAM techniques  Atlanta representation  Atlanta frame-aware linear SLAM framework  Atlanta structure  linear Kalman filter  linear RGB-D SLAM  simultaneous localization and mapping  tracking-by-detection scheme  scene structure  camera motion  planar map  synthetic datasets  real datasets  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Tracking  Kalman filters  Visualization  Robustness 
Abstract: We present a new linear method for RGB-D based simultaneous localization and mapping (SLAM). Compared to existing techniques relying on the Manhattan world assumption defined by three orthogonal directions, our approach is designed for the more general scenario of the Atlanta world. It consists of a vertical direction and a set of horizontal directions orthogonal to the vertical direction and thus can represent a wider range of scenes. Our approach leverages the structural regularity of the Atlanta world to decouple the non-linearity of camera pose estimations. This allows us separately to estimate the camera rotation and then the translation, which bypasses the inherent non-linearity of traditional SLAM techniques. To this end, we introduce a novel tracking-by-detection scheme to estimate the underlying scene structure by Atlanta representation. Thereby, we propose an Atlanta frame-aware linear SLAM framework which jointly estimates the camera motion and a planar map supporting the Atlanta structure through a linear Kalman filter. Evaluations on both synthetic and real datasets demonstrate that our approach provides favorable performance compared to existing state-of-the-art methods while extending their working range to the Atlanta world.


Title: Probabilistic Data Association via Mixture Models for Robust Semantic SLAM
Key Words: Gaussian processes  image sensors  mobile robots  object detection  probability  robot vision  SLAM (robots)  target tracking  probabilistic data association  mixture models  robust semantic SLAM  robotic systems  cameras  lidar  visual models  reliable navigation  semantic uncertainty inherent  geometric uncertainty inherent  object detection methods  data association ambiguity  nonlinear Gaussian formulation  data association variables  max-marginalization  standard Gaussian posterior assumptions  max-mixture-type model  multiple data association hypotheses  indoor navigation tasks  outdoor semantic navigation tasks  semantic SLAM approaches  simultaneous localization and mapping  noisy odometry  Semantics  Simultaneous localization and mapping  Robustness  Optimization  Object detection  Uncertainty 
Abstract: Modern robotic systems sense the environment geometrically, through sensors like cameras, lidar, and sonar, as well as semantically, often through visual models learned from data, such as object detectors. We aim to develop robots that can use all of these sources of information for reliable navigation, but each is corrupted by noise. Rather than assume that object detection will eventually achieve near perfect performance across the lifetime of a robot, in this work we represent and cope with the semantic and geometric uncertainty inherent in object detection methods. Specifically, we model data association ambiguity, which is typically non-Gaussian, in a way that is amenable to solution within the common nonlinear Gaussian formulation of simultaneous localization and mapping (SLAM). We do so by eliminating data association variables from the inference process through max-marginalization, preserving standard Gaussian posterior assumptions. The result is a max-mixture-type model that accounts for multiple data association hypotheses. We provide experimental results on indoor and outdoor semantic navigation tasks with noisy odometry and object detection and find that the ability of the proposed approach to represent multiple hypotheses, including the "null" hypothesis, gives substantial robustness advantages in comparison to alternative semantic SLAM approaches.


Title: Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy
Key Words: closed loop systems  Global Positioning System  mobile robots  navigation  robot vision  SLAM (robots)  stereo image processing  tracking  representative state-of-the-art visual-inertial SLAM systems  visual estimation module  stereo visual-inertial SLAM systems  open-loop analysis  closed-loop navigation tasks  accurate trajectory tracking  visualinertial SLAM systems  closed-loop benchmarking simulation  visual-inertial estimation  trajectory tracking performance  Visualization  Navigation  Simultaneous localization and mapping  Benchmark testing  Estimation 
Abstract: Visual-inertial SLAM is essential for robot navigation in GPS-denied environments, e.g. indoor, underground. Conventionally, the performance of visual-inertial SLAM is evaluated with open-loop analysis, with a focus on the drift level of SLAM systems. In this paper, we raise the question on the importance of visual estimation latency in closed-loop navigation tasks, such as accurate trajectory tracking. To understand the impact of both drift and latency on visualinertial SLAM systems, a closed-loop benchmarking simulation is conducted, where a robot is commanded to follow a desired trajectory using the feedback from visual-inertial estimation. By extensively evaluating the trajectory tracking performance of representative state-of-the-art visual-inertial SLAM systems, we reveal the importance of latency reduction in visual estimation module of these systems. The findings suggest directions of future improvements for visual-inertial SLAM.


Title: Learning error models for graph SLAM
Key Words: autonomous aerial vehicles  graph theory  mobile robots  path planning  robot vision  SLAM (robots)  resistance distance  covisibility graph  simulated UAV coverage path  uncertainty models  monocular graph SLAM  topological features  error model learning  UAV coverage path planning trajectories  Simultaneous localization and mapping  Resistance  Uncertainty  Computational modeling  Computer architecture  Predictive models  Cameras 
Abstract: Following recent developments, this paper investigates the possibility to predict uncertainty models for monocular graph SLAM using topological features of the problem. An architecture to learn relative (i.e. inter-keyframe) uncertainty models using the resistance distance in the covisibility graph is presented. The proposed architecture is applied to simulated UAV coverage path planning trajectories and an analysis of the approaches strengths and shortcomings is provided.


Title: Adversarial Feature Training for Generalizable Robotic Visuomotor Control
Key Words: feature extraction  learning (artificial intelligence)  robot vision  robotic policy training  large-scale data collection  task-setup  task-irrelevant objects  interactive samples  adversarial training  deep RL capabilities  transfer learning  robotic tasks  adversarial feature training  generalizable robotic visuomotor control  deep reinforcement learning  action-selection policies  image pixels mapping  visuomotor robotic policy training  Task analysis  Training  Visualization  Feature extraction  Robots  Trajectory  Clutter 
Abstract: Deep reinforcement learning (RL) has enabled training action-selection policies, end-to-end, by learning a function which maps image pixels to action outputs. However, it's application to visuomotor robotic policy training has been limited because of the challenge of large-scale data collection when working with physical hardware. A suitable visuomotor policy should perform well not just for the task-setup it has been trained for, but also for all varieties of the task, including novel objects at different viewpoints surrounded by task-irrelevant objects. However, it is impractical for a robotic setup to sufficiently collect interactive samples in a RL framework to generalize well to novel aspects of a task. In this work, we demonstrate that by using adversarial training for domain transfer, it is possible to train visuomotor policies based on RL frameworks, and then transfer the acquired policy to other novel task domains. We propose to leverage the deep RL capabilities to learn complex visuomotor skills for uncomplicated task setups, and then exploit transfer learning to generalize to new task domains provided only still images of the task in the target domain. We evaluate our method on two real robotic tasks, picking and pouring, and compare it to a number of prior works, demonstrating its superiority.


Title: Real-Time UAV Path Planning for Autonomous Urban Scene Reconstruction
Key Words: autonomous aerial vehicles  computational geometry  image reconstruction  path planning  robot vision  SLAM (robots)  unmanned aerial vehicles  large-scale scene mapping  autonomous urban scene reconstruction  point cloud reconstruction  reconstruction quality  large-scale scene reconstruction  real-time UAV path planning  SLAM  Buildings  Image reconstruction  Three-dimensional displays  Path planning  Drones  Cameras  Layout 
Abstract: Unmanned aerial vehicles (UAVs) are frequently used for large-scale scene mapping and reconstruction. However, in most cases, drones are operated manually, which should be more effective and intelligent. In this article, we present a method of real-time UAV path planning for autonomous urban scene reconstruction. Considering the obstacles and time costs, we utilize the top view to generate the initial path. Then we estimate the building heights and take close-up pictures that reveal building details through a SLAM framework. To predict the coverage of the scene, we propose a novel method which combines information on reconstructed point clouds and possible coverage areas. The experimental results reveal that the reconstruction quality of our method is good enough. Our method is also more time-saving than the state-of-the-arts.


Title: Learning-based Path Planning for Autonomous Exploration of Subterranean Environments
Key Words: autonomous aerial vehicles  graph theory  learning by example  mobile robots  optical radar  path planning  robot programming  sampled data systems  tunnels  autonomous exploration  subterranean environments  aerial robots  training expert  imitation learning  underground mine drifts  tunnels  graph based path planner  learning based path planning  LiDAR  range data sampling  Robot sensing systems  Path planning  Training  Training data  Planning  Robot kinematics 
Abstract: In this work we present a new methodology on learning-based path planning for autonomous exploration of subterranean environments using aerial robots. Utilizing a recently proposed graph-based path planner as a "training expert" and following an approach relying on the concepts of imitation learning, we derive a trained policy capable of guiding the robot to autonomously explore underground mine drifts and tunnels. The algorithm utilizes only a short window of range data sampled from the onboard LiDAR and achieves an exploratory behavior similar to that of the training expert with a more than an order of magnitude reduction in computational cost, while simultaneously relaxing the need to maintain a consistent and online reconstructed map of the environment. The trained path planning policy is extensively evaluated both in simulation and experimentally within field tests relating to the autonomous exploration of underground mines.


Title: A Fast and Accurate Solution for Pose Estimation from 3D Correspondences
Key Words: approximation theory  computational geometry  computer vision  convex programming  least squares approximations  minimisation  pose estimation  pose estimation  point-to-plane correspondences  computer vision  least-squares problem  global minimizer  real-time applications  local minimizer  Cayley-Gibbs-Rodriguez parameterization  first-order optimality conditions  3D correspondences  CGR parameterization  Three-dimensional displays  Pose estimation  Cost function  Approximation algorithms  Real-time systems  Iterative closest point algorithm  Simultaneous localization and mapping 
Abstract: Estimating pose from given 3D correspondences, including point-to-point, point-to-line and point-to-plane correspondences, is a fundamental task in computer vision with many applications. We present a fast and accurate solution for the least-squares problem of this task. Previous works mainly focus on studying the way to find the global minimizer of the least-squares problem. However, existing works that show the ability to achieve the global minimizer are still unsuitable for real-time applications. Furthermore, as one of contributions of this paper, we prove that there exist ambiguous configurations for any number of lines and planes. These configurations have several solutions in theory, which makes the correct solution may come from a local minimizer when the data are with noise. Previous works based on convex optimization which is unable to find local minimizers do not work in the ambiguous configuration. Our algorithm is efficient and able to reveal local minimizers. We employ the Cayley-Gibbs-Rodriguez (CGR) parameterization of the rotation to derive a general rational cost for the three cases of 3D correspondences. The main contribution of this paper is to solve the first-order optimality conditions of the least-squares problem, which are of a complicated rational form. The central idea of our algorithm is to introduce some intermediate unknowns to simplify the problem. Extensive experimental results show that our algorithm is more stable than previous algorithms when the number N of correspondences is small. Besides, when N is large, our algorithm achieves the same accuracy as the state-of-the-art algorithm [1], but our algorithm is about 7 times faster than [1] in real applications.


Title: Reliable Data Association for Feature-Based Vehicle Localization using Geometric Hashing Methods
Key Words: feature extraction  optical radar  reliability  road vehicles  sensor fusion  feature-based vehicle localization  data association  local environment  plausible feature associations  safe localization  localization features  geometric hashing methods  error propagation  cylindrical objects  LiDAR data  Feature extraction  Reliability  Data mining  Noise measurement  Standards  Visualization  Object recognition 
Abstract: Reliable data association represents a main challenge of feature-based vehicle localization and is the key to integrity of localization. Independent of the type of features used, incorrect associations between detected and mapped features will provide erroneous position estimates. Only if the uniqueness of a local environment is represented by the features that are stored in the map, the reliability of localization is enhanced. In this work, a new approach based on Geometric Hashing is introduced to the field of data association for feature-based vehicle localization. Without any information on a prior position, the proposed method allows to efficiently search large map regions for plausible feature associations. Therefore, odometry and GNSS-based inputs can be neglected, which reduces the risk of error propagation and enables safe localization. The approach is demonstrated on approximately 10min of data recorded in an urban scenario. Cylindrical objects without distinctive descriptors, which were extracted from LiDAR data, serve as localization features. Experimental results both demonstrate the feasibility as well as limitations of the approach.


Title: Bounded haptic teleoperation of a quadruped robotâs foot posture for sensing and manipulation
Key Words: control engineering computing  force feedback  haptic interfaces  legged locomotion  mechanical engineering computing  motion control  position control  quadratic programming  robot dynamics  telerobotics  quadruped robot ANYmal  force feedback  bounded haptic teleoperation  control framework  operator-guided haptic exploration  torso  foot posture control  whole-body controller  analytical Cartesian impedance controllers  null space projector  contact forces  force-feedback  7D haptic joystick  Impedance  Aerospace electronics  Robot kinematics  Haptic interfaces  Torso  Force 
Abstract: This paper presents a control framework to teleoperate a quadruped robot's foot for operator-guided haptic exploration of the environment. Since one leg of a quadruped robot typically only has 3 actuated degrees of freedom (DoFs), the torso is employed to assist foot posture control via a hierarchical whole-body controller. The foot and torso postures are controlled by two analytical Cartesian impedance controllers cascaded by a null space projector. The contact forces acting on supporting feet are optimized by quadratic programming (QP). The foot's Cartesian impedance controller may also estimate contact forces from trajectory tracking errors, and relay the force-feedback to the operator. A 7D haptic joystick, Sigma.7, transmits motion commands to the quadruped robot ANYmal, and renders the force feedback. Furthermore, the joystick's motion is bounded by mapping the foot's feasible force polytope constrained by the friction cones and torque limits in order to prevent the operator from driving the robot to slipping or falling over. Experimental results demonstrate the efficiency of the proposed framework.


Title: BatVision: Learning to See 3D Spatial Layout with Two Ears
Key Words: acoustic signal processing  audio signal processing  bioacoustics  cameras  ear  image colour analysis  image sensors  mechanoception  mobile robots  object detection  path planning  robot vision  stereo image processing  visual perception  machine vision  3D spatial layout  ears  nonvisual perception  artificial systems  ultrasound complement camera-based vision  information gain  harness sound  machine perception  low- cost BatVision system  short chirps  artificial human pinnae pair  stereo camera  color images  scene depths  trained BatVision  2D visual scenes  vision system  robot navigation  Microphones  Visualization  Ear  Chirp  Cameras  Three-dimensional displays  Training 
Abstract: Many species have evolved advanced non-visual perception while artificial systems fall behind. Radar and ultrasound complement camera-based vision but they are often too costly and complex to set up for very limited information gain. In nature, sound is used effectively by bats, dolphins, whales, and humans for navigation and communication. However, it is unclear how to best harness sound for machine perception.Inspired by bats' echolocation mechanism, we design a low- cost BatVision system that is capable of seeing the 3D spatial layout of space ahead by just listening with two ears. Our system emits short chirps from a speaker and records returning echoes through microphones in an artificial human pinnae pair. During training, we additionally use a stereo camera to capture color images for calculating scene depths. We train a model to predict depth maps and even grayscale images from the sound alone. During testing, our trained BatVision provides surprisingly good predictions of 2D visual scenes from two 1D audio signals. Such a sound to vision system would benefit robot navigation and machine vision, especially in low-light or no-light conditions. Our code and data are publicly available.


Title: The OmniScape Dataset
Key Words: cameras  image segmentation  motorcycles  object detection  stereo image processing  traffic engineering computing  omnidirectional images  semantic segmentation  depth map  ground truth images  CARLA Simulator  open-source simulator  catadioptric images  OmniScape dataset  autonomous driving research  Grand Theft Auto V  two-wheeled vehicles  motorcycle  Cameras  Semantics  Vehicle dynamics  Motorcycles  Image segmentation  Virtual environments  Roads 
Abstract: Despite the utility and benefits of omnidirectional images in robotics and automotive applications, there are no datasets of omnidirectional images available with semantic segmentation, depth map, and dynamic properties. This is due to the time cost and human effort required to annotate ground truth images. This paper presents a framework for generating omnidirectional images using images that are acquired from a virtual environment. For this purpose, we demonstrate the relevance of the proposed framework on two well-known simulators: CARLA Simulator, which is an open-source simulator for autonomous driving research, and Grand Theft Auto V (GTA V), which is a very high quality video game. We explain in details the generated OmniScape dataset, which includes stereo fisheye and catadioptric images acquired from the two front sides of a motorcycle, including semantic segmentation, depth map, intrinsic parameters of the cameras and the dynamic parameters of the motorcycle. It is worth noting that the case of two-wheeled vehicles is more challenging than cars due to the specific dynamic of these vehicles.


Title: Calibrating a Soft ERT-Based Tactile Sensor with a Multiphysics Model and Sim-to-real Transfer Learning
Key Words: calibration  inverse problems  learning (artificial intelligence)  neural nets  robots  tactile sensors  tomography  soft ERT-based tactile sensor  sim-to-real transfer learning  electrical resistance tomography  finite element multiphysics model  contact pressure distributions  voltage measurements  model parameters  single-point dataset  contact force  calibration method  ERT-based tactile sensors  Fabrics  Computational modeling  Tactile sensors  Mathematical model  Electrodes  Force  Conductivity 
Abstract: Tactile sensors based on electrical resistance tomography (ERT) have shown many advantages for implementing a soft and scalable whole-body robotic skin; however, calibration is challenging because pressure reconstruction is an ill-posed inverse problem. This paper introduces a method for calibrating soft ERT-based tactile sensors using sim-to-real transfer learning with a finite element multiphysics model. The model is composed of three simple models that together map contact pressure distributions to voltage measurements. We optimized the model parameters to reduce the gap between the simulation and reality. As a preliminary study, we discretized the sensing points into a 6 by 6 grid and synthesized single- and two-point contact datasets from the multiphysics model. We obtained another single-point dataset using the real sensor with the same contact location and force used in the simulation. Our new deep neural network architecture uses a de-noising network to capture the simulation-to-real gap and a reconstruction network to estimate contact force from voltage measurements. The proposed approach showed 82% hit rate for localization and 0.51 N of force estimation error performance in singlecontact tests and 78.5% hit rate for localization and 5.0 N of force estimation error in two-point contact tests. We believe this new calibration method has the possibility to improve the sensing performance of ERT-based tactile sensors.


Title: Enabling Topological Planning with Monocular Vision
Key Words: learning (artificial intelligence)  mobile robots  multi-agent systems  path planning  robot vision  sensors  SLAM (robots)  heuristic priors  intelligent planning  monocular SLAM  low texture  highly cluttered environments  robust sparse map representation  monocular vision  learned sensor  high-level structure  sparse vertices  known free space  mapping technique  subgoal planning applications  enabling topological planning  topological strategies  navigation  possible actions  Planning  Image edge detection  Navigation  Robot sensing systems  Buildings  Robustness 
Abstract: Topological strategies for navigation meaningfully reduce the space of possible actions available to a robot, allowing use of heuristic priors or learning to enable computationally efficient, intelligent planning. The challenges in estimating structure with monocular SLAM in low texture or highly cluttered environments have precluded its use for topological planning in the past. We propose a robust sparse map representation that can be built with monocular vision and overcomes these shortcomings. Using a learned sensor, we estimate high-level structure of an environment from streaming images by detecting sparse "vertices" (e.g., boundaries of walls) and reasoning about the structure between them. We also estimate the known free space in our map, a necessary feature for planning through previously unknown environments. We show that our mapping technique can be used on real data and is sufficient for planning and exploration in simulated multi-agent search and learned subgoal planning applications.


Title: SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  navigation  neurocontrollers  robot vision  robust control  SnapNav  mapless visual navigation  sparse directional guidance  visual reference  robotics  deep neural network  visual navigation system  two-level hierarchy  directional commands  real-time control  obstacle avoidance  autonomous navigation  learning-based visual navigation  robust control  Robots  Navigation  Visualization  Task analysis  Training  Collision avoidance  Turning 
Abstract: Learning-based visual navigation still remains a challenging problem in robotics, with two overarching issues: how to transfer the learnt policy to unseen scenarios, and how to deploy the system on real robots. In this paper, we propose a deep neural network based visual navigation system, SnapNav. Unlike map-based navigation or Visual-Teach-and-Repeat (VT&R), SnapNav only receives a few snapshots of the environment combined with directional guidance to allow it to execute the navigation task. Additionally, SnapNav can be easily deployed on real robots due to a two-level hierarchy: a high level commander that provides directional commands and a low level controller that provides real-time control and obstacle avoidance. This also allows us to effectively use simulated and real data to train the different layers of the hierarchy, facilitating robust control. Extensive experimental results show that SnapNav achieves a highly autonomous navigation ability compared to baseline models, enabling sparse, map-less navigation in previously unseen environments.


Title: Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping
Key Words: C++ language  control engineering computing  graph theory  image reconstruction  image segmentation  learning (artificial intelligence)  public domain software  robot vision  SLAM (robots)  open-source C++ library  visual-inertial SLAM libraries  ORB-SLAM  VINS-Mono  semantic labeling  visual-inertial odometry module  state estimation  robust pose graph optimizer  global trajectory estimation  lightweight 3D mesher module  fast mesh reconstruction  3D metric-semantic reconstruction module  semantically labeled images  metric-semantic SLAM  real-time metric-semantic localization and mapping  Kimera  deep learning  Three-dimensional displays  Simultaneous localization and mapping  Robustness  Semantics  Libraries  Visualization  Real-time systems 
Abstract: We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.


Title: High Resolution Soft Tactile Interface for Physical Human-Robot Interaction
Key Words: cameras  control engineering computing  haptic interfaces  human-robot interaction  image processing  tactile sensors  touch (physiological)  high resolution soft tactile interface  physical human-robot interaction  tactile interactions  intuitive communication tool  fundamental method  tactile abilities  mechanical safety  sensory intelligence  human-sized geometries  soft tactile interfaces  intrinsically safe mechanical properties  nonlinear characteristics  robotic system  completely soft interface  human upper limbs  high resolution tactile sensory readings  human finger  tactile input  human forearm  safe tactile interface  Robot sensing systems  Cameras  Safety  Task analysis  Visualization 
Abstract: If robots and humans are to coexist and cooperate in society, it would be useful for robots to be able to engage in tactile interactions. Touch is an intuitive communication tool as well as a fundamental method by which we assist each other physically. Tactile abilities are challenging to engineer in robots, since both mechanical safety and sensory intelligence are imperative. Existing work reveals a trade-off between these principles- tactile interfaces that are high in resolution are not easily adapted to human-sized geometries, nor are they generally compliant enough to guarantee safety. On the other hand, soft tactile interfaces deliver intrinsically safe mechanical properties, but their non-linear characteristics render them difficult for use in timely sensing and control. We propose a robotic system that is equipped with a completely soft and therefore safe tactile interface that is large enough to interact with human upper limbs, while producing high resolution tactile sensory readings via depth camera imaging of the soft interface. We present and validate a data-driven model that maps point cloud data to contact forces, and verify its efficacy by demonstrating two real-world applications. In particular, the robot is able to react to a human finger's pokes and change its pose based on the tactile input. In addition, we also demonstrate that the robot can act as an assistive device that dynamically supports and follows a human forearm from underneath.


Title: Shared Control Templates for Assistive Robotics
Key Words: assisted living  handicapped aids  manipulators  medical robotics  mobile robots  path planning  service robots  constraint-based shared control  specific user command mappings  task execution  impairments  control interface  motor disability  light-weight robotic manipulators  assistive robotics  shared control templates  low-dimensional interface  high-dimensional tasks  human-readable format  state transitions  Task analysis  Robot kinematics  Manipulators  Wheelchairs  Manifolds  Rehabilitation robotics 
Abstract: Light-weight robotic manipulators can be used to restore the manipulation capability of people with a motor disability. However, manipulating the environment poses a complex task, especially when the control interface is of low bandwidth, as may be the case for users with impairments. Therefore, we propose a constraint-based shared control scheme to define skills which provide support during task execution. This is achieved by representing a skill as a sequence of states, with specific user command mappings and different sets of constraints being applied in each state. New skills are defined by combining different types of constraints and conditions for state transitions, in a human-readable format. We demonstrate its versatility in a pilot experiment with three activities of daily living. Results show that even complex, high-dimensional tasks can be performed with a low-dimensional interface using our shared control approach.


Title: Semantic Linking Maps for Active Visual Object Search
Key Words: inference mechanisms  manipulators  mobile robots  robot vision  search problems  Semantic Linking Maps model  target object  landmark objects  probabilistic inter-object spatial relations  hybrid search strategy  SLiM-based search strategy  Fetch mobile manipulation robot  mobile robots  common human environments  unseen target objects  reasoning  search space  common spatial relations  active visual object search strategy  Search problems  Robots  Probabilistic logic  Semantics  Buildings  Inference algorithms  Visualization 
Abstract: We aim for mobile robots to function in a variety of common human environments. Such robots need to be able to reason about the locations of previously unseen target objects. Landmark objects can help this reasoning by narrowing down the search space significantly. More specifically, we can exploit background knowledge about common spatial relations between landmark and target objects. For example, seeing a table and knowing that cups can often be found on tables aids the discovery of a cup. Such correlations can be expressed as distributions over possible pairing relationships of objects. In this paper, we propose an active visual object search strategy method through our introduction of the Semantic Linking Maps (SLiM) model. SLiM simultaneously maintains the belief over a target object's location as well as landmark objects' locations, while accounting for probabilistic inter-object spatial relations. Based on SLiM, we describe a hybrid search strategy that selects the next best view pose for searching for the target object based on the maintained belief. We demonstrate the efficiency of our SLiM-based search strategy through comparative experiments in simulated environments. We further demonstrate the realworld applicability of SLiM-based search in scenarios with a Fetch mobile manipulation robot.


Title: VALID: A Comprehensive Virtual Aerial Image Dataset
Key Words: computer vision  feature extraction  image classification  image segmentation  object detection  stereo image processing  aerial imagery  unmanned aerial vehicle tasks  single ground truth type  virtual environment  high-resolution images  virtual scenes  comprehensive virtual aerial image dataset  visual ground truth data  Image segmentation  Semantics  Task analysis  Object detection  Image color analysis  Benchmark testing  Labeling 
Abstract: Aerial imagery plays an important role in land-use planning, population analysis, precision agriculture, and unmanned aerial vehicle tasks. However, existing aerial image datasets generally suffer from the problem of inaccurate labeling, single ground truth type, and few category numbers. In this work, we implement a simulator that can simultaneously acquire diverse visual ground truth data in the virtual environment. Based on that, we collect a comprehensive Virtual AeriaL Image Dataset named VALID, consisting of 6690 high-resolution images, all annotated with panoptic segmentation on 30 categories, object detection with oriented bounding box, and binocular depth maps, collected in 6 different virtual scenes and 5 various ambient conditions (sunny, dusk, night, snow and fog). To our knowledge, VALID is the first aerial image dataset that can provide panoptic level segmentation and complete dense depth maps. We analyze the characteristics of VALID and evaluate state-of-the-art methods for multiple tasks to provide reference baselines. The experiment results demonstrate that VALID is well presented and challenging. The dataset is available at https://sites.google.com/view/valid-dataset/.


Title: Intensity Scan Context: Coding Intensity and Geometry Relations for Loop Closure Detection
Key Words: computational geometry  feature extraction  image matching  mobile robots  optical radar  robot vision  SLAM (robots)  stereo image processing  intensity scan context  light detection and ranging sensor  discard intensity reading  geometric relation  intensity structure re-identification  coding intensity  simultaneous localization and mapping  LiDAR sensor  3D loop closure detection  geometrical only descriptor matching  place recognition  robot navigation  fast point feature histogram  Geometry  Laser radar  Three-dimensional displays  Histograms  Simultaneous localization and mapping  Rough surfaces  Surface roughness 
Abstract: Loop closure detection is an essential and challenging problem in simultaneous localization and mapping (SLAM). It is often tackled with light detection and ranging (LiDAR) sensor due to its view-point and illumination invariant properties. Existing works on 3D loop closure detection often leverage on matching of local or global geometrical-only descriptors which discard intensity reading. In this paper we explore the intensity property from LiDAR scan and show that it can be effective for place recognition. We propose a novel global descriptor, intensity scan context (ISC), that explores both geometry and intensity characteristics. To improve the efficiency for loop closure detection, an efficient two-stage hierarchical re-identification process is proposed, including binary-operation based fast geometric relation retrieval and intensity structure re-identification. Thorough experiments including both local experiment and public datasets test have been conducted to evaluate the performance of the proposed method. Our method achieves better recall rate and recall precision than existing geometric-only methods.


Title: TextSLAM: Visual SLAM with Planar Text Features
Key Words: augmented reality  data visualisation  navigation  robot vision  SLAM (robots)  stereo image processing  text analysis  text detection  text object integration  augmented reality  navigation  scene understanding  illumination-invariant photometric error  TextSLAM  text detection  text-based visual SLAM  3D text maps  visual SLAM pipeline  planar text features  visual SLAM system  planar feature  Simultaneous localization and mapping  Three-dimensional displays  Visualization  Feature extraction  Navigation  Cameras  Robustness 
Abstract: We propose to integrate text objects in man-made scenes tightly into the visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to treat each detected text as a planar feature which is rich of textures and semantic meanings. The text feature is compactly represented by three parameters and integrated into visual SLAM by adopting the illumination-invariant photometric error. We also describe important details involved in implementing a full pipeline of text-based visual SLAM. To our best knowledge, this is the first visual SLAM method tightly coupled with the text features. We tested our method in both indoor and outdoor environments. The results show that with text features, the visual SLAM system becomes more robust and produces much more accurate 3D text maps that could be useful for navigation and scene understanding in robotic or augmented reality applications.


Title: Redesigning SLAM for Arbitrary Multi-Camera Systems
Key Words: cameras  distance measurement  robot vision  SLAM (robots)  stereo image processing  sensor-specific modifications  SLAM systems  robustness  camera configurations  adaptive SLAM system  multicamera setup  visual SLAM  adaptive initialization  scalable voxel-based map  sensor-agnostic information-theoretic keyframe selection algorithm  visual front-end design  visual-inertial odometry  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Uncertainty  Visualization 
Abstract: Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information- theoretic keyframe selection algorithm, and a scalable voxel- based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual- inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.


Title: Dynamic SLAM: The Need For Speed
Key Words: feature extraction  image motion analysis  image segmentation  mobile robots  path planning  robot vision  SLAM (robots)  rigid moving objects  static structure  dynamic structure  rigid objects  object-aware dynamic SLAM algorithm  model-free  significant motion constraints  3D models  SLAM based approaches  unstructured dynamic environments  autonomous systems  increased deployment  simultaneous localisation  static world assumption  Simultaneous localization and mapping  Heuristic algorithms  Dynamics  Three-dimensional displays  Solid modeling  Tracking 
Abstract: The static world assumption is standard in most simultaneous localisation and mapping (SLAM) algorithms. Increased deployment of autonomous systems to unstructured dynamic environments is driving a need to identify moving objects and estimate their velocity in real-time. Most existing SLAM based approaches rely on a database of 3D models of objects or impose significant motion constraints. In this paper, we propose a new feature-based, model-free, object-aware dynamic SLAM algorithm that exploits semantic segmentation to allow estimation of motion of rigid objects in a scene without the need to estimate the object poses or have any prior knowledge of their 3D models. The algorithm generates a map of dynamic and static structure and has the ability to extract velocities of rigid moving objects in the scene. Its performance is demonstrated on simulated, synthetic and real-world datasets.


Title: âSLAM: Dense SLAM meets Automatic Differentiation
Key Words: gradient methods  graph theory  learning (artificial intelligence)  robot vision  SLAM (robots)  automatic differentiation  dense simultaneous localization  learning-based approaches  representation learning approaches  classical SLAM systems  differentiable function  optimize task performance  typical dense SLAM system  âSLAM  posing SLAM systems  differentiable computational graphs  differentiable trust-region optimizers  task-based error signals  Simultaneous localization and mapping  Optimization  Three-dimensional displays  Damping  Task analysis  Neural networks 
Abstract: The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Learning-based approaches have the potential to leverage data or task performance to directly inform the representation. However, blending representation learning approaches with "classical" SLAM systems has remained an open question, because of their highly modular and complex nature. A SLAM system transforms raw sensor inputs into a distribution over the state(s) of the robot and the environment. If this transformation (SLAM) were expressible as a differentiable function, we could leverage task-based error signals over the outputs of this function to learn representations that optimize task performance. However, this is infeasible as several components of a typical dense SLAM system are non-differentiable. In this work, we propose âSLAM (gradSLAM), a methodology for posing SLAM systems as differentiable computational graphs, which unifies gradient-based learning and SLAM. We propose differentiable trust-region optimizers, surface measurement and fusion schemes, and raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM1.


Title: Sample Complexity of Probabilistic Roadmaps via Îµ-nets
Key Words: computational complexity  deterministic algorithms  graph theory  probability  shortest Î´-clear path  sample complexity  probabilistic roadmaps  Îµ-nets  optimality guarantees  deterministic sampling distribution  motion planning problem  parameter completeness  Planning  Complexity theory  Probabilistic logic  Two dimensional displays  Robots  Collision avoidance  Benchmark testing 
Abstract: We study fundamental theoretical aspects of probabilistic roadmaps (PRM) in the finite time (non-asymptotic) regime. In particular, we investigate how completeness and optimality guarantees of the approach are influenced by the underlying deterministic sampling distribution X and connection radius r > 0. We develop the notion of (Î´, Îµ)-completeness of the parameters X, r, which indicates that for every motion-planning problem of clearance at least Î´ > 0, PRM using X, r returns a solution no longer than 1+Îµ times the shortest Î´-clear path. Leveraging the concept of e-nets, we characterize in terms of lower and upper bounds the number of samples needed to guarantee (Î´, Îµ)-completeness. This is in contrast with previous work which mostly considered the asymptotic regime in which the number of samples tends to infinity. In practice, we propose a sampling distribution inspired by e-nets that achieves nearly the same coverage as grids while using fewer samples.


Title: How to Keep HD Maps for Automated Driving Up To Date
Key Words: cartography  data privacy  road vehicles  traffic engineering computing  dedicated mapping vehicles  low traversal frequencies  anonymized data  up-to-dateness  crowdsourced data  automatically trigger map update jobs  map patches  date HD map  automated driving functions  HD maps  automotive high definition digital map generation  automated driving up to date  Roads  Vehicle dynamics  Topology  Robot sensing systems  Shape 
Abstract: The current state of the art in automotive high definition digital (HD) map generation based on dedicated mapping vehicles cannot reliably keep these maps up to date because of the low traversal frequencies. Anonymized data collected from the fleet of vehicles that is already on the road provides a huge potential to outperform such state of the art solutions in robustness, safety and up-to-dateness of the map while achieving comparable quality. We thus present a solution based on crowdsourced data to (i) detect changes in the map independent of the type of change, (ii) automatically trigger map update jobs for parts of the map, and (iii) create and integrate map patches to keep the map always up to date. The developed solution provides a crowdsourced up to date HD map to make reliable prior information on lane markings and road edges available to automated driving functions.


Title: UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes
Key Words: cameras  image matching  image registration  inertial navigation  optical radar  satellite navigation  urban canyon  urban terrain  Hong Kong  San Francisco  IMU  GNSS-based solutions  LIDAR  global navigation satellite system  urban scene localization  urban scene mapping  inertia measurement units  camera-based methods  inertia navigation  visual feature matching  point cloud registration  full sensor suite dataset  UrbanLoco  Global navigation satellite system  Cameras  Laser radar  Urban areas  Robot sensing systems  Trajectory  Satellites 
Abstract: Mapping and localization is a critical module of autonomous driving, and significant achievements have been reached in this field. Beyond Global Navigation Satellite System (GNSS), research in point cloud registration, visual feature matching, and inertia navigation has greatly enhanced the accuracy and robustness of mapping and localization in different scenarios. However, highly urbanized scenes are still challenging: LIDAR- and camera-based methods perform poorly with numerous dynamic objects; the GNSS-based solutions experience signal loss and multi-path problems; the inertia measurement units (IMU) suffer from drifting. Unfortunately, current public datasets either do not adequately address this urban challenge or do not provide enough sensor information related to map-ping and localization. Here we present UrbanLoco: a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers. Our dataset includes a wide variety of urban terrains: urban canyons, bridges, tunnels, sharp turns, etc. More importantly, our dataset includes information from LIDAR, cameras, IMU, and GNSS receivers. Now the dataset is publicly available through the link in the footnote 1.


Title: Map As the Hidden Sensor: Fast Odometry-Based Global Localization
Key Words: distance measurement  mobile robots  path planning  sensors  tensors  odometry-based global localization  ambiguous observations  odometry drift  blind robots  robot state  belief tensor  map-corrected odometry localization  map traversability  robotics applications  hidden sensor  Robot sensing systems  Tensile stress  Trajectory  Robustness  Uncertainty  Real-time systems 
Abstract: Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.


Title: Accurate position tracking with a single UWB anchor
Key Words: inertial systems  Kalman filters  mobile robots  nonlinear filters  object tracking  observability  position measurement  SLAM (robots)  ultra wideband technology  velocity measurement  ultrawideband technology  UWB anchor  UWB range  moving robot tracking  position tracking  robotic applications  localization systems  optical tracking  9 DoF inertial measurement unit  UWB ranging source  UWB technology  robot speed estimation  orientation estimation  IMU sensor  observability  extended Kalman filter  EKF  robot pose estimation  Robot sensing systems  Estimation  Observability  Velocity measurement  Distance measurement  Mobile robots 
Abstract: Accurate localization and tracking are a fundamental requirement for robotic applications. Localization systems like GPS, optical tracking, simultaneous localization and mapping (SLAM) are used for daily life activities, research, and commercial applications. Ultra-wideband (UWB) technology provides another venue to accurately locate devices both indoors and outdoors. In this paper, we study a localization solution with a single UWB anchor, instead of the traditional multi-anchor setup. Besides the challenge of a single UWB ranging source, the only other sensor we require is a low-cost 9 DoF inertial measurement unit (IMU). Under such a configuration, we propose continuous monitoring of UWB range changes to estimate the robot speed when moving on a line. Combining speed estimation with orientation estimation from the IMU sensor, the system becomes temporally observable. We use an Extended Kalman Filter (EKF) to estimate the pose of a robot. With our solution, we can effectively correct the accumulated error and maintain accurate tracking of a moving robot.


Title: MPC-based Controller with Terrain Insight for Dynamic Legged Locomotion
Key Words: convolutional neural nets  hydraulic actuators  legged locomotion  neurocontrollers  predictive control  robot dynamics  on-board mapping  contact sequence task  convolutional neural network  model predictive controller  on-board sensing  MPC-based controller  terrain insight  dynamic legged locomotion  hydraulically actuated quadruped robot HyQReal  Legged locomotion  Trajectory  Task analysis  Computational modeling  Dynamics  Foot 
Abstract: We present a novel control strategy for dynamic legged locomotion in complex scenarios that considers information about the morphology of the terrain in contexts when only on-board mapping and computation are available. The strategy is built on top of two main elements: first a contact sequence task that provides safe foothold locations based on a convolutional neural network to perform fast and continuous evaluation of the terrain in search of safe foothold locations; then a model predictive controller that considers the foothold locations given by the contact sequence task to optimize target ground reaction forces. We assess the performance of our strategy through simulations of the hydraulically actuated quadruped robot HyQReal traversing rough terrain under realistic on-board sensing and computing conditions.


Title: Grasp for Stacking via Deep Reinforcement Learning
Key Words: grippers  industrial manipulators  learning systems  neurocontrollers  optimal control  stacking  deep reinforcement learning  integrated robotic arm system  object grasping  model-free deep Q-learning method  grasping-stacking task  GSN  grasping for stacking network  industrial environments  GNet  optimal location  long-range planning  Grasping  Stacking  Task analysis  Feature extraction  Manipulators  Training 
Abstract: Integrated robotic arm system should contain both grasp and place actions. However, most grasping methods focus more on how to grasp objects, while ignoring the placement of the grasped objects, which limits their applications in various industrial environments. In this research, we propose a model-free deep Q-learning method to learn the grasping-stacking strategy end-to-end from scratch. Our method maps the images to the actions of the robotic arm through two deep networks: the grasping network (GNet) using the observation of the desk and the pile to infer the gripper's position and orientation for grasping, and the stacking network (SNet) using the observation of the platform to infer the optimal location when placing the grasped object. To make a long-range planning, the two observations are integrated in the grasping for stacking network (GSN). We evaluate the proposed GSN on a grasping-stacking task in both simulated and real-world scenarios.


Title: Super-Pixel Sampler: a Data-driven Approach for Depth Sampling and Reconstruction
Key Words: image colour analysis  image reconstruction  image sampling  random processes  data-driven approach  depth sampling  depth acquisition  active illumination  autonomous navigation  robotic navigation  mechanical templates  sampling templates  autonomous vehicles  solid-state depth sensors  adaptive framework  simple reconstruction algorithm  generic reconstruction algorithm  sampling reconstruction algorithm  random sampling  depth completion algorithms  single-pixel prototype sampler  RGB sampling strategies  single depth sampling strategies  piecewise planar depth model  superpixel sampler  SPS  Image reconstruction  Adaptation models  Estimation  Cameras  Robot sensing systems  Navigation 
Abstract: Depth acquisition, based on active illumination, is essential for autonomous and robotic navigation. LiDARs (Light Detection And Ranging) with mechanical, fixed, sampling templates are commonly used in today's autonomous vehicles. An emerging technology, based on solid-state depth sensors, with no mechanical parts, allows fast and adaptive scans. In this paper, we propose an adaptive, image-driven, fast, sampling and reconstruction strategy. First, we formulate a piece-wise planar depth model and estimate its validity for indoor and outdoor scenes. Our model and experiments predict that, in the optimal case, adaptive sampling strategies with about 20-60 piece-wise planar structures can approximate well a depth map. This translates to requiring a single depth sample for every 1200 RGB samples (less than 0.1%), providing strong motivation to investigate an adaptive framework. Second, we introduce SPS (Super-Pixel Sampler), a simple, generic, sampling and reconstruction algorithm, based on super-pixels. Our sampling improves grid and random sampling, consistently, for a wide variety of reconstruction methods. Third, we propose an extremely simple and fast reconstruction for our sampler. It achieves state-of-the-art results, compared to complex image- guided depth completion algorithms, reducing the required sampling rate by a factor of 3-4. A single-pixel prototype sampler built in our lab illustrates the concept.


Title: Physics-based Simulation of Continuous-Wave LIDAR for Localization, Calibration and Tracking
Key Words: calibration  computerised instrumentation  differentiation  gradient methods  optical radar  optical sensors  optical tracking  optimisation  radar receivers  parameter estimation  2D continuous-wave LIDAR sensors  light detection and ranging sensors  depth measurements  localization pipelines  autonomous robots  perception stack  physics-based simulation  sensor measurements  gradient-based optimization  Hokuyo URG-04LX LIDAR  surface-light interactions  physically plausible model  laser light  calibration  time-of-flight cameras  depth sensors  Laser radar  Measurement by laser beam  Mathematical model  Sensor phenomena and characterization  Surface emitting lasers  Laser modes 
Abstract: Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.


Title: A Spatial-temporal Multiplexing Method for Dense 3D Surface Reconstruction of Moving Objects
Key Words: image coding  image filtering  image matching  image restoration  image texture  multiplexing  object recognition  robot vision  stereo image processing  surface reconstruction  high reconstruction accuracy  fast image acquisition  spatial-temporal multiplexing method  moving objects  three-dimensional reconstruction  robotic applications  robotic recognition  spatial-multiplexing time-multiplexing structured-light techniques  image acquisition time  spatial-temporal encoded patterns  dense 3D surface reconstruction  texture map  image blur  high-frequency phase-shifting fringes  spatial-coded texture  Image reconstruction  Multiplexing  Three-dimensional displays  Surface reconstruction  Robots  Encoding  Reliability 
Abstract: Three-dimensional reconstruction of dynamic objects is important for robotic applications, for example, the robotic recognition and manipulation. In this paper, we present a novel 3D surface reconstruction method for moving objects. The proposed method combines the spatial-multiplexing and time-multiplexing structured-light techniques that have advantages of less image acquisition time and accurate 3D reconstruction, respectively. A set of spatial-temporal encoded patterns are designed, where a spatial-encoded texture map is embedded into the temporal-encoded three-step phase-shifting fringes. The specifically designed spatial-coded texture assigns high-uniqueness codeword to any window on the image which helps to eliminate the phase ambiguity. In addition, the texture is robust to noise and image blur. Combining this texture with high-frequency phase-shifting fringes, high reconstruction accuracy would be ensured. This method only requires 3 patterns to uniquely encode a surface, which facilitates the fast image acquisition for each reconstruction step. A filtering stereo matching algorithm is proposed for the spatial-temporal multiplexing method to improve the matching reliability. Moreover, the reconstruction precision is further enhanced by a correspondence refinement algorithm. Experiments validate the performance of the proposed method including the high accuracy, the robustness to noise and the ability to reconstruct moving objects.


Title: egoTEB: Egocentric, Perception Space Navigation Using Timed-Elastic-Bands
Key Words: aerospace navigation  aerospace robotics  collision avoidance  graph theory  mobile robots  Monte Carlo methods  motion control  optimisation  trajectory control  grid-based representations  optimization graph  local planning map  egoTEB  timed-elastic-bands  TEB hierarchical planner  real-time navigation  collision avoidance  goal directed motion  multitrajectory optimization based synthesis method  topologically distinct trajectory candidates  factor graph approach  egocentric perception space navigation  egocentric perception space representations  Monte Carlo evaluations  autonomous mobile robot  Trajectory  Optimization  Navigation  Planning  Robots  Collision avoidance  Topology 
Abstract: The TEB hierarchical planner for real-time navigation through unknown environments is highly effective at balancing collision avoidance with goal directed motion. Designed over several years and publications, it implements a multi-trajectory optimization based synthesis method for identifying topologically distinct trajectory candidates through navigable space. Unfortunately, the underlying factor graph approach to the optimization problem induces a mismatch between grid-based representations and the optimization graph, which leads to several time and optimization inefficiencies. This paper explores the impact of using egocentric, perception space representations for the local planning map. Doing so alleviates many of the identified issues related to TEB and leads to a new method called egoTEB. Timing experiments and Monte Carlo evaluations in benchmark worlds quantify the benefits of egoTEB for navigation through uncertain environments.


Title: Congestion-aware Evacuation Routing using Augmented Reality Devices
Key Words: augmented reality  emergency management  optimisation  congestion-aware evacuation  congestion-aware routing solution  indoor evacuation  real-time individual-customized evacuation routes  multiple destinations  population density map  obtained on-the-fly  congestion distribution  optimal solution  time-efficient evacuation route  AR devices  user-end augmented reality devices  Sociology  Statistics  Routing  Real-time systems  Path planning  Robots  Headphones 
Abstract: We present a congestion-aware routing solution for indoor evacuation, which produces real-time individual-customized evacuation routes among multiple destinations while keeping tracks of all evacuees' locations. A population density map, obtained on-the-fly by aggregating locations of evacuees from user-end Augmented Reality (AR) devices, is used to model the congestion distribution inside a building. To efficiently search the evacuation route among all destinations, a variant of A* algorithm is devised to obtain the optimal solution in a single pass. In a series of simulated studies, we show that the proposed algorithm is more computationally optimized compared to classic path planning algorithms; it generates a more time-efficient evacuation route for each individual that minimizes the overall congestion. A complete system using AR devices is implemented for a pilot study in real-world environments, demonstrating the efficacy of the proposed approach.


Title: Flexure Hinge-based Biomimetic Thumb with a Rolling-Surface Metacarpal Joint
Key Words: biomimetics  bone  control system synthesis  dexterous manipulators  end effectors  manipulator kinematics  motion control  position control  surgery  flexure hinge-based biomimetic thumb  rolling-surface metacarpal joint  grasping  dexterous manipulation  kinematic multiplicity  robotic hand  kinematic model  surgical techniques  motion capture data  end effector  task-space velocities  tendon excursion velocity  human thumb state contribution  data representation  effector velocity  Joints  Fasteners  Thumb  Ellipsoids  Robots  Prototypes  Ceramics 
Abstract: The human thumb's state contribution to grasping and dexterous manipulation of objects is a function of the kinematic multiplicity of joints and structure of the bones, joints, and ligaments. This paper looks at the design and evaluation of a human-like thumb for use in a robotic hand, where the thumb's state contribution to grasping and dexterous manipulation is a function of a simplified kinematic model based on that of the human thumb, but also on empirical trials of surgical techniques to retain functionality while reducing the number of joints in the thumb. Motion Capture Data of the End Effector is analyzed with the measured excursion of the tendons to determine the relationship between tendon velocities and task-space velocities. After validating the procedure experimentally, a simplified metric is proposed to represent this data, and shows that our prototype is predicted to have a relatively smooth mapping between tendon excursion velocity and end effector velocity.


Title: Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors
Key Words: groupware  image fusion  mobile robots  multi-robot systems  path planning  sensor fusion  SLAM (robots)  dynamic collaborative mapping  multimodal environmental perception  heterogeneous sensor fusion model  local 3D maps  night rainforest  3D map fusion missions  multimodal sensors  long-term operation  collaborative robots  dynamic environment  dynamic objects  Collaboration  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Robot vision systems 
Abstract: Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.


Title: Real-Time Graph-Based SLAM with Occupancy Normal Distributions Transforms
Key Words: graph theory  least squares approximations  mobile robots  normal distribution  robot vision  SLAM (robots)  occupancy grid map  graph-based SLAM  occupancy normal distribution transforms  normal distributions transforms  simultaneous localization and mapping  mobile robotics  least squares problem  nonlinear optimizers  global NDT scan matcher  Simultaneous localization and mapping  Cost function  Google  Jacobian matrices  Gaussian distribution 
Abstract: Simultaneous Localization and Mapping (SLAM) is one of the basic problems in mobile robotics. While most approaches are based on occupancy grid maps, Normal Distributions Transforms (NDT) and mixtures like Occupancy Normal Distribution Transforms (ONDT) have been shown to represent sensor measurements more accurately. In this work, we slightly re-formulate the (O)NDT matching function such that it becomes a least squares problem that can be solved with various robust numerical and analytical non-linear optimizers. Further, we propose a novel global (O)NDT scan matcher for loop closure. In our evaluation, our NDT and ONDT methods are able to outperform the occupancy grid map based ones we adopted from Google's Cartographer implementation.


Title: Spatio-Temporal Non-Rigid Registration of 3D Point Clouds of Plants
Key Words: agriculture  cameras  image registration  robot vision  spatio-temporal nonrigid registration  3D point clouds  sensor data  agricultural robotics  plant science  agricultural tasks  automated temporal plant-trait analysis  plant performance monitoring  plant registration  Three-dimensional displays  Skeleton  Hidden Markov models  Strain  Topology  Simultaneous localization and mapping 
Abstract: Analyzing sensor data of plants and monitoring plant performance is a central element in different agricultural robotics applications. In plant science, phenotyping refers to analyzing plant traits for monitoring growth, for describing plant properties, or characterizing the plant's overall performance. It plays a critical role in the agricultural tasks and in plant breeding. Recently, there is a rising interest in using 3D data obtained from laser scanners and 3D cameras to develop automated non-intrusive techniques for estimating plant traits. In this paper, we address the problem of registering 3D point clouds of the plants over time, which is a backbone of applications interested in tracking spatio-temporal traits of individual plants. Registering plants over time is challenging due to its changing topology, anisotropic growth, and non-rigid motion in between scans. We propose a novel approach that exploits the skeletal structure of the plant and determines correspondences over time and drives the registration process. Our approach explicitly accounts for the non-rigidity and the growth of the plant over time in the registration. We tested our approach on a challenging dataset acquired over the course of two weeks and successfully registered the 3D plant point clouds recorded with a laser scanner forming a basis for developing systems for automated temporal plant-trait analysis.


Title: Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV
Key Words: distance measurement  mobile robots  optical radar  path planning  robot vision  SLAM (robots)  FoV  autonomous vehicles  autonomous navigation  path planning  LOAM algorithm  LiDAR odometry and mapping  robot pose localization  Laser radar  Feature extraction  Three-dimensional displays  Measurement by laser beam  Laser noise  Real-time systems  Spinning 
Abstract: LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot's pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.


Title: Active SLAM using 3D Submap Saliency for Underwater Volumetric Exploration
Key Words: graph theory  mobile robots  navigation  path planning  robot vision  SLAM (robots)  underwater volumetric exploration  active SLAM framework  3D underwater environments  multibeam sonar  integrated SLAM  volumetric free-space information  informative loop closures  navigation policy  3D visual dictionary  submap saliency  sensor information  pose-graph SLAM formulation  global occupancy grid map  uncertainty-agnostic framework  Simultaneous localization and mapping  Three-dimensional displays  Uncertainty  Conferences  Automation  Sonar  Planning 
Abstract: In this paper, we present an active SLAM framework for volumetric exploration of 3D underwater environments with multibeam sonar. Recent work in integrated SLAM and planning performs localization while maintaining volumetric free-space information. However, an absence of informative loop closures can lead to imperfect maps, and therefore unsafe behavior. To solve this, we propose a navigation policy that reduces vehicle pose uncertainty by balancing between volumetric exploration and revisitation. To identify locations to revisit, we build a 3D visual dictionary from real-world sonar data and compute a metric of submap saliency. Revisit actions are chosen based on propagated pose uncertainty and sensor information gain. Loop closures are integrated as constraints in our pose-graph SLAM formulation and these deform the global occupancy grid map. We evaluate our performance in simulation and real-world experiments, and highlight the advantages over an uncertainty-agnostic framework.


Title: Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM
Key Words: mobile robots  path planning  pose estimation  robot vision  service robots  SLAM (robots)  simultaneous localization and mapping  data sequences  robotic autonomy  service robots  real-world indoor scenes  OpenLORIS-Scene datasets  SLAM problems  pose estimation  Simultaneous localization and mapping  Robot kinematics  Cameras  Synchronization  Trajectory 
Abstract: Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.


Title: Goal-Directed Occupancy Prediction for Lane-Following Actors
Key Words: mobile robots  motion estimation  prediction theory  road safety  road traffic  road vehicles  roads  robot vision  traffic engineering computing  complex road networks  mapped road topology  dynamic road actors  mapped lane geometry  mode collapse problem  goal-directed occupancy prediction  lane-following actors  shared roads  safe autonomous driving  possible vehicle behaviors  possible goal reasoning  local scene context multimodality  high-level action set  future spatial occupancy prediction  Roads  Predictive models  Trajectory  Topology  Geometry  Task analysis  Autonomous vehicles 
Abstract: Predicting the possible future behaviors of vehicles that drive on shared roads is a crucial task for safe autonomous driving. Many existing approaches to this problem strive to distill all possible vehicle behaviors into a simplified set of high-level actions. However, these action categories do not suffice to describe the full range of maneuvers possible in the complex road networks we encounter in the real world. To combat this deficiency, we propose a new method that leverages the mapped road topology to reason over possible goals and predict the future spatial occupancy of dynamic road actors. We show that our approach is able to accurately predict future occupancy that remains consistent with the mapped lane geometry and naturally captures multi-modality based on the local scene context while also not suffering from the mode collapse problem observed in prior work.


Title: Brno Urban Dataset - The New Data for Self-Driving Agents and Mapping Tasks
Key Words: cameras  Global Positioning System  infrared detectors  mobile robots  optical radar  SLAM (robots)  WUXGA cameras  3D LiDAR  inertial measurement unit  infrared camera  differential RTK GNSS receiver  centimetre accuracy  public dataset  submillisecond precision  autonomous driving  Brno Urban dataset  self-driving agents  mapping tasks  Brno-Czech Republic  https://github.com/RoboticsBUT/Brno-Urban-Dataset  Cameras  Sensors  Global Positioning System  Global navigation satellite system  Receivers  Laser radar  Synchronization 
Abstract: Autonomous driving is a dynamically growing field of research, where quality and amount of experimental data is critical. Although several rich datasets are available these days, the demands of researchers and technical possibilities are evolving. Through this paper, we bring a new dataset recorded in Brno - Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with submillisecond precision to allow wider range of applications. At the time of publishing of this paper, recordings of more than 350 km of rides in varying environment are shared at: https://github.com/RoboticsBUT/Brno-Urban-Dataset.


Title: Camera Tracking in Lighting Adaptable Maps of Indoor Environments
Key Words: feature extraction  image colour analysis  image sequences  indoor environment  lighting  object tracking  rendering (computer graphics)  direct dense camera tracking approach  lighting adaptable map representation  lighting invariant map representations  lighting conditions  visual localization methods  indoor environments  Lighting  Cameras  Visualization  Indoor environments  Estimation  Mathematical model  Adaptation models 
Abstract: Tracking the pose of a camera is at the core of visual localization methods used in many applications. As the observations of a camera are inherently affected by lighting, it has always been a challenge for these methods to cope with varying lighting conditions. Thus far, this issue has mainly been approached with the intent to increase robustness by choosing lighting invariant map representations. In contrast, our work aims at explicitly exploiting lighting effects for camera tracking. To achieve this, we propose a lighting adaptable map representation for indoor environments that allows real-time rendering of the scene illuminated by an arbitrary subset of the lamps contained in the model. Our method for estimating the light setting from the current camera observation enables us to adapt the model according to the lighting conditions present in the scene. As a result, lighting effects like cast shadows do no longer act as disturbances that demand robustness but rather as beneficial features when matching observations against the map. We leverage these capabilities in a direct dense camera tracking approach and demonstrate its performance in realworld experiments in scenes with varying lighting conditions.


Title: Passive Quadrupedal Gait Synchronization for Extra Robotic Legs Using a Dynamically Coupled Double Rimless Wheel Model
Key Words: gait analysis  legged locomotion  motion control  robot dynamics  springs (mechanical)  synchronisation  wheels  extra robotic legs system  robotic augmentation  human operator  articulated robot legs  human-XRL quadruped system  rear legs  quadrupedal robots  quadrupedal locomotion  coupler design parameters  passive quadrupedal gait synchronization  dynamically coupled double rimless wheel system  PoincarÃ© return map  numerical simulation  Legged locomotion  Wheels  Synchronization  Couplers  Robot kinematics  Human Augmentation  Supernumerary Robotic Limbs  Exoskeletons  Locomotion  Nonlinear Dynamics 
Abstract: The Extra Robotic Legs (XRL) system is a robotic augmentation worn by a human operator consisting of two articulated robot legs that walk with the operator and help bear a heavy backpack payload. It is desirable for the Human-XRL quadruped system to walk with the rear legs lead the front by 25% of the gait period, minimizing the energy lost from foot impacts while maximizing balance stability. Unlike quadrupedal robots, the XRL cannot command the human's limbs to coordinate quadrupedal locomotion. Using a pair of Rimless Wheel models, it is shown that the systems coupled with a spring and damper converge to the desired 25% phase difference. A PoincarÃ© return map was generated using numerical simulation to examine the convergence properties to different coupler design parameters, and initial conditions. The Dynamically Coupled Double Rimless Wheel system was physically realized with a spring and dashpot chosen from the theoretical results, and initial experiments indicate that the desired synchronization properties may be achieved within several steps using this set of passive components alone.


Title: Low-cost GelSight with UV Markings: Feature Extraction of Objects Using AlexNet and Optical Flow without 3D Image Reconstruction
Key Words: cameras  convolutional neural nets  feature extraction  image classification  image recognition  image reconstruction  image sequences  object recognition  UV markings  UV ink markers  low-cost GelSight sensor  nonGelsight captured images  optical flow algorithm  feature extraction  convolutional neural networks  CNN  2D image conversion  feature recognition  Prototypes  Light emitting diodes  Feature extraction  Image recognition  Three-dimensional displays  Lighting  Coatings 
Abstract: GelSight sensor has been used to study microgeometry of objects since 2009 in tactile sensing applications. Elastomer, reflective coating, lighting, and camera were the main challenges of making a GelSight sensor within a short period. The recent addition of permanent markers to the GelSight was a new era in shear/slip studies. In our previous studies, we introduced Ultraviolet (UV) ink and UV LEDs as a new form of marker and lighting respectively. UV ink markers are invisible using ordinary LED but can be made visible using UV LED. Currently, recognition of objects or surface textures using GelSight sensor is done using fusion of camera-only images and GelSight captured images with permanent markings. Those images are fed to Convolutional Neural Networks (CNN) to classify objects. However, our novel approach in using low-cost GelSight sensor with UV markings, the 3D height map to 2D image conversion, and the additional non-Gelsight captured images for training the CNN can be eliminated. AlexNet and optical flow algorithm have been used for feature recognition of five coins without UV markings and shear/slip of the coin in GelSight with UV markings respectively. Our results on confusion matrix show that, on average coin recognition can reach 93.4% without UV markings using AlexNet. Therefore, our novel method of using GelSight with UV markings would be useful to recognize full/partial object, shear/slip, and force applied to the objects without any 3D image reconstruction.


Title: Highly Robust Visual Place Recognition Through Spatial Matching of CNN Features
Key Words: convolutional neural nets  image coding  image matching  image resolution  visual databases  image resolution  VGG16 CNN architecture  matching CNN features  query image  VGG16 Convolutional Neural Network architecture  Spatial Matching Visual Place Recognition  SSM-VPR  optimal image resolutions  Visualization  Robustness  Semantics  Task analysis  Histograms  Correlation  Simultaneous localization and mapping  Visual Place Recognition  Convolutional Neural Networks  SLAM  Loop Closure  Life-long Navigation 
Abstract: We revise, improve and extend the system previously introduced by us and named SSM-VPR (Semantic and Spatial Matching Visual Place Recognition), largely boosting its performance above the current state of the art. The system encodes images of places by employing the activations of different layers of a pre-trained, off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. It consists of two stages: given a query image of a place, (1) a list of candidates is selected from a database of places and (2) the candidates are geometrically compared with the query. The comparison is made by matching CNN features and, equally important, their spatial locations, selecting the best candidate as the recognized place. The performance of the system is maximized by finding optimal image resolutions during the second stage and by exploiting temporal correlation between consecutive frames in the employed datasets.


Title: A technical framework for human-like motion generation with autonomous anthropomorphic redundant manipulators
Key Words: human-robot interaction  learning (artificial intelligence)  manipulator kinematics  mechanical variables control  medical robotics  mobile robots  motion control  redundant manipulators  trajectory control  technical framework  motion generation  autonomous anthropomorphic redundant manipulators  co-bots  industrial settings  people assistance  robot motions  classic solutions  anthropomorphic movement generation  optimization procedures  neuroscientific literature  learning methods  motion variability  high dimensional datasets  human upper limb principal motion modes  functional analysis  robot trajectory optimization  redundant anthropomorphic kinematic architectures  human model  functional mode extraction  human trajectories  robotic manipulator  advanced human-robot interaction  industrial co-botics  human assistance  Kinematics  Manipulators  Trajectory  Computer architecture  Cost function 
Abstract: The need for users' safety and technology accept-ability has incredibly increased with the deployment of co-bots physically interacting with humans in industrial settings, and for people assistance. A well-studied approach to meet these requirements is to ensure human-like robot motions. Classic solutions for anthropomorphic movement generation usually rely on optimization procedures, which build upon hypotheses devised from neuroscientific literature, or capitalize on learning methods. However, these approaches come with limitations, e.g. limited motion variability or the need for high dimensional datasets. In this work, we present a technique to directly embed human upper limb principal motion modes computed through functional analysis in the robot trajectory optimization. We report on the implementation with manipulators with redundant anthropomorphic kinematic architectures - although dissimilar with respect to the human model used for functional mode extraction - via Cartesian impedance control. In our experiments, we show how human trajectories mapped onto a robotic manipulator still exhibit the main characteristics of human-likeness, e.g. low jerk values. We discuss the results with respect to the state of the art, and their implications for advanced human-robot interaction in industrial co-botics and for human assistance.


Title: An Intelligent Spraying System with Deep Learning-based Semantic Segmentation of Fruit Trees in Orchards
Key Words: agriculture  agrochemicals  cameras  crops  image capture  image classification  image segmentation  learning (artificial intelligence)  nozzles  sensor fusion  pear orchard  deep learning-based intelligent spraying system  fruit tree detection system  SegNet model  semantic segmentation structure  deep learning model  nozzle  data fusion  RGB-D camera  image capture  pesticides  environmental safety  image classification  Spraying  Cameras  Semantics  Image segmentation  Vegetation  Decoding  Machine learning 
Abstract: This study proposes an intelligent spraying system with semantic segmentation of fruit trees in a pear orchard. A fruit tree detection system was developed using the SegNet model, a semantic segmentation structure. The system is trained with images categorized into five distinct classes. The learned deep learning model performed with an accuracy of 83.79%. Further, we fusion depth data from an RGB-D camera to prevent the tree in the background from being detected. To operate the nozzles, each image captured from the camera is separated lengthwise into quarters and mapped to the nozzles. Then, the nozzle was opened when the area of fruit trees in each zone exceeded 20%. Two types of field experiments were performed in a pear orchard to verify the effectiveness of our system. From the results obtained, we can confirm the satisfactory performance of our deep learning-based intelligent spraying system. It is expected that the introduction of this system to actual farms will signicantly reduce the amount of pesticide used and will make the work environment safer for farmers.


Title: SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic
Key Words: control engineering computing  multi-agent systems  road traffic  road vehicles  telecommunication traffic  traffic engineering computing  SUMMIT  urban driving  massive mixed traffic  unregulated urban crowd  high-speed traffic participants  high-fidelity simulator  crowd-driving algorithms  open-source OpenStreetMap map database  multiagent motion prediction model  unregulated urban traffic  heterogeneous agents  autonomous driving simulation  realistic traffic behaviors  crowd-driving settings  Roads  Robot sensing systems  Context modeling  Planning  Automobiles  Geometry  Kinematics 
Abstract: Autonomous driving in an unregulated urban crowd is an outstanding challenge, especially, in the presence of many aggressive, high-speed traffic participants. This paper presents SUMMIT, a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physics and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control and planning, and end-to-end learning. We provide a context-aware planner together with benchmark scenarios and show that SUMMIT generates complex, realistic traffic behaviors in challenging crowd-driving settings.


Title: Reactive Control and Metric-Topological Planning for Exploration
Key Words: collision avoidance  graph theory  image sequences  mobile robots  navigation  robot vision  optic flow  insect visuomotor system  obstacle detection  topological graph  metric-topological planning  autonomous navigation  robotic platforms  bio-inspired reactive control  spatial decomposition  obstacle avoidance  Fourier residual analysis  image processing  continuous occupancy grid  graph edge  Optical feedback  Optical sensors  Optical imaging  Planning  Navigation  Harmonic analysis  Mathematical model  exploration  centering  control  mapping 
Abstract: Autonomous navigation in unknown environments with the intent of exploring all traversable areas is a significant challenge for robotic platforms. In this paper, a simple yet reliable method for exploring unknown environments is presented based on bio-inspired reactive control and metric-topological planning. The reactive control algorithm is modeled after the spatial decomposition of wide and small-field patterns of optic flow in the insect visuomotor system. Centering behaviour and small obstacle detection and avoidance are achieved through wide-field integration and Fourier residual analysis of instantaneous measured nearness respectively. A topological graph is estimated using image processing techniques on a continuous occupancy grid. Node paths are rapidly generated to navigate to the nearest unexplored edge in the graph. It is shown through rigorous field-testing that the proposed control and planning method is robust, reliable, and computationally efficient.


Title: Information Theoretic Active Exploration in Signed Distance Fields
Key Words: deterministic algorithms  mobile robots  optimisation  path planning  sensors  tree searching  robot sensing trajectories  autonomous TSDF mapping  TSDF uncertainty  sensor measurements  efficient uncertainty prediction  long-horizon optimization  deterministic tree-search algorithm  information gain  TSDF distribution  efficient planning  uninformative sensing trajectories  active TSDF mapping approach  simulated environments  information theoretic active exploration  occupancy mapping  mobile robot  truncated signed distance field  robot motion primitive sequences  branch-and-bound pruning  complex visibility constraints  Robot sensing systems  Trajectory  Measurement uncertainty  Standards  Uncertainty 
Abstract: This paper focuses on exploration and occupancy mapping of unknown environments using a mobile robot. While a truncated signed distance field (TSDF) is a popular, efficient, and highly accurate representation of occupancy, few works have considered optimizing robot sensing trajectories for autonomous TSDF mapping. We propose an efficient approach for maintaining TSDF uncertainty and predicting its evolution from potential future sensor measurements without actually receiving them. Efficient uncertainty prediction is critical for long-horizon optimization of potential sensing trajectories. We develop a deterministic tree-search algorithm that evaluates the information gain between the TSDF distribution and potential observations along sequences of robot motion primitives. Efficient planning is achieved by branch-and-bound pruning of uninformative sensing trajectories. The effectiveness of our active TSDF mapping approach is evaluated in several simulated environments with complex visibility constraints.


Title: Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure
Key Words: feature extraction  graph theory  image matching  image registration  image segmentation  learning (artificial intelligence)  legged locomotion  neural nets  optical radar  optimisation  pose estimation  robot vision  SLAM (robots)  stereo image processing  online LiDAR-SLAM  legged robot  robust registration  deep-learned loop closure  3D factor-graph LiDAR-SLAM system  industrial environments  point clouds  inertial-kinematic state estimator  ICP registration  loop proposal mechanism  deep learning method  odometry  loop closure factors  pose graph optimization  SLAM map  risk alignment prediction method  deeply learned feature-based loop closure detector  Laser radar  Legged locomotion  Three-dimensional displays  Simultaneous localization and mapping  Iterative closest point algorithm 
Abstract: In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.


Title: Voxel Map for Visual SLAM
Key Words: computer vision  feature extraction  image representation  image retrieval  SLAM (robots)  visual SLAM systems  camera field-of-view  voxel map representation  keyframe map  map points retrieval  simultaneous localization and mapping  Simultaneous localization and mapping  Three-dimensional displays  Cameras  Visualization  Cognition  Feature extraction  Task analysis 
Abstract: In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multi-camera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable.


Title: Exploration of 3D terrains using potential fields with elevation-based local distortions
Key Words: mobile robots  optical radar  radar receivers  stereo image processing  terrain mapping  exploration approach  potential fields  uneven terrains  high declivity regions  ground robot  elevation-based local distortions  mobile robots  numerous outdoor tasks  military applications  3D terrain exploration  patrolling application  delivery application  2D LIDAR sensors  Distortion  Robot sensing systems  Three-dimensional displays  Boundary conditions  Two dimensional displays 
Abstract: Mobile robots can be used in numerous outdoor tasks such as patrolling, delivery and military applications. In order to deploy mobile robots in this kind of environment, where there are different challenges like slopes, elevations, or even holes, they should be able to detect such challenges and determine the best path to accomplish their tasks. In this paper, we are proposing an exploration approach based on potential fields with local distortions, in which we define preferences in uneven terrains to avoid high declivity regions without compromising the best path. The approach was implemented and tested in simulated environments, considering a ground robot embedded with two 2D LIDAR sensors, and the experiments demonstrated the efficiency of our method.


Title: CMTS: A Conditional Multiple Trajectory Synthesizer for Generating Safety-Critical Driving Scenarios
Key Words: Bayes methods  data analysis  mobile robots  road safety  road vehicles  safety-critical software  trajectory control  CMTS  conditional multiple trajectory synthesizer  safety-critical driving scenarios  naturalistic driving trajectory generation  autonomous driving algorithms  collision-free scenarios  safety-critical cases  near-miss scenarios  off-the-shelf datasets  generative model  conditional probability  trajectory predictions  autonomous vehicle safety  safety-critical data synthesizing framework  variational Bayesian methods  Trajectory  Interpolation  Roads  Training  Aerospace electronics  Data models  Autonomous vehicles 
Abstract: Naturalistic driving trajectory generation is crucial for the development of autonomous driving algorithms. However, most of the data is collected in collision-free scenarios leading to the sparsity of the safety-critical cases. When considering safety, testing algorithms in near-miss scenarios that rarely show up in off-the-shelf datasets and are costly to accumulate is a vital part of the evaluation. As a remedy, we propose a safety-critical data synthesizing framework based on variational Bayesian methods and term it as Conditional Multiple Trajectory Synthesizer (CMTS). We extend a generative model to connect safe and collision driving data by representing their distribution in the latent space and use conditional probability to adapt to different maps. Sampling from the mixed distribution enables us to synthesize the safety-critical data not shown in the safe or collision datasets. Experimental results demonstrate that the generated dataset covers many different realistic scenarios, especially the near-misses. We conclude that the use of data generated by CMTS can improve the accuracy of trajectory predictions and autonomous vehicle safety.


Title: LiDAR Inertial Odometry Aided Robust LiDAR Localization System in Changing City Scenes
Key Words: distance measurement  graph theory  inertial systems  maximum likelihood estimation  motion estimation  optical radar  localization estimation  inertial LiDAR intensity  matching estimation  LiDAR localization system  environmental change detection method  kinematic estimation  frame-to-frame motion estimation  multiresolution occupancy grid based LiDAR inertial odometry  pose graph fusion framework  Apollo-SouthBay dataset  MAP estimation problem  Laser radar  Estimation  Robustness  Roads  Windows  Optimization  Autonomous vehicles 
Abstract: Environmental fluctuations pose crucial challenges to a localization system in autonomous driving. We present a robust LiDAR localization system that maintains its kinematic estimation in changing urban scenarios by using a dead reckoning solution implemented through a LiDAR inertial odometry. Our localization framework jointly uses information from complementary modalities such as global matching and LiDAR inertial odometry to achieve accurate and smooth localization estimation. To improve the performance of the LiDAR odometry, we incorporate inertial and LiDAR intensity cues into an occupancy grid based LiDAR odometry to enhance frame-to-frame motion and matching estimation. Multi-resolution occupancy grid is implemented yielding a coarse-to-fine approach to balance the odometry's precision and computational requirement. To fuse both the odometry and global matching results, we formulate a MAP estimation problem in a pose graph fusion framework that can be efficiently solved. An effective environmental change detection method is proposed that allows us to know exactly when and what portion of the map requires an update. We comprehensively validate the effectiveness of the proposed approaches using both the Apollo-SouthBay dataset and our internal dataset. The results confirm that our efforts lead to a more robust and accurate localization system, especially in dynamically changing urban scenarios.


Title: Kidnapped Radar: Topological Radar Localisation using Rotationally-Invariant Metric Learning
Key Words: convolutional neural nets  CW radar  feature extraction  FM radar  learning (artificial intelligence)  nearest neighbour methods  radar imaging  radar tracking  polar nature  radar scan formation  cylindrical convolutions  anti-aliasing blurring  azimuth-wise max-pooling  rotational invariance  enforced metric space  topological localisation system  random rotational perturbation  kidnapped radar  topological radar localisation  rotationally-invariant metric learning  large-scale topological localisation  frequency-modulated continuous-wave scanning radar  efficient learning-based approach  radar data  polar radar scans  NetVLAD architectures  visual domain  feature extraction  radar-focused mobile autonomy dataset  CNN architectures  reference trajectory  nearest neighbour  place recognition  root architecture  convolutional neural network  distance 280.0 km  Measurement  Radar imaging  Robot sensing systems  Azimuth  Feature extraction  Trajectory  radar  localisation  place recognition  deep learning  metric learning 
Abstract: This paper presents a system for robust, large-scale topological localisation using Frequency-Modulated Continuous-Wave scanning radar which extends the state-of-the-art by an efficient, learning-based approach to handle radar data for localisation. We learn a metric space for embedding polar radar scans using CNN and NetVLAD architectures traditionally applied to the visual domain. However, we tailor the feature extraction for more suitability to the polar nature of radar scan formation using cylindrical convolutions, anti-aliasing blurring, and azimuth-wise max-pooling; all in order to bolster the rotational invariance. The enforced metric space is then used to encode a reference trajectory, serving as a map, which is queried for nearest neighbour for recognition of places at run-time. We demonstrate the performance of our topological localisation system over the course of many repeat forays using the largest radar-focused mobile autonomy dataset released to date, totalling 280 km of urban driving, a small portion of which we also use to learn the weights of the modified architecture. As this work represents a novel application for radar, we analyse the utility of the proposed method via a comprehensive set of metrics which provide insight into the efficacy when used in a realistic system, showing improved performance over the root architecture even in the face of random rotational perturbation.


Title: Global visual localization in LiDAR-maps through shared 2D-3D embedding space
Key Words: image recognition  learning (artificial intelligence)  mobile robots  neural nets  optical radar  robot vision  SLAM (robots)  global visual localization  LiDAR-maps  place recognition  autonomous driving field  vision-based approaches  image database  high definition 3D maps  deep neural network  shared embedding space  3D-LiDAR place recognition  3D DNN  2D-3D embedding space  Oxford Robotcar Dataset  image w.r.t.  Three-dimensional displays  Task analysis  Laser radar  Feature extraction  Visualization  Robots  Two dimensional displays 
Abstract: Global localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using the Oxford Robotcar Dataset, which encompasses a wide range of weather and light conditions.


Title: LOL: Lidar-only Odometry and Localization in 3D point cloud maps*
Key Words: distance measurement  image enhancement  image matching  image segmentation  object detection  object recognition  optical radar  LOL system  3D point cloud maps  Lidar-equipped vehicles  3D point segment matching method  Lidar-only odometry and localization algorithm  Kitti datasets  Three-dimensional displays  Laser radar  Sensors  Iterative closest point algorithm  Image color analysis  Trajectory  Real-time systems 
Abstract: In this paper we deal with the problem of odometry and localization for Lidar-equipped vehicles driving in urban environments, where a premade target map exists to localize against. In our problem formulation, to correct the accumulated drift of the Lidar-only odometry we apply a place recognition method to detect geometrically similar locations between the online 3D point cloud and the a priori offline map. In the proposed system, we integrate a state-of-the-art Lidaronly odometry algorithm with a recently proposed 3D point segment matching method by complementing their advantages. Also, we propose additional enhancements in order to reduce the number of false matches between the online point cloud and the target map, and to refine the position estimation error whenever a good match is detected. We demonstrate the utility of the proposed LOL system on several Kitti datasets of different lengths and environments, where the relocalization accuracy and the precision of the vehicle's trajectory were significantly improved in every case, while still being able to maintain real-time performance.


Title: Fast and accurate intracorporeal targeting through an anatomical orifice exhibiting unknown behavior.
Key Words: adaptive control  biomechanics  biomedical equipment  force control  manipulators  medical robotics  motion control  position control  surgery  anatomical orifice  minimally invasive surgery  interaction forces  patient anatomy  orifice behavior  adaptive control scheme  wrist velocity  tip velocity  intracorporeal targeting  instrument tip positioning  3DOF wrist center positioning problem  Robots  Instruments  Wrist  Surgery  Force  Adaptation models  Kinematics 
Abstract: Surgery may involve precise instrument tip positioning in a minimally invasive way. During these operations, the instrument is inserted in the body through an orifice. The movements of the instrument are constrained by interaction forces arising at the orifice level. The physical constraints may drastically vary depending on the patient's anatomy. This introduces uncertainties that challenge the positioning task for a robot. Indeed, it raises an antagonism: On one side, the required precision appeals for a rigid behavior. On the other side, forces applied at the entry point should be limited, which requires softness. In this paper we choose to minimize forces at the orifice by using a passive ball joint wrist to manipulate the instrument. From a control perspective, this leads to consider the task as a 3 DOF wrist center positioning problem, whose softness can be achieved through conventional low impedance control. However, positioning the wrist center, even with a high static precision, does not allow to achieve a high precision of the instrument tip positioning when the orifice behavior is not known. To cope with this problem, we implement a controller that servos the tip position by commanding the wrist position. In order to deal with uncertainties, we exploit an adaptive control scheme that identifies in real-time the unknown mapping between the wrist velocity and the tip velocity. Both simulations and in vitro experimental results show the efficiency of the control law.


Title: Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras
Key Words: calibration  cameras  image colour analysis  image representation  image sensors  learning (artificial intelligence)  pose estimation  robot vision  stereo image processing  multiple depth sensors  imperfect camera calibration  uncalibrated cameras  camera-views  single view robotic agents  voxel grid  relative pose estimation  3D scene representations  registered output  explicit 3D representations  sensor dropout  insertion tasks  task performance  multicamera approach  uncalibrated RGB camera  precise manipulation tasks  closed-loop end-to-end learning  multiview approach  multiple uncalibrated cameras  precise 3D manipulation  Task analysis  Cameras  Robot vision systems  Three-dimensional displays  Robot kinematics 
Abstract: In this work, we present an effective multi-view approach to closed-loop end-to-end learning of precise manipulation tasks that are 3D in nature. Our method learns to accomplish these tasks using multiple statically placed but uncalibrated RGB camera views without building an explicit 3D representation such as a pointcloud or voxel grid. This multi-camera approach achieves superior task performance on difficult stacking and insertion tasks compared to single-view baselines. Single view robotic agents struggle from occlusion and challenges in estimating relative poses between points of interest. While full 3D scene representations (voxels or pointclouds) are obtainable from registered output of multiple depth sensors, several challenges complicate operating off such explicit 3D representations. These challenges include imperfect camera calibration, poor depth maps due to object properties such as reflective surfaces, and slower inference speeds over 3D representations compared to 2D images. Our use of static but uncalibrated cameras does not require camera-robot or camera-camera calibration making the proposed approach easy to setup and our use of sensor dropout during training makes it resilient to the loss of camera-views after deployment.


Title: Learning Affordance Space in Physical World for Vision-based Robotic Object Manipulation
Key Words: image texture  learning (artificial intelligence)  manipulators  neural nets  probability  robot vision  pixel-wise probability affordance map  image space  world space  viewpoints  multiple-object pushing  multiple-object grasping  physical world  vision-based robotic object manipulation  Affordance Space Perception Network  deep neural network  3D affordance space  training strategy  task-agnostic framework  singular-object pushing  singular-object grasping  Robots  Task analysis  Robustness  Grasping  Data models  Calibration  Adaptation models 
Abstract: What is a proper representation for objects in manipulation? What would human try to perceive when manipulating a new object in a new environment? In fact, instead of focusing on the texture and illumination, human can infer the "affordance" [36] of the objects from vision. Here "affordance" describes the object's intrinsic property that affords a particular type of manipulation. In this work, we investigate whether such affordance can be learned by a deep neural network. In particular, we propose an Affordance Space Perception Network (ASPN) that takes an image as input and outputs an affordance map. Different from existing works that infer the pixel-wise probability affordance map in image space, our affordance is defined in the real world space, thus eliminates the need of hand-eye calibration. In addition, we extend the representation ability of affordance by defining it in a 3D affordance space and propose a novel training strategy to improve the performance. Trained purely with simulation data, ASPN can achieve significant performance in the real world. It is a task-agnostic framework and can handle different objects, scenes and viewpoints. Extensive real-world experiments demonstrate the accuracy and robustness of our approach. We achieve the success rates of 94.2% for singular-object pushing and 92.4% for multiple-object pushing. We also achieve the success rates of 97.2% for singular-object grasping and 95.4% for multiple-object grasping, which outperform current state-of-the-art methods.


Title: Arm-hand motion-force coordination for physical interactions with non-flat surfaces using dynamical systems: Toward compliant robotic massage
Key Words: biomechanics  dexterous manipulators  force control  motion control  path planning  regression analysis  support vector machines  unified motion-force control approach  human limb  compliant robotic massage  dynamical system approach  skin surface  robot fingers  complexity increases  manipulation tasks  dynamical systems  nonflat surface  physical interactions  arm-hand motion-force coordination  desired motion patterns  unknown surface  mannequin arm  Allegro robotic hand  KUKA IIWA robotic arm  robotic fingers  DS-based impedance control  desired motions  distance-to-surface mapping  Surface impedance  Robot kinematics  Task analysis  Force  Manipulators  Thumb 
Abstract: Many manipulation tasks require coordinated motions for arm and fingers. Complexity increases when the task requires to control for the force at contact against a non-flat surface; This becomes even more challenging when this contact is done on a human. All these challenges are regrouped when one, for instance, massages a human limb. When massaging, the robotic arm is required to continuously adapt its orientation and distance to the limb while the robot fingers exert desired patterns of forces and motion on the skin surface. To address these challenges, we adopt a Dynamical System (DS) approach that offers a unified motion-force control approach and enables to easily coordinate multiple degrees of freedom. As each human limb may slightly differ, we learn a model of the surface using support vector regression (SVR) which enable us to obtain a distance-to-surface mapping. The gradient of this mapping, along with the DS, generates the desired motions for the interaction with the surface. A DS-based impedance control for the robotic fingers allows to control separately for force along the normal direction of the surface while moving in the tangential plane. We validate our approach using the KUKA IIWA robotic arm and Allegro robotic hand for massaging a mannequin arm covered with a skin-like material. We show that our approach allows for 1) reactive motion planning to reach for an unknown surface, 2) following desired motion patterns on the surface, and 3) exerting desired interaction forces profiles. Our results show the effectiveness of our approach; especially the robustness toward uncertainties for shape and the given location of the surface.


Title: Differentiable Mapping Networks: Learning Structured Map Representations for Sparse Visual Localization
Key Words: gradient methods  image representation  learning (artificial intelligence)  neural net architecture  particle filtering (numerical methods)  robot vision  DMN architecture  end-to-end differentiable  sparse visual localization  end-to-end learning  differentiable mapping network  spatially structured view-embedding map  subsequent visual localization  learning structured map representations  Street View dataset  particle filter  gradient descent  robotics  neural network architecture  Task analysis  Visualization  Feature extraction  Neural networks  Robot kinematics  Three-dimensional displays 
Abstract: Mapping and localization, preferably from a small number of observations, are fundamental tasks in robotics. We address these tasks by combining spatial structure (differentiable mapping) and end-to-end learning in a novel neural network architecture: the Differentiable Mapping Network (DMN). The DMN constructs a spatially structured view-embedding map and uses it for subsequent visual localization with a particle filter. Since the DMN architecture is end-to-end differentiable, we can jointly learn the map representation and localization using gradient descent. We apply the DMN to sparse visual localization, where a robot needs to localize in a new environment with respect to a small number of images from known viewpoints. We evaluate the DMN using simulated environments and a challenging real-world Street View dataset. We find that the DMN learns effective map representations for visual localization. The benefit of spatial structure increases with larger environments, more viewpoints for mapping, and when training data is scarce. Project website: https://sites.google.com/view/differentiable-mapping.


Title: Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using Video Demonstration
Key Words: convolutional neural nets  feature extraction  image representation  learning (artificial intelligence)  video signal processing  task-specific objects  intended task  imitation learning  video demonstration  end-to-end self-supervised feature representation network  video-based task imitation  multilevel spatial attention module  spatial features  weighted combination  multiple intermediate feature maps  respective feature maps  metric learning loss  multiple view points  AT-Net features  reinforcement learning problem  attentive task-net  self supervised task-attention network  neural connections  learning task-specific feature embeddings  temporally consecutive frames  publicly available multiview pouring dataset  RL agent  Gazebo simulator  CNN pipeline  Task analysis  Measurement  Robots  Feature extraction  Training  Visualization  Pipelines 
Abstract: This paper proposes an end-to-end self-supervised feature representation network named Attentive Task-Net or AT-Net for video-based task imitation. The proposed AT-Net incorporates a novel multi-level spatial attention module to highlight spatial features corresponding to the intended task demonstrated by the expert. The neural connections in AT-Net ensure the relevant information in the demonstration is amplified and the irrelevant information is suppressed while learning task-specific feature embeddings. This is achieved by a weighted combination of multiple intermediate feature maps of the input image at different stages of the CNN pipeline. The weights of the combination are given by the compatibility scores, predicted by the attention module for respective feature maps. The AT-Net is trained using a metric learning loss which aims to decrease the distance between the feature representations of concurrent frames from multiple view points and increase the distance between temporally consecutive frames. The AT-Net features are then used to formulate a reinforcement learning problem for task imitation. Through experiments on the publicly available Multi-view pouring dataset, it is demonstrated that the output of the attention module highlights the task-specific objects while suppressing the rest of the background. The efficacy of the proposed method is further validated by qualitative and quantitative comparison with a state-of-the-art technique along with intensive ablation studies. The proposed method is implemented to imitate a pouring task where an RL agent is learned with the AT-Net in Gazebo simulator. Our findings show that the AT-Net achieves 6.5% decrease in alignment error along with a reduction in the number of training iterations by almost 155k over the state-of-the-art while satisfactorily imitating the intended task.


Title: Soft, Round, High Resolution Tactile Fingertip Sensors for Dexterous Robotic Manipulation
Key Words: dexterous manipulators  geometry  tactile sensors  tactile fingertip sensors  dexterous robotic manipulation  dexterous multifingered hands  illumination geometry  dexterous manipulation tasks  Three-dimensional displays  Robot sensing systems  Plastics  Geometry  Light emitting diodes  Cameras 
Abstract: High resolution tactile sensors are often bulky and have shape profiles that make them awkward for use in manipulation. This becomes important when using such sensors as fingertips for dexterous multi-fingered hands, where boxy or planar fingertips limit the available set of smooth manipulation strategies. High resolution optical based sensors such as GelSight have until now been constrained to relatively flat geometries due to constraints on illumination geometry. Here, we show how to construct a rounded fingertip that utilizes a form of light piping for directional illumination. Our sensors can replace the standard rounded fingertips of the Allegro hand. They can capture high resolution maps of the contact surfaces, and can be used to support various dexterous manipulation tasks.


Title: Object-oriented Semantic Graph Based Natural Question Generation
Key Words: convolutional neural nets  feature extraction  graph theory  learning (artificial intelligence)  natural language processing  object detection  recurrent neural nets  robot vision  sequential scenes  recurrent neural network  feature extraction  autonomous robots  graph convolutional network  object-oriented semantic graphs  semantic graph mapping  natural question generation  Semantics  Feature extraction  Object oriented modeling  Neural networks  Convolution  Autonomous robots 
Abstract: Generating a natural question can enable autonomous robots to propose problems according to their surroundings. However, recent studies on question generation rarely consider semantic graph mapping, which is widely used to understand environments. In this paper, we introduce a method to generate natural questions using object-oriented semantic graphs. First, a graph convolutional network extracts features from the graph. Then, a recurrent neural network generates the natural question from the extracted features. Using graphs, we can generate natural questions for both single and sequential scenes. The proposed method outperforms conventional methods on a publicly available dataset for single scenes and can generate questions for sequential scenes.


Title: Deep compositional robotic planners that follow natural language commands
Key Words: convolutional neural nets  mobile robots  natural language processing  path planning  deep compositional robotic planners  natural language commands  sampling-based robotic planner  continuous configuration space  complex command  sampling-based planner  recurrent hierarchical deep network  Robots  Task analysis  Planning  Natural languages  Aerospace electronics  Cognition  Space exploration 
Abstract: We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments.


Title: Visual Servoing-based Navigation for Monitoring Row-Crop Fields
Key Words: agricultural robots  agriculture  agrochemicals  crops  mobile robots  path planning  robot vision  visual servoing  visual servoing-based navigation  autonomous navigation  field robots  precision agriculture tasks  agrochemicals  visual-based navigation framework  crop-row structure  row-crop fields monitoring  Agriculture  Navigation  Cameras  Robot vision systems  Visualization  Monitoring 
Abstract: Autonomous navigation is a pre-requisite for field robots to carry out precision agriculture tasks. Typically, a robot has to navigate along a crop field multiple times during a season for monitoring the plants, for applying agrochemicals, or for performing targeted interventions. In this paper, we propose a visual-based navigation framework tailored to row-crop fields that exploits the regular crop-row structure present in fields. Our approach uses only the images from on-board cameras without the need for performing explicit localization or maintaining a map of the field. Thus, it can operate without expensive RTK-GPS solutions often used in agricultural automation systems. Our navigation approach allows the robot to follow the crop rows accurately and handles the switch to the next row seamlessly within the same framework. We implemented our approach using C++ and ROS and thoroughly tested it in several simulated fields with different shapes and sizes. We also demonstrated the system running at frame-rate on an actual robot operating on a test row-crop field. The code and data have been published.


Title: A Novel Calibration Method between a Camera and a 3D LiDAR with Infrared Images
Key Words: calibration  cameras  image filtering  infrared imaging  optical radar  3D LiDAR  infrared images  infrared filter  calibration method  simultaneous location and mapping  Velodyne VLP-16 sensor  Conferences  Automation 
Abstract: Fusions of LiDARs (light detection and ranging) and cameras have been effectively and widely employed in the communities of autonomous vehicles, virtual reality and mobile mapping systems (MMS) for different purposes, such as localization, high definition map or simultaneous location and mapping. However, the extrinsic calibration between a camera and a 3D LiDAR is a fundamental prerequisite to guarantee its performance. Some previous methods are inaccurate, have calibration error that is several times the beam divergence, and often require special calibration objects, thereby limiting their ubiquitous use for calibration. To overcome these shortcomings, we propose a novel and high-accuracy method for the extrinsic calibration between a camera and a 3D LiDAR. Our approach relies on the infrared images from a camera with an infrared filter, and the 2D-3D corresponding points in a scene with the corners of a wall can be extracted to calculate the six extrinsic parameters. Experiments using the Velodyne VLP-16 sensor show that the method can achieve an extrinsic accuracy at the level of the beam divergence, which is fully analyzed and validated from two different aspects. Therefore, the calibration method in this paper is highly accurate, effective and does not require special complicated calibration objects; thus, it meets the requirements of practical applications.


Title: OneShot Global Localization: Instant LiDAR-Visual Pose Estimation
Key Words: cameras  feature extraction  image matching  image segmentation  image sequences  learning (artificial intelligence)  neural nets  optical radar  pose estimation  robot vision  learning-based descriptors  point cloud segments  degree-of-freedom  LiDAR scan  neural network architecture  segment retrieval rates  LiDAR-only description  OneShot global localization  autonomous navigation tasks  LiDAR-visual pose estimation  LiDAR-only approach  degree-of-freedom pose  NCLT dataset  Three-dimensional displays  Laser radar  Robots  Sensors  Image segmentation  Neural networks  Visualization 
Abstract: Globally localizing in a given map is a crucial ability for robots to perform a wide range of autonomous navigation tasks. This paper presents OneShot - a global localization algorithm that uses only a single 3D LiDAR scan at a time, while outperforming approaches based on integrating a sequence of point clouds. Our approach, which does not require the robot to move, relies on learning-based descriptors of point cloud segments and computes the full 6 degree-of-freedom pose in a map. The segments are extracted from the current LiDAR scan and are matched against a database using the computed descriptors. Candidate matches are then verified with a geometric consistency test. We additionally present a strategy to further improve the performance of the segment descriptors by augmenting them with visual information provided by a camera. For this purpose, a custom-tailored neural network architecture is proposed. We demonstrate that our LiDAR-only approach outperforms a state-of-the-art baseline on a sequence of the KITTI dataset and also evaluate its performance on the challenging NCLT dataset. Finally, we show that fusing in visual information boosts segment retrieval rates by up to 26% compared to LiDAR-only description.


Title: Cross-context Visual Imitation Learning from Demonstrations
Key Words: learning (artificial intelligence)  robots  general imitation learning method  robotic system  context translation model  depth prediction model  multimodal inverse dynamics model  depth observation  inverse model maps  multimodal observations  cross-context learning advantage  cross-context visual imitation learning  color observation  block stacking tasks  Context modeling  Robots  Task analysis  Inverse problems  Visualization  Predictive models  Feature extraction 
Abstract: Imitation learning enables robots to learn a task by simply watching the demonstration of the task. Current imitation learning methods usually require the learner and demonstrator to occur in the same context. This limits their scalability to practical applications. In this paper, we propose a more general imitation learning method which allows the learner and the demonstrator to come from different contexts, such as different viewpoints, backgrounds, and object positions and appearances. Specifically, we design a robotic system consisting of three models: context translation model, depth prediction model and multi-modal inverse dynamics model. First, the context translation model translates the demonstration to the context of learner from a different context. Then combining the color observation and depth observation as inputs, the inverse model maps the multi-modal observations into actions to reproduce the demonstration, where the depth observation is provided by a depth prediction model. By performing the block stacking tasks both in simulation and real world, we prove the cross-context learning advantage of the proposed robotic system over other systems.


Title: Fast Local Planning and Mapping in Unknown Off-Road Terrain
Key Words: collision avoidance  graph theory  mobile robots  motion control  remotely operated vehicles  SLAM (robots)  trajectory control  off-road terrain  on-line mapping  planning solution  obstacle detection  terrain gradient map  simple cost map  adaptable cost map  optimal paths  control input space  kinematic forward simulation  generated feasible trajectories  optimal trajectory  time operation  frequency 10.0 Hz  frequency 30.0 Hz  Trajectory  Robots  Planning  Aerospace electronics  Microsoft Windows  Three-dimensional displays  Real-time systems 
Abstract: In this paper, we present a fast, on-line mapping and planning solution for operation in unknown, off-road, environments. We combine obstacle detection along with a terrain gradient map to make simple and adaptable cost map. This map can be created and updated at 10 Hz. An A* planner finds optimal paths over the map. Finally, we take multiple samples over the control input space and do a kinematic forward simulation to generated feasible trajectories. Then the most optimal trajectory, as determined by the cost map and proximity to A* path, is chosen and sent to the controller. Our method allows real time operation at rates of 30 Hz. We demonstrate the efficiency of our method in various off-road terrain at high speed.


Title: An Actor-Critic Approach for Legible Robot Motion Planner
Key Words: human-robot interaction  learning (artificial intelligence)  mobile robots  path planning  recurrent neural nets  actor-critic approach  legible robot motion planner  human-robot collaboration  human partners  mutual learning  legibility evaluator  policy network  deep reinforcement learning  sequence model  Seq2Seq  motion predictor  maps motion  legible reward  human-subject experiments  real-human data  recurrent neural networks based sequence  Task analysis  Robot motion  Robot kinematics  Biological neural networks  Trajectory  Training 
Abstract: In human-robot collaboration, it is crucial for the robot to make its intentions clear and predictable to the human partners. Inspired by the mutual learning and adaptation of human partners, we suggest an actor-critic approach for a legible robot motion planner. This approach includes two neural networks and a legibility evaluator: 1) A policy network based on deep reinforcement learning (DRL); 2) A Recurrent Neural Networks (RNNs) based sequence to sequence (Seq2Seq) model as a motion predictor; 3) A legibility evaluator that maps motion to legible reward. Through a series of human-subject experiments, we demonstrate that with a simple handicraft function and no real-human data, our method lead to improved collaborative performance against a baseline method and a non-prediction method.


Title: Planetary Rover Exploration Combining Remote and In Situ Measurements for Active Spectroscopic Mapping
Key Words: aerospace robotics  learning (artificial intelligence)  mobile robots  path planning  planetary rovers  planetary rover exploration combining remote  active spectroscopic mapping  planetary rover missions  heavy reliance  ground control  real-time information  autonomous mapping  exploration approach  planetary rovers  machine learning model  rover measurements  spectroscopic data  information theory  nonmyopic path  exploration productivity  actual rover  Feature extraction  Extraterrestrial measurements  Robot kinematics  Adaptation models  Productivity  Mars 
Abstract: Maintaining high levels of productivity for planetary rover missions is very difficult due to limited communication and heavy reliance on ground control. There is a need for autonomy that enables more adaptive and efficient actions based on real-time information. This paper presents an autonomous mapping and exploration approach for planetary rovers. We first describe a machine learning model that actively combines remote and rover measurements for mapping. We focus on spectroscopic data because they are commonly used to investigate surface composition. We then incorporate notions from information theory and non-myopic path planning to improve exploration productivity. Finally, we demonstrate the feasibility and successful performance of our approach via spectroscopic investigations of Cuprite, Nevada; a well-studied region of mineralogical and geological interest. We first perform a detailed analysis in simulations, and then validate those results with an actual rover in the field in Nevada.


Title: AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM
Key Words: calibration  cameras  sensitivity analysis  SLAM (robots)  calibration parameters  calibration sensitivity analysis  joint angle noise  joint angle values  calibration code  dynamic camera clusters  visual SLAM  time-varying set  extrinsic calibration transformations  DCC calibration accuracy  configuration space  pixel re-projection error  fiducial target  dynamic camera cluster  pose-loop error optimization  Cameras  Calibration  Robot vision systems  Simultaneous localization and mapping  Vehicle dynamics  Optimization  Measurement uncertainty 
Abstract: In order to relate information across cameras in a Dynamic Camera Cluster (DCC), an accurate time-varying set of extrinsic calibration transformations need to be determined. Previous calibration approaches rely solely on collecting measurements from a known fiducial target which limits calibration accuracy as insufficient excitation of the gimbal is achieved. In this paper, we improve DCC calibration accuracy by collecting measurements over the entire configuration space of the gimbal and achieve a 10X improvement in pixel re-projection error. We perform a joint optimization over the calibration parameters between any number of cameras and unknown joint angles using a pose-loop error optimization approach, thereby avoiding the need for overlapping fields-of-view. We test our method in simulation and provide a calibration sensitivity analysis for different levels of camera intrinsic and joint angle noise. In addition, we provide a novel analysis of the degenerate parameters in the calibration when joint angle values are unknown, which avoids situations in which the calibration cannot be uniquely recovered. The calibration code will be made available at https://github.com/TRAILab/AC-DCC.


Title: Error estimation and correction in a spiking neural network for map formation in neuromorphic hardware
Key Words: error correction  mobile robots  neural chips  neural nets  path planning  pose estimation  SLAM (robots)  error correction  SNN mechanism  neuromorphic device  form-factor neuromorphic chip  spiking neural network  map formation  neuromorphic hardware  neural networks  robot control  error estimation  simultaneous localization and mapping  robot pose estimation  SNN-based SLAM  path integration speed  Neurons  Robots  Sociology  Statistics  Light emitting diodes  Neuromorphics  Synapses 
Abstract: Neuromorphic hardware offers computing platforms for the efficient implementation of spiking neural networks (SNNs) that can be used for robot control. Here, we present such an SNN on a neuromorphic chip that solves a number of tasks related to simultaneous localization and mapping (SLAM): forming a map of an unknown environment and, at the same time, estimating the robot's pose. In particular, we present an SNN mechanism to detect and estimate errors when the robot revisits a known landmark and updates both the map and the path integration speed to reduce the error. The whole system is fully realized in a neuromorphic device, showing the feasibility of a purely SNN-based SLAM, which could be efficiently implemented in a small form-factor neuromorphic chip.


Title: Slip-Based Nonlinear Recursive Backstepping Path Following Controller for Autonomous Ground Vehicles
Key Words: compensation  control nonlinearities  convergence  feedback  feedforward  mobile robots  motion control  nonlinear control systems  observers  path planning  position control  robot dynamics  robot kinematics  robust control  stability  steering systems  variable structure systems  kinematic controller  feedforward slip compensation  variable structure controller  graceful motion  yaw rate commands  backstepping dynamic controller  robust steering commands  output feedback control  autonomous ground vehicles  vehicle steering control  graceful lateral motion  couples yaw-rate based path  steering angle  heading error  slip-based nonlinear recursive backstepping path following controller  observer based sideslip estimates  path following accuracy  error convergence  slip-based kinematic model  dynamic model  path following error  robustness  high gain observer  stability analysis  Kinematics  Vehicle dynamics  Backstepping  Tracking  Dynamics  Tires  Convergence 
Abstract: Path following accuracy and error convergence with graceful motion in vehicle steering control is challenging due to the competing nature of these requirements, especially across a range of operating speeds. This work is founded upon slip-based kinematic and dynamic models, which allow derivation of controllers considering error due to sideslip and the mapping between steering commands and graceful lateral motion. A novel recursive backstepping steering controller is proposed that better couples yaw-rate based path following commands to steering angle and rate. Observer based sideslip estimates are combined with heading error in the kinematic controller to provide feedforward slip compensation. Path following error is compensated by a Variable Structure Controller (VSC) to balance graceful motion, path following error, and robustness. Yaw rate commands are used by a backstepping dynamic controller to generate robust steering commands. A High Gain Observer (HGO) estimates sideslip and yaw rate for output feedback control. Stability analysis is provided and peaking is addressed. Field experimental results evaluate the work and provide comparisons to MPC.


Title: MulRan: Multimodal Range Dataset for Urban Place Recognition
Key Words: geophysical image processing  geophysical techniques  image recognition  mobile robots  object recognition  optical radar  radar imaging  robot vision  multimodal range dataset  radio detection and ranging  light detection and ranging  urban environment  range sensor-based place recognition  6D baseline trajectories  place recognition ground truth  image-format data  time-stamped 1D intensity arrays  polar images  image data  radar place recognition method  LiDAR  longer-range measurements  urban place recognition  MulRan  Laser radar  Radar imaging  Three-dimensional displays  Urban areas  Simultaneous localization and mapping 
Abstract: This paper introduces a multimodal range dataset namely for radio detection and ranging (radar) and light detection and ranging (LiDAR) specifically targeting the urban environment. By extending our workshop paper [1] to a larger scale, this dataset focuses on the range sensor-based place recognition and provides 6D baseline trajectories of a vehicle for place recognition ground truth. Provided radar data support both raw-level and image-format data, including a set of time-stamped 1D intensity arrays and 360Â° polar images, respectively. In doing so, we provide flexibility between raw data and image data depending on the purpose of the research. Unlike existing datasets, our focus is at capturing both temporal and structural diversities for range-based place recognition research. For evaluation, we applied and validated that our previous location descriptor and its search algorithm [2] are highly effective for radar place recognition method. Furthermore, the result shows that radar-based place recognition outperforms LiDAR-based one exploiting its longer-range measurements. The dataset is available from https://sites.google.com/view/mulran-pr.


Title: GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization
Key Words: cameras  optimisation  pose estimation  robot vision  SLAM (robots)  GPO  global plane optimization  homography estimation  homography decomposition  monocular SLAM initialization  monocular simultaneous localization and mapping problem  camera poses  chessboard dataset  Cameras  Simultaneous localization and mapping  Optimization  Matrix decomposition  Transmission line matrix methods  Estimation  Three-dimensional displays 
Abstract: Initialization is essential to monocular Simultaneous Localization and Mapping (SLAM) problems. This paper focuses on a novel initialization method for monocular SLAM based on planar features. The algorithm starts by homography estimation in a sliding window. It then proceeds to a global plane optimization (GPO) to obtain camera poses and the plane normal. 3D points can be recovered using planar constraints without triangulation. The proposed method fully exploits the plane information from multiple frames and avoids the ambiguities in homography decomposition. We validate our algorithm on the collected chessboard dataset against baseline implementations and present extensive analysis. Experimental results show that our method outperforms the ne-tuned baselines in both accuracy and real-time.


Title: Large-Scale Volumetric Scene Reconstruction using LiDAR
Key Words: cameras  image colour analysis  image reconstruction  image representation  optical radar  large-scale 3D scene reconstruction  autonomous driving  volumetric depth fusion  indoor applications  commodity RGB-D cameras  high reconstruction quality  LiDAR sensors  autonomous cars  large-scale mapping  urban area  meshed representation  real world application  large-scale volumetric scene reconstruction  distance 3.7 km  Three-dimensional displays  Laser radar  Image reconstruction  Graphics processing units  Sensor fusion  Weight measurement 
Abstract: Large-scale 3D scene reconstruction is an important task in autonomous driving and other robotics applications as having an accurate representation of the environment is necessary to safely interact with it. Reconstructions are used for numerous tasks ranging from localization and mapping to planning. In robotics, volumetric depth fusion is the method of choice for indoor applications since the emergence of commodity RGB-D cameras due to its robustness and high reconstruction quality. In this work we present an approach for volumetric depth fusion using LiDAR sensors as they are common on most autonomous cars. We present a framework for large-scale mapping of urban areas considering loop closures. Our method creates a meshed representation of an urban area from recordings over a distance of 3.7km with a high level of detail on consumer graphics hardware in several minutes. The whole process is fully automated and does not need any user interference. We quantitatively evaluate our results from a real world application. Also, we investigate the effects of the sensor model that we assume on reconstruction quality by using synthetic data.


Title: Topological Mapping for Manhattan-like Repetitive Environments
Key Words: convolutional neural nets  graph theory  image representation  optimisation  SLAM (robots)  topology  Manhattan properties  topological graph  unoptimized Pose Graph  topological Manhattan relations  ground-truth Pose Graph  real-world indoor warehouse scenes  Manhattan-like repetitive environments  topological mapping framework  neighbouring nodes  indoor warehouse setting  warehouse topological construct  deep convolutional network  Siamese-style neural network  backend pose graph optimization framework  Manhattan graph aided loop closure relations  Topology  Network topology  Simultaneous localization and mapping  Neural networks  Optimization 
Abstract: We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework.


Title: Aggressive Online Control of a Quadrotor via Deep Network Representations of Optimality Principles
Key Words: aircraft control  autonomous aerial vehicles  control engineering computing  helicopters  mobile robots  neural nets  optimisation  time optimal control  power optimality  time optimality  deep neural network  robotic applications  optimality principles  deep network representations  aggressive online control  time-optimal maneuvers  offline optimal control method  aggressive quadrotor control  Trajectory  Optimal control  Stability analysis  Neural networks  Delays  Training  Drones 
Abstract: Optimal control holds great potential to improve a variety of robotic applications. The application of optimal control on-board limited platforms has been severely hindered by the large computational requirements of current state of the art implementations. In this work, we make use of a deep neural network to directly map the robot states to control actions. The network is trained offline to imitate the optimal control computed by a time consuming direct nonlinear method. A mixture of time optimality and power optimality is considered with a continuation parameter used to select the predominance of each objective. We apply our networks (termed G&CNets) to aggressive quadrotor control, first in simulation and then in the real world. We give insight into the factors that influence the `reality gap' between the quadrotor model used by the offline optimal control method and the real quadrotor. Furthermore, we explain how we set up the model and the control structure on-board of the real quadrotor to successfully close this gap and perform time-optimal maneuvers in the real world. Finally, G&CNet's performance is compared to state-of-the-art differential-flatness-based optimal control methods. We show, in the experiments, that G&CNets lead to significantly faster trajectory execution due to, in part, the less restrictive nature of the allowed state-to-input mappings.


Title: Driving Style Encoder: Situational Reward Adaptation for General-Purpose Planning in Automated Driving
Key Words: learning (artificial intelligence)  path planning  predictive control  road traffic control  situational reward adaptation  general-purpose planning algorithms  automated driving  planning algorithm  driving kinematics  linear reward function  driving situation  deep learning approach  situation-dependent reward functions  sampled driving policies  driving style  planning cycle  Planning  Neural networks  Entropy  Kinematics  Machine learning  Tuning  Training 
Abstract: General-purpose planning algorithms for automated driving combine mission, behavior, and local motion planning. Such planning algorithms map features of the environment and driving kinematics into complex reward functions. To achieve this, planning experts often rely on linear reward functions. The specification and tuning of these reward functions is a tedious process and requires significant experience. Moreover, a manually designed linear reward function does not generalize across different driving situations. In this work, we propose a deep learning approach based on inverse reinforcement learning that generates situation-dependent reward functions. Our neural network provides a mapping between features and actions of sampled driving policies of a model-predictive control-based planner and predicts reward functions for upcoming planning cycles. In our evaluation, we compare the driving style of reward functions predicted by our deep network against clustered and linear reward functions. Our proposed deep learning approach outperforms clustered linear reward functions and is at par with linear reward functions with a-priori knowledge about the situation.


Title: Localising PMDs through CNN Based Perception of Urban Streets
Key Words: computer vision  convolutional neural nets  feature extraction  Kalman filters  learning (artificial intelligence)  nonlinear filters  object detection  robot vision  common environmental landmarks  point features  higher level information  common vision based approaches  low level hand  EKF framework  practical CNN  typical suburban streets  localiser  PMD  CNN based perception  urban streets  localisation scheme  complementary approaches  outdoor vision based localisation  convolutional neural networks  necessary perceptual information  camera images  lane markings  manhole covers  vector distance  binary image  ground surface boundaries  CNN based detection  novel extended Kalman filter  Feature extraction  Transforms  Cameras  Semantics  Two dimensional displays  Data mining  Three-dimensional displays 
Abstract: The main contribution of this paper is a novel Extended Kalman Filter (EKF) based localisation scheme that fuses two complementary approaches to outdoor vision based localisation. This EKF is aided by a front end consisting of two Convolutional Neural Networks (CNNs) that provide the necessary perceptual information from camera images. The first approach involves a CNN based extraction of information corresponding to artefacts such as curbs, lane markings, and manhole covers to localise on a vector distance transform representation of a binary image of these ground surface boundaries. The second approach involves a CNN based detection of common environmental landmarks such as tree trunks and light poles, which are represented as point features on a sparse map. Utilising CNNs to obtain higher level information about the environment enables this framework to avoid the typical pitfalls of common vision based approaches that use low level hand crafted features for localisation. The EKF framework makes it possible to deal with false positives and missed detections that are inevitable in a practical CNN, to produce a location estimate together with its associated uncertainty. Experiments using a Personal Mobility Device (PMD) driven in typical suburban streets are presented to demonstrate the effectiveness of the proposed localiser.


Title: Chance Constrained Simultaneous Path Planning and Task Assignment for Multiple Robots with Stochastic Path Costs
Key Words: distributed algorithms  graph theory  multi-robot systems  path planning  probability  stochastic processes  simultaneous path planning  multiple robots  stochastic path costs  stochastic edge costs  robot team  stochastic travel costs  chance-constrained simultaneous task assignment  deterministic simultaneous task assignment  shortest paths  task locations  linear assignment problem  CC-STAP  Robots  Task analysis  Collision avoidance  Path planning  Resource management  Random variables  Planning 
Abstract: We present a novel algorithm for simultaneous task assignment and path planning on a graph (or roadmap) with stochastic edge costs. In this problem, the initially unassigned robots and tasks are located at known positions in a roadmap. We want to assign a unique task to each robot and compute a path for the robot to go to its assigned task location. Given the mean and variance of travel cost of each edge, our goal is to develop algorithms that, with high probability, the total path cost of the robot team is below a minimum value in any realization of the stochastic travel costs. We formulate the problem as a chance-constrained simultaneous task assignment and path planning problem (CC-STAP). We prove that the optimal solution of CC-STAP can be obtained by solving a sequence of deterministic simultaneous task assignment and path planning problems in which the travel cost is a linear combination of mean and variance of the edge cost. We show that the deterministic problem can be solved in two steps. In the first step, robots compute the shortest paths to the task locations and in the second step, the robots solve a linear assignment problem with the costs obtained in the first step. We also propose a distributed algorithm that solves CC-STAP near-optimally. We present simulation results on randomly generated networks and data to demonstrate that our algorithm is scalable with the number of robots (or tasks) and the size of the network.


Title: Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM
Key Words: data visualisation  image representation  learning (artificial intelligence)  mobile robots  SLAM (robots)  view-based labelling  spatial AI systems  real-time height map fusion  map-based labelling  generated scene model  input view-wise data  estimate labels  clear groups  labelling scenes  semantic labels  geometric models  persistent scene representations  real-time SLAM  map-based semantic labelling  Labeling  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Image reconstruction  Real-time systems 
Abstract: Generally capable Spatial AI systems must build persistent scene representations where geometric models are combined with meaningful semantic labels. The many approaches to labelling scenes can be divided into two clear groups: view-based which estimate labels from the input view-wise data and then incrementally fuse them into the scene model as it is built; and map-based which label the generated scene model. However, there has so far been no attempt to quantitatively compare view-based and map-based labelling. Here, we present an experimental framework and comparison which uses real-time height map fusion as an accessible platform for a fair comparison, opening up the route to further systematic research in this area.


Title: Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans
Key Words: collision avoidance  control engineering computing  learning (artificial intelligence)  mobile robots  navigation  path planning  robot dynamics  robot programming  time-efficient path planning behavior  dynamic crowds  social-safety-awareness  reinforcement learning  2D laser scans  mapless collision-avoidance navigation  ego-safety  pedestrians  robot tests  Collision avoidance  Navigation  Training  Robot sensing systems  Lasers  Path planning 
Abstract: We consider the problem of mapless collision-avoidance navigation where humans are present using 2D laser scans. Our proposed method uses ego-safety to measure collision from the robot's perspective and social-safety to measure the impact of robot's actions on surrounding pedestrians. Specifically, the social-safety part predicts the intrusion impact of the robot's action into the interaction area with surrounding humans. We train the policy using reinforcement learning on a simple simulator and directly evaluate the learned policy in Gazebo and real robot tests. Experiments show the learned policy smoothly transferred to different scenarios without any fine tuning. We observe that our method demonstrates time-efficient path planning behavior with high success rate in the mapless navigation task. Furthermore, we test our method in a navigation task among dynamic crowds, considering both low and high volume traffic. Our learned policy demonstrates cooperative behavior that actively drives our robot into traffic flows while showing respect to nearby pedestrians. Evaluation videos are at https://sites.google.com/view/ssw-batman.


Title: Bioinspired object motion filters as the basis of obstacle negotiation in micro aerial systems
Key Words: aerospace robotics  collision avoidance  feedback  image filtering  image motion analysis  image sequences  microrobots  object detection  robot vision  object motion filters  obstacle negotiation  microaerial systems  biological visual guidance  machine vision system  motion vision  dense optic flow map  insect vision inspired object motion filter model  microracing drone  proximaldistal object separation  early-stage motion detection  feedback control loop  Visualization  Kernel  Optical filters  Drones  Band-pass filters  Biological system modeling  Insects  motion vision  obstacle avoidance  visual guidance 
Abstract: All animals and robots that move in the world must navigate to a goal while clearing obstacles. Using vision to accomplish such task has several advantages in cost and payload, which explains the prevalence of biological visual guidance. However, the computational overhead has been an obvious concern when increasing number of pixels and frames that need to be analyzed in real-time for a machine vision system. The use of motion vision and optic flow has been a popular bio-inspired solution for this problem. However, many early-stage motion detection approaches rely on special hardware (e.g. event-cameras) or extensive computation (e.g. dense optic flow map). Here we demonstrate a method to combine an insect vision inspired object motion filter model with simple visual guidance rules to fly through a cluttered environment. We have implemented a complete feedback control loop in a micro racing drone and achieved proximaldistal object separation through only two object motion filters. We discuss the key constraints and the scalability of this approach for future development.


Title: Analytical Expressions of Serial Manipulator Jacobians and their High-Order Derivatives based on Lie Theory*
Key Words: approximation theory  end effectors  iterative methods  Jacobian matrices  Lie algebras  manipulator kinematics  manipulator Jacobian  high-order derivatives  Lie theory  higher-order derivatives  higher-order Jacobian derivatives  serial manipulator kinematics  joint variables  joint-space coordinates  serial manipulator Jacobians  task-space Cartesian coordinates  inertial-fixed frames  body-fixed frames  KUKA LRB iiwa7 R800 manipulator  Jacobian matrices  Manipulators  Kinematics  Acceleration  Fasteners 
Abstract: Serial manipulator kinematics provide a mapping between joint variables in joint-space coordinates, and end-effector configurations in task-space Cartesian coordinates. Velocity mappings are represented via the manipulator Jacobian produced by direct differentiation of the forward kinematics. Acquisition of acceleration, jerk, and snap expressions, typically utilized for accurate trajectory-tracking, requires the computation of high-order Jacobian derivatives. As compared to conventional numerical/D-H approaches, this paper proposes a novel methodology to derive the Jacobians and their high-order derivatives symbolically, based on Lie theory, which requires that the derivatives are calculated with respect to each joint variable and time. Additionally, the technique described herein yields a mathematically sound solution to the high-order Jacobian derivatives, which distinguishes it from other relevant works. Performing computations with respect to the two inertial-fixed and body-fixed frames, the analytical form of the spatial and body Jacobians are derived, as well as their higher-order derivatives, without resorting to any approximations, whose expressions would depend explicitly on the joint state and the choice of reference frames. The proposed method provides more tractable computation of higher-order Jacobian derivatives, while its effectiveness has been verified by conducting a comparative analysis based on experimental data extracted from a KUKA LRB iiwa7 R800 manipulator.


Title: Automatic tool for Gazebo world construction: from a grayscale image to a 3D solid model
Key Words: control engineering computing  laser ranging  mobile robots  SLAM (robots)  solid modelling  Gazebo world construction  grayscale image  3D solid model  robot simulators  simulated physical environment  2D image  2D laser range finder data  Gazebo simulator  3D Collada  simultaneous localization and mapping  real-time factor  SLAM missions  RTF  Tools  Solid modeling  Three-dimensional displays  Robot sensing systems  Gray-scale  Collision avoidance 
Abstract: Robot simulators provide an easy way for evaluation of new concepts and algorithms in a simulated physical environment reducing development time and cost. Therefore it is convenient to have a tool that quickly creates a 3D landscape from an arbitrary 2D image or 2D laser range finder data. This paper presents a new tool that automatically constructs such landscapes for Gazebo simulator. The tool converts a grayscale image into a 3D Collada format model, which could be directly imported into Gazebo. We run three different simultaneous localization and mapping (SLAM) algorithms within three varying complexity environments that were constructed with our tool. A real-time factor (RTF) was used as an efficiency benchmark. Successfully completed SLAM missions with acceptable RTF levels demonstrated the efficiency of the tool. The source code is available for free academic use.


Title: Toward Sim-to-Real Directional Semantic Grasping
Key Words: control engineering computing  end effectors  grippers  image colour analysis  learning (artificial intelligence)  rendering (computer graphics)  robot vision  directional semantic grasping  deep reinforcement learning  double deep Q-network  robot simulator  rendering  monocular RGB images  wrist mounted camera  cartesian robot control  crossentropy method  domain randomization  end effector  Grippers  Grasping  Cameras  Training  Robot vision systems 
Abstract: We address the problem of directional semantic grasping, that is, grasping a specific object from a specific direction. We approach the problem using deep reinforcement learning via a double deep Q-network (DDQN) that learns to map downsampled RGB input images from a wrist-mounted camera to Q-values, which are then translated into Cartesian robot control commands via the cross-entropy method (CEM). The network is learned entirely on simulated data generated by a custom robot simulator that models both physical reality (contacts) and perceptual quality (high-quality rendering). The reality gap is bridged using domain randomization. The system is an example of end-to-end (mapping input monocular RGB images to output Cartesian motor commands) grasping of objects from multiple pre-defined object-centric orientations, such as from the side or top. We show promising results in both simulation and the real world, along with some challenges faced and the need for future research in this area.


Title: Learning to See before Learning to Act: Visual Pre-training for Manipulation
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  object detection  robot vision  visual priors  vision-based manipulation  transfer learning  passive vision task  data distribution  active manipulation task  affordance maps  vision networks  zero-shot adaptation  zero robotic experience  visual pre-training  object detection  object manipulation  affordance prediction networks  Task analysis  Robots  Visualization  Predictive models  Grasping  Head  Data models 
Abstract: Does having visual priors (e.g. the ability to detect objects) facilitate learning to perform vision-based manipulation (e.g. picking up objects)? We study this problem under the framework of transfer learning, where the model is first trained on a passive vision task (i.e., the data distribution does not depend on the agent's decisions), then adapted to perform an active manipulation task (i.e., the data distribution does depend on the agent's decisions). We find that pre-training on vision tasks significantly improves generalization and sample efficiency for learning to manipulate objects. However, realizing these gains requires careful selection of which parts of the model to transfer. Our key insight is that outputs of standard vision models highly correlate with affordance maps commonly used in manipulation. Therefore, we explore directly transferring model parameters from vision networks to affordance prediction networks, and show that this can result in successful zero-shot adaptation, where a robot can pick up certain objects with zero robotic experience. With just a small amount of robotic experience, we can further fine-tune the affordance model to achieve better results. With just 10 minutes of suction experience or 1 hour of grasping experience, our method achieves ~ 80% success rate at picking up novel objects.


Title: Practical Persistence Reasoning in Visual SLAM
Key Words: mobile robots  robot vision  SLAM (robots)  static environments  dynamic environments  persistence filters  ORB-SLAM  visual SLAM algorithm  persistence filtering  persistence reasoning  semistatic environments  Simultaneous localization and mapping  Visualization  Estimation  Cognition  Probability  Feature extraction 
Abstract: Many existing SLAM approaches rely on the assumption of static environments for accurate performance. However, several robot applications require them to traverse repeatedly in semi-static or dynamic environments. There has been some recent research interest in designing persistence filters to reason about persistence in such scenarios. Our goal in this work is to incorporate such persistence reasoning in visual SLAM. To this end, we incorporate persistence filters [1] into ORB-SLAM, a well-known visual SLAM algorithm. We observe that the simple integration of their proposal results in inefficient persistence reasoning. Through a series of modifications and using two locally collected datasets, we demonstrate the utility of such persistence filtering as well as our customizations in ORB-SLAM. Overall, incorporating persistence filtering could result in a significant reduction in map size (about 30% in the best case) and a corresponding reduction in run-time while retaining similar accuracy to methods that use much larger maps.


Title: FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow
Key Words: cameras  image colour analysis  image motion analysis  image reconstruction  image segmentation  image sequences  mobile robots  motion estimation  robot vision  SLAM (robots)  dynamic environments  visual SLAM  moving objects  static environment features  lead  wrong camera motion estimation  dense RGB-D SLAM solution  camera ego-motion estimation  static background reconstructions  optical flow residuals  dynamic semantics  RGB-D point clouds  camera tracking  background reconstruction  dense reconstruction results  dynamic scenes  static environments  dynamic dense RGB-D SLAM  Cameras  Dynamics  Optical imaging  Simultaneous localization and mapping  Three-dimensional displays  Two dimensional displays  Robustness 
Abstract: Dynamic environments are challenging for visual SLAM since the moving objects occlude the static environment features and lead to wrong camera motion estimation. In this paper, we present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions. Our novelty is using optical flow residuals to highlight the dynamic semantics in the RGB-D point clouds and provide more accurate and efficient dynamic/static segmentation for camera tracking and background reconstruction. The dense reconstruction results on public datasets and real dynamic scenes indicate that the proposed approach achieved accurate and efficient performances in both dynamic and static environments compared to state-of-the-art approaches.


Title: Towards the Probabilistic Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction
Key Words: geometry  image reconstruction  learning (artificial intelligence)  neural nets  probability  stereo image processing  probabilistic fusion  standard pipelines  deep learning  standard 3D reconstruction pipelines  open problem  deep neural network  error models  standard 3D reconstruction system  dense depth maps  discrete probability distributions  nonparametric probability distributions  multiview stereo approaches  geometry- based systems  learned single-view depth prior  Standards  Probability distribution  Fuses  Image reconstruction  Three-dimensional displays  Uncertainty  Probability density function 
Abstract: The best way to combine the results of deep learning with standard 3D reconstruction pipelines remains an open problem. While systems that pass the output of traditional multi-view stereo approaches to a network for regularisation or refinement currently seem to get the best results, it may be preferable to treat deep neural networks as separate components whose results can be probabilistically fused into geometry- based systems. Unfortunately, the error models required to do this type of fusion are not well understood, with many different approaches being put forward. Recently, a few systems have achieved good results by having their networks predict probability distributions rather than single values. We propose using this approach to fuse a learned single-view depth prior into a standard 3D reconstruction system. Our system is capable of incrementally producing dense depth maps for a set of keyframes. We train a deep neural network to predict discrete, nonparametric probability distributions for the depth of each pixel from a single image. We then fuse this "probability volume" with another probability volume based on the photometric consistency between subsequent frames and the keyframe image. We argue that combining the probability volumes from these two sources will result in a volume that is better conditioned. To extract depth maps from the volume, we minimise a cost function that includes a regularisation term based on network predicted surface normals and occlusion boundaries. Through a series of experiments, we demonstrate that each of these components improves the overall performance of the system.


Title: The Tiercel: A novel autonomous micro aerial vehicle that can map the environment by flying into obstacles
Key Words: cameras  collision avoidance  image sensors  mobile robots  navigation  robot vision  SLAM (robots)  space vehicles  autonomous microaerial vehicle  autonomous Tiercel robots  collision detector design  fisheye camera  reflective obstacles  transparent obstacles  collision-resilient robot  Tiercel MAV  autonomous navigation  autonomous flight  Collision avoidance  Cameras  Robot vision systems  Planning 
Abstract: Autonomous flight through unknown environments in the presence of obstacles is a challenging problem for micro aerial vehicles (MAVs). A majority of the current state-of-art research assumes obstacles as opaque objects that can be easily sensed by optical sensors such as cameras or LiDARs. However in indoor environments with glass walls and windows, or scenarios with smoke and dust, robots (even birds) have a difficult time navigating through the unknown space.In this paper, we present the design of a new class of micro aerial vehicles that achieves autonomous navigation and are robust to collisions. In particular, we present the Tiercel MAV: a small, agile, light weight and collision-resilient robot powered by a cellphone grade CPU. Our design exploits contact to infer the presence of transparent or reflective obstacles like glass walls, integrating touch with visual perception for SLAM. The Tiercel is able to localize using visual-inertial odometry (VIO) running on board the robot with a single downward facing fisheye camera and an IMU. We show how our collision detector design and experimental set up enable us to characterize the impact of collisions on VIO. We further develop a planning strategy to enable the Tiercel to fly autonomously in an unknown space, sustaining collisions and creating a 2D map of the environment. Finally we demonstrate a swarm of three autonomous Tiercel robots safely navigating and colliding through an obstacle field to reach their objectives.


Title: Integrated moment-based LGMD and deep reinforcement learning for UAV obstacle avoidance
Key Words: autonomous aerial vehicles  collision avoidance  control engineering computing  image motion analysis  image sequences  learning (artificial intelligence)  mobile robots  neural nets  object detection  robot vision  SLAM (robots)  visual perception  deep reinforcement learning  UAV obstacle avoidance  learning-based reaction local planner  microUAVs  image moment  illuminance variation  mapless navigation  moment-based LGMD  bioinspired monocular vision perception method  Navigation  Collision avoidance  Robustness  Lighting  Robots  Optical imaging  Machine learning 
Abstract: In this paper, a bio-inspired monocular vision perception method combined with a learning-based reaction local planner for obstacle avoidance of micro UAVs is presented. The system is more computationally efficient than other vision-based perception and navigation methods such as SLAM and optical flow because it does not need to calculate accurate distances. To improve the robustness of perception against illuminance change, the input image is remapped using image moment which is independent of illuminance variation. After perception, a local planner is trained using deep reinforcement learning for mapless navigation. The proposed perception and navigation methods are evaluated in some realistic simulation environments. The result shows that this light-weight monocular perception and navigation system works well in different complex environments without accurate depth information.


Title: LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery
Key Words: image matching  image resolution  optical radar  stereo image processing  dense depth maps  depth information  light detection and ranging  accurate depth map  stereo imagery  stereo systems  high-quality dense depth maps  stereo matching algorithms  sparse depth map  high-resolution LIDAR  Laser radar  Task analysis  Training  Feature extraction  Estimation  Convolution  Correlation 
Abstract: An accurate depth map of the environment is critical to the safe operation of autonomous robots and vehicles. Currently, either light detection and ranging (LIDAR) or stereo matching algorithms are used to acquire such depth information. However, a high-resolution LIDAR is expensive and produces sparse depth map at large range; stereo matching algorithms are able to generate denser depth maps but are typically less accurate than LIDAR at long range. This paper combines these approaches together to generate high-quality dense depth maps. Unlike previous approaches that are trained using ground-truth labels, the proposed model adopts a self-supervised training process. Experiments show that the proposed method is able to generate high-quality dense depth maps and performs robustly even with low-resolution inputs. This shows the potential to reduce the cost by using LIDARs with lower resolution in concert with stereo systems while maintaining high resolution.


Title: Monocular Visual-Inertial Odometry in Low-Textured Environments with Smooth Gradients: A Fully Dense Direct Filtering Approach
Key Words: calibration  distance measurement  gradient methods  image filtering  image texture  inertial systems  interpolation  vectors  low-textured environments  fully dense direct filtering approach  visual texture  direct photometric approaches  image information  information propagation  complexity reduction approach  state vector  monocular visual-inertial odometry approaches  higher order covariance propagation  state handling improvement  Cameras  Uncertainty  Estimation  Mathematical model  Motion estimation  Integrated circuits  Robots 
Abstract: State of the art visual-inertial odometry approaches suffer from the requirement of high gradients and sufficient visual texture. Even direct photometric approaches select a subset of the image with high-gradient areas and ignore smooth gradients or generally low-textured areas. In this work, we show that taking all image information (i.e. every single pixel) enables visual-inertial odometry even on areas with very low texture and smooth gradients, inherently interpolating and estimating the scene with no texture based on its informative surrounding. This information propagation is only possible as we estimate all states and their uncertainties (robot pose, extrinsic sensor calibration, and scene depth) jointly in a fully dense filter framework. Our complexity reduction approach enables real-time execution despite the large size of the state vector. Compared to our previous basic feasibility study on this topic, this work includes higher order covariance propagation and improved state handling for a significant performance gain, thorough comparisons to state-of-the-art algorithms, larger mapping components with uncertainty, self-calibration capability, and real-data tests.


Title: View-Invariant Loop Closure with Oriented Semantic Landmarks
Key Words: geometry  pose estimation  robot vision  SLAM (robots)  view-invariant loop closure  oriented semantic landmarks  simultaneous localization and mapping  monocular semantic SLAM system  object identity  inter-object geometry  view-invariant loop detection  ORB-SLAM  local appearance-based features  indoor scenes  object orientation estimation  geometrical detailed semantic maps  object translation  object scale  Cameras  Simultaneous localization and mapping  Semantics  Trajectory  Layout  Robustness  Estimation 
Abstract: Recent work on semantic simultaneous localization and mapping (SLAM) have shown the utility of natural objects as landmarks for improving localization accuracy and robustness. In this paper we present a monocular semantic SLAM system that uses object identity and inter-object geometry for view-invariant loop detection and drift correction. Our system's ability to recognize an area of the scene even under large changes in viewing direction allows it to surpass the mapping accuracy of ORB-SLAM, which uses only local appearance-based features that are not robust to large viewpoint changes. Experiments on real indoor scenes show that our method achieves mean drift reduction of 70% when compared directly to ORB-SLAM. Additionally, we propose a method for object orientation estimation, where we leverage the tracked pose of a moving camera under the SLAM setting to overcome ambiguities caused by object symmetry. This allows our SLAM system to produce geometrically detailed semantic maps with object orientation, translation, and scale.


Title: Active Acoustic Contact Sensing for Soft Pneumatic Actuators
Key Words: elastic constants  manipulator dynamics  pneumatic actuators  soft pneumatic actuators  active acoustic sensor  contact sensors  soft actuator  embedded speaker  PneuFlex actuator  active sensors  active acoustic contact sensing  Panda robot arm  embedded microphone  Actuators  Robot sensing systems  Acoustics  Microphones  Acoustic measurements 
Abstract: We present an active acoustic sensor that turns soft pneumatic actuators into contact sensors. The whole surface of the actuator becomes a sensor, rendering the question of where best to place a contact sensor unnecessary. At the same time, the compliance of the soft actuator remains unaffected. A small, embedded speaker emits a frequency sweep which travels through the actuator before it is recorded with an embedded microphone. The specific contact state of the actuator affects how the sound is modulated while traversing the structure. We learn to recognize these changes in the sound and map them to the corresponding contact locations. We demonstrate the method on the PneuFlex actuator. The active acoustic sensor achieves a classification rate of 93% and mean regression error of 3.7mm. It is robust against background noises and different objects. Finally, we test it on a Panda robot arm and show that it is unaffected by motor noises and other active sensors.


Title: A Flexible Method for Performance Evaluation of Robot Localization
Key Words: image motion analysis  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  robot localization  research issue  mobile robotics  performance assessment  robot SLAM algorithms  localization accuracy  SLAM algorithm  benchmark datasets  motion capture  environment-specific  spatial coverage  SLAM performance evaluation  distinctive markers  robot navigation environment  generative latent optimization problem  local robot-to-marker  global robot  Simultaneous localization and mapping  Navigation  Cameras  Performance evaluation  Robot localization 
Abstract: An important research issue in mobile robotics is performance assessment of robot SLAM algorithms in terms of their localization accuracy. Typically, SLAM algorithms are evaluated with the help of benchmark datasets or expensive equipment such as motion capture. Benchmark datasets however, are environment-specific, and use of motion capture constrains spatial coverage and affordability. In this paper, we present a novel method for SLAM performance evaluation, which only uses distinctive markers (such as AR tags), randomly placed in the robot navigation environment at arbitrary locations, and observes these markers with a camera onboard of the robot. Formulated as a generative latent optimization (GLO) problem, our method uses the local robot-to-marker poses to evaluate the global robot pose estimates by a SLAM algorithm and therefore its performance. Through extensive experiments on two robots, three localization/SLAM algorithms and both LiDAR and RGB-D sensors, we demonstrate the feasibility and accuracy of our proposed method.


Title: FarSee-Net: Real-Time Semantic Segmentation by Efficient Multi-scale Context Aggregation and Feature Space Super-resolution
Key Words: convolutional neural nets  feature extraction  image resolution  image sampling  image segmentation  object detection  real-time systems  robot vision  feature space superresolution  FarSee-Net  real time semantic segmentation  cascaded factorized atrous spatial pyramid pooling  feature maps  convolutional neural networks  multiscale context aggregation  object scale variations  robotic applications  subsampled image  Semantics  Convolution  Image segmentation  Feature extraction  Real-time systems  Spatial resolution 
Abstract: Real-time semantic segmentation is desirable in many robotic applications with limited computation resources. One challenge of semantic segmentation is to deal with the object scale variations and leverage the context. How to perform multi-scale context aggregation within limited computation budget is important. In this paper, firstly, we introduce a novel and efficient module called Cascaded Factorized Atrous Spatial Pyramid Pooling (CF-ASPP). It is a lightweight cas-caded structure for Convolutional Neural Networks (CNNs) to efficiently leverage context information. On the other hand, for runtime efficiency, state-of-the-art methods will quickly decrease the spatial size of the inputs or feature maps in the early network stages. The final high-resolution result is usually obtained by non-parametric up-sampling operation (e.g. bilinear interpolation). Differently, we rethink this pipeline and treat it as a super-resolution process. We use optimized super-resolution operation in the up-sampling step and improve the accuracy, especially in sub-sampled input image scenario for real-time applications. By fusing the above two improvements, our methods provide better latency-accuracy trade-off than the other state-of-the-art methods. In particular, we achieve 68.4% mIoU at 84 fps on the Cityscapes test set with a single Nivida Titan X (Maxwell) GPU card. The proposed module can be plugged into any feature extraction CNN and benefits from the CNN structure development.


Title: ACNN: a Full Resolution DCNN for Medical Image Segmentation
Key Words: biomedical MRI  computerised tomography  convolutional neural nets  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  neural nets  surgery  medical image segmentation  robot-assisted Minimally Invasive Surgeries  current DCNNs  sampling layer  receptive field  spatial dimension  feature maps  atrous convolutional layers  ACNN  magnetic resonance imaging  computed tomography image segmentation  segmentation Intersection  Atrous convolutional neural network  full-resolution DCNN  U-Net  Deeplabv3+  Image segmentation  Convolution  Biomedical imaging  Image resolution  Convolutional neural networks  Three-dimensional displays  Robots 
Abstract: Deep Convolutional Neural Networks (DCNNs) are used extensively in medical image segmentation and hence 3D navigation for robot-assisted Minimally Invasive Surgeries (MISs). However, current DCNNs usually use down sampling layers for increasing the receptive field and gaining abstract semantic information. These down sampling layers decrease the spatial dimension of feature maps, which can be detrimental to image segmentation. Atrous convolution is an alternative for the down sampling layer. It increases the receptive field whilst maintains the spatial dimension of feature maps. In this paper, a method for effective atrous rate setting is proposed to achieve the largest and fully-covered receptive field with a minimum number of atrous convolutional layers. Furthermore, a new and full resolution DCNN - Atrous Convolutional Neural Network (ACNN), which incorporates cascaded atrous II-blocks, residual learning and Instance Normalization (IN) is proposed. Application results of the proposed ACNN to Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) image segmentation demonstrate that the proposed ACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net and Deeplabv3+, but with reduced trainable parameters.


Title: Abstractions for computing all robotic sensors that suffice to solve a planning problem
Key Words: computational complexity  control engineering computing  data structures  graph theory  planning (artificial intelligence)  robots  search problems  sensors  robotic sensors  planning problem  search algorithms  sensor designs  design trade-offs  sensor maps  potential sensors  outer limits  search space  data structures  single special representative  task domain knowledge  sensor technology  particular problem instances  sensor characterization pairs  yielding solutions  Robot sensing systems  Planning  Sensor phenomena and characterization  Uncertainty  Collision avoidance 
Abstract: Whether a robot can perform some specific task depends on several aspects, including the robot's sensors and the plans it possesses. We are interested in search algorithms that treat plans and sensor designs jointly, yielding solutions-i.e., plan and sensor characterization pairs-if and only if they exist. Such algorithms can help roboticists explore the space of sensors to aid in making design trade-offs. Generalizing prior work where sensors are modeled abstractly as sensor maps on p-graphs, the present paper increases the potential sensors which can be sought significantly. But doing so enlarges a problem currently on the outer limits of being considered tractable. Toward taming this complexity, two contributions are made: (1) we show how to represent the search space for this more general problem and describe data structures that enable whole sets of sensors to be summarized via a single special representative; (2) we give a means by which other structure (either task domain knowledge, sensor technology or fabrication constraints) can be incorporated to reduce the sets to be enumerated. These lead to algorithms that we have implemented and which suffice to solve particular problem instances, albeit only of small scale. Nevertheless, the algorithm aids in helping understand what attributes sensors must possess and what information they must provide in order to ensure a robot can achieve its goals despite non-determinism.


Title: Flydar: Magnetometer-based High Angular Rate Estimation during Gyro Saturation for SLAM
Key Words: gyroscopes  Kalman filters  magnetometers  mobile robots  nonlinear filters  optical radar  SLAM (robots)  Flydar  magnetometer-based high angular rate estimation  SLAM  simultaneous localisation and mapping  Flying Li-DAR  EKF-based algorithm  sinusoidal magnetometer measurement  continuously rotating airframe  IMU sensors  gyro measurement  gyro bias  gyro saturation condition  rotating locomotion  robot hovering angular velocity  Robots  Magnetometers  Sensors  Estimation  Frequency measurement  Saturation magnetization  Angular velocity 
Abstract: In this paper, the high angular rate estimation for simultaneous localisation and mapping (SLAM) of a Flying Li-DAR (Flydar) is presented. The proposed EKF-based algorithm exploits the sinusoidal magnetometer measurement generated by the continuously rotating airframe for estimation of the robot hovering angular velocity. Significantly, the proposed method does not rely on additional sensors other than existing IMU sensors already being used for flight stabilization. The gyro measurement and the gyro bias are incorporated as a control input and a filter state respectively to enable estimation even under gyro saturation condition. Additionally, this work proposes leveraging on the inherently rotating locomotion to generate a planar lidar scan using only a single-point laser for possible lightweight autonomy. The proposed estimation method was experimentally evaluated on a ground rotating rig up to twice the gyro saturation limit with an effective rms error of 0.0045Hz; and on the proposed aerial platform - Flydar - hovering beyond the saturation limit with a rms error of 0.0056Hz. Lastly, the proposed method for SLAM using the rotating dynamics of Flydar was demonstrated with a localisation accuracy of 0.11m.


Title: Map-Predictive Motion Planning in Unknown Environments
Key Words: mobile robots  navigation  path planning  predictive control  trajectory control  map prediction  data-driven method  autonomous navigation  hallway environments  naÃ¯ve frontier pursuit method  heuristic methods  map-predictive motion planning  dynamically-constrained robots  trajectory planning  robot position  frontier selection heuristics  Robots  Trajectory  Planning  Navigation  Collision avoidance  Safety  Cognition 
Abstract: Algorithms for motion planning in unknown environments are generally limited in their ability to reason about the structure of the unobserved environment. As such, current methods generally navigate unknown environments by relying on heuristic methods to choose intermediate objectives along frontiers. We present a unified method that combines map prediction and motion planning for safe, time-efficient au-tonomous navigation of unknown environments by dynamically-constrained robots. We propose a data-driven method for predicting the map of the unobserved environment, using the robot's observations of its surroundings as context. These map predictions are then used to plan trajectories from the robot's position to the goal without requiring frontier selection. We applied this map-predictive motion planning strategy to randomly generated winding hallway environments, yielding substantial improvement in trajectory duration over a naÃ¯ve frontier pursuit method. We also experimentally demonstrate similar performance to methods using more sophisticated fron-tier selection heuristics while significantly reducing computation time.


Title: An Efficient and Continuous Approach to Information-Theoretic Exploration
Key Words: computational complexity  information theory  mobile robots  path planning  robot vision  SLAM (robots)  information-theoretic exploration  continuous occupancy map framework  |Î| measurement beams  recursive structure  robotics applications  autonomous navigation task  Robot sensing systems  Mutual information  Distortion measurement  Gain measurement  Time measurement 
Abstract: Exploration of unknown environments is embedded and essential in many robotics applications. Traditional algorithms, that decide where to explore by computing the expected information gain of an incomplete map from future sensor measurements, are limited to very powerful computational platforms. In this paper, we describe a novel approach for computing this expected information gain efficiently, as principally derived via mutual information. The key idea behind the proposed approach is a continuous occupancy map framework and the recursive structure it reveals. This structure makes it possible to compute the expected information gain of sensor measurements across an entire map much faster than computing each measurements' expected gain independently. Specifically, for an occupancy map composed of |M| cells and a range sensor that emits |Î| measurement beams, the algorithm (titled FCMI) computes the information gain corresponding to measurements made at each cell in O(|Î||M|) steps. To the best of our knowledge, this complexity bound is better than all existing methods for computing information gain. In our experiments, we observe that this novel, continuous approach is two orders of magnitude faster than the state-of-the-art FSMI algorithm.


Title: A Feature-Based Underwater Path Planning Approach using Multiple Perspective Prior Maps
Key Words: autonomous underwater vehicles  bathymetry  maximum likelihood estimation  mobile robots  navigation  path planning  remotely operated vehicles  multiple perspective prior maps  path planning methodology  Autonomous Underwater Vehicles  AUV  shallow complex environments  coral reefs  aerial photographic survey  bathymetric information  prior map  navigation graph  test points  shortest paths  destination points  maximum likelihood function  misclassified objects  photo-realistic simulated environment  Navigation  Cameras  Sensors  Uncertainty  Image segmentation  Feature extraction  Robots 
Abstract: This paper presents a path planning methodology which enables Autonomous Underwater Vehicles (AUVs) to navigate in shallow complex environments such as coral reefs. The approach leverages prior information from an aerial photographic survey, and derived bathymetric information of the corresponding area. From these prior maps, a set of features is obtained which define an expected arrangement of objects and bathymetry likely to be perceived by the AUV when underwater. A navigation graph is then constructed by predicting the arrangement of features visible from a set of test points within the prior, which allows the calculation of the shortest paths from any pair of start and destination points. A maximum likelihood function is defined which allows the AUV to match its observations to the navigation graph as it undertakes its mission. To improve robustness, the history of observed features are retained to facilitate possible recovery from non-detectable or misclassified objects. The approach is evaluated using a photo-realistic simulated environment, and results illustrate the merits of the approach even when only a relatively small number of features can be identified from the prior map.


Title: Motion Estimation in Occupancy Grid Maps in Stationary Settings Using Recurrent Neural Networks
Key Words: image filtering  image motion analysis  motion estimation  optical radar  path planning  probability  radar imaging  recurrent neural nets  traffic engineering computing  occupancy grid maps  recurrent neural networks  grid cell  occupancy probability  measurement grid maps  occupancy probabilities  filtered occupancy  network architecture  Computer architecture  Microprocessors  Vehicle dynamics  Measurement by laser beam  Time measurement  Recurrent neural networks  Dynamics 
Abstract: In this work, we tackle the problem of modeling the vehicle environment as dynamic occupancy grid map in complex urban scenarios using recurrent neural networks. Dynamic occupancy grid maps represent the scene in a bird's eye view, where each grid cell contains the occupancy prob-ability and the two dimensional velocity. As input data, our approach relies on measurement grid maps, which contain occupancy probabilities, generated with lidar measurements. Given this configuration, we propose a recurrent neural net-work architecture to predict a dynamic occupancy grid map, i.e. filtered occupancy and velocity of each cell, by using a sequence of measurement grid maps. Our network architecture contains convolutional long-short term memories in order to sequentially process the input, makes use of spatial context, and captures motion. In the evaluation, we quantify improvements in estimating the velocity of braking and turning vehicles compared to the state-of-the-art. Additionally, we demonstrate that our approach provides more consistent velocity estimates for dynamic objects, as well as, less erroneous velocity estimates in static area.


Title: Actively Mapping Industrial Structures with Information Gain-Based Planning on a Quadruped Robot
Key Words: collision avoidance  image representation  legged locomotion  robot vision  industrial structure  mapping industrial structures  information gain-based planning  quadruped robot  online active mapping system  voxel representation  NBV  expected information gain  terrain map  ANYbotics ANYmal robot  Robot sensing systems  Service robots  Solid modeling  Planning  Laser radar 
Abstract: In this paper, we develop an online active mapping system to enable a quadruped robot to autonomously survey large physical structures. We describe the perception, planning and control modules needed to scan and reconstruct an object of interest, without requiring a prior model. The system builds a voxel representation of the object, and iteratively determines the Next-Best-View (NBV) to extend the representation, according to both the reconstruction itself and to avoid collisions with the environment. By computing the expected information gain of a set of candidate scan locations sampled on the as-sensed terrain map, as well as the cost of reaching these candidates, the robot decides the NBV for further exploration. The robot plans an optimal path towards the NBV, avoiding obstacles and un-traversable terrain. Experimental results on both simulated and real-world environments show the capability and efficiency of our system. Finally we present a full system demonstration on the real robot, the ANYbotics ANYmal, autonomously reconstructing a building facade and an industrial structure.


Title: CAPRICORN: Communication Aware Place Recognition using Interpretable Constellations of Objects in Robot Networks
Key Words: feature extraction  image colour analysis  image matching  image representation  mobile robots  multi-robot systems  object detection  robot vision  SLAM (robots)  particular communication bandwidth  limited communication bandwidth  relative object positions  2step decentralized loop closure verification  compact semantic descriptors  bandwidth requirements  communication aware place recognition  interpretable constellations  robot networks  multiple robots  mapping environments  CAPRICORN  exploring environments  3D points  compact spatial descriptors  matching robots  geometric information  global image descriptors  TUM RGB-D SLAM sequence  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Robustness  Visualization  Bandwidth 
Abstract: Using multiple robots for exploring and mapping environments can provide improved robustness and performance, but it can be difficult to implement. In particular, limited communication bandwidth is a considerable constraint when a robot needs to determine if it has visited a location that was previously explored by another robot, as it requires for robots to share descriptions of places they have visited. One way to compress this description is to use constellations, groups of 3D points that correspond to the estimate of a set of relative object positions. Constellations maintain the same pattern from different viewpoints and can be robust to illumination changes or dynamic elements. We present a method to extract from these constellations compact spatial and semantic descriptors of the objects in a scene. We use this representation in a 2step decentralized loop closure verification: first, we distribute the compact semantic descriptors to determine which other robots might have seen scenes with similar objects; then we query matching robots with the full constellation to validate the match using geometric information. The proposed method requires less memory, is more interpretable than global image descriptors, and could be useful for other tasks and interactions with the environment. We validate our system's performance on a TUM RGB-D SLAM sequence and show its benefits in terms of bandwidth requirements.


Title: Monocular Direct Sparse Localization in a Prior 3D Surfel Map
Key Words: cameras  geophysical image processing  object tracking  optimisation  photometry  pose estimation  rendering (computer graphics)  solid modelling  monocular direct sparse localization  prior 3d surfel map  monocular camera  prior surfel map  vertex  normal maps  global planar information  sparse tracked points  image frame  direct photometric errors  camera localization  pose tracking  rendering  optimization  global 6-DoF camera poses  Cameras  Laser radar  Three-dimensional displays  Visualization  Rendering (computer graphics)  Simultaneous localization and mapping 
Abstract: In this paper, we introduce an approach to tracking the pose of a monocular camera in a prior surfel map. By rendering vertex and normal maps from the prior surfel map, the global planar information for the sparse tracked points in the image frame is obtained. The tracked points with and without the global planar information involve both global and local constraints of frames to the system. Our approach formulates all constraints in the form of direct photometric errors within a local window of the frames. The final optimization utilizes these constraints to provide the accurate estimation of global 6-DoF camera poses with the absolute scale. The extensive simulation and real-world experiments demonstrate that our monocular method can provide accurate camera localization results under various conditions.


Title: Monocular Visual Odometry using Learned Repeatability and Description
Key Words: cameras  distance measurement  feature extraction  image reconstruction  learning (artificial intelligence)  pose estimation  stereo image processing  monocular visual odometry  hybrid scheme  camera pose estimation  predicted repeatability maps  patch-wise 3D-2D association  local feature parameterization  adapted mapping module  local reconstruction accuracy  monocular VO system  learned repeatability  learned description  public datasets  robust backend  lightweight backend  Cameras  Feature extraction  Two dimensional displays  Robustness  Pose estimation  Three-dimensional displays  Visual odometry 
Abstract: Robustness and accuracy for monocular visual odometry (VO) under challenging environments are widely concerned. In this paper, we present a monocular VO system leveraging learned repeatability and description. In a hybrid scheme, the camera pose is initially tracked on the predicted repeatability maps in a direct manner and then refined with the patch-wise 3D-2D association. The local feature parameterization and the adapted mapping module further boost different functionalities in the system. Extensive evaluations on challenging public datasets are performed. The competitive performance on camera pose estimation demonstrates the effectiveness of our method. Additional studies on the local reconstruction accuracy and running time exhibit that our system is capable of maintaining a robust and lightweight backend.


Title: Photometric Path Planning for Vision-Based Navigation
Key Words: cameras  manipulators  navigation  path planning  robot vision  photometric path planning  vision-based navigation system  visual memory  topological map  virtual camera  navigability  visual path  navigation stage  onboard camera  top view image  learning stage  urban scene  Visualization  Navigation  Cameras  Visual servoing 
Abstract: We present a vision-based navigation system that uses a visual memory to navigate. Such memory corresponds to a topological map of key images created from moving a virtual camera over a model of the real scene. The advantage of our approach is that it provides a useful insight into the navigability of a visual path without relying on a traditional learning stage. During the navigation stage, the robot is controlled by sequentially comparing the images stored in the memory with the images acquired by the onboard camera.The evaluation is conducted on a robotic arm equipped with a camera and the model of the environment corresponds to a top view image of an urban scene.


Title: A Visual Positioning System for Indoor Blind Navigation
Key Words: cameras  collision avoidance  distance measurement  graph theory  handicapped aids  mobile robots  navigation  particle filtering (numerical methods)  pose estimation  robot vision  visual positioning system  indoor blind navigation  VPS  robotic navigation aid  RNA  assistive navigation  depth-enhanced visual-inertial odometry  RGB-D camera  inertial measurement unit  DVIO method  geometric feature  floor plane  measurement residuals  inertial data  graph optimization framework  Sampson error  near-range visual features  known depth  far-range visual features  estimation accuracy  particle filter localization method  PFL  visually impaired person  heading error  accurate pose estimation  Cameras  RNA  Feature extraction  Visualization  Pose estimation  Navigation  Three-dimensional displays 
Abstract: This paper presents a visual positioning system (VPS) for real-time pose estimation of a robotic navigation aid (RNA) for assistive navigation. The core of the VPS is a new method called depth-enhanced visual-inertial odometry (DVIO) that uses an RGB-D camera and an inertial measurement unit (IMU) to estimate the RNA's pose. The DVIO method extracts the geometric feature (the floor plane) from the camera's depth data and integrates its measurement residuals with that of the visual features and the inertial data in a graph optimization framework for pose estimation. A new measure based on the Sampson error is introduced to describe the measurement residuals of the near-range visual features with a known depth and that of the far-range visual features whose depths are unknown. The measure allows for the incorporation of both types of visual features into graph optimization. The use of the geometric feature and the Sampson error improves pose estimation accuracy and precision. The DVIO method is paired with a particle filter localization (PFL) method to locate the RNA in a 2D floor plan and the information is used to guide a visually impaired person. The PFL reduces the RNA's position and heading error by aligning the camera's depth data with the floor plan map. Together, the DVIO and the PFL allow for accurate pose estimation for wayfinding and 3D mapping for obstacle avoidance. Experimental results demonstrate the usefulness of the RNA in assistive navigation in indoor spaces.


Title: Shared Autonomous Interface for Reducing Physical Effort in Robot Teleoperation via Human Motion Mapping
Key Words: humanoid robots  manipulators  mobile robots  motion control  telerobotics  mobile humanoid robot  general-purpose assistive tasks  motion mapping  human motion  robot teleoperation  autonomous interface  teleoperation interfaces  task completion time  assistance function  teleoperator  autonomous grasping function  Task analysis  Fatigue  Muscles  Grasping  Cameras  Robot vision systems 
Abstract: Motion mapping is an intuitive method of teleoperation with a low learning curve. Our previous study investigates the physical fatigue caused by teleoperating a robot to perform general-purpose assistive tasks and this fatigue affects the operator's performance. The results from that study indicate that physical fatigue happens more in the tasks which involve more precise manipulation and steady posture maintenance. In this paper, we investigate how teleoperation assistance in terms of shared autonomy can reduce the physical workload in robot teleoperation via motion mapping. Specifically, we conduct a user study to compare the muscle effort in teleoperating a mobile humanoid robot to (1) reach and grasp an individual object and (2) collect objects in a cluttered workspace with and without an autonomous grasping function that can be triggered manually by the teleoperator. We also compare the participants' task performance, subjective user experience, and change in attitude towards the usage of teleoperation assistance in the future based on their experience using the assistance function. Our results show that: (1) teleoperation assistance like autonomous grasping can effectively reduce the physical effort, task completion time and number of errors; (2) based on their experience performing the tasks with and without assistance, the teleoperators reported that they would prefer to use automated functions for future teleoperation interfaces.


Title: Multimodal Trajectory Predictions for Urban Environments Using Geometric Relationships between a Vehicle and Lanes
Key Words: feature extraction  object detection  road traffic  road vehicles  traffic engineering computing  autonomous driving systems  traffic behavior  urban environments  road geometries  lane-based multimodal prediction network  arbitrary shapes  traffic lanes  future trajectory  lane geometry  lane feature  generalized geometric relationships  vehicle state  vehicle motion model constraint  prediction method  multimodal trajectory predictions  safe driving systems  LAMP-Net  Trajectory  Predictive models  Hidden Markov models  Acceleration  Geometry  Shape  Urban areas 
Abstract: Implementation of safe and efficient autonomous driving systems requires accurate prediction of the long-term trajectories of surrounding vehicles. High uncertainty in traffic behavior makes it difficult to predict trajectories in urban environments, which have various road geometries. To over-come this problem, we propose a method called lane-based multimodal prediction network (LAMP-Net), which can handle arbitrary shapes and numbers of traffic lanes and predict both the future trajectory along each lane and the probability of each lane being selected. A vector map is used to define the lane geometry and a novel lane feature is introduced to represent the generalized geometric relationships between the vehicle state and lanes. Our network takes this feature as the input and is trained to be versatile for arbitrarily shaped lanes. Moreover, we introduce a vehicle motion model constraint to our network. Our prediction method combined with the constraint significantly enhances prediction accuracy. We evaluate the prediction performance on two datasets which contain a wide variety of real-world traffic scenarios. Experimental results show that our proposed LAMP-Net outperforms state-of-the-art methods.


Title: Towards Adaptive Benthic Habitat Mapping
Key Words: bathymetry  geophysical image processing  neural nets  oceanographic techniques  remotely operated vehicles  seafloor phenomena  sonar  underwater vehicles  habitat model  AUV systems  seafloor imagery  efficient AUV surveys  visually-derived habitat classes  broad-scale bathymetric data  fewer samples  benthic surveys  adaptive benthic habitat  autonomous underwater vehicles  benthic habitat mapping  broadscale bathymetric data  remotely-sensed acoustic data  Feature extraction  Uncertainty  Biological system modeling  Bayes methods  Data models  Neural networks  Backscatter 
Abstract: Autonomous Underwater Vehicles (AUVs) are increasingly being used to support scientific research and monitoring studies. One such application is in benthic habitat mapping where these vehicles collect seafloor imagery that complements broadscale bathymetric data collected using sonar. Using these two data sources, the relationship between remotely-sensed acoustic data and the sampled imagery can be learned, creating a habitat model. As the areas to be mapped are often very large and AUV systems collecting seafloor imagery can only sample from a small portion of the survey area, the information gathered should be maximised for each deployment. This paper illustrates how the habitat models themselves can be used to plan more efficient AUV surveys by identifying where to collect further samples in order to most improve the habitat model. A Bayesian neural network is used to predict visually-derived habitat classes when given broad-scale bathymetric data. This network can also estimate the uncertainty associated with a prediction, which can be deconstructed into its aleatoric (data) and epistemic (model) components. We demonstrate how these structured uncertainty estimates can be utilised to improve the model with fewer samples. Such adaptive approaches to benthic surveys have the potential to reduce costs by prioritizing further sampling efforts. We illustrate the effectiveness of the proposed approach using data collected by an AUV on offshore reefs in Tasmania, Australia.


Title: Efficient two step optimization for large embedded deformation graph based SLAM
Key Words: computational complexity  embedded systems  graph theory  Hessian matrices  robot vision  SLAM (robots)  stereo image processing  parameter estimation  computation complexity  two step optimization  deformable geometry  stereo camera  SLAM applications  large scale embedded deformation graph  Hessian matrix  Simultaneous localization and mapping  Strain  Jacobian matrices  Optimization  Cameras  Deformable models  Geometry 
Abstract: Embedded deformation graph is a widely used technique in deformable geometry and graphical problems. Although the technique has been transmitted to stereo (or RGB-D) camera based SLAM applications, it remains challenging to compromise the computational cost as the model grows. In practice, the processing time grows rapidly in accordance with the expansion of maps. In this paper, we propose an approach to decouple the nodes of deformation graph in large scale dense deformable SLAM and keep the estimation time to be constant. We observe that only partial deformable nodes in the graph are connected to visible points. Based on this fact, the sparsity of the original Hessian matrix is utilized to split the parameter estimation into two independent steps. With this new technique, we achieve faster parameter estimation with amortized computation complexity reduced from O(n2) to almost O(1). As a result, the computational cost barely increases as the map keeps growing. Based on our strategy, the computational bottleneck in large scale embedded deformation graph based applications will be greatly mitigated. The effectiveness is validated by experiments, featuring large scale deformation scenarios.


Title: Dynamic Anchor Selection for Improving Object Localization
Key Words: computer vision  neural nets  object detection  DANet  single-stage object detectors  dynamic anchor selection  anchor boxes  object localization candidates  MS COCO dataset  Feature extraction  Detectors  Computer architecture  Task analysis  Spatial resolution  Head  Object detection 
Abstract: Anchor boxes act as potential object localization candidates allow single-stage detectors to achieve real-time performance, at the cost of localization accuracy when compared to state-of-the-art two-stage detectors. Therefore, correct selection of the scale and aspect ratio associated with an anchor box is crucial for detector performance. In this work, we propose a novel architecture called DANet for improving the localization performance of single-stage object detectors, while maintaining real-time inference. The proposed network achieves this by predicting (1) the combination of aspect ratio and scale per feature map based on object density and (2) localization confidence per anchor box. We evaluate the proposed network using the benchmark dataset. On the MS COCO dataset, DANet achieves 30.9% AP at 51.8 fps using ResNet-18 and 45.3% AP at 7.4 fps using ResNeXt-101. The code and models will be available at https://github.com/PS06/AnchorNet.


Title: Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar
Key Words: distance measurement  feature extraction  image colour analysis  image sensors  image sequences  mobile robots  motion estimation  object recognition  object tracking  radar computing  robot vision  SLAM (robots)  supervised learning  predict robust keypoints  odometry estimation  metric localisation  self-supervised framework  differentiable point-based motion estimator  localisation error  Oxford Radar RobotCar Dataset  point-based radar odometry  Radar  Measurement  Task analysis  Estimation  Robot sensing systems  Computer architecture 
Abstract: This paper presents a self-supervised framework for learning to detect robust keypoints for odometry estimation and metric localisation in radar. By embedding a differentiable point-based motion estimator inside our architecture, we learn keypoint locations, scores and descriptors from localisation error alone. This approach avoids imposing any assumption on what makes a robust keypoint and crucially allows them to be optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from the Oxford Radar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneously solving metric loop closures. Combining these outputs, we provide a framework capable of full mapping and localisation with radar in urban environments.


Title: Learned Critical Probabilistic Roadmaps for Robotic Motion Planning
Key Words: graph theory  mobile robots  motion control  path planning  sampling methods  implicit graph representation  state space  solution trajectories  graph-theoretic techniques  hierarchical graph  uniform sampling  sampling-based motion planning  robotic motion planning  motion planning techniques  learned critical probabilistic roadmaps  critical PRM  critical probabilistic roadmaps  Planning  Robots  Trajectory  Probabilistic logic  Training  Complexity theory  Neural networks 
Abstract: Sampling-based motion planning techniques have emerged as an efficient algorithmic paradigm for solving complex motion planning problems. These approaches use a set of probing samples to construct an implicit graph representation of the robot's state space, allowing arbitrarily accurate representations as the number of samples increases to infinity. In practice, however, solution trajectories only rely on a few critical states, often defined by structure in the state space (e.g., doorways). In this work we propose a general method to identify these critical states via graph-theoretic techniques (betweenness centrality) and learn to predict criticality from only local environment features. These states are then leveraged more heavily via global connections within a hierarchical graph, termed Critical Probabilistic Roadmaps. Critical PRMs are demonstrated to achieve up to three orders of magnitude improvement over uniform sampling, while preserving the guarantees and complexity of sampling-based motion planning. A video is available at https://youtu.be/AYoD-pGd9ms.


Title: Learning Heuristic A*: Efficient Graph Search using Neural Network
Key Words: graph theory  learning (artificial intelligence)  neural nets  optimisation  path planning  search problems  path planning problem  computation load  neural network  optimal paths  optimal cost  global optimality  admissible heuristic function  efficient graph search  learning heuristic A*  LHA*  suboptimality bound  maze-like map  Heuristic algorithms  Training  Path planning  Biological neural networks  Robots  Navigation 
Abstract: In this paper, we consider the path planning problem on a graph. To reduce computation load by efficiently exploring the graph, we model the heuristic function as a neural network, which is trained by a training set derived from optimal paths to estimate the optimal cost between a pair of vertices on the graph. As such heuristic function cannot be proved to be an admissible heuristic to guarantee the global optimality of the path, we adapt an admissible heuristic function for the terminating criteria. Thus, proposed Learning Heuristic A* (LHA*) guarantees the bounded suboptimality of the path. The performance of LHA* was demonstrated by simulations in a maze-like map and compared with the performance of weighted A* with the same suboptimality bound.


Title: 3D-CNN Based Heuristic Guided Task-Space Planner for Faster Motion Planning
Key Words: collision avoidance  convolutional neural nets  learning (artificial intelligence)  manipulators  mobile robots  sampling methods  stereo image processing  trees (mathematics)  TS-RRT  task-space rapidly-exploring random trees  3D-CNN  heuristic guided task-space planner  fully convolutional neural networks  heuristic map  Random Trees  sampling-based planner  collision-free path  robotic manipulation  motion planning  Planning  Task analysis  Robots  Collision avoidance  Heuristic algorithms  Feature extraction  Convolution 
Abstract: Motion planning is important in a wide variety of applications such as robotic manipulation. However, it is still challenging to reliably find a collision-free path within a reasonable time. To address the issue, this paper proposes a novel framework which combines a sampling-based planner and deep learning for faster motion planning, focusing on heuristics. The proposed method extends Task-Space Rapidly-exploring Random Trees (TS-RRT) to guide the trees with a "heuristic map" where every voxel has a cost-to-go value toward the goal. It also utilizes fully convolutional neural networks (CNNs) for producing more appropriate heuristic maps, rather than manually-designed heuristics. To verify the effectiveness of the proposed method, experiments for motion planning using a real environment and mobile manipulator are carried out. The results indicate that it outperforms the existing planners, especially in terms of the average planning time with smaller variance.


Title: Fast Frontier-based Information-driven Autonomous Exploration with an MAV
Key Words: autonomous aerial vehicles  collision avoidance  entropy  microrobots  mobile robots  navigation  octrees  probability  robot vision  MAV  collision-free navigation  autonomous robots  microaerial vehicles  map entropy  occupancy probabilities  utility function  frontier extraction  frontier-based exploration  frontier voxels  map frontiers  frontier-based information-driven autonomous exploration  exploration planner  octree map representation  visual-based navigation  motion planning  octree-based occupancy mapping  sampling-based exploration  Planning  Octrees  Entropy  Robot sensing systems  Measurement  Task analysis  Aerial Systems: Perception and Autonomy  Visual-Based Navigation 
Abstract: Exploration and collision-free navigation through an unknown environment is a fundamental task for autonomous robots. In this paper, a novel exploration strategy for Micro Aerial Vehicles (MAVs) is presented. The goal of the exploration strategy is the reduction of map entropy regarding occupancy probabilities, which is reflected in a utility function to be maximised. We achieve fast and efficient exploration performance with tight integration between our octree-based occupancy mapping approach, frontier extraction, and motion planning-as a hybrid between frontier-based and sampling-based exploration methods. The computationally expensive frontier clustering employed in classic frontier-based exploration is avoided by exploiting the implicit grouping of frontier voxels in the underlying octree map representation. Candidate next-views are sampled from the map frontiers and are evaluated using a utility function combining map entropy and travel time, where the former is computed efficiently using sparse raycasting. These optimisations along with the targeted exploration of frontier-based methods result in a fast and computationally efficient exploration planner. The proposed method is evaluated using both simulated and real-world experiments, demonstrating clear advantages over state-of-the-art approaches.


Title: Map Management Approach for SLAM in Large-Scale Indoor and Outdoor Areas
Key Words: image registration  iterative methods  mobile robots  navigation  robot vision  SLAM (robots)  link-points  multiple indoor areas  outdoor areas  map quality  single map approaches  semantic map management approach  multiple maps  modular map structure  utilized SLAM method  laser scan data  appropriate SLAM configuration  single independent maps  appearance-based method  iterative closest point registration  point clouds  simultaneous localization and mapping configurations  Simultaneous localization and mapping  Lasers  Navigation  Feature extraction  Three-dimensional displays 
Abstract: This work presents a semantic map management approach for various environments by triggering multiple maps with different simultaneous localization and mapping (SLAM) configurations. A modular map structure allows to add, modify or delete maps without influencing other maps of different areas. The hierarchy level of our algorithm is above the utilized SLAM method. Evaluating laser scan data (e.g. the detection of passing a doorway) triggers a new map, automatically choosing the appropriate SLAM configuration from a manually predefined list. Single independent maps are connected by link-points, which are located in an overlapping zone of both maps, enabling global navigation over several maps. Loop- closures between maps are detected by an appearance-based method, using feature matching and iterative closest point (ICP) registration between point clouds. Based on the arrangement of maps and link-points, a topological graph is extracted for navigation purpose and tracking the global robot's position over several maps. Our approach is evaluated by mapping a university campus with multiple indoor and outdoor areas and abstracting a metrical-topological graph. It is compared to a single map running with different SLAM configurations. Our approach enhances the overall map quality compared to the single map approaches by automatically choosing predefined SLAM configurations for different environmental setups.


Title: A Hierarchical Framework for Collaborative Probabilistic Semantic Mapping
Key Words: Bayes methods  expectation-maximisation algorithm  geometry  mobile robots  multi-robot systems  robot vision  single robot semantic mapping  collaborative geometry mapping  semantic point cloud  heterogeneous sensor fusion model  collaborative robots level  3D semantic map fusion algorithm  hierarchical collaborative probabilistic semantic mapping framework  Bayesian rule  probability  expectation-maximization  mathematical modeling  Semantics  Collaboration  Three-dimensional displays  Robot sensing systems  Geometry  Robot kinematics 
Abstract: Performing collaborative semantic mapping is a critical challenge for cooperative robots to maintain a comprehensive contextual understanding of the surroundings. Most of the existing work either focus on single robot semantic mapping or collaborative geometry mapping. In this paper, a novel hierarchical collaborative probabilistic semantic mapping framework is proposed, where the problem is formulated in a distributed setting. The key novelty of this work is the mathematical modeling of the overall collaborative semantic mapping problem and the derivation of its probability decomposition. In the single robot level, the semantic point cloud is obtained based on heterogeneous sensor fusion model and is used to generate local semantic maps. Since the voxel correspondence is unknown in collaborative robots level, an Expectation-Maximization approach is proposed to estimate the hidden data association, where Bayesian rule is applied to perform semantic and occupancy probability update. The experimental results show the high quality global semantic map, demonstrating the accuracy and utility of 3D semantic map fusion algorithm in real missions.


Title: Autonomous Navigation in Unknown Environments using Sparse Kernel-based Occupancy Mapping
Key Words: collision avoidance  image classification  learning (artificial intelligence)  mobile robots  path planning  robot vision  trajectory control  decision boundary  kernel perceptron classifier  online training algorithm  piecewise-polynomial robot trajectories  autonomous navigation  Ackermann-drive robot  sparse kernel-based occupancy  real-time occupancy mapping  autonomous robot  map representation  piecewise-polynomial robot trajectory  piecewise-linear robot trajectory  collision checking algorithm  Support vector machines  Kernel  Training  Robot sensing systems  Collision avoidance  Trajectory 
Abstract: This paper focuses on real-time occupancy mapping and collision checking onboard an autonomous robot navigating in an unknown environment. We propose a new map representation, in which occupied and free space are separated by the decision boundary of a kernel perceptron classifier. We develop an online training algorithm that maintains a very sparse set of support vectors to represent obstacle boundaries in configuration space. We also derive conditions that allow complete (without sampling) collision-checking for piecewise-linear and piecewise-polynomial robot trajectories. We demonstrate the effectiveness of our mapping and collision checking algorithms for autonomous navigation of an Ackermann-drive robot in unknown environments.


Title: Hybrid Topological and 3D Dense Mapping through Autonomous Exploration for Large Indoor Environments
Key Words: image representation  indoor navigation  mobile robots  path planning  robot vision  SLAM (robots)  stereo image processing  topology  indoor environments  topological global representations  3D dense submaps  hybrid global map  autonomous exploration  autonomous navigation  path planning  dense 3D maps  3D dense representations  3D dense mapping systems  hybrid topological mapping  metric 3D maps  standard CPU  Three-dimensional displays  Measurement  Robots  Semantics  Two dimensional displays  Indoor environments  Path planning 
Abstract: Robots require a detailed understanding of the 3D structure of the environment for autonomous navigation and path planning. A popular approach is to represent the environment using metric, dense 3D maps such as 3D occupancy grids. However, in large environments the computational power required for most state-of-the-art 3D dense mapping systems is compromising precision and real-time capability. In this work, we propose a novel mapping method that is able to build and maintain 3D dense representations for large indoor environments using standard CPUs. Topological global representations and 3D dense submaps are maintained as hybrid global map. Submaps are generated for every new visited place. A place (room) is identified as an isolated part of the environment connected to other parts through transit areas (doors). This semantic partitioning of the environment allows for a more efficient mapping and path-planning. We also propose a method for autonomous exploration that directly builds the hybrid representation in real time.We validate the real-time performance of our hybrid system on simulated and real environments regarding mapping and path-planning. The improvement in execution time and memory requirements upholds the contribution of the proposed work.


Title: Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints*
Key Words: computer vision  object detection  optimisation  path planning  pose estimation  robot vision  SLAM (robots)  lifted algorithm  combinatorial complexity  heuristic criterion  planar pose estimation  marker-based mapping  highly ambiguous inputs  PPE ambiguities  possible marker orientation solutions  rotation averaging formulation  marker corners  computer vision  planar markers  clique constraints  robust rotation averaging  marker pose ambiguity  Cameras  Pipelines  Machine-to-machine communications  Image edge detection  Pose estimation  Simultaneous localization and mapping  Histograms 
Abstract: Planar markers are useful in robotics and computer vision for mapping and localisation. Given a detected marker in an image, a frequent task is to estimate the 6DOF pose of the marker relative to the camera, which is an instance of planar pose estimation (PPE). Although there are mature techniques, PPE suffers from a fundamental ambiguity problem, in that there can be more than one plausible pose solutions for a PPE instance. Especially when localisation of the marker corners is noisy, it is often difficult to disambiguate the pose solutions based on reprojection error alone. Previous methods choose between the possible solutions using a heuristic criterion, or simply ignore ambiguous markers.We propose to resolve the ambiguities by examining the consistencies of a set of markers across multiple views. Our specific contributions include a novel rotation averaging formulation that incorporates long-range dependencies between possible marker orientation solutions that arise from PPE ambiguities. We analyse the combinatorial complexity of the problem, and develop a novel lifted algorithm to effectively resolve marker pose ambiguities, without discarding any marker observations. Results on real and synthetic data show that our method is able to handle highly ambiguous inputs, and provides more accurate and/or complete marker-based mapping and localisation.


Title: Look, Listen, and Act: Towards Audio-Visual Embodied Navigation
Key Words: acoustic signal processing  audio signal processing  audio-visual systems  human computer interaction  mobile agents  navigation  path planning  audio-visual embodied navigation  mobile intelligent agents  multiple sensory inputs  sound source  indoor environment  raw egocentric visual data  audio sensory data  audio signal  visual environment  visual pieces  audio pieces  visual perception mapper module  sound perception module  audio-visual observations  simulated multimodal environment  visual-audio-room dataset  Navigation  Visualization  Task analysis  Robot sensing systems  Visual perception  Acoustics  Feature extraction 
Abstract: A crucial ability of mobile intelligent agents is to integrate the evidence from multiple sensory inputs in an environment and to make a sequence of actions to reach their goals. In this paper, we attempt to approach the problem of Audio-Visual Embodied Navigation, the task of planning the shortest path from a random starting location in a scene to the sound source in an indoor environment, given only raw egocentric visual and audio sensory data. To accomplish this task, the agent is required to learn from various modalities, i.e., relating the audio signal to the visual environment. Here we describe an approach to audio-visual embodied navigation that takes advantage of both visual and audio pieces of evidence. Our solution is based on three key ideas: a visual perception mapper module that constructs its spatial memory of the environment, a sound perception module that infers the relative location of the sound source from the agent, and a dynamic path planner that plans a sequence of actions based on the audio-visual observations and the spatial memory of the environment to navigate toward the goal. Experimental results on a newly collected Visual-Audio-Room dataset using the simulated multi-modal environment demonstrate the effectiveness of our approach over several competitive baselines.


Title: CNN-Based Simultaneous Dehazing and Depth Estimation
Key Words: computer vision  convolutional neural nets  correlation methods  image coding  image colour analysis  image denoising  image representation  image sensors  learning (artificial intelligence)  spatial variables measurement  single hazy RGB input  single dense encoder  encoded image representation  dehazing image depth estimation  single image depth estimation  image dehazing  computer vision  convolutional neural networks  CNN  dehazing depth estimation algorithms  traditional haze modeling  depth estimation network  fully scaled depth map  depth-transmission consistency loss  separate decoders  Decoding  Propagation losses  Estimation  Training  Image reconstruction  Task analysis  Scattering 
Abstract: It is difficult for both cameras and depth sensors to obtain reliable information in hazy scenes. Therefore, image dehazing is still one of the most challenging problems to solve in computer vision and robotics. With the development of convolutional neural networks (CNNs), lots of dehazing and depth estimation algorithms using CNNs have emerged. However, very few of those try to solve these two problems at the same time. Focusing on the fact that traditional haze modeling contains depth information in its formula, we propose a CNN-based simultaneous dehazing and depth estimation network. Our network aims to estimate both a dehazed image and a fully scaled depth map from a single hazy RGB input with end-to-end training. The network contains a single dense encoder and four separate decoders; each of them shares the encoded image representation while performing individual tasks. We suggest a novel depth-transmission consistency loss in the training scheme to fully utilize the correlation between the depth information and transmission map. To demonstrate the robustness and effectiveness of our algorithm, we performed various ablation studies and compared our results to those of state-of-the-art algorithms in dehazing and single image depth estimation, both qualitatively and quantitatively. Furthermore, we show the generality of our network by applying it to some real-world examples.


Title: Attention-Guided Lightweight Network for Real-Time Segmentation of Robotic Surgical Instruments
Key Words: convolutional neural nets  edge detection  image capture  image coding  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  robot vision  surgery  robotic surgical instruments  robot-assisted surgery  deep learning models  attention-guided lightweight network  LWANet  lightweight network MobileNetV2  depthwise separable convolution  transposed convolution  surgical instrument  encoder-decoder architecture  attention fusion block  Instruments  Convolution  Semantics  Computational efficiency  Decoding  Surgery  Real-time systems 
Abstract: The real-time segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, it is still a challenging task to implement deep learning models to do real-time segmentation for surgical instruments due to their high computational costs and slow inference speed. In this paper, we propose an attention-guided lightweight network (LWANet), which can segment surgical instruments in real-time. LWANet adopts encoder-decoder architecture, where the encoder is the lightweight network MobileNetV2, and the decoder consists of depthwise separable convolution, attention fusion block, and transposed convolution. Depthwise separable convolution is used as the basic unit to construct the decoder, which can reduce the model size and computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to emphasize target regions, contributing to locating the surgical instrument. Transposed convolution is performed to upsample feature maps for acquiring refined edges. LWANet can segment surgical instruments in real-time while takes little computational costs. Based on 960x544 inputs, its inference speed can reach 39 fps with only 3.39 GFLOPs. Also, it has a small model size and the number of parameters is only 2.06 M. The proposed network is evaluated on two datasets. It achieves state-of-the- art performance 94.10% mean IOU on Cata7 and obtains a new record on EndoVis 2017 with a 4.10% increase on mean IOU.


Title: Automated robotic breast ultrasound acquisition using ultrasound feedback
Key Words: biological tissues  biomedical ultrasonics  feedback  image registration  medical image processing  medical robotics  phantoms  surgery  visual servoing  robotic 3D breast US acquisitions  US feedback  visual servoing algorithm  patient specific scans  tissue deformations  US probe  ultrasound feedback  automated robotic breast ultrasound acquisition  Breast  Probes  Trajectory  Safety  Robot kinematics  Skin 
Abstract: Current challenges in automated robotic breast ultrasound (US) acquisitions include keeping acoustic coupling between the breast and the US probe, minimizing tissue deformations and safety. In this paper, we present how an autonomous 3D breast US acquisition can be performed utilizing a 7DOF robot equipped with a linear US transducer. Robotic 3D breast US acquisitions would increase the diagnostic value of the modality since they allow patient specific scans and have a high reproducibility, accuracy and efficiency. Additionally, 3D US acquisitions allow more flexibility in examining the breast and simplify registration with preoperative images like MRI. To overcome the current challenges, the robot follows a reference- based trajectory adjusted by a visual servoing algorithm. The reference trajectory is a patient specific trajectory coming from e.g. an MRI. The visual servoing algorithm commands in-plane rotations and corrects the probe contact based on confidence maps. A safety aware, intrinsically passive framework is utilised to actuate the robot. The approach is illustrated with experiments on a phantom, which show that the robot only needs minor pre-procedural information to consistently image the phantom while relying mainly on US feedback.


Title: Pathological Airway Segmentation with Cascaded Neural Networks for Bronchoscopic Navigation
Key Words: computerised tomography  image segmentation  lung  medical image processing  neural nets  cascaded 2D-3D model  pathological CT scans  3D adversarial training model  novel 2D neural network  airway tree  pathological abnormalities  preoperative chest CT scans  patient-specific airway maps  peripheral airways  enhanced visualisation  3D airway maps  robotic bronchoscopic intervention  bronchoscopic navigation  cascaded neural networks  pathological airway segmentation  Three-dimensional displays  Atmospheric modeling  Training  Two dimensional displays  Computed tomography  Image segmentation  Solid modeling 
Abstract: Robotic bronchoscopic intervention requires detailed 3D airway maps for both localisation and enhanced visualisation, especially at peripheral airways. Patient-specific airway maps can be generated from preoperative chest CT scans. Due to pathological abnormalities and anatomical variations, automatically delineating the airway tree with distal branches is a challenging task. In the paper, we propose a cascaded 2D+3D model that has been tailored for airway segmentation from pathological CT scans. A novel 2D neural network is developed to generate the initial predictions where the peripheral airways are refined by a 3D adversarial training model. A sampling strategy based on a sequence of morphological operations is employed for the concatenation of the 2D and 3D models. The method has been validated on 20 pathological CT scans with results demonstrating improved segmentation accuracy and consistency, especially in peripheral airways.


Title: Predicting Obstacle Footprints from 2D Occupancy Maps by Learning from Physical Interactions
Key Words: collision avoidance  convolutional neural nets  laser ranging  learning (artificial intelligence)  mobile robots  navigation  robot vision  indoor robot localization  obstacle avoidance  laser scanners  collision events  2D occupancy maps  obstacle footprint prediction  physical interaction learning  horizontal scanning 2D laser range finders  convolutional neural network  Two dimensional displays  Collision avoidance  Image segmentation  Training  Robot sensing systems 
Abstract: Horizontally scanning 2D laser rangefinders are a popular approach for indoor robot localization because of the high accuracy of the sensors and the compactness of the required 2D maps. As the scanners in this configuration only provide information about one slice of the environment, the measurements typically do not capture the full extent of a large variety of obstacles, including chairs or tables. Accordingly, obstacle avoidance based on laser scanners mounted in such a fashion is likely to fail. In this paper, we propose a learning-based approach to predict collisions in 2D occupancy maps. Our approach is based on a convolutional neural network which is trained on a 2D occupancy map and collision events recorded with a bumper while the robot is navigating in its environment. As the network operates on local structures only, it can generalize to new environments. In addition, the robot can collect and integrate new collision examples after an initial training phase. Extensive experiments carried out in simulation and a realistic real-world environment confirm that our approach allows robots to learn from collision events to avoid collisions in the future.


Title: Discrete Deep Reinforcement Learning for Mapless Navigation
Key Words: discrete systems  gradient methods  learning (artificial intelligence)  mobile robots  navigation  optimisation  state-space methods  mapless navigation  discrete state space algorithms  continuous alternatives  double deep Q-network  parallel asynchronous training  training time  proximal policy optimization algorithms  original discrete algorithm  continuous algorithms  continuous deep deterministic policy gradient  multibatch priority experience replay  discrete deep reinforcement  Training  Navigation  Robot kinematics  Robot sensing systems  Optimization  Machine learning 
Abstract: Our goal is to investigate whether discrete state space algorithms are a viable solution to continuous alternatives for mapless navigation. To this end we present an approach based on Double Deep Q-Network and employ parallel asynchronous training and a multi-batch Priority Experience Replay to reduce the training time. Experiments show that our method trains faster and outperforms both the continuous Deep Deterministic Policy Gradient and Proximal Policy Optimization algorithms. Moreover, we train the models in a custom environment built on the recent Unity learning toolkit and show that they can be exported on the TurtleBot3 simulator and to the real robot without further training. Overall our optimized method is 40% faster compared to the original discrete algorithm. This setting significantly reduces the training times with respect to the continuous algorithms, maintaining a similar level of success rate hence being a viable alternative for mapless navigation.


Title: Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps
Key Words: cameras  graph theory  image colour analysis  image sensors  mobile robots  normal distribution  robot vision  SLAM (robots)  camera origin  pose graph  NDT-OM  keyframe-based dense mapping  keyframe-based mapping system  RGB-D sensor  2D view-dependent structures  uncertainty model  RGB-D cameras  view-dependent local maps  normal distribution transform maps  global map  loop closure detection  autonomous robots  SLAM  Ellipsoids  Three-dimensional displays  Robot sensing systems  Cameras  Two dimensional displays  Uncertainty 
Abstract: In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.


Title: Informative Path Planning for Active Field Mapping under Localization Uncertainty
Key Words: Gaussian processes  mobile robots  path planning  informative path planning  active field mapping  localization uncertainty  information gathering algorithms  efficient data collection  fundamental problem  implicit requirement  high-quality maps  informative planning framework  active mapping  Gaussian process model  target environmental field  utility function  field mapping objectives  GP-based mapping scenarios  mean pose uncertainty  map error  indoor temperature mapping scenario  Uncertainty  Planning  Robot sensing systems  Trajectory  Manuals  Robot localization 
Abstract: Information gathering algorithms play a key role in unlocking the potential of robots for efficient data collection in a wide range of applications. However, most existing strategies neglect the fundamental problem of the robot pose uncertainty, which is an implicit requirement for creating robust, high-quality maps. To address this issue, we introduce an informative planning framework for active mapping that explicitly accounts for the pose uncertainty in both the mapping and planning tasks. Our strategy exploits a Gaussian Process (GP) model to capture a target environmental field given the uncertainty on its inputs. For planning, we formulate a new utility function that couples the localization and field mapping objectives in GP-based mapping scenarios in a principled way, without relying on manually-tuned parameters. Extensive simulations show that our approach outperforms existing strategies, reducing mean pose uncertainty and map error. We present a proof of concept in an indoor temperature mapping scenario.


Title: Ensemble of Sparse Gaussian Process Experts for Implicit Surface Mapping with Streaming Data
Key Words: Gaussian processes  mobile robots  path planning  regression analysis  robot vision  SLAM (robots)  sparse Gaussian process experts  implicit surface mapping  streaming data  creating maps  robotics  navigation  compact surface map  continuous implicit surface map  range data  approximate Gaussian process experts  GP models  model complexity  prediction error  real-world data sets  compact surface models  accurate implicit surface models  exact GP regression  subsampled data  Surface treatment  Data models  Predictive models  Covariance matrices  Gaussian processes  Computational modeling  Measurement uncertainty 
Abstract: Creating maps is an essential task in robotics and provides the basis for effective planning and navigation. In this paper, we learn a compact and continuous implicit surface map of an environment from a stream of range data with known poses. For this, we create and incrementally adjust an ensemble of approximate Gaussian process (GP) experts which are each responsible for a different part of the map. Instead of inserting all arriving data into the GP models, we greedily trade-off between model complexity and prediction error. Our algorithm therefore uses less resources on areas with few geometric features and more where the environment is rich in variety. We evaluate our approach on synthetic and real-world data sets and analyze sensitivity to parameters and measurement noise. The results show that we can learn compact and accurate implicit surface models under different conditions, with a performance comparable to or better than that of exact GP regression with subsampled data.


Title: Robust Method for Removing Dynamic Objects from Point Clouds
Key Words: image capture  image filtering  image registration  image representation  object detection  optical radar  robot vision  3D point cloud maps  dynamic object removal  laser scans  lidar scans  object detection  voxel traversal method  Three-dimensional displays  Vehicle dynamics  Octrees  Object detection  Laser modes  Feature extraction 
Abstract: 3D point cloud maps are an accumulation of laser scans obtained at different positions and times. Since laser scans represent a snapshot of the surrounding at the time of capture, they often contain moving objects which may not be observed at all times. Dynamic objects in point cloud maps decrease the quality of maps and affect localization accuracy, hence it is important to remove the dynamic objects from 3D point cloud maps. In this paper, we present a robust method to remove dynamic objects from 3D point cloud maps. Given a registered set of 3D point clouds, we build an occupancy map in which the voxels represent the occupancy state of the volume of space over an extended time period. After building the occupancy map, we use it as a filter to remove dynamic points in lidar scans before adding the points to the map. Furthermore, we accelerate the process of building occupancy maps using object detection and a novel voxel traversal method. Once the occupancy map is built, dynamic object removal can run in real-time. Our approach works well on wide urban roads with stopped or moving traffic and the occupancy maps get better with the inclusion of more lidar scans from the same scene.


Title: Multi-Task Learning for Single Image Depth Estimation and Segmentation Based on Unsupervised Network
Key Words: computer vision  convolutional neural nets  image segmentation  learning (artificial intelligence)  regression analysis  single image depth estimation  unsupervised network  deep neural networks  computer vision tasks  image segmentation  encoder-decoder-based interactive convolutional neural network  multitask learning framework  CNN  pixel depth regression  Image segmentation  Estimation  Task analysis  Training  Feature extraction  Neural networks  Image reconstruction 
Abstract: Deep neural networks have significantly enhanced the performance of various computer vision tasks, including single image depth estimation and image segmentation. However, most existing approaches handle them in supervised manners and require a large number of ground truth labels that consume extensive human efforts and are not always available in real scenarios. In this paper, we propose a novel framework to estimate disparity maps and segment images simultaneously by jointly training an encoder-decoder-based interactive convolutional neural network (CNN) for single image depth estimation and a multiple class CNN for image segmentation. Learning the neural network for one task can be beneficial from simultaneously learning from another one under a multi-task learning framework. We show that our proposed model can learn per-pixel depth regression and segmentation from just a single image input. Extensive experiments on available public datasets, including KITTI, Cityscapes urban, and PASCAL-VOC demonstrate the effectiveness of our model compared with other state-of-the-art methods for both tasks.


Title: 2D to 3D Line-Based Registration with Unknown Associations via Mixed-Integer Programming
Key Words: calibration  image registration  integer programming  iterative methods  mobile robots  robot vision  iterative nearest-neighbor  mixed-integer program  data association  integer variables  3D line-based registration  mixed-integer programming  rigid-body transformation  3D point cloud data  mobile robotics  sensor calibration  linear line-based 2D-3D registration  Three-dimensional displays  Two dimensional displays  Cameras  Cost function  Robot sensing systems  Transforms  Symmetric matrices 
Abstract: Determining the rigid-body transformation be-tween 2D image data and 3D point cloud data has applications for mobile robotics including sensor calibration and localizing into a prior map. Common approaches to 2D-3D registration use least-squares solvers assuming known associations often provided by heuristic front-ends, or iterative nearest-neighbor. We present a linear line-based 2D-3D registration algorithm formulated as a mixed-integer program to simultaneously solve for the correct transformation and data association. Our formulation is explicitly formulated to handle outliers, by modeling associations as integer variables. Additionally, we can constrain the registration to SE(2) to improve runtime and accuracy. We evaluate this search over multiple real-world data sets demonstrating adaptability to scene variation.


Title: Task-Aware Novelty Detection for Visual-based Deep Learning in Autonomous Systems
Key Words: computer vision  decision making  learning (artificial intelligence)  road safety  road traffic  safety-critical software  task-aware novelty detection  self-driving cars  trustworthy prediction  adversarial attacks  life-threatening decisions  learning framework  prediction model  network saliency  learning architecture  decision making  saliency map  in-house indoor driving environment  adversarial attacked images  target prediction  deep-learning driven safety-critical autonomous systems  Training  Task analysis  Predictive models  Roads  Data models  Autonomous systems  Decision making 
Abstract: Deep-learning driven safety-critical autonomous systems, such as self-driving cars, must be able to detect situations where its trained model is not able to make a trustworthy prediction. This ability to determine the novelty of a new input with respect to a trained model is critical for such systems because novel inputs due to changes in the environment, adversarial attacks, or even unintentional noise can potentially lead to erroneous, perhaps life-threatening decisions. This paper proposes a learning framework that leverages information learned by the prediction model in a task-aware manner to detect novel scenarios. We use network saliency to provide the learning architecture with knowledge of the input areas that are most relevant to the decision-making and learn an association between the saliency map and the predicted output to determine the novelty of the input. We demonstrate the efficacy of this method through experiments on real-world driving datasets as well as through driving scenarios in our in-house indoor driving environment where the novel image can be sampled from another similar driving dataset with similar features or from adversarial attacked images from the training dataset. We find that our method is able to systematically detect novel inputs and quantify the deviation from the target prediction through this task-aware approach.


Title: Learning an Action-Conditional Model for Haptic Texture Generation
Key Words: feedback  haptic interfaces  human-robot interaction  image texture  learning (artificial intelligence)  mobile robots  robot vision  tactile sensors  telerobotics  virtual reality  Haptic Texture generation  haptic sensory feedback  user interactions  immersive virtual reality  material properties  haptic vibration feedback  Penn Haptic Texture Toolkit  action-conditional model learning  GelSight measurements  teleoperation system  autonomous robot  GelSight image texture  Haptic interfaces  Autoregressive processes  Force  Acceleration  Predictive models  Solid modeling  Discrete Fourier transforms 
Abstract: Rich haptic sensory feedback in response to user interactions is desirable for an effective, immersive virtual reality or teleoperation system. However, this feedback depends on material properties and user interactions in a complex, non-linear manner. Therefore, it is challenging to model the mapping from material and user interactions to haptic feedback in a way that generalizes over many variations of the user's input. Current methodologies are typically conditioned on user interactions, but require a separate model for each material. In this paper, we present a learned action-conditional model that uses data from a vision-based tactile sensor (GelSight) and user's action as input. This model predicts an induced acceleration that could be used to provide haptic vibration feedback to a user. We trained our proposed model on a publicly available dataset (Penn Haptic Texture Toolkit) that we augmented with GelSight measurements of the different materials. We show that a unified model over all materials outperforms previous methods and generalizes to new actions and new instances of the material categories in the dataset.


Title: Tactile Telerobots for Dull, Dirty, Dangerous, and Inaccessible Tasks
Key Words: computational complexity  dexterous manipulators  haptic interfaces  telerobotics  first-generation telerobot  task complexity  tactile telerobots  inaccessible tasks  highly-dexterous bimanual tactile telerobot  bare human hands  anthropomorphic robot hands  biomimetic tactile sensors  in-hand manipulation  autonomous robotic hands  robotic dexterity  Robot sensing systems  Haptic interfaces  Task analysis  Force  Manipulators  Electrodes 
Abstract: The sense of touch, which is essential for human dexterity, is virtually absent from today's robotic hands. In this work we present progress in creating a highly-dexterous bimanual tactile telerobot, and evaluate its performance compared to bare human hands. The system, consisting of anthropomorphic robot hands, biomimetic tactile sensors, and advanced haptic gloves, enables a human operator to intuitively control and feel what the robotic hands are touching. Through carefully tuned tactile and kinematic mapping it was possible to intuitively perform dexterous operations, including pick and place tasks and even in-hand manipulation, a challenge for most autonomous robotic hands. Performance of the system was evaluated in standard measures of human and robotic dexterity such as the Box and Block test and other YCB benchmarks. This first-generation telerobot was found to have promising performance with the pilot able to do the same tasks in the telerobot between 1/4th to 1/12th the speed of their bare hands depending on the task complexity.


Title: Robotic Control of a Magnetic Swarm for On-Demand Intracellular Measurement
Key Words: biochemistry  biomedical materials  biomedical optical imaging  cellular biophysics  dyes  fluorescence  magnetic particles  medical robotics  micromanipulators  nanomedicine  nanoparticles  pH  fluorescent dyes  biochemical measurements  ion concentrations  signal-to-noise ratios  dye-coated magnetic nanoparticles  magnetic micromanipulation systems  generated swarm  magnetic micromanipulation system  position control accuracy  intracellular pH mapping  global dye treatment  fluorescent dye concentration  intracellular measurement results  robotic control  on-demand intracellular measurement  pH sensitive fluorescent dye-coated magnetic nanoparticles  Coils  Magnetic devices  Magnetic separation  Magnetic resonance imaging  Magnetic particles  Signal to noise ratio  Magnetic nanoparticles 
Abstract: In biology, fluorescent dyes are routinely used for biochemical measurements such as pH and ion concentrations. They, especially when used for detecting a low concentration of ions, suffer from low signal-to-noise ratios (SNR); and increasing the concentration of fluorescent dyes causes more sever cytotoxicity. We invented a new approach that uses a low amount of fluorescent dye-coated magnetic nanoparticles for on-demand, accurately aggregating the nanoparticles and thus fluorescent dyes in a local region inside a cell for intracellular measurement. Experiments proved this approach is capable of achieving a significantly higher SNR and lower cytotoxicity. Different from existing magnetic micromanipulation systems that generate large swarms (several microns and above) or cannot move the generated swarm to an arbitrary position, we developed a five-pole magnetic micromanipulation system and technique for generating a small swarm (e.g., 1 Î¼m; capable of generating a magnetic swarm from 0.52 Î¼m to 52.7 Î¼m with an error <; 7.5 %) and accurately positioning the small swarm (position control accuracy: 0.76 Î¼m). As an example, the system performed intracellular pH mapping using a 1 Î¼m swarm of pH sensitive fluorescent dye-coated magnetic nanoparticles. The swarm had an SNR inside a cell 10 times that by the traditional method, i.e., global dye treatment, with both cases using the same fluorescent dye concentration. Our intracellular measurement results, for the first time, quantitatively revealed the existence of pH gradient and polarized pH distribution in live migrating cells.


Title: Efficient Planning for High-Speed MAV Flight in Unknown Environments Using Online Sparse Topological Graphs
Key Words: aerospace navigation  air safety  autonomous aerial vehicles  collision avoidance  graph theory  infinite horizon  microrobots  mobile robots  probability  robot vision  search problems  high-speed MAV flight  online sparse topological graphs  safe high-speed autonomous navigation  local planning grid  computationally-efficient planning architecture  safe high-speed operation  longer-term memory  motion primitive-based local receding horizon planner  memory-efficient sparse topological graph  planning system  complex simulation environments  robot decision making  probabilistic collision avoidance  safe rerouting  Planning  Collision avoidance  Robot sensing systems  Libraries  Safety  Trajectory 
Abstract: Safe high-speed autonomous navigation for MAVs in unknown environments requires fast planning to enable the robot to adapt and react quickly to incoming information about obstacles within the world. Furthermore, when operating in environments not known a priori, the robot may make decisions that lead to dead ends, necessitating global replanning through a map of the environment outside of a local planning grid. This work proposes a computationally-efficient planning architecture for safe high-speed operation in unknown environments that incorporates a notion of longer-term memory into the planner enabling the robot to accurately plan to locations no longer contained within a local map. A motion primitive-based local receding horizon planner that uses a probabilistic collision avoidance methodology enables the robot to generate safe plans at fast replan rates. To provide global guidance, a memory-efficient sparse topological graph is created online from a time history of the robot's path and a geometric notion of visibility within the environment to search for alternate pathways towards the desired goal if a dead end is encountered. The safety and performance of the proposed planning system is evaluated at speeds up to 10m/s, and the approach is tested in a set of large-scale, complex simulation environments containing dead ends. These scenarios lead to failure cases for competing methods; however, the proposed approach enables the robot to safely reroute and reach the desired goal.


Title: Reactive Temporal Logic Planning for Multiple Robots in Unknown Environments
Key Words: mobile robots  multi-robot systems  path planning  robot dynamics  temporal logic  multiple robots  reactive mission  unknown environment  temporal logic planning approaches  robot dynamics  known environments  abstraction-free LTL planning algorithm  complex mission planning  complex planning tasks  co-safe linear temporal logic formulas  reactive temporal logic planning  Robot sensing systems  Planning  Task analysis  Heuristic algorithms  Automata 
Abstract: This paper proposes a new reactive mission planning algorithm for multiple robots that operate in unknown environments. The robots are equipped with individual sensors that allow them to collectively learn and continuously update a map of the unknown environment. The goal of the robots is to accomplish complex tasks, captured by global co-safe Linear Temporal Logic (LTL) formulas. The majority of existing temporal logic planning approaches rely on discrete abstractions of the robot dynamics operating in known environments and, as a result, they cannot be applied to the more realistic scenarios where the environment is initially unknown. In this paper, we address this novel challenge by proposing the first reactive, and abstraction-free LTL planning algorithm that can be applied for complex mission planning of multiple robots operating in unknown environments. Our algorithm is reactive in the sense that temporal logic planning is adapting to the updated map of the environment and abstraction-free as it does not rely on designing abstractions of robot dynamics. Our proposed algorithm is complete under mild assumptions on the structure of the environment and the sensor models. Our paper provides extensive numerical simulations and hardware experiments that illustrate the theoretical analysis and show that the proposed algorithm can address complex planning tasks in unknown environments.


