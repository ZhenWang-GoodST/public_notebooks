total paper: 900
Title: Metrically-Scaled Monocular SLAM using Learned Scale Factors
Key Words: cameras  graph theory  mobile robots  neural nets  robot vision  SLAM (robots)  geometric SLAM factor graph  SLAM systems  relative geometry  learned depth estimation approaches  learned depth predictions  image space  network architecture  coarse images  GPU acceleration  learned metric data  unary scale factors  hardware accelerators  observable epipolar geometry  monocular SLAM  learned scale factors  monocular simultaneous localization and mapping  hardware acceleration  neural network  Simultaneous localization and mapping  Feature extraction  Cameras  Loss measurement  Neural networks  Estimation 
Abstract: We propose an efficient method for monocular simultaneous localization and mapping (SLAM) that is capable of estimating metrically-scaled motion without additional sensors or hardware acceleration by integrating metric depth predictions from a neural network into a geometric SLAM factor graph. Unlike learned end-to-end SLAM systems, ours does not ignore the relative geometry directly observable in the images. Unlike existing learned depth estimation approaches, ours leverages the insight that when used to estimate scale, learned depth predictions need only be coarse in image space. This allows us to shrink our network to the point that performing inference on a standard CPU becomes computationally tractable.We make several improvements to our network architecture and training procedure to address the lack of depth observability when using coarse images, which allows us to estimate spatially coarse, but depth-accurate predictions in only 30 ms per frame without GPU acceleration. At runtime we incorporate the learned metric data as unary scale factors in a Sim(3) pose graph. Our method is able to generate accurate, scaled poses without additional sensors, hardware accelerators, or special maneuvers and does not ignore or corrupt the observable epipolar geometry. We show compelling results on the KITTI benchmark dataset in addition to real-world experiments with a handheld camera.


Title: Inertial-Only Optimization for Visual-Inertial Initialization
Key Words: feature extraction  least squares approximations  maximum likelihood estimation  optimisation  SLAM (robots)  EuRoC dataset show  time visual-inertial initialization  optimal estimation problem  maximum-a-posteriori estimation  algebraic equations  ad-hoc cost functions  ORB-SLAM visual-inertial boosting  inertial-only optimization  IMU measurement uncertainty  MAP estimation  least squares  Estimation  Trajectory  Simultaneous localization and mapping  Gravity  Visualization  Optimization  Accelerometers 
Abstract: We formulate for the first time visual-inertial initialization as an optimal estimation problem, in the sense of maximum-a-posteriori (MAP) estimation. This allows us to properly take into account IMU measurement uncertainty, which was neglected in previous methods that either solved sets of algebraic equations, or minimized ad-hoc cost functions using least squares. Our exhaustive initialization tests on EuRoC dataset show that our proposal largely outperforms the best methods in the literature, being able to initialize in less than 4 seconds in almost any point of the trajectory, with a scale error of 5.3% on average. This initialization has been integrated into ORB-SLAM Visual-Inertial boosting its robustness and efficiency while maintaining its excellent accuracy.


Title: Keypoint Description by Descriptor Fusion Using Autoencoders
Key Words: convolutional neural nets  image fusion  image matching  learning (artificial intelligence)  robot vision  SLAM (robots)  keypoint description  keypoint matching  computer vision  visual simultaneous localization and mapping  SLAM  matching operation  descriptor fusion model  robust keypoint descriptor  CNN-based descriptors  DFM architecture  CNN models  mean mAP  HardNet  DenseNet169  convolutional neural networks  Fuses  Lighting  Robustness  Computer vision  Simultaneous localization and mapping  Image coding 
Abstract: Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45% and 6.53% higher than HardNet and DenseNet169, respectively.


Title: Towards Noise Resilient SLAM
Key Words: cameras  image colour analysis  image sensors  optimisation  photometry  pose estimation  SLAM (robots)  stereo image processing  RGB-D input  TUM datasets  EuRoC datasets  stereo image pairs  adaptive algorithm  error vector  outlier rejection  computational efficiency  map-point consensus  adaptive virtual camera  noise resilient SLAM  ORB-SLAM2  sparse-indirect SLAM systems  virtual camera location  axial depth error  pose optimization  consensus information  axial noise  lateral noise  depth noise components  axial components  lateral components  noise sources  scale information  SLAM frameworks  depth sensors  photometric invariance properties  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Measurement  Feature extraction  Optimization 
Abstract: Sparse-indirect SLAM systems have been dominantly popular due to their computational efficiency and photometric invariance properties. Depth sensors are critical to SLAM frameworks for providing scale information to the 3D world, yet known to be plagued by a wide variety of noise sources, possessing lateral and axial components. In this work, we demonstrate the detrimental impact of these depth noise components on the performance of the state-of-the-art sparse-indirect SLAM system (ORB-SLAM2). We propose (i) Map-Point Consensus based Outlier Rejection (MC-OR) to counter lateral noise, and (ii) Adaptive Virtual Camera (AVC) to combat axial noise accurately. MC-OR utilizes consensus information between multiple sightings of the same landmark to disambiguate noisy depth and filter it out before pose optimization. In AVC, we introduce an error vector as an accurate representation of the axial depth error. We additionally propose an adaptive algorithm to find the virtual camera location for projecting the error used in the objective function of the pose optimization. Our techniques work equally well for stereo image pairs and RGB-D input directly used by sparse-indirect SLAM systems. Our methods were tested on the TUM (RGB-D) and EuRoC (stereo) datasets and we show that they outperform existing state-of-the-art ORB-SLAM2 by 2-3x, especially in sequences critically affected by depth noise.


Title: LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of Perceptually-Degraded Subterranean Environments
Key Words: distance measurement  geophysical image processing  mobile robots  multi-robot systems  optical radar  robot vision  SLAM (robots)  stereo image processing  terrain mapping  tunnels  long corridors  salient features  spurious loop closures  repetitive appearance  stark contrast  highly-accurate 3D maps  underground extraterrestrial worlds  lidar-based multirobot SLAM system  DARPA subterranean challenge  subterranean operation  accurate lidar-based front-end  perceptually-degraded subterranean environments  complex subterranean environments  off-nominal conditions  uneven terrains  slippery terrains  large-scale autonomous mapping-positioning  simultaneous localization and mapping  unknown subterranean environment  large-scale subterranean environment  complex subterranean environment  inaccurate wheel odometry  disaster response  flexible back-end  robust back-end  tunnel circuit  Simultaneous localization and mapping  Laser radar  Three-dimensional displays  Base stations  Trajectory 
Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, unknown, and complex subterranean environments is a challenging problem. Sensors must operate in off-nominal conditions; uneven and slippery terrains make wheel odometry inaccurate, while long corridors without salient features make exteroceptive sensing ambiguous and prone to drift; finally, spurious loop closures that are frequent in environments with repetitive appearance, such as tunnels and mines, could result in a significant distortion of the entire map. These challenges are in stark contrast with the need to build highly-accurate 3D maps to support a wide variety of applications, ranging from disaster response to the exploration of underground extraterrestrial worlds. This paper reports on the implementation and testing of a lidar-based multi-robot SLAM system developed in the context of the DARPA Subterranean Challenge. We present a system architecture to enhance subterranean operation, including an accurate lidar-based front-end, and a flexible and robust back-end that automatically rejects outlying loop closures. We present an extensive evaluation in large-scale, challenging subterranean environments, including the results obtained in the Tunnel Circuit of the DARPA Subterranean Challenge. Finally, we discuss potential improvements, limitations of the state of the art, and future research directions.


Title: BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors
Key Words: Bayes methods  Gaussian processes  neural nets  object detection  NonMaximum suppression components  common object detection datasets  BayesOD  minimum Gaussian uncertainty error metric  minimum Categorical uncertainty error metric  Bayesian approach  deep object detectors  deep neural networks  uncertainty measures  output predictions  detectors nonmaximum suppression stage  anchor-based object detection  uncertainty estimation approach  standard object detector inference  Uncertainty  Detectors  Neural networks  Bayes methods  Estimation  Object detection  Measurement uncertainty 
Abstract: When incorporating deep neural networks into robotic systems, a major challenge is the lack of uncertainty measures associated with their output predictions. Methods for uncertainty estimation in the output of deep object detectors (DNNs) have been proposed in recent works, but have had limited success due to 1) information loss at the detectors nonmaximum suppression (NMS) stage, and 2) failure to take into account the multitask, many-to-one nature of anchor-based object detection. To that end, we introduce BayesOD, an uncertainty estimation approach that reformulates the standard object detector inference and Non-Maximum suppression components from a Bayesian perspective. Experiments performed on four common object detection datasets show that BayesOD provides uncertainty estimates that are better correlated with the accuracy of detections, manifesting as a significant reduction of 9.77%-13.13% on the minimum Gaussian uncertainty error metric and a reduction of 1.63%-5.23% on the minimum Categorical uncertainty error metric. Code will be released at https://github.com/asharakeh/bayes-od-rc.


Title: Learning Object Placements For Relational Instructions by Hallucinating Scene Representations
Key Words: control engineering computing  convolutional neural nets  human-robot interaction  image classification  image representation  learning (artificial intelligence)  probability  robot vision  object placement learning  relational instructions  spatial relation  convolutional neural network  pixelwise object placement probability estimation  hallucinated high-level scene representation classification  pixelwise relational probabilities  human-robot experiments  single input image  learning signal  3D models  Robots  Training  Three-dimensional displays  Natural languages  Graphical models  Distribution functions  Solid modeling 
Abstract: Robots coexisting with humans in their environment and performing services for them need the ability to interact with them. One particular requirement for such robots is that they are able to understand spatial relations and can place objects in accordance with the spatial relations expressed by their user. In this work, we present a convolutional neural network for estimating pixelwise object placement probabilities for a set of spatial relations from a single input image. During training, our network receives the learning signal by classifying hallucinated high-level scene representations as an auxiliary task. Unlike previous approaches, our method does not require ground truth data for the pixelwise relational probabilities or 3D models of the objects, which significantly expands the applicability in practical applications. Our results obtained using real-world data and human-robot experiments demonstrate the effectiveness of our method in reasoning about the best way to place objects to reproduce a spatial relation. Videos of our experiments can be found at https://youtu.be/zaZkHTWFMKM.


Title: Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  mobile robots  neural nets  adversarial agent training  learned control policies  autonomous driving  deep neural network-driven  adversarial reinforcement learning agent  autonomous vehicle problem  automated black box testing  safety-critical applications  control policy  deep neural networks  autonomous vehicles  robot navigation  robotic arm manipulation  control problems  deep learning  deep control policies  Testing  Autonomous vehicles  Training  Learning (artificial intelligence)  Machine learning  Robots  Robustness 
Abstract: Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.


Title: TRASS: Time Reversal as Self-Supervision
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  optimal control  predictive control  robot programming  robot vision  self-supervision technique  high level supervision  time-reversal model  self-supervised model  goal states  complex manipulation tasks  tetris-style block pairs  visual model predictive control  robot learning  RGB camera input  TRASS  Time Reversal as Self-Supervision  Task analysis  Trajectory  Robots  Transmission line measurements  Visualization  Predictive models  Grippers 
Abstract: A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning based approaches have shown promise in producing robust policies, but require heavy supervision and large number of environment interactions, especially from visual inputs. We propose a novel self-supervision technique that uses time-reversal to provide high level supervision to reach goals. In particular, we introduce the time-reversal model (TRM), a self-supervised model which explores outward from a set of goal states and learns to predict these trajectories in reverse. This provides a high level plan towards goals, allowing us to learn complex manipulation tasks with no demonstrations or exploration at test time. We test our method on the domain of assembly, specifically the mating of tetris-style block pairs. Using our method operating atop visual model predictive control, we are able to assemble tetris blocks on a KuKa IIWA-7 using only uncalibrated RGB camera input, and generalize to unseen block pairs. Project's-page: https://sites.google.com/view/time-reversal.


Title: Advanced BIT* (ABIT*): Sampling-Based Planning with Advanced Graph-Search Techniques
Key Words: approximation theory  graph theory  path planning  robots  sampling methods  search problems  sampling-based approximation  Advanced BIT*  almost-surely asymptotically optimal sampling-based planners  ABIT*  unified planning paradigm  path planning problem  advanced graph-search techniques  robotics  truncated anytime graph-based searches  RRT*  single-query  Planning  Approximation algorithms  Search problems  Path planning  Robots  Acceleration  Conferences 
Abstract: Path planning is an active area of research essential for many applications in robotics. Popular techniques include graph-based searches and sampling-based planners. These approaches are powerful but have limitations.This paper continues work to combine their strengths and mitigate their limitations using a unified planning paradigm. It does this by viewing the path planning problem as the two subproblems of search and approximation and using advanced graph-search techniques on a sampling-based approximation.This perspective leads to Advanced BIT*. ABIT* combines truncated anytime graph-based searches, such as ATD*, with anytime almost-surely asymptotically optimal sampling-based planners, such as RRT*. This allows it to quickly find initial solutions and then converge towards the optimum in an anytime manner. ABIT* outperforms existing single-query, sampling- based planners on the tested problems in ℝ4 and ℝ8, and was demonstrated on real-world problems with NASA/JPL-Caltech.


Title: Dynamic Movement Primitives for moving goals with temporal scaling adaptation
Key Words: human-robot interaction  learning (artificial intelligence)  mobile robots  motion control  human robot collaboration  motion profiles  learned kinematic pattern  KUKA LWR4+ robot  DMP  dynamic movement primitives framework  adaptive temporal scaling  static goal  temporal scaling adaptation  moving goal  Trajectory  Robots  Dynamics  Encoding  Collaboration  Adaptation models  Kinematics 
Abstract: In this work, we propose an augmentation to the Dynamic Movement Primitives (DMP) framework which allows the system to generalize to moving goals without the use of any known or approximation model for estimating the goal's motion. We aim to maintain the demonstrated velocity levels during the execution to the moving goal, generating motion profiles appropriate for human robot collaboration. The proposed method employs a modified version of a DMP, learned by a demonstration to a static goal, with adaptive temporal scaling in order to achieve reaching of the moving goal with the learned kinematic pattern. Only the current position and velocity of the goal are required. The goal's reaching error and its derivative is proved to converge to zero via contraction analysis. The theoretical results are verified by simulations and experiments on a KUKA LWR4+ robot.


Title: Navigating Discrete Difference Equation Governed WMR by Virtual Linear Leader Guided HMPC
Key Words: difference equations  mobile robots  multi-robot systems  nonlinear control systems  path planning  predictive control  reachability analysis  set theory  navigating discrete difference equation governed WMR  virtual linear leader guided HMPC  model predictive control  classical wheeled mobile robot navigation problem  hierarchical MPC  state-of-the-art MPC  WMR navigation  nonexistence  nontrivial linear system  under-approximate reachable set  VLL-MPC  HMPC structure  virtual linear system  under-approximate path  RRT*  Navigation  Linear systems  Stability analysis  Planning  Robots  Mathematical model  Nonlinear dynamical systems 
Abstract: In this paper, we revisit model predictive control (MPC) for the classical wheeled mobile robot (WMR) navigation problem. We prove that the reachable set based hierarchical MPC (HMPC), a state-of-the-art MPC, cannot handle WMR navigation in theory due to the non-existence of non-trivial linear system with an under-approximate reachable set of WMR. Nevertheless, we propose a virtual linear leader guided MPC (VLL-MPC) to enable HMPC structure. Different from current HMPCs, we use a virtual linear system with an under-approximate path set rather than the traditional trace set to guide the WMR. We provide a valid construction of the virtual linear leader. We prove the stability of VLL-MPC, and discuss its complexity. In the experiment, we demonstrate the advantage of VLL-MPC empirically by comparing it with NMPC, LMPC and anytime RRT* in several scenarios.


Title: Aggregation and localization of simple robots in curved environments
Key Words: friction  microrobots  mobile robots  path planning  curved environments  extremely simple robots  biomedical applications  tiny robots moves  shared external stimulus  low-friction models  environment boundaries  Robot sensing systems  Collision avoidance  Computational modeling  Biological system modeling  Propulsion 
Abstract: This paper is about the closely-related problems of localization and aggregation for extremely simple robots, for which the only available action is to move in a given direction as far as the geometry of the environment allows. Such problems may arise, for example, in biomedical applications, wherein a large group of tiny robots moves in response to a shared external stimulus. Specifically, we extend the prior work on these kinds of problems presenting two algorithms for localization in environments with curved (rather than polygonal) boundaries and under low-friction models of interaction with the environment boundaries. We present both simulations and physical demonstrations to validate the approach.


Title: Motion Primitives-based Path Planning for Fast and Agile Exploration using Aerial Robots
Key Words: aerospace robotics  collision avoidance  mobile robots  motion control  motion primitives-based path planning  agile exploration  aerial robots  path planning strategy  microaerial vehicles  volumetric representation  collision-tolerant flying robot  velocity 2.0 m/s  size 0.8 m  Vehicle dynamics  Collision avoidance  Path planning  Unmanned aerial vehicles  Educational robots  Dynamics 
Abstract: This paper presents a novel path planning strategy for fast and agile exploration using aerial robots. Tailored to the combined need for large-scale exploration of challenging and confined environments, despite the limited endurance of micro aerial vehicles, the proposed planner employs motion primitives to identify admissible paths that search the configuration space, while exploiting the dynamic flight properties of small aerial robots. Utilizing a computationally efficient volumetric representation of the environment, the planner provides fast collision-free and future-safe paths that maximize the expected exploration gain and ensure continuous fast navigation through the unknown environment. The new method is field-verified in a set of deployments relating to subterranean exploration and specifically, in both modern and abandoned underground mines in Northern Nevada utilizing a 0.55m-wide collision-tolerant flying robot exploring with a speed of up to 2m/s and navigating sections with width as small as 0.8m.


Title: Unsupervised Anomaly Detection for Self-flying Delivery Drones
Key Words: aerodynamics  autonomous aerial vehicles  control engineering computing  learning (artificial intelligence)  mobile robots  regression analysis  unsupervised anomaly detection  hybrid aerial vehicles  machine learning models  flight profiles  flight log measurements  sensor readings  predictive flight dynamics models  aircraft aerodynamics  self-flying delivery drones  Aerodynamics  Smoothing methods  Robustness  Training  Anomaly detection  Optimization  Aircraft 
Abstract: We propose a novel anomaly detection framework for a fleet of hybrid aerial vehicles executing high-speed package pickup and delivery missions. The detection is based on machine learning models of normal flight profiles, trained on millions of flight log measurements of control inputs and sensor readings. We develop a new scalable algorithm for robust regression which can simultaneously fit predictive flight dynamics models while identifying and discarding abnormal flight missions from the training set. The resulting unsupervised estimator has a very high breakdown point and can withstand massive contamination of training data to uncover what normal flight patterns look like, without requiring any form of prior knowledge of aircraft aerodynamics or manual labeling of anomalies upfront. Across many different anomaly types, spanning simple 3sigma statistical thresholds to turbulence and other equipment anomalies, our models achieve high detection rates across the board. Our method consistently outperforms alternative robust detection methods on synthetic benchmark problems. To the best of our knowledge, dynamics modeling of hybrid delivery drones for anomaly detection at the scale of 100 million measurements from 5000 real flight missions in variable flight conditions is unprecedented.


Title: Keyfilter-Aware Real-Time UAV Object Tracking
Key Words: autonomous aerial vehicles  image filtering  image motion analysis  learning (artificial intelligence)  mobile robots  object detection  object tracking  robot vision  SLAM (robots)  keyframe-based simultaneous localization and mapping  keyfilter restriction  visual tracking  background distraction  filter corruption  boundary effect  unmanned aerial vehicle  correlation filter-based tracking  keyfilter-aware real-time UAV object tracking  Unmanned aerial vehicles  Correlation  Visualization  Object tracking  Frequency-domain analysis  Real-time systems 
Abstract: Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.


Title: Aerial Regrasping: Pivoting with Transformable Multilink Aerial Robot
Key Words: aerospace robotics  dexterous manipulators  grippers  motion control  remotely operated vehicles  stability  aerial regrasping  aerial manipulator  dexterous manipulation  transformable multilink aerial robot  transformable multilink drone  grasping stability  thrust force  continous grasping force  admittance controller  impedance controller  contact aware regrasping  Task analysis  Force  Grasping  Unmanned aerial vehicles  Rotors  End effectors 
Abstract: Regrasping is one of the most common and important manipulation skills used in our daily life. However, aerial regrasping has not been seriously investigated yet, since most of the aerial manipulator lacks dexterous manipulation abilities except for the basic pick-and-place. In this paper, we focus on pivoting a long box, which is one of the most classical problems among regrasping researches, using a transformable multilink aerial robot. First, we improve our previous controller by compensating for the external wrench. Second, we optimize the joints configuration of our transformable multilink drone for stable grasping form under the constraints of thrust force and joints effort. Third, we sequentially optimize the grasping force in the pivoting process. The optimization goal is to generate continous grasping force whilst maximizing the friction force in case of the downwash, which would influence the grasped object and is difficult to model. Fourth, we develop the impedance controller in joint space and admittance controller in task space. As far as we know, it is the first research to achieve extrinsic contact-aware regrasping task on aerial robots.


Title: Grounding Language to Landmarks in Arbitrary Outdoor Environments
Key Words: human-robot interaction  mobile robots  natural language processing  path planning  robot vision  natural language phrases  arbitrary outdoor environments  urban environments  natural language commands  robot control  semantic similarities  Natural languages  Semantics  Robots  Grounding  Training  Planning  Databases 
Abstract: Robots operating in outdoor, urban environments need the ability to follow complex natural language commands which refer to never-before-seen landmarks. Existing approaches to this problem are limited because they require training a language model for the landmarks of a particular environment before a robot can understand commands referring to those landmarks. To generalize to new environments outside of the training set, we present a framework that parses references to landmarks, then assesses semantic similarities between the referring expression and landmarks in a predefined semantic map of the world, and ultimately translates natural language commands to motion plans for a drone. This framework allows the robot to ground natural language phrases to landmarks in a map when both the referring expressions to landmarks and the landmarks themselves have not been seen during training. We test our framework with a 14-person user evaluation demonstrating an end-to-end accuracy of 76.19% in an unseen environment. Subjective measures show that users find our system to have high performance and low workload. These results demonstrate our approach enables untrained users to control a robot in large unseen outdoor environments with unconstrained natural language.


Title: A Synchronization Approach for Achieving Cooperative Adaptive Cruise Control Based Non-Stop Intersection Passing
Key Words: adaptive control  control system synthesis  distributed control  Lyapunov methods  mobile robots  multi-robot systems  position control  road traffic control  road vehicles  stability  velocity control  synchronization approach  adaptive cruise control based nonstop intersection passing  intelligent vehicles  cruise control performance  traffic congestion  increasing traffic flow capacity  CACC problem  synchronization control  spatial-temporal synchronization mechanism  vehicle platoon control  robust CACC  cross-coupling based space synchronization mechanism  distributed control algorithm  single-lane CACC  vehicle-to-vehicle communications  autonomous vehicles  desired platoon trajectory  expected inter-vehicle distance  enter-time scheduling mechanism  high-level intersection control strategy  Lyapunov-based time-domain stability analysis approach  traditional string stability based approach  CACC system  Synchronization  Stability analysis  Cruise control  Robustness  Autonomous vehicles  Motion control  Acceleration 
Abstract: Cooperative adaptive cruise control (CACC) of intelligent vehicles contributes to improving cruise control performance, reducing traffic congestion, saving energy and increasing traffic flow capacity. In this paper, we resolve the CACC problem from the viewpoint of synchronization control, our main idea is to introduce the spatial-temporal synchronization mechanism into vehicle platoon control to achieve the robust CACC and to further realize the non-stop intersection control. Firstly, by introducing the cross-coupling based space synchronization mechanism, a distributed control algorithm is presented to achieve the single-lane CACC in the presence of vehicle-to-vehicle (V2V) communications, which enables autonomous vehicles to track the desired platoon trajectory while synchronizing their longitudinal velocities to keeping the expected inter-vehicle distance. Secondly, by designing the enter-time scheduling mechanism (temporal synchronization), a high-level intersection control strategy is proposed to command vehicles to form a virtual platoon to pass through the intersection without stopping. Thirdly, a Lyapunov-based time-domain stability analysis approach is presented. Compared with the traditional string stability based approach, the proposed approach guarantees the global asymptotical convergence of the proposed CACC system. Experiments in the small-scale simulated system demonstrate the effectiveness of the proposed approach.


Title: Urban Driving with Conditional Imitation Learning
Key Words: cameras  computer vision  decision making  driver information systems  learning (artificial intelligence)  mobile robots  road traffic  road vehicles  real-world urban autonomous driving  human driving demonstrations  user-defined route  single camera view  heavily cropped frames  lateral control  longitudinal control  real-world complexities  end-to-end conditional imitation learning approach  urban routes  simple traffic  autonomous vehicle  European urban streets  urban driving  hand-crafting generalised decision-making rules  Cameras  Sensor fusion  Autonomous vehicles  Roads  Computational modeling  Aerospace electronics  Training 
Abstract: Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.


Title: Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching
Key Words: cameras  distance measurement  gradient methods  image matching  image registration  image sensors  pose estimation  robot vision  SLAM (robots)  stereo image processing  visual SLAM systems  photometric consistency  gradient-based dissimilarity  camera pose estimation  map building  central ingredients  autonomous robots  photometric error  gradient orientation  magnitude-dependent scaling term  stereo estimation  visual odometry systems  direct image registration tasks  robust estimates  scene depth  camera trajectory  mapping capabilities  mobile robots  sensor data registration  Measurement  Robustness  Cameras  Estimation  Visual odometry  Simultaneous localization and mapping  Robot vision systems 
Abstract: Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicate that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.


Title: ICS: Incremental Constrained Smoothing for State Estimation
Key Words: matrix decomposition  mobile robots  optimisation  path planning  robot vision  SLAM (robots)  state estimation  ICS  primal-dual method  matrix factorizations  primal-dual methods  incremental factorization  matrix structure  incremental unconstrained optimization  robot state estimate  smoothing-based estimation methods  state estimation  incremental constrained smoothing  Optimization  Smoothing methods  Time measurement  Integrated circuits  Simultaneous localization and mapping 
Abstract: A robot operating in the world constantly receives information about its environment in the form of new measurements at every time step. Smoothing-based estimation methods seek to optimize for the most likely robot state estimate using all measurements up till the current time step. Existing methods solve for this smoothing objective efficiently by framing the problem as that of incremental unconstrained optimization. However, in many cases observed measurements and knowledge of the environment is better modeled as hard constraints derived from real-world physics or dynamics. A key challenge is that the new optimality conditions introduced by the hard constraints break the matrix structure needed for incremental factorization in these incremental optimization methods. Our key insight is that if we leverage primal-dual methods, we can recover a matrix structure amenable to incremental factorization. We propose a framework ICS that combines a primal-dual method like the Augmented Lagrangian with an incremental Gauss Newton approach that reuses previously computed matrix factorizations. We evaluate ICS on a set of simulated and real-world problems involving equality constraints like object contact and inequality constraints like collision avoidance.


Title: A fast and practical method of indoor localization for resource-constrained devices with limited sensing
Key Words: collision avoidance  indoor radio  mobile robots  navigation  probability  fast method  indoor localization  resource-constrained devices  limited sensing capabilities  wearable devices  sparse WiFi  image-based measurements  dense signal maps  conditional random fields  agent positions  known floor plan  sparse absolute position estimates  motion sequences  low-quality measurements  handheld devices  mobile devices  sensory data  Wireless fidelity  Position measurement  Computational modeling  Dead reckoning  Robot sensing systems  Buildings 
Abstract: We describe and experimentally demonstrate a practical method for indoor localization using measurements obtained from resource-constrained devices with limited sensing capabilities. We focus on handheld/mobile devices but the method can be useful for a variety of wearable devices. Our system works with sparse WiFi or image-based measurements, avoiding laborious site surveying for dense signal maps and runs in real-time. It uses Conditional Random Fields to infer the most probable sequence of agent positions from a known floor plan, dead reckoning and sparse absolute position estimates. Our solution leverages known topology of the environment by pre-computing allowed motion sequences of an agent, which are then used to constraint the motion inferred from the sensory data. The system is evaluated in a typical office building, demonstrating good accuracy and robustness to sparse, low-quality measurements.


Title: Long-Term Robot Navigation in Indoor Environments Estimating Patterns in Traversability Changes
Key Words: mobile robots  navigation  path planning  term robot navigation  traversability changes  mobile robots  hospitals  informed decisions  probabilistic graphical model  currently unobserved locations  indoor environments  Robots  Navigation  Predictive models  Correlation  Probabilistic logic  Planning  Indoor environments 
Abstract: Nowadays, mobile robots are deployed in many indoor environments such as offices or hospitals. These environments are subject to changes in the traversability that often happen following patterns. In this paper, we investigate the problem of navigating in such environments over extended periods of time by capturing and exploiting these patterns to make informed decisions for navigation. Our approach uses a probabilistic graphical model to incrementally estimate a model of the traversability changes from the robot's observations and to make predictions at currently unobserved locations. In the belief space defined by the predictions, we plan paths that trade off the risk to encounter obstacles and the information gain of visiting unknown locations. We implemented our approach and tested it in different indoor environments. The experiments suggest that, in the long run, our approach leads robots to navigate along shorter paths compared to following a greedy shortest path policy.


Title: Sample-and-computation-efficient Probabilistic Model Predictive Control with Random Features
Key Words: approximation theory  Gaussian processes  learning (artificial intelligence)  predictive control  random features  Gaussian processes  reinforcement learning methods  model predictive control  MPC  computational cost  linear Gaussian model  approximated GP dynamics  state prediction  simulated robot control tasks  sample-and-computation-efficient nature  model-based RL method  analytic moment-matching scheme  Training  Predictive models  Kernel  Probabilistic logic  Computational modeling  Computational efficiency  Uncertainty 
Abstract: Gaussian processes (GPs) based Reinforcement Learning (RL) methods with Model Predictive Control (MPC) have demonstrated their excellent sample efficiency. However, since the computational cost of GPs largely depends on the training sample size, learning an accurate dynamics using GPs result in low control frequency in MPC. To alleviate this trade-off and achieve a sample-and-computation-efficient nature, we propose a novel model-based RL method with MPC. Our approach employs a linear Gaussian model with randomized features using the Fastfood as an approximated GP dynamics. Then, we derive an analytic moment-matching scheme in state prediction with the model and uncertain inputs. As a result, the computational cost of the MPC in our RL method does not depend on the training sample size and can improve the control frequency over previous methods. Through experiments with simulated and real robot control tasks, the sample efficiency, as well as the computation efficiency of our model-based RL method, are demonstrated.


Title: Sample-Efficient Robot Motion Learning using Gaussian Process Latent Variable Models
Key Words: Gaussian processes  learning systems  manipulator dynamics  motion control  search problems  sample-efficient robot motion learning  Gaussian process latent variable models  robotic manipulators  household environments  kinesthetic teaching  parametric function  movement primitive  mutual-information-weighted Gaussian process latent variable model  trial-and-error  trajectory production  task dynamics  MP parameter latent space  robot motion task  search space  surrogate model  PS algorithms  policy search reinforcement learning  Gaussian processes  Optimization  Trajectory  Task analysis  Robots  Mutual information  Kernel 
Abstract: Robotic manipulators are reaching a state where we could see them in household environments in the following decade. Nevertheless, such robots need to be easy to instruct by lay people. This is why kinesthetic teaching has become very popular in recent years, in which the robot is taught a motion that is encoded as a parametric function - usually a Movement Primitive (MP)-. This approach produces trajectories that are usually suboptimal, and the robot needs to be able to improve them through trial-and-error. Such optimization is often done with Policy Search (PS) reinforcement learning, using a given reward function. PS algorithms can be classified as model-free, where neither the environment nor the reward function are modelled, or model-based, which can use a surrogate model of the reward function and/or a model for the dynamics of the task. However, MPs can become very high-dimensional in terms of parameters, which constitute the search space, so their optimization often requires too many samples. In this paper, we assume we have a robot motion task characterized with an MP of which we cannot model the dynamics. We build a surrogate model for the reward function, that maps an MP parameter latent space (obtained through a Mutual-information-weighted Gaussian Process Latent Variable Model) into a reward. While we do not model the task dynamics, using mutual information to shrink the task space makes it more consistent with the reward and so the policy improvement is faster in terms of sample efficiency.


Title: Efficient Updates for Data Association with Mixtures of Gaussian Processes
Key Words: covariance matrices  Gaussian processes  mixture models  probability  robot vision  GP mixtures  data association  Gaussian processes  probabilistic approach  important estimation  classification tasks  robotics applications  GP-based methods  sparse methods  covariance matrix  orthogonal approach  GP inference  online update scheme  sparse GPs  memoisation approach  robotic vision applications  Covariance matrices  Robots  Gaussian processes  Mixture models  Sparse representation  Inference algorithms  Probabilistic logic 
Abstract: Gaussian processes (GPs) enable a probabilistic approach to important estimation and classification tasks that arise in robotics applications. Meanwhile, most GP-based methods are often prohibitively slow, thereby posing a substantial barrier to practical applications. Existing "sparse" methods to speed up GPs seek to either make the model more sparse, or find ways to more efficiently manage a large covariance matrix. In this paper, we present an orthogonal approach that memoises (i.e. reuses) previous computations in GP inference. We demonstrate that a substantial speedup can be achieved by incorporating memoisation into applications in which GPs must be updated frequently. Moreover, we derive a novel online update scheme for sparse GPs that can be used in conjunction with our memoisation approach for a synergistic improvement in performance. Across three robotic vision applications, we demonstrate between 40-100% speed-up over the standard method for inference in GP mixtures.


Title: Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot End Effector Position
Key Words: biomechanics  end effectors  medical robotics  position control  surgery  telerobotics  time data driven precision Estimator  RAVEN-II surgical robot end effector position  surgical robots  cable-driven nature  kinematics calcu-lation  reported end effector position  position inaccuracy  data-driven pipeline  robot end effector position precision estimation  improved end effector position error  RMS  entire robot workspace  End effectors  Cameras  Medical robotics  Robot sensing systems  Image edge detection  Surgery 
Abstract: Surgical robots have been introduced to operating rooms over the past few decades due to their high sensitivity, small size, and remote controllability. The cable-driven nature of many surgical robots allows the systems to be dexterous and lightweight, with diameters as low as 5mm. However, due to the slack and stretch of the cables and the backlash of the gears, inevitable uncertainties are brought into the kinematics calcu-lation [1]. Since the reported end effector position of surgical robots like RAVEN-II [2] is directly calculated using the motor encoder measurements and forward kinematics, it may contain relatively large error up to 10mm, whereas semi-autonomous functions being introduced into abdominal surgeries require position inaccuracy of at most 1mm. To resolve the problem, a cost-effective, real-time and data-driven pipeline for robot end effector position precision estimation is proposed and tested on RAVEN-II. Analysis shows an improved end effector position error of around 1mm RMS traversing through the entire robot workspace without high-resolution motion tracker. The open source code, data sets, videos, and user guide can be found at //github.com/HaonanPeng/RAVEN Neural Network Estimator.


Title: Temporal Segmentation of Surgical Sub-tasks through Deep Learning with Multiple Data Sources
Key Words: biomedical ultrasonics  finite state machines  learning (artificial intelligence)  medical computing  medical image processing  medical robotics  surgery  Skill Assessment Working Set  robotic intra-operative ultrasound imaging  da Vinci® Xi surgical system  superior frame-wise state estimation accuracy  temporal segmentation  deep learning  data sources  robot-assisted surgeries  finite-state machines  surgical task  temporal perception  current surgical scene  real-time estimation  task progresses  state estimation models  surgical state estimation models  State estimation  Data models  Task analysis  Feature extraction  Hidden Markov models  Robots  Kinematics 
Abstract: Many tasks in robot-assisted surgeries (RAS) can be represented by finite-state machines (FSMs), where each state represents either an action (such as picking up a needle) or an observation (such as bleeding). A crucial step towards the automation of such surgical tasks is the temporal perception of the current surgical scene, which requires a real-time estimation of the states in the FSMs. The objective of this work is to estimate the current state of the surgical task based on the actions performed or events occurred as the task progresses. We propose Fusion-KVE, a unified surgical state estimation model that incorporates multiple data sources including the Kinematics, Vision, and system Events. Additionally, we examine the strengths and weaknesses of different state estimation models in segmenting states with different representative features or levels of granularity. We evaluate our model on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), as well as a more complex dataset involving robotic intra-operative ultrasound (RIOUS) imaging, created using the da Vinci® Xi surgical system. Our model achieves a superior frame-wise state estimation accuracy up to 89.4%, which improves the state-of-the-art surgical state estimation models in both JIGSAWS suturing dataset and our RIOUS dataset.


Title: Controlling Assistive Robots with Learned Latent Actions
Key Words: control system synthesis  handicapped aids  learning systems  manipulators  telerobotics  learned latent actions  assistive robotic arms  high-dimensional robot behavior  user-friendly latent actions  low-dimensional embeddings  robotic arm  assistive eating  cooking tasks  assistive robot control  physical disabilities  teleoperation  handheld joystick  shared autonomy baselines  Robot kinematics  Task analysis  Manipulators  Aerospace electronics  Robot sensing systems  Glass  Index Terms—Physically assistive devices  cognitive humanrobot interaction  human-centered robotics 
Abstract: Assistive robotic arms enable users with physical disabilities to perform everyday tasks without relying on a caregiver. Unfortunately, the very dexterity that makes these arms useful also makes them challenging to teleoperate: the robot has more degrees-of-freedom than the human can directly coordinate with a handheld joystick. Our insight is that we can make assistive robots easier for humans to control by leveraging latent actions. Latent actions provide a low-dimensional embedding of high-dimensional robot behavior: for example, one latent dimension might guide the assistive arm along a pouring motion. In this paper, we design a teleoperation algorithm for assistive robots that learns latent actions from task demonstrations. We formulate the controllability, consistency, and scaling properties that user-friendly latent actions should have, and evaluate how different low-dimensional embeddings capture these properties. Finally, we conduct two user studies on a robotic arm to compare our latent action approach to both state-of-the-art shared autonomy baselines and a teleoperation strategy currently used by assistive arms. Participants completed assistive eating and cooking tasks more efficiently when leveraging our latent actions, and also subjectively reported that latent actions made the task easier to perform. The video accompanying this paper can be found at: https://youtu.be/wjnhrzugBj4.


Title: On the efficient control of series-parallel compliant articulated robots
Key Words: actuators  legged locomotion  optimisation  position control  robot dynamics  torque  torque control  series-parallel compliant articulated leg prototype  highly-efficient parallel actuation branches  torque allocation  transmission ratio  actuator hardware specifications  periodic squat motions  motion efficiency  parallel actuators  quadratic criteria  optimization based controller  redundant robots  torque distribution  series-parallel compliant articulated robots  Torque  Actuators  Joints  Tendons  Legged locomotion  Topology 
Abstract: Torque distribution in redundant robots that combine the potential of asymmetric series-parallel actuated branches and multi-articulation pose a non-trivial challenge. To address the problem, this work proposes a novel optimization based controller that can accommodate various quadratic criteria to perform the torque distribution among dissimilar series and parallel actuators in order to maximize the motion efficiency. Three candidate criteria are composed and their performances are compared during periodic squat motions with a 3 degree of freedom series-parallel compliant articulated leg prototype. It is first shown that by minimizing a criterion that takes into account the actuator hardware specifications such as torque constant and transmission ratio, the gravity-driven phases can be lengthened. Thereby, this particular criterion results in slightly better performance than when adopting a strategy that maximizes the torque allocation to the higher efficiency actuators. Furthermore, valuable insights such as that the efficacy of maximum utilization of the highly-efficient parallel actuation branches decreases progressively at high frequencies were observed.


Title: Preintegrated Velocity Bias Estimation to Overcome Contact Nonlinearities in Legged Robot Odometry
Key Words: control nonlinearities  distance measurement  graph theory  image fusion  inertial navigation  legged locomotion  motion control  robot dynamics  robot vision  state estimation  preintegrated velocity bias estimation  contact nonlinearities  legged robot odometry  factor graph formulation  quadruped robot  slippery terrain  deformable terrain  preintegrated velocity factor  leg flexibility  leg odometry drift  IMU factors  ANYmal robot  proprioceptive state estimator  Legged locomotion  Robot sensing systems  Velocity measurement  Estimation  Kinematics  Foot 
Abstract: In this paper, we present a novel factor graph formulation to estimate the pose and velocity of a quadruped robot on slippery and deformable terrain. The factor graph introduces a preintegrated velocity factor that incorporates velocity inputs from leg odometry and also estimates related biases. From our experimentation we have seen that it is difficult to model uncertainties at the contact point such as slip or deforming terrain, as well as leg flexibility. To accommodate for these effects and to minimize leg odometry drift, we extend the robot's state vector with a bias term for this preintegrated velocity factor. The bias term can be accurately estimated thanks to the tight fusion of the preintegrated velocity factor with stereo vision and IMU factors, without which it would be unobservable. The system has been validated on several scenarios that involve dynamic motions of the ANYmal robot on loose rocks, slopes and muddy ground. We demonstrate a 26% improvement of relative pose error compared to our previous work and 52% compared to a state-of-the-art proprioceptive state estimator.


Title: Optimized Foothold Planning and Posture Searching for Energy-Efficient Quadruped Locomotion over Challenging Terrains
Key Words: energy conservation  legged locomotion  motion control  optimisation  search problems  torque control  posture searching  energy-efficient quadruped locomotion  energy-efficient locomotion  legged robot  quadrupedal robot  nominal stance parameters  leg torque distribution  foothold planner  standing legs  energy-saving stance posture  stairs climbing  stairs climbing  center of gravity trajectory planner  COG  foothold planning optimization  Legged locomotion  Torque  Thigh  Hip  Knee  Energy efficiency 
Abstract: Energy-efficient locomotion is of primary importance for legged robot to extend operation time in practical applications. This paper presents an approach to achieve energy-efficient locomotion for a quadrupedal robot walking over challenging terrains. Firstly, we optimize the nominal stance parameters based on the analysis of leg torque distribution. Secondly, we proposed the foothold planner and the center of gravity (COG) trajectory planner working together to guide the robot to place its standing legs in an energy-saving stance posture. We have validated the effectiveness of our method on a real quadrupedal robot in experiments including autonomously walking on plain ground and climbing stairs.


Title: Extracting Legged Locomotion Heuristics with Regularized Predictive Control
Key Words: adaptive control  control system synthesis  learning (artificial intelligence)  legged locomotion  motion control  optimisation  predictive control  legged locomotion heuristics  regularized predictive control  legged robots  dynamic maneuvers  difficult terrains  meaningful cost functions  high-fidelity models  timing restrictions  principled regularization heuristics  legged locomotion optimization control  cost space offline  desired commands  optimal control actions  robot states  heuristic candidates  adaptation laws  models online  powerful heuristics  approximate complex dynamics  model simplifications  parameter uncertainty  parameter tuning process  increased capabilities  newly extracted heuristics  controller structure  mini cheetah robot  Cost function  Legged locomotion  Data models  Tuning  Predictive control 
Abstract: Optimization based predictive control is a powerful tool that has improved the ability of legged robots to execute dynamic maneuvers and traverse increasingly difficult terrains. However, it is often challenging and unintuitive to design meaningful cost functions and build high-fidelity models while adhering to timing restrictions. A novel framework to extract and design principled regularization heuristics for legged locomotion optimization control is presented. By allowing a simulation to fully explore the cost space offline, certain states and actions can be constrained or isolated. Data is fit with simple models relating the desired commands, optimal control actions, and robot states to identify new heuristic candidates. Basic parameter learning and adaptation laws are then applied to the models online. This method extracts simple, but powerful heuristics that can approximate complex dynamics and account for errors stemming from model simplifications and parameter uncertainty without the loss of physical intuition while generalizing the parameter tuning process. Results on the Mini Cheetah robot verify the increased capabilities due to the newly extracted heuristics without any modification to the controller structure or gains.


Title: Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning
Key Words: learning (artificial intelligence)  legged locomotion  path planning  predictive control  robot dynamics  robot kinematics  generalizable locomotion skills  hierarchical reinforcement learning  arbitrary goals  hierarchical framework  sample-efficiency  generalizability  learned locomotion skills  real-world robots  goal-oriented locomotion  diverse primitives skills  freedom robot  coarse dynamics models  primitive cycles  model predictive control framework  Daisy hexapod hardware  size 12.0 m  Hardware  Legged locomotion  Training  Task analysis  Planning  Heuristic algorithms 
Abstract: Learning to locomote to arbitrary goals on hardware remains a challenging problem for reinforcement learning. In this paper, we present a hierarchical framework that improves sample-efficiency and generalizability of learned locomotion skills on real-world robots. Our approach divides the problem of goal-oriented locomotion into two sub-problems: learning diverse primitives skills, and using model-based planning to sequence these skills. We parametrize our primitives as cyclic movements, improving sample-efficiency of learning from scratch on a 18 degrees of freedom robot. Then, we learn coarse dynamics models over primitive cycles and use them in a model predictive control framework. This allows us to learn to walk to arbitrary goals up to 12m away, after about two hours of training from scratch on hardware. Our results on a Daisy hexapod hardware and simulation demonstrate the efficacy of our approach at reaching distant targets, in different environments, and with sensory noise.


Title: SoRX: A Soft Pneumatic Hexapedal Robot to Traverse Rough, Steep, and Unstable Terrain
Key Words: actuators  legged locomotion  motion control  pneumatic actuators  SoRX  soft pneumatic hexapedal robot  2-degree-of-freedom soft pneumatic actuator  tripod gait  pneumatically-actuated legged robots  rough terrain  steep terrain  open-loop control  cyclic foot trajectories  legged locomotion  physical testing  Legged locomotion  Pneumatic systems  Soft robotics  Pneumatic actuators  Trajectory 
Abstract: Soft robotics technology creates new ways for legged robots to interact with and adapt to their environment. In this paper we develop i) a new 2-degree-of-freedom soft pneumatic actuator, and ii) a novel soft robotic hexapedal robot called SoRX that leverages the new actuators. Simulation and physical testing confirm that the proposed actuator can generate cyclic foot trajectories that are appropriate for legged locomotion. Consistent with other hexapedal robots (and animals), SoRX employs an alternating tripod gait to propel itself forward. Experiments reveal that SoRX can reach forward speeds of up to 0.44 body lengths per second, or equivalently 101 mm/s. With a size of 230 mm length, 140 mm width and 100 mm height, and weight of 650 grams, SoRX is among the fastest tethered soft pneumatically-actuated legged robots to date. The motion capabilities of SoRX are evaluated through five experiments: running, step climbing, and traversing rough terrain, steep terrain, and unstable terrain. Experimental results show that SoRX is able to operate over challenging terrains in open-loop control and by following the same alternating tripod gait across all experimental cases.


Title: Optimal Sequential Task Assignment and Path Finding for Multi-Agent Robotic Assembly Planning
Key Words: assembly planning  collision avoidance  mobile robots  multi-agent systems  multi-robot systems  robotic assembly  nonholonomic differential-drive robots  optimal sequential task assignment  collision-free trajectories  robotic manufacturing  collision-free routing  multiagent robotic assembly planning  path finding  Robots  Task analysis  Schedules  Manufacturing  Collision avoidance  Routing  Production facilities 
Abstract: We study the problem of sequential task assignment and collision-free routing for large teams of robots in applications with inter-task precedence constraints (e.g., task A and task B must both be completed before task C may begin). Such problems commonly occur in assembly planning for robotic manufacturing applications, in which sub-assemblies must be completed before they can be combined to form the final product. We propose a hierarchical algorithm for computing makespan-optimal solutions to the problem. The algorithm is evaluated on a set of randomly generated problem instances where robots must transport objects between stations in a "factory" grid world environment. In addition, we demonstrate in high-fidelity simulation that the output of our algorithm can be used to generate collision-free trajectories for non-holonomic differential-drive robots.


Title: Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning
Key Words: control engineering computing  learning (artificial intelligence)  mobile robots  multi-robot systems  navigation  path planning  optimal paths  multiple robots  dynamics randomization  differential drive robots  dynamic environment  obstacle complexities  multirobot navigation problem  deep reinforcement learning framework  optimal target locations  DRL based framework  navigation policy  Collision avoidance  Navigation  Robot sensing systems  Robot kinematics  Training  Adaptation models 
Abstract: The challenges of multi-robot navigation in dynamic environments lie in uncertainties in obstacle complexities, partially observation of robots, and policy implementation from simulations to the real world. This paper presents a cooperative approach to address the multi-robot navigation problem (MRNP) under dynamic environments using a deep reinforcement learning (DRL) framework, which can help multiple robots jointly achieve optimal paths despite a certain degree of obstacle complexities. The novelty of this work includes threefold: (1) developing a cooperative architecture that robots can exchange information with each other to select the optimal target locations; (2) developing a DRL based framework which can learn a navigation policy to generate the optimal paths for multiple robots; (3) developing a training mechanism based on dynamics randomization which can make the policy generalized and achieve the maximum performance in the real world. The method is tested with Gazebo simulations and 4 differential drive robots. Both simulation and experiment results validate the superior performance of the proposed method in terms of success rate and travel time when compared with the other state-of-art technologies.


Title: Adaptive Directional Path Planner for Real-Time, Energy-Efficient, Robust Navigation of Mobile Robots
Key Words: energy conservation  graph theory  mobile robots  path planning  robust control  sample based methods  sub-optimal memory-intensive  Adaptive Directional Planner algorithm  robust local path planning  autonomous navigation  form factor mobile robots  low memory footprint  robust navigation  unknown environments  complex environments  fundamental capability  robotic applications  optimal robot path planning  complex memory intensive task  adaptive directional path planner  ADP algorithm implementation  memory size 28.0 KByte  Mobile robots  Trajectory  Real-time systems  Navigation  Kinematics 
Abstract: Autonomous navigation through unknown and complex environments is a fundamental capability that is essential in almost all robotic applications. Optimal robot path planning is critical to enable efficient navigation. Path planning is a complex, compute and memory intensive task. Traditional methods employ either graph based search methods or sample based methods to implement path planning, which are sub-optimal and compute/memory-intensive. To this end, an Adaptive Directional Planner (ADP) algorithm is devised to achieve real-time, energy-efficient, memory-optimized, robust local path planning for enabling efficient autonomous navigation of mobile robots. The ADP algorithm ensures that the paths are optimal and kinematically-feasible. Further, the proposed algorithm is tested with different challenging scenarios verifying the functionality and robustness. The ADP algorithm implementation results demonstrate 40- 60X less number of nodes and 40 - 50X less execution time compared to the standard TP-RRT schemes, without compromising on accuracy. Finally, the algorithm has also been implemented as an accelerator for non-holonomic, multi-shape, small form factor mobile robots to provide a silicon solution with high performance and low memory footprint (28KB).


Title: Exploiting sparsity in robot trajectory optimization with direct collocation and geometric algorithms
Key Words: geometry  Lie algebras  Lie groups  optimal control  optimisation  robots  trajectory control  geometric algorithm  robot trajectory optimization  Lie group method  floating-point operations  first-order information  sparsity exploitation  numerical differentiation  analytical differentiation  Lie algebras  state equations  recursive algorithms  articulated robots  direct collocation algorithm  Robots  Heuristic algorithms  Jacobian matrices  Optimal control  System dynamics  Optimization 
Abstract: This paper presents a robot trajectory optimization formulation that builds upon numerical optimal control and Lie group methods. In particular, the inherent sparsity of direct collocation is carefully analyzed to dramatically reduce the number of floating-point operations to get first-order information of the problem. We describe how sparsity exploitation is employed with both numerical and analytical differentiation. Furthermore, the use of geometric algorithms based on Lie groups and their associated Lie algebras allow to analytically evaluate the state equations and their derivatives with efficient recursive algorithms. We demonstrate the scalability of the proposed formulation with three different articulated robots, such as a finger, a mobile manipulator and a humanoid composed of five, eight and more than twenty degrees of freedom, respectively. The performance of our implementation in C++ is also validated and compared against a state-of-the-art general purpose numerical optimal control solver.


Title: Bi-Convex Approximation of Non-Holonomic Trajectory Optimization
Key Words: approximation theory  convex programming  minimisation  quadratic programming  robot kinematics  nonholonomic trajectory optimization  nonholonomic kinematics  nonlinearly maps control input  nonconvex  bi-convex cost  constraint functions  bi-convex part  nonholonomic behavior  nonlinear penalty  nonlinear costs  bi-convex structure  bi-convex approximation  autonomous cars  fixed-wing aerial vehicles  computational tractability  alternating minimization  sequential quadratic programming  interior-point methods  Trajectory optimization  Minimization  Computational modeling  Collision avoidance  Robots  Mathematical model 
Abstract: Autonomous cars and fixed-wing aerial vehicles have the so-called non-holonomic kinematics which non-linearly maps control input to states. As a result, trajectory optimization with such a motion model becomes highly non-linear and non-convex. In this paper, we improve the computational tractability of non-holonomic trajectory optimization by reformulating it in terms of a set of bi-convex cost and constraint functions along with a non-linear penalty. The bi-convex part acts as a relaxation for the non-holonomic trajectory optimization while the residual of the penalty dictates how well its output obeys the non-holonomic behavior. We adopt an alternating minimization approach for solving the reformulated problem and show that it naturally leads to the replacement of the challenging non-linear penalty with a globally valid convex surrogate. Along with the common cost functions modeling goal-reaching, trajectory smoothness, etc., the proposed optimizer can also accommodate a class of non-linear costs for modeling goal-sets, while retaining the bi-convex structure. We benchmark the proposed optimizer against off-the-shelf solvers implementing sequential quadratic programming and interior-point methods and show that it produces solutions with similar or better cost as the former while significantly outperforming the latter. Furthermore, as compared to both off-the-shelf solvers, the proposed optimizer achieves more than 20x reduction in computation time.


Title: Fast, Versatile, and Open-loop Stable Running Behaviors with Proprioceptive-only Sensing using Model-based Optimization
Key Words: legged locomotion  motion control  open loop systems  optimisation  robot dynamics  stability  state feedback  model-based trajectory optimization  direct-drive robot  direct-collocation-formulated optimization  single-legged planar robot  open-loop stable motion primitives  proprioceptive-only sensing  versatile dynamic behaviors  expensive inertial sensors  agile control  stable control  model-based optimization  open-loop stable running behaviors  Legged locomotion  Force  Optimization  Springs  Modulation  Hip 
Abstract: As we build our legged robots smaller and cheaper, stable and agile control without expensive inertial sensors becomes increasingly important. We seek to enable versatile dynamic behaviors on robots with limited modes of state feedback, specifically proprioceptive-only sensing. This work uses model-based trajectory optimization methods to design open-loop stable motion primitives. We specifically design running gaits for a single-legged planar robot, and can generate motion primitives in under 3 seconds, approaching online-capable speeds. A direct-collocation-formulated optimization generated axial force profiles for the direct-drive robot to achieve desired running speed and apex height. When implemented in hardware, these trajectories produced open-loop stable running. Further, the measured running achieved the desired speed within 10% of the speed specified for the optimization in spite of having no control loop actively measuring or controlling running speed. Additionally, we examine the shape of the optimized force profile and observe features that may be applicable to open-loop stable running in general.


Title: Wasserstein Distributionally Robust Motion Planning and Control with Safety Constraints Using Conditional Value-at-Risk
Key Words: collision avoidance  decision making  mobile robots  optimisation  path planning  predictive control  probability  robust control  Wasserstein distributionally robust motion planning  safety constraints  conditional value-at-risk  optimization-based decision-making tool  safe motion planning  pre-specified threshold  probability distribution  obstacles  Wasserstein ball  available empirical distribution  out-of-sample performance guarantee  risk constraint  computationally tractable method  distributionally robust model predictive control problem  distributionally robust method  Robustness  Safety  Planning  Robots  Trajectory  Probability distribution  Collision avoidance 
Abstract: In this paper, we propose an optimization-based decision-making tool for safe motion planning and control in an environment with randomly moving obstacles. The unique feature of the proposed method is that it limits the risk of unsafety by a pre-specified threshold even when the true probability distribution of the obstacles' movements deviates, within a Wasserstein ball, from an available empirical distribution. Another advantage is that it provides a probabilistic out-of-sample performance guarantee of the risk constraint. To develop a computationally tractable method for solving the distributionally robust model predictive control problem, we propose a set of reformulation procedures using (i) the Kantorovich duality principle, (ii) the extremal representation of conditional value-at-risk, and (iii) a geometric expression of the distance to the union of halfspaces. The performance and utility of this distributionally robust method are demonstrated through simulations using a 12D quadrotor model in a 3D environment.


Title: Grasp Control for Enhancing Dexterity of Parallel Grippers
Key Words: dexterous manipulators  friction  grippers  manipulator dynamics  materials handling  motion control  path planning  tactile sensors  grasp control  enhancing dexterity  parallel grippers  robust grasp controller  slipping avoidance  model-based algorithm  modified LuGre friction model  rotational frictional sliding motions  limit surface concept  computationally efficient method  minimum grasping force  tangential loads  torsional loads  control modalities  robot motion  automatically generates robot motions  gripper commands  Force  Grippers  Friction  Dynamics  Mathematical model  Computational modeling  Grasping 
Abstract: A robust grasp controller for both slipping avoidance and controlled sliding is proposed based on force/tactile feedback only. The model-based algorithm exploits a modified LuGre friction model to consider rotational frictional sliding motions. The modification relies on the Limit Surface concept where a novel computationally efficient method is introduced to compute in real-time the minimum grasping force to balance tangential and torsional loads. The two control modalities are considered by the robot motion planning algorithm that automatically generates robot motions and gripper commands to solve complex manipulation tasks in a material handling application.


Title: Theoretical Derivation and Realization of Adaptive Grasping Based on Rotational Incipient Slip Detection
Key Words: adaptive control  force control  friction  grippers  manipulators  robust control  object manipulation  grasp force control algorithm  adaptive grasping  rotational incipient slip detection  robotics  incipient slip robust detection  center of gravity  friction coefficient  Conferences  Automation 
Abstract: Manipulating objects whose physical properties are unknown remains one of the greatest challenges in robotics. Controlling grasp force is an essential aspect of handling unknown objects without slipping or crushing them. Although extensive research has been carried out on grasp force control, unknown object manipulation is still difficult because conventional approaches assume that object properties (mass, center of gravity, friction coefficient, etc.) are known for grasp force control. One of the approaches to address this issue is incipient slip detection. However, there has been few detailed investigations of robust detection and control of incipient slip on rotational case. This study makes contributions on deriving the theoretical model of incipient slip and proposes a new algorithm to detect incipient slip. Additionally, a novel sensor configuration and a grasp force control algorithm based on the derived theoretical model are proposed. Finally, the proposed algorithm is evaluated by grasping objects with different weights and moments including a fragile pastry (éclair).


Title: Grasp State Assessment of Deformable Objects Using Visual-Tactile Fusion Perception
Key Words: convolutional neural nets  deformation  dexterous manipulators  force control  grippers  image classification  image fusion  learning (artificial intelligence)  robot vision  tactile sensors  dexterity grasping  automatic force control  classification  tactile sensor  wrist camera  robotic arm  deformable objects  3D convolution based visual tactile fusion deep neural network  adaptive grasping  extensive grasping  C3D-VTFN  sliding deformation  grasp state assessment  Visualization  Grasping  Feature extraction  Task analysis  Tactile sensors 
Abstract: Humans can quickly determine the force required to grasp a deformable object to prevent its sliding or excessive deformation through vision and touch, which is still a challenging task for robots. To address this issue, we propose a novel 3D convolution-based visual-tactile fusion deep neural network (C3D-VTFN) to evaluate the grasp state of various deformable objects in this paper. Specifically, we divide the grasp states of deformable objects into three categories of sliding, appropriate and excessive. Also, a dataset for training and testing the proposed network is built by extensive grasping and lifting experiments with different widths and forces on 16 various deformable objects with a robotic arm equipped with a wrist camera and a tactile sensor. As a result, a classification accuracy as high as 99.97% is achieved. Furthermore, some delicate grasp experiments based on the proposed network are implemented in this paper. The experimental results demonstrate that the C3D-VTFN is accurate and efficient enough for grasp state assessment, which can be widely applied to automatic force control, adaptive grasping, and other visual-tactile spatiotemporal sequence learning problems.


Title: Beyond Top-Grasps Through Scene Completion
Key Words: cameras  end effectors  grippers  image colour analysis  image sensors  path planning  position control  robot vision  camera images  grasp success rate  simulated images  top-grasps  scene completion  six-degree-of-freedom grasps  simulated viewpoints  generation method  fully convolutional grasp quality CNN  end-to-end grasp  Shape  Cameras  Grasping  Planning  Robot vision systems  Pipelines 
Abstract: Current end-to-end grasp planning methods propose grasps in the order of seconds that attain high grasp success rates on a diverse set of objects, but often by constraining the workspace to top-grasps. In this work, we present a method that allows end-to-end top-grasp planning methods to generate full six-degree-of-freedom grasps using a single RGBD view as input. This is achieved by estimating the complete shape of the object to be grasped, then simulating different viewpoints of the object, passing the simulated viewpoints to an end-to-end grasp generation method, and finally executing the overall best grasp. The method was experimentally validated on a Franka Emika Panda by comparing 429 grasps generated by the state-of-the-art Fully Convolutional Grasp Quality CNN, both on simulated and real camera images. The results show statistically significant improvements in terms of grasp success rate when using simulated images over real camera images, especially when the real camera viewpoint is angled. Code and video are available at https://irobotics.aalto.fi/beyond-topgrasps-through-scene-completion/.


Title: Dex-Net AR: Distributed Deep Grasp Planning Using a Commodity Cellphone and Augmented Reality App
Key Words: Apple computers  augmented reality  computational geometry  control engineering computing  dexterous manipulators  distributed processing  image colour analysis  image sequences  mobile computing  path planning  robot vision  smart phones  distributed deep grasp planning  commodity cellphone  augmented reality app  consumer demand  mobile phone applications  AR apps  RGB image sequence  point clouds  distributed pipeline  Dex-Net AR  Dex-Net grasp planner  error estimation  robot gripper  iPhone  ARKit  Three-dimensional displays  Cameras  Planning  Sensors  Smart phones  Feature extraction  Robots 
Abstract: Consumer demand for augmented reality (AR) in mobile phone applications, such as the Apple ARKit. Such applications have potential to expand access to robot grasp planning systems such as Dex-Net. AR apps use structure from motion methods to compute a point cloud from a sequence of RGB images taken by the camera as it is moved around an object. However, the resulting point clouds are often noisy due to estimation errors. We present a distributed pipeline, Dex-Net AR, that allows point clouds to be uploaded to a server in our lab, cleaned, and evaluated by Dex-Net grasp planner to generate a grasp axis that is returned and displayed as an overlay on the object. We implement Dex-Net AR using the iPhone and ARKit and compare results with those generated with high-performance depth sensors. The success rates with AR on harder adversarial objects are higher than traditional depth images. The server URL is https://sites.google.com/berkeley.edu/dex-net-ar/home.


Title: FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation using Monocular Fisheye Camera for Autonomous Driving
Key Words: cameras  distance measurement  feature extraction  image classification  image motion analysis  image sampling  learning (artificial intelligence)  mobile robots  object detection  road vehicles  robot vision  traffic engineering computing  video signal processing  autonomous driving  nonlinear distortions  complex algorithms  Euclidean distance estimation  fisheye cameras  automotive scenes  accurate depth supervision  dense depth supervision  self-supervised learning approaches  self-supervised scale-aware framework  raw monocular fisheye videos  applying rectification  piece-wise linear approximation  fisheye projection surface  re-sampling distortion  monocular methods  unseen fisheye video  self-supervised scale-aware distance estimation  monocular fisheye camera  Cameras  Estimation  Training  Euclidean distance  Image reconstruction  Three-dimensional displays  Robot vision systems 
Abstract: Fisheye cameras are commonly used in applications like autonomous driving and surveillance to provide a large field of view (> 180o). However, they come at the cost of strong non-linear distortions which require more complex algorithms. In this paper, we explore Euclidean distance estimation on fisheye cameras for automotive scenes. Obtaining accurate and dense depth supervision is difficult in practice, but self-supervised learning approaches show promising results and could potentially overcome the problem. We present a novel self-supervised scale-aware framework for learning Euclidean distance and ego-motion from raw monocular fisheye videos without applying rectification. While it is possible to perform piece-wise linear approximation of fisheye projection surface and apply standard rectilinear models, it has its own set of issues like re-sampling distortion and discontinuities in transition regions. To encourage further research in this area, we will release our dataset as part of the WoodScape project [1]. We further evaluated the proposed algorithm on the KITTI dataset and obtained state-of-the-art results comparable to other self-supervised monocular methods. Qualitative results on an unseen fisheye video demonstrate impressive performance1.


Title: Omnidirectional Depth Extension Networks
Key Words: calibration  cameras  computer vision  convolutional neural nets  feature extraction  image reconstruction  image sensors  learning (artificial intelligence)  mobile robots  robot vision  stereo image processing  omnidirectional depth extension networks  omnidirectional 360° camera  autonomous robots  perception ability  FoV  depth sensors  perception system  low-cost 3D sensing system  omnidirectional camera  calibrated projective depth camera  recorded omnidirectional image  omnidirectional depth extension convolutional neural network  spherical feature  feature encoding layers  deformable convolutional spatial propagation network  feature decoding layers  omnidirectional coordination  projective coordination  feature learning  estimated depths  proposed ODE-CNN  popular 360D dataset  depth error  Convolution  Estimation  Sensors  Cameras  Transforms  Kernel  Task analysis 
Abstract: Omnidirectional 360° camera proliferates rapidly for autonomous robots since it significantly enhances the perception ability by widening the field of view (FoV). However, corresponding 360° depth sensors, which are also critical for the perception system, are still difficult or expensive to have. In this paper, we propose a low-cost 3D sensing system that combines an omnidirectional camera with a calibrated projective depth camera, where the depth from the limited FoV can be automatically extended to the rest of recorded omnidirectional image. To accurately recover the missing depths, we design an omnidirectional depth extension convolutional neural network (ODE-CNN), in which a spherical feature transform layer (SFTL) is embedded at the end of feature encoding layers, and a deformable convolutional spatial propagation network (D-CSPN) is appended at the end of feature decoding layers. The former re-samples the neighborhood of each pixel in the omnidirectional coordination to the projective coordination, which reduce the difficulty of feature learning, and the later automatically finds a proper context to well align the structures in the estimated depths via CNN w.r.t. the reference image, which significantly improves the visual quality. Finally, we demonstrate the effectiveness of proposed ODE-CNN over the popular 360D dataset, and show that ODE-CNN significantly outperforms (relatively 33% reduction in depth error) other state-of-the-art (SoTA) methods.


Title: 3D Orientation Estimation and Vanishing Point Extraction from Single Panoramas Using Convolutional Neural Network
Key Words: cameras  computer vision  convolutional neural nets  feature extraction  image classification  regression analysis  stereo image processing  convolutional neural network  3D orientation estimation  computer vision  3D scene understanding  single spherical panorama  labeled 3D orientation  vanishing point extraction  single panoramas  VOP60K  Google Street View  pinhole cameras  panorama geometric information  two column vector regression loss  rotation matrix  CNN architecture  classification loss  edge extractor layer  Cameras  Three-dimensional displays  Feature extraction  Google  Estimation  Data mining  Robot vision systems 
Abstract: 3D orientation estimation is a key component of many important computer vision tasks such as autonomous navigation and 3D scene understanding. This paper presents a new CNN architecture to estimate the 3D orientation of an omnidirectional camera with respect to the world coordinate system from a single spherical panorama. To train the proposed architecture, we leverage a dataset of panoramas named VOP60K from Google Street View with labeled 3D orientation, including 50 thousand panoramas for training and 10 thousand panoramas for testing. Previous approaches usually estimate 3D orientation under pinhole cameras. However, for a panorama, due to its larger field of view, previous approaches cannot be suitable. In this paper, we propose an edge extractor layer to utilize the low-level and geometric information of panorama, an attention module to fuse different features generated by previous layers. A regression loss for two column vectors of the rotation matrix and classification loss for the position of vanishing points are added to optimize our network simultaneously. The proposed algorithm is validated on our benchmark, and experimental results clearly demonstrate that it outperforms previous methods.


Title: Curvature sensing with a spherical tactile sensor using the color-interference of a marker array
Key Words: cameras  colour  elasticity  force sensors  image colour analysis  indentation  tactile sensors  subtractive color-mixing principle  planar manufacturing process  functional features  ChromaTouch  millimeter-size indentation  elastic membrane  3-dimensional displacement field  distributed tactile sensor  fine distributed sense  robots  fine manipulation  local shape  soft fingers  cues  marker array  color-interference  spherical tactile sensor  curvature sensing  normal sensing  contact mechanics  Hertz contact theory  curved surface  relative motion  colored patches  local 3d displacement  measurement points  spherical shape  flat functional panels  flat surface  size 40 mm  Shape  Cameras  Tactile sensors  Three-dimensional displays 
Abstract: The only way to perceive a small object held between our fingers is to trust our sense of touch. Touch provides cues about the state of the contact even if its view is occluded by the finger. The interaction between the soft fingers and the surface reveals crucial information, such as the local shape of the object, that plays a central role in fine manipulation. In this work, we present a new spherical sensor that endows robots with a fine distributed sense of touch. This sensor is an evolution of our distributed tactile sensor that measures the dense 3-dimensional displacement field of an elastic membrane, using the subtractive color-mixing principle. We leverage a planar manufacturing process that enables the design and manufacturing of the functional features on a flat surface. The flat functional panels are then folded to create a spherical shape able to sense a wide variety of objects.The resulting 40mm-diameter spherical sensor has 77 measurement points, each of which gives an estimation of the local 3d displacement, normal and tangential to the surface. Each marker is built around 2 sets of colored patches placed at different depths. The relative motion and resulting hue of each marker, easily captured by an embedded RGB camera, provides a measurement of their 3d motion. To benchmark the sensor, we compared the measurements obtained while pressing the sensor on a curved surface with Hertz contact theory, a hallmark of contact mechanics. While the mechanics did strictly follow Hertz contact theory, using the shear and normal sensing, ChromaTouch can estimate the curvature of an object after a millimeter-size indentation of the sensor.


Title: Center-of-Mass-based Robust Grasp Planning for Unknown Objects Using Tactile-Visual Sensors
Key Words: dexterous manipulators  grippers  path planning  robot vision  tactile sensors  Franka Emika robot arm  tactile sensors  multisensor modules  regrasp planner  slip detection  visual sensors  center-of-mass-based robust grasp planning  Grasping  Tactile sensors  Force  Robustness 
Abstract: An unstable grasp pose can lead to slip, thus an unstable grasp pose can be predicted by slip detection. A regrasp is required afterwards to correct the grasp pose in order to finish the task. In this work, we propose a novel regrasp planner with multi-sensor modules to plan grasp adjustments with the feedback from a slip detector. Then a regrasp planner is trained to estimate the location of center of mass, which helps robots find an optimal grasp pose. The dataset in this work consists of 1 025 slip experiments and 1 347 regrasps collected by one pair of tactile sensors, an RGB-D camera and one Franka Emika robot arm equipped with joint force/torque sensors. We show that our algorithm can successfully detect and classify the slip for 5 unknown test objects with an accuracy of 76.88% and a regrasp planner increases the grasp success rate by 31.0% compared to the state-of-the-art vision-based grasping algorithm.


Title: OmniTact: A Multi-Directional High-Resolution Touch Sensor
Key Words: cameras  computer vision  convolutional neural nets  dexterous manipulators  gels  image resolution  microsensors  neurocontrollers  sensor fusion  state estimation  tactile sensors  dexterous robotic manipulation  deep convolutional neural networks  electrical connector  multidirectional high-resolution touch sensor  low-resolution signals  multidirectional high-resolution tactile sensor  robotic hands  multiple microcameras  multidirectional deformations  gel-based skin  contact state variables  image processing  computer vision methods  state estimation problem  robotic control task  OmniTact combination  Cameras  Tactile sensors  Sensitivity  Task analysis 
Abstract: Incorporating touch as a sensing modality for robots can enable finer and more robust manipulation skills. Existing tactile sensors are either flat, have small sensitive fields or only provide low-resolution signals. In this paper, we introduce OmniTact, a multi-directional high-resolution tactile sensor. OmniTact is designed to be used as a fingertip for robotic manipulation with robotic hands, and uses multiple micro-cameras to detect multi-directional deformations of a gel-based skin. This provides a rich signal from which a variety of different contact state variables can be inferred using modern image processing and computer vision methods. We evaluate the capabilities of OmniTact on a challenging robotic control task that requires inserting an electrical connector into an outlet, as well as a state estimation problem that is representative of those typically encountered in dexterous robotic manipulation, where the goal is to infer the angle of contact of a curved finger pressing against an object. Both tasks are performed using only touch sensing and deep convolutional neural networks to process images from the sensor's cameras. We compare with a state-of-the-art tactile sensor that is only sensitive on one side, as well as a state-of-the-art multi-directional tactile sensor, and find that OmniTact's combination of high-resolution and multi-directional sensing is crucial for reliably inserting the electrical connector and allows for higher accuracy in the state estimation task. Videos and supplementary material can be found here4.


Title: Highly sensitive bio-inspired sensor for fine surface exploration and characterization
Key Words: assembling  magnetic field measurement  magnetic sensors  magnetoresistive devices  permanent magnets  signal detection  surface texture  surface topography measurement  tunnelling magnetoresistance  bioinspired sensor  fine surface exploration  texture sensing  robotics  texture topography sensor  ciliary structure  biological structure  permanent magnetization  tunneling magnetoresistance sensor  surface texture  electronic signal acquisition board  topography scanner  elastic cilia brush  TMR sensor  surface roughness  size 7.0 mum  size 9.2 mum to 213.0 mum  Robot sensing systems  Tunneling magnetoresistance  Magnetic separation  Substrates  Bridge circuits  Magnetic tunneling 
Abstract: Texture sensing is one of the types of information sensed by humans through touch, and is thus of interest to robotics that this type of information can be acquired and processed. In this work we present a texture topography sensor based on a ciliary structure, a biological structure found in many organisms. The device consists of up to 9 elastic cilia with permanent magnetization assembled on top of a highly sensitive tunneling magnetoresistance (TMR) sensor, within a compact footprint of 6×6 mm2. When these cilia brush against some textured surface, their movement and vibrations give rise to a signal that can be correlated to the characteristics of the texture being measured. We also present an electronic signal acquisition board, used in this work. Various configurations of cilia sizes are tested, with the most precise being capable of differentiating different types of sandpaper from 9.2 μm to 213 μm average surface roughness with a 7 μm resolution. As a topography scanner the sensor was able to scan a 20 μm high step in a flat surface.


Title: Implementing Tactile and Proximity Sensing for Crack Detection
Key Words: crack detection  learning (artificial intelligence)  object detection  pattern classification  tactile sensors  telerobotics  video cameras  proximity sensing  remote characterisation  physical robot-environment interaction  automatic crack detection  proximity sensor  physical environment  fibre optics  cracks  bumps  undulations  machine learning  classifier  average crack detection accuracy  width classification accuracy  Kruskal-Wallis results  force data  proximity data  optical fibres  extreme environments  Robot sensing systems  Force  Surface cracks  Optics  Feature extraction 
Abstract: Remote characterisation of the environment during physical robot-environment interaction is an important task commonly accomplished in telerobotics. This paper demonstrates how tactile and proximity sensing can be efficiently used to perform automatic crack detection. A custom-designed integrated tactile and proximity sensor is implemented. It measures the deformation of its body when interacting with the physical environment and distance to the environment's objects with the help of fibre optics. This sensor was used to slide across different surfaces and the data recorded during the experiments was used to detect and classify cracks, bumps and undulations. The proposed method uses machine learning techniques (mean absolute value as feature and random forest as classifier) to detect cracks and determine their width. An average crack detection accuracy of 86.46% and width classification accuracy of 57.30% is achieved. Kruskal-Wallis results (p<; 0.001) indicate statistically significant differences among results obtained when analysing only force data, only proximity data and both force and proximity data. In contrast to previous techniques, which mainly rely on visual modality, the proposed approach based on optical fibres is suitable for operation in extreme environments, such as nuclear facilities in which nuclear radiation may damage the electronic components of video cameras.


Title: Novel Proximity Sensor for Realizing Tactile Sense in Suction Cups
Key Words: capacitance measurement  capacitive sensors  deformation  mechanical variables measurement  object detection  tactile sensors  partial contact position estimation  push-in stroke detection  deformation detection  tactile sensor module  capacitive proximity sensor module  suction cup  Robot sensing systems  Capacitance  Electrodes  Capacitance measurement  Sensitivity  Service robots 
Abstract: We propose a new capacitive proximity sensor that detects deformations of a suction cup as a tactile sense. We confirmed that one sensor module provides three applications for reliable picking and a simplified setup. The first application is the picking height decision. The second one is the placing height decision for detecting whether the grasped object is placed on the placement surface. These two applications are achieved by detecting the push-in stroke of the suction cup. The final application is detection of whether the suction cup is in partial contact or full contact with the object. This function can correct the picking posture as well as detect whether picking is possible before the pull-up motion. We also demonstrate that the partial contact position can be estimated in real time.


Title: Schmidt-EKF-based Visual-Inertial Moving Object Tracking
Key Words: image motion analysis  image representation  Kalman filters  object tracking  pose estimation  robot vision  target tracking  tracking sensor  target motion model  Schmidt-EKF-based visual-inertial moving object tracking  tightly-coupled estimation  visual-inertial localization  joint estimation system  Schmidt-Kalman Filter  ego-motion accuracy degradation  robot-centric representation  object pose tracking  Target tracking  Robot sensing systems  Estimation  Three-dimensional displays  Dynamics 
Abstract: In this paper we investigate the effect of tightly-coupled estimation on the performance of visual-inertial localization and dynamic object pose tracking. In particular, we show that while a joint estimation system outperforms its decoupled counterpart when given a "proper" model for the target's motion, inconsistent modeling, such as choosing improper levels for the target's propagation noises, can actually lead to a degradation in ego-motion accuracy. To address the realistic scenario where a good prior knowledge of the target's motion model is not available, we design a new system based on the Schmidt-Kalman Filter (SKF), in which target measurements do not update the navigation states, however all correlations are still properly tracked. This allows for both consistent modeling of the target errors and the ability to update target estimates whenever the tracking sensor receives non-target data such as bearing measurements to static, 3D environmental features. We show in extensive simulation that this system, along with a robot-centric representation of the target, leads to robust estimation performance even in the presence of an inconsistent target motion model. Finally, the system is validated in a real-world experiment, and is shown to offer accurate localization and object pose tracking performance.


Title: Learning View and Target Invariant Visual Servoing for Navigation
Key Words: convolutional neural nets  feedback  learning (artificial intelligence)  mobile robots  path planning  robot vision  visual servoing  deep reinforcement learning  mobile robot navigation  deep convolutional network controller  viewpoint invariant visual servoing  target invariant visual servoing  feedback control error  Visual servoing  Navigation  Visualization  Feature extraction  Task analysis  Semantics  Cameras 
Abstract: The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller to reach the desired goal. We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment [1]. The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches.


Title: Tightly-Coupled Single-Anchor Ultra-wideband-Aided Monocular Visual Odometry System
Key Words: distance measurement  feature extraction  graph theory  least squares approximations  optimisation  pose estimation  position measurement  robot vision  Levenberg-Marquardt nonlinear least squares optimization scheme  scale factor  visual features  pose-graph optimization scheme  landmark reprojection errors  visual drift  monocular visual feature observations  distance measurements  ultrawideband-aided monocular visual odometry system  single-anchor monocular visual odometry system  tightly-coupled odometry framework  anchor position estimation  robot operating system  Cameras  Distance measurement  Robot sensing systems  Visualization 
Abstract: In this work, we propose a tightly-coupled odometry framework, which combines monocular visual feature observations with distance measurements provided by a single ultra-wideband (UWB) anchor with an initial guess for its location. Firstly, the scale factor and the anchor position in the vision frame will be simultaneously estimated using a variant of Levenberg-Marquardt non-linear least squares optimization scheme. Once the scale factor is obtained, the map of visual features is updated with the new scale. Subsequent ranging errors in a sliding window are continuously monitored and the estimation procedure will be reinitialized to refine the estimates. Lastly, range measurements and anchor position estimates are fused when needed into a pose-graph optimization scheme to minimize both the landmark reprojection errors and ranging errors, thus reducing the visual drift and improving the system robustness. The proposed method is implemented in Robot Operating System (ROS) and can function in real-time. The performance is validated on both public datasets and real-life experiments and compared with state-of-the-art methods.


Title: Scaling Local Control to Large-Scale Topological Navigation
Key Words: learning (artificial intelligence)  mobile robots  path planning  robot vision  deep learning  robot perception  reliability issue  world images  mechanical constraints  local controller  large-scale visual topological navigation  large-scale environments  local control  large-scale topological navigation  Navigation  Trajectory  Robustness  Robot kinematics  Planning  Visualization 
Abstract: Visual topological navigation has been revitalized recently thanks to the advancement of deep learning that substantially improves robot perception. However, the scalability and reliability issue remain challenging due to the complexity and ambiguity of real world images and mechanical constraints of real robots. We present an intuitive approach to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust. Our approach achieves state-of-the-art results in trajectory following and planning in large-scale environments. It also generalizes well to real robots and new environments without retraining or finetuning.


Title: Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual Navigation
Key Words: graphical user interfaces  humanoid robots  human-robot interaction  learning (artificial intelligence)  mobile robots  path planning  robot vision  cost-effective data collection  third-person demonstrations  camera perspectives  perspective-invariant state features  model-based imitation learning approach  action-labeled human demonstrations  effective policy  zero-shot imitation  legged robot visual navigation  training effective visual navigation policies  expert demonstrations  goal-driven visual navigation policy  high-quality navigation  Feature extraction  Navigation  Legged locomotion  Visualization  Training 
Abstract: Imitation learning is a popular approach for training effective visual navigation policies. However, collecting expert demonstrations for legged robots is challenging as these robots can be hard to control, move slowly, and cannot operate continuously for long periods of time. In this work, we propose a zero-shot imitation learning framework for training a goal-driven visual navigation policy on a legged robot from human demonstrations (third-person perspective), allowing for high-quality navigation and cost-effective data collection. However, imitation learning from third-person demonstrations raises unique challenges. First, these demonstrations are captured from different camera perspectives, which we address via a feature disentanglement network (FDN) that extracts perspective-invariant state features. Second, as transition dynamics vary between systems, we reconstruct missing action labels by either building an inverse model of the robot's dynamics in the feature space and applying it to the human demonstrations or developing a Graphic User Interface (GUI) to label human demonstrations. To train a navigation policy we use a model-based imitation learning approach with FDN and action-labeled human demonstrations. We show that our framework can learn an effective policy for a legged robot, Laikago, from human demonstrations in both simulated and real-world environments. Our approach is zero-shot as the robot never navigates the same paths during training as those at testing time. We justify our framework by performing a comparative study.


Title: Pressure-Driven Manipulator with Variable Stiffness Structure
Key Words: actuators  controllability  deformation  fibre reinforced composites  manipulators  position control  rigidity  air pressure  controllability  position dependent stiffness  manipulator stiffness  variable stiffness structure  pressure driven manipulator  soft rigid stiffness control structure  unidirectional fiber reinforced actuators  soft robot deformability  soft robot compliance  bidirectional in-plane manipulator  Manipulators  Actuators  Encapsulation  Fabrication  Force  Controllability 
Abstract: The high deformability and compliance of soft robots allow safer interaction with the environment. On the other hand, these advantages bring along controllability and predictability challenges which result in loss of force and stiffness output. Such challenges should be addressed in order to improve the overall functional performance and to meet the requirements of real-scenario applications. In this paper, we present a bidirectional in-plane manipulator which consists of two unidirectional fiber-reinforced actuators (FRAs) and a hybrid soft-rigid stiffness control structure (SCS), all of them controlled by air pressure. Both controllability and predictability of the manipulator are enhanced by the hybrid soft-rigid structure. While the FRAs provide positioning and position dependent stiffness, the SCS increases the stiffness of the manipulator without position dependency. The SCS is able to increase the manipulator stiffness by 35%, 30%, and 18%, when one FRA is pressurized at 150 kPa, 75 kPa, and 0 kPa, respectively. Experiments are carried out to present the feasibility of the proposed manipulator.


Title: Human Interface for Teleoperated Object Manipulation with a Soft Growing Robot
Key Words: biomechanics  human-robot interaction  manipulators  robots  service robots  telerobotics  soft growing robot manipulator  body-movement-based interface  pick-and-place manipulation task  human-centered interface  complex manipulation tasks  teleoperated object manipulation  human interface  Robot kinematics  Task analysis  Manipulators  Soft robotics  Tracking  Robot sensing systems 
Abstract: Soft growing robots are proposed for use in applications such as complex manipulation tasks or navigation in disaster scenarios. Safe interaction and ease of production promote the usage of this technology, but soft robots can be challenging to teleoperate due to their unique degrees of freedom. In this paper, we propose a human-centered interface that allows users to teleoperate a soft growing robot for manipulation tasks using arm movements. A study was conducted to assess the intuitiveness of the interface and the performance of our soft robot, involving a pick-and-place manipulation task. The results show that users were able to complete the task 97% of the time and achieve placement errors below 2 cm on average. These results demonstrate that our body-movement-based interface is an effective method for control of a soft growing robot manipulator.


Title: Modulating hip stiffness with a robotic exoskeleton immediately changes gait
Key Words: gait analysis  kinematics  medical control systems  medical robotics  patient rehabilitation  modulating hip stiffness  robotic exoskeleton  healthy kinematics  critical component  assisting rehabilitating impaired locomotion  spatiotemporal gait patterns  mechanical impedance  hip joints  Samsung GEMS-H  virtual spring  short repeated exposures  spatiotemporal measures  stiffness controller  gait behavior  mechanical effect  lower-limb assistive devices  healthy gait patterns  Hip  Exoskeletons  Legged locomotion  Read only memory  Kinematics  Springs  Time measurement 
Abstract: Restoring healthy kinematics is a critical component of assisting and rehabilitating impaired locomotion. Here we tested whether spatiotemporal gait patterns can be modulated by applying mechanical impedance to hip joints. Using the Samsung GEMS-H exoskeleton, we emulated a virtual spring (positive and negative) between the user's legs. We found that applying positive stiffness with the exoskeleton decreased stride time and hip range of motion for healthy subjects during treadmill walking. Conversely, the application of negative stiffness increased stride time and hip range of motion. These effects did not vary over long nor short repeated exposures to applied stiffness. In addition, minimal transient behavior was observed in spatiotemporal measures of gait when the stiffness controller transitioned between on and off states. These results suggest that changes in gait behavior induced by applying hip stiffness were purely a mechanical effect. Together, our findings indicate that applying mechanical impedance using lower-limb assistive devices may be an effective, minimally-encumbering intervention to restore healthy gait patterns.


Title: Knowledge-Guided Reinforcement Learning Control for Robotic Lower Limb Prosthesis
Key Words: artificial limbs  biomechanics  gait analysis  learning (artificial intelligence)  medical robotics  neurophysiology  patient rehabilitation  knowledge transfer  control tuning performance  amputee subject  AB subjects  transfer knowledge  data requirements  RL controller  robotic prosthetic limb  control method  knowledge-guided Q-learning  RL agents learn  controlled prosthesis  prosthesis control  prosthesis device  trans-femoral amputees  passive prostheses  lost functions  robotic lower limb prosthesis  guided reinforcement learning control  Prosthetics  Task analysis  Impedance  Knee  Legged locomotion  Tuning 
Abstract: Robotic prostheses provide new opportunities to better restore lost functions than passive prostheses for trans-femoral amputees. But controlling a prosthesis device automatically for individual users in different task environments is an unsolved problem. Reinforcement learning (RL) is a naturally promising tool. For prosthesis control with a user in the loop, it is desirable that the controlled prosthesis can adapt to different task environments as quickly and smoothly as possible. However, most RL agents learn or relearn from scratch when the environment changes. To address this issue, we propose the knowledge-guided Q-learning (KG-QL) control method as a principled way for the problem. In this report, we collected and used data from two able-bodied (AB) subjects wearing a RL controlled robotic prosthetic limb walking on level ground. Our ultimate goal is to build an efficient RL controller with reduced time and data requirements and transfer knowledge from AB subjects to amputee subjects. Toward this goal, we demonstrate its feasibility by employing OpenSim, a well-established human locomotion simulator. Our results show the OpenSim simulated amputee subject improved control tuning performance over learning from scratch by utilizing knowledge transfer from AB subjects. Also in this paper, we will explore the possibility of information transfer from AB subjects to help tuning for the amputee subjects.


Title: Development of a Twisted String Actuator-based Exoskeleton for Hip Joint Assistance in Lifting Tasks
Key Words: actuators  biomechanics  design engineering  electric motors  handicapped aids  patient rehabilitation  robot dynamics  robot kinematics  torque control  wearable robots  twisted string actuator-based exoskeleton  hip joint assistance  compliant cable-driven exoskeleton  injuries  vocational setting  powerful exoskeleton  inherent TSA advantages  typical torque-speed requirements  exoskeleton design  motor selection  practical exoskeleton prototype  Hip  Exoskeletons  Torque  Pulleys  Actuators  Kinematics  Task analysis 
Abstract: This paper presents a study on a compliant cable-driven exoskeleton for hip assistance in lifting tasks that is aimed at preventing low-back pain and injuries in the vocational setting. In the proposed concept, we used twisted string actuator (TSA) to design a light-weight and powerful exoskeleton that benefits from inherent TSA advantages. We have noted that nonlinear nature of twisted strings' transmission ratio (decreasing with twisting) closely matched typical torque-speed requirements for hip assistance during lifting tasks and tried to use this fact in the exoskeleton design and motor selection. Hip-joint torque and speed required to lift a 10-kg load from stoop to stand were calculated, which gave us a baseline that we used to design and manufacture a practical exoskeleton prototype. Preliminary experimental trials demonstrated that the proposed device was capable of generating required torque and speed at the hip joint while weighing under 6 kg, including battery.


Title: A Novel Portable Lower Limb Exoskeleton for Gravity Compensation during Walking
Key Words: gait analysis  gears  handicapped aids  man-machine systems  medical robotics  motion control  patient rehabilitation  robot kinematics  springs (mechanical)  torque  gravity compensation  walking assistance  spring mechanisms  hip  knee joints  gravity balancing  human leg  mating gears  tension force  springs  safety  user acceptance  design principle  limb joints  single leg exoskeleton  portable passive lower limb exoskeleton  driving torques  Springs  Exoskeletons  Gravity  Legged locomotion  Gears  Potential energy  Hip 
Abstract: This paper presents a novel portable passive lower limb exoskeleton for walking assistance. The exoskeleton is designed with built-in spring mechanisms at the hip and knee joints to realize gravity balancing of the human leg. A pair of mating gears is used to convert the tension force from the built-in springs into balancing torques at hip and knee joints for overcoming the influence of gravity. Such a design makes the exoskeleton has a compact layout with small protrusion, which improves its safety and user acceptance. In this paper, the design principle of gravity balancing is described. Simulation results show a significant reduction of driving torques at the limb joints. A prototype of single leg exoskeleton has been constructed and preliminary test results show the effectiveness of the exoskeleton.


Title: Steerable Burrowing Robot: Design, Modeling and Experiments
Key Words: drag  impact (mechanical)  mobile robots  numerical analysis  robot dynamics  vehicle dynamics  thrusting mechanism  depth dependent model  steerable burrowing robot  vibro-impact mechanism  rotating bevel-tip head  nonholonomic model  steering mechanism  hybrid dynamics model  S-shaped trajectory  Robot kinematics  Needles  Numerical models  Solid modeling  Trajectory  Springs 
Abstract: This paper investigates a burrowing robot that can maneuver and steer while being submerged in a granular medium. The robot locomotes using an internal vibro-impact mechanism and steers using a rotating bevel-tip head. We formulate and investigate a non-holonomic model for the steering mechanism and a hybrid dynamics model for the thrusting mechanism. We perform a numerical analysis of the dynamics of the robot's thrusting mechanism using a simplified, orientation and depth dependent model for the drag forces acting on the robot. We first show, in simulation, that by carefully tuning various control input parameters, the thrusting mechanism can drive the robot both forward and backward. We present several experiments designed to evaluate and verify the simulative results using a proof-of-concept robot. We show that different input amplitudes indeed affect the direction of motion, as suggested by the simulation. We further demonstrate the ability of the robot to perform a simple S-shaped trajectory. These experiments demonstrate the feasibility of the robot's design and fidelity of the model.


Title: High Force Density Gripping with UV Activation and Sacrificial Adhesion
Key Words: adhesion  adhesives  curing  electric motors  grippers  mobile robots  plastics  prototypes  ultraviolet sources  plastic parts  force-to-weight ratio  high force density gripping  UV activation  sacrificial adhesion  light-activated chemical adhesive  ultraviolet light sensitive acrylics  rapid curing  electric motor  proof-of concept prototypes  mobile robots  size 380.0 nm  time 15.0 s to 75.0 s  Grippers  Force  Curing  Adhesives  Chemicals  Mobile robots  Mechanism Design  Manipulation 
Abstract: This paper presents a novel physical gripping framework intended for controlled, high force density attachment on a range of surfaces. Our framework utilizes a light-activated chemical adhesive to attach to surfaces. The cured adhesive is part of a "sacrificial layer," which is shed when the gripper separates from the surface. In order to control adhesive behavior we utilize ultraviolet (UV) light sensitive acrylics which are capable of rapid curing when activated with 380nm light. Once cured, zero input power is needed to hold load. Thin plastic parts can be used as the sacrificial layers, and these can be released using an electric motor. This new gripping framework including the curing load capacity, adhesive deposition, and sacrificial methods are described in detail. Two proof-of concept prototypes are designed, built, and tested. The experimental results illustrate the response time (15-75s depending on load), high holding force-to-weight ratio (10-30), and robustness to material type. Additionally, two drawbacks of this design are discussed: corruption of the gripped surface and a limited number of layers.


Title: Stiffness optimization of a cable driven parallel robot for additive manufacturing
Key Words: cables (mechanical)  industrial robots  optimisation  position control  rigidity  three-dimensional printing  vibration control  stiffness optimization  cable driven parallel robot  additive manufacturing  anchor points  robot stiffness  tool path  platform rigidity  CDPR  3D printing  vibration modes 
Abstract: In this paper, the optimization of the anchor points of a cable driven parallel robot (CDPR) for 3D printing is proposed in order to maximize the rigidity. Indeed, in the context of 3D printing, robot stiffness should guarantee a high level of tool path following accuracy. The optimized platform showed a rigidity improvement in simulation, but also experimentally with a first study of vibration modes. In the same time, this study illustrates the influence of preload in cables on the platform rigidity.


Title: CAMI - Analysis, Design and Realization of a Force-Compliant Variable Cam System
Key Words: cams (mechanical)  compliant mechanisms  end effectors  force control  legged locomotion  motion control  path planning  trajectory control  CAMI  multilegged locomotion  continuous gait transition  end effector trajectory  end effector motions  three dimensional cam system  force compliant variable cam system  bipedal robot  Legged locomotion  Trajectory  End effectors  Couplings  Shape  Actuators 
Abstract: This work presents a novel design concept that achieves multi-legged locomotion using a three-dimensional cam system. A computational framework has been developed to analyze and dimension this cam apparatus, that can perform arbitrary end effector motions within its design constraints. The mechanism enables continuous gait transition and inherent force compliance. With only two motors, any trajectory of a continuous set of gaits can be followed. One motor is used to actuate the system and a second one to morph its movement. To illustrate a possible application of this system, a working prototype of a bipedal robot is developed and validated in hardware. It showcases a smooth velocity change by transitioning through different gaits from standing still to walking fast at 124mm/s within 2.0s, while following the given end effector trajectory with an error of only 2.47mm.


Title: Using Manipulation to Enable Adaptive Ground Mobility
Key Words: adhesion  legged locomotion  manipulators  permanent magnets  propulsion  road vehicles  wheels  swappable propulsors  adhesion forces  wheeled locomotion  legged locomotion  autonomous ground vehicles  terrain  whegs  physical adaptation  multipurpose manipulators  propulsion system  adaptive ground mobility  permanent magnets  functional prototype robot  Legged locomotion  Wheels  Manipulators  Steel  Force  Mechanism design  mobile manipulation  wheeled robots 
Abstract: In order to accomplish various missions, autonomous ground vehicles must operate on a wide range of terrain. While many systems such as wheels and whegs can navigate some types of terrain, none are optimal across all. This creates a need for physical adaptation. This paper presents a broad new approach to physical adaptation that relies on manipulation. Specifically, we explore how multipurpose manipulators can enable ground vehicles to dramatically modify their propulsion system in order to optimize performance across various terrain. While this approach is general and widely applicable, this work focuses on physically switching between wheels and legs. We outline the design of "swappable propulsors" that combine the powerful adhesion forces of permanent magnets with geometric features for easy detachment. We provide analysis on how the swappable propulsors can be manipulated, and use these results to create a functional prototype robot. This robot can use its manipulator to change between wheeled and legged locomotion. Our experimental results illustrate how this approach can enhance energy efficiency and versatility.


Title: Cooperative Autonomy and Data Fusion for Underwater Surveillance With Networked AUVs
Key Words: autonomous underwater vehicles  mobile robots  sensor fusion  target tracking  underwater acoustic communication  AUV cooperative strategies  data fusion  realistic underwater surveillance scenarios  networked AUVs  data sharing  robotic networks  underwater surveillance applications  autonomous underwater vehicles  CMRE Anti-Submarine Warfare network  track management module  robot autonomy software  track classification  T2T association  Target tracking  Robot kinematics  Sonar  Receivers  Signal processing algorithms 
Abstract: Cooperative autonomy and data sharing can largely improve the mission performance of robotic networks in underwater surveillance applications. In this paper, we describe the cooperative autonomy used to control the Autonomous Underwater Vehicles (AUVs) acting as sonar receiver nodes in the CMRE Anti-Submarine Warfare (ASW) network. The paper focuses on a track management module that was integrated in the robot autonomy software for enabling the share of information. Track to track (T2T) associations are used for improving track classification and for creating a common tactical picture, necessary for AUV cooperative strategies. We also present a new cooperative data-driven AUV behaviour that exploits the spatial diversity of multiple robots for improving target tracking and for facilitating T2T associations. We report results with real data collected at sea that validate the approach. The reported results are one of the first examples that show the potential of cooperative autonomy and data fusion in realistic underwater surveillance scenarios characterised by limited communications.


Title: Bidirectional Resonant Propulsion and Localization for AUVs
Key Words: autonomous underwater vehicles  diaphragms  electromagnetic actuators  marine control  mobile robots  motion control  robot vision  SLAM (robots)  electromagnetic voice coil motor  bidirectional resonant propulsion  AUV localization  thrust vectors  diaphragm pump mechanism  resonant motion  actuator design  bidirectional resonant pump  autonomous underwater vehicles  Propulsion  Resonant frequency  Strain  Standards  Damping  Reliability engineering 
Abstract: Battery life, reliability, and localization are prominent challenges in the design of autonomous underwater vehicles (AUVs). This work aims to address facets of these challenges using a single system. We describe the design of a bidirectional resonant pump that uses a single electromagnetic voice coil motor (VCM) capable of rotation around a central two degree-of-freedom flexure stage axis. This actuator design produces highly efficient resonant motion that drives two orthogonally oriented diaphragms simultaneously. The operation of this diaphragm pump mechanism produces both adjustable thrust vectors at the aft surface of the AUV and a monotonic relationship between thrust vectors and operating frequency. We propose using the unique frequency to thrust relationship to enhance AUV localization capabilities. We construct a prototype and use it to experimentally demonstrate the feasibility of the directionally-tunable resonance concept.


Title: Hierarchical Planning in Time-Dependent Flow Fields for Marine Robots
Key Words: autonomous underwater vehicles  computational complexity  graph theory  marine vehicles  mobile robots  path planning  remotely operated vehicles  hierarchical planning  time-dependent flow fields  shortest paths  flow predictions  motion planning  slow marine robots  dynamic ocean currents  time-dependent graphs  polynomial-time algorithm  continuous trajectories  time-varying edge costs  underlying flow field  continuous algorithm  time complexity  path quality properties  autonomous marine vehicle  marine robotics  time-varying ocean predictions  East Australian Current  Planning  Robots  Vehicle dynamics  Oceans  Heuristic algorithms  Prediction algorithms  Trajectory 
Abstract: We present an efficient approach for finding shortest paths in flow fields that vary as a sequence of flow predictions over time. This approach is applicable to motion planning for slow marine robots that are subject to dynamic ocean currents. Although the problem is NP-hard in general form, we incorporate recent results from the theory of finding shortest paths in time-dependent graphs to construct a polynomial-time algorithm that finds continuous trajectories in time-dependent flow fields. The algorithm has a hierarchical structure where a graph is constructed with time-varying edge costs that are derived from sets of continuous trajectories in the underlying flow field. We show that the continuous algorithm retains the time complexity and path quality properties of the discrete graph solution, and demonstrate its application to surface and underwater vehicles including a traversal along the East Australian Current with an autonomous marine vehicle. Results show that the algorithm performs efficiently in practice and can find paths that adapt to changing ocean currents. These results are significant to marine robotics because they allow for efficient use of time-varying ocean predictions for motion planning.


Title: Navigation in the Presence of Obstacles for an Agile Autonomous Underwater Vehicle
Key Words: autonomous underwater vehicles  collision avoidance  feature extraction  mobile robots  navigation  optimisation  robot vision  stereo image processing  fly-overs  AUV  cluttered space  navigation framework  sampling-based correction procedure  obstacles detection  real-time 3D autonomous navigation  agile autonomous underwater vehicle  Trajopt  3D path-optimization planning  visual features detection  Planning  Three-dimensional displays  Navigation  Optimization  Robots  Trajectory  Cameras 
Abstract: Navigation underwater traditionally is done by keeping a safe distance from obstacles, resulting in "fly-overs" of the area of interest. Movement of an autonomous underwater vehicle (AUV) through a cluttered space, such as a shipwreck or a decorated cave, is an extremely challenging problem that has not been addressed in the past. This paper proposes a novel navigation framework utilizing an enhanced version of Trajopt for fast 3D path-optimization planning for AUVs. A sampling-based correction procedure ensures that the planning is not constrained by local minima, enabling navigation through narrow spaces. Two different modalities are proposed: planning with a known map results in efficient trajectories through cluttered spaces; operating in an unknown environment utilizes the point cloud from the visual features detected to navigate efficiently while avoiding the detected obstacles. The proposed approach is rigorously tested, both on simulation and in-pool experiments, proven to be fast enough to enable safe real-time 3D autonomous navigation for an AUV.


Title: Underwater Image Super-Resolution using Deep Residual Multipliers
Key Words: image resolution  learning (artificial intelligence)  neural nets  robot vision  underwater vehicles  single image super-resolution  autonomous underwater robots  adversarial training pipeline  perceptual quality  global content  local style information  USR-248  SISR  state-of-the-art models  deep residual multipliers  deep residual network-based generative model  underwater image super-resolution  noisy visual conditions  Training  Image resolution  Robots  Data models  Cameras  Pipelines  Generators 
Abstract: We present a deep residual network-based generative model for single image super-resolution (SISR) of underwater imagery for use by autonomous underwater robots. We also provide an adversarial training pipeline for learning SISR from paired data. In order to supervise the training, we formulate an objective function that evaluates the perceptual quality of an image based on its global content, color, and local style information. Additionally, we present USR-248, a large-scale dataset of three sets of underwater images of `high' (640×480) and `low' (80 × 60, 160 × 120, and 320×240) resolution. USR-248 contains paired instances for supervised training of 2×, 4×, or 8× SISR models. Furthermore, we validate the effectiveness of our proposed model through qualitative and quantitative experiments and compare the results with several state-of-the-art models' performances. We also analyze its practical feasibility for applications such as scene understanding and attention modeling in noisy visual conditions.


Title: Nonlinear Synchronization Control for Short-Range Mobile Sensors Drifting in Geophysical Flows
Key Words: actuators  mobile radio  oceanographic techniques  synchronisation  telecommunication control  wireless sensor networks  short-range mobile sensors drifting  geophysical flows  ocean monitoring applications  minimal actuation capabilities  active drifters  gyre flows  data exchange  nonlinear synchronization control strategy  rendezvous regions  large-scale mobile sensor networks  numerical simulations  small-scale experiments  Synchronization  Sensors  Vehicle dynamics  Orbits  Robots  Oscillators  Dynamics 
Abstract: This paper presents a synchronization controller for mobile sensors that are minimally actuated and can only communicate with each other over a very short range. This work is motivated by ocean monitoring applications where large-scale sensor networks consisting of drifters with minimal actuation capabilities, i.e., active drifters, are employed. We assume drifters are tasked to monitor regions consisting of gyre flows where their trajectories are periodic. As drifters in neighboring regions move into each other's proximity, it presents an opportunity for data exchange and synchronization to ensure future rendezvous. We present a nonlinear synchronization control strategy to ensure that drifters will periodically rendezvous and maximize the time they are in their rendezvous regions. Numerical simulations and small-scale experiments validate the efficacy of the control strategy and hint at extensions to large-scale mobile sensor networks.


Title: Energy-based Safety in Series Elastic Actuation
Key Words: actuators  elasticity  energy-based safety  series elastic actuation  generic actuation passivity  energy storage  power flow properties  power limits  Safety  Robots  Energy storage  Actuators  Impedance  Torque 
Abstract: This work presents the concept of energy-based safety for series-elastic actuation. Generic actuation passivity and safety is treated, defining several energy storage and power flow properties related to passivity. Safe behaviour is not guaranteed by passivity, but can be guaranteed by energy and power limits that adapt the nominal behaviour of an impedance controller. A discussion on power flows in series-elastic actuation is presented and an appropriate controller is developed. Experimental results validate the effectiveness of the energy-based safety in elastic actuation.


Title: Variable Stiffness Springs for Energy Storage Applications
Key Words: actuators  energy storage  mathematical analysis  rigidity  robot dynamics  springs (mechanical)  variable stiffness actuation technology  variable stiffness springs  energy storage capacity  linear helical springs  variable stiffness actuators  human performance augmentation  spring exoskeleton  controllable volume air spring  mathematical conditions  Springs  Energy storage  Actuators  Potential energy  Strain  Force  Valves 
Abstract: Theory suggests an inverse relation between the stiffness and the energy storage capacity for linear helical springs: reducing the active length of the spring by 50% increases its stiffness by 100%, but reduces its energy storage capacity by 50%. State-of-the-art variable stiffness actuators used to drive robots are characterized by a similar inverse relation, implying reduced energy storage capacity for increased spring stiffness. This relation limits the potential of the variable stiffness actuation technology when it comes to human performance augmentation in natural tasks, e.g., jumping, weight-bearing and running, which may necessitate a spring exoskeleton with large stiffness range and high energy storage capacity. In this paper, we theoretically show that the trade-off between stiffness range and energy storage capacity is not fundamental; it is possible to develop variable stiffness springs with simultaneously increasing stiffness and energy storage capacity. Consistent with the theory, we experimentally show that a controllable volume air spring, has a direct relation between its stiffness range and energy storage capacity. The mathematical conditions presented in this paper may be used to develop actuators that could bypass the limited energy storage capacity of current variable stiffness spring technology.


Title: Parallel-motion Thick Origami Structure for Robotic Design
Key Words: art  control system synthesis  grippers  motion control  paper  parallel-motion thick origami structure  robotic design  three-dimensional shapes  zero-thickness flat paper sheets  origami facets  multiple layer origami structures  parallel-motion gripper  Fasteners  Grippers  Robots  Shape  Actuators  Force  Electronic mail 
Abstract: Structures with origami design enable objects to transform into various three-dimensional shapes. Traditionally origami structures are designed with zero-thickness flat paper sheets. However, the thickness and intersection of origami facets are non-negligible in most cases, uniquely when integrating origami design with robotic design because of the more efficient force transfer between thick plates compared with zero-thickness paper-sheets. Meanwhile, the single-layer-paper oriented initial design limited the shape transformation potential as multiple layer origami structures could conduct more variety of deformation. In this article, we are proposing a general design method of parallel-motion thick origami structures that could apply in robotic design like a parallel-motion gripper.


Title: Real-time Simulation of Non-Deformable Continuous Tracks with Explicit Consideration of Friction and Grouser Geometry
Key Words: friction  mobile robots  motion control  tracked vehicles  trajectory control  velocity control  nondeformable continuous tracks  grouser geometry  real-time simulation  circular segments  robot body  segment link  track rotation  friction  rough terrain  track trajectory  velocity constraints  tracked vehicles  Robots  Trajectory  Tracking  Friction  Collision avoidance  Real-time systems  Wheels 
Abstract: In this study, we developed a real-time simulation method for non-deformable continuous tracks having grousers for rough terrain by explicitly considering the collision and friction between the tracks and the ground. In the proposed simulation method, an arbitrary trajectory of a track is represented with multiple linear and circular segments, each of which is a link connected to a robot body. The proposed method sets velocity constraints between each segment link and the robot body, to simulate the track rotation around the body. To maintain the shape of a track, it also restores the positions of the segment links when required. Experimental comparisons with other existing real-time simulation methods demonstrated that while the proposed method considered the grousers and the friction with the ground, it was comparable to them in terms of the computational speed. Experimental comparison of the simulations based on the proposed method and a physical robot exhibited that the former was comparable to the precise motion of the robot on rough or uneven terrain.


Title: Test Your SLAM! The SubT-Tunnel dataset and metric for mapping
Key Words: mobile robots  public domain software  robot vision  SLAM (robots)  SLAM  open source tools  robotic mapping algorithms  DARPA Subterranean challenge  SubT-Tunnel dataset  subterranean mine rescue dataset  Simultaneous localization and mapping  Cameras  Measurement  Laser radar  Robot vision systems 
Abstract: This paper presents an approach and introduces new open-source tools that can be used to evaluate robotic mapping algorithms. Also described is an extensive subterranean mine rescue dataset based upon the DARPA Subterranean (SubT) challenge including professionally surveyed ground truth. Finally, some commonly available approaches are evaluated using this metric.


Title: Uncertainty Measured Markov Decision Process in Dynamic Environments
Key Words: Markov processes  mobile robots  path planning  robot motion planning  path planning method  tracking robot  dynamic environments  robot path planning  visual occlusions  moving targets  visioning  perception algorithms  partially observable Markov decision  pursuit-evasion  robot tracking  predictive path planning  Uncertainty  Planning  Robots  Markov processes  Target tracking  Probabilistic logic  Measurement uncertainty 
Abstract: Successful robot path planning is challenging in the presence of visual occlusions and moving targets. Classical methods to solve this problem have used visioning and perception algorithms in addition to partially observable markov decision processes to aid in path planning for pursuit-evasion and robot tracking. We present a predictive path planning process that measures and utilizes the uncertainty present during robot motion planning. We develop a variant of subjective logic in combination with the Markov decision process (MDP) and provide a measure for belief, disbelief, and uncertainty in relation to feasible trajectories being generated. We then model the MDP to identify the best path planning method from a list of possible choices. Our results show a high percentage accuracy based on the closest acquired proximity between a target and a tracking robot and a simplified pursuer trajectory in comparison with related work.


Title: Wide-range Load Sensor Using Vacuum Sealed Quartz Crystal Resonator for Simultaneous Biosignals Measurement on Bed
Key Words: biomechanics  biomedical equipment  biomedical measurement  cardiology  crystal resonators  electrocardiography  force measurement  force sensors  medical signal detection  medical signal processing  microfabrication  microsensors  patient monitoring  pneumodynamics  pressure sensors  quartz  sensors  wide-range load sensor  vacuum sealed quartz crystal resonator  biosignal measurement  QCR load sensor  measurement range  sensor structure  force sensor  QCR load sensing system  Bonding  Weight measurement  Robot sensing systems  Force  Heart beat  Resists  Stress 
Abstract: Monitoring of biosignals on a daily basis plays important roles for the health management of elderly. The monitoring system for the daily life, the system should not require the subjects to take special effort like wearing a sensor. We propose biosignals measurement using wide-range load sensor on the bed. The sensing system can detect the body weight, heartbeat and respiration simultaneously by just lying on the bed. We have developed load sensor using quartz crystal resonator (QCR load sensor) as wide-range load sensor. However, the measurement range was not sufficient for the simultaneous measurement of biosgnals on bed. To realize such sensing system, we propose a QCR load sensor utilizing vacuum sealing technology for expanding the measurement range. We improved the oscillation characteristics of the QCR by the vacuum sealing to stabilize the sensor output. Accordingly, the resolution of the sensor was improved. Moreover, the load capacity of the sensor was increased by improving the bonding strength of sensor structure. The fabricated sensor had a measurement range of 0.27 mN - 1180 N (4.4 × 106). This wide enough compared with the conventional force sensor (103 - 104).Also, we developed mechanically robust jig of QCR load sensor for practical use of QCR load sensor. We succeed in simultaneous measurement of weight, heart rate, and respiration rate using fabricated QCR load sensing system. The accuracy of heart rate and respiration rate measurement are 0.4 bpm (0.6 %) and 1.1 brpm (6.1 %), respectively, in standard deviation of error compared with ECG signal.


Title: Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships
Key Words: graph theory  image matching  mobile robots  robot vision  SLAM (robots)  robotics applications  simultaneously localization and mapping  spatial relationship similarities  spatial cues  visual cues  old landmarks  long-term environment changes  landmark information  integrate landmark appearances  worst-case graph matching  place recognition performance  long-term place recognition  worst appearance similarity  similar appearances  worst-case scenario  graph matching problem  visual appearances  angular spatial relationships  graph representation  Visualization  Simultaneous localization and mapping  Robustness  Strain  Image recognition  Tensile stress 
Abstract: Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.


Title: Linear RGB-D SLAM for Atlanta World
Key Words: cameras  image colour analysis  Kalman filters  mobile robots  object detection  object tracking  pose estimation  SLAM (robots)  Manhattan world assumption  orthogonal directions  Atlanta world  vertical direction  horizontal directions  SLAM techniques  Atlanta representation  Atlanta frame-aware linear SLAM framework  Atlanta structure  linear Kalman filter  linear RGB-D SLAM  simultaneous localization and mapping  tracking-by-detection scheme  scene structure  camera motion  planar map  synthetic datasets  real datasets  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Tracking  Kalman filters  Visualization  Robustness 
Abstract: We present a new linear method for RGB-D based simultaneous localization and mapping (SLAM). Compared to existing techniques relying on the Manhattan world assumption defined by three orthogonal directions, our approach is designed for the more general scenario of the Atlanta world. It consists of a vertical direction and a set of horizontal directions orthogonal to the vertical direction and thus can represent a wider range of scenes. Our approach leverages the structural regularity of the Atlanta world to decouple the non-linearity of camera pose estimations. This allows us separately to estimate the camera rotation and then the translation, which bypasses the inherent non-linearity of traditional SLAM techniques. To this end, we introduce a novel tracking-by-detection scheme to estimate the underlying scene structure by Atlanta representation. Thereby, we propose an Atlanta frame-aware linear SLAM framework which jointly estimates the camera motion and a planar map supporting the Atlanta structure through a linear Kalman filter. Evaluations on both synthetic and real datasets demonstrate that our approach provides favorable performance compared to existing state-of-the-art methods while extending their working range to the Atlanta world.


Title: Lidar-Monocular Visual Odometry using Point and Line Features
Key Words: distance measurement  feature extraction  image sequences  mobile robots  motion estimation  optical radar  pose estimation  robot vision  stereo image processing  line features  lidar-visual odometry  pose estimation  line depth extraction  point-line bundle adjustment  purely visual motion tracking method  public KITTI odometry benchmark  lidar-monocular visual odometry approach  point-only based lidar-visual odometry  environment structure information  efficient lidar-monocular visual odometry system  Feature extraction  Cameras  Bundle adjustment  Laser radar  Image segmentation  Optimization 
Abstract: We introduce a novel lidar-monocular visual odometry approach using point and line features. Compared to previous point-only based lidar-visual odometry, our approach leverages more environment structure information by introducing both point and line features into pose estimation. We provide a robust method for point and line depth extraction, and formulate the extracted depth as prior factors for point-line bundle adjustment. This method greatly reduces the features' 3D ambiguity and thus improves the pose estimation accuracy. Besides, we also provide a purely visual motion tracking method and a novel scale correction scheme, leading to an efficient lidar-monocular visual odometry system with high accuracy. The evaluations on the public KITTI odometry benchmark show that our technique achieves more accurate pose estimation than the state-of-the-art approaches, and is sometimes even better than those leveraging semantic information.


Title: Probabilistic Data Association via Mixture Models for Robust Semantic SLAM
Key Words: Gaussian processes  image sensors  mobile robots  object detection  probability  robot vision  SLAM (robots)  target tracking  probabilistic data association  mixture models  robust semantic SLAM  robotic systems  cameras  lidar  visual models  reliable navigation  semantic uncertainty inherent  geometric uncertainty inherent  object detection methods  data association ambiguity  nonlinear Gaussian formulation  data association variables  max-marginalization  standard Gaussian posterior assumptions  max-mixture-type model  multiple data association hypotheses  indoor navigation tasks  outdoor semantic navigation tasks  semantic SLAM approaches  simultaneous localization and mapping  noisy odometry  Semantics  Simultaneous localization and mapping  Robustness  Optimization  Object detection  Uncertainty 
Abstract: Modern robotic systems sense the environment geometrically, through sensors like cameras, lidar, and sonar, as well as semantically, often through visual models learned from data, such as object detectors. We aim to develop robots that can use all of these sources of information for reliable navigation, but each is corrupted by noise. Rather than assume that object detection will eventually achieve near perfect performance across the lifetime of a robot, in this work we represent and cope with the semantic and geometric uncertainty inherent in object detection methods. Specifically, we model data association ambiguity, which is typically non-Gaussian, in a way that is amenable to solution within the common nonlinear Gaussian formulation of simultaneous localization and mapping (SLAM). We do so by eliminating data association variables from the inference process through max-marginalization, preserving standard Gaussian posterior assumptions. The result is a max-mixture-type model that accounts for multiple data association hypotheses. We provide experimental results on indoor and outdoor semantic navigation tasks with noisy odometry and object detection and find that the ability of the proposed approach to represent multiple hypotheses, including the "null" hypothesis, gives substantial robustness advantages in comparison to alternative semantic SLAM approaches.


Title: Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy
Key Words: closed loop systems  Global Positioning System  mobile robots  navigation  robot vision  SLAM (robots)  stereo image processing  tracking  representative state-of-the-art visual-inertial SLAM systems  visual estimation module  stereo visual-inertial SLAM systems  open-loop analysis  closed-loop navigation tasks  accurate trajectory tracking  visualinertial SLAM systems  closed-loop benchmarking simulation  visual-inertial estimation  trajectory tracking performance  Visualization  Navigation  Simultaneous localization and mapping  Benchmark testing  Estimation 
Abstract: Visual-inertial SLAM is essential for robot navigation in GPS-denied environments, e.g. indoor, underground. Conventionally, the performance of visual-inertial SLAM is evaluated with open-loop analysis, with a focus on the drift level of SLAM systems. In this paper, we raise the question on the importance of visual estimation latency in closed-loop navigation tasks, such as accurate trajectory tracking. To understand the impact of both drift and latency on visualinertial SLAM systems, a closed-loop benchmarking simulation is conducted, where a robot is commanded to follow a desired trajectory using the feedback from visual-inertial estimation. By extensively evaluating the trajectory tracking performance of representative state-of-the-art visual-inertial SLAM systems, we reveal the importance of latency reduction in visual estimation module of these systems. The findings suggest directions of future improvements for visual-inertial SLAM.


Title: Learning error models for graph SLAM
Key Words: autonomous aerial vehicles  graph theory  mobile robots  path planning  robot vision  SLAM (robots)  resistance distance  covisibility graph  simulated UAV coverage path  uncertainty models  monocular graph SLAM  topological features  error model learning  UAV coverage path planning trajectories  Simultaneous localization and mapping  Resistance  Uncertainty  Computational modeling  Computer architecture  Predictive models  Cameras 
Abstract: Following recent developments, this paper investigates the possibility to predict uncertainty models for monocular graph SLAM using topological features of the problem. An architecture to learn relative (i.e. inter-keyframe) uncertainty models using the resistance distance in the covisibility graph is presented. The proposed architecture is applied to simulated UAV coverage path planning trajectories and an analysis of the approaches strengths and shortcomings is provided.


Title: SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability
Key Words: human-robot interaction  natural language processing  robot vision  video signal processing  video captioning  computational requirements  fully-attentive captioning algorithm  language generation  transformer layers  decoding stages  image regions  caption quality  autonomous agents  domestic robots  SMArT  robotic explainability  natural language explanations  visual perception  shallow memory-aware transformer training  memory-aware encoding  image captioning  image captioning  Decoding  Computational modeling  Visualization  Magnetic heads  Robots  Natural languages  Encoding 
Abstract: The ability to generate natural language explanations conditioned on the visual perception is a crucial step towards autonomous agents which can explain themselves and communicate with humans. While the research efforts in image and video captioning are giving promising results, this is often done at the expense of the computational requirements of the approaches, limiting their applicability to real contexts. In this paper, we propose a fully-attentive captioning algorithm which can provide state-of-the-art performances on language generation while restricting its computational demands. Our model is inspired by the Transformer model and employs only two Transformer layers in the encoding and decoding stages. Further, it incorporates a novel memory-aware encoding of image regions. Experiments demonstrate that our approach achieves competitive results in terms of caption quality while featuring reduced computational demands. Further, to evaluate its applicability on autonomous agents, we conduct experiments on simulated scenes taken from the perspective of domestic robots.


Title: A 3D-Deep-Learning-based Augmented Reality Calibration Method for Robotic Environments using Depth Sensor Data
Key Words: augmented reality  calibration  cameras  control engineering computing  image sensors  learning (artificial intelligence)  mobile robots  neural nets  public domain software  robot vision  solid modelling  robotic environments  mobile robots  depth camera  deep learning-based calibration  open source 3D point cloud labeling tool  head mounted augmented reality device  3D depth sensor data  Microsoft Hololens  neural network  VoteNet architecture  3D-deep-learning-based augmented reality calibration  Robot sensing systems  Three-dimensional displays  Calibration  Neural networks  Augmented reality  Robot kinematics 
Abstract: Augmented Reality and mobile robots are gaining increased attention within industries due to the high potential to make processes cost and time efficient. To facilitate augmented reality, a calibration between the Augmented Reality device and the environment is necessary. This is a challenge when dealing with mobile robots due to the mobility of all entities making the environment dynamic. On this account, we propose a novel approach to calibrate Augmented Reality devices using 3D depth sensor data. We use the depth camera of a Head Mounted Augmented Reality Device, the Microsoft Hololens, for deep learning-based calibration. Therefore, we modified a neural network based on the recently published VoteNet architecture which works directly on raw point cloud input observed by the Hololens. We achieve satisfying results and eliminate external tools like markers, thus enabling a more intuitive and flexible work flow for Augmented Reality integration. The results are adaptable to work with all depth cameras and are promising for further research. Furthermore, we introduce an open source 3D point cloud labeling tool, which is to our knowledge the first open source tool for labeling raw point cloud data.


Title: Adversarial Feature Training for Generalizable Robotic Visuomotor Control
Key Words: feature extraction  learning (artificial intelligence)  robot vision  robotic policy training  large-scale data collection  task-setup  task-irrelevant objects  interactive samples  adversarial training  deep RL capabilities  transfer learning  robotic tasks  adversarial feature training  generalizable robotic visuomotor control  deep reinforcement learning  action-selection policies  image pixels mapping  visuomotor robotic policy training  Task analysis  Training  Visualization  Feature extraction  Robots  Trajectory  Clutter 
Abstract: Deep reinforcement learning (RL) has enabled training action-selection policies, end-to-end, by learning a function which maps image pixels to action outputs. However, it's application to visuomotor robotic policy training has been limited because of the challenge of large-scale data collection when working with physical hardware. A suitable visuomotor policy should perform well not just for the task-setup it has been trained for, but also for all varieties of the task, including novel objects at different viewpoints surrounded by task-irrelevant objects. However, it is impractical for a robotic setup to sufficiently collect interactive samples in a RL framework to generalize well to novel aspects of a task. In this work, we demonstrate that by using adversarial training for domain transfer, it is possible to train visuomotor policies based on RL frameworks, and then transfer the acquired policy to other novel task domains. We propose to leverage the deep RL capabilities to learn complex visuomotor skills for uncomplicated task setups, and then exploit transfer learning to generalize to new task domains provided only still images of the task in the target domain. We evaluate our method on two real robotic tasks, picking and pouring, and compare it to a number of prior works, demonstrating its superiority.


Title: Efficient Bimanual Manipulation Using Learned Task Schemas
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  parameterizations  sparse-reward tasks  robotic bimanual manipulation tasks  parameterized skills  state-independent task schema  model-free reinforcement learning  robotic systems  Task analysis  Learning (artificial intelligence)  Neural networks  Force  Geometry  End effectors 
Abstract: We address the problem of effectively composing skills to solve sparse-reward tasks in the real world. Given a set of parameterized skills (such as exerting a force or doing a top grasp at a location), our goal is to learn policies that invoke these skills to efficiently solve such tasks. Our insight is that for many tasks, the learning process can be decomposed into learning a state-independent task schema (a sequence of skills to execute) and a policy to choose the parameterizations of the skills in a state-dependent manner. For such tasks, we show that explicitly modeling the schema's state-independence can yield significant improvements in sample efficiency for model-free reinforcement learning algorithms. Furthermore, these schemas can be transferred to solve related tasks, by simply re-learning the parameterizations with which the skills are invoked. We find that doing so enables learning to solve sparse-reward tasks on real-world robotic systems very efficiently. We validate our approach experimentally over a suite of robotic bimanual manipulation tasks, both in simulation and on real hardware. See videos at http://tinyurl.com/chitnis-schema.


Title: Real-Time UAV Path Planning for Autonomous Urban Scene Reconstruction
Key Words: autonomous aerial vehicles  computational geometry  image reconstruction  path planning  robot vision  SLAM (robots)  unmanned aerial vehicles  large-scale scene mapping  autonomous urban scene reconstruction  point cloud reconstruction  reconstruction quality  large-scale scene reconstruction  real-time UAV path planning  SLAM  Buildings  Image reconstruction  Three-dimensional displays  Path planning  Drones  Cameras  Layout 
Abstract: Unmanned aerial vehicles (UAVs) are frequently used for large-scale scene mapping and reconstruction. However, in most cases, drones are operated manually, which should be more effective and intelligent. In this article, we present a method of real-time UAV path planning for autonomous urban scene reconstruction. Considering the obstacles and time costs, we utilize the top view to generate the initial path. Then we estimate the building heights and take close-up pictures that reveal building details through a SLAM framework. To predict the coverage of the scene, we propose a novel method which combines information on reconstructed point clouds and possible coverage areas. The experimental results reveal that the reconstruction quality of our method is good enough. Our method is also more time-saving than the state-of-the-arts.


Title: A Fast Marching Gradient Sampling Strategy for Motion Planning using an Informed Certificate Set
Key Words: collision avoidance  gradient methods  graph theory  mobile robots  path planning  sampling methods  convergence speed  safety certificate algorithms  fast marching gradient sampling strategy  sampling-based motion planning algorithms  marching seed  goal set  informed certificate set  planning space  RRT* algorithms  Planning  Safety  Convergence  Algorithms  Robots  Data structures  Collision avoidance 
Abstract: We present a novel fast marching gradient sampling strategy to accelerate the convergence speed of sampling-based motion planning algorithms. This strategy is based on an informed certificate set which consists of the robot states with exact collision status as well as the minimum distance and the gradient to the nearest obstacle. The informed certificate set covers almost the whole planning space such that it contains rich information for the planner. The best quality point in this set is selected as the marching seed to guide the search graph move steadily to the goal set. The distance and gradient information of the marching seed helps to generate a new sample with almost sure collision status. When a feasible solution has been found, this set can construct the restricted subset that can improve current path quality. This marching gradient sampling strategy is applied to the RRT and RRT* algorithms. Simulation experiments demonstrate that the convergence speed to a feasible solution or to the optimal solution is almost twice faster than that of the safety certificate algorithms.


Title: Privacy-Aware UAV Flights through Self-Configuring Motion Planning
Key Words: aircraft control  autonomous aerial vehicles  collision avoidance  data privacy  decision making  mobile robots  motion control  privacy-aware UAV flights  unmanned aerial vehicle  uncertain obstacles  motion planning algorithms  privacy-preserving requirements  privacy risk aware motion planning method  privacy-sensitive sensor  safety  energy hard constraints  dynamically detected restricted areas  decision making method  test flights  DJI Matrice 100 UAV  self-configuring motion planning  Privacy  Planning  Cameras  Sensors  Trajectory  Safety  Unmanned aerial vehicles 
Abstract: During flights, an unmanned aerial vehicle (UAV) may not be allowed to move across certain areas due to soft constraints such as privacy restrictions. Current methods on self-adaption focus mostly on motion planning such that the trajectory does not trespass predetermined restricted areas. When the environment is cluttered with uncertain obstacles, however, these motion planning algorithms are not flexible enough to find a trajectory that satisfies additional privacy-preserving requirements within a tight time budget during the flights. In this paper, we propose a privacy risk aware motion planning method through the reconfiguration of privacy-sensitive sensors. It minimises environmental impact by re-configuring the sensor during flight, while still guaranteeing the safety and energy hard constraints such as collision avoidance and timeliness. First, we formulate a model for assessing privacy risks of dynamically detected restricted areas. In case the UAV cannot find a feasible solution to satisfy both hard and soft constraints from the current configuration, our decision making method can then produce an optimal reconfiguration of the privacy-sensitive sensor with a more efficient trajectory. We evaluate the proposal through various simulations with different settings in a virtual environment and also validate the approach through real test flights on DJI Matrice 100 UAV.


Title: Improved C-Space Exploration and Path Planning for Robotic Manipulators Using Distance Information
Key Words: collision avoidance  manipulators  path planning  robot kinematics  trees (mathematics)  generalized bur captures large portions  free C-space  accelerated exploration  exact collision-free paths  improved C-space exploration  path planning  robotic manipulators  distance information  geometrical structure  star-like tree  arbitrary number  guaranteed collision-free edges  simple forward kinematics  RRT-like planning algorithm  generalized burs  Collision avoidance  Robot kinematics  Kinematics  Manipulators  Path planning  Planning 
Abstract: We present a simple method to quickly explore C-spaces of robotic manipulators and thus facilitate path planning. The method is based on a novel geometrical structure called generalized bur. It is a star-like tree, rooted at a given point in free C-space, with an arbitrary number of guaranteed collision-free edges computed using distance information from the workspace and simple forward kinematics. Generalized bur captures large portions of free C-space, enabling accelerated exploration. The workspace is assumed to be decomposable into a finite set of (possibly overlapping) convex obstacles. When plugged in a suitable RRT-like planning algorithm, generalized burs enable significant performance improvements, while at the same time enabling exact collision-free paths.


Title: Tuning-Free Contact-Implicit Trajectory Optimization
Key Words: humanoid robots  manipulators  mobile robots  tuning-free contact-implicit trajectory optimization  contact-implicit trajectory optimization framework  contact-interaction trajectories  robot architectures  trivial initial guess  parameter tuning  relaxed contact model  automatic penalty adjustment loop  contact information  mobile robot  nonprehensile manipulation  7-DOF arm  planar locomotion  Robots  Task analysis  Trajectory optimization  Tuning  Computational modeling 
Abstract: We present a contact-implicit trajectory optimization framework that can plan contact-interaction trajectories for different robot architectures and tasks using a trivial initial guess and without requiring any parameter tuning. This is achieved by using a relaxed contact model along with an automatic penalty adjustment loop for suppressing the relaxation. Moreover, the structure of the problem enables us to exploit the contact information implied by the use of relaxation in the previous iteration, such that the solution is explicitly improved with little computational overhead. We test the proposed approach in simulation experiments for non-prehensile manipulation using a 7-DOF arm and a mobile robot and for planar locomotion using a humanoid-like robot in zero gravity. The results demonstrate that our method provides an out-of-the-box solution with good performance for a wide range of applications.


Title: Robust Real-time UAV Replanning Using Guided Gradient-based Optimization and Topological Paths
Key Words: autonomous aerial vehicles  gradient methods  helicopters  mobile robots  optimisation  path planning  search problems  trajectory control  quadrotor trajectory replanning  replanning method  GTO  path-guided optimization approach  topological path searching algorithm  independent trajectory optimization  output superior replanned trajectories  gradient-based trajectory optimization  UAV replanning  Splines (mathematics)  Linear programming  Trajectory optimization  Robustness  Safety 
Abstract: Gradient-based trajectory optimization (GTO) has gained wide popularity for quadrotor trajectory replanning. However, it suffers from local minima, which is not only fatal to safety but also unfavorable for smooth navigation. In this paper, we propose a replanning method based on GTO addressing this issue systematically. A path-guided optimization (PGO) approach is devised to tackle infeasible local minima, which improves the replanning success rate significantly. A topological path searching algorithm is developed to capture a collection of distinct useful paths in 3-D environments, each of which then guides an independent trajectory optimization. It activates a more comprehensive exploration of the solution space and output superior replanned trajectories. Benchmark evaluation shows that our method outplays state-of-the-art methods regarding replanning success rate and optimality. Challenging experiments of aggressive autonomous flight are presented to demonstrate the robustness of our method. We will release our implementation as an open-source package1.


Title: Learning-based Path Planning for Autonomous Exploration of Subterranean Environments
Key Words: autonomous aerial vehicles  graph theory  learning by example  mobile robots  optical radar  path planning  robot programming  sampled data systems  tunnels  autonomous exploration  subterranean environments  aerial robots  training expert  imitation learning  underground mine drifts  tunnels  graph based path planner  learning based path planning  LiDAR  range data sampling  Robot sensing systems  Path planning  Training  Training data  Planning  Robot kinematics 
Abstract: In this work we present a new methodology on learning-based path planning for autonomous exploration of subterranean environments using aerial robots. Utilizing a recently proposed graph-based path planner as a "training expert" and following an approach relying on the concepts of imitation learning, we derive a trained policy capable of guiding the robot to autonomously explore underground mine drifts and tunnels. The algorithm utilizes only a short window of range data sampled from the onboard LiDAR and achieves an exploratory behavior similar to that of the training expert with a more than an order of magnitude reduction in computational cost, while simultaneously relaxing the need to maintain a consistent and online reconstructed map of the environment. The trained path planning policy is extensively evaluated both in simulation and experimentally within field tests relating to the autonomous exploration of underground mines.


Title: Visual-Inertial Telepresence for Aerial Manipulation
Key Words: aerospace robotics  distance measurement  feedback  grippers  haptic interfaces  human-robot interaction  manipulators  object tracking  robot vision  sensor fusion  telerobotics  virtual reality  visual-inertial telepresence  haptic device  virtual reality  3D visual feedback  inertial sensors  object tracking algorithm  marker tracking algorithm  visual-inertial odometry  aerial manipulation  remotely located teleoperator  onboard visual sensors  human in the loop  Three-dimensional displays  Manipulators  Visualization  Cameras  Task analysis  Sensors 
Abstract: This paper presents a novel telepresence system for enhancing aerial manipulation capabilities. It involves not only a haptic device, but also a virtual reality that provides a 3D visual feedback to a remotely-located teleoperator in real-time. We achieve this by utilizing onboard visual and inertial sensors, an object tracking algorithm and a pregenerated object database. As the virtual reality has to closely match the real remote scene, we propose an extension of a marker tracking algorithm with visual-inertial odometry. Both indoor and outdoor experiments show benefits of our proposed system in achieving advanced aerial manipulation tasks, namely grasping, placing, force exertion and peg-in-hole insertion.


Title: Distributed Rotor-Based Vibration Suppression for Flexible Object Transport and Manipulation
Key Words: autonomous aerial vehicles  control engineering computing  controllability  helicopters  mechanical engineering computing  mobile robots  optimisation  path planning  rotors  vibration control  RVM design  optimal placement  flexible object transport  manipulated object  object size  quadrotor usage  distributed RVMs  constrained optimization problem  aerial-ground manipulator system  robot-based vibration suppression module  distributed rotor-based vibration suppression  controllability gramian  multiple aerial-ground manipulator system  Rotors  Vibrations  Mathematical model  Manipulators  Controllability  Torque 
Abstract: The RVM (Robot-based Vibration Suppression Modules) is proposed for the manipulation and transport of a large flexible object. Since the RVM is easily attachable/detachable to the object, this RVM allows distributing over the manipulated object so that it is scalable to the object size. The composition of the system is partly motivated by the MAGMaS (Multiple Aerial-Ground Manipulator System) [1]- [3], however, since the quadrotor usage is mechanically too complicated and its design is not optimized for manipulation, thus we overcome these limitations using distributed RVMs and newly developed theory. For this, we first provide a constrained optimization problem of RVM design with the minimum number of rotors, so that the feasible thrust force is maximized while it minimizes undesirable wrench and its own weight. Then, we derive the full dynamics and elucidate a controllability condition with multiple distributed RVMs and show that even if multiple, their structures turn out similar to [2] composed with a single quadrotor. We also elucidate the optimal placement of the RVM via the usage of controllability gramian which is not even alluded in [2] and established for the first time here. Experiments are performed to demonstrate the effectiveness of the proposed theory.


Title: Aerial Manipulation using Model Predictive Control for Opening a Hinged Door
Key Words: aerospace robotics  collision avoidance  control system synthesis  dynamic programming  manipulators  observers  position control  predictive control  robust control  three-term control  model predictive control  hinged door  environment interaction  aerial robot  multirotor-based aerial manipulator  daily-life moving structure  collision avoidance  differential dynamic programming  disturbance observer  robust controller  Manipulators  Vehicle dynamics  Mathematical model  Dynamics  Trajectory  Servomotors 
Abstract: Existing studies for environment interaction with an aerial robot have been focused on interaction with static surroundings. However, to fully explore the concept of an aerial manipulation, interaction with moving structures should also be considered. In this paper, a multirotor-based aerial manipulator opening a daily-life moving structure, a hinged door, is presented. In order to address the constrained motion of the structure and to avoid collisions during operation, model predictive control (MPC) is applied to the derived coupled system dynamics between the aerial manipulator and the door involving state constraints. By implementing a constrained version of differential dynamic programming (DDP), MPC can generate position setpoints to the disturbance observer (DOB)-based robust controller in real-time, which is validated by our experimental results.


Title: Integrated Motion Planner for Real-time Aerial Videography with a Drone in a Dense Environment
Key Words: autonomous aerial vehicles  collision avoidance  graph theory  mobile robots  object detection  quadratic programming  video recording  dense environment  drone  autonomous videography task  3-D obstacle environment  moving object  target motion prediction module  hierarchical chasing planner  covariant optimization  bi-level structure  smooth planner  graph-search method  chasing corridor  subsequent phase  smooth trajectory  dynamically feasible trajectory  integrated motion planner  real-time aerial videography  autonomous videography task  source code  quadratic programming  Drones  Trajectory  Safety  Optimization  Measurement  Shape  Real-time systems 
Abstract: This work suggests an integrated approach for a drone (or multirotor) to perform an autonomous videography task in a 3-D obstacle environment by following a moving object. The proposed system includes 1) a target motion prediction module which can be applied to dense environments and 2) a hierarchical chasing planner. Leveraging covariant optimization, the prediction module estimates the future motion of the target assuming it efforts to avoid the obstacles. The other module, chasing planner, is in a bi-level structure composed of preplanner and smooth planner. In the first phase, we exploit a graph-search method to plan a chasing corridor which incorporates safety and visibility of target. In the subsequent phase, we generate a smooth and dynamically feasible trajectory within the corridor using quadratic programming (QP). We validate our approach with multiple complex scenarios and actual experiments. The source code and the experiment video can be found in https://github.com/icsl-Jeon/traj_gen_vis and https://www.youtube.com/watch?v=_JSwXBwYRl8.


Title: FG-GMM-based Interactive Behavior Estimation for Autonomous Driving Vehicles in Ramp Merging Control *
Key Words: automobiles  Gaussian processes  graph theory  mobile robots  probability  road traffic control  road vehicles  traffic engineering computing  autonomous driving vehicles  autonomous driving cars  factor graph  human-designed models  FG-GMM-based interactive behavior estimation  ramp merging control  significant social interaction  probabilistic graphical model merging control model  Merging  Automobiles  Estimation  Autonomous vehicles  Probabilistic logic  Mathematical model  Roads 
Abstract: Interactive behavior is important for autonomous driving vehicles, especially for scenarios like ramp merging which require significant social interaction between autonomous driving vehicles and human-driven cars. This paper enhances our previous Probabilistic Graphical Model (PGM) merging control model for the interactive behavior of autonomous driving vehicles. To better estimate the interactive behavior for autonomous driving cars, a Factor Graph (FG) is used to describe the dependency among observations and estimate other cars' intentions. Real trajectories are used to approximate the model instead of human-designed models or cost functions. Forgetting factors and a Gaussian Mixture Model (GMM) are also applied in the intention estimation process for stabilization, interpolation and smoothness. The advantage of the factor graph is that the relationship between its nodes can be described by self-defined functions, instead of probabilistic relationships as in PGM, giving more flexibility. Continuity of GMM also provides higher accuracy than the previous discrete speed transition model. The proposed method enhances the overall performance of intention estimation, in terms of collision rate and average distance between cars after merging, which means it is safer and more efficient.


Title: Cooperative Perception and Localization for Cooperative Driving
Key Words: cooperative systems  Kalman filters  location based services  mobile robots  multi-robot systems  nonlinear filters  road vehicles  sensor fusion  vehicle sensors  extended Kalman filters  fully autonomous road vehicles  cooperative driving  cooperative perception  high fidelity sensors  low fidelity sensors  localization information  Sensor systems  Time measurement  Roads  Fuses  Current measurement  Bandwidth 
Abstract: Fully autonomous vehicles are expected to share the road with less advanced vehicles for a significant period of time. Furthermore, an increasing number of vehicles on the road are equipped with a variety of low-fidelity sensors which provide some perception and localization data, but not at a high enough quality for full autonomy. In this paper, we develop a perception and localization system that allows a vehicle with low-fidelity sensors to incorporate high-fidelity observations from a vehicle in front of it, allowing both vehicles to operate with full autonomy. The resulting system generates perception and localization information that is both low-noise in regions covered by high-fidelity sensors and avoids false negatives in areas only observed by low-fidelity sensors, while dealing with latency and dropout of the communication link between the two vehicles. At its core, the system uses a set of Extended Kalman filters which incorporate observations from both vehicles' sensors and extrapolate them using information about the road geometry. The perception and localization algorithms are evaluated both in simulation and on real vehicles as part of a full cooperative driving system.


Title: Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images
Key Words: collision avoidance  mobile robots  off-road vehicles  robot programming  robot vision  supervised learning  autonomous driving  vision based controllers  navigation learning  model robustmess  planning foresight  self supervised method  collision avoidance  sparse aerial images  off road driving  smooth terrain traversal  visual obstructions  on-board sensors  terrain roughness  model free reinforcement learning  unstructured outdoor environments  on-board camera  rough terrain  Predictive models  Navigation  Cameras  Planning  Computational modeling  Visualization  Robots 
Abstract: We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both first-person and overhead aerial image inputs to our model. We nd that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only first-person imagery. Video and project details can be found at www.cim.mcgill.ca/mrl/offroad_driving/.


Title: Adversarial Feature Disentanglement for Place Recognition Across Changing Appearance
Key Words: feature extraction  image matching  image sequences  mobile robots  neural nets  robot vision  supervised learning  adversarial feature disentanglement  seasonal variation  visual place recognition  image descriptors  adversarial network  image sequences  domain related features  self supervised manner  image matching  Feature extraction  Training  Image reconstruction  Image recognition  Robustness  Machine learning  Neural networks 
Abstract: When robots move autonomously for long-term, varied appearance such as the transition from day to night and seasonal variation brings challenges to visual place recognition. Defining an appearance condition (e.g. a season, a kind of weather) as a domain, we consider that the desired representation for place recognition (i) should be domain-unrelated so that images from different time can be matched regardless of varied appearance, (ii) should be learned in a self-supervised manner without the need of massive manually labeled data, and (iii) should be able to train among multiple domains in one model to keep limited model complexity. This paper sets to find domain-unrelated features across extremely changing appearance, which can be used as image descriptors to match between images collected at different conditions. We propose to use the adversarial network to disentangle domain-unrelated and domain-related features, which are named place and appearance features respectively. During training, only domain information is needed without requiring manually aligned image sequences. Experiments demonstrated that our method can disentangle place and appearance features in both toy case and images from the real world, and the place feature is qualified in place recognition tasks under different appearance conditions. The proposed network is also adaptable to multiple domains without increasing model capacity and shows favorable generalization.


Title: Ground Texture Based Localization Using Compact Binary Descriptors
Key Words: image matching  image texture  pose estimation  robot vision  ground texture based localization  compact binary descriptors  global localization  subsequent local localization updates  compact binary feature descriptors  localization success rates  self-contained method  matching strategy  identity matching  Feature extraction  Cameras  Robots  Latches  Task analysis  Asphalt  Pose estimation 
Abstract: Ground texture based localization is a promising approach to achieve high-accuracy positioning of vehicles. We present a self-contained method that can be used for global localization as well as for subsequent local localization updates, i.e. it allows a robot to localize without any knowledge of its current whereabouts, but it can also take advantage of a prior pose estimate to reduce computation time significantly. Our method is based on a novel matching strategy, which we call identity matching, that is based on compact binary feature descriptors. Identity matching treats pairs of features as matches only if their descriptors are identical. While other methods for global localization are faster to compute, our method reaches higher localization success rates, and can switch to local localization after the initial localization.


Title: Context-Aware Task Execution Using Apprenticeship Learning
Key Words: human-robot interaction  learning (artificial intelligence)  service robots  ubiquitous computing  demonstrated motion  learned policy  perceived behaviour  context-aware task execution  apprenticeship learning  assistive service robots  human-oriented tasks  task parameters  optimal behaviour  robot-to-human object hand-over  reinforcement learning  demonstrator  contextualized variants  demonstrated action  dynamic movement primitives  compact motion representations  model-based C-REPS algorithm  hand-over position  context variables  simulated task executions  evaluating emergent behaviours  context-aware action generalization  Robots  Task analysis  Trajectory  Context modeling  Encoding  Adaptation models  Learning (artificial intelligence) 
Abstract: An essential measure of autonomy in assistive service robots is adaptivity to the various contexts of human-oriented tasks, which are subject to subtle variations in task parameters that determine optimal behaviour. In this work, we propose an apprenticeship learning approach to achieving context-aware action generalization on the task of robot-to-human object hand-over. The procedure combines learning from demonstration and reinforcement learning: a robot first imitates a demonstrator's execution of the task and then learns contextualized variants of the demonstrated action through experience. We use dynamic movement primitives as compact motion representations, and a model-based C-REPS algorithm for learning policies that can specify hand-over position, conditioned on context variables. Policies are learned using simulated task executions, before transferring them to the robot and evaluating emergent behaviours. We additionally conduct a user study involving participants assuming different postures and receiving an object from a robot, which executes hand-overs by either imitating a demonstrated motion, or adapting its motion to hand-over positions suggested by the learned policy. The results confirm the hypothesized improvements in the robot's perceived behaviour when it is context-aware and adaptive, and provide useful insights that can inform future developments.


Title: Hierarchical Interest-Driven Goal Babbling for Efficient Bootstrapping of Sensorimotor skills
Key Words: adaptive control  hierarchical systems  learning (artificial intelligence)  learning systems  manipulators  neurocontrollers  radial basis function networks  stability  hierarchical interest-driven goal babbling  bootstrapping  sensorimotor skills  time-dependent changes  intrinsic motivation signal  online associative radial basis function network  associative dynamic network  parameter-sharing technique  exhaustive parameter tuning  learning process  physical robot manipulator  data-driven robot model learning  stability  Task analysis  Current measurement  Robot sensing systems  Service robots  Stability analysis  Measurement uncertainty 
Abstract: We propose a novel hierarchical online learning scheme for fast and efficient bootstrapping of sensorimotor skills. Our scheme permits rapid data-driven robot model learning in a "learning while behaving" fashion. It is updated continuously to adapt to time-dependent changes and driven by an intrinsic motivation signal. It utilizes an online associative radial basis function network, which is the first associative dynamic network to be constructed from scratch with high stability. Moreover, we propose a parameter-sharing technique to increase efficiency, stabilize the online scheme, avoid exhaustive parameter tuning, and speed up the learning process. We apply our proposed algorithms on a 7-DoF physical robot manipulator and demonstrate their performance and efficiency.


Title: Robot-Supervised Learning for Object Segmentation
Key Words: computer vision  image segmentation  learning (artificial intelligence)  manipulators  object detection  robot manipulator  human supervision  foreground segmentation technique  grasped object  state-of-the-art adaptable in-hand object segmentation  segmentation performance  robot-supervised  unstructured changing environments  deep learning  object detection  human annotators  learning-based segmentation methods  robotics applications  annotated training data  pixelwise segmentation  Robot kinematics  Manipulators  Image segmentation  Robot sensing systems  Object segmentation  Training 
Abstract: To be effective in unstructured and changing environments, robots must learn to recognize new objects. Deep learning has enabled rapid progress for object detection and segmentation in computer vision; however, this progress comes at the price of human annotators labeling many training examples. This paper addresses the problem of extending learning-based segmentation methods to robotics applications where annotated training data is not available. Our method enables pixelwise segmentation of grasped objects. We factor the problem of segmenting the object from the background into two sub-problems: (1) segmenting the robot manipulator and object from the background and (2) segmenting the object from the manipulator. We propose a kinematics-based foreground segmentation technique to solve (1). To solve (2), we train a self-recognition network that segments the robot manipulator. We train this network without human supervision, leveraging our foreground segmentation technique from (1) to label a training set of images containing the robot manipulator without a grasped object. We demonstrate experimentally that our method outperforms state-of-the-art adaptable in-hand object segmentation. We also show that a training set composed of automatically labelled images of grasped objects improves segmentation performance on a test set of images of the same objects in the environment.


Title: Gradient and Log-based Active Learning for Semantic Segmentation of Crop and Weed for Agricultural Robots
Key Words: agriculture  convolutional neural nets  entropy  image sampling  image segmentation  industrial robots  learning (artificial intelligence)  crop  weed  agricultural robots  annotated datasets  supervised learning  tedious time-intensive task  active learning  image data  existing semantic segmentation CNN  growth stage  rough foreground segmentation  substantially different field  challenging datasets  agricultural robotics domain  entropy based sampling  human labeling effort  Image segmentation  Semantics  Task analysis  Agriculture  Robots  Training  Sugar industry 
Abstract: Annotated datasets are essential for supervised learning. However, annotating large datasets is a tedious and time-intensive task. This paper addresses active learning in the context of semantic segmentation with the goal of reducing the human labeling effort. Our application is agricultural robotics and we focus on the task of distinguishing between crop and weed plants from image data. A key challenge in this application is the transfer of an existing semantic segmentation CNN to a new field, in which growth stage, weeds, soil, and weather conditions differ. We propose a novel approach that, given a trained model on one field together with rough foreground segmentation, refines the network on a substantially different field providing an effective method of selecting samples to annotate for supporting the transfer. We evaluated our approach on two challenging datasets from the agricultural robotics domain and show that we achieve a higher accuracy with a smaller number of samples compared to random sampling as well as entropy based sampling, which consequently reduces the required human labeling effort.


Title: Learning How to Walk: Warm-starting Optimal Control Solver with Memory of Motion
Key Words: humanoid robots  iterative methods  learning (artificial intelligence)  legged locomotion  motion control  optimal control  path planning  regression analysis  trajectory control  optimal control solver  locomotion task  humanoid robot  HPP Loco3D  versatile locomotion planner  whole-body trajectory  regression problem  single-step motion  multistep motion  predicted motion  Crocoddyl control solver  Trajectory  Databases  Optimal control  Task analysis  Legged locomotion  Ground penetrating radar 
Abstract: In this paper, we propose a framework to build a memory of motion for warm-starting an optimal control solver for the locomotion task of a humanoid robot. We use HPP Loco3D, a versatile locomotion planner, to generate offline a set of dynamically consistent whole-body trajectory to be stored as the memory of motion. The learning problem is formulated as a regression problem to predict a single-step motion given the desired contact locations, which is used as a building block for producing multi-step motions. The predicted motion is then used as a warm-start for the fast optimal control solver Crocoddyl. We have shown that the approach manages to reduce the required number of iterations to reach the convergence from ~9.5 to only ~3.0 iterations for the single-step motion and from ~6.2 to ~4.5 iterations for the multi-step motion, while maintaining the solution's quality.


Title: Feedback Linearization for Uncertain Systems via Reinforcement Learning
Key Words: approximation theory  continuous time systems  control system synthesis  feedback  function approximation  learning (artificial intelligence)  linearisation techniques  nonlinear control systems  optimisation  uncertain systems  model-free policy optimization techniques  feedback linearization  nonlinear control  nonlinear plant  feedback controller  linear control techniques  exact linearizing controllers  learned linearizing controller  model-free policy optimization algorithms  Feedback linearization  Conferences  Automation  Uncertain systems  Learning (artificial intelligence)  Control design  Nonlinear systems 
Abstract: We present a novel approach to control design for nonlinear systems which leverages model-free policy optimization techniques to learn a linearizing controller for a physical plant with unknown dynamics. Feedback linearization is a technique from nonlinear control which renders the input-output dynamics of a nonlinear plant linear under application of an appropriate feedback controller. Once a linearizing controller has been constructed, desired output trajectories for the nonlinear plant can be tracked using a variety of linear control techniques. However, the calculation of a linearizing controller requires a precise dynamics model for the system. As a result, model-based approaches for learning exact linearizing controllers generally require a simple, highly structured model of the system with easily identifiable parameters. In contrast, the model-free approach presented in this paper is able to approximate the linearizing controller for the plant using general function approximation architectures. Specifically, we formulate a continuous-time optimization problem over the parameters of a learned linearizing controller whose optima are the set of parameters which best linearize the plant. We derive conditions under which the learning problem is (strongly) convex and provide guarantees which ensure the true linearizing controller for the plant is recovered. We then discuss how model-free policy optimization algorithms can be used to solve a discrete-time approximation to the problem using data collected from the real-world plant. The utility of the framework is demonstrated in simulation and on a real-world robotic platform.


Title: Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and Progress Prediction
Key Words: feature extraction  gesture recognition  image segmentation  medical robotics  recurrent neural nets  robot dynamics  robot kinematics  surgery  multitask recurrent neural network  surgical gesture recognition  surgical data science  computer-aided intervention  robotic kinematic information  robot kinematic data  Task analysis  Training  Kinematics  Estimation  Needles  Gesture recognition  Surgery 
Abstract: Surgical gesture recognition is important for surgical data science and computer-aided intervention. Even with robotic kinematic information, automatically segmenting surgical steps presents numerous challenges because surgical demonstrations are characterized by high variability in style, duration and order of actions. In order to extract discriminative features from the kinematic signals and boost recognition accuracy, we propose a multi-task recurrent neural network for simultaneous recognition of surgical gestures and estimation of a novel formulation of surgical task progress. To show the effectiveness of the presented approach, we evaluate its application on the JIGSAWS dataset, that is currently the only publicly available dataset for surgical gesture recognition featuring robot kinematic data. We demonstrate that recognition performance improves in multi-task frameworks with progress estimation without any additional manual labelling and training.


Title: Neural Network based Inverse Dynamics Identification and External Force Estimation on the da Vinci Research Kit
Key Words: force control  force sensors  mean square error methods  medical robotics  neurocontrollers  robot dynamics  surgical robotic systems  internal joint torques  robot inverse dynamics  da Vinci surgical robot  environment forces  model-based approaches  external force sensor  external force estimation  da Vinci research kit  neural network based inverse dynamics identification  normalized rootmean-square error  NRMSE  tool/tissue interaction forces  Robots  Dynamics  Force  Estimation  Training  Biological neural networks 
Abstract: Most current surgical robotic systems lack the ability to sense tool/tissue interaction forces, which motivates research in methods to estimate these forces from other available measurements, primarily joint torques. These methods require the internal joint torques, due to the robot inverse dynamics, to be subtracted from the measured joint torques. This paper presents the use of neural networks to estimate the inverse dynamics of the da Vinci surgical robot, which enables estimation of the external environment forces. Experiments with motions in free space demonstrate that the neural networks can estimate the internal joint torques within 10% normalized rootmean-square error (NRMSE), which outperforms model-based approaches in the literature. Comparison with an external force sensor shows that the method is able to estimate environment forces within about 10% NRMSE.


Title: Reliable Trajectories for Dynamic Quadrupeds using Analytical Costs and Learned Initializations
Key Words: learning (artificial intelligence)  legged locomotion  motion control  navigation  nonlinear programming  path planning  predictive control  reliability  robot dynamics  robust control  trajectory control  dynamic quadrupeds  dynamic traversal  legged robotics  robust dynamic motion  uneven terrain navigation  TOWR  learning based scheme  whole body tracking controller  trajectory optimization for walking robots  model predictive control  dynamic trajectory reliability  nonlinear program  dynamic motions  Legged locomotion  Dynamics  Foot  Trajectory optimization 
Abstract: Dynamic traversal of uneven terrain is a major objective in the field of legged robotics. The most recent model predictive control approaches for these systems can generate robust dynamic motion of short duration; however, planning over a longer time horizon may be necessary when navigating complex terrain. A recently-developed framework, Trajectory Optimization for Walking Robots (TOWR), computes such plans but does not guarantee their reliability on real platforms, under uncertainty and perturbations. We extend TOWR with analytical costs to generate trajectories that a state-of-the-art whole-body tracking controller can successfully execute. To reduce online computation time, we implement a learning-based scheme for initialization of the nonlinear program based on offline experience. The execution of trajectories as long as 16 footsteps and 5.5 s over different terrains by a real quadruped demonstrates the effectiveness of the approach on hardware. This work builds toward an online system which can efficiently and robustly replan dynamic trajectories.


Title: On the Hardware Feasibility of Nonlinear Trajectory Optimization for Legged Locomotion based on a Simplified Dynamics
Key Words: hydraulic actuators  legged locomotion  motion control  path planning  position control  robot dynamics  HyQ robot  Hydraulically actuated Quadruped robot  simplified nonlinear nonconvex trajectory optimization  single rigid body dynamics-based trajectory optimizer  leg collision  leg model  joint positions  admissible contact forces  joint-torque limits  challenging terrain  robust motions  feasibility constraints  motion planning  computational efficiency  simplified dynamics  legged locomotion  nonlinear trajectory optimization  hardware feasibility  Legged locomotion  Foot  Collision avoidance  Force  Trajectory  Aerodynamics 
Abstract: Simplified models are useful to increase the computational efficiency of a motion planning algorithm, but their lack of accuracy have to be managed. We propose two feasibility constraints to be included in a Single Rigid Body Dynamics-based trajectory optimizer in order to obtain robust motions in challenging terrain. The first one finds an approximate relationship between joint-torque limits and admissible contact forces, without requiring the joint positions. The second one proposes a leg model to prevent leg collision with the environment. Such constraints have been included in a simplified nonlinear non-convex trajectory optimization problem. We demonstrate the feasibility of the resulting motion plans both in simulation and on the Hydraulically actuated Quadruped (HyQ) robot, considering experiments on an irregular terrain.


Title: Agile Legged-Wheeled Reconfigurable Navigation Planner Applied on the CENTAURO Robot
Key Words: legged locomotion  motion control  navigation  path planning  search problems  CENTAURO robot  agile legged wheeled reconfigurable navigation planner  hybrid legged-wheeled robots  Theta* based planner  trapezium-like search  Robot kinematics  Mobile robots  Planning  Wheels  Navigation  Collision avoidance 
Abstract: Hybrid legged-wheeled robots such as the CEN-TAURO, are capable of varying their footprint polygon to carry out various agile motions. This property can be advantageous for wheeled-only planning in cluttered spaces, which is our focus. In this paper, we present an improved algorithm that builds upon our previously introduced preliminary footprint varying A* planner, which was based on the rectangular symmetry of the foot support polygon. In particular, we introduce a Theta* based planner with trapezium-like search, which aims to further reduce the limitations imposed upon the wheeled-only navigation of the CENTAURO robot by the low-dimensional search space, maintaining the real-time computational efficiency. The method is tested on the simulated and real full-size CENTAURO robot in cluttered environments.


Title: Bounded haptic teleoperation of a quadruped robot’s foot posture for sensing and manipulation
Key Words: control engineering computing  force feedback  haptic interfaces  legged locomotion  mechanical engineering computing  motion control  position control  quadratic programming  robot dynamics  telerobotics  quadruped robot ANYmal  force feedback  bounded haptic teleoperation  control framework  operator-guided haptic exploration  torso  foot posture control  whole-body controller  analytical Cartesian impedance controllers  null space projector  contact forces  force-feedback  7D haptic joystick  Impedance  Aerospace electronics  Robot kinematics  Haptic interfaces  Torso  Force 
Abstract: This paper presents a control framework to teleoperate a quadruped robot's foot for operator-guided haptic exploration of the environment. Since one leg of a quadruped robot typically only has 3 actuated degrees of freedom (DoFs), the torso is employed to assist foot posture control via a hierarchical whole-body controller. The foot and torso postures are controlled by two analytical Cartesian impedance controllers cascaded by a null space projector. The contact forces acting on supporting feet are optimized by quadratic programming (QP). The foot's Cartesian impedance controller may also estimate contact forces from trajectory tracking errors, and relay the force-feedback to the operator. A 7D haptic joystick, Sigma.7, transmits motion commands to the quadruped robot ANYmal, and renders the force feedback. Furthermore, the joystick's motion is bounded by mapping the foot's feasible force polytope constrained by the friction cones and torque limits in order to prevent the operator from driving the robot to slipping or falling over. Experimental results demonstrate the efficiency of the proposed framework.


Title: Pinbot: A Walking Robot with Locking Pin Arrays for Passive Adaptability to Rough Terrains
Key Words: design engineering  legged locomotion  motion control  robot dynamics  legged robots  passive adaptability  locking pin arrays  pin array mechanisms  walking robot  legged robot design  rough terrain locomotion  unstructured terrains  rough terrains  stable locomotion  Conferences  Automation 
Abstract: To date, many control strategies for legged robots have been proposed for stable locomotion over rough and unstructured terrains. However, these approaches require sensing information throughout locomotion, which may be noisy or unavailable at times. An alternative solution to rough terrain locomotion is a legged robot design that can passively adapt to the variations in the terrain without requiring knowledge of them. This paper presents one such solution in the design of a walking robot that employs pin array mechanisms to passively adapt to rough terrains. The pins are passively dropped over the terrain to conform to its variations and then locked to provide a statically stable stance. Locomotion is achieved with parallel four-bar linkages that swing forward the platforms in an alternating manner. Experimental evaluation of the robot demonstrates that the pin arrays enable legged locomotion over rough terrains under open-loop control.


Title: Planning for the Unexpected: Explicitly Optimizing Motions for Ground Uncertainty in Running
Key Words: legged locomotion  motion control  optimisation  robot dynamics  ground uncertainty  actuation plans  dynamic model  bipedal running  fixed body trajectory  passive dynamics  reduced order model  emergent robustness  legged robots  legged locomotion  linked inputs  input linking  hybrid dynamics  running model  optimization procedure  standard trajectory optimization  robust gaits  Legged locomotion  Springs  Dynamics  Foot  Robustness  Acceleration 
Abstract: We propose a method to generate actuation plans for a reduced order, dynamic model of bipedal running. This method explicitly enforces robustness to ground uncertainty. The plan generated is not a fixed body trajectory that is aggressively stabilized: instead, the plan interacts with the passive dynamics of the reduced order model to create emergent robustness. The goal is to create plans for legged robots that will be robust to imperfect perception of the environment, and to work with dynamics that are too complex to optimize in real-time. Working within this dynamic model of legged locomotion, we optimize a set of disturbance cases together with the nominal case, all with linked inputs. The input linking is nontrivial due to the hybrid dynamics of the running model but our solution is effective and has analytical gradients. The optimization procedure proposed is significantly slower than a standard trajectory optimization, but results in robust gaits that reject disturbances extremely effectively without any replanning required.


Title: One-Shot Multi-Path Planning for Robotic Applications Using Fully Convolutional Networks
Key Words: convolutional neural nets  iterative methods  mobile robots  neurocontrollers  path planning  robot action execution  motion trajectory  iterative methods  fully convolutional neural network  network prediction iteration  optimal paths  single path predictions  simultaneously generated paths  shot multipath planning  robotic applications  Two dimensional displays  Three-dimensional displays  Training  Robots  Path planning  Prediction algorithms  Planning 
Abstract: Path planning is important for robot action execution, since a path or a motion trajectory for a particular action has to be defined first before the action can be executed. Most of the current approaches are iterative methods where the trajectory is generated by predicting the next state based on the current state. Here we propose a novel method by utilising a fully convolutional neural network, which allows generation of complete paths even for several agents with one network prediction iteration. We demonstrate that our method is able to successfully generate optimal or close to optimal paths (less than 10% longer) in more than 99% of the cases for single path predictions in 2D and 3D environments. Furthermore, we show that the network is - without specific training on such cases - able to create (close to) optimal paths in 96% of the cases for two and in 84% of the cases for three simultaneously generated paths.


Title: Efficient Iterative Linear-Quadratic Approximations for Nonlinear Multi-Player General-Sum Differential Games
Key Words: approximation theory  decision making  differential games  iterative methods  linear quadratic control  multi-agent systems  multi-robot systems  nonlinear control systems  iterative linear-quadratic regulator  linear dynamics  quadratic costs  linear-quadratic games  complex interactive behavior  efficient iterative linear-quadratic approximations  nonlinear multiplayer general-sum differential games  robotics  multiple decision making agents  expressive theoretical framework  multiagent problems  numerical solution techniques  state dimension  single agent optimal control problem  ILQR  repeated approximations  three-player 14-state simulated intersection problem  hardware collision-avoidance test  time 0.25 s  time 50.0 ms  Games  Heuristic algorithms  Approximation algorithms  Optimal control  Iterative methods  Trajectory  Automobiles 
Abstract: Many problems in robotics involve multiple decision making agents. To operate efficiently in such settings, a robot must reason about the impact of its decisions on the behavior of other agents. Differential games offer an expressive theoretical framework for formulating these types of multi-agent problems. Unfortunately, most numerical solution techniques scale poorly with state dimension and are rarely used in real-time applications. For this reason, it is common to predict the future decisions of other agents and solve the resulting decoupled, i.e., single-agent, optimal control problem. This decoupling neglects the underlying interactive nature of the problem; however, efficient solution techniques do exist for broad classes of optimal control problems. We take inspiration from one such technique, the iterative linear-quadratic regulator (ILQR), which solves repeated approximations with linear dynamics and quadratic costs. Similarly, our proposed algorithm solves repeated linear-quadratic games. We experimentally benchmark our algorithm in several examples with a variety of initial conditions and show that the resulting strategies exhibit complex interactive behavior. Our results indicate that our algorithm converges reliably and runs in real-time. In a three-player, 14-state simulated intersection problem, our algorithm initially converges in <; 0.25 s. Receding horizon invocations converge in <; 50 ms in a hardware collision-avoidance test.


Title: Path-Following Model Predictive Control of Ballbots
Key Words: mobile robots  predictive control  robot dynamics  model predictive control  path-following tasks  dynamically unstable mobile robots  single ball  simplied version  physical ballbot system  high fidelity model  online implementation  quaternion-based model  Robots  Solid modeling  Friction  Quaternions  Planning  Acceleration  Trajectory 
Abstract: This paper introduces a novel approach for model predictive control of ballbots for path-following tasks. Ballbots are dynamically unstable mobile robots which are designed to balance on a single ball. The model presented in this paper is a simplied version of a full quaternion-based model of ballbots' underactuated dynamics which is suited for online implementation. Furthermore, the approach is extended to handle nearby obstacles directly in the MPC formulation. The presented controller is validated through simulation on a high fidelity model as well as through real-world experiments on a physical ballbot system.


Title: Underactuated Waypoint Trajectory Optimization for Light Painting Photography
Key Words: control system synthesis  nonlinear control systems  optimisation  pendulums  photography  trajectory control  underactuated waypoint trajectory optimization  light painting photography  control engineering  auxiliary optimization variables  waypoint activations  letter drawing task  long exposure photography  r control engineering  Linear programming  Task analysis  Trajectory optimization  Painting  Photography 
Abstract: Despite their abundance in robotics and nature, underactuated systems remain a challenge for control engineering. Trajectory optimization provides a generally applicable solution, however its efficiency strongly depends on the skill of the engineer to frame the problem in an optimizer-friendly way. This paper proposes a procedure that automates such problem reformulation for a class of tasks in which the desired trajectory is specified by a sequence of waypoints. The approach is based on introducing auxiliary optimization variables that represent waypoint activations. To validate the proposed method, a letter drawing task is set up where shapes traced by the tip of a rotary inverted pendulum are visualized using long exposure photography.


Title: Whole-Body Walking Generation using Contact Parametrization: A Non-Linear Trajectory Optimization Approach
Key Words: humanoid robots  legged locomotion  optimal control  optimisation  robot dynamics  trajectory control  humanoid robot model  walking surface  contact parametrization  complementarity-free  predefined contact sequence  optimal control  walking trajectories  dynamic equations  optimization problem  direct multiple shooting approach  body walking generation  nonlinear trajectory optimization  centroidal dynamics  humanoid robot kinematics  humanoid robot dynamics  Legged locomotion  Foot  Force  Trajectory  Robot kinematics  Mathematical model 
Abstract: In this paper, we describe a planner capable of generating walking trajectories by using the centroidal dynamics and the full kinematics of a humanoid robot model. The interaction between the robot and the walking surface is modeled explicitly through a novel contact parametrization. The approach is complementarity-free and does not need a predefined contact sequence. By solving an optimal control problem we obtain walking trajectories. In particular, through a set of constraints and dynamic equations, we model the robot in contact with the ground. We describe the objective the robot needs to achieve with a set of tasks. The whole optimal control problem is transcribed into an optimization problem via a Direct Multiple Shooting approach and solved with an off-the-shelf solver. We show that it is possible to achieve walking motions automatically by specifying a minimal set of references, such as a constant desired Center of Mass velocity and a reference point on the ground.


Title: Controlling Fast Height Variation of an Actively Articulated Wheeled Humanoid Robot Using Center of Mass Trajectory
Key Words: adaptive control  humanoid robots  legged locomotion  motion control  nonlinear dynamical systems  optimal control  path planning  quadratic programming  robot dynamics  robot kinematics  robust control  splines (mathematics)  stability criteria  trajectory control  task-space inverse dynamics controller  optimal 7th order spline coefficients  dynamic stability  TSID controller  simplified passive dynamics model  Aerobot platform  fast height adaptation  actively articulated wheeled humanoid robot  center of mass trajectory  hybrid wheel-legged robots  complex terrain  purely wheeled morphologies  highly adaptive behaviours  nonlinear dynamics control problem  hybrid humanoid platform  offline trajectory optimisation  optimal center of mass kinematic trajectories  fast height variation control  nonlinear zero moment point  optimal control  sequential quadratic programming  robot kinematics  stability criterion  motion plan  task Jacobians  robust control  Aerodynamics  Mathematical model  Acceleration  Robot kinematics  Stability analysis 
Abstract: Hybrid wheel-legged robots have begun to demonstrate the ability to adapt to complex terrain traditionally inaccessible to purely wheeled morphologies. Further research is needed into how their dynamics can be optimally controlled for developing highly adaptive behaviours on challenging terrain. Using optimal center of mass (COM) kinematic trajectories, this work examines the nonlinear dynamics control problem for fast height adaptation on the hybrid humanoid platform known as Aerobot. We explore the dynamics control problem through experimentation with an offline trajectory optimisation (TO) method and a task-space inverse dynamics (TSID) controller for varying the robot's height. Our TO approach uses sequential quadratic programming (SQP) to solve optimal 7th order spline coefficients for the robot's kinematics. The nonlinear Zero Moment Point (ZMP) is used to model a stability criterion that is constrained in the TO problem to ensure dynamic stability. Our TSID controller follows motion plans based on using task jacobians and a simplified passive dynamics model of the Aerobot platform. Results exhibit fast height adaptation on the Aerobot platform with significantly differing results between the control methods that prompts new research into how it may be controlled online.


Title: Contact-Aware Controller Design for Complementarity Systems
Key Words: control system synthesis  mobile robots  motion control  multi-robot systems  optimisation  robot dynamics  robust control  tactile sensors  multicontact motion  combinatoric structure  real-time control  tactile sensors  robust control  complementarity structure  contact dynamics  control framework  multicontact robotics problems  contact-aware controller design  robotic tasks  locomotion  Lyapunov methods  Control systems  Force  Dynamics  Task analysis  Tactile sensors 
Abstract: While many robotic tasks, like manipulation and locomotion, are fundamentally based in making and breaking contact with the environment, state-of-the-art control policies struggle to deal with the hybrid nature of multi-contact motion. Such controllers often rely heavily upon heuristics or, due to the combinatoric structure in the dynamics, are unsuitable for real-time control. Principled deployment of tactile sensors offers a promising mechanism for stable and robust control, but modern approaches often use this data in an ad hoc manner, for instance to guide guarded moves. In this work, by exploiting the complementarity structure of contact dynamics, we propose a control framework which can close the loop on rich, tactile sensors. Critically, this framework is non-combinatoric, enabling optimization algorithms to automatically synthesize provably stable control policies. We demonstrate this approach on three different underactuated, multi-contact robotics problems.


Title: Learning to Generate 6-DoF Grasp Poses with Reachability Awareness
Key Words: convolutional neural nets  learning systems  manipulators  neurocontrollers  stability  3D CNN  6-DoF grasp poses  reachability awareness  voxel-based deep 3D convolutional neural network  reachability predictor  robot  grasp pose stability  Grasping  Three-dimensional displays  Robot kinematics  Planning  Measurement  Data models  Grasping  Deep Learning in Robotics and Automation  Perception for Grasping and Manipulation 
Abstract: Motivated by the stringent requirements of unstructured real-world where a plethora of unknown objects reside in arbitrary locations of the surface, we propose a voxel-based deep 3D Convolutional Neural Network (3D CNN) that generates feasible 6-DoF grasp poses in unrestricted workspace with reachability awareness. Unlike the majority of works that predict if a proposed grasp pose within the restricted workspace will be successful solely based on grasp pose stability, our approach further learns a reachability predictor that evaluates if the grasp pose is reachable or not from robot's own experience. To avoid the laborious real training data collection, we exploit the power of simulation to train our networks on a large-scale synthetic dataset. This work is an early attempt that simultaneously learns grasping reachability while proposing feasible grasp poses with 3D CNN. Experimental results in both simulation and real-world demonstrate that our approach outperforms several other methods and achieves 82.5% grasping success rate on unknown objects.


Title: Enhancing Grasp Pose Computation in Gripper Workspace Spheres
Key Words: computational geometry  dexterous manipulators  grippers  path planning  pose estimation  position control  robot vision  grasp pose computation  gripper workspace spheres  registered point cloud  gripper position sampling  orientation sampling  object orientation estimation  jaw gripper  Franka Panda gripper  geometric based methods  multifingered hands  Intel RealSense-D435 depth camera  Grippers  Three-dimensional displays  Ellipsoids  Grasping  Measurement  Shape  Planning  grasping  manipulation 
Abstract: In this paper, enhancement to the novel grasp planning algorithm based on gripper workspace spheres is presented. Our development requires a registered point cloud of the target from different views, assuming no prior knowledge of the object, nor any of its properties. This work features a new set of metrics for grasp pose candidates evaluation, as well as exploring the impact of high object sampling on grasp success rates. In addition to gripper position sampling, we now perform orientation sampling about the x, y, and z-axes, hence the grasping algorithm no longer require object orientation estimation. Successful experiments have been conducted on a simple jaw gripper (Franka Panda gripper) as well as a complex, high Degree of Freedom (DoF) hand (Allegro hand) as a proof of its versatility. Higher grasp success rates of 76% and 85.5% respectively has been reported by real world experiments.


Title: Minimal Work: A Grasp Quality Metric for Deformable Hollow Objects
Key Words: deformation  elasticity  grippers  linear programming  Robotiq gripper  UR5 robot  object empirical stiffness  linear program  manipulation task  wrench resistance  physical grasps  work quality metric  wrench-based quality metrics  real-world grasps  gripper jaw displacements  grasp force  external wrench  object deformation  robot grasping  deformable hollow objects  grasp quality metric  Measurement  Force  Task analysis  Strain  Computational modeling  Friction  Grippers 
Abstract: Robot grasping of deformable hollow objects such as plastic bottles and cups is challenging, as the grasp should resist disturbances while minimally deforming the object so as not to damage it or dislodge liquids. We propose minimal work as a novel grasp quality metric that combines wrench resistance and object deformation. We introduce an efficient algorithm to compute the work required to resist an external wrench for a manipulation task by solving a linear program. The algorithm first computes the minimum required grasp force and an estimation of the gripper jaw displacements based on the object's empirical stiffness at different locations. The work done by the jaws is the product of the grasp force and the displacements. Grasps requiring minimal work are considered to be of high quality. We collect 460 physical grasps with a UR5 robot and a Robotiq gripper. We consider a grasp to be successful if it completes the task without damaging the object or dislodging the content. Physical experiments suggest that the minimal work quality metric reaches 74.2% balanced accuracy, a metric that is the raw accuracy normalized by the number of successful and failed real-world grasps, and is up to 24.2% higher than classical wrench-based quality metrics.


Title: Hierarchical 6-DoF Grasping with Approaching Direction Selection
Key Words: convolutional neural nets  entropy  geometry  grippers  hierarchical systems  iterative methods  learning systems  neurocontrollers  optimisation  position control  cluttered objects  cross entropy method  iterative direction optimization  derivative-free optimization  geometry-based prior  point clouds  input grasp representations  robot arm  detection problem  hierarchical approach  robot grasping  hierarchical 6-DoF grasping  surface normal directions  approaching direction selection method  grasp quality  fully convolutional grasp quality network  Grasping  Three-dimensional displays  Grippers  Service robots  Manipulators  Geometry 
Abstract: In this paper, we tackle the problem of 6-DoF grasp detection which is crucial for robot grasping in cluttered real-world scenes. Unlike existing approaches which synthesize 6-DoF grasp data sets and train grasp quality networks with input grasp representations based on point clouds, we rather take a novel hierarchical approach which does not use any 6-DoF grasp data. We cast the 6-DoF grasp detection problem as a robot arm approaching direction selection problem using the existing 4-DoF grasp detection algorithm, by exploiting a fully convolutional grasp quality network for evaluating the quality of an approaching direction. To select the best approaching direction with the highest grasp quality, we propose an approaching direction selection method which leverages a geometry-based prior and a derivative-free optimization method. Specifically, we optimize the direction iteratively using the cross entropy method with initial samples of surface normal directions. Our algorithm efficiently finds diverse 6-DoF grasps by the novel way of evaluating and optimizing approaching directions. We validate that the proposed method outperforms other selection methods in scenarios with cluttered objects in a physics-based simulator. Finally, we show that our method outperforms the state-of-the-art grasp detection method in real-world experiments with robots.


Title: Geometric Characterization of Two-Finger Basket Grasps of 2-D Objects: Contact Space Formulation
Key Words: dexterous manipulators  grippers  contact space formulation  two-finger basket grasps  high-dimensional configuration space  low-dimensional contact space  two-finger contacts  object boundary  critical finger opening  two-finger robot hand  geometric techniques  depth and drop-off finger opening  Robots  Gravity  Search problems  Security  Layout  Safety  Air pollution 
Abstract: This paper considers basket grasps, where a two-finger robot hand forms a basket that can safely lift and carry rigid objects in a 2-D gravitational environment. The two-finger basket grasps form special points in a high-dimensional configuration space of the object and two-finger robot hand. This paper establishes that all two-finger basket grasps can be found in a low-dimensional contact space that parametrizes the two-finger contacts along the supported object boundary. Using contact space, each basket grasp is associated with its depth that provides a security measure while carrying the object, as well as its safety margin away from a critical finger opening where the object drops-off into its intended destination. Geometric techniques that compute the depth and drop-off finger opening are described and illustrated with detailed graphical and numerical examples.


Title: BatVision: Learning to See 3D Spatial Layout with Two Ears
Key Words: acoustic signal processing  audio signal processing  bioacoustics  cameras  ear  image colour analysis  image sensors  mechanoception  mobile robots  object detection  path planning  robot vision  stereo image processing  visual perception  machine vision  3D spatial layout  ears  nonvisual perception  artificial systems  ultrasound complement camera-based vision  information gain  harness sound  machine perception  low- cost BatVision system  short chirps  artificial human pinnae pair  stereo camera  color images  scene depths  trained BatVision  2D visual scenes  vision system  robot navigation  Microphones  Visualization  Ear  Chirp  Cameras  Three-dimensional displays  Training 
Abstract: Many species have evolved advanced non-visual perception while artificial systems fall behind. Radar and ultrasound complement camera-based vision but they are often too costly and complex to set up for very limited information gain. In nature, sound is used effectively by bats, dolphins, whales, and humans for navigation and communication. However, it is unclear how to best harness sound for machine perception.Inspired by bats' echolocation mechanism, we design a low- cost BatVision system that is capable of seeing the 3D spatial layout of space ahead by just listening with two ears. Our system emits short chirps from a speaker and records returning echoes through microphones in an artificial human pinnae pair. During training, we additionally use a stereo camera to capture color images for calculating scene depths. We train a model to predict depth maps and even grayscale images from the sound alone. During testing, our trained BatVision provides surprisingly good predictions of 2D visual scenes from two 1D audio signals. Such a sound to vision system would benefit robot navigation and machine vision, especially in low-light or no-light conditions. Our code and data are publicly available.


Title: Self-Supervised Learning for Alignment of Objects and Sound
Key Words: human-robot interaction  learning (artificial intelligence)  object detection  source separation  human-robot interaction  scene understanding  sound source separation task  self-supervised learning framework  object detection  sound separation modules  sound components  visual information  audio information  Visualization  Videos  Feature extraction  Object detection  Spectrogram  Training  Robots 
Abstract: The sound source separation problem has many useful applications in the field of robotics, such as human-robot interaction, scene understanding, etc. However, it remains a very challenging problem. In this paper, we utilize both visual and audio information of videos to perform the sound source separation task. A self-supervised learning framework is proposed to implement the object detection and sound separation modules simultaneously. Such an approach is designed to better find the alignment between the detected objects and separated sound components. Our experiments, conducted on both the synthetic and real datasets, validate this approach and demonstrate the effectiveness of the proposed model in the task of object and sound alignment.


Title: The OmniScape Dataset
Key Words: cameras  image segmentation  motorcycles  object detection  stereo image processing  traffic engineering computing  omnidirectional images  semantic segmentation  depth map  ground truth images  CARLA Simulator  open-source simulator  catadioptric images  OmniScape dataset  autonomous driving research  Grand Theft Auto V  two-wheeled vehicles  motorcycle  Cameras  Semantics  Vehicle dynamics  Motorcycles  Image segmentation  Virtual environments  Roads 
Abstract: Despite the utility and benefits of omnidirectional images in robotics and automotive applications, there are no datasets of omnidirectional images available with semantic segmentation, depth map, and dynamic properties. This is due to the time cost and human effort required to annotate ground truth images. This paper presents a framework for generating omnidirectional images using images that are acquired from a virtual environment. For this purpose, we demonstrate the relevance of the proposed framework on two well-known simulators: CARLA Simulator, which is an open-source simulator for autonomous driving research, and Grand Theft Auto V (GTA V), which is a very high quality video game. We explain in details the generated OmniScape dataset, which includes stereo fisheye and catadioptric images acquired from the two front sides of a motorcycle, including semantic segmentation, depth map, intrinsic parameters of the cameras and the dynamic parameters of the motorcycle. It is worth noting that the case of two-wheeled vehicles is more challenging than cars due to the specific dynamic of these vehicles.


Title: An ERT-based Robotic Skin with Sparsely Distributed Electrodes: Structure, Fabrication, and DNN-based Signal Processing
Key Words: biomedical electrodes  carbon nanotubes  manipulators  neural nets  tactile sensors  tomography  cylindrical surface  sensor output images  3D-shaped sensors  ERT-based robotic skin  sparsely distributed electrodes  DNN-based signal processing  electrical resistance tomography  large-scale tactile sensor  conductivity distribution  physical model  curved surface  electrode configuration  edge region  sensor performance  carbon nanotube-dispersed solution  conductive sensing domain  Robot sensing systems  Electrodes  Conductivity  Image reconstruction  Inverse problems 
Abstract: Electrical resistance tomography (ERT) has previously been utilized to develop a large-scale tactile sensor because this approach enables the estimation of the conductivity distribution among the electrodes based on a known physical model. Such a sensor made with a stretchable material can conform to a curved surface. However, this sensor cannot fully cover a cylindrical surface because in such a configuration, the edges of the sensor must meet each other. The electrode configuration becomes irregular in this edge region, which may degrade the sensor performance. In this paper, we introduce an ERT-based robotic skin with evenly and sparsely distributed electrodes. For implementation, we sprayed a carbon nanotube (CNT)-dispersed solution to form a conductive sensing domain on a cylindrical surface. The electrodes were firmly embedded in the surface so that the wires were not exposed to the outside. The sensor output images were estimated using a deep neural network (DNN), which was trained with noisy simulation data. An indentation experiment revealed that the localization error of the sensor was 5.2 ± 3.3 mm, which is remarkable performance with only 30 electrodes. A frame rate of up to 120 Hz could be achieved with a sensing domain area of 90 cm2. The proposed approach simplifies the fabrication of 3D-shaped sensors, allowing them to be easily applied to existing robot arms in a seamless and robust manner.


Title: FBG-Based Triaxial Force Sensor Integrated with an Eccentrically Configured Imaging Probe for Endoluminal Optical Biopsy
Key Words: biological tissues  biomedical optical imaging  Bragg gratings  fibre optic sensors  force measurement  force sensors  medical image processing  support vector machines  temperature sensors  FBG-based triaxial force sensor integrated  eccentrically configured imaging probe  endoluminal optical biopsy  endoluminal intervention  lesion  robotic bronchoscopy  FBG sensors  conical substrate  eccentric inner lumen  flexible imaging probe  laser-profiled continuum robot  temperature sensors  sensor substrate  developed triaxial force sensor  Robot sensing systems  Temperature sensors  Force  Force sensors  Optical fibers 
Abstract: Accurate force sensing is important for endoluminal intervention in terms of both safety and lesion targeting. This paper develops an FBG-based force sensor for robotic bronchoscopy by configuring three FBG sensors at the lateral side of a conical substrate. It allows a large and eccentric inner lumen for the interventional instrument, enabling a flexible imaging probe inside to perform optical biopsy. The force sensor is embodied with a laser-profiled continuum robot and thermo drift is fully compensated by three temperature sensors integrated on the circumference surface of the sensor substrate. Different decoupling approaches are investigated, and nonlinear decoupling is adopted based on the cross-validation SVM and a Gaussian kernel function, achieving an accuracy of 10.58 mN, 14.57 mN and 26.32 mN along X, Y and Z axis, respectively. The tissue test is also investigated to further demonstrate the feasibility of the developed triaxial force sensor.


Title: Calibrating a Soft ERT-Based Tactile Sensor with a Multiphysics Model and Sim-to-real Transfer Learning
Key Words: calibration  inverse problems  learning (artificial intelligence)  neural nets  robots  tactile sensors  tomography  soft ERT-based tactile sensor  sim-to-real transfer learning  electrical resistance tomography  finite element multiphysics model  contact pressure distributions  voltage measurements  model parameters  single-point dataset  contact force  calibration method  ERT-based tactile sensors  Fabrics  Computational modeling  Tactile sensors  Mathematical model  Electrodes  Force  Conductivity 
Abstract: Tactile sensors based on electrical resistance tomography (ERT) have shown many advantages for implementing a soft and scalable whole-body robotic skin; however, calibration is challenging because pressure reconstruction is an ill-posed inverse problem. This paper introduces a method for calibrating soft ERT-based tactile sensors using sim-to-real transfer learning with a finite element multiphysics model. The model is composed of three simple models that together map contact pressure distributions to voltage measurements. We optimized the model parameters to reduce the gap between the simulation and reality. As a preliminary study, we discretized the sensing points into a 6 by 6 grid and synthesized single- and two-point contact datasets from the multiphysics model. We obtained another single-point dataset using the real sensor with the same contact location and force used in the simulation. Our new deep neural network architecture uses a de-noising network to capture the simulation-to-real gap and a reconstruction network to estimate contact force from voltage measurements. The proposed approach showed 82% hit rate for localization and 0.51 N of force estimation error performance in singlecontact tests and 78.5% hit rate for localization and 5.0 N of force estimation error in two-point contact tests. We believe this new calibration method has the possibility to improve the sensing performance of ERT-based tactile sensors.


Title: Sim-to-Real Transfer for Optical Tactile Sensing
Key Words: cameras  learning (artificial intelligence)  mobile robots  neural nets  tactile sensors  sim-to-real transfer methods  TacTip optical tactile sensor  deformable tip  soft body simulation  Unity physics engine  domain randomisation techniques  real-world data  optical tactile sensing  deep learning  reinforcement learning methods  flexible robot controllers  complex robot controllers  training data  data collection  size 1.0 mm  Robot sensing systems  Pins  Force  Strain  Data models 
Abstract: Deep learning and reinforcement learning methods have been shown to enable learning of flexible and complex robot controllers. However, the reliance on large amounts of training data often requires data collection to be carried out in simulation, with a number of sim-to-real transfer methods being developed in recent years. In this paper, we study these techniques for tactile sensing using the TacTip optical tactile sensor, which consists of a deformable tip with a camera observing the positions of pins inside this tip. We designed a model for soft body simulation which was implemented using the Unity physics engine, and trained a neural network to predict the locations and angles of edges when in contact with the sensor. Using domain randomisation techniques for sim-to-real transfer, we show how this framework can be used to accurately predict edges with less than 1 mm prediction error in real-world testing, without any real-world data at all.


Title: Semi-Empirical Simulation of Learned Force Response Models for Heterogeneous Elastic Objects
Key Words: elastic deformation  image representation  robot vision  elastically deformable objects  data-driven models  point-based surface representation  inhomogeneous force response model  nonlinear force response model  robotic arm  arbitrary rigid object  Hertzian contact model  heterogeneous elastic objects  semiempirical method  point stiffness models  Force  Probes  Deformable models  Data models  Robot sensing systems  Strain 
Abstract: This paper presents a semi-empirical method for simulating contact with elastically deformable objects whose force response is learned using entirely data-driven models. A point-based surface representation and an inhomogeneous, nonlinear force response model are learned from a robotic arm acquiring force-displacement curves from a small number of poking interactions. The simulator then estimates displacement and force response when the deformable object is in contact with an arbitrary rigid object. It does so by estimating displacements by solving a Hertzian contact model, and sums the expected forces at individual surface points through querying the learned point stiffness models as a function of their expected displacements. Experiments on a variety of challenging objects show that our approach learns force response with sufficient accuracy to generate plausible contact response for novel rigid objects.


Title: Low-Cost Fiducial-based 6-Axis Force-Torque Sensor
Key Words: force sensors  strain gauges  6-axis force-torque sensor  six-axis force-torque sensors  hard-touse  fiducial-based design  inexpensive webcam  consumer-grade 3D printer  open-source software  applied force-torque  browser-based interface  open source design files  human-computer interfaces  six-axis force-torque sensing  traditional strain-gauge based sensors  open-source sensor design  Robot sensing systems  Force  Three-dimensional displays  Webcams  Sensitivity  Prototypes 
Abstract: Commercial six-axis force-torque sensors suffer from being some combination of expensive, fragile, and hard-touse. We propose a new fiducial-based design which addresses all three points. The sensor uses an inexpensive webcam and can be fabricated using a consumer-grade 3D printer. Open-source software is used to estimate the 3D pose of the fiducials on the sensor, which is then used to calculate the applied force-torque. A browser-based (installation free) interface demonstrates ease-of-use. The sensor is very light and can be dropped or thrown with little concern. We characterize our prototype in dynamic conditions under compound loading, finding a mean R2 of over 0.99 for the Fx, Fy, Mx, and My axes, and over 0.87 and 0.90 for the Fz and Mz axes respectively. The open source design files allow the sensor to be adapted for diverse applications ranging from robot fingers to human-computer interfaces, while the sdesign principle allows for quick changes with minimal technical expertise. This approach promises to bring six-axis force-torque sensing to new applications where the precision, cost, and fragility of traditional strain-gauge based sensors are not appropriate. The open-source sensor design can be viewed at http://sites.google.com/view/fiducialforcesensor.


Title: Reliable frame-to-frame motion estimation for vehicle-mounted surround-view camera systems
Key Words: cameras  distance measurement  mobile robots  motion estimation  optimisation  path planning  robot vision  frame-to-frame visual odometry  vehicle-mounted surround-view camera system  reliable frame-to-frame motion estimation  vehicle-mounted surround-view camera systems  surround-view multicamera system  autonomous driving  3D point related optimization variables  two-view optimization scheme  nonholonomic characteristics  relative displacements  nonholonomic vehicle motion  overly simplified assumptions  single camera  existing camera  relative vehicle displacement  Conferences  Automation  Reliability  Motion estimation  Cameras  Robot vision systems 
Abstract: Modern vehicles are often equipped with a surround-view multi-camera system. The current interest in autonomous driving invites the investigation of how to use such systems for a reliable estimation of relative vehicle displacement. Existing camera pose algorithms either work for a single camera, make overly simplified assumptions, are computationally expensive, or simply become degenerate under non-holonomic vehicle motion. In this paper, we introduce a new, reliable solution able to handle all kinds of relative displacements in the plane despite the possibly non-holonomic characteristics. We furthermore introduce a novel two-view optimization scheme which minimizes a geometrically relevant error without relying on 3D point related optimization variables. Our method leads to highly reliable and accurate frame-to-frame visual odometry with a full-size, vehicle-mounted surround-view camera system.


Title: Enabling Topological Planning with Monocular Vision
Key Words: learning (artificial intelligence)  mobile robots  multi-agent systems  path planning  robot vision  sensors  SLAM (robots)  heuristic priors  intelligent planning  monocular SLAM  low texture  highly cluttered environments  robust sparse map representation  monocular vision  learned sensor  high-level structure  sparse vertices  known free space  mapping technique  subgoal planning applications  enabling topological planning  topological strategies  navigation  possible actions  Planning  Image edge detection  Navigation  Robot sensing systems  Buildings  Robustness 
Abstract: Topological strategies for navigation meaningfully reduce the space of possible actions available to a robot, allowing use of heuristic priors or learning to enable computationally efficient, intelligent planning. The challenges in estimating structure with monocular SLAM in low texture or highly cluttered environments have precluded its use for topological planning in the past. We propose a robust sparse map representation that can be built with monocular vision and overcomes these shortcomings. Using a learned sensor, we estimate high-level structure of an environment from streaming images by detecting sparse "vertices" (e.g., boundaries of walls) and reasoning about the structure between them. We also estimate the known free space in our map, a necessary feature for planning through previously unknown environments. We show that our mapping technique can be used on real data and is sufficient for planning and exploration in simulated multi-agent search and learned subgoal planning applications.


Title: DeepMEL: Compiling Visual Multi-Experience Localization into a Deep Neural Network
Key Words: distance measurement  image colour analysis  mobile robots  neurocontrollers  path planning  pose estimation  robot vision  robust control  stereo image processing  outdoor driving  deep neural network  visual odometry  vision-based path following  unstructured outdoor environments  visual multiexperience localization  colour-constant imaging  multiexperience VT&R  DeepMEL  stereo visual teach and repeat  robust long-range path following  environmental conditions  pose estimates  in-the-loop path following  Pose estimation  Image edge detection  Neural networks  Robots  Lighting  Snow 
Abstract: Vision-based path following allows robots to autonomously repeat manually taught paths. Stereo Visual Teach and Repeat (VT&R) [1] accomplishes accurate and robust long-range path following in unstructured outdoor environments across changing lighting, weather, and seasons by relying on colour-constant imaging [2] and multi-experience localization [3]. We leverage multi-experience VT&R together with two datasets of outdoor driving on two separate paths spanning different times of day, weather, and seasons to teach a deep neural network to predict relative pose for visual odometry (VO) and for localization with respect to a path. In this paper we run experiments exclusively on datasets to study how the network generalizes across environmental conditions. Based on the results we believe that our system achieves relative pose estimates sufficiently accurate for in-the-loop path following and that it is able to localize radically different conditions against each other directly (i.e. winter to spring and day to night), a capability that our hand-engineered system does not have.


Title: SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  navigation  neurocontrollers  robot vision  robust control  SnapNav  mapless visual navigation  sparse directional guidance  visual reference  robotics  deep neural network  visual navigation system  two-level hierarchy  directional commands  real-time control  obstacle avoidance  autonomous navigation  learning-based visual navigation  robust control  Robots  Navigation  Visualization  Task analysis  Training  Collision avoidance  Turning 
Abstract: Learning-based visual navigation still remains a challenging problem in robotics, with two overarching issues: how to transfer the learnt policy to unseen scenarios, and how to deploy the system on real robots. In this paper, we propose a deep neural network based visual navigation system, SnapNav. Unlike map-based navigation or Visual-Teach-and-Repeat (VT&R), SnapNav only receives a few snapshots of the environment combined with directional guidance to allow it to execute the navigation task. Additionally, SnapNav can be easily deployed on real robots due to a two-level hierarchy: a high level commander that provides directional commands and a low level controller that provides real-time control and obstacle avoidance. This also allows us to effectively use simulated and real data to train the different layers of the hierarchy, facilitating robust control. Extensive experimental results show that SnapNav achieves a highly autonomous navigation ability compared to baseline models, enabling sparse, map-less navigation in previously unseen environments.


Title: Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping
Key Words: C++ language  control engineering computing  graph theory  image reconstruction  image segmentation  learning (artificial intelligence)  public domain software  robot vision  SLAM (robots)  open-source C++ library  visual-inertial SLAM libraries  ORB-SLAM  VINS-Mono  semantic labeling  visual-inertial odometry module  state estimation  robust pose graph optimizer  global trajectory estimation  lightweight 3D mesher module  fast mesh reconstruction  3D metric-semantic reconstruction module  semantically labeled images  metric-semantic SLAM  real-time metric-semantic localization and mapping  Kimera  deep learning  Three-dimensional displays  Simultaneous localization and mapping  Robustness  Semantics  Libraries  Visualization  Real-time systems 
Abstract: We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.


Title: CityLearn: Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning
Key Words: decision making  image representation  learning (artificial intelligence)  mobile robots  navigation  neural nets  robot vision  high-dimensional data  decision-making problems  deep reinforcement learning  place recognition feedback  visual navigation tasks  sample-efficient navigation policy learning  CityLearn environments  visual place recognition  extreme visual appearance changes  realistic environments  navigation algorithms  bimodal image representations  compact image representations  goal destination feedback  deep learning techniques  sample complexity  Navigation  Visualization  Task analysis  Robot sensing systems  Machine learning  Training 
Abstract: Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal per dataset. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.


Title: High Resolution Soft Tactile Interface for Physical Human-Robot Interaction
Key Words: cameras  control engineering computing  haptic interfaces  human-robot interaction  image processing  tactile sensors  touch (physiological)  high resolution soft tactile interface  physical human-robot interaction  tactile interactions  intuitive communication tool  fundamental method  tactile abilities  mechanical safety  sensory intelligence  human-sized geometries  soft tactile interfaces  intrinsically safe mechanical properties  nonlinear characteristics  robotic system  completely soft interface  human upper limbs  high resolution tactile sensory readings  human finger  tactile input  human forearm  safe tactile interface  Robot sensing systems  Cameras  Safety  Task analysis  Visualization 
Abstract: If robots and humans are to coexist and cooperate in society, it would be useful for robots to be able to engage in tactile interactions. Touch is an intuitive communication tool as well as a fundamental method by which we assist each other physically. Tactile abilities are challenging to engineer in robots, since both mechanical safety and sensory intelligence are imperative. Existing work reveals a trade-off between these principles- tactile interfaces that are high in resolution are not easily adapted to human-sized geometries, nor are they generally compliant enough to guarantee safety. On the other hand, soft tactile interfaces deliver intrinsically safe mechanical properties, but their non-linear characteristics render them difficult for use in timely sensing and control. We propose a robotic system that is equipped with a completely soft and therefore safe tactile interface that is large enough to interact with human upper limbs, while producing high resolution tactile sensory readings via depth camera imaging of the soft interface. We present and validate a data-driven model that maps point cloud data to contact forces, and verify its efficacy by demonstrating two real-world applications. In particular, the robot is able to react to a human finger's pokes and change its pose based on the tactile input. In addition, we also demonstrate that the robot can act as an assistive device that dynamically supports and follows a human forearm from underneath.


Title: Design and Validation of a Soft Robotic Ankle-Foot Orthosis (SR-AFO) Exosuit for Inversion and Eversion Ankle Support
Key Words: actuators  biomedical equipment  biomedical measurement  finite element analysis  gait analysis  mechanoception  medical robotics  muscle  orthotics  patient rehabilitation  frontal plane  sagittal plane  SR-AFO exosuit  wearable ankle robot  ankle stiffness  soft robotic ankle-foot orthosis exosuit  eversion ankle support  pressure 50.0 kPa  pressure 30.0 kPa  Actuators  Fabrics  Solids  Structural beams  Load modeling  Soft robotics  Soft Robotics  Wearable Robots  Assistive Robots  Rehabilitation 
Abstract: This paper presents a soft robotic ankle-foot orthosis (SR-AFO) exosuit designed to provide support to the human ankle in the frontal plane without restricting natural motion in the sagittal plane. The SR-AFO exosuit incorporates inflatable fabric-based actuators with a hollow cylinder design which requires less volume than the commonly used solid cylinder design for the same deflection. The actuators were modeled and characterized using finite element analysis techniques and experimentally validated. The SR-AFO exosuit was evaluated on healthy participants in both a sitting position using a wearable ankle robot and a standing position using a dual-axis robotic platform to characterize the effect of the exosuit on the change of 2D ankle stiffness in the sagittal and frontal planes. For both sitting and standing test protocols, a trend of increasing ankle stiffness in the frontal plane was observed up to 50 kPa while stiffness in the sagittal plane remained relatively constant over pressure levels. During quiet standing, the exosuit could effectively change eversion stiffness at the ankle joint from about 20 to 70 Nm/rad at relatively low- pressure levels (<; 30 kPa). Eversion stiffness was 84.9 Nm/rad at 50 kPa, an increase of 387.5% from the original free foot stiffness.


Title: Velocity Field based Active-Assistive Control for Upper Limb Rehabilitation Exoskeleton Robot
Key Words: biomechanics  Kalman filters  medical robotics  motion control  observers  path planning  patient rehabilitation  torque control  wearable robots  upper limb rehabilitation exoskeleton robot  time-dependent trajectories  task-based rehabilitation exercise  multijoint motion  assistive mechanism  active-assistive control system  joint-position-dependent velocity field  task motion pattern  time-independent assistance  active motions  assistive motions  rehabilitation task  single joint tasks  Kalman filter based interactive torque observer  subject active motion intention  subject torque exertion  Task analysis  Torque  Robots  Kalman filters  Observers  Control systems  Sensors 
Abstract: There are limitations of conventional active-assistive control for upper limb rehabilitation exoskeleton robot, such as 1). prior time-dependent trajectories are generally required, 2). task-based rehabilitation exercise involving multi-joint motion is hard to implement, and 3). assistive mechanism normally is so inflexible that the resulting exercise performed by the subjects becomes inefficient. In this paper, we propose a novel velocity field based active-assistive control system to address these issues. First, we design a Kalman filter based interactive torque observer to obtain subjects' active intention of motion. Next, a joint-position-dependent velocity field which can be automatically generated via the task motion pattern is proposed to provide the time-independent assistance to the subjects. We further propose a novel integration method that combines the active and assistive motions based on the performance and the involvement of subjects to guide them to perform the task more voluntarily and precisely. The experiment results show that both the execution time and the subjects' torque exertion are reduced while performing both given single joint tasks and task-oriented multi-joint tasks as compared with the related work in the literature. To sum up, the proposed system not only can efficiently retain subjects' active intention but also can assist them to accomplish the rehabilitation task more precisely.


Title: Design, Development, and Control of a Tendon-actuated Exoskeleton for Wrist Rehabilitation and Training
Key Words: actuators  biomechanics  diseases  medical robotics  neurophysiology  patient rehabilitation  patient treatment  robot-mediated therapies  robotic technologies  social contexts  motion training  device design  wrist mobility  tendon-actuated exoskeleton  wrist rehabilitation  robot rehabilitation  emerging promising topic  neuroscience  neurological diseases  rehabilitation process  Exoskeletons  Wrist  Robots  Sensors  Tracking  Magnetometers  Medical treatment 
Abstract: Robot rehabilitation is an emerging and promising topic that incorporates robotics with neuroscience and rehabilitation to define new methods for supporting patients with neurological diseases. As a consequence, the rehabilitation process could increase the efficacy exploiting the potentialities of robot-mediated therapies. Nevertheless, nowadays clinical effectiveness is not enough to widely introduce robotic technologies in such social contexts. In this paper we propose a step further, presenting an innovative exoskeleton for wrist flexion/extension and adduction/abduction motion training. It is designed to be wearable and easy to control and manage. It can be used by the patient in collaboration with the therapist or autonomously. The paper introduces the main steps of device design and development and presents some tests conducted with an user with limited wrist mobility.


Title: Impedance Control of a Transfemoral Prosthesis using Continuously Varying Ankle Impedances and Multiple Equilibria
Key Words: artificial limbs  gait analysis  least squares approximations  medical robotics  prosthetics  springs (mechanical)  vibration control  squares estimation  impedance controller  squares optimization method  knee impedance  impedance control  lower limb prostheses  human joint torque  perturbation studies  least squares estimates  ankle impedance parameters  powered transfemoral prosthesis  Impedance  Damping  Torque  Optimization  Knee  Prosthetics  Perturbation methods 
Abstract: Impedance controllers are popularly used in the field of lower limb prostheses and exoskeleton development. Such controllers assume the joint to be a spring-damper system described by a discrete set of equilibria and impedance parameters. These parameters are estimated via a least squares optimization that minimizes the difference between the controller's output torque and human joint torque. Other researchers have used perturbation studies to determine empirical values for ankle impedance. The resulting values vary greatly from the prior least squares estimates. While perturbation studies are more credible, they require immense investment. This paper extended the least squares approach to reproduce the results of perturbation studies. The resulting ankle impedance parameters were successfully tested on a powered transfemoral prosthesis, AMPRO II. Further, the paper investigated the effect of multiple equilibria on the least squares estimation and the performance of the impedance controller. Finally, the paper uses the proposed least squares optimization method to estimate knee impedance.


Title: Gait patterns generation based on basis functions interpolation for the TWIN lower-limb exoskeleton*
Key Words: gait analysis  injuries  interpolation  legged locomotion  medical robotics  neurophysiology  orthotics  patient rehabilitation  gait trajectory pattern  TWIN exoskeleton  spinal-cord injury patient  spinal cord injuries  rehabilitation centers  basis function interpolation method  stable trajectory pattern  feasible trajectory pattern  exoskeletons  biomedical orthotic devices  TWIN lower-limb exoskeleton  basis function interpolation  gait pattern generation  Trajectory  Legged locomotion  Exoskeletons  Torso  Hip  Foot  Knee 
Abstract: Since the uprising of new biomedical orthotic devices, exoskeletons have been put in the spotlight for their possible use in rehabilitation. Even if these products might share some commonalities among them in terms of overall structure, degrees of freedom and possible actions, they quite often differ in their approach on how to generate a feasible, stable and comfortable gait trajectory pattern. This paper introduces three proposed trajectories that were generated by using a basis function interpolation method and by working closely with two major rehabilitation centers in Italy. The whole procedure has been focused on the concepts of a configurable walk for patients that suffer from spinal cord injuries. We tested the solutions on a group of healthy volunteers and on a spinal-cord injury patient with the use of the new TWIN exoskeleton developed at the Rehab Technologies Lab at the Italian Institute of Technology.


Title: Human-Centric Active Perception for Autonomous Observation
Key Words: aerospace robotics  Markov processes  mobile communication  mobile robots  optimisation  space vehicles  autonomous observation systems  human activity  multiobjective optimization  autonomous human observation problem  robot-centric costs  scalarization-based MultiObjective MDP methods  NASA Astrobee robot operating  human-centric active perception  robot autonomy  SemiMDP formulation  constrained MDP method  NASA Astrobee robot  Task analysis  Cameras  Robot vision systems  Collision avoidance  Cost function 
Abstract: As robot autonomy improves, robots are increasingly being considered in the role of autonomous observation systems - free-flying cameras capable of actively tracking human activity within some predefined area of interest. In this work, we formulate the autonomous observation problem through multi-objective optimization, presenting a novel Semi-MDP formulation of the autonomous human observation problem that maximizes observation rewards while accounting for both human- and robot-centric costs. We demonstrate that the problem can be solved with both scalarization-based Multi-Objective MDP methods and Constrained MDP methods, and discuss the relative benefits of each approach. We validate our work on activity tracking using a NASA Astrobee robot operating within a simulated International Space Station environment.


Title: Prediction of Human Full-Body Movements with Motion Optimization and Recurrent Neural Networks
Key Words: gradient methods  human-robot interaction  image motion analysis  mobile robots  optimisation  path planning  recurrent neural nets  robot vision  robot trajectory planning  gradient-based trajectory optimization  human full-body movement prediction  motion prediction  trajectory optimization  environmental constraints  short-term dynamics  long-term prediction  internal body dynamics  short-term prediction  recurrent neural network  motion optimization  Optimization  Trajectory  Predictive models  Recurrent neural networks  Collision avoidance  Robot kinematics 
Abstract: Human movement prediction is difficult as humans naturally exhibit complex behaviors that can change drastically from one environment to the next. In order to alleviate this issue, we propose a prediction framework that decouples short-term prediction, linked to internal body dynamics, and long-term prediction, linked to the environment and task constraints. In this work we investigate encoding short-term dynamics in a recurrent neural network, while we account for environmental constraints, such as obstacle avoidance, using gradient-based trajectory optimization. Experiments on real motion data demonstrate that our framework improves the prediction with respect to state-of-the-art motion prediction methods, as it accounts to beforehand unseen environmental structures. Moreover we demonstrate on an example, how this framework can be used to plan robot trajectories that are optimized to coordinate with a human partner.


Title: Predicting and Optimizing Ergonomics in Physical Human-Robot Cooperation Tasks
Key Words: biomechanics  ergonomics  graph theory  human-robot interaction  mobile robots  multi-robot systems  path planning  telerobotics  32 DoF bimanual mobile robot  ergonomic-enhanced planner  reduced ergonomic cost  physical human-robot cooperation tasks  action sequences  continuous physical interaction  computational model  ergonomics assessment  human motion capture data  prediction model  informed graph search algorithm  ergonomic assessment  bimanual human-robot cooperation tasks  Ergonomics  Robots  Task analysis  Optimization  Predictive models  Computational modeling  Force 
Abstract: This paper presents a method to incorporate ergonomics into the optimization of action sequences for bi-manual human-robot cooperation tasks with continuous physical interaction. Our first contribution is a novel computational model of the human that allows prediction of an ergonomics assessment corresponding to each step in a task. The model is learned from human motion capture data in order to predict the human pose as realistically as possible. The second contribution is a combination of this prediction model with an informed graph search algorithm, which allows computation of human-robot cooperative plans with improved ergonomics according to the incorporated method for ergonomic assessment. The concepts have been evaluated in simulation and in a small user study in which the subjects manipulate a large object with a 32 DoF bimanual mobile robot as partner. For all subjects, the ergonomic-enhanced planner shows their reduced ergonomic cost compared to a baseline planner.


Title: Active Reward Learning for Co-Robotic Vision Based Exploration in Bandwidth Limited Environments
Key Words: decision theory  learning (artificial intelligence)  Markov processes  mobile robots  query processing  robot vision  making queries  regret-based criterion  active reward learning strategy  co-robotic vision based exploration  robotic explorer  bandwidth-limited environments  autonomous visual exploration  high-dimensional observation space  communication strategy  reward model  observation model  human operator  scientifically relevant images  POMDP problem formulation  Robot sensing systems  Semantics  Bandwidth  Computational modeling  Visualization  Trajectory 
Abstract: We present a novel POMDP problem formulation for a robot that must autonomously decide where to go to collect new and scientifically relevant images given a limited ability to communicate with its human operator. From this formulation we derive constraints and design principles for the observation model, reward model, and communication strategy of such a robot, exploring techniques to deal with the very high-dimensional observation space and scarcity of relevant training data. We introduce a novel active reward learning strategy based on making queries to help the robot minimize path "regret" online, and evaluate it for suitability in autonomous visual exploration through simulations. We demonstrate that, in some bandwidth-limited environments, this novel regret-based criterion enables the robotic explorer to collect up to 17% more reward per mission than the next-best criterion.


Title: VariPath: A Database for Modelling the Variance of Human Pathways in Manual and HRC Processes with Heavy-Duty Robots
Key Words: human-robot interaction  industrial robots  occupational safety  path planning  heavy-duty robots  human pathway variations  human-robot collaboration  HRC process  women walking pathways  varipath database  planning process  safety  Legged locomotion  Service robots  Task analysis  Planning  Atmospheric measurements  Particle measurements 
Abstract: Unlike robots, humans do not have constant movements. Their pathways are individually changeable and influenced by circumstances. This paper presents a method to investigate human pathway variations in a real study. In systematically selected tasks, human pathways are examined for 100 participants in manual and human-robot collaboration (HRC) scenarios. As a result, the variations of pathways are presented depending on various features: e.g. in nearly all cases the variance of women's walking pathways is smaller than that of men. VariPath database can be used in any planning process of manual or HRC scenarios to ensure safety and efficiency.


Title: A Compact and Low-cost Robotic Manipulator Driven by Supercoiled Polymer Actuators
Key Words: coils  design engineering  dexterous manipulators  electroactive polymer actuators  grippers  mobile robots  pneumatic actuators  fragile fruit  pick and place demonstration  Fin Ray Effect inspired soft gripper  ball-and-socket joints  Joule heating  electrical activation  bio-inspired robotic manipulator design  bio-inspired arm  manipulator prototype  SCP actuators  robotic arm  muscle-like form  twisting coiling polymer fibers  artificial muscle  supercoiled polymer actuator  low-cost robotic manipulator  Actuators  Manipulators  Muscles  Grippers  Polymers 
Abstract: The supercoiled polymer (SCP) actuator is a novel artificial muscle, which is manufactured by twisting and coiling polymer fibers. This new artificial muscle is soft, low-cost and shows good linearity. Being utilized as an actuator, the artificial muscle could generate significant mechanical power in a muscle-like form upon electrical activation by Joule heating. In this study, we adopt this new artificial muscle to actuate a novel designed robotic manipulator, which is composed of two parts. The first part is a robotic arm based on the inspiration of the musculoskeletal system. The arm is fabricated with two ball-and-socket joints as skeleton and SCP actuators as driven muscles. The second part is a Fin Ray Effect inspired soft gripper that can perform grasping tasks on fragile objects. The manipulator prototype is fabricated and experimental tests are conducted including both simple but effective control of the bio-inspired arm as well as characterization of the gripper. Lastly, a pick and place demonstration of a fragile fruit is performed utilizing the proposed manipulator. We envision that the bio-inspired robotic manipulator design driven by SCP actuators could potentially be used in other robotic applications.


Title: Internally-Balanced Magnetic Mechanisms Using a Magnetic Spring for Producing a Large Amplified Clamping Force
Key Words: clamps  control system synthesis  force control  magnetic devices  mobile robots  multi-robot systems  permanent magnets  springs (mechanical)  magnetic spring  magnetic mechanisms  clamping force  permanent magnet  force control  attractive force  internally-balanced magnetic unit  magnetic devices  internal force  internally-balanced magnetic mechanisms  IB magnet  nonlinear spring  unlike-pole pair  wall-climbing robots  ceiling-dangling drones  modular swarm robots  robotic clamp  Springs  Force  Magnetic noise  Magnetic shielding  Magnetic levitation  Magnetic liquids  Magnetic separation  Mechanism Design of Manipulators  Force Control 
Abstract: To detach a permanent magnet using a control force much smaller than its original attractive force, the internally-balanced magnetic unit (IB Magnet) was invented. It has been applied to magnetic devices such as wall-climbing robots, ceiling-dangling drones, and modular swarm robots. In contrast to its significant reduction rate with regard to the control force, the IB Magnet has two major problems in its nonlinear spring, which serves the purpose of cancelling out the internal force on the magnet. These problems include the complicated design procedure and the trade-off relationship between balancing the precision and the volume of the mechanism. This paper proposes a principle for a new balancing method for the IB Magnet. This method uses a like-pole pair of magnets as a magnetic spring, whose repulsive force should equal the attractive force of an unlike-pole pair. To verify the proposed principle, a prototype of the IB Magnet was designed using a magnetic spring and verified through experiments such that its reduction rate is comparable to those of conventional IB Magnets. Moreover, a robotic clamp was developed as an application example that contains the proposed IB Magnets as its internal mechanism.


Title: Design and Compensation Control of a Flexible Instrument for Endoscopic Surgery
Key Words: actuators  compensation  end effectors  endoscopes  grippers  medical robotics  surgery  flexible instrument  endoscopic surgery  snake-like robots  flexible tendon-driven instruments  microsurgical tasks  standard endoscopic surgeries  articulated wrists  distal-roll gripper  compensation control scheme  end-effector rolling motion  Tendons  Instruments  Grippers  Gears  Robots  Joints  Surgery 
Abstract: Snake-like robots for endoscopic surgery make it possible to reach deep-seated lesions. With the use of small flexible tendon-driven instruments, it is possible to perform bimanual micro-surgical tasks that are challenging for standard endoscopic surgeries. Existing devices, however, lack articulated wrists and rolling motion of the end-effector. This paper presents a new instrument design with a distal-roll gripper for snake-like robots. The developed 5 DoFs miniaturized instruments with a diameter of 3 mm enable the deployment into narrow endoluminal channels. Issues related to actuation coupling, tendon slack, and backlash are addressed. Experimental results show that the distal-roll gripper can rotate 106°, and the actuated joints can achieve good repeatability and accuracy with the proposed compensation control scheme.


Title: Distance and Steering Heuristics for Streamline-Based Flow Field Planning
Key Words: computational fluid dynamics  flow simulation  marine robots  mobile robots  path planning  robot dynamics  artificial flow field  East Australian current  streamline-based flow field planning  motion planning  streamline-based planning  fluid dynamics  travel distance  incompressible flows  ocean currents  distance functions  Euclidean distance  stream function  steering heuristics  ocean prediction data  autonomous marine robots  Planning  Aerospace electronics  Vehicle dynamics  Two dimensional displays  Space exploration  Space vehicles  Oceans 
Abstract: Motion planning for vehicles under the influence of flow fields can benefit from the idea of streamline-based planning, which exploits ideas from fluid dynamics to achieve computational efficiency. Important to such planners is an efficient means of computing the travel distance and direction between two points in free space, but this is difficult to achieve in strong incompressible flows such as ocean currents. We propose two useful distance functions in analytical form that combine Euclidean distance with values of the stream function associated with a flow field, and with an estimation of the strength of the opposing flow between two points. Further, we propose steering heuristics that are useful for steering towards a sampled point. We evaluate these ideas by integrating them with RRT* and comparing the algorithm's performance with state-of-the-art methods in an artificial flow field and in actual ocean prediction data in the region of the dominant East Australian Current between Sydney and Brisbane. Results demonstrate the method's computational efficiency and ability to find high-quality paths outperforming state-of-the-art methods, and show promise for practical use with autonomous marine robots.


Title: Enhancing Coral Reef Monitoring Utilizing a Deep Semi-Supervised Learning Approach
Key Words: convolutional neural nets  feature extraction  marine engineering  object detection  supervised learning  deep neural network  sample extraction  coral object dataset  coral reef monitoring  deep semisupervised learning approach  coral species detection  convolutional neural network-based object detector  Detectors  Training  Object detection  Monitoring  Tracking  Predictive models  Semantics 
Abstract: Coral species detection underwater is a challenging problem. There are many cases when even the experts (marine biologists) fail to recognize corals, hence limiting ground truth annotation for training a robust detection system. Identifying coral species is fundamental for enabling the monitoring of coral reefs, a task currently performed by humans, which can be automated with the use of underwater robots. By employing temporal cues using a tracker on a high confidence prediction by a convolutional neural network-based object detector, we augment the collected dataset for the retraining of the object detector. However, using trackers to extract examples also introduces hard or mislabelled samples, which is counterproductive and will deteriorate the performance of the detector. In this work, we show that employing a simple deep neural network to filter out hard or mislabelled samples can help regulate sample extraction. We empirically evaluate our approach in a coral object dataset, collected via an Autonomous Underwater Vehicle (AUV) and human divers, that shows the benefit of incorporating extracted examples obtained from tracking. This work also demonstrates how controlling sample generation by tracking using a simple deep neural network can further improve an object detector.


Title: DOB-Net: Actively Rejecting Unknown Excessive Time-Varying Disturbances
Key Words: control system synthesis  feedback  learning (artificial intelligence)  neurocontrollers  observers  optimal control  position control  recurrent neural nets  robots  robot control capabilities  disturbance dynamics observer network  controller network  conventional DOB mechanisms  recurrent neural networks  optimal control signals  conventional feedback controllers  DOB-Net  disturbance OB-server network  observer-integrated reinforcement learning  Observers  History  Vehicle dynamics  Robots  Optimization  Optimal control  Dynamics 
Abstract: This paper presents an observer-integrated Reinforcement Learning (RL) approach, called Disturbance OB-server Network (DOB-Net), for robots operating in environments where disturbances are unknown and time-varying, and may frequently exceed robot control capabilities. The DOB-Net integrates a disturbance dynamics observer network and a controller network. Originated from conventional DOB mechanisms, the observer is built and enhanced via Recurrent Neural Networks (RNNs), encoding estimation of past values and prediction of future values of unknown disturbances in RNN hidden state. Such encoding allows the controller generate optimal control signals to actively reject disturbances, under the constraints of robot control capabilities. The observer and the controller are jointly learned within policy optimization by advantage actor critic. Numerical simulations on position regulation tasks have demonstrated that the proposed DOB-Net significantly outperforms conventional feedback controllers and classical RL policy.


Title: How far are Pneumatic Artificial Muscles from biological muscles?
Key Words: biomechanics  elasticity  electroactive polymer actuators  legged locomotion  muscle  pneumatic actuators  pneumatic artificial muscles  biological muscles  artificial copies  force generation mechanism  PAM force-length  additive passive parallel elastic element  PAM dynamic behaviors  dynamic muscle-like model  living creatures  multiplicative formulation  two-segmented leg  legged robots  Muscles  Mathematical model  Force  Biological system modeling  Robots  Dynamics 
Abstract: There is a long history demonstrating humans' tendency to create artificial copies of living creatures. For moving machines called robots, actuators play a key role in developing human-like movements. Among different types of actuation, PAMs (pneumatic artificial muscles) are known as the most similar ones to biological muscles. In addition to similarities in force generation mechanism (tension based), the well-accepted argumentation from Klute et al., states that the PAM force-length (fl) behavior is close to biological muscles, while the force-velocity (fv) pattern is different. Using the multiplicative formulation of the pressure (as an activation term), fl and fv beside an additive passive parallel elastic element, we present a new model of PAM. This muscle-based model can predict PAM dynamic behaviors with high precision. With a second experiment on a two-segmented leg, the proposed model is verified to predict the generated forces of PAMs in an antagonistic arrangement. Such a dynamic muscle-like model of artificial muscles can be used for the design and control of legged robots to generate robust, efficient and versatile gaits.


Title: Shared Control Templates for Assistive Robotics
Key Words: assisted living  handicapped aids  manipulators  medical robotics  mobile robots  path planning  service robots  constraint-based shared control  specific user command mappings  task execution  impairments  control interface  motor disability  light-weight robotic manipulators  assistive robotics  shared control templates  low-dimensional interface  high-dimensional tasks  human-readable format  state transitions  Task analysis  Robot kinematics  Manipulators  Wheelchairs  Manifolds  Rehabilitation robotics 
Abstract: Light-weight robotic manipulators can be used to restore the manipulation capability of people with a motor disability. However, manipulating the environment poses a complex task, especially when the control interface is of low bandwidth, as may be the case for users with impairments. Therefore, we propose a constraint-based shared control scheme to define skills which provide support during task execution. This is achieved by representing a skill as a sequence of states, with specific user command mappings and different sets of constraints being applied in each state. New skills are defined by combining different types of constraints and conditions for state transitions, in a human-readable format. We demonstrate its versatility in a pilot experiment with three activities of daily living. Results show that even complex, high-dimensional tasks can be performed with a low-dimensional interface using our shared control approach.


Title: Enabling Robots to Understand Incomplete Natural Language Instructions Using Commonsense Reasoning
Key Words: common-sense reasoning  control engineering computing  human-robot interaction  natural language processing  text analysis  environmental context  natural language instruction  unconstrained natural language  language-model-based commonsense reasoning  commonsense knowledge  spoken natural language  LMCR  parsing  verb frames  unstructured textual corpora  robot  Natural languages  Robot sensing systems  Cognition  Task analysis  Semantics  Neural networks 
Abstract: Enabling robots to understand instructions provided via spoken natural language would facilitate interaction between robots and people in a variety of settings in homes and workplaces. However, natural language instructions are often missing information that would be obvious to a human based on environmental context and common sense, and hence does not need to be explicitly stated. In this paper, we introduce Language-Model-based Commonsense Reasoning (LMCR), a new method which enables a robot to listen to a natural language instruction from a human, observe the environment around it, and automatically fill in information missing from the instruction using environmental context and a new commonsense reasoning approach. Our approach first converts an instruction provided as unconstrained natural language into a form that a robot can understand by parsing it into verb frames. Our approach then fills in missing information in the instruction by observing objects in its vicinity and leveraging commonsense reasoning. To learn commonsense reasoning automatically, our approach distills knowledge from large unstructured textual corpora by training a language model. Our results show the feasibility of a robot learning commonsense knowledge automatically from web-based textual corpora, and the power of learned commonsense reasoning models in enabling a robot to autonomously perform tasks based on incomplete natural language instructions.


Title: A Holistic Approach in Designing Tabletop Robot’s Expressivity
Key Words: computer animation  control engineering computing  control system synthesis  humanoid robots  robot modalities  zoomorphic-designed robots  robot design  animated characters  table top robot  animation techniques  robot hardware  Haru  Animation  Hardware  Neck  Light emitting diodes  Mouth  Service robots 
Abstract: Defining a robot's expressivity is a difficult task that requires thoughtful consideration of the potential of various robot modalities and a model of communication that humans understand. Humanoid and zoomorphic-designed robots can easily take cues from human and animals, respectively when designing their expressivity. However, a robot design that is neither human nor animal-like does not have a clear model to follow in terms of designing expressivity. Animation presents a potential model in these circumstances as animated characters in movies take various forms, sizes, shapes and styles, and are successful in defining expressivity that is widely accepted across different languages and cultures. In this paper, we discuss the development and design of the expressivity of Haru, a table top robot that is neither human nor animal-like and the application of animation expertise to the holistic treatment of the different modalities. The method maximizes animation techniques and expertise normally applied to movies to generate expressivity that is then transferred to the robot hardware. Experimental results show that the robot's expressivity generated using our method is easily understood and are preferred to the conventional approach of generating expressions.


Title: DirtNet: Visual Dirt Detection for Autonomous Cleaning Robots
Key Words: cleaning  feature extraction  object detection  robot vision  service robots  office item detection system  YOLOv3 framework  visual dirt detection  autonomous cleaning  vacuum cleaning  professional cleaning robots  DirtNet  Cleaning  Object detection  Robots  Training  Task analysis  Image resolution  Visualization 
Abstract: Visual dirt detection is becoming an important capability of modern professional cleaning robots both for optimizing their wet cleaning results and for facilitating demand-oriented daily vacuum cleaning. This paper presents a robust, fast, and reliable dirt and office item detection system for these tasks based on an adapted YOLOv3 framework. Its superiority over state-of-the-art dirt detection systems is demonstrated in several experiments. The paper furthermore features a dataset generator for creating any number of realistic training images from a small set of real scene, dirt, and object examples.


Title: Semantic Linking Maps for Active Visual Object Search
Key Words: inference mechanisms  manipulators  mobile robots  robot vision  search problems  Semantic Linking Maps model  target object  landmark objects  probabilistic inter-object spatial relations  hybrid search strategy  SLiM-based search strategy  Fetch mobile manipulation robot  mobile robots  common human environments  unseen target objects  reasoning  search space  common spatial relations  active visual object search strategy  Search problems  Robots  Probabilistic logic  Semantics  Buildings  Inference algorithms  Visualization 
Abstract: We aim for mobile robots to function in a variety of common human environments. Such robots need to be able to reason about the locations of previously unseen target objects. Landmark objects can help this reasoning by narrowing down the search space significantly. More specifically, we can exploit background knowledge about common spatial relations between landmark and target objects. For example, seeing a table and knowing that cups can often be found on tables aids the discovery of a cup. Such correlations can be expressed as distributions over possible pairing relationships of objects. In this paper, we propose an active visual object search strategy method through our introduction of the Semantic Linking Maps (SLiM) model. SLiM simultaneously maintains the belief over a target object's location as well as landmark objects' locations, while accounting for probabilistic inter-object spatial relations. Based on SLiM, we describe a hybrid search strategy that selects the next best view pose for searching for the target object based on the maintained belief. We demonstrate the efficiency of our SLiM-based search strategy through comparative experiments in simulated environments. We further demonstrate the realworld applicability of SLiM-based search in scenarios with a Fetch mobile manipulation robot.


Title: Active Depth Estimation: Stability Analysis and its Applications
Key Words: cameras  image sequences  Lyapunov methods  mobile robots  robot vision  solid modelling  stability  SfM  incremental active depth estimation  chronological sequence  image frames  camera actuation  stability analysis  control inputs  image plane  vision-controlled structure-from-motion scheme  depth estimation filter  Lyapunov theory  Cameras  Three-dimensional displays  Stability analysis  Asymptotic stability  Convergence  Estimation error 
Abstract: Recovering the 3D structure of the surrounding environment is an essential task in any vision-controlled Structure-from-Motion (SfM) scheme. This paper focuses on the theoretical properties of the SfM, known as the incremental active depth estimation. The term incremental stands for estimating the 3D structure of the scene over a chronological sequence of image frames. Active means that the camera actuation is such that it improves estimation performance. Starting from a known depth estimation filter, this paper presents the stability analysis of the filter in terms of the control inputs of the camera. By analyzing the convergence of the estimator using the Lyapunov theory, we relax the constraints on the projection of the 3D point in the image plane when compared to previous results. Nonetheless, our method is capable of dealing with the cameras' limited field-of-view constraints. The main results are validated through experiments with simulated data.


Title: Intensity Scan Context: Coding Intensity and Geometry Relations for Loop Closure Detection
Key Words: computational geometry  feature extraction  image matching  mobile robots  optical radar  robot vision  SLAM (robots)  stereo image processing  intensity scan context  light detection and ranging sensor  discard intensity reading  geometric relation  intensity structure re-identification  coding intensity  simultaneous localization and mapping  LiDAR sensor  3D loop closure detection  geometrical only descriptor matching  place recognition  robot navigation  fast point feature histogram  Geometry  Laser radar  Three-dimensional displays  Histograms  Simultaneous localization and mapping  Rough surfaces  Surface roughness 
Abstract: Loop closure detection is an essential and challenging problem in simultaneous localization and mapping (SLAM). It is often tackled with light detection and ranging (LiDAR) sensor due to its view-point and illumination invariant properties. Existing works on 3D loop closure detection often leverage on matching of local or global geometrical-only descriptors which discard intensity reading. In this paper we explore the intensity property from LiDAR scan and show that it can be effective for place recognition. We propose a novel global descriptor, intensity scan context (ISC), that explores both geometry and intensity characteristics. To improve the efficiency for loop closure detection, an efficient two-stage hierarchical re-identification process is proposed, including binary-operation based fast geometric relation retrieval and intensity structure re-identification. Thorough experiments including both local experiment and public datasets test have been conducted to evaluate the performance of the proposed method. Our method achieves better recall rate and recall precision than existing geometric-only methods.


Title: TextSLAM: Visual SLAM with Planar Text Features
Key Words: augmented reality  data visualisation  navigation  robot vision  SLAM (robots)  stereo image processing  text analysis  text detection  text object integration  augmented reality  navigation  scene understanding  illumination-invariant photometric error  TextSLAM  text detection  text-based visual SLAM  3D text maps  visual SLAM pipeline  planar text features  visual SLAM system  planar feature  Simultaneous localization and mapping  Three-dimensional displays  Visualization  Feature extraction  Navigation  Cameras  Robustness 
Abstract: We propose to integrate text objects in man-made scenes tightly into the visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to treat each detected text as a planar feature which is rich of textures and semantic meanings. The text feature is compactly represented by three parameters and integrated into visual SLAM by adopting the illumination-invariant photometric error. We also describe important details involved in implementing a full pipeline of text-based visual SLAM. To our best knowledge, this is the first visual SLAM method tightly coupled with the text features. We tested our method in both indoor and outdoor environments. The results show that with text features, the visual SLAM system becomes more robust and produces much more accurate 3D text maps that could be useful for navigation and scene understanding in robotic or augmented reality applications.


Title: Redesigning SLAM for Arbitrary Multi-Camera Systems
Key Words: cameras  distance measurement  robot vision  SLAM (robots)  stereo image processing  sensor-specific modifications  SLAM systems  robustness  camera configurations  adaptive SLAM system  multicamera setup  visual SLAM  adaptive initialization  scalable voxel-based map  sensor-agnostic information-theoretic keyframe selection algorithm  visual front-end design  visual-inertial odometry  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Uncertainty  Visualization 
Abstract: Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information- theoretic keyframe selection algorithm, and a scalable voxel- based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual- inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.


Title: Dynamic SLAM: The Need For Speed
Key Words: feature extraction  image motion analysis  image segmentation  mobile robots  path planning  robot vision  SLAM (robots)  rigid moving objects  static structure  dynamic structure  rigid objects  object-aware dynamic SLAM algorithm  model-free  significant motion constraints  3D models  SLAM based approaches  unstructured dynamic environments  autonomous systems  increased deployment  simultaneous localisation  static world assumption  Simultaneous localization and mapping  Heuristic algorithms  Dynamics  Three-dimensional displays  Solid modeling  Tracking 
Abstract: The static world assumption is standard in most simultaneous localisation and mapping (SLAM) algorithms. Increased deployment of autonomous systems to unstructured dynamic environments is driving a need to identify moving objects and estimate their velocity in real-time. Most existing SLAM based approaches rely on a database of 3D models of objects or impose significant motion constraints. In this paper, we propose a new feature-based, model-free, object-aware dynamic SLAM algorithm that exploits semantic segmentation to allow estimation of motion of rigid objects in a scene without the need to estimate the object poses or have any prior knowledge of their 3D models. The algorithm generates a map of dynamic and static structure and has the ability to extract velocities of rigid moving objects in the scene. Its performance is demonstrated on simulated, synthetic and real-world datasets.


Title: ∇SLAM: Dense SLAM meets Automatic Differentiation
Key Words: gradient methods  graph theory  learning (artificial intelligence)  robot vision  SLAM (robots)  automatic differentiation  dense simultaneous localization  learning-based approaches  representation learning approaches  classical SLAM systems  differentiable function  optimize task performance  typical dense SLAM system  ∇SLAM  posing SLAM systems  differentiable computational graphs  differentiable trust-region optimizers  task-based error signals  Simultaneous localization and mapping  Optimization  Three-dimensional displays  Damping  Task analysis  Neural networks 
Abstract: The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Learning-based approaches have the potential to leverage data or task performance to directly inform the representation. However, blending representation learning approaches with "classical" SLAM systems has remained an open question, because of their highly modular and complex nature. A SLAM system transforms raw sensor inputs into a distribution over the state(s) of the robot and the environment. If this transformation (SLAM) were expressible as a differentiable function, we could leverage task-based error signals over the outputs of this function to learn representations that optimize task performance. However, this is infeasible as several components of a typical dense SLAM system are non-differentiable. In this work, we propose ∇SLAM (gradSLAM), a methodology for posing SLAM systems as differentiable computational graphs, which unifies gradient-based learning and SLAM. We propose differentiable trust-region optimizers, surface measurement and fusion schemes, and raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM1.


Title: Learning local behavioral sequences to better infer non-local properties in real multi-robot systems
Key Words: learning (artificial intelligence)  multi-robot systems  neurocontrollers  recurrent neural nets  two-wheeled robotic platform  local behavioral sequences  multirobot systems  multirobot team  traditional observer-based approach  machine learning methods  remote teammate localization modules  long-short-term-memory  Robot sensing systems  Robot kinematics  Training  Multi-robot systems  Machine learning  Shape 
Abstract: When members of a multi-robot team follow regular motion rules sensitive to robots and other environmental factors within sensing range, the team itself may become an informational fabric for gaining situational awareness without explicit signalling among robots. In our previous work [1], we used machine learning to develop a scalable module, trained only on data from 3-robot teams, that could predict the positions of all robots in larger multi-robot teams based only on observations of the movement of a robot's nearest neighbor. Not only was this approach scalable from 3-to-many robots, but it did not require knowledge of the control laws of the robots under observation, as would a traditional observer-based approach. However, performance was only tested in simulation and could only be a substitute for explicit communication for short periods of time or in cases of very low sensing noise. In this work, we apply more sophisticated machine learning methods to data from a physically realized robotic team to develop Remote Teammate Localization (ReTLo) modules that can be used in realistic environments. To be specific, we adopt Long-Short-Term-Memory (LSTM) [2] to learn the evolution of behaviors in a modular team, which has the effect of greatly reducing errors from regression outcomes. In contrast with our previous work in simulation, all of the experiments conducted in this work were conducted on the Thymio physical, two-wheeled robotic platform.


Title: Unsupervised Geometry-Aware Deep LiDAR Odometry
Key Words: distance measurement  geometry  optical radar  radar computing  reliability  supervised learning  unsupervised learning  unsupervised geometry-aware deep LiDAR odometry  visual perception  supervised learning-based approaches  supervised training  ground-truth pose labels  trainable LO  uncertainty-aware loss  LeGO-LOAM  unsupervised learning-based approaches  egomotion estimation approaches  Stereo-VO datasets  complex urban datasets  Oxford RobotCar datasets  KITTI  Three-dimensional displays  Laser radar  Training  Estimation  Two dimensional displays  Robot sensing systems 
Abstract: Learning-based ego-motion estimation approaches have recently drawn strong interest from researchers, mostly focusing on visual perception. A few learning-based approaches using Light Detection and Ranging (LiDAR) have been re-ported; however, they heavily rely on a supervised learning manner. Despite the meaningful performance of these approaches, supervised training requires ground-truth pose labels, which is the bottleneck for real-world applications. Differing from these approaches, we focus on unsupervised learning for LiDAR odometry (LO) without trainable labels. Achieving trainable LO in an unsupervised manner, we introduce the uncertainty-aware loss with geometric confidence, thereby al-lowing the reliability of the proposed pipeline. Evaluation on the KITTI, Complex Urban, and Oxford RobotCar datasets demonstrate the prominent performance of the proposed method compared to conventional model-based methods. The proposed method shows a comparable result against SuMa (in KITTI), LeGO-LOAM (in Complex Urban), and Stereo-VO (in Oxford RobotCar). The video and extra-information of the paper are described in https://sites.google.com/view/deeplo.


Title: SA-Net: Robust State-Action Recognition for Learning from Observations
Key Words: control engineering computing  image colour analysis  learning (artificial intelligence)  manipulators  mobile robots  neural nets  robot vision  robust state-action recognition  LfO methods  SA-Net  RGB-D data streams  replicated robotic applications  mobile ground robots  robotic manipulator  physical robot  deep neural network architecture  learning from observation  Image recognition  Task analysis  Robot sensing systems  Feature extraction  Robot kinematics  Object detection 
Abstract: Learning from observation (LfO) offers a new paradigm for transferring task behavior to robots. LfO requires the robot to observe the task being performed and decompose the sensed streaming data into sequences of state-action pairs, which are then input to LfO methods. Thus, recognizing the state-action pairs correctly and quickly in sensed data is a crucial prerequisite. We present SA-Net a deep neural network architecture that recognizes state-action pairs from RGB-D data streams. SA-Net performs well in two replicated robotic applications of LfO - one involving mobile ground robots and another involving a robotic manipulator - which demonstrates that the architecture could generalize well to differing contexts. Comprehensive evaluations including deployment on a physical robot show that SA-Net significantly improves on the accuracy of the previous methods under various conditions.


Title: A Generative Approach for Socially Compliant Navigation
Key Words: learning (artificial intelligence)  mobile robots  optimisation  path planning  robot vision  socially compliant navigation  robots navigation  socially compliant behavior  optimization objectives  inverse reinforcement learning approaches  natural behavior  generative navigation algorithm  navigation path  latent social rules  trained social navigation behavior  NaviGAN  Navigation  Robots  Force  Generators  Trajectory  Learning (artificial intelligence)  Collision avoidance 
Abstract: Robots navigating in human crowds need to optimize their paths not only for their task performance but also for their compliance to social norms. One of the key challenges in this context is the lack of standard metrics for evaluating and optimizing a socially compliant behavior. Existing works in social navigation can be grouped according to the differences in their optimization objectives. For instance, the reinforcement learning approaches tend to optimize on the comfort aspect of the socially compliant navigation, whereas the inverse reinforcement learning approaches are designed to achieve natural behavior. In this paper, we propose NaviGAN, a generative navigation algorithm that jointly optimizes both of the comfort and naturalness aspects. Our approach is designed as an adversarial training framework that can learn to generate a navigation path that is both optimized for achieving a goal and for complying with latent social rules. A set of experiments has been carried out on multiple datasets to demonstrate the strengths of the proposed approach quantitatively. We also perform extensive experiments using a physical robot in a realworld environment to qualitatively evaluate the trained social navigation behavior. The video recordings of the robot experiments can be found in the link: https://youtu.be/61blDymjCpw.


Title: Scalable Multi-Task Imitation Learning with Autonomous Improvement
Key Words: learning (artificial intelligence)  learning systems  robots  task analysis  deploying learning-based systems  scalable multitask imitation learning  sparse task-agnostic reward signals  reinforcement learning algorithms  continuous improvement  prior imitation learning approaches  initial demonstration dataset  learned latent space  multitask demonstration data  multitask setting  autonomous improvement  supervised imitation  autonomous data collection  imitation learning system  robot learning  stable approach  Task analysis  Robots  Learning (artificial intelligence)  Standards  Trajectory  Data collection  Learning systems 
Abstract: While robot learning has demonstrated promising results for enabling robots to automatically acquire new skills, a critical challenge in deploying learning-based systems is scale: acquiring enough data for the robot to effectively generalize broadly. Imitation learning, in particular, has remained a stable and powerful approach for robot learning, but critically relies on expert operators for data collection. In this work, we target this challenge, aiming to build an imitation learning system that can continuously improve through autonomous data collection, while simultaneously avoiding the explicit use of reinforcement learning, to maintain the stability, simplicity, and scalability of supervised imitation. To accomplish this, we cast the problem of imitation with autonomous improvement into a multi-task setting. We utilize the insight that, in a multi-task setting, a failed attempt at one task might represent a successful attempt at another task. This allows us to leverage the robot's own trials as demonstrations for tasks other than the one that the robot actually attempted. Using an initial dataset of multitask demonstration data, the robot autonomously collects trials which are only sparsely labeled with a binary indication of whether the trial accomplished any useful task or not. We then embed the trials into a learned latent space of tasks, trained using only the initial demonstration dataset, to draw similarities between various trials, enabling the robot to achieve one-shot generalization to new tasks. In contrast to prior imitation learning approaches, our method can autonomously collect data with sparse supervision for continuous improvement, and in contrast to reinforcement learning algorithms, our method can effectively improve from sparse, task-agnostic reward signals.


Title: A New Path Planning Architecture to Consider Motion Uncertainty in Natural Environment
Key Words: collision avoidance  mobile robots  motion control  probability  random processes  trees (mathematics)  uncertain systems  rough environments  uncertainty propagation  rapidly-exploring random tree  position uncertainty  motion uncertainty  natural environment  wheeled robots  path planning architecture  path-following  path replanning  probability  collision avoidance  Uncertainty  Robot kinematics  Path planning  Mobile robots  Planning  Global Positioning System 
Abstract: This paper proposes a new path planning algorithm to consider motion uncertainty for wheeled robots in rough environments. The proposed method uses particles to express the uncertainty propagation in complicated environments constructed with various types of terrain. Also, RRT (Rapidly-exploring Random Tree) is expanded based on the uncertainty of each node in order to prevent increasing the accumulated position uncertainty. As a result, the generated path reduces the times of path-following and re-planning based on inaccurate localization information. The effectiveness of the proposed method is evaluated in simulation using motion uncertainty models obtained by experiments. The results show that the proposed method decreases the position uncertainty while keeping the probability to avoid collisions and to reach the goal area compared with conventional approaches.


Title: Revisiting the Asymptotic Optimality of RRT*
Key Words: computational complexity  sampling methods  search problems  trees (mathematics)  mathematically-rigorous proof  asymptotic optimality  RRT*  asymptotically-optimal motion planning  optimality proof  sampling-based algorithms  connection radius  Robustness  Robots  Heuristic algorithms  Manganese  Planning  Aerodynamics  Safety 
Abstract: RRT* is one of the most widely used sampling-based algorithms for asymptotically-optimal motion planning. RRT* laid the foundations for optimality in motion planning as a whole, and inspired the development of numerous new algorithms in the field, many of which build upon RRT* itself. In this paper, we first identify a logical gap in the optimality proof of RRT*, which was developed by Karaman and Frazzoli (2011). Then, we present an alternative and mathematically-rigorous proof for asymptotic optimality. Our proof suggests that the connection radius used by RRT* should be increased from γ (log n/n)1/d to γ' (log n/n)1/(d+1) in order to account n n for the additional dimension of time that dictates the samples' ordering. Here γ, γ' are constants, and n, d are the number of samples and the dimension of the problem, respectively.


Title: Sample Complexity of Probabilistic Roadmaps via ε-nets
Key Words: computational complexity  deterministic algorithms  graph theory  probability  shortest δ-clear path  sample complexity  probabilistic roadmaps  ε-nets  optimality guarantees  deterministic sampling distribution  motion planning problem  parameter completeness  Planning  Complexity theory  Probabilistic logic  Two dimensional displays  Robots  Collision avoidance  Benchmark testing 
Abstract: We study fundamental theoretical aspects of probabilistic roadmaps (PRM) in the finite time (non-asymptotic) regime. In particular, we investigate how completeness and optimality guarantees of the approach are influenced by the underlying deterministic sampling distribution X and connection radius r > 0. We develop the notion of (δ, ε)-completeness of the parameters X, r, which indicates that for every motion-planning problem of clearance at least δ > 0, PRM using X, r returns a solution no longer than 1+ε times the shortest δ-clear path. Leveraging the concept of e-nets, we characterize in terms of lower and upper bounds the number of samples needed to guarantee (δ, ε)-completeness. This is in contrast with previous work which mostly considered the asymptotic regime in which the number of samples tends to infinity. In practice, we propose a sampling distribution inspired by e-nets that achieves nearly the same coverage as grids while using fewer samples.


Title: Reinforcement Learning Based Manipulation Skill Transferring for Robot-assisted Minimally Invasive Surgery
Key Words: end effectors  Gaussian processes  human-robot interaction  learning (artificial intelligence)  manipulator dynamics  medical robotics  motion control  regression analysis  surgery  complex tasks demonstrations  reinforcement learning algorithm based manipulation skill transferring technique  robot-assisted minimally invasive surgery  Gaussian mixture model  Gaussian mixture regression  multiple demonstrations  trial phase performed offline  practical surgical operation  KUKA LWR4+ robot  human manipulation skill  surgical robots  Learning (artificial intelligence)  Robots  Surgery  Trajectory  Task analysis  Shape  Education 
Abstract: The complexity of surgical operation can be released significantly if surgical robots can learn the manipulation skills by imitation from complex tasks demonstrations such as puncture, suturing, and knotting, etc.. This paper proposes a reinforcement learning algorithm based manipulation skill transferring technique for robot-assisted Minimally Invasive Surgery by Teaching by Demonstration. It employed Gaussian mixture model and Gaussian mixture Regression based dynamic movement primitive to model the high-dimensional human-like manipulation skill after multiple demonstrations. Furthermore, this approach fascinates the learning and trial phase performed offline, which reduces the risks and cost for the practical surgical operation. Finally, it is demonstrated by transferring manipulation skills for reaching and puncture using a KUKA LWR4+ robot in a lab setup environment. The results show the effectiveness of the proposed approach for modelling and learning of human manipulation skill.


Title: Safe Mission Planning under Dynamical Uncertainties
Key Words: collision avoidance  mobile robots  Monte Carlo methods  uncertain systems  uncertainty model  path planning  dynamical uncertainties  probabilistic model  safe robot mission planning  Monte Carlo method  collision free path  Uncertainty  Planning  Automata  Computational modeling  Hazards 
Abstract: This paper considers safe robot mission planning in uncertain dynamical environments. This problem arises in applications such as surveillance, emergency rescue, and autonomous driving. It is a challenging problem due to mod-eling and integrating dynamical uncertainties into a safe planning framework, and finding a solution in a computationally tractable way. In this work, we first develop a probabilistic model for dynamical uncertainties. Then, we provide a framework to generate a path that maximizes safety for complex missions by incorporating the uncertainty model. We also devise a Monte Carlo method to obtain a safe path efficiently. Finally, we evaluate the performance of our approach and compare it to potential alternatives in several case studies.


Title: A Morphable Aerial-Aquatic Quadrotor with Coupled Symmetric Thrust Vectoring
Key Words: autonomous aerial vehicles  autonomous underwater vehicles  design engineering  helicopters  mobile robots  stability  single design difficult  normal aerial vehicles  rotational acceleration  quadrotor based vehicle  vehicle body  design considerations  aerial-aquatic quadrotor  coupled symmetric thrust vectoring  aerial-aquatic vehicles  fluid resistance  energy efficient position  morphable aerial-aquatic quadrotor  mechanical actuation  static stability  Buoyancy  Robots  Force  Torque  Prototypes  Propellers 
Abstract: Hybrid aerial-aquatic vehicles have the unique ability of travelling in both air and water and can benefit from both lower fluid resistance in air and energy efficient position holding in water. However, they have to address the differing requirements which make optimising a single design difficult. While existing examples have shown the possibility of such vehicles, they are mostly structurally identical to normal aerial vehicles with minor adjustments to work underwater. Instead of using rotational acceleration to direct a component of thrust in surge and sway, we propose a quadrotor based vehicle that tilts its rotors about the respective arm so that a larger component of thrust can be directed in the lateral plane or in the opposite direction without rotating the vehicle body. A small-scale prototype of this design is presented here, detailing the design considerations including mechanical actuation, static stability and waterproofing.


Title: An Autonomous Intercept Drone with Image-based Visual Servo
Key Words: autonomous aerial vehicles  cameras  robot vision  visual servoing  autonomous intercept drone  unwanted drone  radio wave gun  image-based visual servo algorithm  Cameras  Visualization  Mathematical model  Drones  Channel models  Servomotors  Angular velocity 
Abstract: For most people on the ground, facing an unwanted drone buzzing around overhead, there is not a lot that we can do, especially if it is out of gun (radio wave gun or shotgun) range. A solution to this is to use intercept drones that seek out and bring down other drones. In order to make the interception autonomous, an image-based visual servo algorithm is designed with a forward-looking monocular camera. The control command, namely the angular velocity and thrust, is generated for intercept drones to implement accurate and fast interception. The proposed method is demonstrated in both hardware-in-the-loop simulation and demonstrative flight experiments.


Title: On the Human Control of a Multiple Quadcopters with a Cable-suspended Payload System
Key Words: actuators  attitude control  autonomous aerial vehicles  control system synthesis  helicopters  mobile robots  multi-robot systems  position control  cable-suspended payload system  human control  multiple quadcopters system  leader quadcopter  payload attitude controller  cable attitude controller  quadcopter-payload system  Payloads  Oscillators  Angular velocity  Attitude control  Vehicle dynamics  Trajectory  Quadcopters  Human control  Cable-suspended payload  Collaborative transportation  Multi-agents 
Abstract: A quadcopter is an under-actuated system with only four control inputs for six degrees of freedom, and yet the human control of a quadcopter is simple enough to be learned with some practice. In this work, we consider the problem of human control of a multiple quadcopters system to transport a cable-suspended payload. The coupled dynamics of the system, due to the inherent physical constraints, is used to develop a leader-follower architecture where the leader quadcopter is controlled directly by a human operator and the followers are controlled with the proposed Payload Attitude Controller and Cable Attitude Controller. Experiments, where a human operator flew a two quadcopters system to transport a cable-suspended payload, were conducted to study the performance of proposed controller. The results demonstrated successful implementation of human control in these systems. This work presents the possibility of enabling manual control for on-the-go maneuvering of the quadcopter-payload system which motivates aerial transportation in the unknown environments.


Title: A*3D Dataset: Towards Autonomous Driving in Challenging Environments
Key Words: image annotation  image colour analysis  mobile robots  object detection  optical radar  road vehicle radar  stereo image processing  traffic engineering computing  A*3D dataset  self-driving cars  3D object detection  3D object annotations  autonomous driving research  nuScenes dataset  KITTI dataset  high-density images  LiDAR data  RGB images  computer vision tasks  Three-dimensional displays  Laser radar  Autonomous vehicles  Cameras  Calibration  Object detection  Robot sensing systems 
Abstract: With the increasing global popularity of self-driving cars, there is an immediate need for challenging real-world datasets for benchmarking and training various computer vision tasks such as 3D object detection. Existing datasets either represent simple scenarios or provide only day-time data. In this paper, we introduce a new challenging A*3D dataset which consists of RGB images and LiDAR data with a significant diversity of scene, time, and weather. The dataset consists of high-density images (≈ 10 times more than the pioneering KITTI dataset), heavy occlusions, a large number of nighttime frames (≈ 3 times the nuScenes dataset), addressing the gaps in the existing datasets to push the boundaries of tasks in autonomous driving research to more challenging highly diverse environments. The dataset contains 39K frames, 7 classes, and 230K 3D object annotations. An extensive 3D object detection benchmark evaluation on the A*3D dataset for various attributes such as high density, day-time/night-time, gives interesting insights into the advantages and limitations of training and testing 3D object detection in real-world setting.


Title: How to Keep HD Maps for Automated Driving Up To Date
Key Words: cartography  data privacy  road vehicles  traffic engineering computing  dedicated mapping vehicles  low traversal frequencies  anonymized data  up-to-dateness  crowdsourced data  automatically trigger map update jobs  map patches  date HD map  automated driving functions  HD maps  automotive high definition digital map generation  automated driving up to date  Roads  Vehicle dynamics  Topology  Robot sensing systems  Shape 
Abstract: The current state of the art in automotive high definition digital (HD) map generation based on dedicated mapping vehicles cannot reliably keep these maps up to date because of the low traversal frequencies. Anonymized data collected from the fleet of vehicles that is already on the road provides a huge potential to outperform such state of the art solutions in robustness, safety and up-to-dateness of the map while achieving comparable quality. We thus present a solution based on crowdsourced data to (i) detect changes in the map independent of the type of change, (ii) automatically trigger map update jobs for parts of the map, and (iii) create and integrate map patches to keep the map always up to date. The developed solution provides a crowdsourced up to date HD map to make reliable prior information on lane markings and road edges available to automated driving functions.


Title: UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes
Key Words: cameras  image matching  image registration  inertial navigation  optical radar  satellite navigation  urban canyon  urban terrain  Hong Kong  San Francisco  IMU  GNSS-based solutions  LIDAR  global navigation satellite system  urban scene localization  urban scene mapping  inertia measurement units  camera-based methods  inertia navigation  visual feature matching  point cloud registration  full sensor suite dataset  UrbanLoco  Global navigation satellite system  Cameras  Laser radar  Urban areas  Robot sensing systems  Trajectory  Satellites 
Abstract: Mapping and localization is a critical module of autonomous driving, and significant achievements have been reached in this field. Beyond Global Navigation Satellite System (GNSS), research in point cloud registration, visual feature matching, and inertia navigation has greatly enhanced the accuracy and robustness of mapping and localization in different scenarios. However, highly urbanized scenes are still challenging: LIDAR- and camera-based methods perform poorly with numerous dynamic objects; the GNSS-based solutions experience signal loss and multi-path problems; the inertia measurement units (IMU) suffer from drifting. Unfortunately, current public datasets either do not adequately address this urban challenge or do not provide enough sensor information related to map-ping and localization. Here we present UrbanLoco: a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers. Our dataset includes a wide variety of urban terrains: urban canyons, bridges, tunnels, sharp turns, etc. More importantly, our dataset includes information from LIDAR, cameras, IMU, and GNSS receivers. Now the dataset is publicly available through the link in the footnote 1.


Title: Map As the Hidden Sensor: Fast Odometry-Based Global Localization
Key Words: distance measurement  mobile robots  path planning  sensors  tensors  odometry-based global localization  ambiguous observations  odometry drift  blind robots  robot state  belief tensor  map-corrected odometry localization  map traversability  robotics applications  hidden sensor  Robot sensing systems  Tensile stress  Trajectory  Robustness  Uncertainty  Real-time systems 
Abstract: Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.


Title: Ultra-High-Accuracy Visual Marker for Indoor Precise Positioning
Key Words: attitude control  attitude measurement  autonomous aerial vehicles  indoor navigation  mobile robots  position control  robot vision  local positioning  marker coordinate system  high-accuracy global positioning  ultra-high-accuracy visual marker  indoor precise positioning  indoor positioning  indoor mobile robots  drones  general-purpose technology  attitude measurement  multiple dynamic moires  lenticular lens  attitude estimation error  marker position error  reprojection error  size 10.0 m  size 1.0 cm  size 10.0 cm  attitude accuracy  Visualization  Lenses  Position measurement  Cameras  Measurement uncertainty  Pose estimation 
Abstract: Indoor positioning technology is essential for indoor mobile robots and drones. However, there has never been a general-purpose technology or infrastructure that enables indoor positioning with an accuracy of less than 10 cm. We have developed an attitude measurement method using multiple dynamic moires with a lenticular lens and developed an ultra- high-accuracy visual marker with an attitude estimation error of less than 0.1°. We also developed a calculation method that minimizes the marker position error by reminimizing reprojection error using its good attitude accuracy. We proved that accurate local positioning with a position error of about 1 cm in a marker coordinate system is possible even when a marker is shot from a distance of 10 m. In addition, a demonstration test was performed in a public space, and it was shown that high-accuracy global positioning with a position error of about 10 cm is possible.


Title: Accurate position tracking with a single UWB anchor
Key Words: inertial systems  Kalman filters  mobile robots  nonlinear filters  object tracking  observability  position measurement  SLAM (robots)  ultra wideband technology  velocity measurement  ultrawideband technology  UWB anchor  UWB range  moving robot tracking  position tracking  robotic applications  localization systems  optical tracking  9 DoF inertial measurement unit  UWB ranging source  UWB technology  robot speed estimation  orientation estimation  IMU sensor  observability  extended Kalman filter  EKF  robot pose estimation  Robot sensing systems  Estimation  Observability  Velocity measurement  Distance measurement  Mobile robots 
Abstract: Accurate localization and tracking are a fundamental requirement for robotic applications. Localization systems like GPS, optical tracking, simultaneous localization and mapping (SLAM) are used for daily life activities, research, and commercial applications. Ultra-wideband (UWB) technology provides another venue to accurately locate devices both indoors and outdoors. In this paper, we study a localization solution with a single UWB anchor, instead of the traditional multi-anchor setup. Besides the challenge of a single UWB ranging source, the only other sensor we require is a low-cost 9 DoF inertial measurement unit (IMU). Under such a configuration, we propose continuous monitoring of UWB range changes to estimate the robot speed when moving on a line. Combining speed estimation with orientation estimation from the IMU sensor, the system becomes temporally observable. We use an Extended Kalman Filter (EKF) to estimate the pose of a robot. With our solution, we can effectively correct the accumulated error and maintain accurate tracking of a moving robot.


Title: Preference-Based Learning for Exoskeleton Gait Optimization
Key Words: gait analysis  learning (artificial intelligence)  legged locomotion  optimisation  wearable robots  personalized gait optimization framework  lower-body exoskeleton  numerical objectives  preference-based interactive learning  CoSpar algorithm  pairwise preferences  exoskeleton walking  nonintuitive behavior  numerical feedback  human walking trajectory features  user-preferred parameters  adapting personalizing exoskeletons  exoskeleton gait optimization  Exoskeletons  Legged locomotion  Optimization  Bayes methods  Reliability  Trajectory 
Abstract: This paper presents a personalized gait optimization framework for lower-body exoskeletons. Rather than optimizing numerical objectives such as the mechanical cost of transport, our approach directly learns from user prefer-ences, e.g., for comfort. Building upon work in preference-based interactive learning, we present the CoSpar algorithm. CoSpar prompts the user to give pairwise preferences between trials and suggest improvements; as exoskeleton walking is a non-intuitive behavior, users can provide preferences more easily and reliably than numerical feedback. We show that CoSpar performs competitively in simulation and demonstrate a prototype implementation of CoSpar on a lower-body exoskeleton to optimize human walking trajectory features. In the experiments, CoSpar consistently found user-preferred parameters of the exoskeleton's walking gait, which suggests that it is a promising starting point for adapting and personalizing exoskeletons (or other assistive devices) to individual users.


Title: Adaptive Neural Trajectory Tracking Control for Flexible-Joint Robots with Online Learning
Key Words: actuators  adaptive control  backpropagation  control system synthesis  feedforward  flexible manipulators  manipulator dynamics  neurocontrollers  nonlinear control systems  position control  stability  online backpropagation  collaborative robots  multilayer neural network  control architecture  flexible joint dynamics  control bandwidth  control design  space manipulators  online learning  flexible-joint robots  adaptive neural trajectory tracking control  series-elastic joint actuators  joint flexibility  Baxter robot  commanded joint position  outer loop control  nonlinear basis functions  internal weights  tracking error  output layer weights  linear output layer  robot dynamics  linear-in-parameter representation  feedforward control  approximate unknown dynamics  Artificial neural networks  Trajectory  Manipulator dynamics  Aerodynamics 
Abstract: Collaborative robots and space manipulators contain significant joint flexibility. It complicates the control design, compromises the control bandwidth, and limits the tracking accuracy. The imprecise knowledge of the flexible joint dynamics compounds the challenge. In this paper, we present a new control architecture for controlling flexible-joint robots. Our approach uses a multi-layer neural network to approximate unknown dynamics needed for the feedforward control. The network may be viewed as a linear-in-parameter representation of the robot dynamics, with the nonlinear basis of the robot dynamics connected to the linear output layer. The output layer weights are updated based on the tracking error and the nonlinear basis. The internal weights of the nonlinear basis are updated by online backpropagation to further reduce the tracking error. To use time scale separation to reduce the coupling of the two steps - the update of the internal weights is at a lower rate compared to the update of the output layer weights. With the update of the output layer weights, our controller adapts quickly to the unknown dynamics change and disturbances (such as attaching a load). The update of the internal weights would continue to improve the converge of the nonlinear basis functions. We show the stability of the proposed scheme under the "outer loop" control, where the commanded joint position is considered as the control input. Simulation and physical experiments are conducted to demonstrate the performance of the proposed controller on a Baxter robot, which exhibits significant joint flexibility due to the series-elastic joint actuators.


Title: Adaptive Unknown Object Rearrangement Using Low-Cost Tabletop Robot
Key Words: learning (artificial intelligence)  mobile robots  object detection  robot vision  tabletop environment  object rearrangement  low-cost tabletop robot  object rearrangement planning  learning-based methods  single-step interaction  adaptive learning procedure  size 3.5 cm  Physics  Engines  Task analysis  Optimization  Robots  Adaptation models  Planning 
Abstract: Studies on object rearrangement planning typically consider known objects. Some learning-based methods can predict the movement of an unknown object after single-step interaction, but require intermediate targets, which are generated manually, to achieve the rearrangement task. In this work, we propose a framework for unknown object rearrangement. Our system first models an object through a small-amount of identification actions and adjust the model parameters during task execution. We implement the proposed framework based on a low-cost tabletop robot (under 180 USD) to demonstrate the advantages of using a physics engine to assist action prediction. Experimental results reveal that after running our adaptive learning procedure, the robot can successfully arrange a novel object using an average of five discrete pushes on our tabletop environment and satisfy a precise 3.5 cm translation and 5° rotation criterion.


Title: Unsupervised Learning and Exploration of Reachable Outcome Space
Key Words: search problems  unsupervised learning  reachable outcome space  sparse rewards settings  reinforcement learning  learning process  search strategy  TAXONS  task agnostic exploration  population-based divergent-search approach  diverse policies  high-dimensional observation  task-specific information  low-dimensional outcome space  learned outcome space  ground-truth outcome space  unsupervised learning  Task analysis  Robots  Training  Space exploration  Aerospace electronics  Extraterrestrial measurements 
Abstract: Performing Reinforcement Learning in sparse rewards settings, with very little prior knowledge, is a challenging problem since there is no signal to properly guide the learning process. In such situations, a good search strategy is fundamental. At the same time, not having to adapt the algorithm to every single problem is very desirable. Here we introduce TAXONS, a Task Agnostic eXploration of Outcome spaces through Novelty and Surprise algorithm. Based on a population-based divergent-search approach, it learns a set of diverse policies directly from high-dimensional observations, without any task-specific information. TAXONS builds a repertoire of policies while training an autoencoder on the high-dimensional observation of the final state of the system to build a low-dimensional outcome space. The learned outcome space, combined with the reconstruction error, is used to drive the search for new policies. Results show that TAXONS can find a diverse set of controllers, covering a good part of the ground-truth outcome space, while having no information about such space.


Title: Context-aware Cost Shaping to Reduce the Impact of Model Error in Receding Horizon Control
Key Words: mobile robots  predictive control  probability  remotely operated vehicles  robot dynamics  stochastic systems  model error  receding horizon control  repetitive path-following task  robot dynamics  simple learned dynamics model  MPC horizon  stochastic MPC  prediction horizon  online model learning  ground robot  context-aware cost shaping  stochastic model predictive control  Robots  Computational modeling  Predictive models  Aerodynamics  Cost function  Task analysis  Stochastic processes 
Abstract: This paper presents a method to enable a robot using stochastic Model Predictive Control (MPC) to achieve high performance on a repetitive path-following task. In particular, we consider the case where the accuracy of the model for robot dynamics varies significantly over the path-motivated by the fact that the models used in MPC must be computationally efficient, which limits their expressive power. Our approach is based on correcting the cost predicted using a simple learned dynamics model over the MPC horizon. This discourages the controller from taking actions that lead to higher cost than would have been predicted using the dynamics model. In addition, stochastic MPC provides a quantitative measure of safety by limiting the probability of violating state and input constraints over the prediction horizon. Our approach is unique in that it combines both online model learning and cost learning over the prediction horizon and is geared towards operating a robot in changing conditions. We demonstrate our algorithm in simulation and experiment on a ground robot that uses a stereo camera for localization.


Title: Design and Kinematic Modeling of a Novel Steerable Needle for Image-Guided Insertion
Key Words: biomedical ultrasonics  medical image processing  medical robotics  needles  path planning  pose estimation  surgery  tumours  image-guided insertion  novel steerable needle  kinematics model  passive needle tip articulation  needle path consistency  articulated tip  needle design  tissue mechanics  needle-tissue interaction  needle placement  percutaneous tumor ablation  biopsy tumor ablation  Needles  Kinematics  Fasteners  Path planning  Shape  Electron tubes  Laser beam cutting 
Abstract: Needle-based procedures, such as biopsy and percutaneous tumor ablation, highly depend on the accuracy of needle placement. The accuracy is significantly affected by the needle-tissue interaction no matter what needles (straight or steerable) are used. Due to the unknown tissue mechanics, it is challenging to achieve high accuracy in practice. This paper hence proposes a needle design with an articulated tip for increased steerability and improved needle path consistency. Due to the passive needle tip articulation, tissue mechanics always plays a dominant role such that the needle creates similar paths with approximately piece-wise constant curvature in different tissues. Kinematics model for the proposed needle is presented. The algorithms of path planning and needle tip pose estimation under external imaging modality are developed. Experimental verifications were conducted to demonstrate the needle's steerability as well as the target-reaching capability with obstacles avoidance.


Title: Robotic needle insertion in moving soft tissues using constraint-based inverse Finite Element simulation
Key Words: biological tissues  end effectors  finite element analysis  inverse problems  medical image processing  medical robotics  needles  robotic needle insertion  soft tissues  robotic steering  flexible needle  predefined path  inverse problem  robot end effector  constraint-based formulation  simulation-guided needle insertion  direct simulation  respiratory motion  numerical simulation  constraint-based inverse finite element simulation  Needles  Robots  Mathematical model  Linear programming  Computational modeling  Numerical models  Inverse problems 
Abstract: This paper introduces a method for robotic steering of a flexible needle inside moving and deformable tissues. The method relies on a set of objective functions allowing to automatically steer the needle along a predefined path. In order to follow the desired trajectory, an inverse problem linking the motion of the robot end effector with the objective functions is solved using a Finite Element simulation. The main contribution of the article is the new constraint-based formulation of the objective functions allowing to: 1) significantly reduce the computation time; 2) increase the accuracy and stability of the simulation-guided needle insertion. The method is illustrated, and its performances are characterized in a realistic framework, using a direct simulation of the respiratory motion generated from in vivo data of a pig. Despite the highly non-linear behavior of the numerical simulation and the significant deformations occurring during the insertion, the obtained performances enable the possibility to follow the trajectory with the desired accuracy for medical purpose.


Title: Collaborative Robot-Assisted Endovascular Catheterization with Generative Adversarial Imitation Learning
Key Words: blood vessels  catheters  diagnostic radiography  haemodynamics  learning (artificial intelligence)  medical image processing  medical robotics  pulsatile flow  surgery  reduced procedural duration  deep reinforcement learning technologies  complex endovascular tasks  reduced fatigue  cognitive workload  model-based approaches  model-free generative adversarial imitation learning  standard arterial catherization task  automation policies  catheter motions  collaborative robot-assisted endovascular catheterization  master-slave systems  clinical benefits  radiation doses  vascular anatomical model  Catheters  Robots  Task analysis  Training  Catheterization  Instruments  Surgery 
Abstract: Master-slave systems for endovascular catheterization have brought major clinical benefits including reduced radiation doses to the operators, improved precision and stability of the instruments, as well as reduced procedural duration. Emerging deep reinforcement learning (RL) technologies could potentially automate more complex endovascular tasks with enhanced success rates, more consistent motion and reduced fatigue and cognitive workload of the operators. However, the complexity of the pulsatile flows within the vasculature and non-linear behavior of the instruments hinder the use of model-based approaches for RL. This paper describes model-free generative adversarial imitation learning to automate a standard arterial catherization task. The automation policies have been trained in a pre-clinical setting. Detailed validation results show high success rates after skill transfer to a different vascular anatomical model. The quality of the catheter motions also shows less mean and maximum contact forces compared to manual-based approaches.


Title: GA3C Reinforcement Learning for Surgical Steerable Catheter Path Planning
Key Words: biomedical MRI  catheters  collision avoidance  learning (artificial intelligence)  medical image processing  medical robotics  path planning  robot kinematics  trajectory smoothness  GA3C Reinforcement Learning  surgical steerable catheter path planning  path planning algorithms  steerable catheters  anatomical obstacles avoidance  insertion length  needle kinematics  smooth trajectories  path planning problem  reinforcement learning problem  trajectory planning model  optimal trajectories  obstacle clearance  kinematic constraints  MRI images processing  path planning model  curvilinear trajectories  RRT* algorithms  obstacle avoidance  Trajectory  Needles  Three-dimensional displays  Learning (artificial intelligence)  Catheters  Kinematics 
Abstract: Path planning algorithms for steerable catheters, must guarantee anatomical obstacles avoidance, reduce the insertion length and ensure the compliance with needle kinematics. The majority of the solutions in literature focuses on graph based or sampling based methods, both limited by the impossibility to directly obtain smooth trajectories. In this work we formulate the path planning problem as a reinforcement learning problem and show that the trajectory planning model, generated from the training, can provide the user with optimal trajectories in terms of obstacle clearance and kinematic constraints. We obtain 2D and 3D environments from MRI images processing and we implement a GA3C algorithm to create a path planning model, able to generalize on different patients anatomies. The curvilinear trajectories obtained from the model in 2D and 3D environments are compared to the ones obtained by A* and RRT* algorithms. Our method achieves state-of-the-art performances in terms of obstacle avoidance, trajectory smoothness and computational time proving this algorithm as valid planning method for complex environments.


Title: MPC-based Controller with Terrain Insight for Dynamic Legged Locomotion
Key Words: convolutional neural nets  hydraulic actuators  legged locomotion  neurocontrollers  predictive control  robot dynamics  on-board mapping  contact sequence task  convolutional neural network  model predictive controller  on-board sensing  MPC-based controller  terrain insight  dynamic legged locomotion  hydraulically actuated quadruped robot HyQReal  Legged locomotion  Trajectory  Task analysis  Computational modeling  Dynamics  Foot 
Abstract: We present a novel control strategy for dynamic legged locomotion in complex scenarios that considers information about the morphology of the terrain in contexts when only on-board mapping and computation are available. The strategy is built on top of two main elements: first a contact sequence task that provides safe foothold locations based on a convolutional neural network to perform fast and continuous evaluation of the terrain in search of safe foothold locations; then a model predictive controller that considers the foothold locations given by the contact sequence task to optimize target ground reaction forces. We assess the performance of our strategy through simulations of the hydraulically actuated quadruped robot HyQReal traversing rough terrain under realistic on-board sensing and computing conditions.


Title: An Adaptive Supervisory Control Approach to Dynamic Locomotion Under Parametric Uncertainty
Key Words: adaptive control  control system synthesis  feedback  legged locomotion  parameter estimation  robot dynamics  uncertain systems  feedback control  estimator design  dynamic locomotion  parametric uncertainty  robotic systems  parameter estimation  adaptive supervisory control  dynamically walking bipedal robot  Switches  Uncertainty  Robots  Supervisory control  Adaptive control  Libraries 
Abstract: This paper presents an adaptive control scheme for robotic systems that operate in the face of-potentially large-structured uncertainty. The proposed adaptive controller employs an on-line supervisor that utilizes logic-based switching among a finite set of controllers to identify uncertain parameters, and adapt the behavior of the system based on a current estimate of their value. To achieve this, the adaptive control approach in this paper combines on-line parameter estimation and feedback control while avoiding some of the inherent difficulties of classical adaptive control strategies. Furthermore, the proposed supervisory control architecture is modular as it relies on established "off-the-shelf" feedback control law and estimator design approaches, instead of cus-tomizing the overall design to the specific requirements of an adaptive control algorithm. We demonstrate the efficacy of the method on the problem of a dynamically-walking bipedal robot delivering a payload of unknown mass, and show that, by switching to the controller that is the "best" according to a current estimate of the uncertainty, the system maintains a low energy cost during its operation.


Title: Joint Space Position/Torque Hybrid Control of the Quadruped Robot for Locomotion and Push Reaction
Key Words: force control  legged locomotion  motion control  position control  robot dynamics  torque control  torque control  contact-force-control  joint space hybrid control  legged robot platform  hybrid control algorithm  robot displayed stability  mammal-type quadruped robot  dynamic locomotion  push reaction abilities  Cartesian space  external push disturbances  Force  Aerospace electronics  Legged locomotion  Torque  Robot kinematics  Dynamics 
Abstract: This paper proposes a novel algorithm for joint space position/torque hybrid control of a mammal-type quadruped robot. With this control algorithm, the robot demonstrated both dynamic locomotion and push reaction abilities without the need for torque control in the ab/ad joints. Based on the tipping and slipping condition of the legged robot, we showed that reaction to a typical push in the horizontal direction does not require full contact-force-control in the frontal plane. Furthermore, we showed that position/torque hybrid control in Cartesian space is directly applicable to joint space hybrid control due to the joint configuration of the quadruped robot. We conducted experiments on our legged robot platform to verify the performance of our hybrid control algorithm. With this approach, the robot displayed stability while walking and reacting to external push disturbances.


Title: Improved Performance on Moving-Mass Hopping Robots with Parallel Elasticity
Key Words: actuators  elasticity  legged locomotion  robot dynamics  springs (mechanical)  stability  mechanical design  robotic hopping  moving-mass hopping robots  hop heights  single-spring model  double-spring model  hopping effort  parallel spring  hybrid systems models  rigorous trajectory optimization method  one-dimensional hopping robot  actuator  single spring  moving-mass robot  stable hopping  ground contact  Springs  Actuators  Force  Robot kinematics  Dynamics  Jacobian matrices 
Abstract: Robotic Hopping is challenging from the perspective of both modeling the dynamics as well as the mechanical design due to the short period of ground contact in which to actuate on the world. Previous work has demonstrated stable hopping on a moving-mass robot, wherein a single spring was utilized below the body of the robot. This paper finds that the addition of a spring in parallel to the actuator greatly improves the performance of moving mass hopping robots. This is demonstrated through the design of a novel one-dimensional hopping robot. For this robot, a rigorous trajectory optimization method is developed using hybrid systems models with experimentally tuned parameters. Simulation results are used to study the effects of a parallel spring on energetic efficiency, stability, and hopping effort. We find that the double-spring model had 2.5x better energy efficiency than the single-spring model, and was able to hop using 40% less peak force from the actuator. Furthermore, the double-spring model produces stable hopping without the need for stabilizing controllers. These concepts are demonstrated experimentally on a novel hopping robot, wherein hop heights up to 40cm were achieved with comparable efficiency and stability.


Title: Vision Aided Dynamic Exploration of Unstructured Terrain with a Small-Scale Quadruped Robot
Key Words: collision avoidance  gait analysis  legged locomotion  robot dynamics  fully sensorized Mini-Cheetah quadruped robot  unstructured terrain  dynamic exploration  dynamic trotting  highly irregular terrain  obstacle avoidance  foothold adjustment  evaluation algorithms  effective filtering  MIT Mini-Cheetah  Intel RealSense sensors  jumping  dynamic locomotion  effective sensor integration  walking speed  obstacle clearance  narrow paths  cluttered environments  rough terrain locomotion capability  rescue scenarios  disaster response  mobile platforms  legged robots  small-scale quadruped robot  size 0.3 m  mass 9.0 kg  Cameras  Robot vision systems  Robot kinematics  Legged locomotion 
Abstract: Legged robots have been highlighted as promising mobile platforms for disaster response and rescue scenarios because of their rough terrain locomotion capability. In cluttered environments, small robots are desirable as they can maneuver through small gaps, narrow paths, or tunnels. However small robots have their own set of difficulties such as limited space for sensors, limited obstacle clearance, and scaled-down walking speed. In this paper, we extensively address these difficulties via effective sensor integration and exploitation of dynamic locomotion and jumping. We integrate two Intel RealSense sensors into the MIT Mini-Cheetah, a 0.3 m tall, 9 kg quadruped robot. Simple and effective filtering and evaluation algorithms are used for foothold adjustment and obstacle avoidance. We showcase the exploration of highly irregular terrain using dynamic trotting and jumping with the small-scale, fully sensorized Mini-Cheetah quadruped robot.


Title: Distributed Attack-Robust Submodular Maximization for Multi-Robot Planning
Key Words: control system security  distributed algorithms  divide and conquer methods  multi-robot systems  optimisation  path planning  target tracking  distributed attack-robust submodular maximization  multirobot planning  swarm-robotics applications  multirobot motion  attack-robust algorithms  robust optimization  distributed robust maximization  DRM performance  multiple robots  denial-of-service attacks  DoS attacks  large-scale robotic applications  general-purpose distributed algorithm  divide-and-conquer approach  robot communication range  close-to-optimal performance  active target tracking  Planning  Target tracking  Robot kinematics  Partitioning algorithms  Robot sensing systems  Robustness 
Abstract: We aim to guard swarm-robotics applications against denial-of-service (DoS) attacks that result in withdrawals of robots. We focus on applications requiring the selection of actions for each robot, among a set of available ones, e.g., which trajectory to follow. Such applications are central in large-scale robotic applications, e.g., multi-robot motion planning for target tracking. But the current attack-robust algorithms are centralized, and scale quadratically with the problem size (e.g., number of robots). In this paper, we propose a general-purpose distributed algorithm towards robust optimization at scale, with local communications only. We name it distributed robust maximization (DRM). DRM proposes a divide-and-conquer approach that distributively partitions the problem among K cliques of robots. The cliques optimize in parallel, independently of each other. That way, DRM also offers computational speed-ups up to 1/K2 the running time of its centralized counterparts. K depends on the robots' communication range, which is given as input to DRM. DRM also achieves a close-to-optimal performance. We demonstrate DRM's performance in Gazebo and MATLAB simulations, in scenarios of active target tracking with multiple robots. We observe DRM achieves significant computational speed-ups (it is 3 to 4 orders faster) and, yet, nearly matches the tracking performance of its centralized counterparts.


Title: Multirobot Patrolling Against Adaptive Opponents with Limited Information
Key Words: mobile robots  multi-agent systems  multi-robot systems  optimisation  multirobot patrolling  adaptive opponents  patrolling problem  multiple agents  time consuming  Robot kinematics  Task analysis  Delays  Optimization  Games 
Abstract: We study a patrolling problem where multiple agents are tasked with protecting an environment where one or more adversaries are trying to compromise targets of varying value. The objective of the patrollers is to move between targets to quickly spot when an attack is taking place and then diffuse it. Differently from most related literature, we do not assume that attackers have full knowledge of the strategies followed by the patrollers, but rather build a model at run time through repeated observations of how often they visit certain targets. We study three different solutions to this problem. The first two partition the environment using either a fast heuristic or an exact method that is significantly more time consuming. The third method, instead does not partition the environment, but rather lets every patroller roam over the entire environment. After having identified strengths and weaknesses of each method, we contrast their performances against attackers using different algorithms to decide whether to attack or not.


Title: Distributed Optimization of Nonlinear, Non-Gaussian, Communication-Aware Information using Particle Methods
Key Words: entropy  mobile robots  multi-robot systems  optimisation  wireless sensor networks  mobile robotic sensor networks  neighbor robots  conditional mutual information  communication properties  specific measurement set  particle methods  information computation  distributed optimization  local utility design  communication-aware information gathering  sampling procedures  entropy reduction  Robot sensing systems  Optimization  Mutual information  Planning  Atmospheric measurements  Particle measurements 
Abstract: This paper presents a distributed optimization framework and its local utility design for communication-aware information gathering by mobile robotic sensor networks. The main idea of the optimization is that each robot decides based on its local utility that considers the decisions of other neighbor robots higher in a given hierarchy. The local utility is designed as conditional mutual information that captures sensing and communication properties. Sampling procedures using a specific measurement set and particle methods are applied to compute the designed utility, which allows nonlinear, non-Gaussian properties of targets, sensing, and communication. Simulation results describe the presented distributed optimization shows more improved estimates and entropy reduction than another approach that does not consider communication properties. Simulation results also verify the presented distributed optimization using the described approach for information computation has better results than using other approaches that simplify the communication-aware information.


Title: Targeted Drug Delivery: Algorithmic Methods for Collecting a Swarm of Particles with Uniform, External Forces
Key Words: computational complexity  deterministic algorithms  drug delivery systems  drugs  learning (artificial intelligence)  neural nets  optimisation  actuation steps  deterministic algorithms  targeted drug delivery  maze-like environment  vascular system  basic scenario  global external force  fluidic flow  deep learning  Magnetic resonance imaging  Robots  Tumors  Force  Blood  Electromagnetics  Machine learning 
Abstract: We investigate algorithmic approaches for targeted drug delivery in a complex, maze-like environment, such as a vascular system. The basic scenario is given by a large swarm of micro-scale particles ("agents") and a particular target region ("tumor") within a system of passageways. Agents are too small to contain on-board power or computation and are instead controlled by a global external force that acts uniformly on all particles, such as an applied fluidic flow or electromagnetic field. The challenge is to deliver all agents to the target region with a minimum number of actuation steps. We provide a number of results for this challenge. We show that the underlying problem is NP-hard, which explains why previous work did not provide provably efficient algorithms. We also develop a number of algorithmic approaches that greatly improve the worst-case guarantees for the number of required actuation steps. We evaluate our algorithmic approaches by a number of simulations, both for deterministic algorithms and searches supported by deep learning, which show that the performance is practically promising.


Title: Enhancing Bilevel Optimization for UAV Time-Optimal Trajectory using a Duality Gap Approach
Key Words: autonomous aerial vehicles  bang-bang control  duality (mathematics)  mobile robots  Newton method  nonlinear programming  path planning  sensitivity analysis  time optimal control  sensitivity analysis  NLP solvers  parametric optimization problems  quasiNewton method  geometric path  time-optimal velocity profile  hierarchical optimization  bang-bang control structure  nonlinear programming solvers  dynamic robotic vehicles  time-optimal trajectories  duality gap approach  UAV time-optimal trajectory  gap-free bilevel optimization  interior-point method  Trajectory  Cost function  Switches  Acceleration  Robustness  Robots 
Abstract: Time-optimal trajectories for dynamic robotic vehicles are difficult to compute even for state-of-the-art nonlinear programming (NLP) solvers, due to nonlinearity and bang-bang control structure. This paper presents a bilevel optimization framework that addresses these problems by decomposing the spatial and temporal variables into a hierarchical optimization. Specifically, the original problem is divided into an inner layer, which computes a time-optimal velocity profile along a given geometric path, and an outer layer, which refines the geometric path by a Quasi-Newton method. The inner optimization is convex and efficiently solved by interior-point methods. The gradients of the outer layer can be analytically obtained using sensitivity analysis of parametric optimization problems. A novel contribution is to introduce a duality gap in the inner optimization rather than solving it to optimality; this lets the optimizer realize warm-starting of the interior-point method, avoids non-smoothness of the outer cost function caused by active inequality constraint switching. Like prior bilevel frameworks, this method is guaranteed to return a feasible solution at any time, but converges faster than gap-free bilevel optimization. Numerical experiments on a drone model with velocity and acceleration limits show that the proposed method performs faster and more robustly than gap-free bilevel optimization and general NLP solvers.


Title: Constrained Sampling-based Trajectory Optimization using Stochastic Approximation
Key Words: approximation theory  discrete systems  gradient methods  optimal control  optimisation  sampling methods  stochastic processes  constrained problems  stochastic search  box control constraints  nonlinear state constraints  discrete dynamical systems  sampling-based trajectory optimization methodology  stochastic approximation  constrained sampling-based trajectory optimization  nonsmooth penalty functions  control inputs  truncated parameterized distributions  Heuristic algorithms  Trajectory optimization  Convergence  Approximation algorithms  Robots 
Abstract: We propose a sampling-based trajectory optimization methodology for constrained problems. We extend recent works on stochastic search to deal with box control constraints, as well as nonlinear state constraints for discrete dynamical systems. Regarding the former, our strategy is to optimize over truncated parameterized distributions on control inputs. Furthermore, we show how non-smooth penalty functions can be incorporated into our framework to handle state constraints. Simulations on cartpole and quadcopter show that our approach outperforms previous methods on constrained sampling-based optimization, in terms of quality of solutions and convergence speed.


Title: Learning Control Policies from Optimal Trajectories
Key Words: feedback  Gaussian processes  industrial robots  learning systems  mobile robots  nonlinear control systems  optimal control  optimisation  pendulums  trajectory control  time-dependent optimal trajectories  optimal feedback control policies  real-world systems  frequent correction  model errors  optimal reference trajectories  high dimensional state space  learning control policies  optimally control robotic systems  high dimensional nonlinear system dynamic models  Gaussian processes  swing-up problem  underactuated pendulum  energy-minimal point-to-point movement  3-DOF industrial robot  Trajectory  Computational modeling  Task analysis  Numerical models  Robots  Feedback control  Optimal control 
Abstract: The ability to optimally control robotic systems offers significant advantages for their performance. While time-dependent optimal trajectories can numerically be computed for high dimensional nonlinear system dynamic models, constraints and objectives, finding optimal feedback control policies for such systems is hard. This is unfortunate, as without a policy, the control of real-world systems requires frequent correction or replanning to compensate for disturbances and model errors.In this paper, a feedback control policy is learned from a set of optimal reference trajectories using Gaussian processes. Information from existing trajectories and the current policy is used to find promising start points for the computation of further optimal trajectories. This aspect is important as it avoids exhaustive sampling of the complete state space, which is impractical due to the high dimensional state space, and to focus on the relevant region.The presented method has been applied in simulation to a swing-up problem of an underactuated pendulum and an energy-minimal point-to-point movement of a 3-DOF industrial robot.


Title: Crocoddyl: An Efficient and Versatile Framework for Multi-Contact Optimal Control
Key Words: dynamic programming  iterative methods  motion control  optimal control  Crocoddyl  contact robot control by differential dynamic library  open-source framework  multicontact optimal control  state trajectory  control policy  sparse analytical derivatives  differential geometry  floating-base systems  FDDP  computation time  infeasible state-control trajectories  highly-dynamic maneuvers  feasibility-driven differential dynamic programming  Optimal control  Trajectory  Optimization  Heuristic algorithms  Dynamic programming  Robots  Acceleration 
Abstract: We introduce Crocoddyl (Contact RObot COntrol by Differential DYnamic Library), an open-source framework tailored for efficient multi-contact optimal control. Crocoddyl efficiently computes the state trajectory and the control policy for a given predefined sequence of contacts. Its efficiency is due to the use of sparse analytical derivatives, exploitation of the problem structure, and data sharing. It employs differential geometry to properly describe the state of any geometrical system, e.g. floating-base systems. Additionally, we propose a novel optimal control algorithm called Feasibility-driven Differential Dynamic Programming (FDDP). Our method does not add extra decision variables which often increases the computation time per iteration due to factorization. FDDP shows a greater globalization strategy compared to classical Differential Dynamic Programming (DDP) algorithms. Concretely, we propose two modifications to the classical DDP algorithm. First, the backward pass accepts infeasible state-control trajectories. Second, the rollout keeps the gaps open during the early "exploratory" iterations (as expected in multipleshooting methods with only equality constraints). We showcase the performance of our framework using different tasks. With our method, we can compute highly-dynamic maneuvers (e.g. jumping, front-flip) within few milliseconds.


Title: Grasp for Stacking via Deep Reinforcement Learning
Key Words: grippers  industrial manipulators  learning systems  neurocontrollers  optimal control  stacking  deep reinforcement learning  integrated robotic arm system  object grasping  model-free deep Q-learning method  grasping-stacking task  GSN  grasping for stacking network  industrial environments  GNet  optimal location  long-range planning  Grasping  Stacking  Task analysis  Feature extraction  Manipulators  Training 
Abstract: Integrated robotic arm system should contain both grasp and place actions. However, most grasping methods focus more on how to grasp objects, while ignoring the placement of the grasped objects, which limits their applications in various industrial environments. In this research, we propose a model-free deep Q-learning method to learn the grasping-stacking strategy end-to-end from scratch. Our method maps the images to the actions of the robotic arm through two deep networks: the grasping network (GNet) using the observation of the desk and the pile to infer the gripper's position and orientation for grasping, and the stacking network (SNet) using the observation of the platform to infer the optimal location when placing the grasped object. To make a long-range planning, the two observations are integrated in the grasping for stacking network (GSN). We evaluate the proposed GSN on a grasping-stacking task in both simulated and real-world scenarios.


Title: CAGE: Context-Aware Grasping Engine
Key Words: learning (artificial intelligence)  manipulators  ubiquitous computing  context-aware grasping engine  semantic grasping  stable grasps  specific object manipulation tasks  task constraints  semantic representation  grasp contexts  object states  memorization  semantic grasps  CAGE  statistically significant margins  memorization balancing  robot  Semantics  Task analysis  Grasping  Feature extraction  Robots  Cognition  Context modeling 
Abstract: Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide & Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: https://github.com/wliu88/railsemanticgrasping.


Title: Super-Pixel Sampler: a Data-driven Approach for Depth Sampling and Reconstruction
Key Words: image colour analysis  image reconstruction  image sampling  random processes  data-driven approach  depth sampling  depth acquisition  active illumination  autonomous navigation  robotic navigation  mechanical templates  sampling templates  autonomous vehicles  solid-state depth sensors  adaptive framework  simple reconstruction algorithm  generic reconstruction algorithm  sampling reconstruction algorithm  random sampling  depth completion algorithms  single-pixel prototype sampler  RGB sampling strategies  single depth sampling strategies  piecewise planar depth model  superpixel sampler  SPS  Image reconstruction  Adaptation models  Estimation  Cameras  Robot sensing systems  Navigation 
Abstract: Depth acquisition, based on active illumination, is essential for autonomous and robotic navigation. LiDARs (Light Detection And Ranging) with mechanical, fixed, sampling templates are commonly used in today's autonomous vehicles. An emerging technology, based on solid-state depth sensors, with no mechanical parts, allows fast and adaptive scans. In this paper, we propose an adaptive, image-driven, fast, sampling and reconstruction strategy. First, we formulate a piece-wise planar depth model and estimate its validity for indoor and outdoor scenes. Our model and experiments predict that, in the optimal case, adaptive sampling strategies with about 20-60 piece-wise planar structures can approximate well a depth map. This translates to requiring a single depth sample for every 1200 RGB samples (less than 0.1%), providing strong motivation to investigate an adaptive framework. Second, we introduce SPS (Super-Pixel Sampler), a simple, generic, sampling and reconstruction algorithm, based on super-pixels. Our sampling improves grid and random sampling, consistently, for a wide variety of reconstruction methods. Third, we propose an extremely simple and fast reconstruction for our sampler. It achieves state-of-the-art results, compared to complex image- guided depth completion algorithms, reducing the required sampling rate by a factor of 3-4. A single-pixel prototype sampler built in our lab illustrates the concept.


Title: Physics-based Simulation of Continuous-Wave LIDAR for Localization, Calibration and Tracking
Key Words: calibration  computerised instrumentation  differentiation  gradient methods  optical radar  optical sensors  optical tracking  optimisation  radar receivers  parameter estimation  2D continuous-wave LIDAR sensors  light detection and ranging sensors  depth measurements  localization pipelines  autonomous robots  perception stack  physics-based simulation  sensor measurements  gradient-based optimization  Hokuyo URG-04LX LIDAR  surface-light interactions  physically plausible model  laser light  calibration  time-of-flight cameras  depth sensors  Laser radar  Measurement by laser beam  Mathematical model  Sensor phenomena and characterization  Surface emitting lasers  Laser modes 
Abstract: Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.


Title: A Spatial-temporal Multiplexing Method for Dense 3D Surface Reconstruction of Moving Objects
Key Words: image coding  image filtering  image matching  image restoration  image texture  multiplexing  object recognition  robot vision  stereo image processing  surface reconstruction  high reconstruction accuracy  fast image acquisition  spatial-temporal multiplexing method  moving objects  three-dimensional reconstruction  robotic applications  robotic recognition  spatial-multiplexing time-multiplexing structured-light techniques  image acquisition time  spatial-temporal encoded patterns  dense 3D surface reconstruction  texture map  image blur  high-frequency phase-shifting fringes  spatial-coded texture  Image reconstruction  Multiplexing  Three-dimensional displays  Surface reconstruction  Robots  Encoding  Reliability 
Abstract: Three-dimensional reconstruction of dynamic objects is important for robotic applications, for example, the robotic recognition and manipulation. In this paper, we present a novel 3D surface reconstruction method for moving objects. The proposed method combines the spatial-multiplexing and time-multiplexing structured-light techniques that have advantages of less image acquisition time and accurate 3D reconstruction, respectively. A set of spatial-temporal encoded patterns are designed, where a spatial-encoded texture map is embedded into the temporal-encoded three-step phase-shifting fringes. The specifically designed spatial-coded texture assigns high-uniqueness codeword to any window on the image which helps to eliminate the phase ambiguity. In addition, the texture is robust to noise and image blur. Combining this texture with high-frequency phase-shifting fringes, high reconstruction accuracy would be ensured. This method only requires 3 patterns to uniquely encode a surface, which facilitates the fast image acquisition for each reconstruction step. A filtering stereo matching algorithm is proposed for the spatial-temporal multiplexing method to improve the matching reliability. Moreover, the reconstruction precision is further enhanced by a correspondence refinement algorithm. Experiments validate the performance of the proposed method including the high accuracy, the robustness to noise and the ability to reconstruct moving objects.


Title: DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR Scans
Key Words: Bayes methods  control engineering computing  convolutional neural nets  filtering theory  image segmentation  image sequences  learning (artificial intelligence)  mobile robots  neural net architecture  object detection  optical radar  path planning  recursive estimation  robot vision  SLAM (robots)  DeepTemporalSeg  temporally consistent semantic segmentation  3d LiDAR scans  semantic characteristics  autonomous robot operation  deep convolutional neural network  DCNN  LiDAR scan  pedestrian  bicyclist  dense blocks  depth separable convolutions  current semantic state  recursive estimation  isolated erroneous predictions  neural network architectures  Bayes filter approach  KITTI tracking benchmark  Semantics  Laser radar  Image segmentation  Task analysis  Three-dimensional displays  Neural networks  Automobiles 
Abstract: Understanding the semantic characteristics of the environment is a key enabler for autonomous robot operation. In this paper, we propose a deep convolutional neural network (DCNN) for semantic segmentation of a LiDAR scan into the classes car, pedestrian and bicyclist. This architecture is based on dense blocks and efficiently utilizes depth separable convolutions to limit the number of parameters while still maintaining the state-of-the-art performance. To make the predictions from the DCNN temporally consistent, we propose a Bayes filter based method. This method uses the predictions from the neural network to recursively estimate the current semantic state of a point in a scan. This recursive estimation uses the knowledge gained from previous scans, thereby making the predictions temporally consistent and robust towards isolated erroneous predictions. We compare the performance of our proposed architecture with other state-of-the-art neural network architectures and report substantial improvement. For the proposed Bayes filter approach, we shows results on various sequences in the KITTI tracking benchmark.


Title: Discrete Bimanual Manipulation for Wrench Balancing
Key Words: manipulators  motion control  payload limitations  single arm  grasped object  wrenches  payload limits  dual-arm robot  robot arms changes  wrench imbalance  discrete bimanual manipulation  grasp points  balanced configuration  robot experiments  grasping force  wrench balancing  Manipulators  Force  Grippers  Planning  Grasping  Payloads 
Abstract: Dual-arm robots can overcome grasping force and payload limitations of a single arm by jointly grasping an object. However, if the distribution of mass of the grasped object is not even, each arm will experience different wrenches that can exceed its payload limits. In this work, we consider the problem of balancing the wrenches experienced by a dual-arm robot grasping a rigid tray. The distribution of wrenches among the robot arms changes due to objects being placed on the tray. We present an approach to reduce the wrench imbalance among arms through discrete bimanual manipulation. Our approach is based on sequential sliding motions of the grasp points on the surface of the object, to attain a more balanced configuration. We validate our modeling approach and system design through a set of robot experiments.


Title: NeuroTac: A Neuromorphic Optical Tactile Sensor applied to Texture Recognition
Key Words: biomedical equipment  biomedical optical imaging  biomimetics  feedback  haptic interfaces  image classification  image texture  medical image processing  prosthetics  skin  tactile sensors  touch (physiological)  NeuroTac  texture recognition  artificial tactile sensing capabilities  rival human touch  robotics  prosthetics  biomimetic tactile sensors  grasping  manipulation tasks  biomimetic hardware design  TacTip sensor  layered papillae structure  human glabrous skin  event-based camera  spike trains  texture classification task  spike coding methods  artificial textures  spike-based output  biomimetic tactile perception algorithms  neuromorphic optical tactile sensor  Encoding  Neuromorphics  Tactile sensors  Cameras  Task analysis 
Abstract: Developing artificial tactile sensing capabilities that rival human touch is a long-term goal in robotics and prosthetics. Gradually more elaborate biomimetic tactile sensors are being developed and applied to grasping and manipulation tasks to help achieve this goal. Here we present the neuroTac, a novel neuromorphic optical tactile sensor. The neuroTac combines the biomimetic hardware design from the TacTip sensor which mimicks the layered papillae structure of human glabrous skin, with an event-based camera (DAVIS240, iniVation) and algorithms which transduce contact information in the form of spike trains. The performance of the sensor is evaluated on a texture classification task, with four spike coding methods being implemented and compared: Intensive, Spatial, Temporal and Spatiotemporal. We found timing-based coding methods performed with the highest accuracy over both artificial and natural textures. The spike-based output of the neuroTac could enable the development of biomimetic tactile perception algorithms in robotics as well as non-invasive and invasive haptic feedback methods in prosthetics.


Title: Reducing Uncertainty in Pose Estimation under Complex Contacts via Force Forecast
Key Words: haptic interfaces  manipulators  pose estimation  regression analysis  robotic assembly  trees (mathematics)  sphere-tree representation  least-uncertain estimate  relative contact  multiregion complex contacts  contact types  contact locations  object shapes  object poses  complex shapes  pose estimation  force forecast  autonomous robotic manipulation  simulated complex contacts  force sensing  constraint-based haptic simulation algorithm  three-pin peg-in-hole robotic assembly tasks  contact-rich two-pin peg-in-hole assembly tasks  calibration  regression model  Force  Uncertainty  Robot sensing systems  Task analysis  Calibration 
Abstract: How to reduce uncertainty in object pose estimation under complex contacts is crucial to autonomous robotic manipulation and assembly. In this paper, we introduce an approach through forecasting contact force from simulated complex contacts with calibration based on real force sensing. A constraint-based haptic simulation algorithm is used with sphere-tree representation of contacting objects to compute contact poses and forces, and through matching the computed forces to measured real force data via a regression model, the least-uncertain estimate of the relative contact pose is obtained. Our approach can handle multi-region complex contacts and does not make any assumption about contact types or contact locations. It also does not have restriction on object shapes. We have applied the force forecast approach to reducing uncertainty in estimating object poses in challenging peg-in-hole robotic assembly tasks and demonstrate the effectiveness of the approach by successful completion of contact-rich two-pin and three-pin real peg-in-hole assembly tasks with complex shapes of pins and holes.


Title: Comparison of Constrained and Unconstrained Human Grasp Forces Using Fingernail Imaging and Visual Servoing
Key Words: dexterous manipulators  force measurement  grippers  robot vision  visual servoing  fingernail imaging  force measurement  visual tracking system  force collaboration  constrained human grasp forces  unconstrained human grasp forces  visual servoing  3D fingertip forces  robotic arms  Grasping  Force  Estimation  Cameras  Mathematical model  Force measurement 
Abstract: Fingernail imaging has been proven to be effective in prior works [1], [2] for estimating the 3D fingertip forces with a maximum RMS estimation error of 7%. In the current research, fingernail imaging is used to perform unconstrained grasp force measurement on multiple fingers to study human grasping. Moreover, two robotic arms with mounted cameras and a visual tracking system have been devised to keep the human fingers in the camera frame during the experiments. Experimental tests have been conducted for six human subjects under both constrained and unconstrained grasping conditions, and the results indicate a significant difference in force collaboration among the fingers between the two grasping conditions. Another interesting result according to the experiments is that in comparison to constrained grasping, unconstrained grasp forces are more evenly distributed over the fingers and there is less force variation (more steadiness) in each finger force. These results validate the importance of measuring grasp forces in an unconstrained manner in order to study how humans naturally grasp objects.


Title: Robust Vision-based Obstacle Avoidance for Micro Aerial Vehicles in Dynamic Environments
Key Words: collision avoidance  helicopters  mobile robots  predictive control  robot dynamics  robot vision  state estimation  robust vision-based obstacle avoidance  microaerial vehicle  dynamic environments  on-board vision-based approach  moving obstacle  efficient obstacle detection  tracking algorithm  depth image pairs  robust collision avoidance  chance-constrained model predictive controller  collision probability  account MAV dynamics  state estimation  obstacle sensing uncertainties  on-line collision avoidance  Collision avoidance  Uncertainty  Cameras  Robustness  Sensors  Predictive models  State estimation 
Abstract: In this paper, we present an on-board vision-based approach for avoidance of moving obstacles in dynamic environments. Our approach relies on an efficient obstacle detection and tracking algorithm based on depth image pairs, which provides the estimated position, velocity and size of the obstacles. Robust collision avoidance is achieved by formulating a chance-constrained model predictive controller (CC-MPC) to ensure that the collision probability between the micro aerial vehicle (MAV) and each moving obstacle is below a specified threshold. The method takes into account MAV dynamics, state estimation and obstacle sensing uncertainties. The proposed approach is implemented on a quadrotor equipped with a stereo camera and is tested in a variety of environments, showing effective on-line collision avoidance of moving obstacles.


Title: Proximity Estimation Using Vision Features Computed On Sensor
Key Words: collision avoidance  computer vision  feature extraction  image sensors  microcontrollers  mobile robots  recurrent neural nets  robot vision  sensor arrays  VLSI  neural network output  image capture  control system  trained neural network  fully connected layer-recurrent  training data  infrared proximity sensors  vision output  sparse feature description data  feature algorithms  pixel processor array chip  embedded 256×256 processor SIMD array  image sensor  RC model car  microcontroller  SCAMP-5 vision chip  vision system integrating  experimental vehicle  blobs  corner points  abstract features  monocular vision based proximity estimation system  vision features computed  velocity 0.64 m/s to 1.8 m/s  time 4.0 ms  Hardware  Estimation  Neural networks  Machine vision  Feature extraction  Registers  Image edge detection 
Abstract: This paper presents a monocular vision based proximity estimation system using abstract features, such as corner points, blobs and edges, as inputs to a neural network. An experimental vehicle was built using a vision system integrating the SCAMP-5 vision chip, a micro-controller, and an RC model car. The vision chip includes image sensor with embedded 256×256 processor SIMD array. The pixel processor array chip was programmed to capture images and run the feature algorithms directly on the focal plane, and then digest them so that only sparse feature description data were read-out in the form of 40 values. By logging the vision output and the output from three infrared proximity sensors, training data were obtained to train three fully connected layer-recurrent neural networks with fewer than 700 parameters each. The trained neural network was able to estimate the proximity to the level of accuracy sufficient for a reactive collision avoidance behaviour to be achieved. The latency of the control system, from image capture to neural network output, was under 4ms, enabling the vehicles to avoid obstacles while moving at 0.64m/s to 1.8m/s in the experiment.


Title: Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles
Key Words: cameras  distance measurement  feature extraction  image registration  mobile robots  motion estimation  optimisation  path planning  robot vision  steering systems  tree searching  efficient globally-optimal correspondence-less visual odometry  planar ground vehicles  2 DoF Ackermann steering model  downward facing camera  simple image registration problem  2-parameter planar homography  ground-plane features  plane-based Ackermann motion estimation  correspondence-based hypothesise  test schemes  fronto-parallel motion  branch-and-bound optimisation technique  low-dimensional parametrisation  Cameras  Optimization  Transmission line matrix methods  Land vehicles  Motion estimation  Image registration  Real-time systems 
Abstract: The motion of planar ground vehicles is often non-holonomic, and as a result may be modelled by the 2 DoF Ackermann steering model. We analyse the feasibility of estimating such motion with a downward facing camera that exerts fronto-parallel motion with respect to the ground plane. This turns the motion estimation into a simple image registration problem in which we only have to identify a 2-parameter planar homography. However, one difficulty that arises from this setup is that ground-plane features are indistinctive and thus hard to match between successive views. We encountered this difficulty by introducing the first globally-optimal, correspondence-less solution to plane-based Ackermann motion estimation. The solution relies on the branch-and-bound optimisation technique. Through the low-dimensional parametrisation, a derivation of tight bounds, and an efficient implementation, we demonstrate how this technique is eventually amenable to accurate real-time motion estimation. We prove its property of global optimality and analyse the impact of assuming a locally constant centre of rotation. Our results on real data finally demonstrate a significant advantage over the more traditional, correspondence-based hypothesise-and-test schemes.


Title: egoTEB: Egocentric, Perception Space Navigation Using Timed-Elastic-Bands
Key Words: aerospace navigation  aerospace robotics  collision avoidance  graph theory  mobile robots  Monte Carlo methods  motion control  optimisation  trajectory control  grid-based representations  optimization graph  local planning map  egoTEB  timed-elastic-bands  TEB hierarchical planner  real-time navigation  collision avoidance  goal directed motion  multitrajectory optimization based synthesis method  topologically distinct trajectory candidates  factor graph approach  egocentric perception space navigation  egocentric perception space representations  Monte Carlo evaluations  autonomous mobile robot  Trajectory  Optimization  Navigation  Planning  Robots  Collision avoidance  Topology 
Abstract: The TEB hierarchical planner for real-time navigation through unknown environments is highly effective at balancing collision avoidance with goal directed motion. Designed over several years and publications, it implements a multi-trajectory optimization based synthesis method for identifying topologically distinct trajectory candidates through navigable space. Unfortunately, the underlying factor graph approach to the optimization problem induces a mismatch between grid-based representations and the optimization graph, which leads to several time and optimization inefficiencies. This paper explores the impact of using egocentric, perception space representations for the local planning map. Doing so alleviates many of the identified issues related to TEB and leads to a new method called egoTEB. Timing experiments and Monte Carlo evaluations in benchmark worlds quantify the benefits of egoTEB for navigation through uncertain environments.


Title: Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation
Key Words: learning (artificial intelligence)  manipulators  neural nets  robot vision  visual robotic manipulation  robotic visual data  reinforcement learning algorithms  robotic learning  state estimation  latent state representation  deep reinforcement learning  unlabeled real robot data  robot experience  time-contrastive techniques  learned state representation  vision-based reinforcement learning agent  standard visual domain adaptation techniques  self-supervised sim-to-real adaptation  sequence-based supervised objectives  contrastive forward dynamics loss  Robots  Task analysis  Adaptation models  Visualization  Stacking  Data models  Training 
Abstract: Collecting and automatically obtaining reward signals from real robotic visual data for the purposes of training reinforcement learning algorithms can be quite challenging and time-consuming. Methods for utilizing unlabeled data can have a huge potential to further accelerate robotic learning. We consider here the problem of performing manipulation tasks from pixels. In such tasks, choosing an appropriate state representation is crucial for planning and control. This is even more relevant with real images where noise, occlusions and resolution affect the accuracy and reliability of state estimation. In this work, we learn a latent state representation implicitly with deep reinforcement learning in simulation, and then adapt it to the real domain using unlabeled real robot data. We propose to do so by optimizing sequence-based self- supervised objectives. These use the temporal nature of robot experience, and can be common in both the simulated and real domains, without assuming any alignment of underlying states in simulated and unlabeled real images. We further propose a novel such objective, the Contrastive Forward Dynamics loss, which combines dynamics model learning with time-contrastive techniques. The learned state representation that results from our methods can be used to robustly solve a manipulation task in simulation and to successfully transfer the learned skill on a real system. We demonstrate the effectiveness of our approaches by training a vision-based reinforcement learning agent for cube stacking. Agents trained with our method, using only 5 hours of unlabeled real robot data for adaptation, shows a clear improvement over domain randomization, and standard visual domain adaptation techniques for sim-to-real transfer.


Title: Meta Reinforcement Learning for Sim-to-real Domain Adaptation
Key Words: learning (artificial intelligence)  medical robotics  dynamic conditions  task-specific trajectory generation model  KUKA LBR 4+ robot  sim-to-real domain transfer  robotic policy training  meta reinforcement learning  Adaptation models  Task analysis  Trajectory  Robots  Training  Learning (artificial intelligence)  Heuristic algorithms 
Abstract: Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.


Title: Variational Auto-Regularized Alignment for Sim-to-Real Control
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  neural nets  variational auto-regularized alignment  sim-to-real control  general-purpose simulators  variational autoencoder  black-box simulation  latent space  encoder training  simulation parameter distribution  matching parameter distributions  ABB YuMi robot hardware  Hardware  Decoding  Computational modeling  Neural networks  Training  Trajectory  Benchmark testing 
Abstract: General-purpose simulators can be a valuable data source for flexible learning and control approaches. However, training models or control policies in simulation and then directly applying to hardware can yield brittle control. Instead, we propose a novel way to use simulators as regularizers. Our approach regularizes a decoder of a variational autoencoder to a black-box simulation, with the latent space bound to a subset of simulator parameters. This enables successful encoder training from a small number of real-world trajectories (10 in our experiments), yielding a latent space with simulation parameter distribution that matches the real-world setting. We use a learnable mixture for the latent prior/posterior, which implies a highly flexible class of densities for the posterior fit. Our approach is scalable and does not require restrictive distributional assumptions. We demonstrate ability to recover matching parameter distributions on a range of benchmarks, challenging custom simulation environments and several real-world scenarios. Our experiments using ABB YuMi robot hardware show ability to help reinforcement learning approaches overcome cases of severe sim-to-real mismatch.


Title: Experience Selection Using Dynamics Similarity for Efficient Multi-Source Transfer Learning Between Robots
Key Words: control engineering computing  helicopters  learning (artificial intelligence)  multi-robot systems  robot programming  robust control  user experience  robot systems  multisource inter-robot transfer learning  quadrotor experiments  real source quadrotor  virtual source quadrotor  experience selection  dynamics similarity  robotics literature  knowledge transfer  learning process  robust control theory  data-efficiency algorithm  Measurement  Task analysis  Heuristic algorithms  Robot sensing systems  Robust control  Trajectory 
Abstract: In the robotics literature, different knowledge transfer approaches have been proposed to leverage the experience from a source task or robot-real or virtual-to accelerate the learning process on a new task or robot. A commonly made but infrequently examined assumption is that incorporating experience from a source task or robot will be beneficial. In practice, inappropriate knowledge transfer can result in negative transfer or unsafe behaviour. In this work, inspired by a system gap metric from robust control theory, the ν-gap, we present a data-efficient algorithm for estimating the similarity between pairs of robot systems. In a multi-source inter-robot transfer learning setup, we show that this similarity metric allows us to predict relative transfer performance and thus informatively select experiences from a source robot before knowledge transfer. We demonstrate our approach with quadrotor experiments, where we transfer an inverse dynamics model from a real or virtual source quadrotor to enhance the tracking performance of a target quadrotor on arbitrary hand-drawn trajectories. We show that selecting experiences based on the proposed similarity metric effectively facilitates the learning of the target quadrotor, improving performance by 62% compared to a poorly selected experience.


Title: DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning
Key Words: intelligent robots  learning (artificial intelligence)  mobile robots  path planning  robot dynamics  robot vision  reality gap  joint perception  on-demand compute architecture  training optimal policies  robust evaluation  deep reinforcement learning  robotic control agent  raw camera images  robust path planning  DeepRacer  autonomous racing platform  Sim2Real reinforcement learning  end-to-end experimentation  RL  intelligent control systems  monocular camera  physical world  robust reinforcement learning  model-free learning  Training  Automobiles  Robots  Computational modeling  Cameras  Robustness  Navigation 
Abstract: DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub2.


Title: A closed-loop and ergonomic control for prosthetic wrist rotation
Key Words: artificial limbs  biomechanics  closed loop systems  electromyography  ergonomics  medical signal processing  open-loop scheme  ergonomic posture  control scheme  prostheses users  body compensation  closed-loop control  prosthetic level  control loop  correcting errors  upper-limb prostheses control  prosthetic wrist rotation  ergonomic control  Prosthetics  Task analysis  Wrist  Ergonomics  Wires  Robots  Muscles 
Abstract: Beyond the ultimate goal of prosthetics, repairing all the capabilities of amputees, the development line of upper-limb prostheses control mainly relies on three aspects: the robustness, the intuitiveness and the reduction of mental fatigue. Many complex structures and algorithms are proposed but no one question a common open-loop nature, where the user is the one in charge of correcting errors. Yet, closing the control loop at the prosthetic level may help to improve the three main lines of research cited above. One major issue to build a closed-loop control is the definition of a reliable error signal; this paper proposes to use body compensations, naturally exhibited by prostheses users when the motion of their device is inaccurate, as such. The described control scheme measures these compensatory movements and makes the prosthesis move in order to bring back the user into an ergonomic posture. The function of the prosthesis is no longer to perform a given motion but rather to correct the posture of its user while s/he focus on performing an endpoint task. This concept was validated and compared to a standard open-loop scheme, for the control of a prosthetic wrist, with five healthy subjects completing a dedicated task with a customized transradial prosthesis. Results show that the presented closed-loop control allows for more intuitiveness and less mental burden without enhancing body compensation.


Title: Comparison of online algorithms for the tracking of multiple magnetic targets in a myokinetic control interface*
Key Words: Kalman filters  medical robotics  optimisation  prosthetics  surgery  telerobotics  tracking  localization algorithms  optimization  Levenberg-Marquardt algorithm  trust region reflective algorithm  robotics applications  remote tracking  multiple magnetic targets  myokinetic control interface  magnetic tracking algorithms  biomedical applications  teleoperated surgical robots  upper limb prostheses  Magnetostatics  Magnetic separation  Robots  Magnetic sensors  Magnetic devices 
Abstract: Magnetic tracking algorithms can be used to determine the position and orientation of magnetic markers or devices. These techniques are particularly interesting for biomedical applications such as teleoperated surgical robots or the control of upper limb prostheses. The performance of different algorithms used for magnetic tracking was compared in the past. However, in most cases, those algorithms were required to track a single magnet.Here we investigated the performance of three localization algorithms in tracking up to 9 magnets: two optimization-based (Levenberg-Marquardt algorithm, LMA, and Trust Region Reflective algorithm, TRRA) and one recursion-based (Unscented Kalman Filter, UKF). The tracking accuracy of the algorithms and their computation time were investigated through simulations.The accuracy of the three algorithms, when tracking up to six magnets, was similar, leading to estimation errors varying from 0.06 ± 0.02 mm to 2.26 ± 0.07 mm within a 100 mm × 54 mm × 100 mm workspace, at the highest sampling frequency. In all cases, computation times under 300 ms for the UKF and 45 ms for the LMA/TRRA were obtained. The TRRA showed the best tracking performance overall.These outcomes are of interest for a wide range of robotics applications that require remote tracking.


Title: Congestion-aware Evacuation Routing using Augmented Reality Devices
Key Words: augmented reality  emergency management  optimisation  congestion-aware evacuation  congestion-aware routing solution  indoor evacuation  real-time individual-customized evacuation routes  multiple destinations  population density map  obtained on-the-fly  congestion distribution  optimal solution  time-efficient evacuation route  AR devices  user-end augmented reality devices  Sociology  Statistics  Routing  Real-time systems  Path planning  Robots  Headphones 
Abstract: We present a congestion-aware routing solution for indoor evacuation, which produces real-time individual-customized evacuation routes among multiple destinations while keeping tracks of all evacuees' locations. A population density map, obtained on-the-fly by aggregating locations of evacuees from user-end Augmented Reality (AR) devices, is used to model the congestion distribution inside a building. To efficiently search the evacuation route among all destinations, a variant of A* algorithm is devised to obtain the optimal solution in a single pass. In a series of simulated studies, we show that the proposed algorithm is more computationally optimized compared to classic path planning algorithms; it generates a more time-efficient evacuation route for each individual that minimizes the overall congestion. A complete system using AR devices is implemented for a pilot study in real-world environments, demonstrating the efficacy of the proposed approach.


Title: Human-robot interaction for robotic manipulator programming in Mixed Reality
Key Words: augmented reality  human-robot interaction  manipulators  robot programming  human-robot interaction  robotic manipulator programming  mixed reality  interactive programming  HoloLens glasses  robotic operation system  robotic manipulators  robot location  point cloud analysis  virtual markers  menus  pick-and-place operation  contact operations execution  UR10e robot  KUKA iiwa robot  Virtual reality  Trajectory  Collision avoidance  Manipulators  Programming  Service robots 
Abstract: The paper presents an approach for interactive programming of the robotic manipulator using mixed reality. The developed system is based on the HoloLens glasses connected through Robotic Operation System to Unity engine and robotic manipulators. The system gives a possibility to recognize the real robot location by the point cloud analysis, to use virtual markers and menus for the task creation, to generate a trajectory for execution in the simulator or on the real manipulator. It also provides the possibility of scaling virtual and real worlds for more accurate planning. The proposed framework has been tested on pick-and-place and contact operations execution by UR10e and KUKA iiwa robots.


Title: Heart Rate Sensing with a Robot Mounted mmWave Radar
Key Words: cardiology  convolutional neural nets  health care  medical robotics  medical signal processing  patient monitoring  patient rehabilitation  telemedicine  mBeats features  deep neural network predictor  robust operation  post-operative rehabilitation  heart rate sensing  robot mounted mmWave radar  post-operative recovery  noncontact heart rate monitoring  static wall-mounted device  millimeter wave radar system  periodic heart rate measurements  users daily activities  mmWave servoing module  Heart rate  Robot sensing systems  Radar  Monitoring  Estimation  Legged locomotion 
Abstract: Heart rate monitoring at home is a useful metric for assessing health e.g. of the elderly or patients in post-operative recovery. Although non-contact heart rate monitoring has been widely explored, typically using a static, wall-mounted device, measurements are limited to a single room and sensitive to user orientation and position. In this work, we propose mBeats, a robot mounted millimeter wave (mmWave) radar system that provide periodic heart rate measurements under different user poses, without interfering in a users daily activities. mBeats contains a mmWave servoing module that adaptively adjusts the sensor angle to the best reflection pro le. Furthermore, mBeats features a deep neural network predictor, which can estimate heart rate from the lower leg and additionally provides estimation uncertainty. Through extensive experiments, we demonstrate accurate and robust operation of mBeats in a range of scenarios. We believe by integrating mobility and adaptability, mBeats can empower many down-stream healthcare applications at home, such as palliative care, post-operative rehabilitation and telemedicine.


Title: Flow Compensation for Hydraulic Direct-Drive System with a Single-rod Cylinder Applied to Biped Humanoid Robot
Key Words: flow control  humanoid robots  legged locomotion  pressure control  robot dynamics  valves  hydraulic direct-drive system  biped humanoid robot  hydraulic direct drive system  simple equipment configuration  single-rod cylinder  flow rate  passive flow compensation valve  valve state  flow control  pressure control  Conferences  Automation 
Abstract: Biped robots require massive power on each leg while walking, hopping, and running. We have developed a flow-based control system-called hydraulic direct drive system- that can achieve high output while avoiding spatial limitations. To implement the proposed system with simple equipment configuration, a pump and single-rod cylinder are connected in a closed loop. However, because compensation for flow rate is impossible in a completely closed loop, owing to the difference in the pressure receiving area caused by the rod, a passive flow compensation valve is employed. This valve has a simple structure and is easy to implement. Further, an additional sensor is required to detect the open/close state because the valve state will cause an error in flow control. Therefore, we implemented a model in the controller to predict the state of the flow compensation valve and formulated a method of switching from flow control to pressure control according to the predicted state. Experimental results indicate that the error of the joint angle is reduced to less than 1.6 degrees for walking patterns, and stable walking is realized when the system is installed in biped humanoid robots.


Title: Mechanically Programmed Miniature Origami Grippers
Key Words: actuators  bending  grippers  miniature origami grippers  robotic gripper design  customizable grasping tasks  miniature fingers  single actuator input  grasping tasks  Grippers  Kinematics  Grasping  Actuators  Springs  Steel  Robots 
Abstract: This paper presents a robotic gripper design that can perform customizable grasping tasks at the millimeter scale. The design is based on the origami string, a mechanism with a single degree of freedom that can be mechanically programmed to approximate arbitrary paths in space. By using this concept, we create miniature fingers that bend at multiple joints with a single actuator input. The shape and stiffness of these fingers can be varied to fit different grasping tasks by changing the crease pattern of the string. We show that the experimental behavior of these strings follows their analytical models and that they can perform a variety of tasks including pinching, wrapping, and twisting common objects such as pencils, bottle caps, and blueberries.


Title: Bio-inspired Tensegrity Fish Robot
Key Words: autonomous underwater vehicles  biomechanics  bone  elasticity  mobile robots  muscle  robot dynamics  servomotors  underwater vehicles  fish robots  body stiffness  fish swimming  mechanical stiffness  tensegrity-based underwater robots  tensegrity class  elastic cables  rigid body segments  body shape  tensegrity systems  fish-like robots  bio-inspired tensegrity fish robot  Conferences  Automation 
Abstract: This paper presents a method to create fish-like robots with tensegrity systems and describes a prototype modeled on the body shape of the rainbow trout with a length of 400 mm and a mass of 102 g that is driven by a waterproof servomotor. The structure of the tensegrity robot consists of rigid body segments and elastic cables that represent bone/tissue and muscles of fish, respectively. This structural configuration employing the tensegrity class 2 is much simpler than other tensegrity-based underwater robots. It also allows the tuning of the mechanical stiffness, which is often said to be an important factor in fish swimming. In our robot, the body stiffness can be tuned by changing the cross-section of the cables and their pre-stretch ratio. We characterize the robot in terms of body stiffness, swimming speed, and thrust force while varying the body stiffness i.e., the cross-section of the elastic cables. The results show that the body stiffness of the robot can be designed to approximate that of the real fish and modulate its performance characteristics. The measured swimming speed of the robot is 0.23 m/s (0.58 BL/s), which is comparable to other fish robots of the same type. Strouhal number of the robot 0.54 is close to that of the natural counterpart, suggesting that the presented method is an effective engineering approach to realize the swimming characteristics of real fish.


Title: Gaussian-Dirichlet Random Fields for Inference over High Dimensional Categorical Observations
Key Words: Bayes methods  Gaussian processes  image classification  mobile robots  path planning  robot vision  vectors  low dimensional vector observations  Gaussian-Dirichlet random fields  high dimensional categorical observations  spatio-temporal distribution  imaging sensor  image classifier  Dirichlet distributions  Gaussian processes  high dimensional categorical measurements  taxonomic observations  informative path planning techniques  Gaussian processes  Biological system modeling  Spatiotemporal phenomena  Graphical models  Data models  Robots  Oceans 
Abstract: We propose a generative model for the spatio-temporal distribution of high dimensional categorical observations. These are commonly produced by robots equipped with an imaging sensor such as a camera, paired with an image classifier, potentially producing observations over thousands of categories. The proposed approach combines the use of Dirichlet distributions to model sparse co-occurrence relations between the observed categories using a latent variable, and Gaussian processes to model the latent variable's spatio-temporal distribution. Experiments in this paper show that the resulting model is able to efficiently and accurately approximate the temporal distribution of high dimensional categorical measurements such as taxonomic observations of microscopic organisms in the ocean, even in unobserved (held out) locations, far from other samples. This work's primary motivation is to enable deployment of informative path planning techniques over high dimensional categorical fields, which until now have been limited to scalar or low dimensional vector observations.


Title: Investigation of a Multistable Tensegrity Robot applied as Tilting Locomotion System*
Key Words: actuators  bifurcation  mechanical stability  mobile robots  motion control  numerical analysis  robot dynamics  robot kinematics  vibration control  locomotion characteristics  actuation strategy  compliant tensegrity structure  multistable tensegrity robot  multiple stable equilibrium configurations  tilting locomotion system  Prototypes  Bifurcation  Robots  Mathematical model  Shape  Reliability  Topology 
Abstract: This paper describes the development of a tilting locomotion system based on a compliant tensegrity structure with multiple stable equilibrium configurations. A tensegrity structure featuring 4 stable equilibrium states is considered. The mechanical model of the structure is presented and the according equations of motion are derived. The variation of the length of selected structural members allows to influence the prestress state and the corresponding shape of the tensegrity structure. Based on bifurcation analyses a reliable actuation strategy to control the current equilibrium state is designed. In this work, the tensegrity structure is assumed to be in contact with a horizontal plane due to gravity. The derived actuation strategy is utilized to generate tilting locomotion by successively changing the equilibrium state. Numerical simulations are evaluated considering the locomotion characteristics. In order to validate this theoretical approach a prototype is developed. Experiments regarding to the equilibrium configurations, the actuation strategy and the locomotion characteristics are evaluated using image processing tools and motion capturing. The results verify the theoretical data and confirm the working principle of the investigated tilting locomotion system. This approach represents a feasible actuation strategy to realize a reliable tilting locomotion utilizing the multistability of compliant tensegrity structures.


Title: A Novel Articulated Soft Robot Capable of Variable Stiffness through Bistable Structure
Key Words: actuators  biomechanics  dexterous manipulators  elasticity  manipulator dynamics  position control  servomotors  variable stiffness  bistable structure  unstructured environments  dynamic environments  highly dissipative nature  elastic materials results  load capability  rigid joints  compliant bistable structures  bending stiffness  articulated soft robot  force transmission  position accuracy  mechanical constrain  construction method  servomotor  variable workspace  dexterous manipulation  tip load  Soft robotics  Manipulators  Force  Springs  Kinematics  Shape  Articulated Soft Robot  Variable Stiffness  Bistable Structure  Locking Function 
Abstract: Soft robot has demonstrated promise in unstructured and dynamic environments due to unique advantages, such as safe interaction, adaptiveness, easy to actuate, and easy fabrication. However, the highly dissipative nature of elastic materials results in small stiffness of soft robot which limits certain functions, such as force transmission, position accuracy, and load capability. In this paper, we present a novel articulated soft robot with variable stiffness. The robot is constructed by rigid joints and compliant bistable structures in series. Each joint can be independently locked through triggering the bistable structure to touch the mechanical constrain. Thus, the bending stiffness of the joint can be magnified which increases the stiffness of the articulated soft robot. Through this construction method, even driven by only one servomotor, the robot demonstrates variable workspace and stiffness which have the potential of dexterous manipulation and maintaining shape under tip load.


Title: Modeling and Experiments on the Swallowing and Disgorging Characteristics of an Underwater Continuum Manipulator
Key Words: bending  elasticity  grippers  hydraulic systems  manipulator dynamics  muscle  pneumatic actuators  underwater continuum manipulator  compliant materials  McKibben water hydraulic artificial muscle  WHAM  mechanical properties  kinematics model  soft grippers  bending procedure  disgorging procedure  mouth-tongue collaborative soft robot  single-segment soft robot arm  swallowing procedure  Manipulators  Muscles  Grippers  Kinematics  Hydraulic systems  Actuators 
Abstract: Soft robots apply compliant materials to perform motions and behaviors not typically achievable by rigid robots. An underwater, compliant, multi-segment continuum manipulator that can bend, swallow, disgorge is developed in this study. The manipulator is driven by McKibben water hydraulic artificial muscle (WHAM). The mechanical properties of the WHAM are tested and analyzed experimentally. The kinematics model, which concerns about the variable diameter structure of the soft grippers, are established to simulate the behaviors of the manipulator among the bending, swallowing and disgorging procedure. A mouth-tongue collaborative soft robot assembled with another single-segment soft robot arm is presented. And its functions are experimentally testified. The distinctive functions were verified according to the experimental results.


Title: Salamanderbot: A soft-rigid composite continuum mobile robot to traverse complex environments
Key Words: legged locomotion  motion control  continuously deformable slender body structure  salamanderbot  cable-driven bellows-like origami module  powered wheels  origami structure  soft-rigid composite continuum mobile robot  exploration applications  mobile soft robots  Yoshimura crease pattern  velocity 303.1 mm/s  radius 79.9 mm  Mobile robots  Wheels  Gears  Manipulators  DC motors  Kinematics 
Abstract: Soft robots are theoretically well-suited to rescue and exploration applications where their flexibility allows for the traversal of highly cluttered environments. However, most existing mobile soft robots are not fast or powerful enough to effectively traverse three dimensional environments. In this paper, we introduce a new mobile robot with a continuously deformable slender body structure, the SalamanderBot, which combines the flexibility and maneuverability of soft robots, with the speed and power of traditional mobile robots. It consists of a cable-driven bellows-like origami module based on the Yoshimura crease pattern mounted between sets of powered wheels. The origami structure allows the body to deform as necessary to adapt to complex environments and terrains, while the wheels allow the robot to reach speeds of up to 303.1 mm/s (2.05 body-length/s). Salamanderbot can climb up to 60-degree slopes and perform sharp turns with a minimum turning radius of 79.9 mm (0.54 body-length).


Title: Flexure Hinge-based Biomimetic Thumb with a Rolling-Surface Metacarpal Joint
Key Words: biomimetics  bone  control system synthesis  dexterous manipulators  end effectors  manipulator kinematics  motion control  position control  surgery  flexure hinge-based biomimetic thumb  rolling-surface metacarpal joint  grasping  dexterous manipulation  kinematic multiplicity  robotic hand  kinematic model  surgical techniques  motion capture data  end effector  task-space velocities  tendon excursion velocity  human thumb state contribution  data representation  effector velocity  Joints  Fasteners  Thumb  Ellipsoids  Robots  Prototypes  Ceramics 
Abstract: The human thumb's state contribution to grasping and dexterous manipulation of objects is a function of the kinematic multiplicity of joints and structure of the bones, joints, and ligaments. This paper looks at the design and evaluation of a human-like thumb for use in a robotic hand, where the thumb's state contribution to grasping and dexterous manipulation is a function of a simplified kinematic model based on that of the human thumb, but also on empirical trials of surgical techniques to retain functionality while reducing the number of joints in the thumb. Motion Capture Data of the End Effector is analyzed with the measured excursion of the tendons to determine the relationship between tendon velocities and task-space velocities. After validating the procedure experimentally, a simplified metric is proposed to represent this data, and shows that our prototype is predicted to have a relatively smooth mapping between tendon excursion velocity and end effector velocity.


Title: Ibex: A reconfigurable ground vehicle with adaptive terrain navigation capability
Key Words: friction  mobile robots  off-road vehicles  optimisation  remotely operated vehicles  robot dynamics  stability  vehicle dynamics  reconfigurable ground vehicle  adaptive terrain navigation capability  unmanned ground vehicle  dynamic wheelbase  adaptive thrust  friction optimization  steep slopes  slippery surfaces  surface topography  impedance-based stabilization module  mechanical oscillatory transients  Ibex  Force  Wheels  Surface topography  Surface impedance  Land vehicles  Kinematics  Drag 
Abstract: This paper presents a unique unmanned ground vehicle with a dynamic wheelbase and an adaptive thrust based friction optimization scheme that aids in the traversal of steep slopes and slippery surfaces. The vehicle is capable of adapting itself to the surface topography using an impedance-based stabilization module to minimize the mechanical oscillatory transients induced during its motion. A detailed analysis of its modules has been elucidated in this paper based on the vehicle parameters. The proposed methodologies have been integrated and tested on a customized prototype. Experimental validation and simulation for the proposed modules at various terrain conditions have been carried out to authenticate its performance.


Title: Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors
Key Words: groupware  image fusion  mobile robots  multi-robot systems  path planning  sensor fusion  SLAM (robots)  dynamic collaborative mapping  multimodal environmental perception  heterogeneous sensor fusion model  local 3D maps  night rainforest  3D map fusion missions  multimodal sensors  long-term operation  collaborative robots  dynamic environment  dynamic objects  Collaboration  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Robot vision systems 
Abstract: Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.


Title: Generating Locomotion with Effective Wheel Radius Manipulation
Key Words: mobile robots  motion control  motor drives  road vehicles  robot dynamics  vehicle dynamics  wheels  motor drives  sloped terrain  wheel rotation  plain centre hub drive  active ride height selection  wheel radius manipulation  locomotion generation  slope traversability  wheel pose control  centre of gravity manipulation  Wheels  Gravity  Acceleration  Mathematical model  Torque  Axles  Actuators 
Abstract: Travel over sloped terrain is difficult as an incline changes the interaction between each wheel and the ground resulting in an unbalanced load distribution which can lead to loss of traction and instability. This paper presents a novel approach to generating wheel rotation for primary locomotion by only changing its centre of rotation, or as a complimentary locomotion source to increase versatility of a plain centre hub drive. This is done using linear actuators within a wheel to control the position of the centre hub and induce a moment on the wheel from gravity. In doing so our platform allows for active ride height selection and individual wheel pose control. We present the system with calculations outlining the theoretical properties and perform experiments to validate the concept under loading via multiple gaits to show motion on slopes, and sustained motion over extended distance. We envision applications in conjunction to assist current motor drives and increasing slope traversability by allowing body pose and centre of gravity manipulation, or as a primary locomotion system.


Title: A GNC Architecture for Planetary Rovers with Autonomous Navigation
Key Words: aerospace navigation  aerospace robotics  distance measurement  Mars  mobile robots  optimal control  path planning  planetary rovers  robot vision  SLAM (robots)  trajectory control  GNC architecture  planetary rovers  autonomous navigation  Mars exploration missions  sample fetching rover  autonomous capabilities  two-level architecture  terrain  local path replanning  trajectory control  global localization  planetary exploration  planetary analog field test campaigns  guidance navigation and control architecture  hazard detection  visual odometry  adaptive SLAM algorithm  optimal path planning  Hazards  Navigation  Space vehicles  Autonomous robots  Computer architecture  Cameras  Trajectory 
Abstract: This paper proposes a Guidance, Navigation, and Control (GNC) architecture for planetary rovers targeting the conditions of upcoming Mars exploration missions such as Mars 2020 and the Sample Fetching Rover (SFR). The navigation requirements of these missions demand a control architecture featuring autonomous capabilities to achieve a fast and long traverse. The proposed solution presents a two-level architecture where the efficient navigation (lower) level is always active and the full navigation (upper) level is enabled according to the difficulty of the terrain. The first level is an efficient implementation of the basic functionalities for autonomous navigation based on hazard detection, local path replanning, and trajectory control with visual odometry. The second level implements an adaptive SLAM algorithm that improves the relative localization, evaluates the traversability of the terrain ahead for a more optimal path planning, and performs global (absolute) localization that corrects the pose drift during longer traverses. The architecture provides a solution for long-range, low supervision, and fast planetary exploration. Both navigation levels have been validated on planetary analog field test campaigns.


Title: Learning Face Recognition Unsupervisedly by Disentanglement and Self-Augmentation
Key Words: face recognition  feature extraction  home automation  unsupervised learning  video surveillance  triplet network  augmentation network  identity-aware features  face samples  identity-irrelevant features  home robot applications  face recognition system  smart home environment  environment-specific face recognition model  unsupervised learning  self-augmentation  healthcare application  camera position  surveillance video  identity-aware feature extraction  spatiotemporal characteristic  face image disentanglement 
Abstract: As the growth of smart home, healthcare, and home robot applications, learning a face recognition system which is specific for a particular environment and capable of self-adapting to the temporal changes in appearance (e.g., caused by illumination or camera position) is nowadays an important topic. In this paper, given a video of a group of people, which simulates the surveillance video in a smart home environment, we propose a novel approach which unsuper- visedly learns a face recognition model based on two main components: (1) a triplet network that extracts identity-aware feature from face images for performing face recognition by clustering, and (2) an augmentation network that is conditioned on the identity-aware features and aims at synthesizing more face samples. Particularly, the training data for the triplet network is obtained by using the spatiotemporal characteristic of face samples within a video, while the augmentation network learns to disentangle a face image into identity-aware and identity-irrelevant features thus is able to generate new faces of the same identity but with variance in appearance. With taking the richer training data produced by augmentation network, the triplet network is further fine-tuned and achieves better performance in face recognition. Extensive experiments not only show the efficacy of our model in learning an environment- specific face recognition model unsupervisedly, but also verify its adaptability to various appearance changes.


Title: PARC: A Plan and Activity Recognition Component for Assistive Robots
Key Words: assisted living  mobile robots  robot vision  activity recognition solutions  human-object interaction  low-level actions  goal recognition algorithm  low-cost robotics platform  assistive robots  mobile robot assistants  daily living activities  RGB-D camera  Activity recognition  Hidden Markov models  Robot sensing systems  Feature extraction  Clustering algorithms  Activity recognition  Plan recognition  Computer vision  Robotic assistant  Activities For Daily Living  Object affordance  Particle filter 
Abstract: Mobile robot assistants have many applications, such as helping people in their daily living activities. These robots have to detect and recognize the actions and goals of the humans they are assisting. While there are several wide-spread plan and activity recognition solutions for controlled environments with many built-in sensors, like smart-homes, there is a lack of such systems for mobile robots operating in open settings, such as an apartment. We propose a module for the recognition of activities and goals for daily living by mobile robots, in real time and for complex activities. Our approach recognizes human-object interaction using an RGB-D camera to infer low-level actions which are sent to a goal recognition algorithm. Results show that our approach is both in real time and requires little computational resources, which facilitates its deployment on a mobile and low-cost robotics platform.


Title: A Multilayer-Multimodal Fusion Architecture for Pattern Recognition of Natural Manipulations in Percutaneous Coronary Interventions
Key Words: control engineering computing  human-robot interaction  manipulators  medical computing  medical robotics  sensor fusion  multilayer-multimodal fusion architecture  pattern recognition  natural manipulations  percutaneous coronary interventions  robotic systems  robot-assisted procedures  human-robot interfaces  guidewire manipulations  multimodal behaviors  rule-based fusion algorithms  singlelayer recognition architecture  robot-assisted PCI  HRI  X-ray radiation reduction  medical staff  Sensors  Robots  Muscles  Feature extraction  Force  Catheters  Surgery 
Abstract: The increasingly-used robotic systems can provide precise delivery and reduce X-ray radiation to medical staff in percutaneous coronary interventions (PCI), but natural manipulations of interventionalists are forgone in most robot-assisted procedures. Therefore, it is necessary to explore natural manipulations to design more advanced human-robot interfaces (HRI). In this study, a multilayer-multimodal fusion architecture is proposed to recognize six typical subpatterns of guidewire manipulations in conventional PCI. The synchronously acquired multimodal behaviors from ten subjects are used as the inputs of the fusion architecture. Six classification-based and two rule-based fusion algorithms are evaluated for performance comparisons. Experimental results indicate that the multimodal fusion brings significant accuracy improvement in comparison with single-modal schemes. Furthermore, the proposed architecture can achieve the overall accuracy of 96.90%, much higher than that of a singlelayer recognition architecture (92.56%). These results have indicated the potential of the proposed method for facilitating the development of HRI for robot-assisted PCI.


Title: Real-Time Graph-Based SLAM with Occupancy Normal Distributions Transforms
Key Words: graph theory  least squares approximations  mobile robots  normal distribution  robot vision  SLAM (robots)  occupancy grid map  graph-based SLAM  occupancy normal distribution transforms  normal distributions transforms  simultaneous localization and mapping  mobile robotics  least squares problem  nonlinear optimizers  global NDT scan matcher  Simultaneous localization and mapping  Cost function  Google  Jacobian matrices  Gaussian distribution 
Abstract: Simultaneous Localization and Mapping (SLAM) is one of the basic problems in mobile robotics. While most approaches are based on occupancy grid maps, Normal Distributions Transforms (NDT) and mixtures like Occupancy Normal Distribution Transforms (ONDT) have been shown to represent sensor measurements more accurately. In this work, we slightly re-formulate the (O)NDT matching function such that it becomes a least squares problem that can be solved with various robust numerical and analytical non-linear optimizers. Further, we propose a novel global (O)NDT scan matcher for loop closure. In our evaluation, our NDT and ONDT methods are able to outperform the occupancy grid map based ones we adopted from Google's Cartographer implementation.


Title: Spatio-Temporal Non-Rigid Registration of 3D Point Clouds of Plants
Key Words: agriculture  cameras  image registration  robot vision  spatio-temporal nonrigid registration  3D point clouds  sensor data  agricultural robotics  plant science  agricultural tasks  automated temporal plant-trait analysis  plant performance monitoring  plant registration  Three-dimensional displays  Skeleton  Hidden Markov models  Strain  Topology  Simultaneous localization and mapping 
Abstract: Analyzing sensor data of plants and monitoring plant performance is a central element in different agricultural robotics applications. In plant science, phenotyping refers to analyzing plant traits for monitoring growth, for describing plant properties, or characterizing the plant's overall performance. It plays a critical role in the agricultural tasks and in plant breeding. Recently, there is a rising interest in using 3D data obtained from laser scanners and 3D cameras to develop automated non-intrusive techniques for estimating plant traits. In this paper, we address the problem of registering 3D point clouds of the plants over time, which is a backbone of applications interested in tracking spatio-temporal traits of individual plants. Registering plants over time is challenging due to its changing topology, anisotropic growth, and non-rigid motion in between scans. We propose a novel approach that exploits the skeletal structure of the plant and determines correspondences over time and drives the registration process. Our approach explicitly accounts for the non-rigidity and the growth of the plant over time in the registration. We tested our approach on a challenging dataset acquired over the course of two weeks and successfully registered the 3D plant point clouds recorded with a laser scanner forming a basis for developing systems for automated temporal plant-trait analysis.


Title: Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV
Key Words: distance measurement  mobile robots  optical radar  path planning  robot vision  SLAM (robots)  FoV  autonomous vehicles  autonomous navigation  path planning  LOAM algorithm  LiDAR odometry and mapping  robot pose localization  Laser radar  Feature extraction  Three-dimensional displays  Measurement by laser beam  Laser noise  Real-time systems  Spinning 
Abstract: LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot's pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.


Title: Active SLAM using 3D Submap Saliency for Underwater Volumetric Exploration
Key Words: graph theory  mobile robots  navigation  path planning  robot vision  SLAM (robots)  underwater volumetric exploration  active SLAM framework  3D underwater environments  multibeam sonar  integrated SLAM  volumetric free-space information  informative loop closures  navigation policy  3D visual dictionary  submap saliency  sensor information  pose-graph SLAM formulation  global occupancy grid map  uncertainty-agnostic framework  Simultaneous localization and mapping  Three-dimensional displays  Uncertainty  Conferences  Automation  Sonar  Planning 
Abstract: In this paper, we present an active SLAM framework for volumetric exploration of 3D underwater environments with multibeam sonar. Recent work in integrated SLAM and planning performs localization while maintaining volumetric free-space information. However, an absence of informative loop closures can lead to imperfect maps, and therefore unsafe behavior. To solve this, we propose a navigation policy that reduces vehicle pose uncertainty by balancing between volumetric exploration and revisitation. To identify locations to revisit, we build a 3D visual dictionary from real-world sonar data and compute a metric of submap saliency. Revisit actions are chosen based on propagated pose uncertainty and sensor information gain. Loop closures are integrated as constraints in our pose-graph SLAM formulation and these deform the global occupancy grid map. We evaluate our performance in simulation and real-world experiments, and highlight the advantages over an uncertainty-agnostic framework.


Title: Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM
Key Words: mobile robots  path planning  pose estimation  robot vision  service robots  SLAM (robots)  simultaneous localization and mapping  data sequences  robotic autonomy  service robots  real-world indoor scenes  OpenLORIS-Scene datasets  SLAM problems  pose estimation  Simultaneous localization and mapping  Robot kinematics  Cameras  Synchronization  Trajectory 
Abstract: Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.


Title: Informing Multi-Modal Planning with Synergistic Discrete Leads
Key Words: continuous systems  discrete systems  manipulators  multimodal planning  robotic manipulation problems  continuous infinity  object grasping  manipulation plan  single-mode motions  valid transitions  manipulation planners  multimodal structure  mode-specific planners  general layered planning approach  pick-and-place manipulation domain  synergistic discrete leads  specific mode transitions  useful mode transitions  bias search  Planning  Manifolds  Task analysis  Lead  Probabilistic logic  Robot motion 
Abstract: Robotic manipulation problems are inherently continuous, but typically have underlying discrete structure, e.g., whether or not an object is grasped. This means many problems are multi-modal and in particular have a continuous infinity of modes. For example, in a pick-and-place manipulation domain, every grasp and placement of an object is a mode. Usually manipulation problems require the robot to transition into different modes, e.g., going from a mode with an object placed to another mode with the object grasped. To successfully find a manipulation plan, a planner must find a sequence of valid single-mode motions as well as valid transitions between these modes. Many manipulation planners have been proposed to solve tasks with multi-modal structure. However, these methods require mode-specific planners and fail to scale to very cluttered environments or to tasks that require long sequences of transitions. This paper presents a general layered planning approach to multi-modal planning that uses a discrete "lead" to bias search towards useful mode transitions. The difficulty of achieving specific mode transitions is captured online and used to bias search towards more promising sequences of modes. We demonstrate our planner on complex scenes and show that significant performance improvements are tied to both our discrete "lead" and our continuous representation.


Title: Hierarchical Coverage Path Planning in Complex 3D Environments
Key Words: autonomous aerial vehicles  hierarchical systems  image resolution  image sampling  mobile robots  path planning  robot vision  complex three-dimensional environment  nooks  crannies  coverage planning  multiresolution hierarchical framework  three-dimensional scenes  hierarchical coverage path planning  lightweight UAV  low-level sampling  complex 3D environments  Planning  Robot sensing systems  Cameras  Octrees  Three-dimensional displays  Surface treatment 
Abstract: State-of-the-art coverage planning methods perform well in simple environments but take an ineffectively long time to converge to an optimal solution in complex three-dimensional (3D) environments. As more structures are present in the same volume of workspace, these methods slow down as they spend more time searching for all of the nooks and crannies concealed in three-dimensional spaces. This work presents a method for coverage planning that employs a multi-resolution hierarchical framework to solve the problem at two different levels, producing much higher efficiency than the state-of-the-art. First, a high-level algorithm separates the environment into multiple subspaces at different resolutions and computes an order of the subspaces for traversal. Second, a low-level sampling-based algorithm solves for paths within the subspaces for detailed coverage. In experiments, we evaluate our method using real-world datasets from complex three-dimensional scenes. Our method finds paths that are constantly shorter and converges at least ten times faster than the state-of-the-art. Further, we show results of a physical experiment where a lightweight UAV follows the paths to realize the coverage.


Title: Perception-aware time optimal path parameterization for quadrotors
Key Words: autonomous aerial vehicles  helicopters  mobile robots  optimisation  path planning  perception-aware time optimal path parameterization  quadrotors  perception-aware time optimal path parametrization  quadrotor systems  on-board navigation  estimation algorithms  planning  efficient time optimal path parametrization algorithm  quadrotor platform  vision-driven vehicles  Trajectory  Cameras  Planning  Task analysis  Aerodynamics  Heuristic algorithms  Navigation 
Abstract: The increasing popularity of quadrotors has given rise to a class of predominantly vision-driven vehicles. This paper addresses the problem of perception-aware time optimal path parametrization for quadrotors. Although many different choices of perceptual modalities are available, the low weight and power budgets of quadrotor systems makes a camera ideal for on-board navigation and estimation algorithms. However, this does come with a set of challenges. The limited field of view of the camera can restrict the visibility of salient regions in the environment, which dictates the necessity to consider perception and planning jointly. The main contribution of this paper is an efficient time optimal path parametrization algorithm for quadrotors with limited field of view constraints. We show in a simulation study that a state-of-the-art controller can track planned trajectories, and we validate the proposed algorithm on a quadrotor platform in experiments.


Title: Generating Visibility-Aware Trajectories for Cooperative and Proactive Motion Planning
Key Words: mobile robots  motion estimation  object detection  path planning  road traffic  road vehicles  vehicles  autonomous vehicle  ego vehicle  visibility-aware planning  visibility-aware trajectories  proactive motion planning  cooperative motion planning  partially-occluded intersection  emergent behavior  Trajectory  Uncertainty  Planning  Safety  Autonomous vehicles  Splines (mathematics) 
Abstract: The safety of an autonomous vehicle not only depends on its own perception of the world around it, but also on the perception and recognition from other vehicles. If an ego vehicle considers the uncertainty other vehicles have about itself, then by reducing the estimated uncertainty it can increase its safety. In this paper, we focus on how an ego vehicle plans its trajectories through the blind spots of other vehicles. We create visibility-aware planning, where the ego vehicle chooses its trajectories such that it reduces the perceived uncertainty other vehicles may have about the state of the ego vehicle. We present simulations of traffic and highway environments, where an ego vehicle must pass another vehicle, make a lane change, or traverse a partially-occluded intersection. Emergent behavior shows that when using visibility-aware planning, the ego vehicle spends less time in a blind spot, and may slow down before entering the blind spot so as to increase the likelihood other vehicles perceive the ego vehicle.


Title: An obstacle-interaction planning method for navigation of actuated vine robots
Key Words: bending  collision avoidance  mobile robots  motion control  soft robotics  reliable robot-environment interaction models  obstacle-interaction model  robot tip  obstacle-interaction planning method  actuated vine robot navigation  wrinkling deformation  bending deformation  Soft robotics  Strain  Deformable models  Planning  Kinematics  Pneumatic systems 
Abstract: The field of soft robotics is grounded on the idea that, due to their inherent compliance, soft robots can safely interact with the environment. Thus, the development of effective planning and control pipelines for soft robots should incorporate reliable robot-environment interaction models. This strategy enables soft robots to effectively exploit contacts to autonomously navigate and accomplish tasks in the environment. However, for a class of soft robots, namely vine-inspired, tip-extending or "vine" robots, such interaction models and the resulting planning and control strategies do not exist. In this paper, we analyze the behavior of vine robots interacting with their environment and propose an obstacle-interaction model that characterizes the bending and wrinkling deformation induced by the environment. Starting from this, we devise a novel obstacle-interaction planning method for these robots. We show how obstacle interactions can be effectively leveraged to enlarge the set of reachable workspace for the robot tip, and verify our findings with both simulated and real experiments. Our work improves the capabilities of this new class of soft robot, helping to advance the field of soft robotics.


Title: Distributed Consensus Control of Multiple UAVs in a Constrained Environment
Key Words: autonomous aerial vehicles  control system synthesis  decentralised control  distributed control  multi-robot systems  position control  tracking  trees (mathematics)  multiple UAVs  constrained environment  consensus problem  multiple unmanned aerial vehicles  environmental constraints  general communication topology  directed spanning tree  position transformation function  dynamic reference position  yaw angle  asymmetric topology  local tracking controller  distributed consensus control  Topology  Decentralized control  Protocols  Unmanned aerial vehicles  Tracking loops  Heuristic algorithms  Attitude control 
Abstract: In this paper, we investigate the consensus problem of multiple unmanned aerial vehicles (UAVs) in the presence of environmental constraints under a general communication topology containing a directed spanning tree. First, based on a position transformation function, we propose a novel dynamic reference position and yaw angle for each UAV to cope with both the asymmetric topology and the constraints. Then, the backstepping-like design methodology is presented to derive a local tracking controller for each UAV such that its position and yaw angle can converge to the reference ones. The proposed protocol is distributed in the sense that, the input update of each UAV dynamically relies only on local state information from its neighborhood set and the constraints, and it does not require any additional centralized information. It is demonstrated that under the proposed protocol, all UAVs reach consensus without violation of the environmental constraints. Finally, simulation and experimental results are provided to demonstrate the performance of the protocol.


Title: Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using Learned Interactions
Key Words: aerodynamics  aerospace robotics  control system synthesis  decentralised control  learning (artificial intelligence)  multi-robot systems  neurocontrollers  nonlinear control systems  particle swarm optimisation  stability  close-proximity multirotor control  learned interactions  Neural-Swarm  nonlinear decentralized stable controller  close-proximity flight  multirotor swarms  close-proximity control  complex aerodynamic interaction effects  safety distances  nominal dynamics model  regularized permutation-invariant Deep Neural Network  high-order multivehicle interactions  larger swarm sizes  baseline nonlinear  stable nonlinear tracking controller  Vehicle dynamics  Aerodynamics  Neural networks  Rotors  Stability analysis  Training  Robots 
Abstract: In this paper, we present Neural-Swarm, a nonlinear decentralized stable controller for close-proximity flight of multirotor swarms. Close-proximity control is challenging due to the complex aerodynamic interaction effects between multirotors, such as downwash from higher vehicles to lower ones. Conventional methods often fail to properly capture these interaction effects, resulting in controllers that must maintain large safety distances between vehicles, and thus are not capable of close-proximity flight. Our approach combines a nominal dynamics model with a regularized permutation-invariant Deep Neural Network (DNN) that accurately learns the high-order multi-vehicle interactions. We design a stable nonlinear tracking controller using the learned model. Experimental results demonstrate that the proposed controller significantly outperforms a baseline nonlinear tracking controller with up to four times smaller worst-case height tracking errors. We also empirically demonstrate the ability of our learned model to generalize to larger swarm sizes.


Title: Line Coverage with Multiple Robots
Key Words: computational complexity  graph theory  integer programming  linear programming  mobile robots  multi-robot systems  path planning  robot tour generation  multiple robots  line coverage problem  mixed integer linear program  NP-hard  merge-embed-merge  MEM algorithm  graph simplification  graph partitioning  Roads  Task analysis  Robot sensing systems  Routing  Heuristic algorithms  Partitioning algorithms 
Abstract: The line coverage problem is the coverage of linear environment features (e.g., road networks, power lines), modeled as 1D segments, by one or more robots while respecting resource constraints (e.g., battery capacity, flight time) for each of the robots. The robots incur direction dependent costs and resource demands as they traverse the edges. We treat the line coverage problem as an optimization problem, with the total cost of the tours as the objective, by formulating it as a mixed integer linear program (MILP). The line coverage problem is NP-hard and hence we develop a heuristic algorithm, Merge-Embed-Merge (MEM). We compare it against the optimal MILP approach and a baseline heuristic algorithm, Extended Path Scanning. We show the MEM algorithm is fast and suitable for real-time applications. To tackle large-scale problems, our approach performs graph simplification and graph partitioning, followed by robot tour generation for each of the partitioned subgraphs. We demonstrate our approach on a large graph with 4,658 edges and 4,504 vertices that represents an urban region of about 16 sq. km. We compare the performance of the algorithms on several small road networks and experimentally demonstrate the approach using UAVs on the UNC Charlotte campus road network.


Title: Visual Coverage Maintenance for Quadcopters Using Nonsmooth Barrier Functions
Key Words: aircraft control  helicopters  image sensors  mobile robots  multi-robot systems  robot vision  quadcopters  visual sensors  coverage holes  coverage quality  sufficient conditions  nonsmooth barrier functions  visual coverage maintenance  coverage control  necessary conditions  Visualization  Monitoring  Robot sensing systems  Switches  Space missions 
Abstract: This paper presents a coverage control algorithm for teams of quadcopters with downward facing visual sensors that prevents the appearance of coverage holes in-between the monitored areas while maximizing the coverage quality as much as possible. We derive necessary and sufficient conditions for preventing the appearance of holes in-between the fields of views among trios of robots. Because this condition can be expressed as logically combined constraints, control nonsmooth barrier functions are implemented to enforce it. An algorithm which extends control nonsmooth barrier functions to hybrid systems is implemented to manage the switching among barrier functions caused by the changes of the robots composing trio. The performance and validity of the proposed algorithm are evaluated in simulation as well as on a team of quadcopters.


Title: Goal-Directed Occupancy Prediction for Lane-Following Actors
Key Words: mobile robots  motion estimation  prediction theory  road safety  road traffic  road vehicles  roads  robot vision  traffic engineering computing  complex road networks  mapped road topology  dynamic road actors  mapped lane geometry  mode collapse problem  goal-directed occupancy prediction  lane-following actors  shared roads  safe autonomous driving  possible vehicle behaviors  possible goal reasoning  local scene context multimodality  high-level action set  future spatial occupancy prediction  Roads  Predictive models  Trajectory  Topology  Geometry  Task analysis  Autonomous vehicles 
Abstract: Predicting the possible future behaviors of vehicles that drive on shared roads is a crucial task for safe autonomous driving. Many existing approaches to this problem strive to distill all possible vehicle behaviors into a simplified set of high-level actions. However, these action categories do not suffice to describe the full range of maneuvers possible in the complex road networks we encounter in the real world. To combat this deficiency, we propose a new method that leverages the mapped road topology to reason over possible goals and predict the future spatial occupancy of dynamic road actors. We show that our approach is able to accurately predict future occupancy that remains consistent with the mapped lane geometry and naturally captures multi-modality based on the local scene context while also not suffering from the mode collapse problem observed in prior work.


Title: Intent-Aware Pedestrian Prediction for Adaptive Crowd Navigation
Key Words: adaptive control  collision avoidance  control engineering computing  mobile robots  navigation  pedestrians  road traffic control  traffic engineering computing  personal space violation  adaptive navigation policy  pedestrian motion  intent-aware pedestrian prediction  adaptive crowd navigation  mobile robots  pedestrian rich environments  robotic assistance  pedestrian navigation  real-world pedestrian datasets  Navigation  Trajectory  Robots  Prediction algorithms  Training  Collision avoidance  Uncertainty 
Abstract: Mobile robots capable of navigating seamlessly and safely in pedestrian rich environments promise to bring robotic assistance closer to our daily lives. In this paper we draw on insights of how humans move in crowded spaces to explore how to recognize pedestrian navigation intent, how to predict pedestrian motion and how a robot may adapt its navigation policy dynamically when facing unexpected human movements. Our approach is to develop algorithms that replicate this behavior. We experimentally demonstrate the effectiveness of our prediction algorithm using real-world pedestrian datasets and achieve comparable or better prediction accuracy compared to several state-of-the-art approaches. Moreover, we show that confidence of pedestrian prediction can be used to adjust the risk of a navigation policy adaptively to afford the most comfortable level as measured by the frequency of personal space violation in comparison with baselines. Furthermore, our adaptive navigation policy is able to reduce the number of collisions by 43% in the presence of novel pedestrian motion not seen during training.


Title: Brno Urban Dataset - The New Data for Self-Driving Agents and Mapping Tasks
Key Words: cameras  Global Positioning System  infrared detectors  mobile robots  optical radar  SLAM (robots)  WUXGA cameras  3D LiDAR  inertial measurement unit  infrared camera  differential RTK GNSS receiver  centimetre accuracy  public dataset  submillisecond precision  autonomous driving  Brno Urban dataset  self-driving agents  mapping tasks  Brno-Czech Republic  https://github.com/RoboticsBUT/Brno-Urban-Dataset  Cameras  Sensors  Global Positioning System  Global navigation satellite system  Receivers  Laser radar  Synchronization 
Abstract: Autonomous driving is a dynamically growing field of research, where quality and amount of experimental data is critical. Although several rich datasets are available these days, the demands of researchers and technical possibilities are evolving. Through this paper, we bring a new dataset recorded in Brno - Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with submillisecond precision to allow wider range of applications. At the time of publishing of this paper, recordings of more than 350 km of rides in varying environment are shared at: https://github.com/RoboticsBUT/Brno-Urban-Dataset.


Title: Imitative Reinforcement Learning Fusing Vision and Pure Pursuit for Self-driving
Key Words: generalisation (artificial intelligence)  intelligent robots  learning (artificial intelligence)  road vehicles  sensor fusion  steering systems  traffic engineering computing  pretrained IPP model  CARLA driving benchmark  generalization capability  pure pursuit  autonomous urban driving navigation  two-stage framework  visual information  pure-pursuit method  steering angle  imitation learning performance  driving data  reinforcement learning method  deep deterministic policy gradient  IPP-RL framework  Learning (artificial intelligence)  Meteorology  Robustness  Task analysis  Navigation  Training  Autonomous vehicles 
Abstract: Autonomous urban driving navigation is still an open problem and has ample room for improvement in unknown complex environments and terrible weather conditions. In this paper, we propose a two-stage framework, called IPP-RL, to handle these problems. IPP means an Imitation learning method fusing visual information with the additional steering angle calculated by Pure-Pursuit (PP) method, and RL means using Reinforcement Learning for further training. In our IPP model, the visual information captured by camera can be compensated by the calculated steering angle, thus it could perform well under bad weather conditions. However, imitation learning performance is limited by the driving data severely. Thus we use a reinforcement learning method-Deep Deterministic Policy Gradient (DDPG)-in the second stage training, which shares the learned weights from pretrained IPP model. In this way, our IPP-RL can lower the dependency of imitation learning on demonstration data and solve the problem of low exploration efficiency caused by randomly initialized weights in reinforcement learning. Moreover, we design a more reasonable reward function and use the n-step return to update the critic-network in DDPG. Our experiments on CARLA driving benchmark demonstrate that our IPP-RL is robust to lousy weather conditions and shows remarkable generalization capability in unknown environments on navigation task.


Title: ROI-cloud: A Key Region Extraction Method for LiDAR Odometry and Localization
Key Words: Bayes methods  distance measurement  feature extraction  image filtering  image matching  image registration  image sampling  Monte Carlo methods  optical radar  pose estimation  robot vision  ROI-cloud  key region extraction method  LiDAR odometry  LiDAR scan  on-board IMU/odometry data  Bayes filtering  Monte Carlo sampling  autonomous robot  LiDAR localization  voxelized cube set  pose estimation  point set registration  massive point cloud data  Feature extraction  Three-dimensional displays  Laser radar  Heuristic algorithms  Vehicle dynamics  Robots  Urban areas 
Abstract: We present a novel key region extraction method of point cloud, ROI-cloud, for LiDAR odometry and localization with autonomous robots. Traditional methods process massive point cloud data in every region within the field of view. In dense urban environments, however, processing redundant and dynamic regions of point cloud is time-consuming and harmful to the results of matching algorithms. In this paper, a voxelized cube set, ROI-cloud, is proposed to solve this problem by exclusively reserving the regions of interest for better point set registration and pose estimation. 3D space is firstly voxelized into weighted cubes. The key idea is to update their weights continually and extract cubes with high importance as key regions. By extracting geometrical features of a LiDAR scan, the importance of each cube is evaluated as a new measurement. With the help of on-board IMU/odometry data as well as new measurements, the weights of cubes are updated recursively through Bayes filtering. Thus, dynamic and redundant point cloud inside cubes with low importance are discarded by means of Monte Carlo sampling. Our method is validated on various datasets, and results indicate that the ROI-cloud improves the existing method in both accuracy and speed.


Title: To Learn or Not to Learn: Visual Localization from Essential Matrices
Key Words: cameras  feature extraction  image representation  learning (artificial intelligence)  neural nets  pose estimation  robot vision  SLAM (robots)  visual localization  scene-specific representations  relative pose estimation  feature-based approach  deep learning  camera  autonomous robots  Cameras  Visualization  Three-dimensional displays  Pipelines  Pose estimation  Image retrieval 
Abstract: Visual localization is the problem of estimating a camera within a scene and a key technology for autonomous robots. State-of-the-art approaches for accurate visual localization use scene-specific representations, resulting in the overhead of constructing these models when applying the techniques to new scenes. Recently, learned approaches based on relative pose estimation have been proposed, carrying the promise of easily adapting to new scenes. However, they are currently significantly less accurate than state-of-the-art approaches. In this paper, we are interested in analyzing this behavior. To this end, we propose a novel framework for visual localization from relative poses. Using a classical feature-based approach within this framework, we show state-of-the-art performance. Replacing the classical approach with learned alternatives at various levels, we then identify the reasons for why deep learned approaches do not perform well. Based on our analysis, we make recommendations for future work.


Title: Hierarchical Multi-Process Fusion for Visual Place Recognition
Key Words: image fusion  mobile robots  object recognition  robot vision  hierarchical multiprocess fusion  multiple complementary techniques  visual localization  multisensor fusion  varying performance characteristics  hierarchical localization system  localization hypotheses  localization performance  final localization stage  parallel fusion  visual place recognition  Databases  Visualization  Feature extraction  Robots  Pipelines  Histograms 
Abstract: Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpointinvariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques.


Title: Fast, Compact and Highly Scalable Visual Place Recognition through Sequence-based Matching of Overloaded Representations
Key Words: image matching  image representation  image sequences  mobile robots  quantisation (signal)  robot vision  storage management  hashing overload approach  compact place recognition  overloaded representations  storage footprint  place recognition system  ultra-compact place representations  sublinear storage scaling  sublinear computational scaling  visual place recognition  match selections  sequence-based matching  scalar quantization-based hashing  mobile robot  Quantization (signal)  Visualization  Indexes  Robots  Principal component analysis  Benchmark testing  Image recognition 
Abstract: Visual place recognition algorithms trade off three key characteristics: their storage footprint, their computational requirements, and their resultant performance, often expressed in terms of recall rate. Significant prior work has investigated highly compact place representations, sub-linear computational scaling and sub-linear storage scaling techniques, but have always involved a significant compromise in one or more of these regards, and have only been demonstrated on relatively small datasets. In this paper we present a novel place recognition system which enables for the first time the combination of ultra-compact place representations, near sub-linear storage scaling and extremely lightweight compute requirements. Our approach exploits the inherently sequential nature of much spatial data in the robotics domain and inverts the typical target criteria, through intentionally coarse scalar quantization-based hashing that leads to more collisions but is resolved by sequence-based matching. For the first time, we show how effective place recognition rates can be achieved on a new very large 10 million place dataset, requiring only 8 bytes of storage per place and 37K unitary operations to achieve over 50% recall for matching a sequence of 100 frames, where a conventional stateof-the-art approach both consumes 1300 times more compute and fails catastrophically. We present analysis investigating the effectiveness of our hashing overload approach under varying sizes of quantized vector length, comparison of near miss matches with the actual match selections and characterise the effect of variance re-scaling of data on quantization. Resource link: https://github.com/oravus/CoarseHash.


Title: Vision-based Multi-MAV Localization with Anonymous Relative Measurements Using Coupled Probabilistic Data Association Filter
Key Words: aerospace robotics  microrobots  mobile robots  multi-robot systems  pose estimation  probability  robot vision  SLAM (robots)  target tracking  multiMAV system  robot team  vision based detection  distance measurements  coupled probabilistic data association filter  nonlinear measurements  visual based robot to robot detection  vision based multiMAV localization  robot localization  robot pose estimation  multiple microaerial vehicles  Robot kinematics  Robot sensing systems  Noise measurement  Probabilistic logic  Task analysis  Cameras 
Abstract: We address the localization of robots in a multi-MAV system where external infrastructure like GPS or motion capture systems may not be available. Our approach lends itself to implementation on platforms with several constraints on size, weight, and power (SWaP). Particularly, our framework fuses the onboard VIO with the anonymous, visual-based robot-to-robot detection to estimate all robot poses in one common frame, addressing three main challenges: 1) the initial configuration of the robot team is unknown, 2) the data association between each vision-based detection and robot targets is unknown, and 3) the vision-based detection yields false negatives, false positives, inaccurate, and provides noisy bearing, distance measurements of other robots. Our approach extends the Coupled Probabilistic Data Association Filter [1] to cope with nonlinear measurements. We demonstrate the superior performance of our approach over a simple VIO-based method in a simulation with the measurement models statistically modeled using the real experimental data. We also show how onboard sensing, estimation, and control can be used for formation flight.


Title: MANGA: Method Agnostic Neural-policy Generalization and Adaptation
Key Words: learning (artificial intelligence)  robot dynamics  MANGA  multiple environments  dynamics parameters  motor noise variations  policy learning  system identification  unknown environment  dynamics configurations  dynamics conditioned policies  off-policy state-transition rollouts  training method  method agnostic neural-policy generalization and adaptation  transferring policies  Training  Robots  Task analysis  Encoding  Decoding  Heuristic algorithms  Adaptation models 
Abstract: In this paper we target the problem of transferring policies across multiple environments with different dynamics parameters and motor noise variations, by introducing a framework that decouples the processes of policy learning and system identification. Efficiently transferring learned policies to an unknown environment with changes in dynamics configurations in the presence of motor noise is very important for operating robots in the real world, and our work is a novel attempt in that direction. We introduce MANGA: Method Agnostic Neural-policy Generalization and Adaptation, that trains dynamics conditioned policies and efficiently learns to estimate the dynamics parameters of the environment given off-policy state-transition rollouts in the environment. Our scheme is agnostic to the type of training method used - both reinforcement learning (RL) and imitation learning (IL) strategies can be used. We demonstrate the effectiveness of our approach by experimenting with four different MuJoCo agents and comparing against previously proposed transfer baselines.


Title: Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference
Key Words: learning (artificial intelligence)  mobile robots  navigation  robust control  fast adaptation  deep reinforcement learning-based navigation skills  human preference  robot navigation  robustness  maximum velocities  reward components  optimal choice  real-world service scenarios  deep RL navigation method  reward functions  Bayesian deep learning method  preference data  diverse navigation skills  deep RL navigation agents  Navigation  Collision avoidance  Bayes methods  Training  Robot kinematics  Machine learning 
Abstract: Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.


Title: Variational Inference with Mixture Model Approximation for Applications in Robotics
Key Words: approximation theory  Bayes methods  control engineering computing  inference mechanisms  mixture models  robots  statistical distributions  variational inference  mixture model approximation  robot configurations  Bayesian computation  Robots  Mixture models  Kinematics  Bayes methods  Optimization  Task analysis  Planning 
Abstract: We propose to formulate the problem of representing a distribution of robot configurations (e.g. joint angles) as that of approximating a product of experts. Our approach uses variational inference, a popular method in Bayesian computation, which has several practical advantages over sampling-based techniques. To be able to represent complex and multimodal distributions of configurations, mixture models are used as approximate distribution. We show that the problem of approximating a distribution of robot configurations while satisfying multiple objectives arises in a wide range of problems in robotics, for which the properties of the proposed approach have relevant consequences. Several applications are discussed, including learning objectives from demonstration, planning, and warm-starting inverse kinematics problems. Simulated experiments are presented with a 7-DoF Panda arm and a 28-DoF Talos humanoid.


Title: Passive Quadrupedal Gait Synchronization for Extra Robotic Legs Using a Dynamically Coupled Double Rimless Wheel Model
Key Words: gait analysis  legged locomotion  motion control  robot dynamics  springs (mechanical)  synchronisation  wheels  extra robotic legs system  robotic augmentation  human operator  articulated robot legs  human-XRL quadruped system  rear legs  quadrupedal robots  quadrupedal locomotion  coupler design parameters  passive quadrupedal gait synchronization  dynamically coupled double rimless wheel system  Poincaré return map  numerical simulation  Legged locomotion  Wheels  Synchronization  Couplers  Robot kinematics  Human Augmentation  Supernumerary Robotic Limbs  Exoskeletons  Locomotion  Nonlinear Dynamics 
Abstract: The Extra Robotic Legs (XRL) system is a robotic augmentation worn by a human operator consisting of two articulated robot legs that walk with the operator and help bear a heavy backpack payload. It is desirable for the Human-XRL quadruped system to walk with the rear legs lead the front by 25% of the gait period, minimizing the energy lost from foot impacts while maximizing balance stability. Unlike quadrupedal robots, the XRL cannot command the human's limbs to coordinate quadrupedal locomotion. Using a pair of Rimless Wheel models, it is shown that the systems coupled with a spring and damper converge to the desired 25% phase difference. A Poincaré return map was generated using numerical simulation to examine the convergence properties to different coupler design parameters, and initial conditions. The Dynamically Coupled Double Rimless Wheel system was physically realized with a spring and dashpot chosen from the theoretical results, and initial experiments indicate that the desired synchronization properties may be achieved within several steps using this set of passive components alone.


Title: Optimal Fast Entrainment Waveform for Indirectly Controlled Limit Cycle Walker Against External Disturbances
Key Words: legged locomotion  limit cycles  numerical analysis  oscillators  robot dynamics  stability  optimal fast entrainment waveform  phase recovery  phase reduction theory  entrainment effect  limit cycle walking  indirectly controlled limit cycle walker  external disturbances  occasional perturbation  closed orbit  phase space  successive perturbation  accumulated deviation  control law  disturbed phase  wobbling mass  wobbling motion  Limit-cycles  Legged locomotion  Perturbation methods  Mathematical model  Oscillators  Convergence  Trajectory 
Abstract: After occasional perturbation, it is crucial to spontaneously control the limit cycle walking so that it quickly returns to its closed orbit in phase space. Otherwise, its stability can not be sufficiently guaranteed if the speed of recovery is slow while successive perturbation is applied. The accumulated deviation may eventually drive the phase outside the basin of attraction, leading to failure of the walking. In this sense, a control law that quickly recovers the disturbed phase before encountering the following perturbations is indispensable. With this consideration, here we analytically derive an optimal fast entrainment waveform that maximizes the speed of phase recovery based on phase reduction theory. Our theoretical method is numerically evaluated using a limit cycle walker, which is indirectly controlled by the oscillation of a wobbling mass via entrainment effect. The obtained waveform is used as the desired trajectory of the wobbling motion. The simulation results show that the waveform we derived achieves the best performance among all candidates. Our method helps to enhance the stability of limit cycle walking.


Title: Correspondence Identification in Collaborative Robot Perception through Maximin Hypergraph Matching
Key Words: concave programming  graph theory  image matching  image representation  multi-robot systems  object detection  robot vision  collaborative multirobot perception  nonconvex noncontinuous optimization problem  multirobot coordination  collaborative robot perception  hypergraph matching approach  Collaboration  Robot kinematics  Object recognition  Optimization  Robot sensing systems  Robustness 
Abstract: Correspondence identification is an essential problem for collaborative multi-robot perception, with the objective of deciding the correspondence of objects that are observed in the field of view of each robot. In this paper, we introduce a novel maximin hypergraph matching approach that formulates correspondence identification as a hypergraph matching problem. The proposed approach incorporates both spatial relationships and appearance features of objects to improve representation capabilities. It also integrates the maximin theorem to optimize the worst-case scenario in order to address distractions caused by non-covisible objects. In addition, we design an optimization algorithm to address the formulated non-convex non-continuous optimization problem. We evaluate our approach and compare it with seven previous techniques in two application scenarios, including multi-robot coordination on real robots and connected autonomous driving in simulations. Experimental results have validated the effectiveness of our approach in identifying object correspondence from partially overlapped views in collaborative perception, and have shown that the proposed maximin hypergraph matching approach outperforms previous techniques and obtains state-of-the-art performance.


Title: Flying batteries: In-flight battery switching to increase multirotor flight time
Key Words: aerospace control  helicopters  mobile robots  secondary cells  in-flight battery switching  multirotor flight time  mid-air docking  primary battery  quadcopter flight  docking platform  flying battery  secondary battery  arbitrary switching  Batteries  Switches  Legged locomotion  Switching circuits  Aerodynamics  Connectors  Propellers 
Abstract: We present a novel approach to increase the flight time of a multirotor via mid-air docking and in-flight battery switching. A main quadcopter flying using a primary battery has a docking platform attached to it. A `flying battery' - a small quadcopter carrying a secondary battery - is equipped with docking legs that can mate with the main quadcopter's platform. Connectors between the legs and the platform establish electrical contact on docking, and enable power transfer from the secondary battery to the main quadcopter. A custom-designed circuit allows arbitrary switching between the primary battery and secondary battery. We demonstrate the concept in a flight experiment involving repeated docking, battery switching, and undocking. This is shown in the video attachment. The experiment increases the flight time of the main quadcopter by a factor of 4.7× compared to solo flight, and 2.2× a theoretical limit for that given multirotor. Importantly, this increase in flight time is not associated with a large increase in overall vehicle mass or size, leaving the main quadcopter in fundamentally the same safety class.


Title: Optimal Control of an Energy-Recycling Actuator for Mobile Robotics Applications
Key Words: actuators  clutches  electric motors  energy consumption  gears  integer programming  mobile robots  optimal control  power consumption  quadratic programming  springs (mechanical)  torque  actuator power consumption  mobile robot design  elastic energy  electrical energy consumption  optimal control  given actuator design  optimized actuator energy consumption  optimized gear motor  simulated energy-recycling actuator  mobile robotics applications  Springs  Actuators  Torque  Power demand  Force  Robots  Gears  Optimization and optimal control  force control  prosthetics and exoskeletons 
Abstract: Actuator power consumption is a limiting factor in mobile robot design. In this paper we introduce the concept of an energy-recycling actuator, which uses an array of springs and clutches to capture and return elastic energy in parallel with an electric motor. Engaging and disengaging clutches appropriately could reduce electrical energy consumption without sacrificing controllability, but presents a challenging control problem. We formulated the optimal control objective of minimizing actuator power consumption as a mixed-integer quadratic program (MIQP) and solved for the global minimum. For a given actuator design and a wide range of simulated torque and rotation patterns, all corresponding to zero net work over one cycle, we compared optimized actuator energy consumption to that of an optimized gear motor with simple parallel elasticity. The simulated energy-recycling actuator consumed less electrical energy: 57% less on average and 80% less in the best case. These results demonstrate an effective approach to optimal control of this type of system, and suggest that energy-recycling actuators could substantially reduce power consumption in some robotics applications.


Title: An NMPC Approach using Convex Inner Approximations for Online Motion Planning with Guaranteed Collision Avoidance
Key Words: collision avoidance  continuous time systems  convex programming  mobile robots  nonlinear control systems  predictive control  trajectory control  trajectory optimization  NMPC approach  mobile robots  continuous time collision avoidance  kinodynamic feasibility  nonlinear model predictive control  convex inner approximation  online motion planning  continuous time collision free trajectories  Collision avoidance  Robots  Trajectory optimization  Planning  Dynamics 
Abstract: Even though mobile robots have been around for decades, trajectory optimization and continuous time collision avoidance remain subject of active research. Existing methods trade off between path quality, computational complexity, and kinodynamic feasibility. This work approaches the problem using a nonlinear model predictive control (NMPC) framework, that is based on a novel convex inner approximation of the collision avoidance constraint. The proposed Convex Inner ApprOximation (CIAO) method finds kinodynamically feasible and continuous time collision free trajectories, in few iterations, typically one. For a feasible initialization, the approach is guaranteed to find a feasible solution, i.e. it preserves feasibility. Our experimental evaluation shows that CIAO outperforms state of the art baselines in terms of planning efficiency and path quality. Experiments show that it also efficiently scales to high-dimensional systems. Furthermore real-world experiments demonstrate its capability of unifying trajectory optimization and tracking for safe motion planning in dynamic environments.


Title: Action Image Representation: Learning Scalable Deep Grasping Policies with Zero Real World Data
Key Words: convolutional neural nets  feature extraction  grippers  image colour analysis  image representation  learning (artificial intelligence)  robot vision  vectors  Action image representation  zero real world data  end-to-end deep-grasping policy  grasp quality  object-gripper relationship  deep convolutional network  Action Image representation  color images  depth images  combined color-depth  scalable deep grasping policy learning  salient feature extraction  Grasping  Robot sensing systems  Image representation  Proposals  Grippers  Task analysis 
Abstract: This paper introduces Action Image, a new grasp proposal representation that allows learning an end-to-end deep-grasping policy. Our model achieves 84% grasp success on 172 real world objects while being trained only in simulation on 48 objects with just naive domain randomization. Similar to computer vision problems, such as object detection, Action Image builds on the idea that object features are invariant to translation in image space. Therefore, grasp quality is invariant when evaluating the object-gripper relationship; a successful grasp for an object depends on its local context, but is independent of the surrounding environment. Action Image represents a grasp proposal as an image and uses a deep convolutional network to infer grasp quality. We show that by using an Action Image representation, trained networks are able to extract local, salient features of grasping tasks that generalize across different objects and environments. We show that this representation works on a variety of inputs, including color images (RGB), depth images (D), and combined color-depth (RGB-D). Our experimental results demonstrate that networks utilizing an Action Image representation exhibit strong domain transfer between training on simulated data and inference on real-world sensor streams. Finally, our experiments show that a network trained with Action Image improves grasp success (84% vs. 53%) over a baseline model with the same structure, but using actions encoded as vectors.


Title: High Accuracy and Efficiency Grasp Pose Detection Scheme with Dense Predictions
Key Words: grippers  image colour analysis  image resolution  learning (artificial intelligence)  neural nets  object detection  pose estimation  robot vision  parallel plate gripper  neural network  pixel location  nonmaximum suppression  sampling grasps  learning-based grasp pose detection scheme  channels images  resolution RGB image  robot grasping success rate  Cornell Grasp Dataset  predicted grasps  dense predictions  dense grasp predictions  ranking procedures  clustering procedures  NMS  nonmaximum suppression strategy  detection model  generating grasp proposals  intermediate procedures  detection accuracy  fine-tuning steps  detection algorithms  Predictive models  Solid modeling  Grippers  Robots  Three-dimensional displays  Feature extraction  Image edge detection 
Abstract: Learning-based grasp pose detection algorithms have boosted the performance of robot grasping, but they usually need manually fine-tuning steps to find the balance between detection accuracy and efficient. In this paper, we discard these intermediate procedures, like sampling grasps and generating grasp proposals, and propose an end-to-end grasp pose detection model. Our model uses the RGB image as the input and predicts the single grasp pose in each small grid of the image. Furthermore, the best grasps are found by non-maximum suppression (NMS) strategy. The clustering and ranking procedures are left for NMS while the network only generates dense grasp predictions, which keeps the network simple and efficient. To achieve dense predictions, the predicted grasps of our detection model are represented by the 6 channels images with each pixel location representing a rated grasp. To the best of our knowledge, our model is the first neural network that attaches a grasp pose in pixel level. The model achieves 96.5% accuracy which costs 14ms for prediction of a 480×360 resolution RGB image in Cornell Grasp Dataset, and 90.4% robot grasping success rate for unknown objects with a parallel plate gripper in the real environment.


Title: Transferable Active Grasping and Real Embodied Dataset
Key Words: image colour analysis  learning (artificial intelligence)  manipulators  robot vision  stereo image processing  cluttered scenes  robot vision systems  reinforcement learning framework  3D vision architectures  RGB-D cameras  3-stage transferable active grasping pipeline  real embodied dataset  RED  Grasping  Clutter  Cameras  Image segmentation  Robot vision systems 
Abstract: Grasping in cluttered scenes is challenging for robot vision systems, as detection accuracy can be hindered by partial occlusion of objects. We adopt a reinforcement learning (RL) framework and 3D vision architectures to search for feasible viewpoints for grasping by the use of hand-mounted RGB-D cameras. To overcome the disadvantages of photo-realistic environment simulation, we propose a large-scale dataset called Real Embodied Dataset (RED), which includes full-viewpoint real samples on the upper hemisphere with amodal annotation and enables a simulator that has real visual feedback. Based on this dataset, a practical 3-stage transferable active grasping pipeline is developed, that is adaptive to unseen clutter scenes. In our pipeline, we propose a novel mask-guided reward to overcome the sparse reward issue in grasping and ensure category-irrelevant behavior. The grasping pipeline and its possible variants are evaluated with extensive experiments both in simulation and on a real-world UR-5 robotic arm.


Title: PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation Algorithm from Sparse Point Clouds
Key Words: edge detection  feature extraction  image annotation  image colour analysis  learning (artificial intelligence)  manipulators  neural nets  robot vision  PointNet++ grasping  sparse point clouds  robot manipulation  local feature extractor  deep learning  multiobject scene  multiobject dataset  grasp generation algorithm  multiobject grasp detection algorithm  Ferrari-Canny metrics  PointNet++ based network  Three-dimensional displays  Feature extraction  Grasping  Measurement  Training  Cameras  Pipelines 
Abstract: Grasping for novel objects is important for robot manipulation in unstructured environments. Most of current works require a grasp sampling process to obtain grasp candidates, combined with local feature extractor using deep learning. This pipeline is time-costly, expecially when grasp points are sparse such as at the edge of a bowl.In this paper, we propose an end-to-end approach to directly predict the poses, categories and scores (qualities) of all the grasps. It takes the whole sparse point clouds as the input and requires no sampling or search process. Moreover, to generate training data of multi-object scene, we propose a fast multi-object grasp detection algorithm based on Ferrari Canny metrics. A single-object dataset (79 objects from YCB object set, 23.7k grasps) and a multi-object dataset (20k point clouds with annotations and masks) are generated. A PointNet++ based network combined with multi-mask loss is introduced to deal with different training points. The whole weight size of our network is only about 11.6M, which takes about 102ms for a whole prediction process using a GeForce 840M GPU. Our experiment shows our work get 71.43% success rate and 91.60% completion rate, which performs better than current state-of-art works.


Title: Clear Grasp: 3D Shape Estimation of Transparent Objects for Manipulation
Key Words: control engineering computing  convolutional neural nets  image colour analysis  image reconstruction  image representation  image sensors  learning (artificial intelligence)  manipulators  robot vision  stereo image processing  ClearGrasp  monocular depth estimation baselines  transparent objects  3D shape estimation  3D geometry  RGB-D image  transparent surfaces  occlusion boundaries  robotic manipulation  Three-dimensional displays  Geometry  Estimation  Solid modeling  Cameras  Image edge detection  Optimization 
Abstract: Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp - a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms' performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials: https://sites.google.com/view/cleargrasp.


Title: YCB-M: A Multi-Camera RGB-D Dataset for Object Recognition and 6DoF Pose Estimation
Key Words: cameras  feature extraction  image colour analysis  image segmentation  object detection  object recognition  pose estimation  stereo image processing  2D bounding boxes  3D cameras  YCB-M  multicamera RGB-D dataset  estimation system  object recognition  3D bounding boxes  ground truth 6DoF poses  YCB object  camera model  robust algorithms  estimation algorithms  6DoF pose estimation  Cameras  Robot vision systems  Three-dimensional displays  Pose estimation  Two dimensional displays 
Abstract: While a great variety of 3D cameras have been introduced in recent years, most publicly available datasets for object recognition and pose estimation focus on one single camera. In this work, we present a dataset of 32 scenes that have been captured by 7 different 3D cameras, totaling 49,294 frames. This allows evaluating the sensitivity of pose estimation algorithms to the specifics of the used camera and the development of more robust algorithms that are more independent of the camera model. Vice versa, our dataset enables researchers to perform a quantitative comparison of the data from several different cameras and depth sensing technologies and evaluate their algorithms before selecting a camera for their specific task. The scenes in our dataset contain 20 different objects from the common benchmark YCB object and model set [1], [2]. We provide full ground truth 6DoF poses for each object, per-pixel segmentation, 2D and 3D bounding boxes and a measure of the amount of occlusion of each object. We have also performed an initial evaluation of the cameras using our dataset on a state-of-the-art object recognition and pose estimation system [3].


Title: Self-supervised 6D Object Pose Estimation for Robot Manipulation
Key Words: image segmentation  intelligent robots  manipulators  pose estimation  robot vision  supervised learning  robot manipulation  self-supervised 6D object pose estimation  self-supervised learning  object configuration  pose estimation modules  object segmentation  6D pose estimation performance  robots skill teaching  Three-dimensional displays  Pose estimation  Cameras  Training  Manipulators  Grasping 
Abstract: To teach robots skills, it is crucial to obtain data with supervision. Since annotating real world data is time-consuming and expensive, enabling robots to learn in a self- supervised way is important. In this work, we introduce a robot system for self-supervised 6D object pose estimation. Starting from modules trained in simulation, our system is able to label real world images with accurate 6D object poses for self-supervised learning. In addition, the robot interacts with objects in the environment to change the object configuration by grasping or pushing objects. In this way, our system is able to continuously collect data and improve its pose estimation modules. We show that the self-supervised learning improves object segmentation and 6D pose estimation performance, and consequently enables the system to grasp objects more reliably. A video showing the experiments can be found at https://youtu.be/W1Y0Mmh1Gd8.


Title: Evaluation of Non-collocated Force Feedback Driven by Signal-independent Noise
Key Words: brain-computer interfaces  feedback  force feedback  haptic interfaces  medical computing  neurophysiology  prosthetics  exploratory action  conventional haptic interface  iBCI-based prostheses control strategies  neural prostheses  intracortical brain computer interface  input signal  robotic prostheses  paralysis  signal-independent noise  noncollocated force feedback  signal-to-noise ratio  virtual environment  noncollocated haptic feedback  Force  Haptic interfaces  Task analysis  Noise measurement  Virtual environments  Signal to noise ratio  Probes 
Abstract: Individuals living with paralysis or amputation can operate robotic prostheses using input signals based on their intent or attempt to move. Because sensory function is lost or diminished in these individuals, haptic feedback must be non-collocated. The intracortical brain computer interface (iBCI) has enabled a variety of neural prostheses for people with paralysis. An important attribute of the iBCI is that its input signal contains signal-independent noise. To understand the effects of signal-independent noise on a system with non-collocated haptic feedback and inform iBCI-based prostheses control strategies, we conducted an experiment with a conventional haptic interface as a proxy for the iBCI. Ablebodied users were tasked with locating an indentation within a virtual environment using input from their right hand. Non-collocated haptic feedback of the interaction forces in the virtual environment was augmented with noise of three different magnitudes and simultaneously rendered on users' left hands. We found increases in distance error of the guess of the indentation location, mean time per trial, mean peak absolute displacement and speed of tool movements during localization for the highest noise level compared to the other two levels. The findings suggest that users have a threshold of disturbance rejection and that they attempt to increase their signal-to-noise ratio through their exploratory actions.


Title: Tactile sensing based on fingertip suction flow for submerged dexterous manipulation
Key Words: dexterous manipulators  grippers  motion control  neurocontrollers  recurrent neural nets  tactile sensors  saltwater corrosion  low-light conditions  robust electrical parts  mechanical parts  underwater robots  mobile manipulation tasks  suction flow mechanism  orifice occlusion  ambient pressure  tactile sensing modality  automated robotic behaviors  fingertip suction flow  submerged dexterous manipulation  robotic systems  Electron tubes  Sea measurements  Tactile sensors  Oceans 
Abstract: The ocean is a harsh and unstructured environment for robotic systems; high ambient pressures, saltwater corrosion and low-light conditions demand machines with robust electrical and mechanical parts that are able to sense and respond to the environment. Prior work shows that the addition of gentle suction flow to the hands of underwater robots can aid in the handling of objects during mobile manipulation tasks. The current paper explores using this suction flow mechanism as a new modality for tactile sensing; by monitoring orifice occlusion we can get a sense of how objects make contact in the hand. The electronics required for this sensor can be located remotely from the hand and the signal is insensitive to large changes in ambient pressure associated with diving depth. In this study, suction is applied to the fingertips of a two-fingered compliant gripper and suction-based tactile sensing is monitored while an object is pulled out of a pinch grasp. As a proof of concept, a recurrent neural network model was trained to predict external force trends using only the suction signals. This tactile sensing modality holds the potential to enable automated robotic behaviors or to provide operators of remotely operated vehicles with additional feedback in a robust fashion suitable for ocean deployment.


Title: Online Trajectory Planning Through Combined Trajectory Optimization and Function Approximation: Application to the Exoskeleton Atalante
Key Words: control engineering computing  function approximation  learning (artificial intelligence)  mobile robots  optimisation  path planning  robust control  trajectory control  wearable robots  trajectory optimization  function approximation  autonomous robots  online trajectory planning  guided trajectory learning  self-balanced exoskeleton  Atalante  robust control strategies  Trajectory optimization  Function approximation  Task analysis  Legged locomotion 
Abstract: Autonomous robots require online trajectory planning capability to operate in the real world. Efficient offline trajectory planning methods already exist, but are computationally demanding, preventing their use online. In this paper, we present a novel algorithm called Guided Trajectory Learning that learns a function approximation of solutions computed through trajectory optimization while ensuring accurate and reliable predictions. This function approximation is then used online to generate trajectories. This algorithm is designed to be easy to implement, and practical since it does not require massive computing power. It is readily applicable to any robotics systems and effortless to set up on real hardware since robust control strategies are usually already available. We demonstrate the computational performance of our algorithm on flat-foot walking with the self-balanced exoskeleton Atalante.


Title: Act, Perceive, and Plan in Belief Space for Robot Localization
Key Words: mobile robots  object recognition  path planning  belief space  robot localization  interleaved acting  planning technique  low-level geometric features  task planner  perception tasks  state spaces  Uncertainty  Planning  Task analysis  Object detection  Robot sensing systems  Probabilistic logic 
Abstract: In this paper, we outline an interleaved acting and planning technique to rapidly reduce the uncertainty of the estimated robot's pose by perceiving relevant information from the environment, as recognizing an object or asking someone for a direction. Generally, existing localization approaches rely on low-level geometric features such as points, lines, and planes. While these approaches provide the desired accuracy, they may require time to converge, especially with incorrect initial guesses. In our approach, a task planner computes a sequence of action and perception tasks to actively obtain relevant information from the robot's perception system. We validate our approach in large state spaces, to show how the approach scales, and in real environments, to show the applicability of our method on real robots. We prove that our approach is sound, probabilistically complete, and tractable in practical cases.


Title: Fast and resilient manipulation planning for target retrieval in clutter
Key Words: collision avoidance  image retrieval  manipulators  mobile robots  object detection  resilient manipulation planning  clutter  task and motion planning  robotic manipulator  target object retrieval  collision-free path  object rearrangement  TAMP framework  static environments  pick-and-place actions  baseline methods  Planning  Task analysis  Clutter  Collision avoidance  Manipulators  Search problems 
Abstract: This paper presents a task and motion planning (TAMP) framework for a robotic manipulator in order to retrieve a target object from clutter. We consider a configuration of objects in a confined space with a high density so no collision-free path to the target exists. The robot must relocate some objects to retrieve the target without collisions. For fast completion of object rearrangement, the robot aims to optimize the number of pick-and-place actions which often determines the efficiency of a TAMP framework.We propose a task planner incorporating motion planning to generate executable plans which aims to minimize the number of pick-and-place actions. In addition to fully known and static environments, our method can deal with uncertain and dynamic situations incurred by occluded views. Our method is shown to reduce the number of pick-and-place actions compared to baseline methods (e.g., at least 28.0% of reduction in a known static environment with 20 objects).


Title: Untethered Soft Millirobot with Magnetic Actuation
Key Words: bending  magnetic actuators  magnetic flux  microrobots  mobile robots  motion control  permanent magnets  polymers  robot kinematics  rods (structures)  untethered soft millirobot  magnetic actuation  scalable designs  moulding technique  acrylonitrile butadiene styrene filaments  embedded permanent magnets  soft-body  soft-robots  external uniform magnetic field control system  magnetic flux densities  soft-robotic body  magnetic field strength  motion modes  magnetic field inputs  hollow rod-like structures  polydimethylsiloxane  3D printed polylactic acid rings  pivot walking  rolling motion  tumbling motion  wiggling motion  side-tapping motion  wavy motion  deflection curve  navigation  minimally invasive in vivo applications  bending angle  Robots  Permanent magnets  Magnetic resonance imaging  Programmable logic arrays  Fabrication  Plastics  Three-dimensional displays 
Abstract: This paper presents scalable designs and fabrication, actuation, and manipulation techniques for soft millirobots under uniform magnetic field control. The millirobots were fabricated through an economic and robust moulding technique using polydimethylsiloxane (PDMS), acrylonitrile butadiene styrene (ABS) filaments, and 3D printed polylactic acid (PLA) rings. The soft millirobots were simple hollow rod-like structures with different configurations of embedded permanent magnets inside of their soft-body or at their ends. The soft-robots were actuated using six different motion modes including: pivot walking, rolling, tumbling, side-tapping, wiggling, and wavy-motion under an external uniform magnetic field control system. The velocities of the millirobots under different motion modes were analyzed under varying magnetic flux densities (B). Moreover, deformation of the soft-robotic body in response to the magnetic field strength was measured and a deflection curve showing bending angle (φ) was produced. Soft millirobots were navigated through a maze using a combination of the available motion modes. Different arrangements of the embedded permanent magnets enabled individual soft millirobots to respond heterogeneously under the same magnetic field inputs towards performing assembly and disassembly operation as modular subunits. Overall, this soft millirobot platform shows enormous potential for minimally invasive in vivo applications.


Title: Accelerated Robot Learning via Human Brain Signals
Key Words: brain  electroencephalography  feedback  learning (artificial intelligence)  medical robotics  medical signal processing  accelerated robot learning  human brain signals  reinforcement learning  RL algorithms struggle  learning signal  robot learning process  error-related signal  measurable using electroencephelography  robotic agents  sparse reward settings  human observer  robot attempts  noisy error feedback signal  supervised learning  RL learning process  robotic navigation task  Task analysis  Robots  Electroencephalography  Navigation  Hafnium  Learning (artificial intelligence)  Training 
Abstract: In reinforcement learning (RL), sparse rewards are a natural way to specify the task to be learned. However, most RL algorithms struggle to learn in this setting since the learning signal is mostly zeros. In contrast, humans are good at assessing and predicting the future consequences of actions and can serve as good reward/policy shapers to accelerate the robot learning process. Previous works have shown that the human brain generates an error-related signal, measurable using electroencephelography (EEG), when the human perceives the task being done erroneously. In this work, we propose a method that uses evaluative feedback obtained from human brain signals measured via scalp EEG to accelerate RL for robotic agents in sparse reward settings. As the robot learns the task, the EEG of a human observer watching the robot attempts is recorded and decoded into noisy error feedback signal. From this feedback, we use supervised learning to obtain a policy that subsequently augments the behavior policy and guides exploration in the early stages of RL. This bootstraps the RL learning process to enable learning from sparse reward. Using a simple robotic navigation task as a test bed, we show that our method achieves a stable obstacle-avoidance policy with high success rate, outperforming learning from sparse rewards only that struggles to achieve obstacle avoidance behavior or fails to advance to the goal.


Title: Real-Time Robot Reach-To-Grasp Movements Control Via EOG and EMG Signals Decoding
Key Words: biomechanics  brain-computer interfaces  electromyography  electro-oculography  grippers  human-robot interaction  man-machine systems  medical robotics  medical signal processing  neurophysiology  signal classification  time robot reach-to-grasp movement control  grasping task  industrial robot  eye movements  electromyography signals  real-time human-robot interface system  human-controlled assistive devices  dexterous devices  EOG signals  robot arms  real-time control  HRI systems  robot control  EMG signals  real-time decoding  EMG decoding  UR-10 robot arm  Electrooculography  Electromyography  Real-time systems  Decoding  Electrodes  Service robots 
Abstract: In this paper, we propose a real-time human-robot interface (HRI) system, where Electrooculography (EOG) and Electromyography (EMG) signals were decoded to perform reach-to-grasp movements. For that, five different eye movements (up, down, left, right and rest) were classified in real-time and translated into commands to steer an industrial robot (UR-10) to one of the four approximate target directions. Thereafter, EMG signals were decoded to perform the grasping task using an attached gripper to the UR-10 robot arm. The proposed system was tested offline on three different healthy subjects, and mean validation accuracy of 93.62% and 99.50% were obtained across the three subjects for EOG and EMG decoding, respectively. Furthermore, the system was successfully tested in real-time with one subject, and mean online accuracy of 91.66% and 100% were achieved for EOG and EMG decoding, respectively. Our results obtained by combining real-time decoding of EOG and EMG signals for robot control show overall the potential of this approach to develop powerful and less complex HRI systems. Overall, this work provides a proof-of-concept for successful real-time control of robot arms using EMG and EOG signals, paving the way for the development of more dexterous and human-controlled assistive devices.


Title: High-Density Electromyography Based Control of Robotic Devices: On the Execution of Dexterous Manipulation Tasks
Key Words: biomechanics  biomedical electrodes  dexterous manipulators  electromyography  manipulators  medical robotics  medical signal processing  patient rehabilitation  prosthetics  telerobotics  EMG signals  object motion decoding  muscle importances  muscle importance results  decoded motions  fingered robotic hand  robotic devices  dexterous manipulation tasks  electromyography-based interfaces  robotics studies  teleoperation  telemanipulation applications  EMG-based control  prosthetic rehabilitation devices  assistive rehabilitation devices  robotic rehabilitation devices  grasping tasks  learning scheme  High Density Electromyography sensors  in-hand manipulation motions  object space  myoelectric activations  human forearm  hand muscles  yaw motions  geometric center  myoelectric data  high-density electromyography-based control  HD-EMG electrode arrays  Muscles  Electromyography  Task analysis  Electrodes  Robots  Decoding  Feature extraction 
Abstract: Electromyography (EMG) based interfaces have been used in various robotics studies ranging from teleoperation and telemanipulation applications to the EMG based control of prosthetic, assistive, or robotic rehabilitation devices. But most of these studies have focused on the decoding of user's motion or on the control of the robotic devices in the execution of simple tasks (e.g., grasping tasks). In this work, we present a learning scheme that employs High Density Electromyography (HD-EMG) sensors to decode a set of dexterous, in-hand manipulation motions (in the object space) based on the myoelectric activations of human forearm and hand muscles. To do that, the subjects were asked to perform roll, pitch, and yaw motions manipulating two different cubes. The first cube was designed to have a center of mass coinciding with the geometric center of the cube, while for the second cube the center of mass was shifted 14 mm to the right (off-centered design). Regarding the acquisition of the myoelectric data, custom HD-EMG electrode arrays were designed and fabricated. Using these arrays, a total of 89 EMG signals were extracted. The object motion decoding was formulated as a regression problem using the Random Forests (RF) technique and the muscle importances were studied using the inherent feature variables importance calculation procedure of the RF. The muscle importance results show that different subjects use different strategies to execute the same motions on same object when the weight is off-centered. Finally, the decoded motions were used to control a five fingered robotic hand in a proof-of-concept application.


Title: Perception-Action Coupling in Usage of Telepresence Cameras
Key Words: cameras  control system synthesis  manipulators  robot vision  telecontrol  telerobotics  visual feedback  teleoperation assistance design  telepresence camera selection  standalone cameras  wearable cameras  manipulation motions  active perception control  human motor system  natural perception-action coupling  autonomous camera selection  active perception motions  coordinated manipulation  telepresence tele-action robots  telepresence cameras  robot teleoperation  telepresence system  Cameras  Task analysis  Robot vision systems  Robot kinematics  Telepresence  Teleoperators 
Abstract: Telepresence tele-action robots enable human workers to reliably perform difficult tasks in remote, cluttered, and human environments. However, the effort to control coordinated manipulation and active perception motions may exhaust and intimidate novice workers. We hypothesize that such cognitive efforts would be effectively reduced if the teleoperators are provided with autonomous camera selection and control aligned with the natural perception-action coupling of the human motor system. Thus, we conducted a user study to investigate the coordination of active perception control and manipulation motions performed with visual feedback from various wearable and standalone cameras in a telepresence scenario. Our study discovered rich information about telepresence camera selection to inform telepresence system configuration and possible teleoperation assistance design for reduced cognitive effort in robot teleoperation.


Title: A technical framework for human-like motion generation with autonomous anthropomorphic redundant manipulators
Key Words: human-robot interaction  learning (artificial intelligence)  manipulator kinematics  mechanical variables control  medical robotics  mobile robots  motion control  redundant manipulators  trajectory control  technical framework  motion generation  autonomous anthropomorphic redundant manipulators  co-bots  industrial settings  people assistance  robot motions  classic solutions  anthropomorphic movement generation  optimization procedures  neuroscientific literature  learning methods  motion variability  high dimensional datasets  human upper limb principal motion modes  functional analysis  robot trajectory optimization  redundant anthropomorphic kinematic architectures  human model  functional mode extraction  human trajectories  robotic manipulator  advanced human-robot interaction  industrial co-botics  human assistance  Kinematics  Manipulators  Trajectory  Computer architecture  Cost function 
Abstract: The need for users' safety and technology accept-ability has incredibly increased with the deployment of co-bots physically interacting with humans in industrial settings, and for people assistance. A well-studied approach to meet these requirements is to ensure human-like robot motions. Classic solutions for anthropomorphic movement generation usually rely on optimization procedures, which build upon hypotheses devised from neuroscientific literature, or capitalize on learning methods. However, these approaches come with limitations, e.g. limited motion variability or the need for high dimensional datasets. In this work, we present a technique to directly embed human upper limb principal motion modes computed through functional analysis in the robot trajectory optimization. We report on the implementation with manipulators with redundant anthropomorphic kinematic architectures - although dissimilar with respect to the human model used for functional mode extraction - via Cartesian impedance control. In our experiments, we show how human trajectories mapped onto a robotic manipulator still exhibit the main characteristics of human-likeness, e.g. low jerk values. We discuss the results with respect to the state of the art, and their implications for advanced human-robot interaction in industrial co-botics and for human assistance.


Title: Real-Time Adaptive Assembly Scheduling in Human-Multi-Robot Collaboration According to Human Capability*
Key Words: assembling  genetic algorithms  multi-agent systems  multi-robot systems  robotic assembly  scheduling  human-multirobot collaboration  human capability  optimal assembly scheduling  robot adaptation  human-single-robot interaction  human-multirobot interaction  multiagent interactions  real-time adaptive assembly scheduling approach  formulated adaptive assembly scheduling problem  human-multirobot assembly tasks  Robots  Job shop scheduling  Task analysis  Real-time systems  Schedules  Adaptive scheduling  Adaptation models 
Abstract: Human-multi-robot collaboration is becoming more and more common in intelligent manufacturing. Optimal assembly scheduling of such systems plays a critical role in their production efficiency. Existing approaches mostly consider humans as agents with assumed or known capabilities, which leads to suboptimal performance in realistic applications where human capabilities usually change. In addition, most robot adaptation focuses on human-single-robot interaction and the adaptation in human-multi-robot interaction with changing human capability still remains challenging due to the complexity of the heterogeneous multi-agent interactions. This paper proposes a real-time adaptive assembly scheduling approach for human-multi-robot collaboration by modeling and incorporating changing human capability. A genetic algorithm is also designed to derive implementable solutions for the formulated adaptive assembly scheduling problem. The proposed approaches are validated through different simulated human-multi-robot assembly tasks and the results demonstrate the effectiveness and advantages of the proposed approaches.


Title: Microscope-Guided Autonomous Clear Corneal Incision
Key Words: eye  medical computing  medical robotics  ophthalmic lenses  surgery  ex-vivo porcine eyes  microscope-guided autonomous clear corneal incision  ophthalmic microscope system  multiaxes robot  self-sealing incision  autonomous robotic system  cataract surgery  Robots  Surgery  Feature extraction  Iris  Cameras  Mirrors  Cataracts 
Abstract: Clear Corneal Incision, a challenging step in cataract surgery, and important to the overall quality of the surgery. New surgeons usually spend one full year trying to perfect their incision, but even after such rigorous training deficient incisions can still occur. This paper proposes an autonomous robotic system for this self-sealing incision. A conventional ophthalmic microscope system with a monocular camera is utilized to capture the surgical scene, ascertain the robot's position, and estimate depth information. Kinematics with a remote centre of motion (RCM) is designed for a multi-axes robot to perform the incision route. The experimental results on ex-vivo porcine eyes show the autonomous Clear Corneal Incision has a stricter three-plane structure than a surgeon-made incision, which is closer to the ideal incision.


Title: Asynchronous and decoupled control of the position and the stiffness of a spatial RCM tensegrity mechanism for needle manipulation*
Key Words: manipulator kinematics  mean square error methods  medical robotics  needles  position control  surgery  asynchronous control  decoupled control  spatial RCM tensegrity mechanism  needle manipulation  spatial remote center  double parallelogram system  percutaneous needle insertion  decoupled modulation  control methodology  position tracking  needle guide  RCM tensegrity mechanism  spatial remote center of motion  Needles  Kinematics  Mathematical model  Actuators  Modulation  Robot sensing systems 
Abstract: This paper introduces a 2-DOF spatial remote center of motion (RCM) tensegrity mechanism, based on a double parallelogram system, dedicated for percutaneous needle insertion. The originality of this mechanism is its ability to be reconfigured and its capacity to perform a decoupled modulation of its stiffness in an asynchronous way. To do so, an analytical stiffness model of the robot is established, and a control methodology is proposed. A prototype of the robot is developed and assessed experimentally. The position tracking is evaluated using a 6-DOF magnetic tracker sensor showing a root mean square error less than 0.8° in both directions of the needle guide.


Title: Redundancy Resolution Integrated Model Predictive Control of CDPRs: Concept, Implementation and Experiments
Key Words: cables (mechanical)  predictive control  robot kinematics  robust control  trajectory control  MPC scheme  fully-constrained cable-driven parallel robots  cable tension distribution  pick-and-place task  maximum tension  tracking error minimization  redundancy resolution integrated model predictive control  CDPRs  cable tension limits  robustness  payload mass  Power cables  Robots  Real-time systems  Kinematics  Payloads  Safety  Robustness 
Abstract: This paper introduces a Model Predictive Control (MPC) strategy for fully-constrained Cable-Driven Parallel Robots. The main advantage of the proposed scheme lies in its ability to explicitly handle cable tension limits. Indeed, the cable tension distribution is performed as an integral part of the main control architecture. This characteristic significantly improves the safety of the system. Experimental results demonstrate this advantage addressing a typical pick-and-place task with two different scenarios: nominal cable tension limits and reduced maximum tension. Satisfactory tracking errors were obtained in the first scenario. In the second scenario, the desired trajectory escapes from the workspace defined by the new set of tension limits. The proposed MPC scheme is able to minimize the tracking errors without violating the tension limits. Satisfying results were also obtained regarding robustness against uncertainties on the payload mass.


Title: Mechanics for Tendon Actuated Multisection Continuum Arms
Key Words: actuators  bending  biomechanics  inspection  medical robotics  robot kinematics  tendon actuated multisection continuum arms  bending deformations  high mechanical coupling  variable length-based kinematic models  continuum arm curve parameter kinematics  robot  Tendons  Robots  Computational modeling  Strain  Numerical models  Kinematics  Deformable models 
Abstract: Tendon actuated multisection continuum arms have high potential for inspection applications in highly constrained spaces. They generate motion by axial and bending deformations. However, because of the high mechanical coupling between continuum sections, variable length-based kinematic models produce poor results. A new mechanics model for tendon actuated multisection continuum arms is proposed in this paper. The model combines the continuum arm curve parameter kinematics and concentric tube kinematics to correctly account for the large axial and bending deformations observed in the robot. Also, the model is computationally efficient and utilizes tendon tensions as the joint space variables thus eliminating the actuator length related problems such as slack and backlash. A recursive generalization of the model is also presented. Despite the high coupling between continuum sections, numerical results show that the model can be used for generating correct forward and inverse kinematic results. The model is then tested on a thin and long multisection continuum arm. The results show that the model can be used to successfully model the deformation.


Title: Trajectory Optimization for a Six-DOF Cable-Suspended Parallel Robot with Dynamic Motions Beyond the Static Workspace
Key Words: cables (mechanical)  manipulator dynamics  optimisation  path planning  position control  trajectory control  cable-suspended parallel robot  static workspace  trajectory optimization formulation  dynamic trajectories  six-degree-of-freedom  low-dimensional dynamic models  narrow feasible state space  dynamic similarity  point-mass CSPR  feasible force polyhedra  transition trajectories  highly dynamic motions  periodic trajectories  Dynamics  Planning  Robots  Trajectory optimization  Chebyshev approximation  Dynamic trajectory planning  optimization and optimal control  cable-suspended parallel robots 
Abstract: This paper presents a trajectory optimization formulation for planning dynamic trajectories of a six-degree-of-freedom (six-DOF) cable-suspended parallel robot (CSPR) that extend beyond the static workspace. The optimization is guided by low-dimensional dynamic models to overcome the local minima and accelerate the exploration of the narrow feasible state space. The dynamic similarity between the six-DOF CSPR and the three-DOF point-mass CSPR is discussed with the analyses of their feasible force polyhedra. Finally, the transition trajectories of a three-DOF CSPR are used as the initial guess of the translational part of the six-DOF motion. With the proposed approach, highly dynamic motions for a six-DOF CSPR are efficiently generated with multiple oscillations. The feasibility is demonstrated by point-to-point and periodic trajectories in the physics simulation.


Title: An Efficient Planning and Control Framework for Pruning Fruit Trees
Key Words: agricultural products  image colour analysis  industrial manipulators  intelligent robots  robot vision  motion planning  sequence cut points  robotic pruning  control framework  pruning fruit trees  dormant pruning  fresh market tree fruit production  industrial manipulator  eye-in-hand RGB-D camera configuration  pneumatic cutter  Planning  Cameras  Three-dimensional displays  Manipulators  Vegetation  Pipelines 
Abstract: Dormant pruning is a major cost component of fresh market tree fruit production, nearly equal in scale to harvesting the fruit. However, relatively little focus has been given to the problem of pruning trees autonomously. In this paper, we introduce a robotic system consisting of an industrial manipulator, an eye-in-hand RGB-D camera configuration, and a custom pneumatic cutter. The system is capable of planning and executing a sequence of cuts while making minimal assumptions about the environment. We leverage a novel planning framework designed for high-throughput operation which builds upon previous work to reduce motion planning time and sequence cut points intelligently. In end-to-end experiments with a set of ten different branch configurations, the system achieved a high success rate in plan execution and a 1.5x speedup in throughput versus a baseline planner, representing a significant step towards the goal of practical implementation of robotic pruning.


Title: Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation
Key Words: genetic algorithms  iterative methods  mobile robots  motion control  navigation  path planning  simulated environments  genetic algorithm  resulting parameter sets  substantial performance improvements  agricultural robots  context dependant iterative parameter optimisation  robust robot navigation  autonomous mobile robotics  motion control  path planning  robust performance  robot model  parameter tuning  underlying algorithm  substantial combinatorial challenge  extensive manual tuning  navigation actions  spatial context  navigation task  respective navigation algorithms  iterative optimisation  performance metrics  Navigation  Robots  Optimization  Tuning  Robustness  Genetic algorithms  Measurement 
Abstract: Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a "black box", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.


Title: Extending Riemmanian Motion Policies to a Class of Underactuated Wheeled-Inverted-Pendulum Robots
Key Words: collision avoidance  humanoid robots  manipulators  mobile robots  motion control  pendulums  robot dynamics  wheels  RMP formalism  underacutated systems  fully-actuated subsystem  residual dynamics  manipulation tasks  7-DoF system  second-order motion policies  RMP-based approaches  operational space control  fully state-dependent  collision avoidance bahaviors  control input  Riemmanian motion policies  underactuated wheeled-inverted-pendulum humanoid robot  Task analysis  Manifolds  Manipulator dynamics  Aerospace electronics  Humanoid robots  Measurement 
Abstract: Riemannian Motion Policies (RMPs) have recently been introduced as a way to specify second-order motion policies defined on robot task spaces. RMP-based approaches have the advantage of being more general than traditional approaches based on operational space control; for example, the generalized task inertia in an RMP can be fully state-dependent, which is particularly effective in designing collision avoidance bahaviors. But until now RMPs have been applied only to fully actuated systems, i.e. systems for which each degree of freedom (DoF) can be directly actuated by a control input. In this paper, we present a method that extends the RMP formalism to a class of underacutated systems whose dynamics are amenable to a decomposition into a fully-actuated subsystem and a residual dynamics. We show the efficacy of the approach by constructing a suitable decomposition for a Wheeled-Inverted-Pendulum (WIP) humanoid robot and applying our method to derive motion policies for combined locomotion and manipulation tasks. Simulation results are presented for a 7-DoF system with one degree of underactuation.


Title: Singularity-Free Inverse Dynamics for Underactuated Systems with a Rotating Mass
Key Words: manipulator dynamics  matrix algebra  motion control  nonlinear control systems  path planning  position control  singularity-free inverse dynamics  underactuated system  rotating mass  motion control  configuration singularities  configuration space  inertial coupling  small-amplitude sine wave  nonlinear dynamics  rolling system  singularity regions  coupling singularities  rolling carrier  Mathematical model  Couplings  Integrated circuits  Trajectory  Kinematics  Robots  Tensile stress 
Abstract: Motion control of underactuated systems through the inverse dynamics contains configuration singularities. These limitations in configuration space mainly stem from the inertial coupling that passive joints/bodies create. In this study, we present a model that is free from singularity while the trajectory of the rotating mass has a small-amplitude sine wave around its circle. First, we derive the modified non-linear dynamics for a rolling system. Also, the singularity regions for this underactuated system is demonstrated. Then, the wave parameters are designed under certain conditions to remove the coupling singularities. We obtain these conditions from the positive definiteness of the inertia matrix in the inverse dynamics. Finally, the simulation results are confirmed by using a prescribed Beta function on the specified states of the rolling carrier. Because our algebraic method is integrated into the non-linear dynamics, the proposed solution has a great potential to be extended to the Lagrangian mechanics with multiple degrees-of-freedom.


Title: Robust capture of unknown objects with a highly under-actuated gripper
Key Words: actuators  adhesion  adhesives  dexterous manipulators  friction  grippers  under-actuated gripper  robotic grippers  high-friction materials  scaling forces  adhesion-controlled friction  adhesion-based grippers  high-friction interfaces  robust capture  size 65.0 cm  Grippers  Friction  Couplings  Stability analysis  Force  Torque  Shape 
Abstract: Capturing large objects of unknown shape and orientation remains a challenge for most robotic grippers. We present a highly under-actuated gripper well suited for this task. Prior work shows two primary limitations to these grippers: the grip force of each link tends to decrease as the number of links increases, and the stability of an under-actuated linkage depends on the configuration of the links so grippers with many links are unlikely to be stable for arbitrary surfaces. We address these concerns by implementing two complementary methods of stabilization: using high-friction materials and scaling forces into the surface. We show that gecko-inspired adhesives provide an adhesion-controlled friction that can stabilize the gripper and improve grasp performance without the need of large normal forces. The under-actuated linkages also conform around arbitrary shapes and provide capability beyond prior adhesion-based grippers. With these high-friction interfaces, we show highly under-actuated linkages successfully grasp in many configurations without strict stability. The gripper is capable of holding over 30 N and consists of two tendon driven linkages that are each 65 cm long. This type of gripper is well suited for tasks without a predefined target geometry or orientation such as satellite servicing.


Title: SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic
Key Words: control engineering computing  multi-agent systems  road traffic  road vehicles  telecommunication traffic  traffic engineering computing  SUMMIT  urban driving  massive mixed traffic  unregulated urban crowd  high-speed traffic participants  high-fidelity simulator  crowd-driving algorithms  open-source OpenStreetMap map database  multiagent motion prediction model  unregulated urban traffic  heterogeneous agents  autonomous driving simulation  realistic traffic behaviors  crowd-driving settings  Roads  Robot sensing systems  Context modeling  Planning  Automobiles  Geometry  Kinematics 
Abstract: Autonomous driving in an unregulated urban crowd is an outstanding challenge, especially, in the presence of many aggressive, high-speed traffic participants. This paper presents SUMMIT, a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physics and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control and planning, and end-to-end learning. We provide a context-aware planner together with benchmark scenarios and show that SUMMIT generates complex, realistic traffic behaviors in challenging crowd-driving settings.


Title: A Model-Based Reinforcement Learning and Correction Framework for Process Control of Robotic Wire Arc Additive Manufacturing
Key Words: learning (artificial intelligence)  process control  rapid prototyping (industrial)  robotic welding  three-dimensional printing  welds  wires  integrated learning-correction framework  model-based reinforcement learning  process parameters  inter-layer geometric digression  process control  robot arm  3D metallic objects  layer by layer fashion  multilayer multibead deposition control  robotic wire arc additive manufacturing  weld beads  error stacking  material wastage reduction  MLMB print  Training  Predictive models  Process control  Adaptation models  Printing  Robots  Three-dimensional displays 
Abstract: Robotic Wire Arc Additive Manufacturing (WAAM) utilizes a robot arm as a motion system to build 3D metallic objects by depositing weld beads one above the other in a layer by layer fashion. A key part of this approach is the process study and control of Multi-Layer Multi-Bead (MLMB) deposition, which is very sensitive to process parameters and prone to error stacking. Despite its importance, it has been receiving less attention than its single bead counterpart in literature, probably due to the higher experimental overhead and complexity of modeling. To address these challenges, this paper proposes an integrated learning-correction framework, adapted from Model-Based Reinforcement Learning, to iteratively learn the direct effect of process parameters on MLMB print while simultaneously correct for any inter-layer geometric digression such that the final output is still satisfactory. The advantage is that this learning architecture can be used in conjunction with actual parts printing (hence, in-situ study), thus minimizing the required training time and material wastage. The proposed learning framework is implemented on an actual robotic WAAM system and experimentally evaluated.


Title: Optimizing performance in automation through modular robots
Key Words: flexible manufacturing systems  industrial robots  mobile robots  robot dynamics  robot kinematics  modular robots  flexible manufacturing  module composition  cycle time  energy efficiency  kinematic constraints  dynamic constraints  industrial robots  randomly generated tasks  performance metrics  modular robot  proModular.1  obstacle constraints  Cartesian space  Kinematics  Service robots  Collision avoidance  Trajectory  Task analysis  Heuristic algorithms 
Abstract: Flexible manufacturing and automation require robots that can be adapted to changing tasks. We propose to use modular robots that are customized from given modules for a specific task. This work presents an algorithm for proposing a module composition that is optimal with respect to performance metrics such as cycle time and energy efficiency, while considering kinematic, dynamic, and obstacle constraints. Tasks are defined as trajectories in Cartesian space, as a list of poses for the robot to reach as fast as possible, or as dexterity in a desired workspace. In a simulated comparison with commercially available industrial robots, we demonstrate the superiority of our approach in randomly generated tasks with respect to the chosen performance metrics. We use our modular robot proModular.1 for the comparison.


Title: Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning
Key Words: graph theory  learning (artificial intelligence)  manipulators  mobile robots  neural nets  multiobject manipulation  relational reinforcement learning  learning robotic manipulation tasks  outrageous data requirements  task curriculum  graph-based relational architectures  simulated block stacking task  step-wise sparse rewards  zero-shot generalization  Task analysis  Stacking  Robots  Learning (artificial intelligence)  Poles and towers  Training  Three-dimensional displays 
Abstract: Learning robotic manipulation tasks using reinforcement learning with sparse rewards is currently impractical due to the outrageous data requirements. Many practical tasks require manipulation of multiple objects, and the complexity of such tasks increases with the number of objects. Learning from a curriculum of increasingly complex tasks appears to be a natural solution, but unfortunately, does not work for many scenarios. We hypothesize that the inability of the state- of-the-art algorithms to effectively utilize a task curriculum stems from the absence of inductive biases for transferring knowledge from simpler to complex tasks. We show that graph-based relational architectures overcome this limitation and enable learning of complex tasks when provided with a simple curriculum of tasks with increasing numbers of objects. We demonstrate the utility of our framework on a simulated block stacking task. Starting from scratch, our agent learns to stack six blocks into a tower. Despite using step-wise sparse rewards, our method is orders of magnitude more data- efficient and outperforms the existing state-of-the-art method that utilizes human demonstrations. Furthermore, the learned policy exhibits zero-shot generalization, successfully stacking blocks into taller towers and previously unseen configurations such as pyramids, without any further training.


Title: SwarmMesh: A Distributed Data Structure for Cooperative Multi-Robot Applications
Key Words: control engineering computing  data handling  data structures  distributed processing  mobile robots  multi-robot systems  storage management  topology  data item position  near-perfect data retention  distributed data structure  cooperative multirobot applications  distributed storage  mobile robots  shared global memory  external storage infrastructure  swarm topology  data storage  SwarmMesh  data type  Robots  Data structures  Peer-to-peer computing  Overlay networks  Routing  Distributed databases  Topology 
Abstract: We present an approach to the distributed storage of data across a swarm of mobile robots that forms a shared global memory. We assume that external storage infrastructure is absent, and that each robot is capable of devoting a quota of memory and bandwidth to distributed storage. Our approach is motivated by the insight that in many applications data is collected at the periphery of a swarm topology, but the periphery also happens to be the most dangerous location for storing data, especially in exploration missions. Our approach is designed to promote data storage in the locations in the swarm that best suit a specific feature of interest in the data, while accounting for the constantly changing topology due to individual motion. We analyze two possible features of interest: the data type and the data item position in the environment. We assess the performance of our approach in a large set of simulated experiments. The evaluation shows that our approach is capable of storing quantities of data that exceed the memory of individual robots, while maintaining near-perfect data retention in high-load conditions.


Title: Avalanche victim search via robust observers
Key Words: adaptive control  autonomous aerial vehicles  emergency management  multi-robot systems  observers  rescue robots  robust control  sensor fusion  avalanche victim search  robust observers  victim localization  ARVA sensor  adaptive control  UAVs  least square identifier  Receivers  Transmitters  Drones  Observers  Trajectory  Electromagnetics  Adaptive control  Search and Rescue  Robust Control 
Abstract: This paper introduces a new approach for victim localization in avalanches that will be exploited by UAVs using the ARVA sensor. We show that the nominal ARVA measurement can be linearly related to a quantity that is sufficient to reconstruct the victim position. We explicitly deal with a robust scenario in which the measurement is actually perturbed by a noise that grows with the distance to the victim and we propose an adaptive control scheme made of a least-square identifier and a trajectory generator whose role is both to guarantee the persistence of excitation for the identifier and to steer the ARVA receiver towards the victim. We show that the system succeeds in localizing the victim in a domain where the ARVA output is sufficiently informative and illustrate its performance in simulation. This new approach could significantly reduce the searching time by providing an exploitable estimate before having reached the victim. The work is framed within the EU project AirBorne whose goals is to develop at TRL8 a drone for quick localization of victims in avalanche scenarios.


Title: Reactive Control and Metric-Topological Planning for Exploration
Key Words: collision avoidance  graph theory  image sequences  mobile robots  navigation  robot vision  optic flow  insect visuomotor system  obstacle detection  topological graph  metric-topological planning  autonomous navigation  robotic platforms  bio-inspired reactive control  spatial decomposition  obstacle avoidance  Fourier residual analysis  image processing  continuous occupancy grid  graph edge  Optical feedback  Optical sensors  Optical imaging  Planning  Navigation  Harmonic analysis  Mathematical model  exploration  centering  control  mapping 
Abstract: Autonomous navigation in unknown environments with the intent of exploring all traversable areas is a significant challenge for robotic platforms. In this paper, a simple yet reliable method for exploring unknown environments is presented based on bio-inspired reactive control and metric-topological planning. The reactive control algorithm is modeled after the spatial decomposition of wide and small-field patterns of optic flow in the insect visuomotor system. Centering behaviour and small obstacle detection and avoidance are achieved through wide-field integration and Fourier residual analysis of instantaneous measured nearness respectively. A topological graph is estimated using image processing techniques on a continuous occupancy grid. Node paths are rapidly generated to navigate to the nearest unexplored edge in the graph. It is shown through rigorous field-testing that the proposed control and planning method is robust, reliable, and computationally efficient.


Title: Information Theoretic Active Exploration in Signed Distance Fields
Key Words: deterministic algorithms  mobile robots  optimisation  path planning  sensors  tree searching  robot sensing trajectories  autonomous TSDF mapping  TSDF uncertainty  sensor measurements  efficient uncertainty prediction  long-horizon optimization  deterministic tree-search algorithm  information gain  TSDF distribution  efficient planning  uninformative sensing trajectories  active TSDF mapping approach  simulated environments  information theoretic active exploration  occupancy mapping  mobile robot  truncated signed distance field  robot motion primitive sequences  branch-and-bound pruning  complex visibility constraints  Robot sensing systems  Trajectory  Measurement uncertainty  Standards  Uncertainty 
Abstract: This paper focuses on exploration and occupancy mapping of unknown environments using a mobile robot. While a truncated signed distance field (TSDF) is a popular, efficient, and highly accurate representation of occupancy, few works have considered optimizing robot sensing trajectories for autonomous TSDF mapping. We propose an efficient approach for maintaining TSDF uncertainty and predicting its evolution from potential future sensor measurements without actually receiving them. Efficient uncertainty prediction is critical for long-horizon optimization of potential sensing trajectories. We develop a deterministic tree-search algorithm that evaluates the information gain between the TSDF distribution and potential observations along sequences of robot motion primitives. Efficient planning is achieved by branch-and-bound pruning of uninformative sensing trajectories. The effectiveness of our active TSDF mapping approach is evaluated in several simulated environments with complex visibility constraints.


Title: Bayesian Learning-Based Adaptive Control for Safety Critical Systems
Key Words: adaptive control  Bayes methods  belief networks  control engineering computing  Gaussian processes  learning (artificial intelligence)  Lyapunov methods  Markov processes  neural nets  probability  safety-critical software  stability  real-time performance  deep neural networks  model uncertainties  Bayesian model learning  adaptive control framework  control Lyapunov functions  control barrier functions  tractable Bayesian model  safety-critical high-speed Mars rover missions  Bayesian learning-based adaptive control  deep learning  safety-critical systems  high-speed terrestrial mobility  Safety  Adaptation models  Bayes methods  Stochastic processes  Uncertainty  Computational modeling  Switches  Robust/Adaptive Control of Robotic Systems  Robot Safety  Probability and Statistical Methods  Bayesian Adaptive Control  Deep Learning  Mars Rover 
Abstract: Deep learning has enjoyed much recent success, and applying state-of-the-art model learning methods to controls is an exciting prospect. However, there is a strong reluctance to use these methods on safety-critical systems, which have constraints on safety, stability, and real-time performance. We propose a framework which satisfies these constraints while allowing the use of deep neural networks for learning model uncertainties. Central to our method is the use of Bayesian model learning, which provides an avenue for maintaining appropriate degrees of caution in the face of the unknown. In the proposed approach, we develop an adaptive control framework leveraging the theory of stochastic CLFs (Control Lyapunov Functions) and stochastic CBFs (Control Barrier Functions) along with tractable Bayesian model learning via Gaussian Processes or Bayesian neural networks. Under reasonable assumptions, we guarantee stability and safety while adapting to unknown dynamics with probability 1. We demonstrate this architecture for high-speed terrestrial mobility targeting potential applications in safety-critical high-speed Mars rover missions.


Title: Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure
Key Words: feature extraction  graph theory  image matching  image registration  image segmentation  learning (artificial intelligence)  legged locomotion  neural nets  optical radar  optimisation  pose estimation  robot vision  SLAM (robots)  stereo image processing  online LiDAR-SLAM  legged robot  robust registration  deep-learned loop closure  3D factor-graph LiDAR-SLAM system  industrial environments  point clouds  inertial-kinematic state estimator  ICP registration  loop proposal mechanism  deep learning method  odometry  loop closure factors  pose graph optimization  SLAM map  risk alignment prediction method  deeply learned feature-based loop closure detector  Laser radar  Legged locomotion  Three-dimensional displays  Simultaneous localization and mapping  Iterative closest point algorithm 
Abstract: In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.


Title: Voxel Map for Visual SLAM
Key Words: computer vision  feature extraction  image representation  image retrieval  SLAM (robots)  visual SLAM systems  camera field-of-view  voxel map representation  keyframe map  map points retrieval  simultaneous localization and mapping  Simultaneous localization and mapping  Three-dimensional displays  Cameras  Visualization  Cognition  Feature extraction  Task analysis 
Abstract: In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multi-camera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable.


Title: Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video
Key Words: interactive video  learning (artificial intelligence)  robot vision  video signal processing  adversarial skill-transfer loss  task domain  learned skill embeddings  entropy-regularized adversarial skill-transfer loss  temporal video coherence  metric learning loss  adversarial loss  task context  unlabeled multiview videos  task-agnostic skill embedding space  reinforcement learning agents  unsupervised robot skill learning  adversarial skill networks  learned embedding  Task analysis  Entropy  Measurement  Training  Robots  Interpolation  Learning (artificial intelligence) 
Abstract: Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.


Title: Visual Odometry Revisited: What Should Be Learnt?
Key Words: convolutional neural nets  distance measurement  geometry  learning (artificial intelligence)  pose estimation  SLAM (robots)  monocular visual odometry algorithm  geometry-based methods  monocular systems  scale-drift issue  deep learning  epipolar geometry  perspective-n-point method  frame-to-frame VO algorithm  DF-VO  scale consistent single-view depth CNN  Cameras  Adaptive optics  Geometry  Optical imaging  Machine learning  Estimation  Two dimensional displays 
Abstract: In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue. Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system. Code is available at here: DF-VO.


Title: 3D Scene Geometry-Aware Constraint for Camera Localization with Deep Learning
Key Words: cameras  convolutional neural nets  feature extraction  image classification  image motion analysis  image reconstruction  image segmentation  learning (artificial intelligence)  mobile robots  motion control  path planning  pose estimation  3D scene geometry-aware constraint  camera localization  deep learning  fundamental component  autonomous driving vehicles  path planning  motion control  end-to-end approaches  convolutional neural network  3D-geometry based traditional methods  compact network  absolute camera  image contents  image-level structural similarity loss  challenging scenes  Cameras  Three-dimensional displays  Training  Geometry  Robot vision systems  Transforms 
Abstract: Camera localization is a fundamental and key component of autonomous driving vehicles and mobile robots to localize themselves globally for further environment perception, path planning and motion control. Recently end-to-end approaches based on convolutional neural network have been much studied to achieve or even exceed 3D-geometry based traditional methods. In this work, we propose a compact network for absolute camera pose regression. Inspired from those traditional methods, a 3D scene geometry-aware constraint is also introduced by exploiting all available information including motion, depth and image contents. We add this constraint as a regularization term to our proposed network by defining a pixel-level photometric loss and an image-level structural similarity loss. To benchmark our method, different challenging scenes including indoor and outdoor environment are tested with our proposed approach and state-of-the-arts. And the experimental results demonstrate significant performance improvement of our method on both prediction accuracy and convergence efficiency.


Title: ACDER: Augmented Curiosity-Driven Experience Replay
Key Words: augmented reality  control engineering computing  learning (artificial intelligence)  learning systems  manipulators  augmented curiosity-driven experience replay  hindsight experience replay  sample-efficiency  automatic exploratory curriculum  dynamic initial states selection  task-relevant states  goal-oriented curiosity-driven exploration  action space  high dimensional continuous state  low exploration efficiency  RL agent  reinforcement learning  sparse feed-back  ACDER  multistep robotic task learning  basic tasks  challenging robotic manipulation tasks  valuable states  Task analysis  Robots  Learning (artificial intelligence)  Training  Incentive schemes  Buffer storage  Games 
Abstract: Exploration in environments with sparse feed-back remains a challenging research problem in reinforcement learning (RL). When the RL agent explores the environment randomly, it results in low exploration efficiency, especially in robotic manipulation tasks with high dimensional continuous state and action space. In this paper, we propose a novel method, called Augmented Curiosity-Driven Experience Replay (ACDER), which leverages (i) a new goal-oriented curiosity-driven exploration to encourage the agent to pursue novel and task-relevant states more purposefully and (ii) the dynamic initial states selection as an automatic exploratory curriculum to further improve the sample-efficiency. Our approach complements Hindsight Experience Replay (HER) by introducing a new way to pursue valuable states. Experiments conducted on four challenging robotic manipulation tasks with binary rewards, including Reach, Push, Pick&Place and Multi-step Push. The empirical results show that our proposed method significantly outperforms existing methods in the first three basic tasks and also achieves satisfactory performance in multi-step robotic task learning.


Title: TrueRMA: Learning Fast and Smooth Robot Trajectories with Recursive Midpoint Adaptations in Cartesian Space
Key Words: learning (artificial intelligence)  motion control  neurocontrollers  robot kinematics  time optimal control  trajectory control  TrueRMA  robot trajectory learning  recursive midpoint adaptations  Cartesian space  differentiable path  inverse kinematics  time optimal parameterization  reinforcement learning  robot movement  neural network training  KUKA iiwa robot  Trajectory  Robots  Kinematics  Task analysis  Training  Neural networks 
Abstract: We present TrueRMA, a data-efficient, model-free method to learn cost-optimized robot trajectories over a wide range of starting points and endpoints. The key idea is to calculate trajectory waypoints in Cartesian space by recursively predicting orthogonal adaptations relative to the midpoints of straight lines. We generate a differentiable path by adding circular blends around the waypoints, calculate the corresponding joint positions with an inverse kinematics solver and calculate a time-optimal parameterization considering velocity and acceleration limits. During training, the trajectory is executed in a physics simulator and costs are assigned according to a user-specified cost function which is not required to be differentiable. Given a starting point and an endpoint as input, a neural network is trained to predict midpoint adaptations that minimize the cost of the resulting trajectory via reinforcement learning. We successfully train a KUKA iiwa robot to keep a ball on a plate while moving between specified points and compare the performance of TrueRMA against two baselines. The results show that our method requires less training data to learn the task while generating shorter and faster trajectories.


Title: Fog Robotics Algorithms for Distributed Motion Planning Using Lambda Serverless Computing
Key Words: computational complexity  control engineering computing  manipulators  mobile robots  motion control  parallel processing  path planning  distributed motion planning  Lambda serverless computing  motion planning algorithms  RRT  computational load  local environment changes  cloud-based serverless lambda computing  parallel computation  serverless computers  learned parallel allocation  Amazon Lambda  sporadically computationally intensive motion planning tasks  fog robotics algorithms  Planning  Parallel processing  FAA  Robot kinematics  Cloud computing  Probabilistic logic 
Abstract: For robots using motion planning algorithms such as RRT and RRT*, the computational load can vary by orders of magnitude as the complexity of the local environment changes. To adaptively provide such computation, we propose Fog Robotics algorithms in which cloud-based serverless lambda computing provides parallel computation on demand. To use this parallelism, we propose novel motion planning algorithms that scale effectively with an increasing number of serverless computers. However, given that the allocation of computing is typically bounded by both monetary and time constraints, we show how prior learning can be used to efficiently allocate resources at runtime. We demonstrate the algorithms and application of learned parallel allocation in both simulation and with the Fetch commercial mobile manipulator using Amazon Lambda to complete a sequence of sporadically computationally intensive motion planning tasks.


Title: Exploration of 3D terrains using potential fields with elevation-based local distortions
Key Words: mobile robots  optical radar  radar receivers  stereo image processing  terrain mapping  exploration approach  potential fields  uneven terrains  high declivity regions  ground robot  elevation-based local distortions  mobile robots  numerous outdoor tasks  military applications  3D terrain exploration  patrolling application  delivery application  2D LIDAR sensors  Distortion  Robot sensing systems  Three-dimensional displays  Boundary conditions  Two dimensional displays 
Abstract: Mobile robots can be used in numerous outdoor tasks such as patrolling, delivery and military applications. In order to deploy mobile robots in this kind of environment, where there are different challenges like slopes, elevations, or even holes, they should be able to detect such challenges and determine the best path to accomplish their tasks. In this paper, we are proposing an exploration approach based on potential fields with local distortions, in which we define preferences in uneven terrains to avoid high declivity regions without compromising the best path. The approach was implemented and tested in simulated environments, considering a ground robot embedded with two 2D LIDAR sensors, and the experiments demonstrated the efficiency of our method.


Title: R3T: Rapidly-exploring Random Reachable Set Tree for Optimal Kinodynamic Planning of Nonlinear Hybrid Systems
Key Words: control engineering computing  formal verification  nonlinear control systems  reachability analysis  robot dynamics  sampling methods  trees (mathematics)  R3T  random reachable set tree  optimal kinodynamic planning  nonlinear hybrid systems  reachability-based variant  rapidly-exploring random tree  multiple polytopes  reachability analysis  nonlinear systems  contact-rich robotic systems  formal verification  Planning  Approximation algorithms  Trajectory  Heuristic algorithms  Measurement  Robots  Silicon 
Abstract: We introduce R3T, a reachability-based variant of the rapidly-exploring random tree (RRT) algorithm that is suitable for (optimal) kinodynamic planning in nonlinear and hybrid systems. We developed tools to approximate reachable sets using polytopes and perform sampling-based planning with them. This method has a unique advantage in hybrid systems: different dynamic modes in the reachable set can be explicitly represented using multiple polytopes. We prove that under mild assumptions, R3T is probabilistically complete in kinodynamic systems, and asymptotically optimal through rewiring. Moreover, R3T provides a formal verification method for reachability analysis of nonlinear systems. The advantages of R3T are demonstrated with case studies on nonlinear, hybrid, and contact-rich robotic systems.


Title: DeepSemanticHPPC: Hypothesis-based Planning over Uncertain Semantic Point Clouds
Key Words: belief networks  computational geometry  image reconstruction  mobile robots  neural nets  path planning  robot vision  DeepSemanticHPPC  hypothesis-based planning  uncertain semantic point clouds  deep Bayesian neural network  flexible point cloud scene representation  sparse visual measurements  hypothesis-based path planner  uncertainty-aware hypothesis-based planner  Uncertainty  Three-dimensional displays  Trajectory  Semantics  Robots  Planning  Neural networks 
Abstract: Planning in unstructured environments is challenging - it relies on sensing, perception, scene reconstruction, and reasoning about various uncertainties. We propose DeepSemanticHPPC, a novel uncertainty-aware hypothesis-based planner for unstructured environments. Our algorithmic pipeline consists of: a deep Bayesian neural network which segments surfaces with uncertainty estimates; a flexible point cloud scene representation; a next-best-view planner which minimizes the uncertainty of scene semantics using sparse visual measurements; and a hypothesis-based path planner that proposes multiple kinematically feasible paths with evolving safety confidences given next-best-view measurements. Our pipeline iteratively decreases semantic uncertainty along planned paths, filtering out unsafe paths with high confidence. We show that our framework plans safe paths in real-world environments where existing path planners typically fail.


Title: Balancing Actuation and Computing Energy in Motion Planning
Key Words: mobile robots  motion control  path planning  balancing actuation  motion planning problems  low-energy robotic vehicles  high-endurance autonomous blimps  computing hardware  actuation hardware  CEIMP  anytime planning algorithm  actuation energy  asymptotic computational complexity  sampling-based motion planning algorithms  compute energy included motion planning algorithm  Planning  Robots  Meters  Task analysis  Hardware  Buildings  Space exploration 
Abstract: We study a novel class of motion planning problems, inspired by emerging low-energy robotic vehicles, such as insect-size flyers, chip-size satellites, and high-endurance autonomous blimps, for which the energy consumed by computing hardware during planning a path can be as large as the energy consumed by actuation hardware during the execution of the same path. We propose a new algorithm, called Compute Energy Included Motion Planning (CEIMP). CEIMP operates similarly to any other anytime planning algorithm, except it stops when it estimates further computing will require more computing energy than potential savings in actuation energy. We show that CEIMP has the same asymptotic computational complexity as existing sampling-based motion planning algorithms, such as PRM*. We also show that CEIMP outperforms the average baseline of using maximum computing resources in realistic computational experiments involving 10 floor plans from MIT buildings. In one representative experiment, CEIMP outperforms the average baseline 90.6% of the time when energy to compute one more second is equal to the energy to move one more meter, and 99.7% of the time when energy to compute one more second is equal to or greater than the energy to move 3 more meters.


Title: Posterior Sampling for Anytime Motion Planning on Graphs with Expensive-to-Evaluate Edges
Key Words: collision avoidance  graph theory  minimisation  mobile robots  7D planning problems  posterior sampling  anytime motion planning  collision checking  computational bottleneck  lazy algorithms  collision uncertainty  shortest path  real-time applications  anytime performance  anytime lazy motion planning algorithm  edge collisions  initial feasible path  shorter paths  Planning  Bayes methods  History  Uncertainty  Geometry  Learning (artificial intelligence)  Search problems 
Abstract: Collision checking is a computational bottleneck in motion planning, requiring lazy algorithms that explicitly reason about when to perform this computation. Optimism in the face of collision uncertainty minimizes the number of checks before finding the shortest path. However, this may take a prohibitively long time to compute, with no other feasible paths discovered during this period. For many real-time applications, we instead demand strong anytime performance, defined as minimizing the cumulative lengths of the feasible paths yielded over time. We introduce Posterior Sampling for Motion Planning (PSMP), an anytime lazy motion planning algorithm that leverages learned posteriors on edge collisions to quickly discover an initial feasible path and progressively yield shorter paths. PSMP obtains an expected regret bound of Õ(√(SAT)) and outperforms comparative baselines on a set of 2D and 7D planning problems.


Title: Preliminary Study of an Aerial Manipulator with Elastic Suspension
Key Words: actuators  aerospace robotics  feedback  manipulator dynamics  propellers  vibration control  elastic suspension  aerial manipulator  contra-rotating propellers  computed torque control strategy  active vibration canceling  feedback linearization control strategy  robotic carrier  Springs  Manipulators  Propellers  Task analysis  Grippers 
Abstract: This paper presents a preliminary study of an Aerial Manipulator suspended by a spring to a robotic carrier. The suspended aerial manipulator is actuated by six pairs of contra-rotating propellers generating a 6-DoF wrench. Simulations show path following results using a computed torque (feedback linearization) control strategy. Active vibration canceling is validated experimentally on a first prototype.


Title: Towards Low-Latency High-Bandwidth Control of Quadrotors using Event Cameras
Key Words: aircraft control  angular velocity control  attitude control  autonomous aerial vehicles  cameras  closed loop systems  feedback  helicopters  Hough transforms  image resolution  image sensors  Kalman filters  PD control  robot vision  state estimation  attitude tracking  state estimator  rotor thrusts  black-and-white disk  angular velocity  roll angle  Kalman filter  Hough transform  dualcopter platform  one-dimensional attitude tracking  drones  quadrotors  low-latency high-bandwidth control  event-based feedback  event-camera-driven closed loop control  proportional-derivative attitude control law  event-based state estimation  temporal resolution  sensor latency  high speed vision-based control  frequency 1.0 kHz  time 12.0 ms  Cameras  Robot vision systems  Transforms  Angular velocity  State estimation  Attitude control 
Abstract: Event cameras are a promising candidate to enable high speed vision-based control due to their low sensor latency and high temporal resolution. However, purely event-based feedback has yet to be used in the control of drones. In this work, a first step towards implementing low-latency high-bandwidth control of quadrotors using event cameras is taken. In particular, this paper addresses the problem of one-dimensional attitude tracking using a dualcopter platform equipped with an event camera. The event-based state estimation consists of a modified Hough transform algorithm combined with a Kalman filter that outputs the roll angle and angular velocity of the dualcopter relative to a horizon marked by a black-and-white disk. The estimated state is processed by a proportional-derivative attitude control law that computes the rotor thrusts required to track the desired attitude. The proposed attitude tracking scheme shows promising results of event-camera-driven closed loop control: the state estimator performs with an update rate of 1 kHz and a latency determined to be 12 ms, enabling attitude tracking at speeds of over 1600°/s.


Title: Perception-constrained and Motor-level Nonlinear MPC for both Underactuated and Tilted-propeller UAVS
Key Words: actuators  aircraft control  autonomous aerial vehicles  control system synthesis  helicopters  motion control  nonlinear control systems  predictive control  propellers  rotors (mechanical)  vehicle dynamics  motor-level Nonlinear MPC  tilted-propeller  Perception-constrained Nonlinear Model Predictive Control framework  real-time control  multirotor aerial vehicles  perceptive sensor  realistic actuator limitations  rotor minimum  maximum speeds  multirotor platforms  underactuated quadrotors  tilted-propellers hexarotors  motor-torque level  Propellers  Task analysis  Robot sensing systems  Real-time systems  Actuators  Vehicle dynamics  Torque 
Abstract: In this paper, we present a Perception-constrained Nonlinear Model Predictive Control (NMPC) framework for the real-time control of multi-rotor aerial vehicles. Our formulation considers both constraints from a perceptive sensor and realistic actuator limitations that are the rotor minimum and maximum speeds and accelerations. The formulation is meant to be generic and considers a large range of multi-rotor platforms (such as underactuated quadrotors or tilted-propellers hexarotors) since it does not rely on differential flatness for the dynamical equations, and a broad range of sensors, such as cameras, lidars, etc.... The perceptive constraints are expressed to maintain visibility of a feature point in the sensor's field of view, while performing a reference maneuver. We demonstrate both in simulation and real experiments that our framework is able to exploit the full capabilities of the multi-rotor, to achieve the motion under the aforementioned constraints, and control in real-time the platform at a motor-torque level, avoiding the use of an intermediate unconstrained trajectory tracker.


Title: CMTS: A Conditional Multiple Trajectory Synthesizer for Generating Safety-Critical Driving Scenarios
Key Words: Bayes methods  data analysis  mobile robots  road safety  road vehicles  safety-critical software  trajectory control  CMTS  conditional multiple trajectory synthesizer  safety-critical driving scenarios  naturalistic driving trajectory generation  autonomous driving algorithms  collision-free scenarios  safety-critical cases  near-miss scenarios  off-the-shelf datasets  generative model  conditional probability  trajectory predictions  autonomous vehicle safety  safety-critical data synthesizing framework  variational Bayesian methods  Trajectory  Interpolation  Roads  Training  Aerospace electronics  Data models  Autonomous vehicles 
Abstract: Naturalistic driving trajectory generation is crucial for the development of autonomous driving algorithms. However, most of the data is collected in collision-free scenarios leading to the sparsity of the safety-critical cases. When considering safety, testing algorithms in near-miss scenarios that rarely show up in off-the-shelf datasets and are costly to accumulate is a vital part of the evaluation. As a remedy, we propose a safety-critical data synthesizing framework based on variational Bayesian methods and term it as Conditional Multiple Trajectory Synthesizer (CMTS). We extend a generative model to connect safe and collision driving data by representing their distribution in the latent space and use conditional probability to adapt to different maps. Sampling from the mixed distribution enables us to synthesize the safety-critical data not shown in the safe or collision datasets. Experimental results demonstrate that the generated dataset covers many different realistic scenarios, especially the near-misses. We conclude that the use of data generated by CMTS can improve the accuracy of trajectory predictions and autonomous vehicle safety.


Title: Navigation Command Matching for Vision-based Autonomous Driving
Key Words: control engineering computing  image colour analysis  learning (artificial intelligence)  mobile robots  multi-agent systems  navigation  path planning  road traffic control  robot vision  robust control  traffic engineering computing  suboptimal policy  CARLA driving benchmark  vision-based autonomous driving  imitative reinforcement learning  robust driving policy  nonsmooth rewards  state-action pairs  smooth rewards  matching measurer  navigation rewards  navigation command matching  attention-guided agent  salient regions  RGB images  Navigation  Task analysis  Trajectory  Learning (artificial intelligence)  Autonomous vehicles  Smoothing methods  Current measurement 
Abstract: Learning an optimal policy for autonomous driving task to confront with complex environment is a long- studied challenge. Imitative reinforcement learning is accepted as a promising approach to learn a robust driving policy through expert demonstrations and interactions with environments. However, this model utilizes non-smooth rewards, which have a negative impact on matching between navigation commands and trajectory (state-action pairs), and degrade the generalizability of an agent. Smooth rewards are crucial to discriminate actions generated from sub-optimal policy. In this paper, we propose a navigation command matching (NCM) model to address this issue. There are two key components in NCM, 1) a matching measurer produces smooth navigation rewards that measure matching between navigation commands and trajectory; 2) attention-guided agent performs actions given states where salient regions in RGB images (i.e. roadsides, lane markings and dynamic obstacles) are highlighted to amplify their influence on the final model. We obtain navigation rewards and store transitions to replay buffer after an episode, so NCM is able to discriminate actions generated from suboptimal policy. Experiments on CARLA driving benchmark show our proposed NCM outperforms previous state-of-the- art models on various tasks in terms of the percentage of successfully completed episodes. Moreover, our model improves generalizability of the agent and obtains good performance even in unseen scenarios.


Title: Kidnapped Radar: Topological Radar Localisation using Rotationally-Invariant Metric Learning
Key Words: convolutional neural nets  CW radar  feature extraction  FM radar  learning (artificial intelligence)  nearest neighbour methods  radar imaging  radar tracking  polar nature  radar scan formation  cylindrical convolutions  anti-aliasing blurring  azimuth-wise max-pooling  rotational invariance  enforced metric space  topological localisation system  random rotational perturbation  kidnapped radar  topological radar localisation  rotationally-invariant metric learning  large-scale topological localisation  frequency-modulated continuous-wave scanning radar  efficient learning-based approach  radar data  polar radar scans  NetVLAD architectures  visual domain  feature extraction  radar-focused mobile autonomy dataset  CNN architectures  reference trajectory  nearest neighbour  place recognition  root architecture  convolutional neural network  distance 280.0 km  Measurement  Radar imaging  Robot sensing systems  Azimuth  Feature extraction  Trajectory  radar  localisation  place recognition  deep learning  metric learning 
Abstract: This paper presents a system for robust, large-scale topological localisation using Frequency-Modulated Continuous-Wave scanning radar which extends the state-of-the-art by an efficient, learning-based approach to handle radar data for localisation. We learn a metric space for embedding polar radar scans using CNN and NetVLAD architectures traditionally applied to the visual domain. However, we tailor the feature extraction for more suitability to the polar nature of radar scan formation using cylindrical convolutions, anti-aliasing blurring, and azimuth-wise max-pooling; all in order to bolster the rotational invariance. The enforced metric space is then used to encode a reference trajectory, serving as a map, which is queried for nearest neighbour for recognition of places at run-time. We demonstrate the performance of our topological localisation system over the course of many repeat forays using the largest radar-focused mobile autonomy dataset released to date, totalling 280 km of urban driving, a small portion of which we also use to learn the weights of the modified architecture. As this work represents a novel application for radar, we analyse the utility of the proposed method via a comprehensive set of metrics which provide insight into the efficacy when used in a realistic system, showing improved performance over the root architecture even in the face of random rotational perturbation.


Title: Global visual localization in LiDAR-maps through shared 2D-3D embedding space
Key Words: image recognition  learning (artificial intelligence)  mobile robots  neural nets  optical radar  robot vision  SLAM (robots)  global visual localization  LiDAR-maps  place recognition  autonomous driving field  vision-based approaches  image database  high definition 3D maps  deep neural network  shared embedding space  3D-LiDAR place recognition  3D DNN  2D-3D embedding space  Oxford Robotcar Dataset  image w.r.t.  Three-dimensional displays  Task analysis  Laser radar  Feature extraction  Visualization  Robots  Two dimensional displays 
Abstract: Global localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using the Oxford Robotcar Dataset, which encompasses a wide range of weather and light conditions.


Title: Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments
Key Words: Gaussian processes  learning (artificial intelligence)  mobile robots  Monte Carlo methods  neural nets  optical radar  path planning  recursive estimation  robot vision  SLAM (robots)  precise lidar-based robot localisation  large-scale environments  global localisation  mobile robots  Monte Carlo Localisation  MCL  fast localisation system  deep-probabilistic model  Gaussian process regression  deep kernel  precise recursive estimator  Gaussian method  deep probabilistic localisation  large-scale localisation  largescale environment  time 0.8 s  size 0.75 m  Robots  Neural networks  Three-dimensional displays  Gaussian processes  Laser radar  Monte Carlo methods  Kernel 
Abstract: This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deeplearned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and nonGaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75 m in a largescale environment of approximately 0.5 km2.


Title: Set-membership state estimation by solving data association
Key Words: mobile robots  position control  robot vision  sensor fusion  SLAM (robots)  state estimation  deterministic approach  data association  underwater robot  sonar data  membership state estimation  localization problem  indistinguishable landmarks  diving phase  unknown initial position  Sonar  State estimation  Rocks  Trajectory  Robot sensing systems  Reliability 
Abstract: This paper deals with the localization problem of a robot in an environment made of indistinguishable landmarks, and assuming the initial position of the vehicle is unknown. This scenario is typically encountered in underwater applications for which landmarks such as rocks all look alike. Furthermore, the position of the robot may be lost during a diving phase, which obliges us to consider unknown initial position. We propose a deterministic approach to solve simultaneously the problems of data association and state estimation, without combinatorial explosion. The efficiency of the method is shown on an actual experiment involving an underwater robot and sonar data.


Title: A Linearly Constrained Nonparametric Framework for Imitation Learning
Key Words: Bayes methods  end effectors  learning systems  predictive control  trajectory control  imitation learning  constrained skills  linearly constrained optimization problem  nonparametric solution  linearly constrained nonparametric framework  human skills learning  constrained motor skills learning  robotic systems  end-effector trajectory  linearly constrained kernelized movement primitives  LC-KMP  probabilistic properties  predictive control  locomotion tasks  grasping tasks  human robot collaborations  Trajectory  Probabilistic logic  Robots  Task analysis  Optimization  Kernel  Grasping 
Abstract: In recent years, a myriad of advanced results have been reported in the community of imitation learning, ranging from parametric to non-parametric, probabilistic to non-probabilistic and Bayesian to frequentist approaches. Meanwhile, ample applications (e.g., grasping tasks and humanrobot collaborations) further show the applicability of imitation learning in a wide range of domains. While numerous literature is dedicated to the learning of human skills in unconstrained environments, the problem of learning constrained motor skills, however, has not received equal attention. In fact, constrained skills exist widely in robotic systems. For instance, when a robot is demanded to write letters on a board, its end-effector trajectory must comply with the plane constraint from the board. In this paper, we propose linearly constrained kernelized movement primitives (LC-KMP) to tackle the problem of imitation learning with linear constraints. Specifically, we propose to exploit the probabilistic properties of multiple demonstrations, and subsequently incorporate them into a linearly constrained optimization problem, which finally leads to a non-parametric solution. In addition, a connection between our framework and the classical model predictive control is provided. Several examples including simulated writing and locomotion tasks are presented to show the effectiveness of our framework.


Title: An Energy-based Approach to Ensure the Stability of Learned Dynamical Systems
Key Words: learning (artificial intelligence)  motion control  nonlinear dynamical systems  regression analysis  stability  energy-based approach  learned dynamical systems  nonlinear dynamical systems  reactive motion generation  stable motions  accurate motions  learning problems  training time  single-step learning  regression technique  single-step approach  energy considerations  learned dynamics  demonstrated motion  Lyapunov methods  Training data  Robots  Electrostatic discharges  Stability analysis  Trajectory  Task analysis 
Abstract: Non-linear dynamical systems represent a compact, flexible, and robust tool for reactive motion generation. The effectiveness of dynamical systems relies on their ability to accurately represent stable motions. Several approaches have been proposed to learn stable and accurate motions from demonstration. Some approaches work by separating accuracy and stability into two learning problems, which increases the number of open parameters and the overall training time. Alternative solutions exploit single-step learning but restrict the applicability to one regression technique. This paper presents a single-step approach to learn stable and accurate motions that work with any regression technique. The approach makes energy considerations on the learned dynamics to stabilize the system at run-time while introducing small deviations from the demonstrated motion. Since the initial value of the energy injected into the system affects the reproduction accuracy, it is estimated from training data using an efficient procedure. Experiments on a real robot and a comparison on a public benchmark shows the effectiveness of the proposed approach.


Title: IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data
Key Words: human-robot interaction  learning (artificial intelligence)  manipulators  RoboTurk Cans dataset  offline learning  IRIS  offline robot manipulation data  offline task demonstrations  robotics  goal-conditioned low-level controller  high-level goal selection mechanism  learning control  learning from large-scale demonstration datasets  Implicit Reinforcement without Interaction at Scale  crowdsourcing  Task analysis  Robots  Trajectory  Iris  Learning (artificial intelligence)  Iris recognition  Grasping 
Abstract: Learning from offline task demonstrations is a problem of great interest in robotics. For simple short-horizon manipulation tasks with modest variation in task instances, offline learning from a small set of demonstrations can produce controllers that successfully solve the task. However, leveraging a fixed batch of data can be problematic for larger datasets and longer-horizon tasks with greater variations. The data can exhibit substantial diversity and consist of suboptimal solution approaches. In this paper, we propose Implicit Reinforcement without Interaction at Scale (IRIS), a novel framework for learning from large-scale demonstration datasets. IRIS factorizes the control problem into a goal-conditioned low-level controller that imitates short demonstration sequences and a high-level goal selection mechanism that sets goals for the low-level and selectively combines parts of suboptimal solutions leading to more successful task completions. We evaluate IRIS across three datasets, including the RoboTurk Cans dataset collected by humans via crowdsourcing, and show that performant policies can be learned from purely offline learning. Additional results at https://sites.google.com/stanford.edu/iris/.


Title: Geometry-aware Dynamic Movement Primitives
Key Words: differential geometry  learning (artificial intelligence)  manipulators  matrix algebra  geometry-aware dynamic movement primitives  robot control problems  manipulability ellipsoids  symmetric positive definite matrices  DMPs  SPD matrices  Euclidean space  mathematically principled framework  SPD manifold  Riemannian metrics  Manifolds  Robots  Symmetric matrices  Standards  Ellipsoids  Switches  Measurement 
Abstract: In many robot control problems, factors such as stiffness and damping matrices and manipulability ellipsoids are naturally represented as symmetric positive definite (SPD) matrices, which capture the specific geometric characteristics of those factors. Typical learned skill models such as dynamic movement primitives (DMPs) can not, however, be directly employed with quantities expressed as SPD matrices as they are limited to data in Euclidean space. In this paper, we propose a novel and mathematically principled framework that uses Riemannian metrics to reformulate DMPs such that the resulting formulation can operate with SPD data in the SPD manifold. Evaluation of the approach demonstrates that beneficial properties of DMPs such as change of the goal during operation apply also to the proposed formulation.


Title: Learning a Pile Loading Controller from Demonstrations
Key Words: control engineering computing  earthmoving equipment  foundations  image representation  mobile robots  neural net architecture  random forests  video signal processing  hydrostatic driving pressure  control signals  application specific deep visual features  Siamese network architecture  random forest regressor  loading distance  autonomous robotic wheel loader  learning-based pile loading controller  controller parameters  low level sensor  boom angle  bucket angle  cross-entropy  contrastive loss  soil type  Visualization  Loading  Robot sensing systems  Task analysis  Feature extraction  Robustness 
Abstract: This work introduces a learning-based pile loading controller for autonomous robotic wheel loaders. Controller parameters are learnt from a small number of demonstrations for which low level sensor (boom angle, bucket angle and hydrostatic driving pressure), egocentric video frames and control signals are recorded. Application specific deep visual features are learnt from demonstrations using a Siamese network architecture and a combination of cross-entropy and contrastive loss. The controller is based on a Random Forest (RF) regressor that provides robustness against changes in field conditions (loading distance, soil type, weather and illumination). The controller is deployed to a real autonomous robotic wheel loader and it outperforms prior art with a clear margin.


Title: Learning Navigation Costs from Demonstration in Partially Observable Environments
Key Words: cost optimal control  dynamic programming  learning (artificial intelligence)  learning systems  mobile robots  navigation  observability  path planning  probability  state-space methods  trajectory control  inverse reinforcement learning  safe navigation  autonomous navigation  unknown partially observable environments  navigation behavior  state-control trajectory  cost function representation  probabilistic occupancy encoder  observation sequence  cost encoder  occupancy features  representation parameters  control policy  value function  state space  motion-planning algorithm  robot navigation  navigation cost learning  dynamic  Robots  Cost function  Navigation  Planning  Feature extraction  Heuristic algorithms  Task analysis 
Abstract: This paper focuses on inverse reinforcement learning (IRL) to enable safe and efficient autonomous navigation in unknown partially observable environments. The objective is to infer a cost function that explains expert-demonstrated navigation behavior while relying only on the observations and state-control trajectory used by the expert. We develop a cost function representation composed of two parts: a probabilistic occupancy encoder, with recurrent dependence on the observation sequence, and a cost encoder, defined over the occupancy features. The representation parameters are optimized by differentiating the error between demonstrated controls and a control policy computed from the cost encoder. Such differentiation is typically computed by dynamic programming through the value function over the whole state space. We observe that this is inefficient in large partially observable environments because most states are unexplored. Instead, we rely on a closed-form subgradient of the cost-to-go obtained only over a subset of promising states via an efficient motion-planning algorithm such as A* or RRT. Our experiments show that our model exceeds the accuracy of baseline IRL algorithms in robot navigation tasks, while substantially improving the efficiency of training and test-time inference.


Title: Towards Bimanual Vein Cannulation: Preliminary Study of a Bimanual Robotic System With a Dual Force Constraint Controller
Key Words: biological tissues  biomechanics  blood vessels  eye  manipulators  medical robotics  phantoms  surgery  surgical tools  dual force constraint controller  robot-assisted retinal surgery  tool-to-sclera forces  cannulation tool  dual force-sensing capability  force information  robot controller  retinal vein cannulation  target vessel  robotic manipulators  tool-to-tissue forces  retinal tissue injury  bimanual cannulation  bimanual robotic system  retinal vein occlusion  occluded vessel  interaction forces  bimanual vein cannulation  tool-to-sclera force  tool-to-vessel force  steady hand eye robot platforms  Tools  Force  Robot sensing systems  Retina  Surgery  Veins 
Abstract: Retinal vein cannulation is a promising approach for treating retinal vein occlusion that involves injecting medicine into the occluded vessel to dissolve the clot. The approach remains largely unexploited clinically due to surgeon limitations in detecting interaction forces between surgical tools and retinal tissue. In this paper, a dual force constraint controller for robot-assisted retinal surgery was presented to keep the tool-to-vessel forces and tool-to-sclera forces below prescribed thresholds. A cannulation tool and forceps with dual force-sensing capability were developed and used to measure force information fed into the robot controller, which was implemented on existing Steady Hand Eye Robot platforms. The robotic system facilitates retinal vein cannulation by allowing a user to grasp the target vessel with the forceps and then enter the vessel with the cannula. The system was evaluated on an eye phantom. The results showed that, while the eyeball was subjected to rotational disturbances, the proposed controller actuates the robotic manipulators to maintain the average tool-to-vessel force at 10.9 mN and 13.1 mN and the average tool-to-sclera force at 38.1 mN and 41.2 mN for the cannula and the forcpes, respectively. Such small tool-to-tissue forces are acceptable to avoid retinal tissue injury. Additionally, two clinicians participated in a preliminary user study of the bimanual cannulation demonstrating that the operation time and tool-to-tissue forces are significantly decreased when using the bimanual robotic system as compared to freehand performance.


Title: Evaluation of a combined grip of pinch and power grips in manipulating a master manipulator
Key Words: biomechanics  dexterous manipulators  grippers  medical robotics  surgery  master manipulator  combined-grip-handle scheme  pinch grip motion  power grip motion  robotic surgery  Conferences  Automation 
Abstract: In conventional robotic surgery, the manipulating methods exhibit limitations that are strongly related to the advantages and disadvantages of a pinch grip and power grip. The context of this study is focused on the introduction of a combined grip to compensate for such restraints. In particular, this study proposed the combined-grip-handle scheme on a master manipulator. In this framework, the position of fingertips was designed to be adjustable in distance and direction to allow for a pinch grip motion around the holding axis of a power grip motion. A ring-bar experiment applying the master-slave scheme was conducted with the master manipulator under several manipulating conditions of the combined grip and the conventional gripping types. Results for using the combined grip demonstrated that the combined grip showed better performance on the positioning operation, compared with the conventional gripping types.


Title: Contact Stability Analysis of Magnetically-Actuated Robotic Catheter Under Surface Motion
Key Words: biological tissues  biomedical MRI  cardiology  catheters  medical image processing  medical robotics  contact stability analysis  magnetically-actuated robotic catheter  contact force quality  lesion formation  lesion size  gap-free lesion  tissue surface motion  contact model  contact force control schemes  heart surface motions  magnetic resonance imaging-actuated robotic catheter  Catheters  Force  Force control  Robots  Stability analysis  Magnetic resonance imaging  Friction 
Abstract: Contact force quality is one of the most critical factors for safe and effective lesion formation during cardiac ablation. The contact force and contact stability plays important roles in determining the lesion size and creating a gap-free lesion. In this paper, the contact stability of a novel magnetic resonance imaging (MRI)-actuated robotic catheter under tissue surface motion is studied. The robotic catheter is modeled using a pseudo-rigid-body model, and the contact model under surface constraint is provided. Two contact force control schemes to improve the contact stability of the catheter under heart surface motions are proposed and their performance are evaluated in simulation.


Title: Fast and accurate intracorporeal targeting through an anatomical orifice exhibiting unknown behavior.
Key Words: adaptive control  biomechanics  biomedical equipment  force control  manipulators  medical robotics  motion control  position control  surgery  anatomical orifice  minimally invasive surgery  interaction forces  patient anatomy  orifice behavior  adaptive control scheme  wrist velocity  tip velocity  intracorporeal targeting  instrument tip positioning  3DOF wrist center positioning problem  Robots  Instruments  Wrist  Surgery  Force  Adaptation models  Kinematics 
Abstract: Surgery may involve precise instrument tip positioning in a minimally invasive way. During these operations, the instrument is inserted in the body through an orifice. The movements of the instrument are constrained by interaction forces arising at the orifice level. The physical constraints may drastically vary depending on the patient's anatomy. This introduces uncertainties that challenge the positioning task for a robot. Indeed, it raises an antagonism: On one side, the required precision appeals for a rigid behavior. On the other side, forces applied at the entry point should be limited, which requires softness. In this paper we choose to minimize forces at the orifice by using a passive ball joint wrist to manipulate the instrument. From a control perspective, this leads to consider the task as a 3 DOF wrist center positioning problem, whose softness can be achieved through conventional low impedance control. However, positioning the wrist center, even with a high static precision, does not allow to achieve a high precision of the instrument tip positioning when the orifice behavior is not known. To cope with this problem, we implement a controller that servos the tip position by commanding the wrist position. In order to deal with uncertainties, we exploit an adaptive control scheme that identifies in real-time the unknown mapping between the wrist velocity and the tip velocity. Both simulations and in vitro experimental results show the efficiency of the control law.


Title: Robotic Swarm Control for Precise and On-Demand Embolization
Key Words: biomedical materials  bioMEMS  blood vessels  haemodynamics  magnetic particles  medical robotics  microfluidics  targeted embolization  swarm control technique  magnetic particles  mean absolute error  aggregation control approach  fluidic shear  magnetic forces  magnetic field  magnetic swarm control strategy  blood vessels  clinical embolization  swarm control capability  fluidic flow environment  magnetic aggregates  magnetic swarms  robotic control  robotic swarm control  Aggregates  Magnetic tunneling  Coils  Junctions  Magnetic particles  Blood vessels  Magnetic separation 
Abstract: Existing approaches for robotic control of magnetic swarms are not capable of generating magnetic aggregates precisely in an arbitrarily specified target region in a fluidic flow environment. Such a swarm control capability is demanded by medical applications such as clinical embolization (i.e., localized clogging of blood vessels). This paper presents a new magnetic swarm control strategy to generate aggregates only in a specified target region under fluidic flow. Within the target region, the magnetic field generates sufficiently large magnetic forces among magnetic particles to maintain the aggregates' integrity at the junctions of blood vessels. In contrast, unintended aggregates outside the target region are disassembled by fluidic shear. The aggregation control approach achieved a mean absolute error of 0.15 mm in positioning a target region and a mean absolute error of 0.30 mm in controlling the target region's radius. With thrombin coating, 1 μm magnetic particles were controlled to perform embolization both in vitro (using microfluidic channel networks) and ex vivo (using porcine tissue). Experiments proved the effectiveness of the swarm control technique for on-demand, targeted embolization.


Title: Bilateral Teleoperation Control of a Redundant Manipulator with an RCM Kinematic Constraint
Key Words: augmented reality  end effectors  haptic interfaces  human-robot interaction  medical robotics  motion control  position control  redundant manipulators  stability  surgery  telerobotics  energy tank model  haptic feedback  KUKA LWR4+ serial robot  Sigma 7 haptic manipulator  redundant manipulator  RCM kinematic constraint  serial robot manipulator  remote center of motion constraint  decoupled cartesian admittance control  end effector  bilateral teleoperation control stability  augmented reality  teleoperated surgery  Manipulators  Surgery  Force  Frequency modulation  Kinematics  Haptic interfaces  Switches 
Abstract: In this paper, a bilateral teleoperation control of a serial robot manipulator, which guarantees a Remote Center of Motion (RCM) constraint in its kinematic level, is developed. A two-layered approach based on the energy tank model is proposed to achieve haptic feedback on the end effector with a pedal switch. The redundancy of the manipulator is exploited to maintain the RCM constraint using the decoupled Cartesian Admittance Control. Transparency and stability of the proposed bilateral teleoperation are demonstrated using a KUKA LWR4+ serial robot and a Sigma 7 haptic manipulator with an RCM constraint in augmented reality. The results prove that the control can achieve not only the bilateral teleoperation but also maintain the RCM constraint.


Title: From Bipedal Walking to Quadrupedal Locomotion: Full-Body Dynamics Decomposition for Rapid Gait Generation
Key Words: gait analysis  legged locomotion  robot dynamics  trajectory control  full-order dynamics  rapid generation  stepping-in-place gaits  diagonally symmetric ambling gait  dynamic bipedal walking  quadrupedal locomotion  rapid gait generation  hybrid dynamics  three-dimensional quadrupedal robot  hybrid zero dynamics framework  bipedal robots  bipedal walking gaits  quadrupedal trajectory  Legged locomotion  Robot kinematics  Dynamics  Nonlinear dynamical systems  Jacobian matrices  Trajectory 
Abstract: This paper systematically decomposes a quadrupedal robot into bipeds to rapidly generate walking gaits and then recomposes these gaits to obtain quadrupedal locomotion. We begin by decomposing the full-order, nonlinear and hybrid dynamics of a three-dimensional quadrupedal robot, including its continuous and discrete dynamics, into two bipedal systems that are subject to external forces. Using the hybrid zero dynamics (HZD) framework, gaits for these bipedal robots can be rapidly generated (on the order of seconds) along with corresponding controllers. The decomposition is achieved in such a way that the bipedal walking gaits and controllers can be composed to yield dynamic walking gaits for the original quadrupedal robot - the result is the rapid generation of dynamic quadruped gaits utilizing the full-order dynamics. This methodology is demonstrated through the rapid generation (3.96 seconds on average) of four stepping-in-place gaits and one diagonally symmetric ambling gait at 0.35 m/s on a quadrupedal robot - the Vision 60, with 36 state variables and 12 control inputs - both in simulation and through outdoor experiments. This suggested a new approach for fast quadrupedal trajectory planning using full-body dynamics, without the need for empirical model simplification, wherein methods from dynamic bipedal walking can be directly applied to quadrupeds.


Title: Posture Control for a Low-Cost Commercially-Available Hexapod Robot*
Key Words: compliance control  force control  gait analysis  image motion analysis  legged locomotion  position control  robot dynamics  torque control  direct force control  Vicon motion capture system  custom-designed platforms  low-cost commercially-available hexapod robot  legged robots  custom-designed robotic platforms  commercially-available robots  low-cost research platforms  low-cost joint actuators  torque control capabilities  hierarchical control system  virtual model control  simple foot force distribution  walking posture control system  Legged locomotion  Foot  Force  Robot sensing systems  Control systems  Engines 
Abstract: Posture control for legged robots has been widely developed on custom-designed robotic platforms, with little work being done on commercially-available robots despite their potential as low-cost research platforms. This paper presents the implementation of a Walking Posture Control system on a commercially-available hexapod robot which utilizes low-cost joint actuators without torque control capabilities. The hierarchical control system employs Virtual Model Control with simple foot force distribution and a novel, position-based Foot Force Controller that enables direct force control during the leg's stance phase and active compliance control during the swing phase. Ground truth measurements in experimental tests, obtained with a Vicon motion capture system, demonstrate the improvement to posture made by the control system on uneven terrain, with the results comparing favorably to those obtained in similar tests on more sophisticated, custom-designed platforms.


Title: Collaborative Multi-Robot Localization in Natural Terrain*
Key Words: autonomous underwater vehicles  filtering theory  mobile robots  Monte Carlo methods  multi-robot systems  path planning  sensor fusion  Monterey Bay  terrain relative navigation  filter architecture  collaborative multirobot localization  standard TRN  Monte Carlo simulation  inter-vehicle range measurements  autonomous underwater vehicle  multirobot information  TRN techniques  covariance intersection  Robots  Correlation  Atmospheric measurements  Particle measurements  Extraterrestrial measurements  Collaboration  Navigation 
Abstract: This paper presents a novel filter architecture that allows a team of vehicles to collaboratively localize using Terrain Relative Navigation (TRN). The work explores several causes of measurement correlation that preclude the use of traditional estimators, and proposes an estimator structure that eliminates one source of measurement correlation while properly incorporating others through the use of Covariance Intersection. The result is a consistent estimator that is able to augment proven TRN techniques with multi-robot information, significantly improving localization for vehicles in uninformative terrain. The approach is demonstrated using field data from an Autonomous Underwater Vehicle (AUV) navigating with TRN in Monterey Bay and simulated inter-vehicle range measurements. In addition, a Monte Carlo simulation was used to quantify the algorithm's performance on one example mission. Monte Carlo results show that a vehicle operating in uninformative terrain has 62% lower localization error when fusing range measurements to two converged AUVs than it would using standard TRN.


Title: Multi-Robot Control Using Coverage Over Time-Varying Non-Convex Domains
Key Words: computational geometry  linearisation techniques  multi-robot systems  time-varying systems  time-varying domains  nonconvex shape  nonconvex coverage problem  control law  time-varying density  time-varying diffeomorphism  multirobot control  time-varying nonconvex domains  coverage control  Robot kinematics  Multi-robot systems  Time-varying systems  Collision avoidance  Robot sensing systems  Transforms 
Abstract: This paper addresses the problem of a domain becoming non-convex while using coverage control of a multirobot system over time-varying domains. When the domain moves around in the workspace, its motion and the presence of obstacles might cause it to deform into some non-convex shape, and the robot team should act in a coordinating manner to maintain coverage. The proposed solution is based on a framework for constructing a diffeomorphism to transform a non-convex coverage problem into a convex one. A control law is developed to capture the effects of time variations (e.g., from a time-varying density, time-varying convex hull of the domain and time-varying diffeomorphism) in the system. Analytic expressions of each term in the control law are found for uniform density case. A simulation and robotic implementation are used to validate the proposed algorithm.


Title: Efficient Large-Scale Multi-Drone Delivery Using Transit Networks
Key Words: autonomous aerial vehicles  computational complexity  graph theory  multi-robot systems  optimisation  near-optimal polynomial-time task allocation algorithm  delivery sequences  two-layer approach  multifaceted complexity  maximum time  comprehensive algorithmic framework  public transit vehicles  efficient large-scale multidrone delivery  transit network  bounded-suboptimal multiagent pathfinding techniques  Drones  Task analysis  Resource management  Routing  Urban areas  Planning 
Abstract: We consider the problem of controlling a large fleet of drones to deliver packages simultaneously across broad urban areas. To conserve energy, drones hop between public transit vehicles (e.g., buses and trams). We design a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery. We address the multifaceted complexity of the problem through a two-layer approach. First, the upper layer assigns drones to package delivery sequences with a near-optimal polynomial-time task allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network while employing efficient bounded-suboptimal multi-agent pathfinding techniques tailored to our setting. Experiments demonstrate the efficiency of our approach on settings with up to 200 drones, 5000 packages, and transit networks with up to 8000 stops in San Francisco and Washington DC. Our results show that the framework computes solutions within a few seconds (up to 2 minutes at most) on commodity hardware, and that drones travel up to 450% of their flight range with public transit.


Title: Resilience in multi-robot target tracking through reconfiguration
Key Words: convex programming  covariance matrices  integer programming  Kalman filters  mobile robots  multi-robot systems  target tracking  multirobot target  resource availability  networked multirobot system  target tracking  sensing resources  computational resources  distributed Kalman filter  sensor measurement noise covariance matrix  sensing quality deteriorates  systems communication graph  sensor quality  active communication links  mixed integer semidefinite programming formulations  agent-centric strategy  team-centric strategy  greedy strategy  Robot sensing systems  Target tracking  Covariance matrices  Kalman filters  Robot kinematics 
Abstract: We address the problem of maintaining resource availability in a networked multi-robot system performing distributed target tracking. In our model, robots are equipped with sensing and computational resources enabling them to track a target's position using a Distributed Kalman Filter (DKF). We use the trace of each robot's sensor measurement noise covariance matrix as a measure of sensing quality. When a robot's sensing quality deteriorates, the systems communication graph is modified by adding edges such that the robot with deteriorating sensor quality may share information with other robots to improve the team's target tracking ability. This computation is performed centrally and is designed to work without a large change in the number of active communication links. We propose two mixed integer semi-definite programming formulations (an `agent-centric' strategy and a `team-centric' strategy) to achieve this goal. We implement both formulations and a greedy strategy in simulation and show that the team-centric strategy outperforms the agent-centric and greedy strategies.


Title: Teleoperation of Multi-Robot Systems to Relax Topological Constraints
Key Words: graph theory  mobile robots  multi-robot systems  telerobotics  motion pattern  graph  multirobot teleoperation  mobile robots  topological constraints  Multi-robot systems  Force  Mobile robots  Force feedback  Collision avoidance  Damping 
Abstract: Multi-robot systems are able to achieve common objectives exchanging information among each other. This is possible exploiting a communication structure, usually modeled as a graph, whose topological properties (such as connectivity) are very relevant in the overall performance of the multirobot system. When considering mobile robots, such properties can change over time: robots are then controlled to preserve them, thus guaranteeing the possibility, for the overall system, to achieve its goals. This, however, implies limitations on the possible motion patterns of the robots, thus reducing the flexibility of the overall multi-robot system. In this paper we introduce teleoperation as a means to reduce these limitations, allowing temporary violations of topological properties, with the aim of increasing the flexibility of the multi-robot system.


Title: Eciton robotica: Design and Algorithms for an Adaptive Self-Assembling Soft Robot Collective
Key Words: mobile robots  multi-robot systems  search problems  self-adjusting systems  Eciton robotica  self-assembling soft robot collective  social insects  centralized control system  army ants build bridges  flexible materials  robotic collectives  flexible robots  self-assembling robotic systems  lattice-based structures  soft robots  amorphous structures  Robot sensing systems  Bridges  Grippers  Vibrations  Self-assembly  Hardware 
Abstract: Social insects successfully create bridges, rafts, nests and other structures out of their own bodies and do so with no centralized control system, simply by following local rules. For example, while traversing rough terrain, army ants (genus Eciton) build bridges which grow and dissolve in response to local traffic. Because these self-assembled structures incorporate smart, flexible materials (i.e. ant bodies) and emerge from local behavior, the bridges are adaptive and dynamic. With the goal of realizing robotic collectives with similar features, we designed a hardware system, Eciton robotica, consisting of flexible robots that can climb over each other to assemble compliant structures and communicate locally using vibration. In simulation, we demonstrate self-assembly of structures: using only local rules and information, robots build and dissolve bridges in response to local traffic and varying terrain. Unlike previous self-assembling robotic systems that focused on lattice-based structures and predetermined shapes, our system takes a new approach where soft robots attach to create amorphous structures whose final self-assembled shape can adapt to the needs of the group.


Title: Stable Tool-Use with Flexible Musculoskeletal Hands by Learning the Predictive Model of Sensor State Transition
Key Words: feedback  feedforward  learning (artificial intelligence)  manipulators  materials handling  sensors  feedforward controls  initial contact state  predictive network  sensor state transition  actual robot sensor information  feedback control  stable tool-use  flexible musculoskeletal hands  predictive model  adaptability  impact resistance  sensors  actuators  Grasping  Muscles  Robot sensing systems  Gold  Tools 
Abstract: The flexible under-actuated musculoskeletal hand is superior in its adaptability and impact resistance. On the other hand, since the relationship between sensors and actuators cannot be uniquely determined, almost all its controls are based on feedforward controls. When grasping and using a tool, the contact state of the hand gradually changes due to the inertia of the tool or impact of action, and the initial contact state is hardly kept. In this study, we propose a system that trains the predictive network of sensor state transition using the actual robot sensor information, and keeps the initial contact state by a feedback control using the network. We conduct experiments of hammer hitting, vacuuming, and brooming, and verify the effectiveness of this study.


Title: Learning to Transfer Dynamic Models of Underactuated Soft Robotic Hands
Key Words: dexterous manipulators  learning (artificial intelligence)  Lyapunov methods  neural nets  underactuated soft robotic hands  transfer learning  data limitations  data collection  physical robots  neural networks  transferred model  trained transition model  dynamic model  chaotic behavior  divergent behavior  upper bound  Lyapunov exponent  Adaptation models  Robots  Data models  Neural networks  Analytical models  Friction  Predictive models 
Abstract: Transfer learning is a popular approach to bypassing data limitations in one domain by leveraging data from another domain. This is especially useful in robotics, as it allows practitioners to reduce data collection with physical robots, which can be time-consuming and cause wear and tear. The most common way of doing this with neural networks is to take an existing neural network, and simply train it more with new data. However, we show that in some situations this can lead to significantly worse performance than simply using the transferred model without adaptation. We find that a major cause of these problems is that models trained on small amounts of data can have chaotic or divergent behavior in some regions. We derive an upper bound on the Lyapunov exponent of a trained transition model, and demonstrate two approaches that make use of this insight. Both show significant improvement over traditional fine-tuning. Experiments performed on real underactuated soft robotic hands clearly demonstrate the capability to transfer a dynamic model from one hand to another.


Title: Periodic movement learning in a soft-robotic arm*
Key Words: end effectors  learning (artificial intelligence)  medical robotics  motion control  trajectory control  cyclic rhythmic patterns  oscillatory signals  actuator  central pattern generator  periodic motion  end-effector  model-free neurodynamic scheme  CPG model  simulation model  learning architecture  periodic movement learning  modular bio-inspired soft-robotic arm  Oscillators  Trajectory  Manipulators  Biological system modeling  Soft robotics  Mathematical model  Reinforcement learning  Central pattern generators  Soft Robotics  Rhythmic movements 
Abstract: In this paper we introduce a novel technique that aims to dynamically control a modular bio-inspired soft-robotic arm in order to perform cyclic rhythmic patterns. Oscillatory signals are produced at the actuator's level by a central pattern generator (CPG), resulting in the generation of a periodic motion by the robot's end-effector. The proposed controller is based on a model-free neurodynamic scheme and is assigned with the task of training a policy that computes the parameters of the CPG model which generates a trajectory with desired features. The proposed methodology is first evaluated with a simulation model, which successfully reproduces the trained targets. Then experiments are also conducted using the real robot. Both procedures validate the efficiency of the learning architecture to successfully complete these tasks.


Title: Mechanism and Model of a Soft Robot for Head Stabilization in Cancer Radiation Therapy
Key Words: actuators  biomechanics  cancer  deformation  elastic deformation  elasticity  electric actuators  finite element analysis  kinematics  medical image processing  medical robotics  radiation therapy  robot kinematics  stress-strain relations  soft robot  head stabilization  cancer radiation  parallel robot mechanism  constituent soft actuators  real-time motion-correction  treatment machine  stress-strain constitutive laws  inverse kinematics  radially symmetric displacement formulation  finite elastic deformation framework  Strain  Actuators  Adaptation models  Stress  Real-time systems  Robots  Cancer 
Abstract: We present a parallel robot mechanism and the constitutive laws that govern the deformation of its constituent soft actuators. Our ultimate goal is the real-time motion-correction of a patient's head deviation from a target pose where the soft actuators control the position of the patient's cranial region on a treatment machine. We describe the mechanism, derive the stress-strain constitutive laws for the individual actuators and the inverse kinematics that prescribes a given deformation, and then present simulation results that validate our mathematical formulation. Our results demonstrate deformations consistent with our radially symmetric displacement formulation under a finite elastic deformation framework.


Title: Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras
Key Words: calibration  cameras  image colour analysis  image representation  image sensors  learning (artificial intelligence)  pose estimation  robot vision  stereo image processing  multiple depth sensors  imperfect camera calibration  uncalibrated cameras  camera-views  single view robotic agents  voxel grid  relative pose estimation  3D scene representations  registered output  explicit 3D representations  sensor dropout  insertion tasks  task performance  multicamera approach  uncalibrated RGB camera  precise manipulation tasks  closed-loop end-to-end learning  multiview approach  multiple uncalibrated cameras  precise 3D manipulation  Task analysis  Cameras  Robot vision systems  Three-dimensional displays  Robot kinematics 
Abstract: In this work, we present an effective multi-view approach to closed-loop end-to-end learning of precise manipulation tasks that are 3D in nature. Our method learns to accomplish these tasks using multiple statically placed but uncalibrated RGB camera views without building an explicit 3D representation such as a pointcloud or voxel grid. This multi-camera approach achieves superior task performance on difficult stacking and insertion tasks compared to single-view baselines. Single view robotic agents struggle from occlusion and challenges in estimating relative poses between points of interest. While full 3D scene representations (voxels or pointclouds) are obtainable from registered output of multiple depth sensors, several challenges complicate operating off such explicit 3D representations. These challenges include imperfect camera calibration, poor depth maps due to object properties such as reflective surfaces, and slower inference speeds over 3D representations compared to 2D images. Our use of static but uncalibrated cameras does not require camera-robot or camera-camera calibration making the proposed approach easy to setup and our use of sensor dropout during training makes it resilient to the loss of camera-views after deployment.


Title: Surfing on an uncertain edge: Precision cutting of soft tissue using torque-based medium classification
Key Words: biological tissues  closed loop systems  image classification  manipulator dynamics  medical robotics  robot vision  surgery  torque measurement  joint torque measurements  closed loop control law  grapefruit cutting task  grapefruit pulp  uncertain edge  precision cutting  soft tissue  torque-based medium classification  visibility constraints  cutting trajectory  binary medium classifier  robotics  Task analysis  Trajectory  Pipelines  Torque  Robot sensing systems  Predictive models 
Abstract: Precision cutting of soft-tissue remains a challenging problem in robotics, due to the complex and unpredictable mechanical behaviour of tissue under manipulation. Here, we consider the challenge of cutting along the boundary between two soft mediums, a problem that is made extremely difficult due to visibility constraints, which means that the precise location of the cutting trajectory is typically unknown. This paper introduces a novel strategy to address this task, using a binary medium classifier trained using joint torque measurements, and a closed loop control law that relies on an error signal compactly encoded in the decision boundary of the classifier. We illustrate this on a grapefruit cutting task, successfully modulating a nominal trajectory t using dynamic movement primitives to follow the boundary between grapefruit pulp and peel using torque based medium classification. Results show that this control strategy is successful in 72 % of attempts in contrast to control using a nominal trajectory, which only succeeds in 50 % of attempts.


Title: Learning to combine primitive skills: A step towards versatile robotic manipulation §
Key Words: image motion analysis  learning (artificial intelligence)  manipulators  path planning  robot vision  dynamic scene changes  visual inputs  task-specific reward engineering  previous limitations  reinforcement learning approach  primitive skills  learning methods  intermediate rewards  complete task demonstrations  vision-based task planning  basic skills  synthetic demonstrations  data augmentation  manipulation tasks  UR5 robotic arm  versatile robotic manipulation  robotics  traditional task  motion planning methods  state observability  Task analysis  Robots  Planning  Learning (artificial intelligence)  Training  Trajectory  Visualization 
Abstract: Manipulation tasks such as preparing a meal or assembling furniture remain highly challenging for robotics and vision. Traditional task and motion planning (TAMP) methods can solve complex tasks but require full state observability and are not adapted to dynamic scene changes. Recent learning methods can operate directly on visual inputs but typically require many demonstrations and/or task-specific reward engineering. In this work we aim to overcome previous limitations and propose a reinforcement learning (RL) approach to task planning that learns to combine primitive skills. First, compared to previous learning methods, our approach requires neither intermediate rewards nor complete task demonstrations during training. Second, we demonstrate the versatility of our vision-based task planning in challenging settings with temporary occlusions and dynamic scene changes. Third, we propose an efficient training of basic skills from few synthetic demonstrations by exploring recent CNN architectures and data augmentation. Notably, while all of our policies are learned on visual inputs in simulated environments, we demonstrate the successful transfer and high success rates when applying such policies to manipulation tasks on a real UR5 robotic arm.


Title: Learning Affordance Space in Physical World for Vision-based Robotic Object Manipulation
Key Words: image texture  learning (artificial intelligence)  manipulators  neural nets  probability  robot vision  pixel-wise probability affordance map  image space  world space  viewpoints  multiple-object pushing  multiple-object grasping  physical world  vision-based robotic object manipulation  Affordance Space Perception Network  deep neural network  3D affordance space  training strategy  task-agnostic framework  singular-object pushing  singular-object grasping  Robots  Task analysis  Robustness  Grasping  Data models  Calibration  Adaptation models 
Abstract: What is a proper representation for objects in manipulation? What would human try to perceive when manipulating a new object in a new environment? In fact, instead of focusing on the texture and illumination, human can infer the "affordance" [36] of the objects from vision. Here "affordance" describes the object's intrinsic property that affords a particular type of manipulation. In this work, we investigate whether such affordance can be learned by a deep neural network. In particular, we propose an Affordance Space Perception Network (ASPN) that takes an image as input and outputs an affordance map. Different from existing works that infer the pixel-wise probability affordance map in image space, our affordance is defined in the real world space, thus eliminates the need of hand-eye calibration. In addition, we extend the representation ability of affordance by defining it in a 3D affordance space and propose a novel training strategy to improve the performance. Trained purely with simulation data, ASPN can achieve significant performance in the real world. It is a task-agnostic framework and can handle different objects, scenes and viewpoints. Extensive real-world experiments demonstrate the accuracy and robustness of our approach. We achieve the success rates of 94.2% for singular-object pushing and 92.4% for multiple-object pushing. We also achieve the success rates of 97.2% for singular-object grasping and 95.4% for multiple-object grasping, which outperform current state-of-the-art methods.


Title: OpenVINS: A Research Platform for Visual-Inertial Estimation
Key Words: calibration  cameras  estimation theory  image filtering  Kalman filters  robot vision  SLAM (robots)  research platform  visual-inertial estimation research  open sourced codebase  visual-inertial systems  visual-inertial estimation features  on-manifold sliding window Kalman filter  consistent First-Estimates Jacobian treatments  modular type system  extendable visual-inertial system simulator  competing estimation performance  OpenVINS  online camera intrinsic calibration  open sourced algorithms  online camera extrinsic calibration  inertial sensor time offset calibration  SLAM landmarks  state management  Cameras  Current measurement  Jacobian matrices  Calibration  Documentation  Estimation  Robot sensing systems 
Abstract: In this paper, we present an open platform, termed OpenVINS, for visual-inertial estimation research for both the academic community and practitioners from industry. The open sourced codebase provides a foundation for researchers and engineers to quickly start developing new capabilities for their visual-inertial systems. This codebase has out of the box support for commonly desired visual-inertial estimation features, which include: (i) on-manifold sliding window Kalman filter, (ii) online camera intrinsic and extrinsic calibration, (iii) camera to inertial sensor time offset calibration, (iv) SLAM landmarks with different representations and consistent First-Estimates Jacobian (FEJ) treatments, (v) modular type system for state management, (vi) extendable visual-inertial system simulator, and (vii) extensive toolbox for algorithm evaluation. Moreover, we have also focused on detailed documentation and theoretical derivations to support rapid development and research, which are greatly lacked in the current open sourced algorithms. Finally, we perform comprehensive validation of the proposed OpenVINS against state-of-the-art open sourced algorithms, showing its competing estimation performance.


Title: Decentralized Collaborative State Estimation for Aided Inertial Navigation
Key Words: communication complexity  inertial navigation  Kalman filters  nonlinear filters  position measurement  state estimation  communication links  versatile filter formulation  independent state estimation  relative position measurements  aided inertial navigation  Q-ESEKF  IMU propagation  communication complexity  decentralized collaborative state estimation  quaternion-based error-state extended Kalman filter  CSE concept  probabilistic reinitialization  prominent benchmark datasets  Cameras  Sensors  Collaboration  State estimation  Three-dimensional displays  Calibration  Robots 
Abstract: In this paper, we present a Quaternion-based Error-State Extended Kalman Filter (Q-ESEKF) based on IMU propagation with an extension for Collaborative State Estimation (CSE) and a communication complexity of O(1) (in terms of required communication links). Our approach combines a versatile filter formulation with the concept of CSE, allowing independent state estimation on each of the agents and at the same time leveraging and statistically maintaining interdependencies between agents, after joint measurements and communication (i.e. relative position measurements) occur. We discuss the development of the overall framework and the probabilistic (re-)initialization of the agent's states upon initial or recurring joint observations. Our approach is evaluated in a simulation framework on two prominent benchmark datasets in 3D.


Title: Analytic Combined IMU Integration (ACI2) For Visual Inertial Navigation
Key Words: calibration  inertial navigation  maximum likelihood estimation  Monte Carlo methods  optimisation  robot vision  sensor fusion  SLAM (robots)  analytic combined IMU integration  visual inertial navigation  batch optimization  visual sensor fusion  robotic tasks  maximum likelihood estimation  partial-fixed estimates  ACI2  inertial measurement unit  Monte-Carlo simulations  Optimization  Jacobian matrices  Maximum likelihood estimation  Time measurement  Cameras  Visualization  Calibration 
Abstract: Batch optimization based inertial measurement unit (IMU) and visual sensor fusion enables high rate localization for many robotic tasks. However, it remains a challenge to ensure that the batch optimization is computationally efficient while being consistent for high rate IMU measurements without marginalization. In this paper, we derive inspiration from maximum likelihood estimation with partial-fixed estimates to provide a unified approach for handing both IMU preintegration and time-offset calibration. We present a modularized analytic combined IMU integrator (ACI2) with elegant derivations for IMU integrations, bias Jabcobians and related covariances. To simplify our derivation, we also prove that the right Jacobians for Hamilton quaterions and SO(3) are equivalent. Finally, we present a time offset calibrator that operates by fixing the linearization point for a given time offset. This reduces re-integration of the IMU measurements and thus improve efficiency. The proposed ACI2 and time-offset calibration is verified by intensive Monte-Carlo simulations generated from real world datasets. A proof-of-concept real world experiment is also conducted to verify the proposed ACI2 estimator.


Title: Second-order Kinematics for Floating-base Robots using the Redundant Acceleration Feedback of an Artificial Sensory Skin
Key Words: calibration  estimation theory  feedback  humanoid robots  Kalman filters  manipulator kinematics  motion control  redundant manipulators  floating-base robots  redundant acceleration feedback  artificial sensory skin  estimation method  second-order kinematics  highly redundant distributed inertial feedback  linear acceleration  robot link  skin acceleration data  link level  state dimensionality reduction  main inertial measurement unit  Sigma-point Kalman filter  joint velocities  REEM-C humanoid robot  Acceleration  Robot sensing systems  Skin  Gyroscopes  Accelerometers  Acceleration Feedback  Artificial Robot Skin  Sigma-point Kalman Filter 
Abstract: In this work, we propose a new estimation method for second-order kinematics for floating-base robots, based on highly redundant distributed inertial feedback. The linear acceleration of each robot link is measured at multiple points using a multimodal, self-configuring and self-calibrating artificial skin. The proposed algorithm is two-fold: i) the skin acceleration data is fused at the link level for state dimensionality reduction; ii) the estimated values are then fused limb-wise with data from the joint encoders and the main inertial measurement unit (IMU), using a Sigma-point Kalman filter. In this manner, it is possible to estimate the joint velocities and accelerations while avoiding the lag and noise amplification phenomena associated with conventional numerical derivation approaches. Experiments performed on the right arm and torso of a REEM-C humanoid robot, demonstrate the consistency of the proposed estimation method.


Title: Clock-based time sync hronization for an event-based camera dataset acquisition platform *
Key Words: cameras  data acquisition  image sensors  microcontrollers  mobile robots  optical radar  robot vision  synchronisation  monocular camera  mobile robotic platform  time synchronization architecture  time synchronization approach  accurate time synchronization  event-based camera dataset acquisition platform  dynamic visual sensor  next-generation vision sensor  event-based vision  dataset creation  temporal accuracy  high temporal resolution  evaluation task  visual data  event camera  ambient environment sensors  data acquisition  clock-based time synchronization  LIDAR  PIC32 microcontroller  Synchronization  Robot sensing systems  Cameras  Laser radar  Clocks  Voltage control  Hardware 
Abstract: The Dynamic Visual Sensor is considered to be a next-generation vision sensor. Since event-based vision is in its early stage of development, a small number of datasets has been created during the last decade. Dataset creation is motivated by the need for real data from one or many sensors. Temporal accuracy of data in such datasets is crucially important since the events have high temporal resolution measured in microseconds and, during an algorithm evaluation task, such type of visual data is usually fused with data from other types of sensors. The main aim of our research is to achieve the most accurate possible time synchronization between an event camera, LIDAR, and ambient environment sensors during a session of data acquisition. All the mentioned sensors as well as a stereo and a monocular camera were installed on a mobile robotic platform. In this work, a time synchronization architecture and algorithm are proposed for time synchronization with an implementation example on a PIC32 microcontroller. The overall time synchronization approach is scalable for other sensors where there is a need for accurate time synchronization between many nodes. The evaluation results of the proposed solution are reported and discussed in the paper.


Title: Model Predictive Impedance Control
Key Words: human-robot interaction  predictive control  collaborative robotics  high performance control  model predictive impedance control  human robot compliant interactions  Robots  Integrated circuit modeling  Impedance  Mathematical model  Task analysis  Collaboration  Impedance control  collaborative robotics  physical human-robot interaction 
Abstract: Robots are more and more often designed in order to perform tasks in synergy with human operators. In this context, a current research focus for collaborative robotics lies in the design of high-performance control solutions, which ensure security in spite of unmodeled external forces. The present work provides a method based on Model Predictive Control (MPC) to allow compliant behavior when interacting with an environment, while respecting practical robotic constraints. The study shows in particular how to define the impedance control problem as a MPC problem. The approach is validated with an experimental setup including a collaborative robot. The obtained results emphasize the ability of this control strategy to solve constraints like speed, energy or jerk limits, which have a direct impact on the operator's security during human-robot compliant interactions.


Title: Kinematic Modeling and Compliance Modulation of Redundant Manipulators Under Bracing Constraints
Key Words: actuators  biomechanics  design engineering  dexterous manipulators  end effectors  manipulator kinematics  motion control  redundant manipulators  kinematic modeling  compliance modulation  redundant manipulators  bracing constraints  low torque actuators  passive safety reasons  human operator  in-situ collaborative robots  conflicting demands  low torque actuation  deep confined spaces  constrained kinematics  endeffector compliance  redundancy resolution framework  directional compliance  end-effector dexterity  kinematic simulation results  redundancy resolution strategy  kinematic conditioning  bracing task  admittance control framework  collaborative control  ISCR  Kinematics  Robots  Redundancy  Task analysis  Collaboration  Torque  Safety  Bracing  redundancy resolution  stiffness modulation  compliance  collaborative robots 
Abstract: Collaborative robots should ideally use low torque actuators for passive safety reasons. However, some applications require these collaborative robots to reach deep into confined spaces while assisting a human operator in physically demanding tasks. In this paper, we consider the use of in-situ collaborative robots (ISCRs) that balance the conflicting demands of passive safety dictating low torque actuation and the need to reach into deep confined spaces. We consider the judicious use of bracing as a possible solution to these conflicting demands and present a modeling framework that takes into account the constrained kinematics and the effect of bracing on the endeffector compliance. We then define a redundancy resolution framework that minimizes the directional compliance of the end-effector while maximizing end-effector dexterity. Kinematic simulation results show that the redundancy resolution strategy successfully decreases compliance and improves kinematic conditioning while satisfying the constraints imposed by the bracing task. Applications of this modeling framework can support future research on the choice of bracing locations and support the formation of an admittance control framework for collaborative control of ISCRs under bracing constraints. Such robots can benefit workers in the future by reducing the physiological burdens that contribute to musculoskeletal injury.


Title: Successive Stiffness Increment and Time Domain Passivity Approach for Stable and High Bandwidth Control of Series Elastic Actuator
Key Words: actuators  elasticity  flexible manipulators  force control  haptic interfaces  human-robot interaction  impact (mechanical)  stability  telerobotics  time domain passivity approach  bandwidth control  human-robot interaction  flexible manipulators  series elastic actuator based manipulators  elastic element  system durability  actuators  SEA manipulator  system bandwidth  impedance control  system stability  successive stiffness increment approach  haptic domain  TDPA  system passivity  impact force  teleoperation  two-port electrical circuit network  size 350.0 m  size 120.0 m  Impedance  Bandwidth  Force  Actuators  Torque  Manipulators  Springs 
Abstract: For safe human-robot interaction, various type of flexible manipulators have been developed. Especially series elastic actuator (SEA) based manipulators have been getting huge attention since the elastic element of SEA prevents people from injury when undesirable collision happens. Moreover, it improves system durability by absorbing impact force, which could damage actuators. However, the elastic element inside SEA manipulator causes low system bandwidth which limits the speed performance of conventional impedance control approaches. To alleviate the low bandwidth issue of impedance controlled SEA while guaranteeing system stability, we implement Time Domain Passivity Approach (TDPA) and Successive Stiffness Increment (SSI) approach, which was invented in haptic and teleoperation domain. Impedance controlled SEA is reformulated as a two-port electrical circuit network for implementing TDPA. In addition, a pair of input and output power conjugate variable, dominating the system passivity is identified for implementing SSI approach. Experimental results showed that TDPA and SSI approach can render the stiffness of the impedance controller, which decides the bandwidth, upto 350 kN/m without any stability issue, while normal impedance controller only render upto 120 kN/m. Although both of the approaches significantly increased the bandwidth of the impedance controlled SEA, TDPA slightly outperformed in stability, and SSI outperformed in tracking.


Title: Arm-hand motion-force coordination for physical interactions with non-flat surfaces using dynamical systems: Toward compliant robotic massage
Key Words: biomechanics  dexterous manipulators  force control  motion control  path planning  regression analysis  support vector machines  unified motion-force control approach  human limb  compliant robotic massage  dynamical system approach  skin surface  robot fingers  complexity increases  manipulation tasks  dynamical systems  nonflat surface  physical interactions  arm-hand motion-force coordination  desired motion patterns  unknown surface  mannequin arm  Allegro robotic hand  KUKA IIWA robotic arm  robotic fingers  DS-based impedance control  desired motions  distance-to-surface mapping  Surface impedance  Robot kinematics  Task analysis  Force  Manipulators  Thumb 
Abstract: Many manipulation tasks require coordinated motions for arm and fingers. Complexity increases when the task requires to control for the force at contact against a non-flat surface; This becomes even more challenging when this contact is done on a human. All these challenges are regrouped when one, for instance, massages a human limb. When massaging, the robotic arm is required to continuously adapt its orientation and distance to the limb while the robot fingers exert desired patterns of forces and motion on the skin surface. To address these challenges, we adopt a Dynamical System (DS) approach that offers a unified motion-force control approach and enables to easily coordinate multiple degrees of freedom. As each human limb may slightly differ, we learn a model of the surface using support vector regression (SVR) which enable us to obtain a distance-to-surface mapping. The gradient of this mapping, along with the DS, generates the desired motions for the interaction with the surface. A DS-based impedance control for the robotic fingers allows to control separately for force along the normal direction of the surface while moving in the tangential plane. We validate our approach using the KUKA IIWA robotic arm and Allegro robotic hand for massaging a mannequin arm covered with a skin-like material. We show that our approach allows for 1) reactive motion planning to reach for an unknown surface, 2) following desired motion patterns on the surface, and 3) exerting desired interaction forces profiles. Our results show the effectiveness of our approach; especially the robustness toward uncertainties for shape and the given location of the surface.


Title: A Bio-Signal Enhanced Adaptive Impedance Controller for Lower Limb Exoskeleton
Key Words: adaptive control  biomechanics  electromyography  gait analysis  medical robotics  medical signal processing  motion control  neurocontrollers  patient rehabilitation  radial basis function networks  robot dynamics  torque control  unpredictable human body movements  complex body movements  elaborate control strategy design  uncertain dynamical parameters  human-exoskeleton interaction  lower limb exoskeleton  bio-signal enhanced adaptive impedance controller  rehabilitation lower-limb exoskeleton  exoskeleton track desired motion trajectory  radial basis function neural network enhanced adaptive impedance controller  surface electromyogram signals  neural network-based torque estimation method  joint torque  human lower extremity dynamics  human operator walking  Exoskeletons  Torque  Estimation  Muscles  Impedance  Artificial neural networks  Legged locomotion 
Abstract: The problem of human-exoskeleton interaction with uncertain dynamical parameters remains an open-ended research area. It requires an elaborate control strategy design of the exoskeleton to accommodate complex and unpredictable human body movements. In this paper, we proposed a novel control approach for the lower limb exoskeleton to realize its task of assisting the human operator walking. The main challenge of this study was to determine the human lower extremity dynamics, such as the joint torque. For this purpose, we developed a neural network-based torque estimation method. It can predict the joint torques of humans with surface electromyogram signals (sEMG). Then an radial basis function neural network (RBF NN) enhanced adaptive impedance controller is employed to ensure exoskeleton track desired motion trajectory of a human operator. Algorithm performance is evaluated with two healthy subjects and the rehabilitation lower-limb exoskeleton developed by Shenzhen Institutes of Advanced Technology (SIAT).


Title: Differentiable Mapping Networks: Learning Structured Map Representations for Sparse Visual Localization
Key Words: gradient methods  image representation  learning (artificial intelligence)  neural net architecture  particle filtering (numerical methods)  robot vision  DMN architecture  end-to-end differentiable  sparse visual localization  end-to-end learning  differentiable mapping network  spatially structured view-embedding map  subsequent visual localization  learning structured map representations  Street View dataset  particle filter  gradient descent  robotics  neural network architecture  Task analysis  Visualization  Feature extraction  Neural networks  Robot kinematics  Three-dimensional displays 
Abstract: Mapping and localization, preferably from a small number of observations, are fundamental tasks in robotics. We address these tasks by combining spatial structure (differentiable mapping) and end-to-end learning in a novel neural network architecture: the Differentiable Mapping Network (DMN). The DMN constructs a spatially structured view-embedding map and uses it for subsequent visual localization with a particle filter. Since the DMN architecture is end-to-end differentiable, we can jointly learn the map representation and localization using gradient descent. We apply the DMN to sparse visual localization, where a robot needs to localize in a new environment with respect to a small number of images from known viewpoints. We evaluate the DMN using simulated environments and a challenging real-world Street View dataset. We find that the DMN learns effective map representations for visual localization. The benefit of spatial structure increases with larger environments, more viewpoints for mapping, and when training data is scarce. Project website: https://sites.google.com/view/differentiable-mapping.


Title: Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using Video Demonstration
Key Words: convolutional neural nets  feature extraction  image representation  learning (artificial intelligence)  video signal processing  task-specific objects  intended task  imitation learning  video demonstration  end-to-end self-supervised feature representation network  video-based task imitation  multilevel spatial attention module  spatial features  weighted combination  multiple intermediate feature maps  respective feature maps  metric learning loss  multiple view points  AT-Net features  reinforcement learning problem  attentive task-net  self supervised task-attention network  neural connections  learning task-specific feature embeddings  temporally consecutive frames  publicly available multiview pouring dataset  RL agent  Gazebo simulator  CNN pipeline  Task analysis  Measurement  Robots  Feature extraction  Training  Visualization  Pipelines 
Abstract: This paper proposes an end-to-end self-supervised feature representation network named Attentive Task-Net or AT-Net for video-based task imitation. The proposed AT-Net incorporates a novel multi-level spatial attention module to highlight spatial features corresponding to the intended task demonstrated by the expert. The neural connections in AT-Net ensure the relevant information in the demonstration is amplified and the irrelevant information is suppressed while learning task-specific feature embeddings. This is achieved by a weighted combination of multiple intermediate feature maps of the input image at different stages of the CNN pipeline. The weights of the combination are given by the compatibility scores, predicted by the attention module for respective feature maps. The AT-Net is trained using a metric learning loss which aims to decrease the distance between the feature representations of concurrent frames from multiple view points and increase the distance between temporally consecutive frames. The AT-Net features are then used to formulate a reinforcement learning problem for task imitation. Through experiments on the publicly available Multi-view pouring dataset, it is demonstrated that the output of the attention module highlights the task-specific objects while suppressing the rest of the background. The efficacy of the proposed method is further validated by qualitative and quantitative comparison with a state-of-the-art technique along with intensive ablation studies. The proposed method is implemented to imitate a pouring task where an RL agent is learned with the AT-Net in Gazebo simulator. Our findings show that the AT-Net achieves 6.5% decrease in alignment error along with a reduction in the number of training iterations by almost 155k over the state-of-the-art while satisfactorily imitating the intended task.


Title: OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong Deep Learning
Key Words: control engineering computing  data visualisation  learning (artificial intelligence)  object recognition  robot vision  service robots  lifelong deep learning  visual algorithms  standard computer vision datasets  adaptive visual perceptual systems  lifelong robotic vision dataset  lifelong object recognition algorithms  lifelong learning algorithms  OpenLORIS-Object dataset  object recognition task  robotic vision  ImageNet dataset  COCO dataset  Task analysis  Lighting  Object recognition  Cameras  Robot vision systems  Clutter 
Abstract: The recent breakthroughs in computer vision have benefited from the availability of large representative datasets (e.g. ImageNet and COCO) for training. Yet, robotic vision poses unique challenges for applying visual algorithms developed from these standard computer vision datasets due to their implicit assumption over non-varying distributions for a fixed set of tasks. Fully retraining models each time a new task becomes available is infeasible due to computational, storage and sometimes privacy issues, while naïve incremental strategies have been shown to suffer from catastrophic forgetting. It is crucial for the robots to operate continuously under open-set and detrimental conditions with adaptive visual perceptual systems, where lifelong learning is a fundamental capability. However, very few datasets and benchmarks are available to evaluate and compare emerging techniques. To fill this gap, we provide a new lifelong robotic vision dataset ("OpenLORIS-Object") collected via RGB-D cameras. The dataset embeds the challenges faced by a robot in the real-life application and provides new benchmarks for validating lifelong object recognition algorithms. Moreover, we have provided a testbed of 9 state-of-the-art lifelong learning algorithms. Each of them involves 48 tasks with 4 evaluation metrics over the OpenLORIS-Object dataset. The results demonstrate that the object recognition task in the ever-changing difficulty environments is far from being solved and the bottlenecks are at the forward/backward transfer designs. Our dataset and benchmark are publicly available at https://lifelong-robotic-vision.github.io/dataset/object.


Title: Joint Rotation Angle Sensing of Flexible Endoscopic Surgical Robots
Key Words: Bragg gratings  closed loop systems  endoscopes  fibre optic sensors  manipulators  medical robotics  motion control  surgery  closed-loop motion control  robotic endoscopic grasper  robotic joint  autonomous robotic surgery  motion hysteresis  endoscopic surgical robot  joint rotation angle sensing  motion control  tendon-sheath mechanisms  Fiber Bragg Grating  Robot sensing systems  Optical fiber sensors  Substrates  Optical fiber theory  Medical robotics  Surgery  Flexible Robots  Surgical Robotics  Fiber Optics Sensor  Tendon Sheath Mechanism. 
Abstract: Accurate motion control of surgical robots is critical for the efficiency and safety of both state-of-the-art teleoperated robotic surgery and the ultimate autonomous robotic surgery. However, fine motion control for a flexible endoscopic surgical robot is highly challenging because of the shape-dependent and speed-dependent motion hysteresis of tendon-sheath mechanisms (TSMs) in the long, tortuous, and dynamically shape-changing robot body. Aiming to achieve precise closed-loop motion control, we propose a small and flexible sensor to directly sense the large and sharp rotations of the articulated joints of a flexible endoscopic surgical robot. The sensor-a Fiber Bragg Grating (FBG) eccentrically embedded in a thin and flexible epoxy substrate-can be significantly bent with a large bending angle range of [-62.9°, 75.5°] and small bending radius of 6.9 mm. Mounted in-between the two pivot-connected links of a joint, the sensor will bend once the joint is actuated, resulting in the wavelength shift of the FBG. In this study, the relationship between the wavelength shift and the rotation angle of the joint was theoretically modeled and then experimentally verified before and after the installation of the sensor in a robotic endoscopic grasper. The sensor, with the calibrated model, can track the rotation of the robotic joint with an RMSE of 3.34°. This small and flexible sensor has good repeatability, high sensitivity (around 147.5 pm/degree), and low hysteresis (7.72%). It is suitable for surgical robots and manipulators whose articulated joints have a large rotation angle and small bending radius.


Title: Soft, Round, High Resolution Tactile Fingertip Sensors for Dexterous Robotic Manipulation
Key Words: dexterous manipulators  geometry  tactile sensors  tactile fingertip sensors  dexterous robotic manipulation  dexterous multifingered hands  illumination geometry  dexterous manipulation tasks  Three-dimensional displays  Robot sensing systems  Plastics  Geometry  Light emitting diodes  Cameras 
Abstract: High resolution tactile sensors are often bulky and have shape profiles that make them awkward for use in manipulation. This becomes important when using such sensors as fingertips for dexterous multi-fingered hands, where boxy or planar fingertips limit the available set of smooth manipulation strategies. High resolution optical based sensors such as GelSight have until now been constrained to relatively flat geometries due to constraints on illumination geometry. Here, we show how to construct a rounded fingertip that utilizes a form of light piping for directional illumination. Our sensors can replace the standard rounded fingertips of the Allegro hand. They can capture high resolution maps of the contact surfaces, and can be used to support various dexterous manipulation tasks.


Title: FootTile: a Rugged Foot Sensor for Force and Center of Pressure Sensing in Soft Terrain
Key Words: biomechanics  force sensors  gait analysis  legged locomotion  pressure sensors  FootTile  rugged foot sensor  pressure sensing  soft terrain  sensor design  standard biomechanical devices  pressure plates  pressure distribution  ground reaction force estimation  sensing capabilities  waterproof sensor  reaction force  force plates  rough terrain  legged locomotion  granular substrate  liquid mud  mass 0.9 g  frequency 330.0 Hz  Robot sensing systems  Force  Legged locomotion  Foot  Sensor arrays  Force sensors 
Abstract: In this paper, we present FootTile, a foot sensor for reaction force and center of pressure sensing in challenging terrain. We compare our sensor design to standard biomechanical devices, force plates and pressure plates. We show that FootTile can accurately estimate force and pressure distribution during legged locomotion. FootTile weighs 0.9 g, has a sampling rate of 330 Hz, a footprint of 10×10 mm and can easily be adapted in sensor range to the required load case. In three experiments, we validate: first, the performance of the individual sensor, second an array of FootTiles for center of pressure sensing and third the ground reaction force estimation during locomotion in granular substrate. We then go on to show the accurate sensing capabilities of the waterproof sensor in liquid mud, as a showcase for real world rough terrain use.


Title: Learning a Control Policy for Fall Prevention on an Assistive Walking Device
Key Words: biomechanics  gait analysis  handicapped aids  humanoid robots  legged locomotion  assistive walking device  fall prevention  control policy  augmented assistive device  models realistic human gait  robust human walking policy  actuators  onboard sensors  recovery policy  fall predictor  Legged locomotion  Perturbation methods  Assistive devices  Sensors  Adaptation models  Biological system modeling 
Abstract: Fall prevention is one of the most important components in senior care. We present a technique to augment an assistive walking device with the ability to prevent falls. Given an existing walking device, our method develops a fall predictor and a recovery policy by utilizing the onboard sensors and actuators. The key component of our method is a robust human walking policy that models realistic human gait under a moderate level of perturbations. We use this human walking policy to provide training data for the fall predictor, as well as to teach the recovery policy how to best modify the person's gait when a fall is imminent. Our evaluation shows that the human walking policy generates walking sequences similar to those reported in biomechanics literature. Our experiments in simulation show that the augmented assistive device can indeed help recover balance from a variety of external perturbations. We also provide a quantitative method to evaluate the design choices for an assistive device.


Title: Soft Pneumatic System for Interface Pressure Regulation and Automated Hands-Free Donning in Robotic Prostheses
Key Words: human-robot interaction  medical robotics  pneumatic systems  pressure control  prosthetics  real-time systems  wearable robots  human-socket interface  wearable device  synthetic forearm model  real time pressure regulation  automated hands free donning  automated underactuated donning mechanism  soft pneumatic socket  robotic prostheses  interface pressure regulation  Sockets  Bladder  Valves  Fingers  Prosthetics  Laser beam cutting 
Abstract: This paper discusses the design and preliminary evaluation of a soft pneumatic socket (SPS) with real-time pressure regulation and an automated underactuated donning mechanism (UDM). The ability to modulate the pressure at the human-socket interface of a prosthesis or wearable device to accommodate user's activities has the potential to make the user more comfortable. Furthermore, a hands-free, underactuated donning mechanism designed to reliably and safely don the socket onto the user may increase the convenience of prostheses and wearable devices. The pneumatic socket and donning mechanism are evaluated on synthetic forearm model designed to closely match the mechanical properties of the human forearm. The pneumatic socket was tested to determine the maximum loads it can withstand before slipping and the displacement of the socket after loading. The donning mechanism was able to successfully don the socket on to the replica forearm with a 100% success rate for the 30 trials that were tested. Both devices were also tested to determine the pressures they impart on the user. The highest pressures the socket can impart on the user is 4psi and the maximum pressure the donning mechanism imparts on the user is 0.83psi. These pressures were found to be lower than the reported pressures that cause pain and tissue damage.


Title: Automated detection of soleus concentric contraction in variable gait conditions for improved exosuit control
Key Words: gait analysis  image segmentation  image sequences  medical robotics  muscle  patient rehabilitation  automated routine  normalized gait cycle  real-time rates  gait cycle  manual estimation  healthy individuals  persons post-stroke walking  comfortable walking speed  onset timing  automated detection  soleus concentric contraction  variable gait conditions  individualized assistance  changing gait  exosuit control strategy  muscle power  soleus muscle  positive power  low-profile ultrasound system  walking individuals  biological mechanisms  frequency 130.0 Hz  Muscles  Legged locomotion  Kinematics  Tendons  Real-time systems  Electromyography 
Abstract: Exosuits can reduce metabolic demand and improve gait. Controllers explicitly derived from biological mechanisms that reflect the user's joint or muscle dynamics should in theory allow for individualized assistance and enable adaptation to changing gait. With the goal of developing an exosuit control strategy based on muscle power, we present an approach for estimating, at real time rates, when the soleus muscle begins to generate positive power. A low-profile ultrasound system recorded B-mode images of the soleus in walking individuals. An automated routine using optical flow segmented the data to a normalized gait cycle and estimated the onset of concentric contraction at real-time rates (~130Hz). Segmentation error was within 1% of the gait cycle compared to using ground reaction forces. Estimation of onset of concentric contraction had a high correlation (R2=0.92) and an RMSE of 2.6% gait cycle relative to manual estimation. We demonstrated the ability to estimate the onset of concentric contraction during fixed speed walking in healthy individuals that ranged from 39.3% to 45.8% of the gait cycle and feasibility in two persons post-stroke walking at comfortable walking speed. We also showed the ability to measure a shift in onset timing to 7% earlier when the biological system adapts from level to incline walking. Finally, we provided an initial evaluation for how the onset of concentric contraction might be used to inform exosuit control in level and incline walking.


Title: Soft Sensing Shirt for Shoulder Kinematics Estimation
Key Words: biomechanics  biomedical measurement  capacitive sensors  coaxial cables  inertial systems  kinematics  mean square error methods  motion measurement  patient monitoring  patient rehabilitation  readout electronics  regression analysis  strain sensors  wearable motion tracking  ground truth optical motion capture system  strain sensor data  joint angle estimation  normalized root mean square errors  joint velocity estimation  recursive feature elimination-based sensor selection analysis  shoulder kinematics estimation  soft strain sensors  unobtrusive approach  noncyclic joint movements  cyclic arm movements  random arm movements  shoulder joint  customized readout electronics board  sewn microcoaxial cables  textile-based capacitive strain sensors  multidegree-of-freedom noncyclic joint movements  Tracking  Shoulder  Electrodes  Capacitive sensors  Robot sensing systems  Strain 
Abstract: Soft strain sensors have been explored as an unobtrusive approach for wearable motion tracking. However, accurate tracking of multi degree-of-freedom (DOF) noncyclic joint movements remains a challenge. This paper presents a soft sensing shirt for tracking shoulder kinematics of both cyclic and random arm movements in 3 DOFs: adduction/abduction, horizontal flexion/extension, and internal/external rotation. The sensing shirt consists of 8 textile-based capacitive strain sensors sewn around the shoulder joint that communicate to a customized readout electronics board through sewn micro-coaxial cables. An optimized sensor design includes passive shielding and demonstrates high linearity and low hysteresis, making it suitable for wearable motion tracking. In a study with a single human subject, we evaluated the tracking capability of the integrated shirt in comparison with a ground truth optical motion capture system. An ensemble-based regression algorithm was implemented in post-processing to estimate joint angles and angular velocities from the strain sensor data. Results demonstrated root mean square errors (RMSEs) less than 4.5° for joint angle estimation and normalized root mean square errors (NRMSEs) less than 4% for joint velocity estimation. Furthermore, we applied a recursive feature elimination (RFE)-based sensor selection analysis to down select the number of sensors for future shirt designs. This sensor selection analysis found that 5 sensors out of 8 were sufficient to generate comparable accuracies.


Title: Motion Reasoning for Goal-Based Imitation Learning
Key Words: inference mechanisms  learning (artificial intelligence)  mobile robots  path planning  robot vision  video signal processing  goal-based imitation learning  third-person video demonstration  human demonstrators  motion reasoning  motion planning  Task analysis  Trajectory  Cognition  Planning  Motion segmentation  Human-robot interaction 
Abstract: We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to recognize the goals that cannot be disambiguated by previous action-based approaches. We evaluate our approach on a new dataset of 96 video demonstrations in a mockup kitchen environment. We show that our motion reasoning plays an important role in recognizing the actual goal of the demonstrator and improves the success rate by over 20%. We further show that by using the automatically inferred goal from the video demonstration, our robot is able to reproduce the same task in a real kitchen environment.


Title: Flexible online adaptation of learning strategy using EEG-based reinforcement signals in real-world robotic applications
Key Words: electroencephalography  human-robot interaction  learning (artificial intelligence)  medical robotics  medical signal processing  learned policy  learning phases  current control strategy  robot learning  intrinsic human feedback  human-robot interaction  intrinsic interactive reinforcement learning approach  human-robot collaboration  flexible adaptation  real-world robotic applications  reinforcement signals  flexible online adaptation  learning progress  Electroencephalography  Decoding  Electronic learning  Task analysis  Human-robot interaction  Robot learning 
Abstract: Flexible adaptation of learning strategy depending on online changes of the user's current intents have a high relevance in human-robot collaboration. In our previous study, we proposed an intrinsic interactive reinforcement learning approach for human-robot interaction, in which a robot learns his/her action strategy based on intrinsic human feedback that is generated in the human's brain as neural signature of the human's implicit evaluation of the robot's actions. Our approach has an inherent property that allows robots to adapt their behavior depending on online changes of the human's current intents. Such flexible adaptation is possible, since robot learning is updated in real time by human's online feedback. In this paper, the adaptivity of robot learning is tested on eight subjects who change their current control strategy by adding a new gesture to the previous used gestures. This paper evaluates the learning progress by analyzing learning phases (before and after adding a new gesture for control). The results show that the robot can adapt the previously learned policy depending on online changes of the user's intents. Especially, learning progress is interrelated with the classification performance of electroencephalograms (EEGs), which are used to measure the human's implicit evaluation of the robot's actions.


Title: Object-oriented Semantic Graph Based Natural Question Generation
Key Words: convolutional neural nets  feature extraction  graph theory  learning (artificial intelligence)  natural language processing  object detection  recurrent neural nets  robot vision  sequential scenes  recurrent neural network  feature extraction  autonomous robots  graph convolutional network  object-oriented semantic graphs  semantic graph mapping  natural question generation  Semantics  Feature extraction  Object oriented modeling  Neural networks  Convolution  Autonomous robots 
Abstract: Generating a natural question can enable autonomous robots to propose problems according to their surroundings. However, recent studies on question generation rarely consider semantic graph mapping, which is widely used to understand environments. In this paper, we introduce a method to generate natural questions using object-oriented semantic graphs. First, a graph convolutional network extracts features from the graph. Then, a recurrent neural network generates the natural question from the extracted features. Using graphs, we can generate natural questions for both single and sequential scenes. The proposed method outperforms conventional methods on a publicly available dataset for single scenes and can generate questions for sequential scenes.


Title: Towards Safe Human-Robot Collaboration Using Deep Reinforcement Learning
Key Words: hazards  human-robot interaction  industrial robots  learning (artificial intelligence)  mobile robots  neural nets  occupational safety  risk management  hazard source  safety engineers  risk assessment processes  deep RL agents  human-robot collaboration  HRC-productivity  deep reinforcement learning  systematic methodology  core components  Task analysis  Training  Hazards  Robot sensing systems  Service robots 
Abstract: Safety in Human-Robot Collaboration (HRC) is a bottleneck to HRC-productivity in industry. With robots being the main source of hazards, safety engineers use over-emphasized safety measures, and carry out lengthy and expensive risk assessment processes on each HRC-layout reconfiguration. Recent advances in deep Reinforcement Learning (RL) offer solutions to add intelligence and comprehensibility of the environment to robots. In this paper, we propose a framework that uses deep RL as an enabling technology to enhance intelligence and safety of the robots in HRC scenarios and, thus, reduce hazards incurred by the robots. The framework offers a systematic methodology to encode the task and safety requirements and context of applicability into RL settings. The framework also considers core components, such as behavior explainer and verifier, which aim for transferring learned behaviors from research labs to industry. In the evaluations, the proposed framework shows the capability of deep RL agents learning collision-free point-to-point motion on different robots inside simulation, as shown in the supplementary video.


Title: Deep compositional robotic planners that follow natural language commands
Key Words: convolutional neural nets  mobile robots  natural language processing  path planning  deep compositional robotic planners  natural language commands  sampling-based robotic planner  continuous configuration space  complex command  sampling-based planner  recurrent hierarchical deep network  Robots  Task analysis  Planning  Natural languages  Aerospace electronics  Cognition  Space exploration 
Abstract: We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments.


Title: Learning User Preferences from Corrections on State Lattices
Key Words: human-robot interaction  learning (artificial intelligence)  learning systems  mobile robots  path planning  robot programming  state lattices  autonomous mobile robots  motion planning problem  robot traffic  motion features  learned user preferences  learning from corrections  algorithm completeness proving  human robot interaction  Task analysis  Lattices  Cost function  Mobile robots  Robot motion  Learning (artificial intelligence) 
Abstract: Enabling a broader range of users to efficiently deploy autonomous mobile robots requires intuitive frameworks for specifying a robot's task and behaviour. We present a novel approach using learning from corrections (LfC), where a user is iteratively presented with a solution to a motion planning problem. Users might have preferences about parts of a robot's environment that are suitable for robot traffic or that should be avoided as well as preferences on the control actions a robot can take. The robot is initially unaware of these preferences; thus, we ask the user to provide a correction to the presented path. We assume that the user evaluates paths based on environment and motion features. From a sequence of corrections we learn weights for these features, which are then considered by the motion planner, resulting in future paths that better fit the user's preferences. We prove completeness of our algorithm and demonstrate its performance in simulations. Thereby, we show that the learned preferences yield good results not only for a set of training tasks but also for test tasks, as well as for different types of user behaviour.


Title: Visual Servoing-based Navigation for Monitoring Row-Crop Fields
Key Words: agricultural robots  agriculture  agrochemicals  crops  mobile robots  path planning  robot vision  visual servoing  visual servoing-based navigation  autonomous navigation  field robots  precision agriculture tasks  agrochemicals  visual-based navigation framework  crop-row structure  row-crop fields monitoring  Agriculture  Navigation  Cameras  Robot vision systems  Visualization  Monitoring 
Abstract: Autonomous navigation is a pre-requisite for field robots to carry out precision agriculture tasks. Typically, a robot has to navigate along a crop field multiple times during a season for monitoring the plants, for applying agrochemicals, or for performing targeted interventions. In this paper, we propose a visual-based navigation framework tailored to row-crop fields that exploits the regular crop-row structure present in fields. Our approach uses only the images from on-board cameras without the need for performing explicit localization or maintaining a map of the field. Thus, it can operate without expensive RTK-GPS solutions often used in agricultural automation systems. Our navigation approach allows the robot to follow the crop rows accurately and handles the switch to the next row seamlessly within the same framework. We implemented our approach using C++ and ROS and thoroughly tested it in several simulated fields with different shapes and sizes. We also demonstrated the system running at frame-rate on an actual robot operating on a test row-crop field. The code and data have been published.


Title: Optimal Routing Schedules for Robots Operating in Aisle-Structures
Key Words: computational complexity  graph theory  mobile robots  optimisation  scheduling  COP-FR  computational complexity  optimal solutions  highly unbalanced rewards  optimal routing schedules  aisle-structures  constant-cost orienteering problem  travel budget  aisle-graph  loosely connected rows  Robots  Optimized production technology  Routing  Heuristic algorithms  Task analysis  Irrigation  Automation 
Abstract: In this paper, we consider the Constant-cost Orienteering Problem (COP) where a robot, constrained by a limited travel budget, aims at selecting a path with the largest reward in an aisle-graph. The aisle-graph consists of a set of loosely connected rows where the robot can change lane only at either end, but not in the middle. Even when considering this special type of graphs, the orienteering problem is known to be intractable. We optimally solve in polynomial time two special cases, COP-FR where the robot can only traverse full rows, and COP-SC where the robot can access the rows only from one side. To solve the general COP, we then apply our special case algorithms as well as a new heuristic that suitably combines them. Despite its light computational complexity and being confined into a very limited class of paths, the optimal solutions for COP-FR turn out to be competitive in terms of achieved rewards even for COP. This is shown by means of extended simulations performed on both real and synthetic scenarios. Furthermore, our new heuristic for the general case outperforms state-of-art algorithms, especially for input with highly unbalanced rewards.


Title: Time Optimal Motion Planning with ZMP Stability Constraint for Timber Manipulation
Key Words: humanoid robots  legged locomotion  mobile robots  motion control  optimisation  path planning  robot kinematics  stability  timber  time optimal motion planning  ZMP stability constraint  timber manipulation  dynamic stability-constrained optimal motion  timber harvesting machine  rough terrain  kinematics model  optimization problem  computation time  motion plan  dynamic stability constraint  zero moment point stability measure  Planning  Kinematics  Manipulator dynamics  Acceleration  Stability analysis  Computational modeling 
Abstract: This paper presents a dynamic stability-constrained optimal motion planning algorithm developed for a timber harvesting machine working on rough terrain. First, the kinematics model of the machine, and the Zero Moment Point (ZMP) stability measure is presented. Then, an approach to simplify the model to gain insight and achieve a fast solution of the optimization problem is introduced. The performance and computation time of the motion plan obtained with the simplified model is compared against that obtained with the full kinematics model of the machine with the help of MATLAB simulations. The results demonstrate feasibility of fast motion planning while satisfying the dynamic stability constraint.


Title: Push and Drag: An Active Obstacle Separation Method for Fruit Harvesting Robots
Key Words: agricultural robots  collision avoidance  control engineering computing  grippers  image colour analysis  image segmentation  industrial robots  mobile robots  object detection  robot vision  point cloud operation  deep learning  object detection  color thresholding  image processing  active obstacle separation  linear motions  zig-zag push  trajectory  separation motion  drag motions  obstacle avoidance  target fruit  fruit harvesting robots  Grippers  Robots  Three-dimensional displays  Drag  Trajectory  Force  Radio frequency 
Abstract: Selectively picking a target fruit surrounded by obstacles is one of the major challenges for fruit harvesting robots. Different from traditional obstacle avoidance methods, this paper presents an active obstacle separation strategy that combines push and drag motions. The separation motion and trajectory are generated based on the 3D visual perception of the obstacle information around the target. A linear push is used to clear the obstacles from the area below the target, while a zig-zag push that contains several linear motions is proposed to push aside more dense obstacles. The zig-zag push can generate multi-directional pushes and the side-to-side motion can break the static contact force between the target and obstacles, thus helping the gripper to receive a target in more complex situations. Moreover, we propose a novel drag operation to address the issue of mis-capturing obstacles located above the target, in which the gripper drags the target to a place with fewer obstacles and then pushes back to move the obstacles aside for further detachment. Furthermore, an image processing pipeline consisting of color thresholding, object detection using deep learning and point cloud operation, is developed to implement the proposed method on a harvesting robot. Field tests show that the proposed method can improve the picking performance substantially. This method helps to enable complex clusters of fruits to be harvested with a higher success rate than conventional methods.


Title: Online Camera-LiDAR Calibration with Sensor Semantic Information
Key Words: calibration  cameras  computer vision  image colour analysis  image motion analysis  optical radar  optimisation  sensor fusion  RGB camera  light detection and ranging sensor  autonomous vehicles  outdoor environment  online calibration technique  optimal rigid motion transformation  mutual information  perceived data  optimization problem  semantic features  temporally synchronized camera  autonomous driving tasks  online camera-LiDAR calibration  sensor semantic information  sensor data fusion  sensor calibration  complex settings  suboptimal results  extrinsic calibration  edge feature based auto-calibration  cutting-edge machine vision  calibration quality metric  LiDAR sensor  Calibration  Cameras  Laser radar  Image edge detection  Robot sensing systems  Semantics  Robustness 
Abstract: As a crucial step of sensor data fusion, sensor calibration plays a vital role in many cutting-edge machine vision applications, such as autonomous vehicles and AR/VR. Existing techniques either require quite amount of manual work and complex settings, or are unrobust and prone to produce suboptimal results. In this paper, we investigate the extrinsic calibration of an RGB camera and a light detection and ranging (LiDAR) sensor, which are two of the most widely used sensors in autonomous vehicles for perceiving the outdoor environment. Specifically, we introduce an online calibration technique that automatically computes the optimal rigid motion transformation between the aforementioned two sensors and maximizes their mutual information of perceived data, without the need of tuning environment settings. By formulating the calibration as an optimization problem with a novel calibration quality metric based on semantic features, we successfully and robustly align pairs of temporally synchronized camera and LiDAR frames in real time. Demonstrated on several autonomous driving tasks, our method outperforms state-of-the-art edge feature based auto-calibration approaches in terms of robustness and accuracy.


Title: Precise 3D Calibration of Wafer Handling Robot by Visual Detection and Tracking of Elliptic-shape Wafers
Key Words: calibration  cameras  feature extraction  Gaussian processes  image reconstruction  image registration  image segmentation  industrial robots  optimisation  pose estimation  production engineering computing  robot kinematics  robot vision  semiconductor device manufacture  semiconductor technology  precise 3D calibration  visual detection  elliptic-shape wafers  3D poses  robot kinematics  robust ellipse detection  tracking algorithm  calibration parameters  robot-camera system  Three-dimensional displays  Image segmentation  Robots  Optimization  Image edge detection  Calibration  Cameras 
Abstract: This work provides a framework for the 3D calibration of wafers and a wafer handling robot by monocular vision. The proposed method precisely reconstructs the 3D poses of wafers from a set of images captured by the camera mounted on the robot. In addition, it calibrates the robot kinematics simultaneously. A robust ellipse detection and tracking algorithm based on the edge arcs is developed to recognize wafers among images. Then a joint optimization is constructed from a multi-object pose graph to solve the 3D poses of wafers and other calibration parameters of the robot-camera system. The proposed tracking method is able to associate multiple incomplete elliptic segments using a Gaussian Mixture Model-based registration algorithm. The algorithm is point-based where no feature descriptor is required. The proposed 3D pose optimization incorporates shape constraints, and is more accurate than the point-wise reconstruction produced by classic bundle adjustment methods.


Title: Globally Optimal Relative Pose Estimation for Camera on a Selfie Stick
Key Words: calibration  motion estimation  optimisation  pose estimation  tree searching  selfie stick  video selfie  short continuous video clip  selfie photos  camera motion  fast branch-and-bound global optimization  globally optimal relative camera pose estimation  spherical joint motion  3-DoF search problem  Cameras  Calibration  Pose estimation  Robustness  Geometry  Transforms  Robot vision systems 
Abstract: Taking selfies has become a photographic trend nowadays. We envision the emergence of the "video selfie" capturing a short continuous video clip (or burst photography) of the user, themselves. A selfie stick is usually used, whereby a camera is mounted on a stick for taking selfie photos. In this scenario, we observe that the camera typically goes through a special trajectory along a sphere surface. Motivated by this observation, in this work, we propose an efficient and globally optimal relative camera pose estimation between a pair of two images captured by a camera mounted on a selfie stick. We exploit the special geometric structure of the camera motion constrained by a selfie stick and define its motion as spherical joint motion. By the new parametrization and calibration scheme, we show that the pose estimation problem can be reduced to a 3-DoF (degrees of freedom) search problem, instead of a generic 6-DoF problem. This allows us to derive a fast branch-and-bound global optimization, which guarantees a global optimum. Thereby, we achieve efficient and robust estimation even in the presence of outliers. By experiments on both synthetic and real-world data, we validate the performance as well as the guaranteed optimality of the proposed method.


Title: Learning Camera Miscalibration Detection
Key Words: calibration  cameras  image colour analysis  image motion analysis  learning (artificial intelligence)  mobile robots  neural nets  robot vision  deep convolutional neural network  semisynthetic dataset generation pipeline  RGB cameras  vision sensors  data-driven approach  robotic platforms  camera miscalibration detection  Cameras  Calibration  Robots  Training  Sensor systems 
Abstract: Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients, or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. This paper focuses on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera's intrinsic parameters is required or not. The code is available at http://github.com/ethz-asl/camera_miscalib_detection.


Title: Robotic General Parts Feeder: Bin-picking, Regrasping, and Kitting
Key Words: grippers  industrial manipulators  materials handling  multi-robot systems  robotic general parts feeder  multiple objects  manufacturing industry  multirobot system  kitting  automatic multiple parts feeding problem  coarse-to-fine manipulation process  multiple robot arms  MPPH  traditional parts feeder  various-shaped industrial parts  robotic bin-picking system  automatic parts feeding  mean picks per hour  Pipelines  Robot sensing systems  Shape  Grippers  Service robots  Manipulators 
Abstract: The automatic parts feeding of multiple objects is an unsolved problem in the manufacturing industry. In this paper, we tackle the problem by proposing a multi-robot system. The system comprises three sub-components which perform bin-picking, regrasping, and kitting. The three subcomponents divide and conquer the automatic multiple parts feeding problem by considering a coarse-to-fine manipulation process. Multiple robot arms are connected in series as a pipeline. The robots are separated into three groups to perform the roles of each sub-component. The accuracy of the state and manipulation are getting higher along with the changes of the sub-components in the pipeline. In the experimental section, the performance of the system is evaluated by using the Mean Picks Per Hour (MPPH) metric and success rate, which are compared to traditional parts feeder and manual labor. The results show that the Mean Picks Per Hour (MPPH) of the proposed system is 351 with eleven various-shaped industrial parts, which is faster than the state-of-the-art robotic bin-picking system. The lead time of the proposed system for new parts is less than that of a traditional parts feeders and/or manual labor.


Title: Planning, Learning and Reasoning Framework for Robot Truck Unloading
Key Words: control engineering computing  decision making  industrial manipulators  inference mechanisms  learning (artificial intelligence)  path planning  production engineering computing  unloading  reasoning framework  industrial manipulator robot  real-time motion planning  complex robotic system  high-level decision-making  belief space planning  offline learning  execution module  robot truck unloading  online decision-making  Planning  Robot sensing systems  Task analysis  Decision making  Collision avoidance  Real-time systems 
Abstract: We consider the task of autonomously unloading boxes from trucks using an industrial manipulator robot. There are multiple challenges that arise: (1) real-time motion planning for a complex robotic system carrying two articulated mechanisms, an arm and a scooper, (2) decision-making in terms of what action to execute next given imperfect information about boxes such as their masses, (3) accounting for the sequential nature of the problem where current actions affect future state of the boxes, and (4) real-time execution that interleaves high-level decision-making with lower level motion planning. In this work, we propose a planning, learning, and reasoning framework to tackle these challenges, and describe its components including motion planning, belief space planning for offline learning, online decision-making based on offline learning, and an execution module to combine decision-making with motion planning. We analyze the performance of the framework on real-world scenarios. In particular, motion planning and execution modules are evaluated in simulation and on a real robot, while offline learning and online decision-making are evaluated in simulated real-world scenarios.


Title: Evaluation of Perception Latencies in a Human-Robot Collaborative Environment
Key Words: humanoid robots  image sensors  optical sensors  optical tracking  EtherCAT channel  perception latency evaluation  3D vision-based sensor system  laser-tracker system  human-robot collaborative environment  actuation system  Robot sensing systems  Measurement by laser beam  Delays  Position measurement  Collaboration  Sensor systems 
Abstract: The latency in vision-based sensor systems used in human-robot collaborative environments is an important safety parameter which in most cases has been neglected by researchers. The main reason for this neglect is the lack of an accurate ground-truth sensor system with a minimal delay to benchmark the vision-sensors against. In this paper the latencies of 3D vision-based sensors are experimentally evaluated and analyzed using an accurate laser-tracker system which communicates on a dedicated EtherCAT channel with minimal delay. The experimental results in the paper demonstrate that the latency in the vision-based sensor system is many orders higher than the latency in the control and actuation system.


Title: Assembly of randomly placed parts realized by using only one robot arm with a general parallel-jaw gripper
Key Words: cost reduction  dexterous manipulators  grippers  industrial manipulators  legged locomotion  materials handling  position control  robotic assembly  robotic assembly  robot arm  peg-in-hole assembly  parallel-jaw gripper  industry assembly lines  parting feeding machine  sorting process  cost reduction  grasping process  two-fingered gripper  design engineering  orientation control  offset position control  Grippers  Fasteners  Robotic assembly  Manipulators  Industries  Sensors 
Abstract: In industry assembly lines, parts feeding machines are widely employed as the prologue of the whole procedure. They play the role of sorting the parts randomly placed in bins to the state with specified pose. With the help of the parts feeding machines, the subsequent assembly processes by robot arm can always start from the same condition. Thus it is expected that function of parting feeding machine and the robotic assembly can be integrated with one robot arm. This scheme can provide great flexibility and can also contribute to reduce the cost. The difficulties involved in this scheme lie in the fact that in the part feeding phase, the pose of the part after grasping may be not proper for the subsequent assembly. Sometimes it can not even guarantee a stable grasp. In this paper, we proposed a method to integrate parts feeding and assembly within one robot arm. This proposal utilizes a specially designed gripper tip mounted on the jaws of a two-fingered gripper. With the modified gripper, in-hand manipulation of the grasped object is realized, which can ensure the control of the orientation and offset position of the grasped object. The proposal in this paper is verified by a simulated assembly in which a robot arm completed the assembly process including parts picking from bin and a subsequent peg-in-hole assembly.


Title: Bio-Inspired Distance Estimation using the Self-Induced Acoustic Signature of a Motor-Propeller System
Key Words: acoustic signal processing  acoustic wave interference  aircraft control  distance measurement  microphones  oscillations  propellers  bio-inspired distance estimation  acoustic signature  motor-propeller system  single microphone  audible frequency band  power spectrum  broadband oscillation  MPS  broadband constructive-destructive interference pattern  Microphones  Acoustics  Robot sensing systems  Interference  Frequency-domain analysis 
Abstract: In this paper we propose an algorithm to actively control the distance of a motor-propeller system (MPS) to a large obstacle using data from a single microphone. The method is based upon a broadband constructive/destructive interference pattern across the audible frequency band that is present when the MPS is near an obstacle. By taking the difference between the power spectrum in the obstacle-free case and the spectrum when recording near an obstacle, a broadband oscillation with respect to frequency is revealed. The frequency of this oscillation is linearly-related to the distance from the microphone to the wall. We present both static and dynamic experiments showcasing the ability of the proposed method to estimate the distance to a wall as well as actively control it.


Title: The Lobster-inspired Antagonistic Actuation Mechanism Towards a Bending Module
Key Words: bending  biomechanics  motion control  pneumatic actuators  robot dynamics  torque control  input pressure  antagonistic soft chambers  mechanical performance  safe compliant actuation  enhanced torque output  lobster leg joint  musculoskeletal structure  bending module  lobster-inspired antagonistic actuation mechanism  maximum torque output  fabricated module  stiffness tuning  angle control  bending angle  Torque  Actuators  Legged locomotion  Hysteresis  Exoskeletons  Pneumatic systems 
Abstract: This paper describes a new type of bending module inspired, in part, by the musculoskeletal structure of the lobster leg joint. The bending module proposed combines enhanced torque output, reconfigurability in assembling, safe compliant actuation, and accurate control on its mechanical performance. In this module, antagonistic soft chambers are enveloped by exoskeleton shells, and the bending angle and the stiffness can be independently adjusted by controlling the input pressure in the two chambers. Theoretical models are developed to characterize the relationships between the input pressure, bending angle, and stiffness, and a controller for angle control and stiffness tuning is constructed with experimental validation. The fabricated module can reach the maximum torque output of 109.7 N·mm under 40 kPa and the stiffness range from 40 to 220 N·mm/rad, demonstrating its capacity to fulfill both safe interactions and forceful tasks.


Title: Emulating duration and curvature of coral snake anti-predator thrashing behaviors using a soft-robotic platform
Key Words: biomechanics  ecology  elastomers  mobile robots  pneumatic actuators  zoology  snake survival  soft robots  fiber-reinforced elastomeric enclosures  anti-predator behaviors  live snakes  curvature values  soft-robotic head  soft robot motion durations  distinct anti-predatory behavior  live snake observations  coral snake anti-predator thrashing  soft-robotic platform  nonlocomotory movements  animal-robot interactions  coral snakes  snake species  Soft robotics  MIMICs  Biology  Fabrication  Strain  Mathematical model 
Abstract: This paper presents a soft-robotic platform for exploring the ecological relevance of non-locomotory movements via animal-robot interactions. Coral snakes (genus Micrurus) and their mimics use vigorous, non-locomotory, and arrhythmic thrashing to deter predation. There is variation across snake species in the duration and curvature of anti-predator thrashes, and it is unclear how these aspects of motion interact to contribute to snake survival. In this work, soft robots composed of fiber-reinforced elastomeric enclosures (FREEs) are developed to emulate the anti-predator behaviors of three genera of snake. Curvature and duration of motion are estimated for both live snakes and robots, providing a quantitative assessment of the robots' ability to emulate snake poses. The curvature values of the fabricated soft-robotic head, midsection, and tail segments are found to overlap with those exhibited by live snakes. Soft robot motion durations were less than or equal to those of snakes for all three genera. Additionally, combinations of segments were selected to emulate three specific snake genera with distinct anti-predatory behavior, producing curvature values that aligned well with live snake observations.


Title: Contact Surface Estimation via Haptic Perception
Key Words: haptic interfaces  legged locomotion  contact surface estimation  haptic perception  legged systems  contact force  surface geometry  vision system  harsh weather  surface information  haptic exploration  Robot sensing systems  Estimation  Friction  Legged locomotion  Foot  Force 
Abstract: Legged systems need to optimize contact force in order to maintain contacts. For this, the controller needs to have the knowledge of the surface geometry and how slippery the terrain is. We can use a vision system to realize the terrain, but the accuracy of the vision system degrades in harsh weather, and it cannot visualize the terrain if it is covered with water or grass. Also, the degree of friction cannot be directly visualized. In this paper, we propose an online method to estimate the surface information via haptic exploration. We also introduce a probabilistic criterion to measure the quality of the estimation. The method is validated on both simulation and a real robot platform.


Title: Local Policy Optimization for Trajectory-Centric Reinforcement Learning
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  nonlinear control systems  nonlinear programming  open loop systems  local policy optimization  trajectory-centric reinforcement learning  local stabilizing policy optimization  trajectory-centric model-based reinforcement learning  global policy optimization  nonlinear systems  robotic manipulation tasks  open-loop trajectory optimization  local policy synthesis  single optimization problem  nonlinear programming  Robustness  Trajectory optimization  Uncertainty  Learning (artificial intelligence)  Robots 
Abstract: The goal of this paper is to present a method for simultaneous trajectory and local stabilizing policy optimization to generate local policies for trajectory-centric model-based reinforcement learning (MBRL). This is motivated by the fact that global policy optimization for non-linear systems could be a very challenging problem both algorithmically and numerically. However, a lot of robotic manipulation tasks are trajectory-centric, and thus do not require a global model or policy. Due to inaccuracies in the learned model estimates, an open-loop trajectory optimization process mostly results in very poor performance when used on the real system. Motivated by these problems, we try to formulate the problem of trajectory optimization and local policy synthesis as a single optimization problem. It is then solved simultaneously as an instance of nonlinear programming. We provide some results for analysis as well as achieved performance of the proposed technique under some simplifying assumptions.


Title: Automatic Snake Gait Generation Using Model Predictive Control
Key Words: drag  friction  mobile robots  motion control  Pareto optimisation  predictive control  robot dynamics  trajectory control  automatic snake gait generation  undulatory gaits  snake robots  movement pattern  serpenoid curve  model predictive control  locomotion gaits  trajectory optimization  snake dynamics  anisotropic dry friction  viscous friction  fluid dynamic effects  Pareto-optimal serpenoid gaits  drag  Friction  Dynamics  Snake robots  Force  Adaptation models  Heuristic algorithms  Optimal control 
Abstract: In this paper, we propose a method for generating undulatory gaits for snake robots. Instead of starting from a pre-defined movement pattern such as a serpenoid curve, we use a Model Predictive Control (MPC) approach to automatically generate effective locomotion gaits via trajectory optimization. An important advantage of this approach is that the resulting gaits are automatically adapted to the environment that is being modeled as part of the snake dynamics. To illustrate this, we use a novel model for anisotropic dry friction, along with existing models for viscous friction and fluid dynamic effects such as drag and added mass. For each of these models, gaits generated without any change in the method or its parameters are as efficient as Pareto-optimal serpenoid gaits tuned individually for each environment. Furthermore, the proposed method can also produce more complex or irregular gaits, e.g. for obstacle avoidance or executing sharp turns.


Title: On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause Detection and Identification
Key Words: aerospace computing  autonomous aerial vehicles  convolutional neural nets  fault diagnosis  learning (artificial intelligence)  neural net architecture  pattern classification  real-time systems  recurrent neural nets  sensor fusion  raw sensor data  drone misoperations  unmanned aerial vehicle fault cause detection  deep learning architectures  fault cause identification  drone software cyberattack  deep convolutional neural network  long short term memory neural network  autoencoder  real time sensor data classification  Drones  Computer crashes  Real-time systems  Robot sensing systems  Computer architecture  Neural networks  Data models 
Abstract: With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor/actuator system, a physical damage/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on real-time sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and realworld experiments on a drone. Empirical results show that our solution is able to detect (with over 90% accuracy) and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 85% accuracy (experimental data)).


Title: GOMP: Grasp-Optimized Motion Planning for Bin Picking
Key Words: collision avoidance  concave programming  dexterous manipulators  grippers  industrial manipulators  manipulator dynamics  motion control  quadratic programming  warehouse automation  GOMP  bin-picking robot  robot dynamics  grasp planner  motion planner  robot bin picking  picks-per-hour  PPH  grasp-analysis tools  robot gripper  grasp-optimized motion planning  warehouse automation  Dex-Net  degree of freedom  sequential quadratic programming  obstacle avoidance  time-minimization  Trajectory  Grippers  Robot sensing systems  Planning  Manipulators  Optimization 
Abstract: Rapid and reliable robot bin picking is a critical challenge in automating warehouses, often measured in picks-per-hour (PPH). We explore increasing PPH using faster motions based on optimizing over a set of candidate grasps. The source of this set of grasps is two-fold: (1) grasp-analysis tools such as Dex-Net generate multiple candidate grasps, and (2) each of these grasps has a degree of freedom about which a robot gripper can rotate. In this paper, we present Grasp-Optimized Motion Planning (GOMP), an algorithm that speeds up the execution of a bin-picking robot's operations by incorporating robot dynamics and a set of candidate grasps produced by a grasp planner into an optimizing motion planner. We compute motions by optimizing with sequential quadratic programming (SQP) and iteratively updating trust regions to account for the non-convex nature of the problem. In our formulation, we constrain the motion to remain within the mechanical limits of the robot while avoiding obstacles. We further convert the problem to a time-minimization by repeatedly shorting a time horizon of a trajectory until the SQP is infeasible. In experiments with a UR5, GOMP achieves a speedup of 9x over a baseline planner.


Title: Motion Planning and Task Allocation for a Jumping Rover Team
Key Words: collision avoidance  integer programming  linear programming  mobile robots  multi-robot systems  planetary rovers  travelling salesman problems  trees (mathematics)  jumping rover team  robotic team  unmanned ground vehicles  hybrid operational modes  multiple traveling salesman problem  mTSP  ground surface  jumping capability  optimal path  mixed-integer linear programming problem  RRT*  multiple UGV  optimized motion  customized jumping rovers  Planning  Task analysis  Smoothing methods  Resource management  Mobile robots  Wheels  Jumping Robots  Multiple Traveling Salesman Problem  Path Planning  Rapidly-exploring Random Tree  Mixed-Integer Linear Programming 
Abstract: This paper presents a cooperative robotic team composed of unmanned ground vehicles (UGVs) with hybrid operational modes to tackle the multiple traveling salesman problem (mTSP) with obstacles. The hybrid operational modes allow every UGV in the team to not only travel on a ground surface but also jump over obstacles. We name these UGVs jumping rovers. The jumping capability provides a flexible form of locomotion by leaping and landing on top of obstacles instead of navigating around obstacles. To solve the mTSP, an optimal path between any two objective points in an mTSP is determined by the optimized rapidly-exploring random tree method, named RRT*, and is further improved through a refined RRT* algorithm to find a smoother path between targets. We then formulate the mTSP as a mixed-integer linear programming (MILP) problem to search for the most cost-effective combination of paths for multiple UGVs. The effectiveness of the hybrid operational modes and optimized motion with assigned tasks is verified in an indoor, physical experimental environment using the customized jumping rovers.


Title: Reoriented Short-Cuts (RSC): An Adjustment Method for Locally Optimal Path Short-Cutting in High DoF Configuration Spaces
Key Words: collision avoidance  computational complexity  Gaussian processes  graph theory  manipulators  mobile robots  optimisation  single homotopy class  asymptotic convergence  single DoF  path segments  RSC  zero-volume convergence region  adjustment method  high DoF configuration spaces  reoriented short-cuts  locally optimal path short-cutting  high degree of freedom problems  informed Gaussian sampling technique  IGS  collision checking computation  robot manipulation  rotation oriented problems  translation oriented problems  Convergence  Optimization  Robots  Planning  Complexity theory  Trajectory  Gaussian distribution 
Abstract: This paper presents Reoriented Short-Cuts (RSC): A modification of the traditional Short-Cut technique, allowing almost sure, single homotopy class, asymptotic convergence in high degree of freedom (DoF) problems. An additional Informed Gaussian Sampling (IGS) technique is also introduced for convergence comparison. Traditionally, Short-Cut methods are used as a final technique to further optimize an initially found path. Typical Short-Cut methods fail as a single DoF may converge faster than the remaining, creating a zero-volume region between path segments and objects, halting further improvements. Previous attempts to solve this separate DoFs individually, drastically increasing collision checking computation. RSC and IGS control the shifting of the vertex to be Short-Cut, moving vertex positions by reorienting the line segments, removing the zero-volume convergence region. These methods are compared to similar strategies in a variety of problems including random worlds, and robot manipulation, to show the convergence across both translation and rotation oriented problems.


Title: Learning Resilient Behaviors for Navigation Under Uncertainty
Key Words: control engineering computing  learning (artificial intelligence)  mobile robots  neural nets  safety-critical software  diverse environments  environmental uncertainty  resilient behaviors  deep reinforcement learning  complex behaviors  adaptive behaviors  uncertainty-aware predictor  uncertainty-aware navigation network  neural network  safety-critical tasks  mobile robot  Uncertainty  Navigation  Robots  Task analysis  Training  Estimation  Learning (artificial intelligence) 
Abstract: Deep reinforcement learning has great potential to acquire complex, adaptive behaviors for autonomous agents automatically. However, the underlying neural network polices have not been widely deployed in real-world applications, especially in these safety-critical tasks (e.g., autonomous driving). One of the reasons is that the learned policy cannot perform flexible and resilient behaviors as traditional methods to adapt to diverse environments. In this paper, we consider the problem that a mobile robot learns adaptive and resilient behaviors for navigating in unseen uncertain environments while avoiding collisions. We present a novel approach for uncertainty-aware navigation by introducing an uncertainty-aware predictor to model the environmental uncertainty, and we propose a novel uncertainty-aware navigation network to learn resilient behaviors in the prior unknown environments. To train the proposed uncertainty-aware network more stably and efficiently, we present the temperature decay training paradigm, which balances exploration and exploitation during the training process. Our experimental evaluation demonstrates that our approach can learn resilient behaviors in diverse environments and generate adaptive trajectories according to environmental uncertainties.


Title: Nonlinear Vector-Projection Control for Agile Fixed-Wing Unmanned Aerial Vehicles
Key Words: aerospace components  aircraft control  attitude control  autonomous aerial vehicles  cascade control  helicopters  mobile robots  nonlinear control systems  trajectory control  nonlinear vector-projection control  agile fixed-wing aircraft  fixed-wing platforms  nonlinear control strategy  autonomous flight  cascaded control structure  inner attitude control loop  Special Orthornormal group  outer position control loop  thrust command  attitude references  lift forces  agile fixed-wing unmanned aerial vehicles  rotorcraft  Aircraft  Control systems  Attitude control  Aerodynamics  Position measurement  Aircraft propulsion 
Abstract: Agile fixed-wing aircraft integrate the efficient, high-speed capabilities of conventional fixed-wing platforms with the extreme maneuverability of rotorcraft. This work presents a nonlinear control strategy that harnesses these capabilities to enable autonomous flight through aggressive, time-constrained, three-dimensional trajectories. The cascaded control structure consists of two parts; an inner attitude control loop developed on the Special Orthornormal group that avoids singularities commonly associated with other parametrizations, and an outer position control loop that jointly determines the thrust command and attitude references by implementing a novel vector-projection algorithm. The objective of the algorithm is to decouple roll from the reference attitude to ensure that thrust and lift forces can always be pointed such that position errors converge to zero. The proposed control system represents a single, unified solution that remains effective throughout the aircraft's flight envelope, including aerobatic operation. Controller performance is verified through simulations and experimental flight tests; results show the unified control scheme is capable of performing a wide range of operations that would normally require multiple, single-purpose controllers, and their associated switching logic.


Title: Adaptive Nonlinear Control of Fixed-Wing VTOL with Airflow Vector Sensing
Key Words: adaptive control  aerodynamics  aerospace components  aircraft control  computational fluid dynamics  mobile robots  nonlinear control systems  rotors  tracking  adaptive nonlinear control  airflow vector sensing  complex aerodynamic interactions  linear force models  rotor forces  three-dimensional airflow sensor  custom-built fixed-wing VTOL  force prediction  baseline flight controllers  fixed-wing vertical take-off and landing aircraft  Force  Aerodynamics  Atmospheric modeling  Aircraft  Rotors  Propellers  Adaptation models 
Abstract: Fixed-wing vertical take-off and landing (VTOL) aircraft pose a unique control challenge that stems from complex aerodynamic interactions between wings and rotors. Thus, accurate estimation of external forces is indispensable for achieving high performance flight. In this paper, we present a composite adaptive nonlinear tracking controller for a fixed- wing VTOL. The method employs online adaptation of linear force models, and generates accurate estimation for wing and rotor forces in real-time based on information from a three-dimensional airflow sensor. The controller is implemented on a custom-built fixed-wing VTOL, which shows improved velocity tracking and force prediction during the transition stage from hover to forward flight, compared to baseline flight controllers.


Title: The Reconfigurable Aerial Robotic Chain: Modeling and Control
Key Words: aerospace robotics  control system synthesis  mobile robots  position control  ARC-Alpha prototype  multilinked microaerial vehicles  reconfigurable aerial robotic chain  multiple parallel angular controllers  model predictive position control loop  controller design  connected aerial vehicles  system dynamics  system extendability  distributed sensing  Robot sensing systems  Robot kinematics  Payloads  Shape  Prototypes 
Abstract: This paper overviews the system design, modeling and control of the Aerial Robotic Chain. This new design corresponds to a reconfigurable robotic system of systems consisting of multilinked micro aerial vehicles that presents the ability to cross narrow sections, morph its shape, ferry significant payloads, offer the potential of distributed sensing and processing, and enable system extendability. We present the system dynamics for any number of connected aerial vehicles, followed by the controller design involving a model predictive position control loop combined with multiple parallel angular controllers on SO(3). Evaluation studies both in simulation and through experiments based on our ARC-Alpha prototype are depicted and involve coordinated maneuvering and shape configuration to cross narrow windows.


Title: Trajectory Tracking Nonlinear Model Predictive Control for an Overactuated MAV
Key Words: actuators  autonomous aerial vehicles  control system synthesis  microrobots  mobile robots  nonlinear control systems  observers  optimal control  predictive control  robot dynamics  robot kinematics  trajectory control  trajectory tracking nonlinear model predictive control  omnidirectional microaerial vehicles  free space  rigid body model based approach  receding horizon  optimal wrench commands  mechanical design  optimal actuator commands  disturbance observer  offset-free tracking  6DoF trajectories  Rotors  Resource management  Actuators  Trajectory  Force  Quaternions  Torque 
Abstract: This work presents a method to control omnidirectional micro aerial vehicles (OMAVs) for the tracking of 6-DoF trajectories in free space. A rigid body model based approach is applied in a receding horizon fashion to generate optimal wrench commands that can be constrained to meet limits given by the mechanical design and actuators of the platform. Allocation of optimal actuator commands is performed in a separate step. A disturbance observer estimates forces and torques that may arise from unmodeled dynamics or external disturbances and fuses them into the optimization to achieve offset-free tracking. Experiments on a fully overactuated MAV show the tracking performance and compare it against a classical PD-based controller.


Title: Optimal Oscillation Damping Control of cable-Suspended Aerial Manipulator with a Single IMU Sensor
Key Words: control system synthesis  damping  feedback  linear quadratic control  linearisation techniques  manipulators  nonlinear control systems  optimal control  pendulums  robust control  vibration control  control action  simplified SAM model  model uncertainties  optimal oscillation damping control  cable-suspended aerial manipulator  single IMU sensor  double pendulum  output feedback linear quadratic regulation problem  minimal energy consumption  Oscillators  Damping  Manipulators  Robot sensing systems  Task analysis  Cranes 
Abstract: This paper presents a design of oscillation damping control for the cable-Suspended Aerial Manipulator (SAM). The SAM is modeled as a double pendulum, and it can generate a body wrench as a control action. The main challenge is the fact that there is only one onboard IMU sensor which does not provide full information on the system state. To overcome this difficulty, we design a controller motivated by a simplified SAM model. The proposed controller is very simple yet robust to model uncertainties. Moreover, we propose a gain tuning rule by formulating the proposed controller in the form of output feedback linear quadratic regulation problem. Consequently, it is possible to quickly dampen oscillations with minimal energy consumption. The proposed approach is validated through simulations and experiments.


Title: TUNERCAR: A Superoptimization Toolchain for Autonomous Racing
Key Words: automobiles  mobile robots  optimisation  path planning  random processes  search problems  autonomous vehicles  online planning  TUNERCAR  superoptimization toolchain  autonomous racing  vehicle parameters  autonomous racecar  systems infrastructure  parallel implementation  CMA-ES  lap time  naive random search  racing strategy  Optimization  Sociology  Statistics  Vehicle dynamics  Hardware  Robots  Planning 
Abstract: TUNERCAR is a toolchain that jointly optimizes racing strategy, planning methods, control algorithms, and vehicle parameters for an autonomous racecar. In this paper, we detail the target hardware, software, simulators, and systems infrastructure for this toolchain. Our methodology employs a parallel implementation of CMA-ES which enables simulations to proceed 6 times faster than real-world rollouts. We show our approach can reduce the lap times in autonomous racing, given a fixed computational budget. For all tested tracks, our method provides the lowest lap time, and relative improvements in lap time between 7-21%. We demonstrate improvements over a naive random search method with equivalent computational budget of over 15 seconds/lap, and improvements over expert solutions of over 2 seconds/lap. We further compare the performance of our method against hand-tuned solutions submitted by over 30 international teams, comprised of graduate students working in the field of autonomous vehicles. Finally, we discuss the effectiveness of utilizing an online planning mechanism to reduce the reality gap between our simulation and actual tests.


Title: Risk Assessment and Planning with Bidirectional Reachability for Autonomous Driving
Key Words: collision avoidance  mobile robots  path planning  risk analysis  road safety  road traffic control  road vehicles  autonomous driving  subsequent planning  risk assessment algorithms  ego vehicle  risk-inducing factors  bidirectional reachability  risk planning  Risk management  Planning  Prediction algorithms  Autonomous vehicles  Robot sensing systems  Navigation  Probabilistic logic 
Abstract: Risk assessment to quantify the danger associated with taking a certain action is critical to navigating safely through crowded urban environments during autonomous driving. Risk assessment and subsequent planning is usually done by first tracking and predicting trajectories of other agents, such as vehicles and pedestrians, and then choosing an action to avoid future collisions. However, few existing risk assessment algorithms handle occlusion and other sensory limitations effectively. One either assesses the risk in the worst-case scenario and thus makes the ego vehicle overly conservative, or predicts as many hidden agents as possible and thus makes the computation intensive. This paper explores the possibility of efficient risk assessment under occlusion via both forward and backward reachability. The proposed algorithm can not only identify the location of risk-inducing factors, but can also be used during motion planning. The proposed method is evaluated on various four-way highly occluded intersections with up to five other vehicles in the scene. Compared with other risk assessment algorithms, the proposed method shows better efficiency, meaning that the ego vehicle reaches the goal at a higher speed. In addition, it also lowers the median collision rate by 7.5× when compared to state of the art techniques.


Title: Game theoretic decision making based on real sensor data for autonomous vehicles’ maneuvers in high traffic
Key Words: decision making  game theory  learning (artificial intelligence)  mobile robots  Monte Carlo methods  road vehicles  sensors  sensor data  autonomous vehicles  iterative multiplayer game  game model  ego-vehicle  vehicle-to-vehicle communication  traffic simulator  game theoretic decision making  cognitive hierarchy reasoning  Monte Carlo reinforcement learning  Games  Automobiles  Mathematical model  Game theory  Autonomous vehicles  Robot sensing systems 
Abstract: This paper presents an approach for implementing game theoretic decision making in combination with realistic sensory data input so as to allow an autonomous vehicle to perform maneuvers, such as lane change or merge in high traffic scenarios. The main novelty of this work, is the use of realistic sensory data input to obtain the observations as input of an iterative multi-player game in a realistic simulator. The game model allows to anticipate reactions of additional vehicles to the movements of the ego-vehicle without using any specific coordination or vehicle-to-vehicle communication. Moreover, direct information from the simulator, such as position or speed of the vehicles is also avoided.The solution of the game is based on cognitive hierarchy reasoning and it uses Monte Carlo reinforcement learning in order to obtain a near-optimal policy towards a specific goal. Moreover, the game proposed is capable of solving different situations using a single policy. The system has been successfully tested and compared with previous techniques using a realistic hybrid simulator, where the ego-vehicle and its sensors are simulated on a 3D simulator and the additional vehicles' behavior is obtained from a traffic simulator.


Title: Enhancing Game-Theoretic Autonomous Car Racing Using Control Barrier Functions
Key Words: collision avoidance  computer games  game theory  game-theoretic autonomous car  control barrier functions  two-player racing game  autonomous ego vehicle  opponent vehicle  two-car racing game  game-theoretic control method hinges  sensitivity-enhanced Nash equilibrium  Collision avoidance  Trajectory  Robots  Acceleration  Safety  Bicycles  Automobiles 
Abstract: In this paper, we consider a two-player racing game, where an autonomous ego vehicle has to be controlled to race against an opponent vehicle, which is either autonomous or human-driven. The approach to control the ego vehicle is based on a Sensitivity-ENhanced NAsh equilibrium seeking (SENNA) method, which uses an iterated best response algorithm in order to optimize for a trajectory in a two-car racing game. This method exploits the interactions between the ego and the opponent vehicle that take place through a collision avoidance constraint. This game-theoretic control method hinges on the ego vehicle having an accurate model and correct knowledge of the state of the opponent vehicle. However, when an accurate model for the opponent vehicle is not available, or the estimation of its state is corrupted by noise, the performance of the approach might be compromised. For this reason, we augment the SENNA algorithm by enforcing Permissive RObust SafeTy (PROST) conditions using control barrier functions. The objective is to successfully overtake or to remain in the front of the opponent vehicle, even when the information about the latter is not fully available. The successful synergy between SENNA and PROST-antithetical to the notable rivalry between the two namesake Formula 1 drivers-is demonstrated through extensive simulated experiments.


Title: SPRINT: Subgraph Place Recognition for INtelligent Transportation
Key Words: image recognition  image retrieval  intelligent transportation systems  nearest neighbour methods  traffic engineering computing  scalable place recognition  environmental conditions  visual place recognition  mobile robotics  image information  image acquisition process  k nearest neighbours  image retrieval  temporal relations  inference time reduction  databases  publicly sourced data  subgraph place recognition for intelligent transportation  SPRINT  Image recognition  Hidden Markov models  Robots  Symmetric matrices  Cameras  Image retrieval 
Abstract: Visual place recognition is an important problem in mobile robotics which aims to localize a robot using image information alone. Recent methods have shown promising results for place recognition under varying environmental conditions by exploiting the sequential nature of the image acquisition process. We show that by using k nearest neighbours based image retrieval as the backend, and exploiting the structure of the image acquisition process which introduces temporal relations between images in the database, the location of possible matches can be restricted to a subset of all the images seen so far. In effect, the original problem space can thus be restricted to a significantly smaller subspace, reducing the inference time significantly. This is particularly important for scalable place recognition over databases containing millions of images. We present large scale experiments using publicly sourced data that show the computational performance of the proposed method under varying environmental conditions.


Title: OneShot Global Localization: Instant LiDAR-Visual Pose Estimation
Key Words: cameras  feature extraction  image matching  image segmentation  image sequences  learning (artificial intelligence)  neural nets  optical radar  pose estimation  robot vision  learning-based descriptors  point cloud segments  degree-of-freedom  LiDAR scan  neural network architecture  segment retrieval rates  LiDAR-only description  OneShot global localization  autonomous navigation tasks  LiDAR-visual pose estimation  LiDAR-only approach  degree-of-freedom pose  NCLT dataset  Three-dimensional displays  Laser radar  Robots  Sensors  Image segmentation  Neural networks  Visualization 
Abstract: Globally localizing in a given map is a crucial ability for robots to perform a wide range of autonomous navigation tasks. This paper presents OneShot - a global localization algorithm that uses only a single 3D LiDAR scan at a time, while outperforming approaches based on integrating a sequence of point clouds. Our approach, which does not require the robot to move, relies on learning-based descriptors of point cloud segments and computes the full 6 degree-of-freedom pose in a map. The segments are extracted from the current LiDAR scan and are matched against a database using the computed descriptors. Candidate matches are then verified with a geometric consistency test. We additionally present a strategy to further improve the performance of the segment descriptors by augmenting them with visual information provided by a camera. For this purpose, a custom-tailored neural network architecture is proposed. We demonstrate that our LiDAR-only approach outperforms a state-of-the-art baseline on a sequence of the KITTI dataset and also evaluate its performance on the challenging NCLT dataset. Finally, we show that fusing in visual information boosts segment retrieval rates by up to 26% compared to LiDAR-only description.


Title: Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-to-end Robot Learning Scheme
Key Words: collision avoidance  control engineering computing  eigenvalues and eigenfunctions  gradient methods  learning (artificial intelligence)  mobile robots  neural nets  neurocontrollers  recurrent neural nets  mobile robot  traditional robotic control suits  profound task-specific knowledge  building  testing control software  deep learning  end-to-end solutions  minimal knowledge  end-to-end linear dynamical systems  LDS  robotic domains  regularization loss component  learning algorithm  learned autonomous system  simulated robotic experiments  stabilizing method  end-to-end robot learning scheme  recurrent neural network compartment  gershgorin loss  Stability analysis  Eigenvalues and eigenfunctions  Recurrent neural networks  Training  Robots  Heuristic algorithms  Task analysis 
Abstract: Traditional robotic control suits require profound task-specific knowledge for designing, building and testing control software. The rise of Deep Learning has enabled end-to-end solutions to be learned entirely from data, requiring minimal knowledge about the application area. We design a learning scheme to train end-to-end linear dynamical systems (LDS)s by gradient descent in imitation learning robotic domains. We introduce a new regularization loss component together with a learning algorithm that improves the stability of the learned autonomous system, by forcing the eigenvalues of the internal state updates of an LDS to be negative reals. We evaluate our approach on a series of real-life and simulated robotic experiments, in comparison to linear and nonlinear Recurrent Neural Network (RNN) architectures. Our results show that our stabilizing method significantly improves test performance of LDS, enabling such linear models to match the performance of contemporary nonlinear RNN architectures. A video of the obstacle avoidance performance of our method on a mobile robot, in unseen environments, compared to other methods can be viewed at https://youtu.be/mhEsCoNao5E.


Title: Mini-Batched Online Incremental Learning Through Supervisory Teleoperation with Kinesthetic Coupling*
Key Words: expectation-maximisation algorithm  intelligent robots  learning (artificial intelligence)  telerobotics  kinesthetic coupling  online incremental learning approach  task execution  teleoperation-based teaching  partial demonstration  dynamic authority distribution  modified partial trajectory  rhythmic peg-in-hole teleoperation task  partial modification  task operation  mini-batched online incremental learning  supervisory teleoperation  expectation-maximization algorithm  Task analysis  Couplings  Force  Trajectory  Robots  Education  Automation 
Abstract: We propose an online incremental learning approach through teleoperation which allows an operator to partially modify a learned model, whenever it is necessary, during task execution. Compared to conventional incremental learning approaches, the proposed approach is applicable for teleoperation-based teaching and it needs only partial demonstration without any need to obstruct the task execution. Dynamic authority distribution and kinesthetic coupling between the operator and the agent helps the operator to correctly perceive the exact instance where modification needs to be asserted in the agent's behaviour online using partial trajectory. For this, we propose a variation of the Expectation-Maximization algorithm for updating original model through mini batches of the modified partial trajectory. The proposed approach reduces human workload and latency for a rhythmic peg-in-hole teleoperation task where online partial modification is required during the task operation.


Title: Recurrent Neural Network Control of a Hybrid Dynamical Transfemoral Prosthesis with EdgeDRNN Accelerator
Key Words: artificial limbs  gait analysis  learning (artificial intelligence)  legged locomotion  medical computing  medical robotics  neurocontrollers  PD control  recurrent neural nets  AMPRO3 prosthetic leg  real-time dynamical system  human-prosthesis system  prosthesis control  recurrent neural network control  hybrid dynamical transfemoral prosthesis  control methods  modulating behaviors  dynamical robotic assistive devices  behavioral cloning  powered transfemoral prostheses  Gated Recurrent Unit  custom hardware accelerator  prosthesis controller  RNN inference  nominal PD controller  EdgeDRNN accelerator  Prosthetics  Legged locomotion  PD control  Trajectory  Real-time systems  Knee 
Abstract: Lower leg prostheses could improve the life quality of amputees by increasing comfort and reducing energy to locomote, but currently control methods are limited in modulating behaviors based upon the human's experience. This paper describes the first steps toward learning complex controllers for dynamical robotic assistive devices. We provide the first example of behavioral cloning to control a powered transfemoral prostheses using a Gated Recurrent Unit (GRU) based recurrent neural network (RNN) running on a custom hardware accelerator that exploits temporal sparsity. The RNN is trained on data collected from the original prosthesis controller. The RNN inference is realized by a novel EdgeDRNN accelerator in real-time. Experimental results show that the RNN can replace the nominal PD controller to realize end-to-end control of the AMPRO3 prosthetic leg walking on flat ground and unforeseen slopes with comparable tracking accuracy. EdgeDRNN computes the RNN about 240 times faster than real time, opening the possibility of running larger networks for more complex tasks in the future. Implementing an RNN on this real-time dynamical system with impacts sets the ground work to incorporate other learned elements of the human-prosthesis system into prosthesis control.


Title: Cross-context Visual Imitation Learning from Demonstrations
Key Words: learning (artificial intelligence)  robots  general imitation learning method  robotic system  context translation model  depth prediction model  multimodal inverse dynamics model  depth observation  inverse model maps  multimodal observations  cross-context learning advantage  cross-context visual imitation learning  color observation  block stacking tasks  Context modeling  Robots  Task analysis  Inverse problems  Visualization  Predictive models  Feature extraction 
Abstract: Imitation learning enables robots to learn a task by simply watching the demonstration of the task. Current imitation learning methods usually require the learner and demonstrator to occur in the same context. This limits their scalability to practical applications. In this paper, we propose a more general imitation learning method which allows the learner and the demonstrator to come from different contexts, such as different viewpoints, backgrounds, and object positions and appearances. Specifically, we design a robotic system consisting of three models: context translation model, depth prediction model and multi-modal inverse dynamics model. First, the context translation model translates the demonstration to the context of learner from a different context. Then combining the color observation and depth observation as inputs, the inverse model maps the multi-modal observations into actions to reproduce the demonstration, where the depth observation is provided by a depth prediction model. By performing the block stacking tasks both in simulation and real world, we prove the cross-context learning advantage of the proposed robotic system over other systems.


Title: Improving Generalisation in Learning Assistance by Demonstration for Smart Wheelchairs
Key Words: Gaussian processes  generalisation (artificial intelligence)  handicapped aids  human-robot interaction  learning (artificial intelligence)  neural nets  wheelchairs  Gaussian process  learning assistance by demonstration  human agent  machine learning models  dimensionality reduction techniques  learning system  custom teleoperation  LAD  generalisation capability  assistive power  learned assistive policy  customised assistance  smart wheelchairs  Wheelchairs  Training  Robots  Vehicles  Haptic interfaces  Sensors  Navigation 
Abstract: Learning Assistance by Demonstration (LAD) is concerned with using demonstrations of a human agent to teach a robot how to assist another human. The concept has previously been used with smart wheelchairs to provide customised assistance to individuals with driving difficulties. A basic premise of this technique is that the learned assistive policy should be able to generalise to environments different than the ones used for training; but this has not been tested before. In this work we evaluate the assistive power and the generalisation capability of LAD using our custom teleoperation and learning system for smart wheelchairs, while seeking to improve it by experimenting with different combinations of dimensionality reduction techniques and machine learning models. Using Autoencoders to reduce the dimension of laserscan data and a Gaussian Process as the learning model, we achieved a 23% improvement in prediction performance against the combination used by the latest work on the field. Using this model to assist a driver exposed to a simulated disability, we observed a 9.8% reduction in track completion times when compared to driving without assistance.


Title: Magnetic Sensor Based Topographic Localization for Automatic Dislocation of Ingested Button Battery
Key Words: cells (electric)  coils  ferromagnetic materials  magnetic sensors  medical robotics  Hall-effect sensors  LR44 button battery  magnet-containing capsule  magnetic sensor  topographic localization  ingested button battery  stomach  battery lands  ingestible magnetic robot  button battery retrieval  localization method  magnetic sensors  direct magnetic field  trilateration method  Batteries  Magnetic sensors  Magnetic resonance imaging  Magnetization  Magnetic separation  Stomach 
Abstract: A button battery accidentally ingested by a toddler or small child can cause severe damage to the stomach within a short period of time. Once a battery lands on the surface of the esophagus or stomach, it can run a current in the tissue and induce a chemical reaction resulting in injury. Following our previous work where we presented an ingestible magnetic robot for button battery retrieval, this study presents a remotely achieved novel localization method of a button battery with commonly available magnetic sensors (Hall-effect sensors). By applying a direct magnetic field to the button battery using an electromagnetic coil, the battery is magnetized, and hence it becomes able to be sensed by Hall-effect sensors. Using a trilateration method, we were able to detect the locations of an LR44 button battery and other ferromagnetic materials at variable distances. Additional four electromagnetic coils were used to autonomously navigate a magnet-containing capsule to dislocate the battery from the affected site.


Title: A Fully Actuated Body-Mounted Robotic Assistant for MRI-Guided Low Back Pain Injection
Key Words: actuators  biomedical MRI  medical image processing  medical robotics  mobile robots  needles  optical tracking  position control  telerobotics  fully actuated body-mounted robotic assistant  MRI-guided low back pain injection  needle alignment module  remotely actuated needle driver module  fully actuated robot  remote actuation design  actuation electronics  robot frame  minimal visible image degradation  size 0.53 mm to 1.45 mm  size 0.7 mm  Needles  Robots  Magnetic resonance imaging  Pain  Force  Back 
Abstract: This paper reports the development of a fully actuated body-mounted robotic assistant for MRI-guided low back pain injection. The robot is designed with a 4-DOF needle alignment module and a 2-DOF remotely actuated needle driver module. The 6-DOF fully actuated robot can operate inside the scanner bore during imaging; hence, minimizing the need of moving the patient in or out of the scanner during the procedure, and thus potentially reducing the procedure time and streamlining the workflow. The robot is built with a lightweight and compact structure that can be attached directly to the patient's lower back using straps; therefore, attenuating the effect of patient motion by moving with the patient. The novel remote actuation design of the needle driver module with beaded chain transmission can reduce the weight and profile on the patient, as well as minimize the imaging degradation caused by the actuation electronics. The free space positioning accuracy of the system was evaluated with an optical tracking system, demonstrating the mean absolute errors (MAE) of the tip position to be 0.99±0.46 mm and orientation to be 0.99±0.65°. Qualitative imaging quality evaluation was performed± on a human volunteer, revealing minimal visible image degradation that should not affect the procedure. The mounting stability of the system was assessed on a human volunteer, indicating the 3D position variation of target movement with respect to the robot frame to be less than 0.7 mm.


Title: Fault Tolerant Control in Shape-Changing Internal Robots
Key Words: biological tissues  fault tolerance  medical robotics  redundancy  surgery  shape-changing internal robots  human body  foreign body  in-vivo robot  implantable robot  fault tolerant control  fault diagnosis  redundancy-based control  fault-tolerant capabilities  fault risks  implantable robots  surgical robots  Implants  Fault tolerance  Fault tolerant systems  Robot sensing systems  Force 
Abstract: It is known that the interior of the human body is one of the most adverse environments for a foreign body, such as an in-vivo robot, and vice-versa. As robots operating in-vivo are increasingly recognized for their capabilities and potential for improved therapies, it is important to ensure their safety, especially for long term treatments when little supervision can be provided. We introduce an implantable robot that is flexible, extendable and symmetric, thus changing shape and size. This design allows the implementation of an effective fault tolerant control, with features such as physical polling for fault diagnosis, retraction and redundancy-based control switching at fault. We demonstrate the fault-tolerant capabilities for an implantable robot that elongates tubular tissues by applying tension to the tissue. In benchtop tests, we show a reduction of the fault risks by at least 83%. The study provides a valuable methodology to enhance safety and efficacy of implantable and surgical robots, and thus to accelerate their adoption.


Title: Evaluation of Increasing Camera Baseline on Depth Perception in Surgical Robotics
Key Words: endoscopes  medical robotics  surgery  visual perception  camera baseline  depth perception  surgical robotics  robot-assisted surgery  surgical trocar  fixed baseline  stereoscopic pickup camera  da Vinci surgical system  robot-assisted surgical systems  flexible baseline design  adaptive baseline camera design  clinical stereoendoscopes  clinical stereoendoscopes  Cameras  Robot vision systems  Surgery  Stereo image processing  Endoscopes  Task analysis 
Abstract: In this paper, we evaluate the effect of increasing camera baselines on depth perception in robot-assisted surgery. Restricted by the diameter of the surgical trocar through which they are inserted, current clinical stereo endoscopes have a fixed baseline of 5.5 mm. To overcome this restriction, we propose using a stereoscopic "pickup" camera with a side-firing design that allows for larger baselines. We conducted a user study with baselines of 10 mm, 15 mm, 20 mm, and 30 mm to evaluate the effect of increasing baseline on depth perception when used with the da Vinci surgical system. Subjects (N=28) were recruited and asked to rank differently sized poles, mounted at a distance of 200 mm from the cameras, according to their increasing order of height when viewed under different baseline conditions. The results showed that subjects performed better as the baseline was increased with the best performance at a 20 mm baseline. This preliminary proof-of-concept study shows that there is opportunity to improve depth perception in robot-assisted surgical systems with a change in endoscope design philosophy. In this paper, we present this change with our side-firing "pickup" camera and its flexible baseline design. Ultimately, this serves as the first step towards an adaptive baseline camera design that maximizes depth perception in surgery.


Title: Toward Autonomous Robotic Micro-Suturing using Optical Coherence Tomography Calibration and Path Planning
Key Words: biological tissues  biomedical optical imaging  calibration  control engineering computing  iterative methods  medical image processing  medical robotics  mobile robots  needles  optical tomography  path planning  phantoms  pose estimation  surgery  autonomous robotic microsuturing  prior autonomous suturing robots  planned entry  suture needle  animal tissue  tissue phantoms  exit points  iterative closest points  keypoint identification  wound detection  robot-needle transforms  robot-OCT  imaging feedback  3D optical coherence tomography system  robotic suturing system  soft tissue  sub-millimeter precision  human surgeons  robotic automation  path planning  optical coherence tomography calibration  Needles  Robots  Wounds  Calibration  Surgery  Imaging  Three-dimensional displays 
Abstract: Robotic automation has the potential to assist human surgeons in performing suturing tasks in microsurgery, and in order to do so a robot must be able to guide a needle with sub-millimeter precision through soft tissue. This paper presents a robotic suturing system that uses 3D optical coherence tomography (OCT) system for imaging feedback. Calibration of the robot-OCT and robot-needle transforms, wound detection, keypoint identification, and path planning are all performed automatically. The calibration method handles pose uncertainty when the needle is grasped using a variant of iterative closest points. The path planner uses the identified wound shape to calculate needle entry and exit points to yield an evenly-matched wound shape after closure. Experiments on tissue phantoms and animal tissue demonstrate that the system can pass a suture needle through wounds with 0.200 mm overall accuracy in achieving the planned entry and exit points, and over 20× more precise than prior autonomous suturing robots.


Title: Improved Multiple Objects Tracking based Autonomous Simultaneous Magnetic Actuation & Localization for WCE
Key Words: biological organs  biomedical optical imaging  endoscopes  interpolation  medical image processing  object tracking  phantoms  autonomous simultaneous magnetic actuation & localization  wireless capsule endoscopy  gastrointestinal examinations  clinical applications  rotating magnet  robotic arm  internal magnetic ring  magnetic fields  external sensor array  spherical linear interpolation  actuation-localization loop  Bezier Curve Gradient  normal vector fitting  frequency 25.0 Hz  Magnetic moments  Magnetic separation  Object tracking  Magnetic resonance imaging  Interpolation  Actuators  Fitting  Simultaneous Magnetic Actuation and Localization  Multiple Objects Tracking  Wireless Capsule Endoscopy 
Abstract: Wireless Capsule Endoscopy (WCE) has the advantage of reducing the invasiveness and pain of gastrointestinal examinations. In this work, we propose a system aimed at autonomously accelerating and locating the WCE inside the intestine for clinical applications. A rotating magnet controlled by a robotic arm is placed outside the patient's body to actuate the capsule with an internal magnetic ring, and the magnetic fields of the two sources are measured by an external sensor array. The original Multiple Objects Tracking method is improved by combining Normal Vector Fitting, Bézier Curve Gradient, and Spherical Linear Interpolation to estimate the 6-D pose of the WCE from a 5-D pose sequence. In order to close the actuation-localization loop, a strategy is presented to react to different states of the capsule. The proposed method is validated via experiments on phantoms as well as on animal intestines. The localization of the capsule shows an accuracy of 3.5mm in position and 9.4° in orientation, and the average update frequency of the estimated 6-D pose reaches 25Hz.


Title: Probe-before-step walking strategy for multi-legged robots on terrain with risk of collapse
Key Words: gait analysis  legged locomotion  mobile robots  stability  probe-before-step walking strategy  multilegged robots  rough terrain  safe footholds  hexapod robot  terrain probing approach  follow-the-leader strategy  stabilisation  Legged locomotion  Probes  Foot  Robot sensing systems  Force  Australia 
Abstract: Multi-legged robots are effective at traversing rough terrain. However, terrains that include collapsible footholds (i.e. regions that can collapse when stepped on) remain a significant challenge, especially since such situations can be extremely difficult to anticipate using only exteroceptive sensing. State-of-the-art methods typically use various stabilisation techniques to regain balance and counter changing footholds. However, these methods are likely to fail if safe footholds are sparse and spread out or if the robot does not respond quickly enough after a foothold collapse. This paper presents a novel method for multi-legged robots to probe and test the terrain for collapses using its legs while walking. The proposed method improves on existing terrain probing approaches, and integrates the probing action into a walking cycle. A follow-the-leader strategy with a suitable gait and stance is presented and implemented on a hexapod robot. The proposed method is experimentally validated, demonstrating the robot can safely traverse terrain containing collapsible footholds.


Title: Representing Multi-Robot Structure through Multimodal Graph Embedding for the Selection of Robot Teams
Key Words: directed graphs  mobile robots  multi-robot systems  unsupervised learning  multirobot system  multirobot structure  human-robot teaming  multimodal graph embedding  robot teams selection  directed graphs  asymmetrical relationships  unsupervised learning  physical robots  multifaceted internal structures  graph embedding-based division methods  Robots  Task analysis  Multi-robot systems  Indexes  Organizations  Resource management  Biology 
Abstract: Multi-robot systems of increasing size and complexity are used to solve large-scale problems, such as area exploration and search and rescue. A key decision in human-robot teaming is dividing a multi-robot system into teams to address separate issues or to accomplish a task over a large area. In order to address the problem of selecting teams in a multi-robot system, we propose a new multimodal graph embedding method to construct a unified representation that fuses multiple information modalities to describe and divide a multi-robot system. The relationship modalities are encoded as directed graphs that can encode asymmetrical relationships, which are embedded into a unified representation for each robot. Then, the constructed multimodal representation is used to determine teams based upon unsupervised learning. We per-form experiments to evaluate our approach on expert-defined team formations, large-scale simulated multi-robot systems, and a system of physical robots. Experimental results show that our method successfully decides correct teams based on the multifaceted internal structures describing multi-robot systems, and outperforms baseline methods based upon only one mode of information, as well as other graph embedding-based division methods.


Title: Connectivity Maintenance: Global and Optimized approach through Control Barrier Functions
Key Words: mobile robots  multi-robot systems  optimal control  optimisation  connectivity maintenance  Control Barrier functions  multirobot system  local connectivity  global connectivity  formation control  Control Barrier Function  control strategy  Robots  Laplace equations  Multi-robot systems  Eigenvalues and eigenfunctions  Maintenance engineering  Task analysis  Control systems 
Abstract: Connectivity maintenance is an essential aspect to consider while controlling a multi-robot system. In general, a multi-robot system should be connected to obtain a certain common objective. Connectivity must be kept regardless of the control strategy or the objective of the multi-robot system. Two main methods exist for connectivity maintenance: keep the initial connections (local connectivity) or allow modifications to the initial connections, but always keeping the overall system connected (global connectivity). In this paper we present a method that allows, at the same time, to maintain global connectivity and to implement the desired control strategy (e.g., consensus, formation control, coverage), all in an optimized fashion. For this purpose, we defined and implemented a Control Barrier Function that can incorporate constraints and objectives. We provide a mathematical proof of the method, and we demonstrate its versatility with simulations of different applications.


Title: Controller Synthesis for Infinitesimally Shape-Similar Formations
Key Words: control system synthesis  decentralised control  multi-robot systems  multirobot team  network structure  controller synthesis  formation control  communication requirements  differential-drive robots  infinitesimally shape-similar formations  network topology  Robot sensing systems  Robot kinematics  Lyapunov methods  Trajectory  Shape 
Abstract: The interplay between network topology and the interaction modalities of a multi-robot team fundamentally impact the types of formations that can be achieved. To explore the trade-offs between network structure and the sensing and communication capabilities of individual robots, this paper applies controller synthesis to formation control of infinitesimally shape-similar frameworks, for which maintaining the relative angles between robots ensures invariance of the framework to translation, rotation, and uniform scaling. Beginning with the development of a controller for the sole purpose of maintaining the formation, the controller-synthesis approach is introduced as a mechanism for incorporating user- designated objectives while ensuring that the formation is maintained. Both centralized and decentralized formulations of the synthesized controller are presented, the resulting sensing and communication requirements are discussed, and the method is demonstrated on a team of differential-drive robots.


Title: A Distributed Source Term Estimation Algorithm for Multi-Robot Systems
Key Words: distributed control  mobile robots  multi-robot systems  airborne chemicals  mobile sensing systems  intelligent systems  odor source localization  homogeneous multirobot systems  multiple mobile robots  distributed system  distributed source term estimation  Robot kinematics  Robot sensing systems  Estimation  Navigation  Probabilistic logic 
Abstract: Finding sources of airborne chemicals with mobile sensing systems finds applications in safety, security, and emergency situations related to medical, domestic, and environmental domains. Given the often critical nature of all the applications, it is important to reduce the amount of time necessary to accomplish this task through intelligent systems and algorithms. In this paper, we extend a previously presented algorithm based on source term estimation for odor source localization for homogeneous multi-robot systems. By gradually increasing the level of coordination among multiple mobile robots, we study the benefits of a distributed system on reducing the amount of time and resources necessary to achieve the task at hand. The method has been evaluated systematically through high-fidelity simulations and in a wind tunnel emulating realistic and repeatable conditions in different coordination scenarios and with different number of robots.


Title: Weighted Buffered Voronoi Cells for Distributed Semi-Cooperative Behavior
Key Words: collision avoidance  computational geometry  game theory  mobile robots  multi-agent systems  multi-robot systems  selfish agent  relative cell  collision-free configuration  egoistic weights  altruistic agents  distributed semi  semicooperative multiagent navigation policies  collision avoidance  dynamic weights  lower relative weight  buffered distance  agent weights  selfish behavior  prioritized behavior  weighted buffered Voronoi tessellation  weighted buffered Voronoi cells  Navigation  Robot kinematics  Collision avoidance  Games  Planning  Safety 
Abstract: This paper introduces the Weighted Buffered Voronoi tessellation, which allows us to define distributed, semicooperative multi-agent navigation policies with guarantees on collision avoidance. We generate the Voronoi cells with dynamic weights that bias the boundary towards the agent with the lower relative weight while always maintaining a buffered distance between two agents. By incorporating agent weights, we can encode selfish or prioritized behavior among agents, where a more selfish agent will have a larger relative cell over less selfish agents. We consider this semi-cooperative since agents do not cooperate in symmetric ways. Furthermore, when all agents start in a collision-free configuration and plan their control actions within their cells, we prove that no agents will collide. Simulations demonstrate the performance of our algorithm for agents navigating to goal locations in a position-swapping game. We observe that agents with more egoistic weights consistently travel shorter paths to their goal than more altruistic agents.


Title: Learning to Control Reconfigurable Staged Soft Arms
Key Words: actuators  calibration  end effectors  learning (artificial intelligence)  tapered arm configurations  soft robotic actuators  arm configurations  quasistatic model only control  controller baseline  control reconfigurable staged soft  system load states  soft arm configurations  stage approach  LSTM calibration routine  current load state  control input generation step  generalized quasistatic model  learned load model  Load modeling  Actuators  Soft robotics  Kinematics  Calibration  Training  Training data 
Abstract: In this work, we present a novel approach for modeling, and classifying between, the system load states introduced when constructing staged soft arm configurations. Through a two stage approach: (1) an LSTM calibration routine is used to identify the current load state then (2) a control input generation step combines a generalized quasistatic model with the learned load model. Our experiments show that accounting for system load allows us to more accurately control tapered arm configurations. We analyze the performance of our method using soft robotic actuators and show it is capable of classifying between different arm configurations at a rate greater than 95%. Additionally, our method is capable of reducing the end-effector error of quasistatic model only control to within 1 cm of our controller baseline.


Title: Modeling and Analysis of SMA Actuator Embedded in Stretchable Coolant Vascular Pursuing Artificial Muscles
Key Words: coils  coolants  cooling  design engineering  energy consumption  industrial robots  muscle  shape memory effects  springs (mechanical)  SMA coil spring  flexible response soft actuator  coolant circulation system  thermomechanical heat transfer model  maximum actuation frequency  SMA actuator embedded  stretchable coolant vascular pursuing artificial muscles  muscle-like SMA actuator  active cooling system  energy consumption  heating phase  design engineering  robots structure  shape memory alloy  frequency 0.5 Hz  Springs  Coolants  Actuators  Mathematical model  Force  Wires 
Abstract: This paper proposes a muscle-like SMA (Shape Memory Alloy) actuator with an active cooling system for efficient response. An SMA coil spring is embedded into a stretchable coolant vascular for soften structure of robots. In order to design a flexible, lightweight, and fast-response soft actuator with the SMA coil spring and coolant circulation system, a modeling based approach has been conducted. Analysis of coolant effects has been conducted in aspects of heating speed, cooling speed, and energy consumption based on both theoretical and empirical studies. From thermomechanical and heat transfer model between SMA and coolant, the actuation times in the case of heating and cooling phase have been estimated. From experimental results, Mineral oil is selected as the optimal coolant, and the maximum actuation frequency was measured as 0.5Hz for 40% contraction lifting 1kg.


Title: Grasping Unknown Objects by Coupling Deep Reinforcement Learning, Generative Adversarial Networks, and Visual Servoing
Key Words: grippers  learning (artificial intelligence)  neural nets  object recognition  pose estimation  robot vision  visual servoing  semicompliant objects  grasping experiments  deep learning  inaccurate agent gripper  visual servoing grasping task  CycleGAN  DRL  deep reinforcement learning grasping agent  generative adversarial networks  Grasping  Robots  Grippers  Training  Task analysis  Gallium nitride  Image segmentation 
Abstract: In this paper, we propose a novel approach for transferring a deep reinforcement learning (DRL) grasping agent from simulation to a real robot, without fine tuning in the real world. The approach utilises a CycleGAN to close the reality gap between the simulated and real environments, in a reverse real-to-sim manner, effectively "tricking" the agent into believing it is still in the simulator. Furthermore, a visual servoing (VS) grasping task is added to correct for inaccurate agent gripper pose estimations derived from deep learning. The proposed approach is evaluated by means of real grasping experiments, achieving a success rate of 83 % on previously seen objects, and the same success rate for previously unseen, semi-compliant objects. The robustness of the approach is demonstrated by comparing it with two baselines, DRL plus CycleGAN, and VS only. The results clearly show that our approach outperforms both baselines.


Title: Incorporating Motion Planning Feasibility Considerations during Task-Agent Assignment to Perform Complex Tasks Using Mobile Manipulators
Key Words: manipulators  mobile robots  multi-robot systems  path planning  task-agent assignment  complex tasks  multiarm mobile manipulators  multiple robotic agents  expensive motion planning queries  speed-up techniques  spatial constraint checking  conservative surrogates  symbolic conditions  high-DOF robotic agents  Task analysis  Planning  Manipulators  Robot kinematics  Containers  Trajectory 
Abstract: Multi-arm mobile manipulators can be represented as a combination of multiple robotic agents from the perspective of task-assignment and motion planning. Depending upon the task, agents might collaborate or work independently. Integrating motion planning with task-agent assignment is a computationally slow process as infeasible assignments can only be detected through expensive motion planning queries. We present three speed-up techniques for addressing this problem-(1) spatial constraint checking using conservative surrogates for motion planners, (2) instantiating symbolic conditions for pruning infeasible assignments, and (3) efficiently caching and reusing previously generated motion plans. We show that the developed method is useful for real-world operations that require complex interaction and coordination among high-DOF robotic agents.


Title: Learning to Scaffold the Development of Robotic Manipulation Skills
Key Words: learning systems  manipulators  position control  robust control  peg insertion  inaccurate motor control  robotic manipulation skills  shallow-depth insertion  wrench manipulation  robot positions  learning loops  learning system  scaffold manipulation skill learning  robot actions  robust manipulation  Task analysis  Tools  Uncertainty  Robot sensing systems  Robustness  Motor drives 
Abstract: Learning contact-rich, robotic manipulation skills is a challenging problem due to the high-dimensionality of the state and action space as well as uncertainty from noisy sensors and inaccurate motor control. To combat these factors and achieve more robust manipulation, humans actively exploit contact constraints in the environment. By adopting a similar strategy, robots can also achieve more robust manipulation. In this paper, we enable a robot to autonomously modify its environment and thereby discover how to ease manipulation skill learning. Specifically, we provide the robot with fixtures that it can freely place within the environment. These fixtures provide hard constraints that limit the outcome of robot actions. Thereby, they funnel uncertainty from perception and motor control and scaffold manipulation skill learning. We propose a learning system that consists of two learning loops. In the outer loop, the robot positions the fixture in the workspace. In the inner loop, the robot learns a manipulation skill and after a fixed number of episodes, returns the reward to the outer loop. Thereby, the robot is incentivised to place the fixture such that the inner loop quickly achieves a high reward. We demonstrate our framework both in simulation and in the real world on three tasks: peg insertion, wrench manipulation and shallow- depth insertion. We show that manipulation skill learning is dramatically sped up through this way of scaffolding.


Title: Online Replanning in Belief Space for Partially Observable Task and Motion Problems
Key Words: manipulators  mobile robots  motion control  path planning  execution system  deterministic cost-sensitive planning  hybrid belief states  partially observable problems  online replanning  belief space  multistep manipulation tasks  autonomous robot  Planning  Task analysis  Bayes methods  Manipulators  Aerospace electronics  Uncertainty 
Abstract: To solve multi-step manipulation tasks in the real world, an autonomous robot must take actions to observe its environment and react to unexpected observations. This may require opening a drawer to observe its contents or moving an object out of the way to examine the space behind it. Upon receiving a new observation, the robot must update its belief about the world and compute a new plan of action. In this work, we present an online planning and execution system for robots faced with these challenges. We perform deterministic cost-sensitive planning in the space of hybrid belief states to select likely-to-succeed observation actions and continuous control actions. After execution and observation, we replan using our new state estimate. We initially enforce that planner reuses the structure of the unexecuted tail of the last plan. This both improves planning efficiency and ensures that the overall policy does not undo its progress towards achieving the goal. Our approach is able to efficiently solve partially observable problems both in simulation and in a real-world kitchen.


Title: A Code for Unscented Kalman Filtering on Manifolds (UKF-M)
Key Words: Kalman filters  Lie groups  manifolds  nonlinear filters  public domain software  state estimation  UKF-M  Lie groups  state estimation problems  unscented Kalman filtering-manifolds  filtering performance  independent open-source  Python frameworks  Matlab frameworks  online repositories  https://github.com/CAOR-MINES-ParisTech/ukfm  Manifolds  Kalman filters  Two dimensional displays  Dispersion  Robots  Covariance matrices 
Abstract: The present paper introduces a novel methodology for Unscented Kalman Filtering (UKF) on manifolds that extends previous work by the authors on UKF on Lie groups. Beyond filtering performance, the main interests of the approach are its versatility, as the method applies to numerous state estimation problems, and its simplicity of implementation for practitioners not being necessarily familiar with manifolds and Lie groups. We have developed the method on two independent open-source Python and Matlab frameworks we call UKF-M, for quickly implementing and testing the approach. The online repositories contain tutorials, documentation, and various relevant robotics examples that the user can readily reproduce and then adapt, for fast prototyping and benchmarking. The code is available at https://github.com/CAOR-MINES-ParisTech/ukfm.


Title: Efficient and precise sensor fusion for non-linear systems with out-of-sequence measurements by example of mobile robotics
Key Words: delays  linear systems  mobile robots  nonlinear systems  sensor fusion  state estimation  precise sensor fusion  nonlinear systems  out-of-sequence measurements  mobile robotics  precise state estimation  sensor fusion algorithm  sensor fusion approaches  suitable approaches  consumer robot use-case  delays  fusion process  estimation bias  estimation performance  Robot sensing systems  Delays  Current measurement  Sensor fusion  Estimation 
Abstract: For most applications in mobile robotics, precise state estimation is essential. Typically, the state estimation is based on the fusion of data from different sensors. In practice, these sensors differ in their characteristics and measurements are available to the sensor fusion algorithm only with delay. Based on a brief survey of sensor fusion approaches that consider delayed and out-of-sequence availability of measurements, suitable approaches for applications in mobile robotics are identified. In a consumer robot use-case, experiments show that the estimation is biased if delayed availability of measurements is not considered appropriately. However, if delays are considered in the fusion process, the estimation bias is reduced to almost zero and in consequence, the estimation performance is distinctly improved. Two computational favorable approximative methods are described and provide almost the same accuracy as - theoretically optimal - brute-force filter recalculation at much lower and well-distributed computational costs.


Title: UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated Input Degradation
Key Words: learning (artificial intelligence)  neural nets  probability  sensor fusion  uncertainty handling  multimodal fusion  unanticipated input degradation  multiple sensor modalities  deep learning architectures  modality-specific output softmax probabilities  uncertainty measures  uncertainty-scaled output  fusion architectures  probabilistic noisy-fusion  data-dependent spatial temperature scaling  uncertainty-aware fusion  UNO  uncertainty-aware noisy fusion  Uncertainty  Degradation  Training  Noise measurement  Robot sensing systems  Entropy 
Abstract: The fusion of multiple sensor modalities, especially through deep learning architectures, has been an active area of study. However, an under-explored aspect of such work is whether the methods can be robust to degradation across their input modalities, especially when they must generalize to degradation not seen during training. In this work, we propose an uncertainty-aware fusion scheme to effectively fuse inputs that might suffer from a range of known and unknown degradation. Specifically, we analyze a number of uncertainty measures, each of which captures a different aspect of uncertainty, and we propose a novel way to fuse degraded inputs by scaling modality-specific output softmax probabilities. We additionally propose a novel data-dependent spatial temperature scaling method to complement these existing uncertainty measures. Finally, we integrate the uncertainty-scaled output from each modality using a probabilistic noisy-or fusion method. In a photo-realistic simulation environment (AirSim), we show that our method achieves significantly better results on a semantic segmentation task, as compared to state-of-art fusion architectures, on a range of degradation (e.g. fog, snow, frost, and various other types of noise), some of which are unknown during training.


Title: Intermittent GPS-aided VIO: Online Initialization and Calibration
Key Words: calibration  cameras  distance measurement  Global Positioning System  Monte Carlo methods  sensor fusion  GPS reference frame  GPS signal  intermittent GPS-aided VIO  online initialization  IMU-camera data fusion  intermittent GPS measurements  sensor fusion  spatiotemporal sensor calibration  sensor reference frames  online calibration method  reference frame initialization procedure  GPS sensor noise  GPS-VIO system  VIO reference frame  online calibration  robust GPS-aided visual inertial odometry  GPS-IMU extrinsics  Monte-Carlo simulations  large-scale real-world experiment  Global Positioning System  Robot sensing systems  Calibration  Cameras  Cloning  Robustness  Visualization 
Abstract: In this paper, we present an efficient and robust GPS-aided visual inertial odometry (GPS-VIO) system that fuses IMU-camera data with intermittent GPS measurements. To perform sensor fusion, spatiotemporal sensor calibration and initialization of the transform between the sensor reference frames are required. We propose an online calibration method for both the GPS-IMU extrinsics and time offset as well as a reference frame initialization procedure that is robust to GPS sensor noise. In addition, we prove the existence of four unobservable directions of the GPS-VIO system when estimating in the VIO reference frame, and advocate a state transformation to the GPS reference frame for full observability. We extensively evaluate the proposed approach in Monte-Carlo simulations where we investigate the system's robustness to different levels of GPS noise and loss of GPS signal, and additionally study the hyper-parameters used in the initialization procedure. Finally, the proposed system is validated in a large-scale real-world experiment.


Title: A Mathematical Framework for IMU Error Propagation with Applications to Preintegration
Key Words: inertial navigation  mathematical analysis  measurement errors  measurement uncertainty  position measurement  sensor fusion  units (measurement)  inertial navigation equations  log-linearity property  mathematical framework  IMU error propagation  inertial measurement units  Lie group SE  sensors  rotating Earth  centrifugal force  Coriolis effect  Earth  Mathematical model  Force  Robot kinematics  Sensors  Uncertainty 
Abstract: To fuse information from inertial measurement units (IMU) with other sensors one needs an accurate model for IMU error propagation in terms of position, velocity and orientation, a triplet we call extended pose. In this paper we leverage a nontrivial result, namely log-linearity of inertial navigation equations based on the recently introduced Lie group SE2(3), to transpose the recent methodology of Barfoot and Furgale for associating uncertainty with poses (position, orientation) of SE(3) when using noisy wheel speeds, to the case of extended poses (position, velocity, orientation) of SE2(3) when using noisy IMUs. Besides, our approach to extended poses combined with log-linearity property allows revisiting the theory of preintegration on manifolds and reaching a further theoretic level in this field. We show exact preintegration formulas that account for rotating Earth, that is, centrifugal force and Coriolis effect, may be derived as a byproduct.


Title: Radar-Inertial Ego-Velocity Estimation for Visually Degraded Environments
Key Words: inertial navigation  millimetre wave radar  mobile robots  optimisation  radar imaging  radar receivers  visual perception  millimeter-wave radar-on-a-chip sensor  inertial measurement unit  batch optimization  sliding window  radar-inertial ego-velocity estimation  visually degraded environments  mobile robot body-frame velocity  radar-inertial velocity estimates  visual-inertial approach  Robot sensing systems  Radar measurements  Doppler radar  Doppler effect  Estimation  Velocity measurement 
Abstract: We present an approach for estimating the body-frame velocity of a mobile robot. We combine measurements from a millimeter-wave radar-on-a-chip sensor and an inertial measurement unit (IMU) in a batch optimization over a sliding window of recent measurements. The sensor suite employed is lightweight, low-power, and is invariant to ambient lighting conditions. This makes the proposed approach an attractive solution for platforms with limitations around payload and longevity, such as aerial vehicles conducting autonomous exploration in perceptually degraded operating conditions, including subterranean environments. We compare our radar-inertial velocity estimates to those from a visual-inertial (VI) approach. We show the accuracy of our method is comparable to VI in conditions favorable to VI, and far exceeds the accuracy of VI when conditions deteriorate.


Title: Position-based Impedance Control of a 2-DOF Compliant Manipulator for a Facade Cleaning Operation*
Key Words: actuators  ball screws  buildings (structures)  cleaning  compliance control  force control  force measurement  force sensors  manipulators  mobile robots  position control  robust control  service robots  walls  wet-type cleaning manipulator  system stability  control performance  robust force measurement mechanism  position-based impedance control  facade cleaning operation  series elastic actuator  force sensor  contact force  cleaning performance  ball screw mechanism  2-DOF compliant manipulator  manipulator stiffness  force tracking  Manipulators  Force  Cleaning  Buildings  Force measurement  Sea measurements  Impedance 
Abstract: This paper presents the design of a compliant manipulator using a series elastic actuator (SEA) and a mechanism for precisely measuring the force acting on the contact part of the manipulator without using a force sensor. It is important to maintain a constant contact force between the compliant manipulator and the wall in order to guarantee cleaning performance, and the ball screw mechanism is used to adapt to changes in the distance and the angle. Position-based impedance control is used to maintain a constant contact force when the manipulator interacts with the wall of the building, and the results confirm that the system stability is guaranteed when using SEA, regardless of the variation in the actual stiffness of the manipulator. The results of extensive experimentation using the test bench demonstrate the force tracking performance against various types of wall changes using the stiff wet-type cleaning manipulator. The results indicate that the stiffness of SEA affects the force tracking performance and system stability under the condition of the manipulator and environment interaction, and that the system stability and control performance can be improved by applying a robust force measurement mechanism to noise.


Title: Robust, Locally Guided Peg-in-Hole using Impedance-Controlled Robots
Key Words: geometry  position control  robots  robust control  state estimation  torque control  impedance-controlled robots  robust peg-in-hole  locally guided peg-in-hole  autonomous execution  peg-in-hole problem  robotic setup  robust behavior  mesh data  task-specific weight  motion generation step  joint torque measurements  sampling-based state estimation framework  peg-in-hole assembly tasks  Task analysis  Uncertainty  Geometry  Robot sensing systems  Robustness  State estimation 
Abstract: We present an approach for the autonomous, robust execution of peg-in-hole assembly tasks. We build on a sampling-based state estimation framework, in which samples are weighted according to their consistency with the position and joint torque measurements. The key idea is to reuse these samples in a motion generation step, where they are assigned a second task-specific weight. The algorithm thereby guides the peg towards the goal along the configuration space. An advantage of the approach is that the user only needs to provide: the geometry of the objects as mesh data, as well as a rough estimate of the object poses in the workspace, and a desired goal state. Another advantage is that the local, online nature of our algorithm leads to robust behavior under uncertainty. The approach is validated in the case of our robotic setup and under varying uncertainties for the classical peg-in-hole problem subject to two different geometries.


Title: Design of Spatial Admittance for Force-Guided Assembly of Polyhedral Parts in Single Point Frictional Contact
Key Words: force control  friction  industrial manipulators  mechanical contact  robotic assembly  polyhedral parts  frictional contact  frictionless contact  nonlinear equations  error reduction function  frictional cases  misalignment-reducing conditions  contact configurations  force-guided assembly 
Abstract: This paper identifies conditions for designing the appropriate spatial admittance to achieve reliable force-guided assembly of polyhedral parts for cases in which a single frictional contact occurs between the two parts. This work is an extension of previous work in which frictionless contact was considered. This paper presents a way to characterize friction without solving a set of complicated non-linear equations. We show that, by modifying the error reduction function and evaluating the function bounds associated with friction, the procedures developed for frictionless contact apply to the frictional cases. Thus, for bounded misalignments, if an admittance satisfies the misalignment-reducing conditions at a finite number of contact configurations, then the admittance will also satisfy the conditions at all intermediate configurations for any value of friction less than the specified upper bound.


Title: Characterisation of Self-locking High-contraction Electro-ribbon Actuators*
Key Words: electric actuators  microactuators  microsensors  solenoids  contraction variation  electro-ribbon actuator  high-contraction electro-ribbon actuators  fluidically-driven technologies  electrically-driven actuators  high power equivalent  mammalian muscle  dielectrophoretic liquid zipping  low-power-consumption solenoids  valves  Actuators  Electrodes  Dielectric liquids  Force  Insulators  Dielectrics  Permittivity 
Abstract: Actuators are essential devices that exert force and do work. The contraction of an actuator (how much it can shorten) is an important property that strongly influences its applications, especially in engineering and robotics. While high contractions have been achieved by thermally- or fluidically-driven technologies, electrically-driven actuators typically cannot contract by more than 50%. Recently developed electro-ribbon actuators are simple, low cost, scalable electroactive devices powered by dielectrophoretic liquid zipping (DLZ) that exhibit high efficiency (~70%), high power equivalent to mammalian muscle (~100 W/kg), contractions exceeding 99%. We characterise the electro-ribbon actuator and explore contraction variation with voltage and load. We describe the unique self-locking behaviour of the electro-ribbon actuator which could allow for low-power-consumption solenoids and valves. Finally, we show the interdependence of constituent material properties and the important role that material choice plays in maximising performance.


Title: Helically Wrapped Supercoiled Polymer (HW-SCP) Artificial Muscles: Design, Characterization, and Modeling
Key Words: actuators  biomechanics  coils  hysteresis  magnetic hysteresis  manipulator dynamics  medical robotics  muscle  pneumatic actuators  robotic muscles  force generation  force performance  force production  HW-SCP actuators  Preisach hysteresis model  polynomial model  high-performance artificial muscles  helically wrapped supercoiled polymer artificial muscles  strain performance  Actuators  Strain  Force  Mathematical model  Muscles  Robots  Yarn 
Abstract: Supercoiled polymer (SCP) artificial muscles exhibit many desirable properties such as large contractions and high power density. However, their full potential as robotic muscles is challenged by insufficient strain or force generation - non-mandrel-coiled SCP actuators produce up to 10-20% strain; mandrel-coiled SCP actuators often lift up to 10-30g of weight. It is strongly desired but difficult to obtain SCP actuators that produce large strain and large force. In this paper, the design, characterization, and modeling of helically wrapped SCP (HW-SCP) actuators are presented, which can produce up to 40-60% strain and lift more than 90g of weight. By adjusting their configuration parameters, their strain and force performance can be changed. Experiments are conducted to characterize the force production, strain, and speed of HW-SCP actuators. A Preisach hysteresis model and a polynomial model are adopted to accurately capture the actuator behaviors. This work contributes to high-performance artificial muscles.


Title: A Variable Stiffness Soft Continuum Robot Based on Pre-charged Air, Particle Jamming, and Origami
Key Words: bellows  bending  compressive strength  continuum mechanics  design engineering  manipulator dynamics  mobile robots  rigidity  structural design  control systems  pre-charged air  particle jamming  origami structure  internal chambers  spine-like chamber  identical chambers  spine chamber  pressurized air  air chambers  robot expansion-contraction  stiffness variation mechanism  lateral stiffness  axial stiffness  prototype robot  variable stiffness soft continuum robot  Robots  Jamming  Force  Tendons  Electron tubes  Wires  Valves  Soft continuum robot  variable stiffness  pre-charged air  particle jamming  origami 
Abstract: Soft continuum robots have many applications such as medical surgeries, service industries, rescue tasks, and underwater exploration. Flexibility and good accessibility of such robots are the key reasons for their popularity. However, the complexity of their structural design and control systems limit their broader applications. In this paper, a novel variable stiffness soft continuum robot based on pre-charged air, particle jamming, and origami is proposed. The robot is a bellow-like origami structure with internal chambers. A spine-like chamber is filled with particles, and three identical chambers surrounding the spine chamber are filled with pressurized air. When the origami structure is compressed, the particles are jammed by the compression force and the increased pressure of the three air chambers, thus increasing the overall stiffness of the robot. The robot expansion-contraction and bending are controlled by three tendons. An analytical model of the proposed stiffness variation mechanism has been developed. The effects of various parameters on the lateral and axial stiffness of the soft continuum robot have been investigated by experimental studies. A prototype robot has been fabricated to demonstrate grasping operations.


Title: SwarmRail: A Novel Overhead Robot System for Indoor Transport and Mobile Manipulation
Key Words: manipulators  mobile robots  multi-robot systems  rails  robotic manipulator arm overhead  continuous overhead manipulation  rail crossings  robot swarm  mobile SwarmRail units  single rail network  indoor transport  mobile manipulation  omnidirectional mobile platform  rail profiles  rail-structure  overhead robot system  Rails  Wheels  Robot sensing systems  Manipulators  Layout 
Abstract: SwarmRail represents a novel solution to overhead manipulation from a mobile unit that drives in an aboveground rail-structure. The concept is based on the combination of omnidirectional mobile platform and L-shaped rail profiles that form a through-going central gap. This gap makes possible mounting a robotic manipulator arm overhead at the underside of the mobile platform. Compared to existing solutions, SwarmRail enables continuous overhead manipulation while traversing rail crossings. It also can be operated in a robot swarm, as it allows for concurrent operation of a group of mobile SwarmRail units inside a single rail network. Experiments on a first functional demonstrator confirm the functional capability of the concept. Potential fields of applications reach from industry over logistics to vertical farming.


Title: Fast Local Planning and Mapping in Unknown Off-Road Terrain
Key Words: collision avoidance  graph theory  mobile robots  motion control  remotely operated vehicles  SLAM (robots)  trajectory control  off-road terrain  on-line mapping  planning solution  obstacle detection  terrain gradient map  simple cost map  adaptable cost map  optimal paths  control input space  kinematic forward simulation  generated feasible trajectories  optimal trajectory  time operation  frequency 10.0 Hz  frequency 30.0 Hz  Trajectory  Robots  Planning  Aerospace electronics  Microsoft Windows  Three-dimensional displays  Real-time systems 
Abstract: In this paper, we present a fast, on-line mapping and planning solution for operation in unknown, off-road, environments. We combine obstacle detection along with a terrain gradient map to make simple and adaptable cost map. This map can be created and updated at 10 Hz. An A* planner finds optimal paths over the map. Finally, we take multiple samples over the control input space and do a kinematic forward simulation to generated feasible trajectories. Then the most optimal trajectory, as determined by the cost map and proximity to A* path, is chosen and sent to the controller. Our method allows real time operation at rates of 30 Hz. We demonstrate the efficiency of our method in various off-road terrain at high speed.


Title: Scaled Autonomy: Enabling Human Operators to Control Robot Fleets
Key Words: mobile robots  multi-robot systems  telerobotics  utility function  real-world mobile robot navigation  robot fleets control  autonomous robots  human operator  teleoperation  Task analysis  Mathematical model  Navigation  Autonomous robots  Hardware  Predictive models 
Abstract: Autonomous robots often encounter challenging situations where their control policies fail and an expert human operator must briefly intervene, e.g., through teleoperation. In settings where multiple robots act in separate environments, a single human operator can manage a fleet of robots by identifying and teleoperating one robot at any given time. The key challenge is that users have limited attention: as the number of robots increases, users lose the ability to decide which robot requires teleoperation the most. Our goal is to automate this decision, thereby enabling users to supervise more robots than their attention would normally allow for. Our insight is that we can model the user's choice of which robot to control as an approximately optimal decision that maximizes the user's utility function. We learn a model of the user's preferences from observations of the user's choices in easy settings with a few robots, and use it in challenging settings with more robots to automatically identify which robot the user would most likely choose to control, if they were able to evaluate the states of all robots at all times. We run simulation experiments and a user study with twelve participants that show our method can be used to assist users in performing a simulated navigation task. We also run a hardware demonstration that illustrates how our method can be applied to a real-world mobile robot navigation task.


Title: An Actor-Critic Approach for Legible Robot Motion Planner
Key Words: human-robot interaction  learning (artificial intelligence)  mobile robots  path planning  recurrent neural nets  actor-critic approach  legible robot motion planner  human-robot collaboration  human partners  mutual learning  legibility evaluator  policy network  deep reinforcement learning  sequence model  Seq2Seq  motion predictor  maps motion  legible reward  human-subject experiments  real-human data  recurrent neural networks based sequence  Task analysis  Robot motion  Robot kinematics  Biological neural networks  Trajectory  Training 
Abstract: In human-robot collaboration, it is crucial for the robot to make its intentions clear and predictable to the human partners. Inspired by the mutual learning and adaptation of human partners, we suggest an actor-critic approach for a legible robot motion planner. This approach includes two neural networks and a legibility evaluator: 1) A policy network based on deep reinforcement learning (DRL); 2) A Recurrent Neural Networks (RNNs) based sequence to sequence (Seq2Seq) model as a motion predictor; 3) A legibility evaluator that maps motion to legible reward. Through a series of human-subject experiments, we demonstrate that with a simple handicraft function and no real-human data, our method lead to improved collaborative performance against a baseline method and a non-prediction method.


Title: Intuitive 3D Control of a Quadrotor in User Proximity with Pointing Gestures
Key Words: aerospace computing  aircraft control  control engineering computing  helicopters  interactive devices  position control  virtual reality  pointing gestures  quadrotor  pointing ray  push button  joystick control  intuitive 3D control  user proximity  3D piloting task  virtual workspace surfaces  position control  human robot interaction  Three-dimensional displays  Drones  Robot kinematics  Shape  Robot sensing systems  Aerospace electronics 
Abstract: We present an approach for controlling the position of a quadrotor in 3D space using pointing gestures; the task is difficult because it is in general ambiguous to infer where, along the pointing ray, the robot should go. We propose and validate a pragmatic solution based on a push button acting as a simple additional input device which switches between different virtual workspace surfaces. Results of a study involving ten subjects show that the approach performs well on a challenging 3D piloting task, where it compares favorably with joystick control.


Title: Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs
Key Words: belief networks  cognition  cognitive systems  graph theory  inference mechanisms  learning (artificial intelligence)  robots  robot knowledge  human interactions  graphical model  object states  parse graph  single-view spatiotemporal parsing  learned representation  inference algorithm  joint pg  effective reasoning  inference capability  states joint inference  human beliefs  socio-cognitive ability  false-beliefs  individual parse graph  small object tracking dataset  Robots  Cognition  Visualization  Graphical models  Psychology  Noise measurement  Object tracking 
Abstract: Aiming to understand how human (false-)belief- a core socio-cognitive ability-would affect human interactions with robots, this paper proposes to adopt a graphical model to unify the representation of object states, robot knowledge, and human (false-)beliefs. Specifically, a parse graph (pg) is learned from a single-view spatiotemporal parsing by aggregating various object states along the time; such a learned representation is accumulated as the robot's knowledge. An inference algorithm is derived to fuse individual pg from all robots across multi-views into a joint pg, which affords more effective reasoning and inference capability to overcome the errors originated from a single view. In the experiments, through the joint inference over pgs, the system correctly recognizes human (false-)belief in various settings and achieves better cross-view accuracy on a challenging small object tracking dataset.


Title: Audiovisual cognitive architecture for autonomous learning of face localisation by a Humanoid Robot
Key Words: cognitive systems  humanoid robots  human-robot interaction  learning (artificial intelligence)  neural nets  robot vision  audiovisual cognitive architecture  autonomous learning  face localisation  humanoid robot  deep learning algorithms  cognitive framework  audiovisual attention  learning generalization process  machine learning  HRI 
Abstract: Newborn infants are naturally attracted to human faces, a crucial source of information for social interaction. In robotics, acquisition of such information is crucial and social robots should also learn to exhibit such social skill. Deep learning algorithms are valid candidates to address the problem of face localisation. However, a major drawback of these methods is the large amount of data and human supervision needed in the training procedure. In this work, we propose a cognitive architecture to address autonomous learning from raw sensory signals without supervision. We demonstrate the success of our cognitive framework for the task of face localisation. The proposed cognitive architecture builds on existing work and uses audiovisual attention and a proactive stereo vision mechanism to autonomously direct a robot's attentive focus towards human faces. The gathered information is used to incrementally generate a dataset that can be used to train a state-of-the-art deep network. The learning system imitates the typical learning process of infants and enhances the learning generalization process by leveraging on the interaction experience with people. The integration of HRI with machine learning, inspired by early development in humans, constitutes an innovative approach for improving autonomous learning in robots.


Title: Planetary Rover Exploration Combining Remote and In Situ Measurements for Active Spectroscopic Mapping
Key Words: aerospace robotics  learning (artificial intelligence)  mobile robots  path planning  planetary rovers  planetary rover exploration combining remote  active spectroscopic mapping  planetary rover missions  heavy reliance  ground control  real-time information  autonomous mapping  exploration approach  planetary rovers  machine learning model  rover measurements  spectroscopic data  information theory  nonmyopic path  exploration productivity  actual rover  Feature extraction  Extraterrestrial measurements  Robot kinematics  Adaptation models  Productivity  Mars 
Abstract: Maintaining high levels of productivity for planetary rover missions is very difficult due to limited communication and heavy reliance on ground control. There is a need for autonomy that enables more adaptive and efficient actions based on real-time information. This paper presents an autonomous mapping and exploration approach for planetary rovers. We first describe a machine learning model that actively combines remote and rover measurements for mapping. We focus on spectroscopic data because they are commonly used to investigate surface composition. We then incorporate notions from information theory and non-myopic path planning to improve exploration productivity. Finally, we demonstrate the feasibility and successful performance of our approach via spectroscopic investigations of Cuprite, Nevada; a well-studied region of mineralogical and geological interest. We first perform a detailed analysis in simulations, and then validate those results with an actual rover in the field in Nevada.


Title: Magnetic Docking Mechanism for Free-flying Space Robots with Spherical Surfaces
Key Words: aerospace robotics  Gaussian processes  learning (artificial intelligence)  mobile robots  position control  regression analysis  control capability  slider guide  robot position error  magnetic docking mechanism  free-flying space robots  spherical surfaces  autonomous operation  International Space Station  fixed guide mechanism  machine learning technique  Gaussian process regression  GPR technique  Robots  Force  Magnetic levitation  Computer interfaces  Shape  Magnetoelasticity  Task analysis 
Abstract: The autonomous operation of robots in the International Space Station (ISS) is required to maximize the use of limited resources and enable astronauts to concentrate more on valuable tasks. To achieve these goals, we are developing a station where the robot approaches, docks, charges, and then undocks. In this paper, the authors propose a magnetic docking mechanism for free-flying robots with spherical surfaces that makes it possible for a robot to dock securely without requiring highly precise guidance, navigation and control capability. By making use of a slider guide and repelling pairs of magnets, this mechanism can achieve tolerance for larger robot position error as compared with the conventional fixed guide mechanism. The experimental results show that the proposed mechanism can effectively enlarge the acceptable error range of poses, and also reduce acceleration at the moment of impact. We also introduce a model to predict the success or failure of docking from the contact condition of the robot and the guide by using a machine learning technique - Gaussian Process Regression (GPR). The prediction results shows that the learnt model can express the contact condition of successful docking.


Title: Barefoot Rover: a Sensor-Embedded Rover Wheel Demonstrating In-Situ Engineering and Science Extractions using Machine Learning
Key Words: aerospace computing  aerospace control  aerospace robotics  control engineering computing  learning (artificial intelligence)  mobile robots  planetary rovers  planetary surfaces  wheels  barefoot rover  machine learning  2D pressure grid  electrochemical impedance spectroscopy sensor  in-situ sensing  sensor-embedded rover wheel  in-situ engineering  planetary exploration missions  Wheels  Robot sensing systems  Rocks  Instruments  Measurement  Surface impedance  DC motors 
Abstract: In this work, we demonstrate an instrumented wheel concept which utilizes a 2D pressure grid, an electrochemical impedance spectroscopy (EIS) sensor and machine learning (ML) to extract meaningful metrics from the interaction between the wheel and surface terrain. These include continuous slip/skid estimation, balance, and sharpness for engineering applications. Estimates of surface hydration, texture, terrain patterns, and regolith physical properties such as cohesion and angle of internal friction are additionally calculated for science applications. Traditional systems rely on post-processing of visual images and vehicle telemetry to estimate these metrics. Through in-situ sensing, these metrics can be calculated in near real time and made available to onboard science and engineering autonomy applications. This work aims to provide a deployable system for future planetary exploration missions to increase science and engineering capabilities through increased knowledge of the terrain.


Title: Concurrent Parameter Identification and Control for Free-Floating Robotic Systems During On-Orbit Servicing
Key Words: aerospace control  aerospace robotics  Jacobian matrices  mobile robots  motion control  parameter estimation  path planning  position control  concurrent parameter identification  free-floating robotic system  on-orbit servicing  uncertain parameters  fast parameter identification method  accurate parameter estimates  system dynamic properties  control scheme compensates  robotic servicer base  parameter information  transposed Jacobian controller  RW angular momentum disturbance rejection  OOS tasks  Task analysis  Parameter estimation  Aerospace electronics  Manipulator dynamics  Adaptive control 
Abstract: To control a free-floating robotic system with uncertain parameters in OOS tasks with high accuracy, a fast parameter identification method, previously developed by the authors, is enhanced further and used concurrently with a controller. The method provides accurate parameter estimates, without any prior knowledge of any system dynamic properties. This control scheme compensates for the accumulated angular momentum on the reaction wheels (RWs), which acts as a disturbance to the robotic servicer base. While any controller using parameter information can be used, a transposed Jacobian controller, modified to include RW angular momentum disturbance rejection, is employed here. Threedimensional simulations demonstrate the method's validity.


Title: A Dual Quaternion-Based Discrete Variational Approach for Accurate and Online Inertial Parameter Estimation in Free-Flying obots
Key Words: aerospace robotics  attitude control  control system synthesis  linear matrix inequalities  manipulator dynamics  mobile robots  motion control  recursive estimation  robust control  dual quaternion-based discrete variational approach  online inertial parameter estimation  free-flying robots  model-based motion control  rigid body inertial parameter estimation  discrete dual quaternion equations  variational mechanics  linear parameter estimation problem  standard localization algorithms  rotational inertia  linear matrix inequality constraints  pseudoinertia matrix  recursive semidefinite programming  Quaternions  Mathematical model  Parameter estimation  Robots  Linear matrix inequalities  Estimation  Programming 
Abstract: The performance of model-based motion control for free-flying robots relies on accurate estimation of their parameters. In this work, a method of rigid body inertial parameter estimation which relies on a variational approach is presented. Instead of discretizing the continuous equations of motion, discrete dual quaternion equations based on variational mechanics are used to formulate a linear parameter estimation problem. This method depends only on the pose of the rigid body obtained from standard localization algorithms. Recursive semi-definite programming is used to estimate the inertial parameters (mass, rotational inertia and center of mass offset) online. Linear Matrix Inequality constraints based on the pseudo-inertia matrix ensure that the estimates obtained are fully physically consistent. Simulation results demonstrate that this method is robust to disturbances and the produced estimates are at least one order of magnitude more accurate when compared to discretization using finite differences.


Title: AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM
Key Words: calibration  cameras  sensitivity analysis  SLAM (robots)  calibration parameters  calibration sensitivity analysis  joint angle noise  joint angle values  calibration code  dynamic camera clusters  visual SLAM  time-varying set  extrinsic calibration transformations  DCC calibration accuracy  configuration space  pixel re-projection error  fiducial target  dynamic camera cluster  pose-loop error optimization  Cameras  Calibration  Robot vision systems  Simultaneous localization and mapping  Vehicle dynamics  Optimization  Measurement uncertainty 
Abstract: In order to relate information across cameras in a Dynamic Camera Cluster (DCC), an accurate time-varying set of extrinsic calibration transformations need to be determined. Previous calibration approaches rely solely on collecting measurements from a known fiducial target which limits calibration accuracy as insufficient excitation of the gimbal is achieved. In this paper, we improve DCC calibration accuracy by collecting measurements over the entire configuration space of the gimbal and achieve a 10X improvement in pixel re-projection error. We perform a joint optimization over the calibration parameters between any number of cameras and unknown joint angles using a pose-loop error optimization approach, thereby avoiding the need for overlapping fields-of-view. We test our method in simulation and provide a calibration sensitivity analysis for different levels of camera intrinsic and joint angle noise. In addition, we provide a novel analysis of the degenerate parameters in the calibration when joint angle values are unknown, which avoids situations in which the calibration cannot be uniquely recovered. The calibration code will be made available at https://github.com/TRAILab/AC-DCC.


Title: An End-Effector Wrist Module for the Kinematically Redundant Manipulation of Arm-Type Robots
Key Words: collision avoidance  end effectors  industrial robots  motion control  redundant manipulators  end effector path tracking  6-DoF robot  8-DoF robot  redundant robot  kinematically redundant manipulation  industrial arm-type robots  dexterity  roll-pitch-roll wrist configuration  singularity free motion  end effector wrist module  collision avoidance  Wrist  Collision avoidance  Kinematics  Redundancy  Jacobian matrices  Service robots  Kinematically redundant manipulation  wrist module  roll-pitch-yaw  wrist singularity  inverse kinematics 
Abstract: Industrial arm-type robots have multiple degrees-of-freedom (DoFs) and high dexterity but the use of the roll-pitch-roll wrist configuration yields singularities inside the reachable workspace. Excessive joint velocities will occur when encountering these singularities. Arm-type robots currently don't have enough dexterity to move the end-effector path away from the wrist singularities. Robots with redundant DoFs can be used to provide additional dexterity to avoid the singularities and reduce the excessive joint velocity. An end-effector wrist module is proposed to provide two redundant DoFs when interfaced with an existing 6-DoF robot. The new 8-DoF robot has a compact roll-pitch-yaw wrist that has no singularities inside the reachable workspace. The highly redundant robot can also be used to avoid collisions in various directions. Path tracking simulation examples are provided to show the advantages of the proposed design when compared with existing redundant or nonredundant robots. We expect that this module can serve as a cost-effective solution in applications where singularity-free motion or collision-free motion is required.


Title: A Bio-Inspired Transportation Network for Scalable Swarm Foraging
Key Words: collision avoidance  mobile robots  multi-robot systems  swarm intelligence  transportation  interrobot collisions  swarm robot foraging  scale-invariant swarm foraging algorithm  hierarchical branching transportation network  ubiquitous fractal branching networks  bioinspired transportation network  Robots  Transportation  Biology  Collision avoidance  Scalability  Task analysis  Explosions 
Abstract: Scalability is a significant challenge for robot swarms. Generally, larger groups of cooperating robots produce more inter-robot collisions, and in swarm robot foraging, larger search arenas result in larger travel costs. This paper demonstrates a scale-invariant swarm foraging algorithm that ensures that each robot finds and delivers targets to a central collection zone at the same rate regardless of the size of the swarm or the search area. Dispersed mobile depots aggregate locally collected targets and transport them to a central place via a hierarchical branching transportation network. This approach is inspired by ubiquitous fractal branching networks such as tree branches and animal cardiovascular networks that deliver resources to cells and determine the scale and pace of life. We demonstrate that biological scaling laws predict how quickly robots forage in simulations of up to thousands of robots searching over thousands of square meters. We then use biological scaling to predict the capacity of depot robots that overcome scaling constraints to produce scale-invariant robot swarms. We verify the claims for large swarms in simulation and implement a simple depot design in hardware.


Title: Stance Control Inspired by Cerebellum Stabilizes Reflex-Based Locomotion on HyQ Robot
Key Words: compliant mechanisms  feedback  legged locomotion  motion control  predictive control  robot dynamics  robust control  reflex based dynamic locomotion  stance control  cerebellum  legged robotics  central pattern generators  cyclic motion  robotic locomotion  reflex feedback  musculoskeletal simulation models  compliant quadruped robots  predictive control  gravity compensation mechanism  HyQ robot  stability module  robust locomotion  Legged locomotion  Cerebellum  Foot  Stability analysis  Robot kinematics 
Abstract: Advances in legged robotics are strongly rooted in animal observations. A clear illustration of this claim is the generalization of Central Pattern Generators (CPG), first identified in the cat spinal cord, to generate cyclic motion in robotic locomotion. Despite a global endorsement of this model, physiological and functional experiments in mammals have also indicated the presence of descending signals from the cerebellum, and reflex feedback from the lower limb sensory cells, that closely interact with CPGs. To this day, these interactions are not fully understood. In some studies, it was demonstrated that pure reflex-based locomotion in the absence of oscillatory signals could be achieved in realistic musculoskeletal simulation models or small compliant quadruped robots. At the same time, biological evidence has attested the functional role of the cerebellum for predictive control of balance and stance within mammals. In this paper, we promote both approaches and successfully apply reflex-based dynamic locomotion, coupled with a balance and gravity compensation mechanism, on the state-of-art HyQ robot. We discuss the importance of this stability module to ensure a correct foot lift-off and maintain a reliable gait. The robotic platform is further used to test two different architectural hypotheses inspired by the cerebellum. An analysis of experimental results demonstrates that the most biologically plausible alternative also leads to better results for robust locomotion.


Title: Error estimation and correction in a spiking neural network for map formation in neuromorphic hardware
Key Words: error correction  mobile robots  neural chips  neural nets  path planning  pose estimation  SLAM (robots)  error correction  SNN mechanism  neuromorphic device  form-factor neuromorphic chip  spiking neural network  map formation  neuromorphic hardware  neural networks  robot control  error estimation  simultaneous localization and mapping  robot pose estimation  SNN-based SLAM  path integration speed  Neurons  Robots  Sociology  Statistics  Light emitting diodes  Neuromorphics  Synapses 
Abstract: Neuromorphic hardware offers computing platforms for the efficient implementation of spiking neural networks (SNNs) that can be used for robot control. Here, we present such an SNN on a neuromorphic chip that solves a number of tasks related to simultaneous localization and mapping (SLAM): forming a map of an unknown environment and, at the same time, estimating the robot's pose. In particular, we present an SNN mechanism to detect and estimate errors when the robot revisits a known landmark and updates both the map and the path integration speed to reduce the error. The whole system is fully realized in a neuromorphic device, showing the feasibility of a purely SNN-based SLAM, which could be efficiently implemented in a small form-factor neuromorphic chip.


Title: Adaptive Visual Shock Absorber with Visual-based Maxwell Model Using a Magnetic Gear
Key Words: feedback  gears  impact (mechanical)  plastic deformation  position control  robot dynamics  shock absorbers  velocity control  adaptive visual shock absorber  visual-based Maxwell model  feedback control  high-speed visual object tracking  velocity control  object noncontact state  object contact  magnetic gear response  plastic deformation control  Strain  Visualization  Force  Shock absorbers  Magnetic gears  End effectors 
Abstract: In this study, a visual shock absorber capable of adapting to free-fall objects with various weights and speeds is designed and realized. The key element is a magnetic gear to passively absorb shock in the moment of contact, which is difficult for traditional feedback control to deal with. The magnetic gear allows the seamless transfer of control from the non-contact state to the contact state. 1000 Hz high-speed visual object tracking is used for preparation with position and velocity control in the object non-contact state. In the moment of object contact, the high backdrivability of the magnetic gear response by hardware provides high responsiveness to external force. After the impact, the plastic deformation control of a parallel-expressed Maxwell model handles the contact state.


Title: Slip-Based Nonlinear Recursive Backstepping Path Following Controller for Autonomous Ground Vehicles
Key Words: compensation  control nonlinearities  convergence  feedback  feedforward  mobile robots  motion control  nonlinear control systems  observers  path planning  position control  robot dynamics  robot kinematics  robust control  stability  steering systems  variable structure systems  kinematic controller  feedforward slip compensation  variable structure controller  graceful motion  yaw rate commands  backstepping dynamic controller  robust steering commands  output feedback control  autonomous ground vehicles  vehicle steering control  graceful lateral motion  couples yaw-rate based path  steering angle  heading error  slip-based nonlinear recursive backstepping path following controller  observer based sideslip estimates  path following accuracy  error convergence  slip-based kinematic model  dynamic model  path following error  robustness  high gain observer  stability analysis  Kinematics  Vehicle dynamics  Backstepping  Tracking  Dynamics  Tires  Convergence 
Abstract: Path following accuracy and error convergence with graceful motion in vehicle steering control is challenging due to the competing nature of these requirements, especially across a range of operating speeds. This work is founded upon slip-based kinematic and dynamic models, which allow derivation of controllers considering error due to sideslip and the mapping between steering commands and graceful lateral motion. A novel recursive backstepping steering controller is proposed that better couples yaw-rate based path following commands to steering angle and rate. Observer based sideslip estimates are combined with heading error in the kinematic controller to provide feedforward slip compensation. Path following error is compensated by a Variable Structure Controller (VSC) to balance graceful motion, path following error, and robustness. Yaw rate commands are used by a backstepping dynamic controller to generate robust steering commands. A High Gain Observer (HGO) estimates sideslip and yaw rate for output feedback control. Stability analysis is provided and peaking is addressed. Field experimental results evaluate the work and provide comparisons to MPC.


Title: Fast and Safe Path-Following Control using a State-Dependent Directional Metric
Key Words: collision avoidance  control system synthesis  Lyapunov methods  navigation  path planning  stability  safe path-following control  fast navigation  safe autonomous navigation  control policy design  ellipsoidal trajectory  ellipsoidal bounds  control design  local environment geometry  medial obstacles  virtual reference governor system  system safety  Lyapunov-function-based designs  state-dependent directional metric  quadratic state-dependent distance metric  Euclidean-norm design  stability  collision avoidance  Robots  Trajectory  Safety  Measurement  Navigation  Collision avoidance  Aerospace electronics 
Abstract: This paper considers the problem of fast and safe autonomous navigation in partially known environments. Our main contribution is a control policy design based on ellipsoidal trajectory bounds obtained from a quadratic state-dependent distance metric. The ellipsoidal bounds are used to embed directional preference in the control design, leading to system behavior that is adapted to local environment geometry, carefully considering medial obstacles while paying less attention to lateral ones. We use a virtual reference governor system to adaptively follow a desired navigation path, slowing down when system safety may be violated and speeding up otherwise. The resulting controller is able to navigate complex environments faster than common Euclidean-norm and Lyapunov-function-based designs, while retaining stability and collision avoidance guarantees.


Title: Backlash-Compensated Active Disturbance Rejection Control of Nonlinear Multi-Input Series Elastic Actuators
Key Words: active disturbance rejection control  actuators  clutches  compensation  elasticity  force control  gears  manipulator dynamics  nonlinear control systems  position measurement  power transmission (mechanical)  three-term control  hybrid motor-brake-clutch series elastic actuator  positional measurement error  backlash-compensated active disturbance rejection control  nonlinear multiinput series elastic actuators  passive compliance  force-controlled robotic manipulators  elastic element  dedicated torque sensors  deflection control  nonlinear deformation  torque requirements  mechanical backlash  multiinput active disturbance rejection controller  error-based controllers  backlash-compensated ADRC  Actuators  Torque  Springs  Brakes  DC motors  Sea measurements  Hysteresis motors 
Abstract: Series elastic actuators with passive compliance have been gaining increasing popularity in force-controlled robotic manipulators. One of the reasons is the actuator's ability to infer the applied torque by measuring the deflection of the elastic element as opposed to directly with dedicated torque sensors. Proper deflection control is pinnacle to achieve a desired output torque and, therefore, small deviances in positional measurements or a nonlinear deformation can have adverse effects on performance. In applications with larger torque requirements, the actuators typically use gear reductions which inherently result in mechanical backlash. This combined with the nonlinear behaviour of the elastic element and unmodelled dynamics, can severely compromise force fidelity.This paper proposes a backlash compensating active disturbance rejection controller (ADRC) for multi-input series elastic actuators. In addition to proper deflection control, a multiinput active disturbance rejection controller is derived and implemented experimentally to mitigate any unmodelled nonlinearities or perturbations to the plant model. The controller is experimentally validated on a hybrid motor-brake-clutch series elastic actuator and the controller performance is compared against traditional error-based controllers. It is shown that the backlash compensated ADRC outperforms classical PID and ADRC methods and is a viable solution to positional measurement error in elastic actuators.


Title: In-Hand Object Pose Tracking via Contact Feedback and GPU-Accelerated Robotic Simulation
Key Words: coprocessors  graphics processing units  manipulators  object tracking  optimisation  particle filtering (numerical methods)  pose estimation  robot vision  complex contact dynamics  GPU-accelerated parallel robot simulations  sample-based optimizers  contact feedback  robot-object interactions  GPU-accelerated robotic simulation  robot hand  vision-based methods  particle filters  static grasp setting  in-hand object pose tracking  manipulation  physics simulation  forward model  point cloud distance error  Pose estimation  Robot sensing systems  Physics  Heuristic algorithms  Cost function 
Abstract: Tracking the pose of an object while it is being held and manipulated by a robot hand is difficult for vision-based methods due to significant occlusions. Prior works have explored using contact feedback and particle filters to localize in-hand objects. However, they have mostly focused on the static grasp setting and not when the object is in motion, as doing so requires modeling of complex contact dynamics. In this work, we propose using GPU-accelerated parallel robot simulations and derivative-free, sample-based optimizers to track in-hand object poses with contact feedback during manipulation. We use physics simulation as the forward model for robot-object interactions, and the algorithm jointly optimizes for the state and the parameters of the simulations, so they better match with those of the real world. Our method runs in real-time (30Hz) on a single GPU, and it achieves an average point cloud distance error of 6mm in simulation experiments and 13mm in the real-world ones.


Title: Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands
Key Words: image registration  pose estimation  occlusion-aware pose estimation  adaptive hands  manipulation tasks  within-hand manipulation  robot hand  depth-based framework  robust pose estimation  adaptive hand  efficient parallel search  point cloud  robust global registration  object types  object pose hypotheses  short response times  in-hand 6D object pose estimation  Three-dimensional displays  Pose estimation  Robot sensing systems  Robustness  Computational modeling  Solid modeling 
Abstract: Many manipulation tasks, such as placement or within-hand manipulation, require the object's pose relative to a robot hand. The task is difficult when the hand significantly occludes the object. It is especially hard for adaptive hands, for which it is not easy to detect the finger's configuration. In addition, RGB-only approaches face issues with texture-less objects or when the hand and the object look similar. This paper presents a depth-based framework, which aims for robust pose estimation and short response times. The approach detects the adaptive hand's state via efficient parallel search given the highest overlap between the hand's model and the point cloud. The hand's point cloud is pruned and robust global registration is performed to generate object pose hypotheses, which are clustered. False hypotheses are pruned via physical reasoning. The remaining poses' quality is evaluated given agreement with observed data. Extensive evaluation on synthetic and real data demonstrates the accuracy and computational efficiency of the framework when applied on challenging, highly-occluded scenarios for different object types. An ablation study identifies how the framework's components help in performance. This work also provides a dataset for in-hand 6D object pose estimation. Code and dataset are available at: https://github.com/wenbowen123/icra20-hand-object-pose.


Title: Robust 6D Object Pose Estimation by Learning RGB-D Features
Key Words: image colour analysis  learning (artificial intelligence)  manipulators  object detection  optimisation  pose estimation  regression analysis  robot vision  video signal processing  RGB-D features  robotic manipulation  local optimization approach  distance between closest point pairs  rotation ambiguity  symmetric objects  rotation regression  local-optimum problem  object location  point-wise vectors  robust 6D object pose estimation  discrete-continuous formulation  LINEMOD  YCB-Video  Feature extraction  Pose estimation  Three-dimensional displays  Robustness  Uncertainty  Image segmentation  Robots 
Abstract: Accurate 6D object pose estimation is fundamental to robotic manipulation and grasping. Previous methods follow a local optimization approach which minimizes the distance between closest point pairs to handle the rotation ambiguity of symmetric objects. In this work, we propose a novel discrete- continuous formulation for rotation regression to resolve this local-optimum problem. We uniformly sample rotation anchors in SO(3), and predict a constrained deviation from each anchor to the target, as well as uncertainty scores for selecting the best prediction. Additionally, the object location is detected by aggregating point-wise vectors pointing to the 3D center. Experiments on two benchmarks: LINEMOD and YCB-Video, show that the proposed method outperforms state-of-the-art approaches. Our code is available at https://github.com/mentian/object-posenet.


Title: Split Deep Q-Learning for Robust Object Singulation*
Key Words: collision avoidance  grippers  learning systems  manipulators  neurocontrollers  policy learning  split deep Q-learning  robust object singulation  robotic manipulation  robotic applications  grasping techniques  pushing policy  lateral pushing movements  reinforcement learning  optimal push policies  split DQN  target object extraction  Grasping  Task analysis  Clutter  Image segmentation  Robustness  Service robots 
Abstract: Extracting a known target object from a pile of other objects in a cluttered environment is a challenging robotic manipulation task encountered in many robotic applications. In such conditions, the target object touches or is covered by adjacent obstacle objects, thus rendering traditional grasping techniques ineffective. In this paper, we propose a pushing policy aiming at singulating the target object from its surrounding clutter, by means of lateral pushing movements of both the neighboring objects and the target object until sufficient 'grasping room' has been achieved. To achieve the above goal we employ reinforcement learning and particularly Deep Qlearning (DQN) to learn optimal push policies by trial and error. A novel Split DQN is proposed to improve the learning rate and increase the modularity of the algorithm. Experiments show that although learning is performed in a simulated environment the transfer of learned policies to a real environment is effective thanks to robust feature selection. Finally, we demonstrate that the modularity of the algorithm allows the addition of extra primitives without retraining the model from scratch.


Title: 6-DOF Grasping for Target-driven Object Manipulation in Clutter
Key Words: manipulators  object detection  robot vision  6DOF grasping  cluttered environments  manipulator  robotic platform  target-driven object manipulation  point cloud observations  collision checking module  grasp sequence  Clutter  Three-dimensional displays  Grasping  Grippers  Robots  Collision avoidance  Geometry 
Abstract: Grasping in cluttered environments is a fundamental but challenging robotic skill. It requires both reasoning about unseen object parts and potential collisions with the manipulator. Most existing data-driven approaches avoid this problem by limiting themselves to top-down planar grasps which is insufficient for many real-world scenarios and greatly limits possible grasps. We present a method that plans 6-DOF grasps for any desired object in a cluttered scene from partial point cloud observations. Our method achieves a grasp success of 80.3%, outperforming baseline approaches by 17.6% and clearing 9 cluttered table scenes (which contain 23 unknown objects and 51 picks in total) on a real robotic platform. By using our learned collision checking module, we can even reason about effective grasp sequences to retrieve objects that are not immediately accessible. Supplementary video can be found here.


Title: MulRan: Multimodal Range Dataset for Urban Place Recognition
Key Words: geophysical image processing  geophysical techniques  image recognition  mobile robots  object recognition  optical radar  radar imaging  robot vision  multimodal range dataset  radio detection and ranging  light detection and ranging  urban environment  range sensor-based place recognition  6D baseline trajectories  place recognition ground truth  image-format data  time-stamped 1D intensity arrays  polar images  image data  radar place recognition method  LiDAR  longer-range measurements  urban place recognition  MulRan  Laser radar  Radar imaging  Three-dimensional displays  Urban areas  Simultaneous localization and mapping 
Abstract: This paper introduces a multimodal range dataset namely for radio detection and ranging (radar) and light detection and ranging (LiDAR) specifically targeting the urban environment. By extending our workshop paper [1] to a larger scale, this dataset focuses on the range sensor-based place recognition and provides 6D baseline trajectories of a vehicle for place recognition ground truth. Provided radar data support both raw-level and image-format data, including a set of time-stamped 1D intensity arrays and 360° polar images, respectively. In doing so, we provide flexibility between raw data and image data depending on the purpose of the research. Unlike existing datasets, our focus is at capturing both temporal and structural diversities for range-based place recognition research. For evaluation, we applied and validated that our previous location descriptor and its search algorithm [2] are highly effective for radar place recognition method. Furthermore, the result shows that radar-based place recognition outperforms LiDAR-based one exploiting its longer-range measurements. The dataset is available from https://sites.google.com/view/mulran-pr.


Title: GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization
Key Words: cameras  optimisation  pose estimation  robot vision  SLAM (robots)  GPO  global plane optimization  homography estimation  homography decomposition  monocular SLAM initialization  monocular simultaneous localization and mapping problem  camera poses  chessboard dataset  Cameras  Simultaneous localization and mapping  Optimization  Matrix decomposition  Transmission line matrix methods  Estimation  Three-dimensional displays 
Abstract: Initialization is essential to monocular Simultaneous Localization and Mapping (SLAM) problems. This paper focuses on a novel initialization method for monocular SLAM based on planar features. The algorithm starts by homography estimation in a sliding window. It then proceeds to a global plane optimization (GPO) to obtain camera poses and the plane normal. 3D points can be recovered using planar constraints without triangulation. The proposed method fully exploits the plane information from multiple frames and avoids the ambiguities in homography decomposition. We validate our algorithm on the collected chessboard dataset against baseline implementations and present extensive analysis. Experimental results show that our method outperforms the ne-tuned baselines in both accuracy and real-time.


Title: Large-Scale Volumetric Scene Reconstruction using LiDAR
Key Words: cameras  image colour analysis  image reconstruction  image representation  optical radar  large-scale 3D scene reconstruction  autonomous driving  volumetric depth fusion  indoor applications  commodity RGB-D cameras  high reconstruction quality  LiDAR sensors  autonomous cars  large-scale mapping  urban area  meshed representation  real world application  large-scale volumetric scene reconstruction  distance 3.7 km  Three-dimensional displays  Laser radar  Image reconstruction  Graphics processing units  Sensor fusion  Weight measurement 
Abstract: Large-scale 3D scene reconstruction is an important task in autonomous driving and other robotics applications as having an accurate representation of the environment is necessary to safely interact with it. Reconstructions are used for numerous tasks ranging from localization and mapping to planning. In robotics, volumetric depth fusion is the method of choice for indoor applications since the emergence of commodity RGB-D cameras due to its robustness and high reconstruction quality. In this work we present an approach for volumetric depth fusion using LiDAR sensors as they are common on most autonomous cars. We present a framework for large-scale mapping of urban areas considering loop closures. Our method creates a meshed representation of an urban area from recordings over a distance of 3.7km with a high level of detail on consumer graphics hardware in several minutes. The whole process is fully automated and does not need any user interference. We quantitatively evaluate our results from a real world application. Also, we investigate the effects of the sensor model that we assume on reconstruction quality by using synthetic data.


Title: Topological Mapping for Manhattan-like Repetitive Environments
Key Words: convolutional neural nets  graph theory  image representation  optimisation  SLAM (robots)  topology  Manhattan properties  topological graph  unoptimized Pose Graph  topological Manhattan relations  ground-truth Pose Graph  real-world indoor warehouse scenes  Manhattan-like repetitive environments  topological mapping framework  neighbouring nodes  indoor warehouse setting  warehouse topological construct  deep convolutional network  Siamese-style neural network  backend pose graph optimization framework  Manhattan graph aided loop closure relations  Topology  Network topology  Simultaneous localization and mapping  Neural networks  Optimization 
Abstract: We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework.


Title: Robust RGB-D Camera Tracking using Optimal Key-frame Selection
Key Words: cameras  image colour analysis  image motion analysis  image reconstruction  image sequences  integer programming  interpolation  iterative methods  motion estimation  SLAM (robots)  optimal key-frame selection  integer programming  VO method  camera motion  elastic-fusion  discontinuous camera motions  robust RGB-D camera tracking  adaptive visual odometry  TUM benchmark sequences  camera trajectory errors  iterative closed point  Cameras  Robustness  Optimization  Tracking  Iterative closest point algorithm  Robot vision systems  Three-dimensional displays 
Abstract: We propose a novel RGB-D camera tracking system that robustly reconstructs hand-held RGB-D camera sequences. The robustness of our system is achieved by two independent features of our method: adaptive visual odometry (VO) and integer programming-based key-frame selection. Our VO method adaptively interpolates the camera motion results of the direct VO (DVO) and the iterative closed point (ICP) to yield more optimal results than existing methods such as Elastic-Fusion. Moreover, our key-frame selection method locates globally optimum key-frames using a comprehensive objective function in a deterministic manner rather than heuristic or experience-based rules that prior methods mostly rely on. As a result, our method can complete reconstruction even if the camera fails to be tracked due to discontinuous camera motions, such as kidnap events, when conventional systems need to backtrack the scene. We validated our tracking system on 25 TUM benchmark sequences against state-of-the-art works, such as ORBSLAM2, Elastic-Fusion, and DVO SLAM, and experimentally showed that our method has smaller and more robust camera trajectory errors than these systems.


Title: Aggressive Online Control of a Quadrotor via Deep Network Representations of Optimality Principles
Key Words: aircraft control  autonomous aerial vehicles  control engineering computing  helicopters  mobile robots  neural nets  optimisation  time optimal control  power optimality  time optimality  deep neural network  robotic applications  optimality principles  deep network representations  aggressive online control  time-optimal maneuvers  offline optimal control method  aggressive quadrotor control  Trajectory  Optimal control  Stability analysis  Neural networks  Delays  Training  Drones 
Abstract: Optimal control holds great potential to improve a variety of robotic applications. The application of optimal control on-board limited platforms has been severely hindered by the large computational requirements of current state of the art implementations. In this work, we make use of a deep neural network to directly map the robot states to control actions. The network is trained offline to imitate the optimal control computed by a time consuming direct nonlinear method. A mixture of time optimality and power optimality is considered with a continuation parameter used to select the predominance of each objective. We apply our networks (termed G&CNets) to aggressive quadrotor control, first in simulation and then in the real world. We give insight into the factors that influence the `reality gap' between the quadrotor model used by the offline optimal control method and the real quadrotor. Furthermore, we explain how we set up the model and the control structure on-board of the real quadrotor to successfully close this gap and perform time-optimal maneuvers in the real world. Finally, G&CNet's performance is compared to state-of-the-art differential-flatness-based optimal control methods. We show, in the experiments, that G&CNets lead to significantly faster trajectory execution due to, in part, the less restrictive nature of the allowed state-to-input mappings.


Title: Refined Analysis of Asymptotically-Optimal Kinodynamic Planning in the State-Cost Space
Key Words: boundary-value problems  motion control  optimal control  piecewise constant techniques  robot dynamics  trees (mathematics)  asymptotically-optimal kinodynamic planning  state-cost space  AO-RRT  tree-based planner  motion planning  kinodynamic constraints  optimality proof  piecewise-constant control function  two-point boundary-value  Lipschitz-continuity  Trajectory  Planning  Aerospace electronics  Robots  Collision avoidance  Cost function  Space exploration 
Abstract: We present a novel analysis of AO-RRT: a tree-based planner for motion planning with kinodynamic constraints, originally described by Hauser and Zhou (AO-X, 2016). AO-RRT explores the state-cost space and has been shown to efficiently obtain high-quality solutions in practice without relying on the availability of a computationally-intensive two-point boundary-value solver. Our main contribution is an optimality proof for the single-tree version of the algorithm-a variant that was not analyzed before. Our proof only requires a mild and easily-verifiable set of assumptions on the problem and system: Lipschitz-continuity of the cost function and the dynamics. In particular, we prove that for any system satisfying these assumptions, any trajectory having a piecewise-constant control function and positive clearance from the obstacles can be approximated arbitrarily well by a trajectory found by AORRT. We also discuss practical aspects of AORRT and present experimental comparisons of variants of the algorithm.


Title: Robust quadcopter control with artificial vector fields*
Key Words: autonomous aerial vehicles  control system synthesis  helicopters  mobile robots  multi-robot systems  nonlinear control systems  path planning  position control  robust control  time-varying systems  robust quadcopter control  artificial vector fields  path tracking control strategy  control laws  vector field  controlled second order integrator  quadcopter model  input-to-state stable  control inputs  Robots  Vehicle dynamics  Robustness  Convergence  Level set  Mathematical model  Force 
Abstract: This article presents a path tracking control strategy for a quadcopter to follow a time varying curve. The control is based on artificial vector fields. The construction of the field is based on a well known technique in the literature. Next, control laws are developed to impose the behavior of the vector field to a second order integrator model. Finally, control laws are developed to impose the dynamics of the controlled second order integrator to a quadcopter model, which assumes the thrust and the angular rates as input commands. Asymptotic convergence of the whole system is proved by showing that the individual systems in cascade connection are input-to-state stable. We also analyze the influence of norm-bounded disturbances in the control inputs to evaluate the robustness of the controller. We show that bounded disturbances originate limited deviations from the target curve. Simulations and a real robot experiment exemplify and validate the developed theory.


Title: Simulation-Based Reinforcement Learning for Real-World Autonomous Driving
Key Words: image segmentation  learning (artificial intelligence)  road vehicles  traffic engineering computing  simulation-based reinforcement learning  real-world autonomous driving  driving system  real-world vehicle  driving policy  RGB images  single camera  semantic segmentation  synthetic data  real-world data  segmentation network  real-world experiments  sim-to-real policy transfer  real-world performance  Training  Visualization  Learning (artificial intelligence)  Semantics  Robots  Image segmentation  Predictive models 
Abstract: We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.


Title: Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions
Key Words: behavioural sciences computing  human-robot interaction  navigation  path planning  pedestrians  road traffic control  road vehicles  traffic engineering computing  virtual reality  automated vehicle interactions  safe navigation  automated vehicles  pedestrian interactions  human-driven vehicles  HDV  hybrid systems model  constant velocity dynamics  long-term pedestrian trajectory prediction  immersive virtual environment  AV interactions  pedestrian crosswalk behavior analysis  pedestrian crosswalk behavior prediction  gap acceptance behavior  AV motion planning  IVE  Predictive models  Trajectory  Legged locomotion  Vehicle dynamics  Virtual environments  Planning  Dynamics 
Abstract: For safe navigation around pedestrians, automated vehicles (AVs) need to plan their motion by accurately predicting pedestrians' trajectories over long time horizons. Current approaches to AV motion planning around crosswalks predict only for short time horizons (1-2 s) and are based on data from pedestrian interactions with human-driven vehicles (HDVs). In this paper, we develop a hybrid systems model that uses pedestrians' gap acceptance behavior and constant velocity dynamics for long-term pedestrian trajectory prediction when interacting with AVs. Results demonstrate the applicability of the model for long-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further, we compared measures of pedestrian crossing behaviors in the immersive virtual environment (when interacting with AVs) to that in the real world (results of published studies of pedestrians interacting with HDVs), and found similarities between the two. These similarities demonstrate the applicability of the hybrid model of AV interactions developed from an immersive virtual environment (IVE) for real-world scenarios for both AVs and HDVs.


Title: The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset
Key Words: CW radar  distance measurement  FM radar  Global Positioning System  millimetre wave radar  optical radar  road vehicle radar  Oxford Radar RobotCar dataset  radar extension  millimetre-wave FMCW scanning radar data  central Oxford route  truth optimised radar odometry  autonomous vehicles  environmental conditions  sensor modalities  AD 2019 01  urban driving  weather condition  traffic condition  lighting condition  Navtech CTS350-X radar  Velodyne HDL-32E 3D LIDAR  GPS-INS receiver  ori.ox.ac.uk/datasets/radar-robotear-dataset  size 280.0 km  memory size 4.7 TByte  Robot sensing systems  Laser radar  Three-dimensional displays  Azimuth  Calibration 
Abstract: In this paper we present The Oxford Radar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.(/P)(P)The data were gathered in January 2019 over thirty-two traversals of a central Oxford route spanning a total of 280 km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7 TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotear-dataset.


Title: Multi-modal Experts Network for Autonomous Driving
Key Words: computational complexity  control engineering computing  expert systems  inference mechanisms  learning (artificial intelligence)  mobile robots  road vehicles  sensory data  autonomous driving  autonomous vehicles  computational complexity  multistage training procedure  end-to-end learning  multimodal experts network architecture  inference time step  mixed discrete-continuous policy  Laser radar  Feature extraction  Cameras  Training  Autonomous vehicles  Robot sensing systems 
Abstract: End-to-end learning from sensory data has shown promising results in autonomous driving. While employing many sensors enhances world perception and should lead to more robust and reliable behavior of autonomous vehicles, it is challenging to train and deploy such network and at least two problems are encountered in the considered setting. The first one is the increase of computational complexity with the number of sensing devices. The other is the phenomena of network overfitting to the simplest and most informative input. We address both challenges with a novel, carefully tailored multi-modal experts network architecture and propose a multi-stage training procedure. The network contains a gating mechanism, which selects the most relevant input at each inference time step using a mixed discrete-continuous policy. We demonstrate the plausibility of the proposed approach on our 1/6 scale truck equipped with three cameras and one LiDAR.


Title: Localising PMDs through CNN Based Perception of Urban Streets
Key Words: computer vision  convolutional neural nets  feature extraction  Kalman filters  learning (artificial intelligence)  nonlinear filters  object detection  robot vision  common environmental landmarks  point features  higher level information  common vision based approaches  low level hand  EKF framework  practical CNN  typical suburban streets  localiser  PMD  CNN based perception  urban streets  localisation scheme  complementary approaches  outdoor vision based localisation  convolutional neural networks  necessary perceptual information  camera images  lane markings  manhole covers  vector distance  binary image  ground surface boundaries  CNN based detection  novel extended Kalman filter  Feature extraction  Transforms  Cameras  Semantics  Two dimensional displays  Data mining  Three-dimensional displays 
Abstract: The main contribution of this paper is a novel Extended Kalman Filter (EKF) based localisation scheme that fuses two complementary approaches to outdoor vision based localisation. This EKF is aided by a front end consisting of two Convolutional Neural Networks (CNNs) that provide the necessary perceptual information from camera images. The first approach involves a CNN based extraction of information corresponding to artefacts such as curbs, lane markings, and manhole covers to localise on a vector distance transform representation of a binary image of these ground surface boundaries. The second approach involves a CNN based detection of common environmental landmarks such as tree trunks and light poles, which are represented as point features on a sparse map. Utilising CNNs to obtain higher level information about the environment enables this framework to avoid the typical pitfalls of common vision based approaches that use low level hand crafted features for localisation. The EKF framework makes it possible to deal with false positives and missed detections that are inevitable in a practical CNN, to produce a location estimate together with its associated uncertainty. Experiments using a Personal Mobility Device (PMD) driven in typical suburban streets are presented to demonstrate the effectiveness of the proposed localiser.


Title: Hybrid Localization using Model- and Learning-Based Methods: Fusion of Monte Carlo and E2E Localizations via Importance Sampling
Key Words: convolutional neural nets  importance sampling  learning (artificial intelligence)  mobile robots  Monte Carlo methods  neurocontrollers  particle filtering (numerical methods)  path planning  motion model  importance sampling  convolutional neural network  CNN  Monte Carlo localization  particle filter  hybrid localization method  learning-based method  model-based method  E2E localization  MCL  CNN predictions  posterior distributions  Atmospheric measurements  Particle measurements  Proposals  Predictive models  Fuses  Learning systems  Monte Carlo methods 
Abstract: This paper proposes a hybrid localization method that fuses Monte Carlo localization (MCL) and convolutional neural network (CNN)-based end-to-end (E2E) localization. MCL is based on particle filter and requires proposal distributions to sample the particles. The proposal distribution is generally predicted using a motion model. However, because the motion model cannot handle unanticipated errors, the predicted distribution is sometimes inaccurate. The use of other ideal proposal distributions, such as the measurement model, can improve robustness against such unanticipated errors. This technique is called importance sampling (IS). However, it is difficult to sample the particles from such ideal distributions because they are not represented in the closed form. Recent works have proved that CNNs with dropout layers represent the posterior distributions over their outputs conditioned on the inputs and the CNN predictions are equivalent to sampling the outputs from the posterior. Therefore, the proposed method utilizes a CNN to sample the particles and fuses them with MCL via IS. Consequently, the advantages of both MCL and E2E localization can be simultaneously leveraged while preventing their disadvantages. Experiments demonstrate that the proposed method can smoothly estimate the robot pose, similar to the model-based method, and quickly re-localize it from the failures, similar to the learning-based method.


Title: Visual Localization with Google Earth Images for Robust Global Pose Estimation of UAVs
Key Words: autonomous aerial vehicles  distance measurement  Global Positioning System  image filtering  image registration  image sensors  mobile robots  multi-robot systems  pose estimation  rendering (computer graphics)  robot vision  Google Earth images  georeferenced rendered images  dense mutual information technique  outdoor GPS-denied environments  image registrations  gimballed visual odometry pipeline  visual localization  robust global pose estimation  multirotor UAV  typical feature-based localizer  Cameras  Image registration  Three-dimensional displays  Visualization  Earth  Robustness  Google 
Abstract: We estimate the global pose of a multirotor UAV by visually localizing images captured during a flight with Google Earth images pre-rendered from known poses. We metrically localize real images with georeferenced rendered images using a dense mutual information technique to allow accurate global pose estimation in outdoor GPS-denied environments. We show the ability to consistently localize throughout a sunny summer day despite major lighting changes while demonstrating that a typical feature-based localizer struggles under the same conditions. Successful image registrations are used as measurements in a filtering framework to apply corrections to the pose estimated by a gimballed visual odometry pipeline. We achieve less than 1 m and 1° RMSE on a 303 m flight and less than 3 m and 3° RMSE on six 1132 m flights as low as 36 m above ground level conducted at different times of the day from sunrise to sunset.


Title: Adaptive Curriculum Generation from Demonstrations for Sim-to-Real Visuomotor Control
Key Words: computer vision  control engineering computing  learning (artificial intelligence)  shaped reward functions  ACGD  policy transfer  real-world manipulation tasks  sim-to-real visuomotor control  reinforcement learning  adaptive curriculum generation  vision-based control policies  Task analysis  Training  Robots  Trajectory  Learning (artificial intelligence)  Adaptation models  Stacking 
Abstract: We propose Adaptive Curriculum Generation from Demonstrations (ACGD) for reinforcement learning in the presence of sparse rewards. Rather than designing shaped reward functions, ACGD adaptively sets the appropriate task difficulty for the learner by controlling where to sample from the demonstration trajectories and which set of simulation parameters to use. We show that training vision-based control policies in simulation while gradually increasing the difficulty of the task via ACGD improves the policy transfer to the real world. The degree of domain randomization is also gradually increased through the task difficulty. We demonstrate zero-shot transfer for two real-world manipulation tasks: pick-and-stow and block stacking. A video showing the results can be found at https://lmb.informatik.uni-freiburg.de/projects/curriculum/.


Title: Accept Synthetic Objects as Real: End-to-End Training of Attentive Deep Visuomotor Policies for Manipulation in Clutter
Key Words: clutter  computer vision  learning (artificial intelligence)  manipulators  data augmentation procedure  network architectures  implicit attention ASOR-IA  explicit attention ASOR-EA  training data  uncluttered environment  cluttered environments  end-to-end training  attentive deep visuomotor policies  end-to-end train multitask deep visuomotor policies  robotic manipulation  reinforcement learning  end-to-end LfD architectures  Accept Synthetic Objects as Real  Clutter  Robots  Training data  Task analysis  Training  Encoding  Visualization 
Abstract: Recent research demonstrated that it is feasible to end-to-end train multi-task deep visuomotor policies for robotic manipulation using variations of learning from demonstration (LfD) and reinforcement learning (RL). In this paper, we extend the capabilities of end-to-end LfD architectures to object manipulation in clutter. We start by introducing a data augmentation procedure called Accept Synthetic Objects as Real (ASOR). Using ASOR we develop two network architectures: implicit attention ASOR-IA and explicit attention ASOR-EA. Both architectures use the same training data (demonstrations in uncluttered environments) as previous approaches. Experimental results show that ASOR-IA and ASOR-EA succeed in a significant fraction of trials in cluttered environments where previous approaches never succeed. In addition, we find that both ASOR-IA and ASOR-EA outperform previous approaches even in uncluttered environments, with ASOR-EA performing better even in clutter compared to the previous best baseline in an uncluttered environment.


Title: Learning of Exception Strategies in Assembly Tasks
Key Words: humanoid robots  mobile robots  position control  principal component analysis  robotic assembly  humanoid robots  LfD framework  exception strategies  peg-in-hole task  Franka-Emika Panda robot  assembly tasks  assembly policy  Robot sensing systems  Task analysis  Trajectory  Robot kinematics  Databases  Statistical learning 
Abstract: Assembly tasks performed with a robot often fail due to unforeseen situations, regardless of the fact that we carefully learned and optimized the assembly policy. This problem is even more present in humanoid robots acting in an unstructured environment where it is not possible to anticipate all factors that might lead to the failure of the given task. In this work, we propose a concurrent LfD framework, which associates demonstrated exception strategies to the given context. Whenever a failure occurs, the proposed algorithm generalizes past experience regarding the current context and generates an appropriate policy that solves the assembly issue. For this purpose, we applied PCA on force/torque data, which generates low dimensional descriptor of the current context. The proposed framework was validated in a peg-in-hole (PiH) task using Franka-Emika Panda robot.


Title: An Open-Source Framework for Rapid Development of Interactive Soft-Body Simulations for Real-Time Training
Key Words: control engineering computing  force feedback  haptic interfaces  manipulators  medical computing  medical robotics  surgery  telerobotics  virtual reality  real-time simulation  interactive manipulation  human-readable front-end interface  commercially available haptic devices  game controllers  da Vinci Research Kit  real-time haptic feedback  multiuser training  manipulation problems  soft-body manipulation  open-source framework  interactive soft-body simulations  real-time training  master telemanipulators  Visualization  Computational modeling  Real-time systems  Training  Robots  Faces  Three-dimensional displays 
Abstract: We present an open-source framework that provides a low barrier to entry for real-time simulation, visualization, and interactive manipulation of user-specifiable soft-bodies, environments, and robots (using a human-readable front-end interface). The simulated soft-bodies can be interacted by a variety of input interface devices including commercially available haptic devices, game controllers, and the Master Tele-Manipulators (MTMs) of the da Vinci Research Kit (dVRK) with real-time haptic feedback. We propose this framework for carrying out multi-user training, user-studies, and improving the control strategies for manipulation problems. In this paper, we present the associated challenges to the development of such a framework and our proposed solutions. We also demonstrate the performance of this framework with examples of soft-body manipulation and interaction with various input devices.


Title: Towards 5-DoF Control of an Untethered Magnetic Millirobot via MRI Gradient Coils
Key Words: biomedical MRI  medical image processing  medical robotics  microrobots  path planning  surgery  untethered magnetic millirobot  MRI gradient coils  electromagnetic field gradients  magnetic resonance imaging devices  power untethered magnetic robots  MRI devices  magnetic pulling forces  drug delivery  MRI-powered untethered magnetic robots  orientation control  three-dimensional fluids  3-DoF position control  path-planning-based 5-DoF control algorithm  optimal controller  robot manufacturing errors  pitch angle  neutral pitching angle  3D Bezier curves  worst-case path-tracking error  position-tracking error  orientation-tracking error  pitch angles  future MRI-powered active imaging  laser surgery  biopsy robots  Magnetic resonance imaging  Robots  Magnetic devices  Coils  Three-dimensional displays  Force  Medical robotics  miniature robots  magnetic actuation  magnetic resonance imaging  optimal control 
Abstract: Electromagnetic field gradients generated by magnetic resonance imaging (MRI) devices pave the way to power untethered magnetic robots remotely. This innovative use of MRI devices allows exerting magnetic pulling forces on untethered magnetic robots, which could be used for navigation, diagnosis, drug delivery and therapeutic procedures inside a human body. So far, MRI-powered untethered magnetic robots lack simultaneous position and orientation control inside three-dimensional (3D) fluids, and therefore, their control has been limited to 3-DoF position control. In this paper, we present a path-planning-based 5-DoF control algorithm to steer and control an MRI-powered untethered robot's position and orientation simultaneously in 3D workspaces in fluids. Eventhough the simulation results show that the proposed optimal controller can successfully control the robot for 5-DoF, in the experiments, we observe a reduced 5-DoF controllability due to the robot manufacturing errors, which result in pitch angle to remain at around the neutral pitching angle at the steady state. The proposed controller was evaluated to track four different paths (linear, planar-horizontal, planar-vertical and 3D paths) generated by 3D Bezier curves. The worst-case path-tracking error was observed for 3D path-following experiments. For this case, the position-tracking error was 2.7±1.8 mm, and the orientation-tracking error was 13.5± 28.7 and 3.7± 10.2 degrees for yaw and pitch angles, respectively. The overall path is completed within 19.6 seconds with 23.6 mm overall displacement and 61.2 and 41.2 degrees of yaw and pitch angle rotation, respectively. Such robots can be used in future MRI-powered active imaging, laser surgery and biopsy robots inside a fluid-filled stomach type of organs.


Title: Balance of Humanoid Robots in a Mix of Fixed and Sliding Multi-Contact Scenarios
Key Words: approximation theory  humanoid robots  legged locomotion  quadratic programming  humanoid robots  multilegged robots  multicontact setting  desired sliding-task motions  center-of-mass  admissible convex area  contact positions  CoM support area  CSA  appropriate CoM position  multiple fixed sliding contacts  HRP-4 humanoid robot  quadratic programming  QP optimization problems  Humanoid robots  Mathematical model  Friction  Task analysis  Gravity  Humanoid and multi-legged robots  balance  multi-contacts  sliding contacts 
Abstract: This study deals with the balance of humanoid or multi-legged robots in a multi-contact setting where a chosen subset of contacts is undergoing desired sliding-task motions. One method to keep balance is to hold the center-of-mass (CoM) within an admissible convex area. This area is calculated based on the contact positions and forces. We introduce a methodology to compute this CoM support area (CSA) for multiple fixed and intentionally sliding contacts. To select the most appropriate CoM position within CSA, we account for (i) constraints of multiple fixed and sliding contacts, (ii) desired wrench distribution for contacts, and (iii) desired CoM position (eventually dictated by other tasks). These are formulated as a quadratic programming (QP) optimization problems. We illustrate our approach with pushing against a wall and wiping, and conducted experiments using the HRP-4 humanoid robot.


Title: Fast Whole-Body Motion Control of Humanoid Robots with Inertia Constraints
Key Words: humanoid robots  legged locomotion  motion control  position control  predictive control  robot dynamics  reduced five mass model  analytical solution  mass distribution  inertial properties  desired foot positioning  CRB inertia properties  model predictive control  dynamic kicking motion  humanoid robots  inertia constraints  analytical method  whole-body motions  fast whole-body motion control  humanoid open platform robot  desired composite rigid body inertia  Legged locomotion  Foot  Kinematics  Humanoid robots  Hip  Computational modeling  Task analysis 
Abstract: We introduce a new, analytical method for generating whole-body motions for humanoid robots, which approximate the desired Composite Rigid Body (CRB) inertia. Our approach uses a reduced five mass model, where four of the masses are attributed to the limbs and one is used for the trunk. This compact formulation allows for finding an analytical solution that combines the kinematics with mass distribution and inertial properties of a humanoid robot. The positioning of the masses in Cartesian space is then directly used to obtain joint angles with relations based on simple geometry. Motions are achieved through the time evolution of poses generated through the desired foot positioning and CRB inertia properties. As a result, we achieve short computation times in the order of tens of microseconds. This makes the method suited for applications with limited computation resources, or leaving them to be spent on higher-layer tasks such as model predictive control. The approach is evaluated by performing a dynamic kicking motion with an igus® Humanoid Open Platform robot.


Title: SL1M: Sparse L1-norm Minimization for contact planning on uneven terrain
Key Words: integer programming  legged locomotion  linear programming  minimisation  trajectory control  kinematic reachability  contact effectors  quasistatic COM trajectory  quasiflat contacts  contact surfaces  SL1M  uneven terrain  legged locomotion  combinatorial contact selection problem  mixed-integer optimization solvers  sparsity properties  L1 norm minimization techniques  online contact replanning  sparse L1-norm minimization  Silicon  Planning  Foot  Minimization  Kinematics  Legged locomotion 
Abstract: One of the main challenges of planning legged locomotion in complex environments is the combinatorial contact selection problem. Recent contributions propose to use integer variables to represent which contact surface is selected, and then to rely on modern mixed-integer (MI) optimization solvers to handle this combinatorial issue. To reduce the computational cost of MI, we exploit the sparsity properties of L1 norm minimization techniques to relax the contact planning problem into a feasibility linear program. Our approach accounts for kinematic reachability of the center of mass (COM) and of the contact effectors. We ensure the existence of a quasi-static COM trajectory by restricting our plan to quasi-flat contacts. For planning 10 steps with less than 10 potential contact surfaces for each phase, our approach is 50 to 100 times faster that its MI counterpart, which suggests potential applications for online contact re-planning. The method is demonstrated in simulation with the humanoid robots HRP-2 and Talos over various scenarios.


Title: Finding Locomanipulation Plans Quickly in the Locomotion Constrained Manifold
Key Words: end effectors  humanoid robots  legged locomotion  manipulator dynamics  mobile robots  motion control  path planning  robot programming  locomanipulation plans  locomotion constrained manifold  end-effector trajectory  injective locomotion constraint manifold  locomotion scheme  admissible manipulation trajectories  weighted-A* graph search  planner output  contact transitions  path progression trajectory  whole-body kinodynamic locomanipulation plan  locomanipulability region  edge transition feasibility  NASA Valkyrie robot platform  dynamic locomotion approach  example locomanipulation scenarios  divergent-component-of-motion  Trajectory  Task analysis  Foot  Manifolds  Pelvis  Legged locomotion 
Abstract: We present a method that finds locomanipulation plans that perform simultaneous locomotion and manipulation of objects for a desired end-effector trajectory. Key to our approach is to consider an injective locomotion constraint manifold that defines the locomotion scheme of the robot and then using this constraint manifold to search for admissible manipulation trajectories. The problem is formulated as a weighted-A* graph search whose planner output is a sequence of contact transitions and a path progression trajectory to construct the whole-body kinodynamic locomanipulation plan. We also provide a method for computing, visualizing, and learning the locomanipulability region, which is used to efficiently evaluate the edge transition feasibility during the graph search. Numerical simulations are performed with the NASA Valkyrie robot platform that utilizes a dynamic locomotion approach, called the divergent-component-of-motion (DCM), on two example locomanipulation scenarios.


Title: Force-based Control of Bipedal Balancing on Dynamic Terrain with the "Tallahassee Cassie" Robotic Platform
Key Words: force control  humanoid robots  legged locomotion  motion control  PD control  position control  springs (mechanical)  bipedal control  minimal model information  dynamic impacts  walking running controllers  modeling information  force-based control  bipedal balancing  Tallahassee Cassie robotic platform  bipedal robots  force-based double support balancing controller  dynamic terrain scenarios  robotic bipedal platform  minimal information  robot model  individual links  pelvis-centric pelvis positions  commanding pelvis positions  model-free PD controller  Pelvis  Legged locomotion  Foot  Dynamics  Vehicle dynamics  Robot kinematics 
Abstract: Out in the field, bipedal robots need to travel on terrain that is uneven, non-rigid, and sometimes moving beneath their feet. We present a force-based double support balancing controller for such dynamic terrain scenarios for bipedal robots, and test it on the robotic bipedal platform "Tallahassee Cassie." The presented controller relies on minimal information about the robot model, requiring its kinematics and overall weight, but not inertias of individual links or components. The controller is pelvis-centric, commanding pelvis positions in Cartesian space, which a model-free PD controller converts to motor torques in joint space. By commanding forces, torques, and a frontal center of pressure in this fashion, Tallahassee Cassie is capable of balancing on a variety of scenarios, from a lifting/sliding platform, to soft foam, to a sudden drop. These results show the potential for bipedal control to balance successfully despite minimal model information, the presence of large dynamic impacts-e.g., falling through trap door, and soft series-spring deflections. These results motivate future work for walking and running controllers on dynamic terrain with relatively low reliance on modeling information.


Title: Dense r-robust formations on lattices
Key Words: energy consumption  multi-robot systems  network theory (graphs)  cubic lattices  dense r-robust formations  robot networks  malicious robots  defective robots  high energy consumption  communication network  robot formations  square lattices  triangular lattices  Lattices  Robot kinematics  Communication networks  Robustness  Robot sensing systems  Energy consumption 
Abstract: Robot networks are susceptible to fail under the presence of malicious or defective robots. Resilient networks in the literature require high connectivity and large communication ranges, leading to high energy consumption in the communication network. This paper presents robot formations with guaranteed resiliency that use smaller communication ranges than previous results in the literature. The formations can be built on triangular and square lattices in the plane, and cubic lattices in the three-dimensional space. We support our theoretical framework with simulations.


Title: Optimizing Topologies for Probabilistically Secure Multi-Robot Systems
Key Words: combinatorial mathematics  computational complexity  graph theory  matrix algebra  Monte Carlo methods  multi-robot systems  optimisation  set theory  statistical distributions  multirobot system  MRS  robot interactions  probability distribution  optimal solution  rooted k-connections problem  graph transformations  weighted matroid intersection algorithm  edge set  interaction graph  optimal security solution  secure multirobot systems  Robots  Probabilistic logic  Security  Optimization  Topology  Observers  Multi-robot systems 
Abstract: In this paper, we optimize the interaction graph of a multi-robot system (MRS) by maximizing its probability of security while requiring the MRS to have the fewest edges possible. Edges that represent robot interactions exist according to a probability distribution and security is defined using the control theoretic notion of left invertibility. To compute an optimal solution to our problem, we first start by reducing our problem to a variation of the rooted k-connections problem using three graph transformations. Then, we apply a weighted matroid intersection algorithm (WMIA) on matroids defined on the edge set of the interaction graph. Although the optimal solution can be found in polynomial time, MRSs are dynamic and their topologies may change faster than the rate at which the optimal security solution can be found. To cope with dynamic behavior, we present two heuristics that relax optimality but execute with much lower time complexity. Finally, we validate our results through Monte Carlo simulations.


Title: Efficient Communication in Large Multi-robot Networks
Key Words: multi-robot systems  peer-to-peer computing  radiocommunication  communication routing  ground-level communication  multirobot coordination frameworks  multirobot system  multirobot networks  peer-to-peer radio communication  Robot kinematics  Routing  Multi-robot systems  Complexity theory  Relays  Communication networks 
Abstract: To achieve coordination in a multi-robot system, the robots typically resort to some form of communication among each other. In most of the multi-robot coordination frameworks, high-level coordination strategies are studied but `how' the ground-level communication takes place, is assumed to be taken care of by another program. In this paper, we study the communication routing problem for large multi-robot systems where the robots have limited communication ranges. The objective is to send a message from a robot to another in the network, routed through a low number of other robots. To this end, we propose a communication model between any pair of robots using peer-to-peer radio communication. Our proposed model is generic to any type of message and guarantees a low hop routing between any pair of robots in this network. These help the robots to exchange large messages (e.g., multi-spectral images) in a short amount of time. Results show that our proposed approach easily scales up to 1000 robots while drastically reducing the space complexity for maintaining the network information.


Title: CyPhyHouse: A programming, simulation, and deployment toolchain for heterogeneous distributed coordination
Key Words: control engineering computing  control system synthesis  learning (artificial intelligence)  middleware  mobile computing  mobile robots  multi-threading  path planning  program debugging  specification languages  heterogeneous distributed coordination  libraries  development tools  application development processes  mobile computing  machine learning  CyPhyHouse  debugging  distributed mobile robotic applications  distributed applications  Koord programming language  controller design  distributed network protocols  platform-independent middleware  path planning  multithreaded simulator  Koord applications  application code  heterogeneous agents  heterogeneous mobile platforms  design cycles  robotic testbed  distributed task allocation  deployment toolchain  hardware-agnostic application  Robot kinematics  Task analysis  Middleware  Collision avoidance  Python 
Abstract: Programming languages, libraries, and development tools have transformed the application development processes for mobile computing and machine learning. This paper introduces CyPhyHouse-a toolchain that aims to provide similar programming, debugging, and deployment benefits for distributed mobile robotic applications. Users can develop hardware-agnostic, distributed applications using the high-level, event driven Koord programming language, without requiring expertise in controller design or distributed network protocols. The modular, platform-independent middleware of CyPhyHouse implements these functionalities using standard algorithms for path planning (RRT), control (MPC), mutual exclusion, etc. A high-fidelity, scalable, multi-threaded simulator for Koord applications is developed to simulate the same application code for dozens of heterogeneous agents. The same compiled code can also be deployed on heterogeneous mobile platforms. The effectiveness of CyPhyHouse in improving the design cycles is explicitly illustrated in a robotic testbed through development, simulation, and deployment of a distributed task allocation application on in-house ground and aerial vehicles.


Title: Chance Constrained Simultaneous Path Planning and Task Assignment for Multiple Robots with Stochastic Path Costs
Key Words: distributed algorithms  graph theory  multi-robot systems  path planning  probability  stochastic processes  simultaneous path planning  multiple robots  stochastic path costs  stochastic edge costs  robot team  stochastic travel costs  chance-constrained simultaneous task assignment  deterministic simultaneous task assignment  shortest paths  task locations  linear assignment problem  CC-STAP  Robots  Task analysis  Collision avoidance  Path planning  Resource management  Random variables  Planning 
Abstract: We present a novel algorithm for simultaneous task assignment and path planning on a graph (or roadmap) with stochastic edge costs. In this problem, the initially unassigned robots and tasks are located at known positions in a roadmap. We want to assign a unique task to each robot and compute a path for the robot to go to its assigned task location. Given the mean and variance of travel cost of each edge, our goal is to develop algorithms that, with high probability, the total path cost of the robot team is below a minimum value in any realization of the stochastic travel costs. We formulate the problem as a chance-constrained simultaneous task assignment and path planning problem (CC-STAP). We prove that the optimal solution of CC-STAP can be obtained by solving a sequence of deterministic simultaneous task assignment and path planning problems in which the travel cost is a linear combination of mean and variance of the edge cost. We show that the deterministic problem can be solved in two steps. In the first step, robots compute the shortest paths to the task locations and in the second step, the robots solve a linear assignment problem with the costs obtained in the first step. We also propose a distributed algorithm that solves CC-STAP near-optimally. We present simulation results on randomly generated networks and data to demonstrate that our algorithm is scalable with the number of robots (or tasks) and the size of the network.


Title: Optimal Topology Selection for Stable Coordination of Asymmetrically Interacting Multi-Robot Systems
Key Words: integer programming  mathematical programming  motion control  multi-robot systems  topology  stable coordinated motion  robot-to-robot interactions  asymmetric interaction topologies  multirobot motion  mixed integer semidefinite programming  multirobot systems  asymmetric interactions  optimal topology selection  Robot kinematics  Topology  Robot sensing systems  Multi-robot systems  Symmetric matrices  Laplace equations 
Abstract: In this paper, we address the problem of optimal topology selection for stable coordination of multi-robot systems with asymmetric interactions. This problem arises naturally for multi-robot systems that interact based on sensing, e.g., with limited field of view (FOV) cameras. From our previous efforts on motion control in such settings, we have shown that not all interaction topologies yield stable coordinated motion when asymmetry exists. At the same time, not all robot-to-robot interactions are of equal quality, and thus we seek to optimize asymmetric interaction topologies subject to the constraint that the topology yields stable multi-robot motion. In this context, we formulate an optimal topology selection problem (OTSP) as a mixed integer semidefinite programming (MISDP) problem to compute optimal topologies that yield stable coordinated motion. Simulation results are provided to corroborate the effectiveness of the proposed OTSP formulation.


Title: Non-Prehensile Manipulation in Clutter with Human-In-The-Loop
Key Words: manipulators  mobile robots  path planning  human-operator guided planning  low-level planner  fully autonomous sampling-based planners  human-in-the-loop  high-level plan  control-based randomized planning  pushing-based manipulation  clutter  nonprehensile manipulation  Robots  Planning  Clutter  Aerospace electronics  1/f noise  Task analysis  Automation 
Abstract: We propose a human-operator guided planning approach to pushing-based manipulation in clutter. Most recent approaches to manipulation in clutter employs randomized planning. The problem, however, remains a challenging one where the planning times are still in the order of tens of seconds or minutes, and the success rates are low for difficult instances of the problem. We build on these control-based randomized planning approaches, but we investigate using them in conjunction with human-operator input. In our framework, the human operator supplies a high-level plan, in the form of an ordered sequence of objects and their approximate goal positions. We present experiments in simulation and on a real robotic setup, where we compare the success rate and planning times of our human-in-the-loop approach with fully autonomous sampling-based planners. We show that with a minimal amount of human input, the low-level planner can solve the problem faster and with higher success rates.


Title: PuzzleFlex: kinematic motion of chains with loose joints
Key Words: linear programming  mechanical stability  mobile robots  motion control  robot kinematics  tolerance analysis  PuzzleFlex  loose joints  free motions  planar assembly  rigid bodies  local distance constraints  configuration space velocities  linear programming formulation  structural stability perturbation analysis  tolerance analysis  Robots  Kinematics  Analytical models  Shape  Jacobian matrices  Aerospace electronics  Linear programming 
Abstract: This paper presents a method of computing free motions of a planar assembly of rigid bodies connected by loose joints. Joints are modeled using local distance constraints, which are then linearized with respect to configuration space velocities, yielding a linear programming formulation that allows analysis of systems with thousands of rigid bodies. Potential applications include analysis of collections of modular robots, structural stability perturbation analysis, tolerance analysis for mechanical systems, and formation control of mobile robots.


Title: Accurate Vision-based Manipulation through Contact Reasoning
Key Words: control engineering computing  inference mechanisms  manipulators  neural nets  robot vision  state estimation  vision-based manipulation  contact reasoning  contact interactions  motion optimization  state estimation  state representation  neural networks  Shape  Predictive models  Planning  Analytical models  Robots  Computational modeling  Task analysis 
Abstract: Planning contact interactions is one of the core challenges of many robotic tasks. Optimizing contact locations while taking dynamics into account is computationally costly and, in environments that are only partially observable, executing contact-based tasks often suffers from low accuracy. We present an approach that addresses these two challenges for the problem of vision-based manipulation. First, we propose to disentangle contact from motion optimization. Thereby, we improve planning efficiency by focusing computation on promising contact locations. Second, we use a hybrid approach for perception and state estimation that combines neural networks with a physically meaningful state representation. In simulation and real-world experiments on the task of planar pushing, we show that our method is more efficient and achieves a higher manipulation accuracy than previous vision-based approaches.


Title: A Probabilistic Framework for Constrained Manipulations and Task and Motion Planning under Uncertainty
Key Words: geometric programming  manipulators  path planning  stochastic processes  trajectory control  probabilistic framework  constrained manipulations  motion planning  manipulation planning framework  hierarchical structure  logic rules  trajectory optimization  large-scale sequential manipulation  tool-use planning problems  LGP formulation  stochastic domain  posterior path distribution  Gaussian path distribution  logic-geometric programming  Robots  Planning  Trajectory optimization  Skeleton  Programming 
Abstract: Logic-Geometric Programming (LGP) is a powerful motion and manipulation planning framework, which represents hierarchical structure using logic rules that describe discrete aspects of problems, e.g., touch, grasp, hit, or push, and solves the resulting smooth trajectory optimization. The expressive power of logic allows LGP for handling complex, large-scale sequential manipulation and tool-use planning problems. In this paper, we extend the LGP formulation to stochastic domains. Based on the control-inference duality, we interpret LGP in a stochastic domain as fitting a mixture of Gaussians to the posterior path distribution, where each logic pro le defines a single Gaussian path distribution. The proposed framework enables a robot to prioritize various interaction modes and to acquire interesting behaviors such as contact exploitation for uncertainty reduction, eventually providing a composite control scheme that is reactive to disturbance.


Title: Planning with Selective Physics-based Simulation for Manipulation Among Movable Objects
Key Words: computer simulation  manipulators  path planning  planning model  robot manipulator  planning with selective physics based simulation  Planning  Collision avoidance  Manipulators  Physics  Computational modeling  Task analysis 
Abstract: Use of physics-based simulation as a planning model enables a planner to reason and generate plans that involve non-trivial interactions with the world. For example, grasping a milk container out of a cluttered refrigerator may involve moving a robot manipulator in between other objects, pushing away the ones that are moveable and avoiding interactions with certain fragile containers. A physics-based simulator allows a planner to reason about the effects of interactions with these objects and to generate a plan that grasps the milk container successfully. The use of physics-based simulation for planning however is underutilized. One of the reasons for it being that physics-based simulations are typically way too slow for being used within a planning loop that typically requires tens of thousands of actions to be evaluated within a matter of a second or two. In this work, we develop a planning algorithm that tries to address this challenge. In particular, it builds on the observation that only a small number of actions actually need to be simulated using physics, and the remaining set of actions, such as moving an arm around obstacles, can be evaluated using a much simpler internal planning model, e.g., a simple collision-checking model. Motivated by this, we develop an algorithm called Planning with Selective Physics-based Simulation that automatically discovers what should be simulated with physics and what can utilize an internal planning model for pick-and-place tasks.


Title: Deep Depth Fusion for Black, Transparent, Reflective and Texture-Less Objects
Key Words: cameras  image colour analysis  image fusion  iterative methods  robot vision  stereo image processing  black objects  stereo cameras  depth values  structured-light camera  light path  depth fusion model  high-quality point clouds  short-range robotic applications  fusion weights  depth images  fused depth  depth prediction  stereo model  iterative closest point algorithm  deep depth fusion  transparent objects  reflective objects  Cameras  Robot vision systems  Three-dimensional displays  Color  Image color analysis  Prediction algorithms 
Abstract: Structured-light and stereo cameras, which are widely used to construct point clouds for robotic applications, have different limitations on estimating depth values. Structured-light cameras fail in black, transparent, and reflective objects, which influence the light path; stereo cameras fail in texture-less objects. In this work, we propose a depth fusion model that complements these two types of methods to generate high-quality point clouds for short-range robotic applications. The model first determines the fusion weights from the two input depth images and then refines the fused depth using color features. We construct a dataset containing the aforementioned challenging objects and report the performance of our proposed model. The results reveal that our method reduces the average L1 distance on depth prediction by 75% and 52% compared with the original depth output of the structured-light camera and the stereo model, respectively. A noticeable improvement on the Iterative Closest Point (ICP) algorithm can be achieved by using the refined depth images output from our method.


Title: LiDAR-enhanced Structure-from-Motion
Key Words: cameras  image enhancement  image matching  image texture  motion estimation  optical radar  radar imaging  stereo image processing  rotating LiDAR  stereo camera pair  sensor motions  image matching  maturing technique  inspection purposes  LiDAR-enhanced structure-from-motion estimation  LiDAR-enhanced SfM pipeline algorithms  pipeline  Laser radar  Cameras  Pipelines  Three-dimensional displays  Visualization  Robustness  Robot sensing systems 
Abstract: Although Structure-from-Motion (SfM) as a maturing technique has been widely used in many applications, state-of-the-art SfM algorithms are still not robust enough in certain situations. For example, images for inspection purposes are often taken in close distance to obtain detailed textures, which will result in less overlap between images and thus decrease the accuracy of estimated motion. In this paper, we propose a LiDAR-enhanced SfM pipeline that jointly processes data from a rotating LiDAR and a stereo camera pair to estimate sensor motions. We show that incorporating LiDAR helps to effectively reject falsely matched images and significantly improve the model consistency in large-scale environments. Experiments are conducted in different environments to test the performance of the proposed pipeline and comparison results with the state-of-the-art SfM algorithms are reported.


Title: Low Latency And Low-Level Sensor Fusion For Automotive Use-Cases
Key Words: belief networks  image fusion  object detection  sensor synchronization  multiple sensors  object detection  low-level sensor fusion  automotive use-cases  probabilistic low level automotive sensor fusion approach  camera data  associated data  sensor modalities  probabilistic fusion  association method  Cameras  Three-dimensional displays  Object detection  Sensor fusion  Robot sensing systems  Two dimensional displays  Radar  sensor fusion  object detection  Bayesian networks 
Abstract: This work proposes a probabilistic low level automotive sensor fusion approach using LiDAR, RADAR and camera data. The method is stateless and directly operates on associated data from all sensor modalities. Tracking is not used, in order to reduce the object detection latency and create existence hypotheses per frame. The probabilistic fusion uses input from 3D and 2D space. An association method using a combination of overlap and distance metrics, avoiding the need for sensor synchronization is proposed. A Bayesian network executes the sensor fusion. The proposed approach is compared with a state of the art fusion system, which is using multiple sensors of the same modality and relies on tracking for object detection. Evaluation was done using low level sensor data recorded in an urban environment. The test results show that the low level sensor fusion reduces the object detection latency.


Title: Robot-Assisted and Wearable Sensor-Mediated Autonomous Gait Analysis§
Key Words: gait analysis  kinematics  medical computing  mobile robots  regression analysis  support vector machines  autonomous gait analysis system  mobile robot  custom-engineered instrumented insoles  on-board RGB-D sensor  inertial sensors  force sensitive resistors  robot companion  walking exercises  support vector regression models  fundamental kinematic gait parameters  optical motion capture system  SVR models  autonomous mobile robots  out-of-the-lab gait analysis  robot-assisted wearable sensor-mediated autonomous gait analysis  Robot sensing systems  Legged locomotion  Task analysis  Instruments  Robot kinematics  Wearable Technology  Instrumented Footwear  Gait Analysis  Assistive Robotics  SportSole 
Abstract: In this paper, we propose an autonomous gait analysis system consisting of a mobile robot and custom-engineered instrumented insoles. The robot is equipped with an on-board RGB-D sensor, the insoles feature inertial sensors and force sensitive resistors. This system is motivated by the need for a robot companion to engage older adults in walking exercises. Support vector regression (SVR) models were developed to extract accurate estimates of fundamental kinematic gait parameters (i.e., stride length, velocity, foot clearance, and step length), from data collected with the robot's on-board RGB-D sensor and with the instrumented insoles during straight walking and turning tasks. The accuracy of each model was validated against ground-truth data measured by an optical motion capture system with N=10 subjects. Results suggest that the combined use of wearable and robot's sensors yields more accurate gait estimates than either sub-system used independently. Additionally, SVR models are robust to inter-subject variability and type of walking task (i.e., straight walking vs. turning), thereby making it unnecessary to collect subject-specific or task-specific training data for the models. These findings indicate the potential of the synergistic use of autonomous mobile robots and wearable sensors for accurate out-of-the-lab gait analysis.


Title: A Control Framework Definition to Overcome Position/Interaction Dynamics Uncertainties in Force-Controlled Tasks
Key Words: control engineering computing  damping  force control  image colour analysis  image sensors  industrial manipulators  pose estimation  position control  production engineering computing  control framework definition  force-controlled tasks  industrial robots  increasing autonomy  manipulator  working environment  robust behavior  industrial interaction tasks  uncertain working scenes  6D pose estimation  featureless parts  variable damping impedance controller  adaptive saturation PI  outer loop  high accuracy force control  force error  overshoots avoidance  task uncertainties  positioning errors  assembly task  force-tracking task  force overshoots  reference force  Force  Pose estimation  Impedance  Task analysis  Three-dimensional displays  Robots  Damping 
Abstract: Within the Industry 4.0 context, industrial robots need to show increasing autonomy. The manipulator has to be able to react to uncertainties/changes in the working environment, displaying a robust behavior. In this paper, a control framework is proposed to perform industrial interaction tasks in uncertain working scenes. The proposed methodology relies on two components: i) a 6D pose estimation algorithm aiming to recognize large and featureless parts; ii) a variable damping impedance controller (inner loop) enhanced by an adaptive saturation PI (outer loop) for high accuracy force control (i.e., zero steady-state force error and force overshoots avoidance). The proposed methodology allows to be robust w.r.t. task uncertainties (i.e. , positioning errors and interaction dynamics). The proposed approach has been evaluated in an assembly task of a side-wall panel to be installed inside the aircraft cabin. As a test platform, the KUKA iiwa 14 R820 has been used together with the Microsoft Kinect 2.0 as RGB-D sensor. Experiments show the reliability in the 6D pose estimation and the high-performance in the force-tracking task, avoiding force overshoots while achieving the tracking of the reference force.


Title: Identification of Compliant Contact Parameters and Admittance Force Modulation on a Non-stationary Compliant Surface
Key Words: adaptive control  force control  manipulators  medical robotics  motion control  parameter estimation  position control  surgery  position-based adaptive force controller  interaction forces  nonstationary environments  fast parameter estimation  compliant contact parameters  admittance force modulation  nonstationary compliant surface  safety-critical applications  mechanical probing strategy  environmental impedance parameters  compliant environments  robotic manipulator autonomous control  manipulator controller design  surgical tasks  motion compensation  Force  Impedance  Force control  Probes  Surface impedance  Manipulators 
Abstract: Although autonomous control of robotic manipulators has been studied for several decades, they are not commonly used in safety-critical applications due to lack of safety and performance guarantees - many of them concerning the modulation of interaction forces. This paper presents a mechanical probing strategy for estimating the environmental impedance parameters of compliant environments, independent a manipulator's controller design, and configuration. The parameter estimates are used in a position-based adaptive force controller to enable control of interaction forces in compliant, stationary, and non-stationary environments. This approach is targeted for applications where the workspace is constrained and non-stationary, and where force control is critical to task success. These applications include surgical tasks involving manipulation of compliant, delicate, moving tissues. Results show fast parameter estimation and successful force modulation that compensates for motion.


Title: Force Adaptation in Contact Tasks with Dynamical Systems
Key Words: adaptive control  force control  learning systems  manipulator dynamics  motion control  radial basis function networks  robots  uncertain systems  online adaptation  state-dependent force correction model  force error  collaborative cleaning task  task adaptation  adaptive force control  reactive behaviours  adaptive behaviours  force adaptation  contact tasks  robot dynamics  force tracking accuracy  compensation model  adaptive framework  force generation  time-invariant dynamical system framework  radial basis functions  KUKA LWR IV+ robotic arm  Force  Robots  Task analysis  Surface impedance  Dynamics  Impedance  Tracking  Force Control  Compliance and Impedance Control  Physical Human-Robot Interaction 
Abstract: In many tasks such as finishing operations, achieving accurate force tracking is essential. However, uncertainties in the robot dynamics and the environment limit the force tracking accuracy. Learning a compensation model for these uncertainties to reduce the force error is an effective approach to overcome this limitation. However, this approach requires an adaptive and robust framework for motion and force generation. In this paper, we use the time-invariant Dynamical System (DS) framework for force adaptation in contact tasks. We propose to improve force tracking accuracy through online adaptation of a state-dependent force correction model encoded with Radial Basis Functions (RBFs). We evaluate our method with a KUKA LWR IV+ robotic arm. We show its efficiency to reduce the force error to a negligible amount with different target forces and robot velocities. Furthermore, we study the effect of the hyper-parameters and provide a guideline for their selection. We showcase a collaborative cleaning task with a human by integrating our method to previous works to achieve force, motion, and task adaptation at the same time. Thereby, we highlight the benefits of using adaptive force control in real-world environments where we need reactive and adaptive behaviours in response to interactions with the environment.


Title: Who2com: Collaborative Perception via Learnable Handshake Communication
Key Words: aircraft communication  autonomous aerial vehicles  image segmentation  learning (artificial intelligence)  multi-agent systems  multi-robot systems  neural nets  visual perception  degraded sensor data  compressed request  aerial robots  semantic segmentation task  collaborative perception  learnable handshake communication  local observations  neighboring agents  multiagent reinforcement learning  bandwidth-sensitive manner  scene understanding tasks  communication protocols  multistage handshake communication mechanism  neural network  Who2com  AirSim simulator  AirSim-CP dataset  Task analysis  Bandwidth  Semantics  Training  Collaboration  Robot sensing systems 
Abstract: In this paper, we propose the problem of collaborative perception, where robots can combine their local observations with those of neighboring agents in a learnable way to improve accuracy on a perception task. Unlike existing work in robotics and multi-agent reinforcement learning, we formulate the problem as one where learned information must be shared across a set of agents in a bandwidth-sensitive manner to optimize for scene understanding tasks such as semantic segmentation. Inspired by networking communication protocols, we propose a multi-stage handshake communication mechanism where the neural network can learn to compress relevant information needed for each stage. Specifically, a target agent with degraded sensor data sends a compressed request, the other agents respond with matching scores, and the target agent determines who to connect with (i.e., receive information from). We additionally develop the AirSim-CP dataset and metrics based on the AirSim simulator where a group of aerial robots perceive diverse landscapes, such as roads, grasslands, buildings, etc. We show that for the semantic segmentation task, our handshake communication method significantly improves accuracy by approximately 20% over decentralized baselines, and is comparable to centralized ones using a quarter of the bandwidth.


Title: Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM
Key Words: data visualisation  image representation  learning (artificial intelligence)  mobile robots  SLAM (robots)  view-based labelling  spatial AI systems  real-time height map fusion  map-based labelling  generated scene model  input view-wise data  estimate labels  clear groups  labelling scenes  semantic labels  geometric models  persistent scene representations  real-time SLAM  map-based semantic labelling  Labeling  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Image reconstruction  Real-time systems 
Abstract: Generally capable Spatial AI systems must build persistent scene representations where geometric models are combined with meaningful semantic labels. The many approaches to labelling scenes can be divided into two clear groups: view-based which estimate labels from the input view-wise data and then incrementally fuse them into the scene model as it is built; and map-based which label the generated scene model. However, there has so far been no attempt to quantitatively compare view-based and map-based labelling. Here, we present an experimental framework and comparison which uses real-time height map fusion as an accessible platform for a fair comparison, opening up the route to further systematic research in this area.


Title: Generative Modeling of Environments with Scene Grammars and Variational Inference
Key Words: grammars  inference mechanisms  mobile robots  object detection  probability  trees (mathematics)  variational inference algorithm  observed environments  nontrivial manipulation-relevant datasets  scene grammars  discrete variables  continuous variables  probabilistic generative model  scene trees  hierarchical relationships  labeled parse trees  Grammar  Robots  Production  Testing  Training  Random variables  Probabilistic logic 
Abstract: In order to understand how a robot will perform in the open world, we aim to establish a quantitative understanding of the distribution of environments that a robot will face when when it is deployed. However, even restricting attention only to the distribution of objects in a scene, these distributions over environments are nontrivial: they describe mixtures of discrete and continuous variables related to the number, type, poses, and attributes of objects in the scene. We describe a probabilistic generative model that uses scene trees to capture hierarchical relationships between collections of objects, as well as a variational inference algorithm for tuning that model to best match a set of observed environments without any need for tediously labeled parse trees. We demonstrate that this model can accurately capture the distribution of a pair of nontrivial manipulation-relevant datasets and be deployed as a density estimator and outlier detector for novel environments.


Title: SHOP-VRB: A Visual Reasoning Benchmark for Object Perception
Key Words: control engineering computing  image representation  inference mechanisms  manipulators  natural language processing  query processing  robot vision  text analysis  object perception  robotics applications  object grasping  object properties  visual text data  household objects  natural language descriptions  question-answer pairs  visual reasoning queries  scene semantic representations  symbolic program execution  disentangled representation  visual inputs  textual inputs  symbolic programs  reasoning process  SHOP-VRB  visual reasoning benchmark  object manipulation  Visualization  Cognition  Benchmark testing  Robots  Image color analysis  Task analysis  Plastics 
Abstract: In this paper we present an approach and a benchmark for visual reasoning in robotics applications, in particular small object grasping and manipulation. The approach and benchmark are focused on inferring object properties from visual and text data. It concerns small household objects with their properties, functionality, natural language descriptions as well as question-answer pairs for visual reasoning queries along with their corresponding scene semantic representations. We also present a method for generating synthetic data which allows to extend the benchmark to other objects or scenes and propose an evaluation protocol that is more challenging than in the existing datasets. We propose a reasoning system based on symbolic program execution. A disentangled representation of the visual and textual inputs is obtained and used to execute symbolic programs that represent a 'reasoning process' of the algorithm. We perform a set of experiments on the proposed benchmark and compare to results from the state of the art methods. These results expose the shortcomings of the existing benchmarks that may lead to misleading conclusions on the actual performance of the visual reasoning systems.


Title: Sensorization of a Continuum Body Gripper for High Force and Delicate Object Grasping
Key Words: closed loop systems  deformation  dexterous manipulators  feedback  grippers  tactile sensors  delicate object grasping  universal grasping  heavy bulky items  lightweight delicate objects  highly flexible latex bladders  Magic Ball origami gripper  tactile sensing  proprioceptive sensing  sensor feedback  closed loop controller  continuum body gripper sensorization  high force object grasping  Grippers  Force  Robot sensing systems  Strain  Bladder  Mechanical sensors 
Abstract: The goal of achieving `universal grasping' where many objects can be handled with minimal control input is the focus of much research due to potential high impact applications ranging from grocery packing to recycling. However, many of the grippers developed suffer from limited sensing capabilities which can prevent handing of both heavy bulky items and also lightweight delicate objects which require fine control when grasping. Sensorizing such grippers is often challenging due to the highly deformable surfaces. We propose a novel sensing approach which uses highly flexible latex bladders. By measuring changes in the air pressure of the bladders, normal force and longitudinal strain can be measured. These sensors have been integrated into a `Magic Ball' origami gripper to provide both tactile and proprioceptive sensing. The sensors show reasonable sensitivity and repeatability, are durable and low-cost, and can be easily integrated into the gripper without affecting performance. When the sensors are used for classification, they enabled identification of 10 objects with over 90% accuracy, and also allow failure to be detected through slippage detection. A control algorithm has been developed which uses the sensor feedback to extend the capabilities of the gripper to include both delicate and strong grasping. It is shown that this closed loop controller enables delicate grasping of potato chips; 80% of those tested were grasped without damage.


Title: A Soft Gripper with Retractable Nails for Advanced Grasping and Manipulation
Key Words: dexterous manipulators  grippers  manipulator dynamics  path planning  pneumatic actuators  manipulation tasks  normal grasping forces  delicate pinch grasps  robotic grasping tasks  soft gripper  advanced grasping  retractable finger nails  reconfigurable palm  finger nail mechanism  Nails  Grippers  Grasping  Three-dimensional displays  Actuators  Payloads  Fingers 
Abstract: This study describes the enhancement of a vacuum actuated soft gripper's grasping capabilities using retractable finger nails and an active re-configurable palm. The finger nail mechanism is pneumatically actuated and enables the gripper to perform complex grasping and manipulation tasks with high repeatability. The retracted nails can exert normal grasping forces of up to 1.8N and enable grasping of objects up to 200μm thick from flat surfaces, while allowing the gripper to execute delicate pinch grasps without complex trajectory or grasp planning. A wide array of robotic grasping tasks that were not possible without nails are also described.


Title: Towards Proactive Navigation: A Pedestrian-Vehicle Cooperation Based Behavioral Model
Key Words: collision avoidance  mobile robots  motion control  path planning  remotely operated vehicles  road traffic control  road vehicles  trajectory control  pedestrian-vehicle cooperation  autonomous vehicle  intelligent transportation  autonomous navigation research  safe proactive navigation  pedestrian-vehicle interaction behavioral model  interaction scenario  quantitative time-varying function  cooperation estimation  cooperation-based trajectory planning model  Navigation  Space vehicles  Predictive models  Strain  Task analysis  Trajectory  Autonomous vehicles 
Abstract: Developing autonomous vehicles capable of navigating safely and socially around pedestrians is a major challenge in intelligent transportation. This challenge cannot be met without understanding pedestrians' behavioral response to an autonomous vehicle, and the task of building a clear and quantitative description of the pedestrian to vehicle interaction remains a key milestone in autonomous navigation research. As a step towards safe proactive navigation in a space shared with pedestrians, this work introduces a pedestrian-vehicle interaction behavioral model. The model estimates the pedestrian's cooperation with the vehicle in an interaction scenario by a quantitative time-varying function. Using this cooperation estimation the pedestrian's trajectory is predicted by a cooperation-based trajectory planning model. Both parts of the model are tested and validated using real-life recorded scenarios of pedestrian-vehicle interaction. The model is capable of describing and predicting agents' behaviors when interacting with a vehicle in both lateral and frontal crossing scenarios.


Title: Studying Navigation as a Form of Interaction: a Design Approach for Social Robot Navigation Methods*
Key Words: human-robot interaction  mobile robots  motion control  navigation  social robot navigation methods  social navigation methods  human sciences fields  mobile robot navigation  robot behavior  social hierarchy  socio-physical context  robot motion  human-robot interaction  human behavior  Navigation  Biological system modeling  Robot sensing systems  Mobile robots  Design methodology  Task analysis 
Abstract: Social Navigation methods attempt to integrate knowledge from Human Sciences fields such as the notion of Proxemics into mobile robot navigation. They are often evaluated in simulations, or lab conditions with informed participants, and studies of the impact of the robot behavior on humans are rare. Humans communicate and interact through many vectors, among which are motion and positioning, which can be related to social hierarchy and the socio-physical context. If a robot is to be deployed among humans, the methods it uses should be designed with this in mind. This work acts as the first step in an ongoing project in which we explore how to design navigation methods for mobile robots destined to be deployed among humans. We aim to consider navigation as more than just a functionality of the robot, and to study the impact of robot motion on humans. In this paper, we focus on the person-following task. We selected a state of the art person-following method as the basis for our method, which we modified and extended in order for it to be more general and adaptable. We conducted pilot experiments using this method on a real mobile robot in ecological contexts. We used results from the experiments to study the Human-Robot Interaction as a whole by analysing both the person-following method and the human behavior. Our preliminary results show that the way in which the robot followed a person had an impact on the interaction that emerged between them.


Title: Robot Plan Model Generation and Execution with Natural Language Interface*
Key Words: human computer interaction  mobile robots  natural language interfaces  path planning  human instructions  service environments  robot plan model generation  natural language interface  interaction inconvenient  verbal interaction-based method  human involvement  human user  unclear instructions  reactive plan model  Task analysis  Service robots  Natural languages  Human-robot interaction  Robot sensing systems  Planning 
Abstract: Verbal interaction between a human and a robot may play a key role in conveying suitable directions for a robot to achieve the goal of a user's request. However, a robot may need to correct task plans or make new decisions with human help, which would make the interaction inconvenient and also increase the interaction time. In this paper, we propose a new verbal interaction-based method that can generate plan models and execute proper actions without human involvement in the middle of performing a task by a robot. To understand the verbal behaviors of humans when giving instructions to a robot, we first conducted a brief user study and found that a human user does not explicitly express the required task. To handle such unclear instructions by a human, we propose two different algorithms that can generate a component of new plan models based on intents and entities parsed from natural language and can resolve the unclear entities existed in human instructions. An experimental scenario with a robot, Cozmo, was tried in the lab environment to test whether or not the proposed method could generate an appropriate plan model. As a result, we found that the robot could successfully accomplish the task following human instructions and also found that the number of interactions and components in the plan model could be reduced as opposed to the general reactive plan model. In the future, we are going to improve the automated process of generating plan models and apply various scenarios under different service environments and robots.


Title: Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans
Key Words: collision avoidance  control engineering computing  learning (artificial intelligence)  mobile robots  navigation  path planning  robot dynamics  robot programming  time-efficient path planning behavior  dynamic crowds  social-safety-awareness  reinforcement learning  2D laser scans  mapless collision-avoidance navigation  ego-safety  pedestrians  robot tests  Collision avoidance  Navigation  Training  Robot sensing systems  Lasers  Path planning 
Abstract: We consider the problem of mapless collision-avoidance navigation where humans are present using 2D laser scans. Our proposed method uses ego-safety to measure collision from the robot's perspective and social-safety to measure the impact of robot's actions on surrounding pedestrians. Specifically, the social-safety part predicts the intrusion impact of the robot's action into the interaction area with surrounding humans. We train the policy using reinforcement learning on a simple simulator and directly evaluate the learned policy in Gazebo and real robot tests. Experiments show the learned policy smoothly transferred to different scenarios without any fine tuning. We observe that our method demonstrates time-efficient path planning behavior with high success rate in the mapless navigation task. Furthermore, we test our method in a navigation task among dynamic crowds, considering both low and high volume traffic. Our learned policy demonstrates cooperative behavior that actively drives our robot into traffic flows while showing respect to nearby pedestrians. Evaluation videos are at https://sites.google.com/view/ssw-batman.


Title: Steering Control of Magnetic Helical Swimmers in Swirling Flows due to Confinement
Key Words: biomechanics  cell motility  computational fluid dynamics  flow simulation  hydrodynamics  magnetic actuators  microrobots  mobile robots  Navier-Stokes equations  position control  propulsion  swirling flow  trajectory control  vortices  prospective robotic agents  rotating magnetic field  magnetized swimmer  helical tail  helical paths  pusher-mode swimmers  rotating magnetic head  microswimmers  swimmer orientation  render orientation-based methods  confined swimmer  control law  swimmer position  swirling flow  helical pusher-mode trajectories  steering control  magnetic helical swimmers  Magnetic fields  Magnetosphere  Magnetic confinement  Magnetohydrodynamics  Propulsion  Navigation  microswimmers  helical swimming  low Reynolds number  steering  control  stability 
Abstract: Artificial microswimmers are prospective robotic agents especially in biomedical applications. A rotating magnetic field can actuate a magnetized swimmer with a helical tail and enable propulsion. Such swimmers exhibit several modes of instability. Inside conduits, for example, hydrodynamic interactions with the boundaries lead to helical paths for pusher-mode swimmers; in this mode the helical tail pushes a rotating magnetic head. State-of-the-art in controlled navigation of micro-swimmers is based on aligning the swimmer orientation according to a reference path, thereby requiring both swimmer orientation and position to be known. Object-orientation is hard to track especially in in vivo scenarios which render orientation-based methods practically unfeasible. Here, we show that the kinematics for a confined swimmer can be linearized by assuming a low wobbling angle. This allows for a control law solely based on the swimmer position. The approach is demonstrated through experiments and two different numerical models: the first is based on the resistive force theory for a swimmer inside a swirling flow represented by a forced vortex and the second is a computational fluid dynamics model, which solves Stokes equations for a swimmer inside a circular channel. Helical pusher-mode trajectories are suppressed significantly for the straight path following problem. The error in real-life experiments remains comparable to those in the state-of-the-art methods.


Title: Sim2real gap is non-monotonic with robot complexity for morphology-in-the-loop flapping wing design
Key Words: aerospace components  aerospace control  learning (artificial intelligence)  mobile robots  biological exemplars  robot design  morphology-in-the-loop flapping wing design  robot complexity  sim2real gap  design complexity  sim2real transfer  high performance robot morphologies  parameterised morphology design space  flapping wing flight  machine learning  Morphology  Robots  Shape  Finite element analysis  Complexity theory  Computational modeling  Machine learning  morphology  simulation to reality  evolution  bio-inspired 
Abstract: Morphology of a robot design is important to its ability to achieve a stated goal and therefore applying machine learning approaches that incorporate morphology in the design space can provide scope for significant advantage. Our study is set in a domain known to be reliant on morphology: flapping wing flight. We developed a parameterised morphology design space that draws features from biological exemplars and apply automated design to produce a set of high performance robot morphologies in simulation. By performing sim2real transfer on a selection, for the first time we measured the shape of the reality gap for variations in design complexity. We found for the flapping wing that the reality gap changes non-monotonically with complexity, suggesting that certain morphology details narrow the gap more than others, and that such details could be identified and further optimised in a future end-to-end automated morphology design process.


Title: Towards biomimicry of a bat-style perching maneuver on structures: the manipulation of inertial dynamics
Key Words: aerospace control  angular momentum  biomimetics  closed loop systems  gears  geometry  mobile robots  motion control  remotely operated vehicles  robot dynamics  bat-style perching maneuver  aerial drone designs  aerial flip turns  landing surface  zero-angular-momentum turns  detachable landing gear  closed-loop manipulations  biomimicry  inertial dynamics manipulation  bat flight characteristics  dynamical system  geometric conservation properties  Aerodynamics  Manipulator dynamics  Mathematical model  Robot sensing systems  Birds 
Abstract: The flight characteristics of bats remarkably have been overlooked in aerial drone designs. Unlike other animals, bats leverage the manipulation of inertial dynamics to exhibit aerial flip turns when they perch. Inspired by this unique maneuver, this work develops and uses a tiny robot called Harpoon to demonstrate that the preparation for upside-down landing is possible through: 1) reorientation towards the landing surface through zero-angular-momentum turns and 2) reaching to the surface through shooting a detachable landing gear. The closed-loop manipulations of inertial dynamics takes place based on a symplectic description of the dynamical system (body and appendage), which is known to exhibit an excellent geometric conservation properties.


Title: Bioinspired object motion filters as the basis of obstacle negotiation in micro aerial systems
Key Words: aerospace robotics  collision avoidance  feedback  image filtering  image motion analysis  image sequences  microrobots  object detection  robot vision  object motion filters  obstacle negotiation  microaerial systems  biological visual guidance  machine vision system  motion vision  dense optic flow map  insect vision inspired object motion filter model  microracing drone  proximaldistal object separation  early-stage motion detection  feedback control loop  Visualization  Kernel  Optical filters  Drones  Band-pass filters  Biological system modeling  Insects  motion vision  obstacle avoidance  visual guidance 
Abstract: All animals and robots that move in the world must navigate to a goal while clearing obstacles. Using vision to accomplish such task has several advantages in cost and payload, which explains the prevalence of biological visual guidance. However, the computational overhead has been an obvious concern when increasing number of pixels and frames that need to be analyzed in real-time for a machine vision system. The use of motion vision and optic flow has been a popular bio-inspired solution for this problem. However, many early-stage motion detection approaches rely on special hardware (e.g. event-cameras) or extensive computation (e.g. dense optic flow map). Here we demonstrate a method to combine an insect vision inspired object motion filter model with simple visual guidance rules to fly through a cluttered environment. We have implemented a complete feedback control loop in a micro racing drone and achieved proximaldistal object separation through only two object motion filters. We discuss the key constraints and the scalability of this approach for future development.


Title: ARCSnake: An Archimedes’ Screw-Propelled, Reconfigurable Serpentine Robot for Complex Environments
Key Words: mobile robots  propulsion  robot dynamics  orientation control  versatile serpentine robot platform  screw threads  mechanical design  electrical design  reconfigurable serpentine robot  serpentine robots  screw propulsion  ARCSnake robot  omni-wheel drive-like motions  NASA-JPL EELS program  NASA-JPL Exobiology Extant Life Surveyor program  Robots  Fasteners  Brushless motors  Skin  Propulsion  Torque  Sensors 
Abstract: This paper presents the design and performance of a new locomotion strategy for serpentine robots using screw propulsion. The ARCSnake robot comprises serially linked, identical modules, each incorporating an Archimedes' screw for propulsion and a universal joint (U-Joint) for orientation control. When serially chained, these modules form a versatile serpentine robot platform which enables the robot to reshape its body configuration for varying environments, typical of a snake. Furthermore, the Archimedes' screws allow for novel omni-wheel drive-like motions by speed controlling their screw threads. This paper considers the mechanical and electrical design, as well as the software architecture for realizing a fully integrated system. The system includes 3N actuators for N segments, each controlled using a BeagleBone Black with a customized power-electronics cape, a 9 Degrees of Freedom (DoF) Inertial Measurement Unit (IMU), and a scalable communication channel over ROS. This robot serves as the first proof-of-concept demonstration of the NASA-JPL Exobiology Extant Life Surveyor (EELS) program that aims to deliver scientific instrumentation deep within the plume vents, caves, and ice sheets of Enceladus and Europa in search for extant lifeforms*.


Title: Real-time Stereo Visual Servoing for Rose Pruning with Robotic Arm
Key Words: cutting  end effectors  gardening  real-time systems  robot vision  service robots  stereo image processing  visual servoing  multiple cameras  single stereo camera  end effector  robotic arm  real time stereo visual servoing  automated robotic rose cutter  rose bush pruning  gardening  rose pruning robots  Cameras  Robot vision systems  Manipulators  Real-time systems  Pipelines  Task analysis 
Abstract: The paper presents a working pipeline which integrates hardware and software in an automated robotic rose cutter. To the best of our knowledge, this is the first robot able to prune rose bushes in a natural environment. Unlike similar approaches like tree stem cutting, the proposed method does not require to scan the full plant, have multiple cameras around the bush, or assume that a stem does not move. It relies on a single stereo camera mounted on the end-effector of the robot and real-time visual servoing to navigate to the desired cutting location on the stem. The evaluation of the whole pipeline shows a good performance in a garden with unconstrained conditions, where finding and approaching a specific location on a stem is challenging due to occlusions caused by other stems and dynamic changes caused by the wind.


Title: Slip-Limiting Controller for Redundant Line-Suspended Robots: Application to Line Ranger
Key Words: angular velocity control  angular velocity measurement  centralised control  mobile robots  motion control  power transmission lines  service robots  wheels  wheel slippage  slip-limiting controller  redundant line-suspended robots  v-shaped wheels  wheel radius  wheel angular velocity measurements  slip limitation  control allocation algorithm  high-level velocity controller  centralized control  line ranger  Wheels  Mobile robots  Angular velocity  Robot sensing systems  Resource management  Velocity measurement 
Abstract: In this paper, a slip-limiting controller for redundant line-suspended robots is presented. This kind of robot is usually equipped with v-shaped wheels, which brings uncertainty about the effective wheel radius, particularly when crossing obstacles. The proposed algorithm is able to estimate and limit wheel slippage in the presence of such uncertainty, relying only on wheel angular velocity measurements. Slip limitation occurs in the control allocation algorithm and hence is decoupled from the high-level velocity controller, allowing a broad applicability in centralized control approaches. Experimental results on Line Ranger show that it effectively reduces wheel slippage compared to traditional centralized control while being more energy efficient than traditional decentralized control approaches.


Title: Interval Search Genetic Algorithm Based on Trajectory to Solve Inverse Kinematics of Redundant Manipulators and Its Application
Key Words: end effectors  genetic algorithms  kinematics  redundant manipulators  search problems  trajectory control  tunnels  interval search strategy  reference point strategy  redundant manipulators  continuous motion  interval search genetic algorithm  inverse kinematics problem  parametric joint angle method  population continuity strategy  trajectory control  evolutionary generation  fitness function  tunnel shotcrete robot  end effector  Manipulators  Kinematics  Sociology  Statistics  Trajectory  Genetic algorithms  Task analysis 
Abstract: In this paper, a new method is proposed to solve the inverse kinematics problem of redundant manipulators. This method demonstrates superior performance on continuous motion by combining interval search genetic algorithm based on trajectory which we propose with parametric joint angle method. In this method, population continuity strategy is utilized to improve search speed and reduce evolutionary generation, interval search strategy is introduced to enhance the search ability and overcome the influence of singularity, and reference point strategy is used to avoid sudden changes of joint variables. By introducing those three strategies, this method is especially suitable for redundant manipulators that perform continuous motion. It can not only obtain solutions of inverse kinematics quickly, but also ensure the motion continuity of manipulator and accuracy of the end effector. Moreover, this algorithm can also perform multi-objective tasks by adjusting the fitness function. Finally, this algorithm is applied to an 8 degree of freedom tunnel shotcrete robot. Field experiments and data analysis show that the algorithm can solve the problem quickly in industrial field, and ensure the motion continuity and accuracy.


Title: Inverse Kinematics for Serial Kinematic Chains via Sum of Squares Optimization
Key Words: convex programming  end effectors  manipulator kinematics  mobile robots  polynomials  position control  degrees of freedom  articulated robots  globally optimal solution  serial manipulators  kinematic constraints  highly redundant serial kinematic chains  joint limit constraints  inverse kinematics problem  convex optimization techniques  numerical methods  nonlinear problem  feasible joint configurations  task-related workspace constraints  sum of squares optimization  Kinematics  Optimization  Conferences  Automation  Robots  Task analysis 
Abstract: Inverse kinematics is a fundamental challenge for articulated robots: fast and accurate algorithms are needed for translating task-related workspace constraints and goals into feasible joint configurations. In general, inverse kinematics for serial kinematic chains is a difficult nonlinear problem, for which closed form solutions cannot easily be obtained. Therefore, computationally efficient numerical methods that can be adapted to a general class of manipulators are of great importance. In this paper, we use convex optimization techniques to solve the inverse kinematics problem with joint limit constraints for highly redundant serial kinematic chains with spherical joints in two and three dimensions. This is accomplished through a novel formulation of inverse kinematics as a nearest point problem, and with a fast sum of squares solver that exploits the sparsity of kinematic constraints for serial manipulators. Our method has the advantages of post-hoc certification of global optimality and a runtime that scales polynomially with the number of degrees of freedom. Additionally, we prove that our convex relaxation leads to a globally optimal solution when certain conditions are met, and demonstrate empirically that these conditions are common and represent many practical instances. Finally, we provide an open source implementation of our algorithm.


Title: Multi-task closed-loop inverse kinematics stability through semidefinite programming
Key Words: closed loop systems  control system synthesis  discrete time systems  humanoid robots  linear matrix inequalities  Lyapunov methods  mathematical programming  mobile robots  robot kinematics  stability  multitask closed-loop inverse kinematics stability  multiobjective task resolution  humanoid robots  local stability problem  closed-loop inverse kinematics algorithm  highly redundant robots  system stability  closed-loop control gains  semidefinite programming problem  discrete-time Lyapunov stability condition  SDP optimization problem  stability conditions  Task analysis  Stability analysis  Robots  Kinematics  Thermal stability  Numerical stability  Asymptotic stability 
Abstract: Today's complex robotic designs comprise in some cases a large number of degrees of freedom, enabling for multi-objective task resolution (e.g., humanoid robots or aerial manipulators). This paper tackles the local stability problem of a hierarchical closed-loop inverse kinematics algorithm for such highly redundant robots. We present a method to guarantee this system stability by performing an online tuning of the closed-loop control gains. We define a semi-definite programming problem (SDP) with these gains as decision variables and a discrete-time Lyapunov stability condition as a linear matrix inequality, constraining the SDP optimization problem and guaranteeing the local stability of the prioritized tasks. To the best of authors' knowledge, this work represents the first mathematical development of an SDP formulation that introduces these stability conditions for a multi-objective closed-loop inverse kinematic problem for highly redundant robots. The validity of the proposed approach is demonstrated through simulation case studies, including didactic examples and a Matlab toolbox for the benefit of the community.


Title: Securing Industrial Operators with Collaborative Robots: Simulation and Experimental Validation for a Carpentry task
Key Words: industrial robots  machine tools  milling  mobile robots  safety  wood  industrial operators  collaborative robot  carpentry task  robotic assistance strategy  machine-tool  wood milling  accidentogenic aspect  physical model  tooling process  safety  Task analysis  Cutting tools  Milling  Force  Service robots  Safety 
Abstract: In this work, a robotic assistance strategy is developed to improve the safety in an artisanal task that involves a strong interaction between a machine-tool and an operator. Wood milling is chosen as a pilot task due to its importance in carpentry and its accidentogenic aspect. A physical model of the tooling process including a human is proposed and a simulator is thereafter developed to better understand situations that are dangerous for the craftsman. This simulator is validated with experiments on three subjects using an harmless mock-up. This validation shows the pertinence of the proposed control approach for the collaborative robot used to increase the safety of the task.


Title: A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning
Key Words: Bayes methods  belief networks  continuous time systems  motion control  path planning  predictive control  probability  reachability analysis  robots  stochastic processes  probabilistic predictive models  human behavior  future motion  observation models  state predictions  robot motion plan  human behavioral data  human motion prediction  Hamilton-Jacobi reachability problem  continuous-time dynamical system  model parameters  worst-case forward reachable set  future state distributions  robust planning  Hamilton-Jacobi reachability-based framework  human motion analysis  safe planning  real-world autonomous systems  Predictive models  Robots  Stochastic processes  Planning  Data models  Computational modeling  Robustness 
Abstract: Real-world autonomous systems often employ probabilistic predictive models of human behavior during planning to reason about their future motion. Since accurately modeling human behavior a priori is challenging, such models are often parameterized, enabling the robot to adapt predictions based on observations by maintaining a distribution over the model parameters. Although this enables data and priors to improve the human model, observation models are difficult to specify and priors may be incorrect, leading to erroneous state predictions that can degrade the safety of the robot motion plan. In this work, we seek to design a predictor which is more robust to misspecified models and priors, but can still leverage human behavioral data online to reduce conservatism in a safe way. To do this, we cast human motion prediction as a Hamilton-Jacobi reachability problem in the joint state space of the human and the belief over the model parameters. We construct a new continuous-time dynamical system, where the inputs are the observations of human behavior, and the dynamics include how the belief over the model parameters change. The results of this reachability computation enable us to both analyze the effect of incorrect priors on future predictions in continuous state and time, as well as to make predictions of the human state in the future. We compare our approach to the worst-case forward reachable set and a stochastic predictor which uses Bayesian inference and produces full future state distributions. Our comparisons in simulation and in hardware demonstrate how our framework can enable robust planning while not being overly conservative, even when the human model is inaccurate. Videos of our experiments can be found at the project website1.


Title: Enhancing Privacy in Robotics via Judicious Sensor Selection
Key Words: data privacy  design  robots  sensors  judicious sensor selection  roboticists  robot design  robotics journals  privacy preservation  robot lifecycle  privacy impact assessments  privacy enhancement  Robot sensing systems  Privacy  Data privacy  Cameras  Task analysis  Law  privacy  privacy by design  robotics  robot design  sensor selection  compliance  privacy impact assessments 
Abstract: Roboticists are grappling with how to address privacy in robot design at a time when regulatory frameworks around the world increasingly require systems to be engineered to preserve and protect privacy. This paper surveys the top robotics journals and conferences over the past four decades to identify contributions with respect to privacy in robot design. Our survey revealed that less than half of one percent of the ~89,120 papers in our study even mention the word privacy. Herein, we propose privacy preserving approaches for roboticists to employ in robot design, including, assessing a robot's purpose and environment; ensuring privacy by design by selecting sensors that do not collect information that is not essential to the core objectives of that robot; embracing both privacy and performance as fundamental design challenges to be addressed early in the robot lifecycle; and performing privacy impact assessments.


Title: Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics
Key Words: learning (artificial intelligence)  mobile robots  nonlinear control systems  nonlinear dynamical systems  predictive control  probability  robust control  stochastic processes  stochastic systems  backup policy  learned policy  control policy  additive stochastic disturbances  nominal dynamics  stochastic nonlinear dynamical systems  stochastic dynamics  safe reinforcement learning  robust model predictive shielding  stochastic systems  statistical learning theory  backup controller  tube-based robust nonlinear model predictive controller  Safety  Robustness  Stochastic processes  Robots  Trajectory  Nonlinear dynamical systems  Heuristic algorithms 
Abstract: We propose a framework for safe reinforcement learning that can handle stochastic nonlinear dynamical systems. We focus on the setting where the nominal dynamics are known, and are subject to additive stochastic disturbances with known distribution. Our goal is to ensure the safety of a control policy trained using reinforcement learning, e.g., in a simulated environment. We build on the idea of model predictive shielding (MPS), where a backup controller is used to override the learned policy as needed to ensure safety. The key challenge is how to compute a backup policy in the context of stochastic dynamics. We propose to use a tube-based robust nonlinear model predictive controller (NMPC) as the backup controller. We estimate the tubes using sampled trajectories, leveraging ideas from statistical learning theory to obtain high-probability guarantees. We empirically demonstrate that our approach can ensure safety in stochastic systems, including cart-pole and a non-holonomic particle with random obstacles.


Title: Segregation of Heterogeneous Swarms of Robots in Curves
Key Words: collision avoidance  decentralised control  mobile robots  multi-robot systems  topology  decentralized control strategy  heterogeneous robot swarms  formation control  collision avoidance strategy  multiple heterogeneous robots  heterogeneous swarm segregation  Collision avoidance  Robot kinematics  Heuristic algorithms  Topology  Convergence  Damping 
Abstract: This paper proposes a decentralized control strategy to reach segregation in heterogeneous robot swarms distributed in curves. The approach is based on a formation control algorithm applied to each robot and a heuristics to compute the distance between the groups, i.e. the distance from the beginning of the curve. We consider that robots can communicate through a fixed underlying topology and also when they are within a certain distance. A convergence proof with a collision avoidance strategy is presented. Simulations and experimental results show that our approach allows a swarm of multiple heterogeneous robots to segregate into groups.


Title: A Fast, Accurate, and Scalable Probabilistic Sample-Based Approach for Counting Swarm Size
Key Words: control engineering computing  distributed algorithms  multi-robot systems  particle swarm optimisation  path planning  counting swarm  distributed algorithm  neighboring robots  robot swarm  Robots  Estimation  Shape  Task analysis  Heuristic algorithms  Random variables  Clocks 
Abstract: This paper describes a distributed algorithm for computing the number of robots in a swarm, only requiring communication with neighboring robots. The algorithm can adjust the estimated count when the number of robots in the swarm changes, such as the addition or removal of robots. Probabilistic guarantees are given, which show the accuracy of this method, and the trade-off between accuracy, speed, and adaptability to changing numbers. The proposed approach is demonstrated in simulation as well as a real swarm of robots.


Title: Bayes Bots: Collective Bayesian Decision-Making in Decentralized Robot Swarms
Key Words: Bayes methods  decision making  multi-robot systems  collective Bayesian decision-making  decentralized robot swarms  distributed Bayesian algorithm  spatially distributed feature  farm field  robotics  decentralized Bayesian algorithms  sparsely distributed robots  decision-making accuracy  bio-inspired positive feedback  fixed-time benchmark algorithm  Bayes bots  bio-inspired approaches  Robot sensing systems  Bayes methods  Decision making  Classification algorithms  Task analysis  Color 
Abstract: We present a distributed Bayesian algorithm for robot swarms to classify a spatially distributed feature of an environment. This type of "go/no-go" decision appears in applications where a group of robots must collectively choose whether to take action, such as determining if a farm field should be treated for pests. Previous bio-inspired approaches to decentralized decision-making in robotics lack a statistical foundation, while decentralized Bayesian algorithms typically require a strongly connected network of robots. In contrast, our algorithm allows simple, sparsely distributed robots to quickly reach accurate decisions about a binary feature of their environment. We investigate the speed vs. accuracy tradeoff in decision-making by varying the algorithm's parameters. We show that making fewer, less-correlated observations can improve decision-making accuracy, and that a well-chosen combination of prior and decision threshold allows for fast decisions with a small accuracy cost. Both speed and accuracy also improved with the addition of bio-inspired positive feedback. This algorithm is also adaptable to the difficulty of the environment. Compared to a fixed-time benchmark algorithm with accuracy guarantees, our Bayesian approach resulted in equally accurate decisions, while adapting its decision time to the difficulty of the environment.


Title: Supervisory Control of Robot Swarms Using Public Events
Key Words: discrete event systems  mobile robots  multi-robot systems  robot swarms  public events  supervisory control theory  formal framework  discrete event systems  correct-by-construction controllers  swarm robotics systems  extended SCT framework  mobile robots  e-puck robots  Collision avoidance  Robot sensing systems  Mobile robots  Generators  Supervisory control 
Abstract: Supervisory Control Theory (SCT) provides a formal framework for controlling discrete event systems. It has recently been used to generate correct-by-construction controllers for swarm robotics systems. Current SCT frameworks are limited, as they support only (private) events that are observable within the same robot. In this paper, we propose an extended SCT framework that incorporates (public) events that are shared among robots. The extended framework allows to model formally the interactions among the robots. It is evaluated using a case study, where a group of mobile robots need to synchronise their movements in space and time-a requirement that is specified at the formal level. We validate our approach through experiments with groups of e-puck robots.


Title: Automatic tool for Gazebo world construction: from a grayscale image to a 3D solid model
Key Words: control engineering computing  laser ranging  mobile robots  SLAM (robots)  solid modelling  Gazebo world construction  grayscale image  3D solid model  robot simulators  simulated physical environment  2D image  2D laser range finder data  Gazebo simulator  3D Collada  simultaneous localization and mapping  real-time factor  SLAM missions  RTF  Tools  Solid modeling  Three-dimensional displays  Robot sensing systems  Gray-scale  Collision avoidance 
Abstract: Robot simulators provide an easy way for evaluation of new concepts and algorithms in a simulated physical environment reducing development time and cost. Therefore it is convenient to have a tool that quickly creates a 3D landscape from an arbitrary 2D image or 2D laser range finder data. This paper presents a new tool that automatically constructs such landscapes for Gazebo simulator. The tool converts a grayscale image into a 3D Collada format model, which could be directly imported into Gazebo. We run three different simultaneous localization and mapping (SLAM) algorithms within three varying complexity environments that were constructed with our tool. A real-time factor (RTF) was used as an efficiency benchmark. Successfully completed SLAM missions with acceptable RTF levels demonstrated the efficiency of the tool. The source code is available for free academic use.


Title: A ROS Gazebo plugin to simulate ARVA sensors
Key Words: aerospace communication  autonomous aerial vehicles  control engineering computing  operating systems (computers)  radio transceivers  rescue robots  robot programming  sensors  ROS Gazebo plugin  forefront technology  Search & Rescue operations  ARVA sensor simulation  transceiver sensor  Appareil de Recherche de Victims en Avalanche  Unmanned Aerial Vehicle  Receivers  Transmitters  Sensors  Electromagnetics  Antennas  Robots  Unmanned aerial vehicles 
Abstract: This paper addresses the problem to simulate ARVA sensors using ROS and Gazebo. ARVA is a French acronym which stands for Appareil de Recherche de Victims en Avalanche and represents the forefront technology adopted in Search & Rescue operations to localize victims of avalanches buried under the snow. The aim of this paper is to describe the mathematical and theoretical background of the transceiver, discussing its implementation and integration with ROS allowing researchers to develop faster and smarter Search &Rescue strategies based on ARVA receiver data. To assess the effectiveness of the proposed sensor model, We present a simulation scenario in which an Unmanned Aerial Vehicle equipped with the transceiver sensor performs a basic S&R pattern using the output of ARVA system.


Title: Is That a Chair? Imagining Affordances Using Simulations of an Articulated Human Body
Key Words: CAD  cameras  image classification  learning (artificial intelligence)  object recognition  pose estimation  articulated human body  object affordances  physical interactions  physical simulations  arbitrarily oriented object  physical sitting interaction  object affordance reasoning  object classification  chair classification  appearance-based deep learning methods  affordances imagining  synthetic 3D CAD models  training data  functional pose predictions  depth camera  Robots  Solid modeling  Cognition  Physics  Three-dimensional displays  Data models  Geometry 
Abstract: For robots to exhibit a high level of intelligence in the real world, they must be able to assess objects for which they have no prior knowledge. Therefore, it is crucial for robots to perceive object affordances by reasoning about physical interactions with the object. In this paper, we propose a novel method to provide robots with an ability to imagine object affordances using physical simulations. The class of chair is chosen here as an initial category of objects to illustrate a more general paradigm. In our method, the robot "imagines" the affordance of an arbitrarily oriented object as a chair by simulating a physical sitting interaction between an articulated human body and the object. This object affordance reasoning is used as a cue for object classification (chair vs non-chair). Moreover, if an object is classified as a chair, the affordance reasoning can also predict the upright pose of the object which allows the sitting interaction to take place. We call this type of poses the functional pose. We demonstrate our method in chair classification on synthetic 3D CAD models. Although our method uses only 30 models for training, it outperforms appearance-based deep learning methods, which require a large amount of training data, when the upright orientation is not assumed to be known a priori. In addition, we showcase that the functional pose predictions of our method align well with human judgments on both synthetic models and real objects scanned by a depth camera.


Title: Toward Sim-to-Real Directional Semantic Grasping
Key Words: control engineering computing  end effectors  grippers  image colour analysis  learning (artificial intelligence)  rendering (computer graphics)  robot vision  directional semantic grasping  deep reinforcement learning  double deep Q-network  robot simulator  rendering  monocular RGB images  wrist mounted camera  cartesian robot control  crossentropy method  domain randomization  end effector  Grippers  Grasping  Cameras  Training  Robot vision systems 
Abstract: We address the problem of directional semantic grasping, that is, grasping a specific object from a specific direction. We approach the problem using deep reinforcement learning via a double deep Q-network (DDQN) that learns to map downsampled RGB input images from a wrist-mounted camera to Q-values, which are then translated into Cartesian robot control commands via the cross-entropy method (CEM). The network is learned entirely on simulated data generated by a custom robot simulator that models both physical reality (contacts) and perceptual quality (high-quality rendering). The reality gap is bridged using domain randomization. The system is an example of end-to-end (mapping input monocular RGB images to output Cartesian motor commands) grasping of objects from multiple pre-defined object-centric orientations, such as from the side or top. We show promising results in both simulation and the real world, along with some challenges faced and the need for future research in this area.


Title: Inferring the Material Properties of Granular Media for Robotic Tasks
Key Words: Bayes methods  calibration  granular flow  granular materials  industrial robots  rolling friction  sliding friction  material properties  granular media  robotic tasks  cereal grains  plastic resin pellets  robotics-integrated industries  pharmaceutical development  accurate simulation  hardware framework  fast physics simulator  granular materials  real-world depth images  grain formations  likelihood-free Bayesian inference  calibrated simulator  unseen granular formations  simulator predictions  Robots  Friction  Numerical models  Bayes methods  Material properties  Task analysis 
Abstract: Granular media (e.g., cereal grains, plastic resin pellets, and pills) are ubiquitous in robotics-integrated industries, such as agriculture, manufacturing, and pharmaceutical development. This prevalence mandates the accurate and efficient simulation of these materials. This work presents a software and hardware framework that automatically calibrates a fast physics simulator to accurately simulate granular materials by inferring material properties from real-world depth images of granular formations (i.e., piles and rings). Specifically, coefficients of sliding friction, rolling friction, and restitution of grains are estimated from summary statistics of grain formations using likelihood-free Bayesian inference. The calibrated simulator accurately predicts unseen granular formations in both simulation and experiment; furthermore, simulator predictions are shown to generalize to more complex tasks, including using a robot to pour grains into a bowl, as well as to create a desired pattern of piles and rings.


Title: KETO: Learning Keypoint Representations for Tool Manipulation
Key Words: control engineering computing  image representation  learning (artificial intelligence)  manipulators  neural nets  KETO  tool manipulation  informative representation  task-specific keypoints  3D point clouds  tool object  deep neural network  informative description  self-supervised robot interactions  task environment  manipulation tasks  task success rates  keypoint prediction  tool generation  learned representations  keypoint representation learning  Tools  Task analysis  Robots  Visualization  Force  Three-dimensional displays  Neural networks 
Abstract: We aim to develop an algorithm for robots to manipulate novel objects as tools for completing different task goals. An efficient and informative representation would facilitate the effectiveness and generalization of such algorithms. For this purpose, we present KETO, a framework of learning keypoint representations of tool-based manipulation. For each task, a set of task-specific keypoints is jointly predicted from 3D point clouds of the tool object by a deep neural network. These keypoints offer a concise and informative description of the object to determine grasps and subsequent manipulation actions. The model is learned from self-supervised robot interactions in the task environment without the need for explicit human annotations. We evaluate our framework in three manipulation tasks with tool use. Our model consistently outperforms state-of-the-art methods in terms of task success rates. Qualitative results of keypoint prediction and tool generation are shown to visualize the learned representations.


Title: Learning to See before Learning to Act: Visual Pre-training for Manipulation
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  object detection  robot vision  visual priors  vision-based manipulation  transfer learning  passive vision task  data distribution  active manipulation task  affordance maps  vision networks  zero-shot adaptation  zero robotic experience  visual pre-training  object detection  object manipulation  affordance prediction networks  Task analysis  Robots  Visualization  Predictive models  Grasping  Head  Data models 
Abstract: Does having visual priors (e.g. the ability to detect objects) facilitate learning to perform vision-based manipulation (e.g. picking up objects)? We study this problem under the framework of transfer learning, where the model is first trained on a passive vision task (i.e., the data distribution does not depend on the agent's decisions), then adapted to perform an active manipulation task (i.e., the data distribution does depend on the agent's decisions). We find that pre-training on vision tasks significantly improves generalization and sample efficiency for learning to manipulate objects. However, realizing these gains requires careful selection of which parts of the model to transfer. Our key insight is that outputs of standard vision models highly correlate with affordance maps commonly used in manipulation. Therefore, we explore directly transferring model parameters from vision networks to affordance prediction networks, and show that this can result in successful zero-shot adaptation, where a robot can pick up certain objects with zero robotic experience. With just a small amount of robotic experience, we can further fine-tune the affordance model to achieve better results. With just 10 minutes of suction experience or 1 hour of grasping experience, our method achieves ~ 80% success rate at picking up novel objects.


Title: Contact-based in-hand pose estimation using Bayesian state estimation and particle filtering
Key Words: assembling  Bayes methods  calibration  force sensors  grippers  industrial manipulators  particle filtering (numerical methods)  pose estimation  robot vision  robotic assembly  state estimation  Bayesian state estimation  particle filtering  industrial assembly tasks  force sensor  robotic gripper  rigid object  contact based inhand pose estimation  Pose estimation  Grippers  Collision avoidance  Robot kinematics  Tactile sensors 
Abstract: In industrial assembly tasks, the position of an object grasped by the robot has to be known with high precision in order to insert or place it. In real applications, this problem is commonly solved by jigs that are specially produced for each part. However, they significantly limit flexibility and are prohibitive when the target parts change often, so a flexible method to localize parts with high accuracy after grasping is desired. To solve this problem, we propose a method that can estimate the position of an object in the robot's hand to sub-millimeter precision, and can improve its estimate incrementally, using only minimal calibration and a force sensor. Our method is applicable to any robotic gripper and any rigid object that the gripper can hold, and requires only a force sensor. We demonstrate that the method can determine the position of an object to a precision of under 1 mm without using any part-specific jigs or equipment.


Title: A Single Multi-Task Deep Neural Network with Post-Processing for Object Detection with Reasoning and Robotic Grasp Detection
Key Words: humanoid robots  inference mechanisms  manipulators  neural nets  object detection  robot vision  robotic grasp detection  object detection  separate networks  target objects  single RGB-D camera  multitask DNN  accurate detections  relationship reasoning  state-of-the-art performance  object grasping tasks  humanoid robot  single multitask deep neural network  deep neural network based object  network output  high-level reasoning  VMRD  Cornell datasets  Robots  Grasping  Cognition  Task analysis  Neural networks  Grippers  Object detection 
Abstract: Applications of deep neural network (DNN) based object and grasp detections could be expanded significantly when the network output is processed by a high-level reasoning over relationship of objects. Recently, robotic grasp detection and object detection with reasoning have been investigated using DNNs. There have been efforts to combine these multitasks using separate networks so that robots can deal with situations of grasping specific target objects in the cluttered, stacked, complex piles of novel objects from a single RGB-D camera. We propose a single multi-task DNN that yields accurate detections of objects, grasp position and relationship reasoning among objects. Our proposed methods yield state-of-the-art performance with the accuracy of 98.6% and 74.2% with the computation speed of 33 and 62 frame per second on VMRD and Cornell datasets, respectively. Our methods also yielded 95.3% grasp success rate for novel object grasping tasks with a 4-axis robot arm and 86.7% grasp success rate in cluttered novel objects with a humanoid robot.


Title: Practical Persistence Reasoning in Visual SLAM
Key Words: mobile robots  robot vision  SLAM (robots)  static environments  dynamic environments  persistence filters  ORB-SLAM  visual SLAM algorithm  persistence filtering  persistence reasoning  semistatic environments  Simultaneous localization and mapping  Visualization  Estimation  Cognition  Probability  Feature extraction 
Abstract: Many existing SLAM approaches rely on the assumption of static environments for accurate performance. However, several robot applications require them to traverse repeatedly in semi-static or dynamic environments. There has been some recent research interest in designing persistence filters to reason about persistence in such scenarios. Our goal in this work is to incorporate such persistence reasoning in visual SLAM. To this end, we incorporate persistence filters [1] into ORB-SLAM, a well-known visual SLAM algorithm. We observe that the simple integration of their proposal results in inefficient persistence reasoning. Through a series of modifications and using two locally collected datasets, we demonstrate the utility of such persistence filtering as well as our customizations in ORB-SLAM. Overall, incorporating persistence filtering could result in a significant reduction in map size (about 30% in the best case) and a corresponding reduction in run-time while retaining similar accuracy to methods that use much larger maps.


Title: FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow
Key Words: cameras  image colour analysis  image motion analysis  image reconstruction  image segmentation  image sequences  mobile robots  motion estimation  robot vision  SLAM (robots)  dynamic environments  visual SLAM  moving objects  static environment features  lead  wrong camera motion estimation  dense RGB-D SLAM solution  camera ego-motion estimation  static background reconstructions  optical flow residuals  dynamic semantics  RGB-D point clouds  camera tracking  background reconstruction  dense reconstruction results  dynamic scenes  static environments  dynamic dense RGB-D SLAM  Cameras  Dynamics  Optical imaging  Simultaneous localization and mapping  Three-dimensional displays  Two dimensional displays  Robustness 
Abstract: Dynamic environments are challenging for visual SLAM since the moving objects occlude the static environment features and lead to wrong camera motion estimation. In this paper, we present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions. Our novelty is using optical flow residuals to highlight the dynamic semantics in the RGB-D point clouds and provide more accurate and efficient dynamic/static segmentation for camera tracking and background reconstruction. The dense reconstruction results on public datasets and real dynamic scenes indicate that the proposed approach achieved accurate and efficient performances in both dynamic and static environments compared to state-of-the-art approaches.


Title: Autonomously Navigating a Surgical Tool Inside the Eye by Learning from Demonstration
Key Words: end effectors  eye  learning by example  medical robotics  navigation  position control  robot programming  surgery  visual servoing  retinal surgery  auditory feedback  autonomous navigation system  needle surgical tool navigation  learning from demonstration  haptic feedback  deep network training  visual servoing  steady hand eye robot  SHER surgical robot  end effector  Tools  Retina  Surgery  Navigation  Task analysis  Trajectory  Robots 
Abstract: A fundamental challenge in retinal surgery is safely navigating a surgical tool to a desired goal position on the retinal surface while avoiding damage to surrounding tissues, a procedure that typically requires tens-of-microns accuracy. In practice, the surgeon relies on depth-estimation skills to localize the tool-tip with respect to the retina in order to perform the tool-navigation task, which can be prone to human error. To alleviate such uncertainty, prior work has introduced ways to assist the surgeon by estimating the tooltip distance to the retina and providing haptic or auditory feedback. However, automating the tool-navigation task itself remains unsolved and largely unexplored. Such a capability, if reliably automated, could serve as a building block to streamline complex procedures and reduce the chance for tissue damage. Towards this end, we propose to automate the tool-navigation task by learning to mimic expert demonstrations of the task. Specifically, a deep network is trained to imitate expert trajectories toward various locations on the retina based on recorded visual servoing to a given goal specified by the user. The proposed autonomous navigation system is evaluated in simulation and in physical experiments using a silicone eye phantom. We show that the network can reliably navigate a needle surgical tool to various desired locations within 137 μm accuracy in physical experiments and 94 μm in simulation on average, and generalizes well to unseen situations such as in the presence of auxiliary surgical tools, variable eye backgrounds, and brightness conditions.


Title: Model Reference Adaptive Control of Multirotor for Missions with Dynamic Change of Payloads During Flight
Key Words: aerospace robotics  aircraft control  attitude control  helicopters  MIMO systems  model reference adaptive control systems  nonlinear control systems  robust control  stability  payloads  multirotor aerial robot  flight controller  flight stability  attitude control  aerial robot system  model reference adaptive control  nonlinear multiple-input and multiple-output  MRAC  Attitude control  Adaptation models  Payloads  MIMO communication  Unmanned aerial vehicles  Adaptive control  Gravity 
Abstract: Carrying payloads in air is a major mission for multirotor aerial robot. However, the presence of payloads on multirotor aerial robot has a risk of degrading the performance of the flight controller. This concern becomes obvious especially when carrying objects not securely attached to the body or performing aerial manipulation. Therefore, controller with the ability to adapt itself to the effects of payloads on flight stability is needed. This paper proposes a novel nonlinear multiple-input and multiple-output (MIMO) model reference adaptive control (MRAC) system for attitude control of multirotor aerial robots which can dynamically compensate change in the position of center of gravity and inertia caused by payloads. Stability and robustness of the controller are experimentally confirmed in quadrotor and transformable multirotor, and experiments modeling practical applications are conducted for each aerial robot system, proving the utility of the controller.


Title: The Tiercel: A novel autonomous micro aerial vehicle that can map the environment by flying into obstacles
Key Words: cameras  collision avoidance  image sensors  mobile robots  navigation  robot vision  SLAM (robots)  space vehicles  autonomous microaerial vehicle  autonomous Tiercel robots  collision detector design  fisheye camera  reflective obstacles  transparent obstacles  collision-resilient robot  Tiercel MAV  autonomous navigation  autonomous flight  Collision avoidance  Cameras  Robot vision systems  Planning 
Abstract: Autonomous flight through unknown environments in the presence of obstacles is a challenging problem for micro aerial vehicles (MAVs). A majority of the current state-of-art research assumes obstacles as opaque objects that can be easily sensed by optical sensors such as cameras or LiDARs. However in indoor environments with glass walls and windows, or scenarios with smoke and dust, robots (even birds) have a difficult time navigating through the unknown space.In this paper, we present the design of a new class of micro aerial vehicles that achieves autonomous navigation and are robust to collisions. In particular, we present the Tiercel MAV: a small, agile, light weight and collision-resilient robot powered by a cellphone grade CPU. Our design exploits contact to infer the presence of transparent or reflective obstacles like glass walls, integrating touch with visual perception for SLAM. The Tiercel is able to localize using visual-inertial odometry (VIO) running on board the robot with a single downward facing fisheye camera and an IMU. We show how our collision detector design and experimental set up enable us to characterize the impact of collisions on VIO. We further develop a planning strategy to enable the Tiercel to fly autonomously in an unknown space, sustaining collisions and creating a 2D map of the environment. Finally we demonstrate a swarm of three autonomous Tiercel robots safely navigating and colliding through an obstacle field to reach their objectives.


Title: On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning
Key Words: grippers  learning (artificial intelligence)  neural net architecture  neurocontrollers  reactive neural networks  fully connected networks  reactive behaviours  actor-critic architecture  robot environment  end-to-end reinforcement learning  robotic learning  pick and place task  behaviour-based reinforcement learning  Brook subsumption architecture  pick and place robotic task  actor-critic policy  activation mechanisms  inhibition mechanisms  gripper  degree-of-freedom  Robots  Task analysis  Training  Computer architecture  Learning (artificial intelligence)  Grasping  Feature extraction 
Abstract: We present a behaviour-based reinforcement learning approach, inspired by Brook's subsumption architecture, in which simple fully connected networks are trained as reactive behaviours. Our working assumption is that a pick and place robotic task can be simplified by leveraging domain knowledge of a robotics developer to decompose and train reactive behaviours; namely, approach, grasp, and retract. Then the robot autonomously learns how to combine reactive behaviours via an Actor-Critic architecture. We use an Actor-Critic policy to determine the activation and inhibition mechanisms of the reactive behaviours in a particular temporal sequence. We validate our approach in a simulated robot environment where the task is about picking a block and taking it to a target position while orienting the gripper from a top grasp. The latter represents an extra degree-of-freedom of which current end-to-end reinforcement learning approaches fail to generalise. Our findings suggest that robotic learning can be more effective if each behaviour is learnt in isolation and then combined them to accomplish the task. That is, our approach learns the pick and place task in 8,000 episodes, which represents a drastic reduction in the number of training episodes required by an end-to-end approach ( 95,000 episodes) and existing state-of-the-art algorithms.


Title: Integrated moment-based LGMD and deep reinforcement learning for UAV obstacle avoidance
Key Words: autonomous aerial vehicles  collision avoidance  control engineering computing  image motion analysis  image sequences  learning (artificial intelligence)  mobile robots  neural nets  object detection  robot vision  SLAM (robots)  visual perception  deep reinforcement learning  UAV obstacle avoidance  learning-based reaction local planner  microUAVs  image moment  illuminance variation  mapless navigation  moment-based LGMD  bioinspired monocular vision perception method  Navigation  Collision avoidance  Robustness  Lighting  Robots  Optical imaging  Machine learning 
Abstract: In this paper, a bio-inspired monocular vision perception method combined with a learning-based reaction local planner for obstacle avoidance of micro UAVs is presented. The system is more computationally efficient than other vision-based perception and navigation methods such as SLAM and optical flow because it does not need to calculate accurate distances. To improve the robustness of perception against illuminance change, the input image is remapped using image moment which is independent of illuminance variation. After perception, a local planner is trained using deep reinforcement learning for mapless navigation. The proposed perception and navigation methods are evaluated in some realistic simulation environments. The result shows that this light-weight monocular perception and navigation system works well in different complex environments without accurate depth information.


Title: Interactive Reinforcement Learning with Inaccurate Feedback
Key Words: feedback  interactive systems  learning (artificial intelligence)  mobile robots  interactive reinforcement learning  human teachers  sensor feedback  learning process  noninteractive RL  policy feedback  feedback source  interactive RL methods  revision estimation-from-partially incorrect resources  REPaIR  physical robot  Maintenance engineering  Robot sensing systems  Estimation  Task analysis  Learning (artificial intelligence)  Computer science 
Abstract: Interactive Reinforcement Learning (RL) enables agents to learn from two sources: rewards taken from observations of the environment, and feedback or advice from a secondary critic source, such as human teachers or sensor feedback. The addition of information from a critic during the learning process allows the agents to learn more quickly than non-interactive RL. There are many methods that allow policy feedback or advice to be combined with RL. However, critics can often give imperfect information. In this work, we introduce a framework for characterizing Interactive RL methods with imperfect teachers and propose an algorithm, Revision Estimation from Partially Incorrect Resources (REPaIR), which can estimate corrections to imperfect feedback over time. We run experiments both in simulations and demonstrate performance on a physical robot, and find that when baseline algorithms do not have prior information on the exact quality of a feedback source, using REPaIR matches or improves the expected performance of these algorithms.


Title: Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning
Key Words: learning systems  optimisation  robots  guided uncertainty-aware policy optimization  sample-efficient policy learning  robust perception system  reinforcement learning  model-based methods  learning-based methods  model-based strategies  peg insertion  GUAPO  model-based policy  Task analysis  Uncertainty  Robot sensing systems  Learning (artificial intelligence)  Cameras  Switches 
Abstract: Traditional robotic approaches rely on an accurate model of the environment, a detailed description of how to perform the task, and a robust perception system to keep track of the current state. On the other hand, reinforcement learning approaches can operate directly from raw sensory inputs with only a reward signal to describe the task, but are extremely sampleinefficient and brittle. In this work, we combine the strengths of model-based methods with the flexibility of learning-based methods to obtain a general method that is able to overcome inaccuracies in the robotics perception/actuation pipeline, while requiring minimal interactions with the environment. This is achieved by leveraging uncertainty estimates to divide the space in regions where the given model-based policy is reliable, and regions where it may have flaws or not be well defined. In these uncertain regions, we show that a locally learned-policy can be used directly with raw sensory inputs. We test our algorithm, Guided Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing peg insertion. Videos are available at: https://sites.google.com/view/guapo-rl.


Title: Benchmark for Skill Learning from Demonstration: Impact of User Experience, Task Complexity, and Start Configuration on Performance
Key Words: human-robot interaction  learning (artificial intelligence)  user experience  task reproductions  demonstration approaches  multiple motion-based learning  task complexity  user experience  skill learning  robot executions  task performance  starting configuration  human demonstrator  physical robot  task models  manipulation tasks  real-world tasks  relative strengths  Task analysis  Robots  Trajectory  Videos  Pressing  Benchmark testing  Complexity theory 
Abstract: We contribute a study benchmarking the performance of multiple motion-based learning from demonstration approaches. Given the number and diversity of existing methods, it is critical that comprehensive empirical studies be performed comparing the relative strengths of these techniques. In particular, we evaluate four approaches based on properties an end user may desire for real-world tasks. To perform this evaluation, we collected data from nine participants, across four manipulation tasks. The resulting demonstrations were used to train 180 task models and evaluated on 720 task reproductions on a physical robot. Our results detail how i) complexity of the task, ii) the expertise of the human demonstrator, and iii) the starting configuration of the robot affect task performance. The collected dataset of demonstrations, robot executions, and evaluations are publicly available. Research insights and guidelines are also provided to guide future research and deployment choices about these approaches.


Title: Robot Programming without Coding
Key Words: control engineering computing  end effectors  learning (artificial intelligence)  robot programming  teaching  telerobotics  robot programming  wearable consumer devices  programming tools  robot teleoperation  salient features  off-the-shelf soft-articulated robotic components  Dynamic Movement Primitives  human trajectories  impedance regulation skills  7-DOF collaborative robots  anthropomorphic end-effectors  robot teaching  Robots  Task analysis  Education  Trajectory  Programming  Three-dimensional displays  Impedance 
Abstract: An approach toward intuitive and easy robot programming, consists to transfer skills from humans to machines, through demonstration. A vast literature exists on learning from multiple demonstrations. This paper, on the other hand, tackles the problem of providing all needed information to execute a certain task by resorting to one single demonstration - hence, a problem closer to programming than to learning. We use wearable consumer devices - but no keyboard nor coding - as programming tools, to let the programmer tele-operate the robot, which in turn records the most salient features and affordances from the object, environment, robot, and human. To enable this goal we combine off-the-shelf soft-articulated robotic components with the framework of Dynamic Movement Primitives, which we contribute to extend to generalize human trajectories and impedance regulation skills. This framework enables to teach robot quickly and in a intuitive way without coding. Experimental tests have been performed on a dual-arm system composed by two 7-dofs collaborative robots equipped with anthropomorphic end-effectors. Experiments show the functionality of the framework and verify the effectiveness of the impedance extension.


Title: Predictive Modeling of Periodic Behavior for Human-Robot Symbiotic Walking
Key Words: biomechanics  gait analysis  humanoid robots  human-robot interaction  intelligent robots  learning systems  man-machine systems  medical robotics  prosthetics  human-robot symbiotic walking  probabilistic framework  periodic behavior  periodic movement regimes  customized models  human walking  latent variables  biomechanical variables  robotic prosthesis  imitation learning approach  human participants  ankle angle control signals  robotic prosthetic ankle  predictive modeling  Periodic Interaction Primitives  Legged locomotion  Robot sensing systems  Biomechanics  Predictive models  Prosthetics  Probabilistic logic 
Abstract: We propose in this paper Periodic Interaction Primitives - a probabilistic framework that can be used to learn compact models of periodic behavior. Our approach extends existing formulations of Interaction Primitives to periodic movement regimes, i.e., walking. We show that this model is particularly well-suited for learning data-driven, customized models of human walking, which can then be used for generating predictions over future states or for inferring latent, biomechanical variables. We also demonstrate how the same framework can be used to learn controllers for a robotic prosthesis using an imitation learning approach. Results in experiments with human participants indicate that Periodic Interaction Primitives efficiently generate predictions and ankle angle control signals for a robotic prosthetic ankle, with MAE of 2.21° in 0.0008s per inference. Performance degrades gracefully in the presence of noise or sensor fall outs. Compared to alternatives, this algorithm functions 20 times faster and performed 4.5 times more accurately on test subjects.


Title: Inferring the Geometric Nullspace of Robot Skills from Human Demonstrations
Key Words: geometry  humanoid robots  industrial robots  learning (artificial intelligence)  robot vision  geometric nullspace  robot skills  human demonstrations  fit geometric nullspaces  geometric constraints  powerful mathematical model  geometric skill description  skill model  learnt skill  simulated industrial robot  iCub humanoid robot  geometric constraint models  Task analysis  Manifolds  Adaptation models  Service robots  Grasping  Data models 
Abstract: In this paper we present a framework to learn skills from human demonstrations in the form of geometric nullspaces, which can be executed using a robot. We collect data of human demonstrations, fit geometric nullspaces to them, and also infer their corresponding geometric constraint models. These geometric constraints provide a powerful mathematical model as well as an intuitive representation of the skill in terms of the involved objects. To execute the skill using a robot, we combine this geometric skill description with the robot's kinematics and other environmental constraints, from which poses can be sampled for the robot's execution. The result of our framework is a system that takes the human demonstrations as input, learns the underlying skill model, and executes the learnt skill with different robots in different dynamic environments. We evaluate our approach on a simulated industrial robot, and execute the final task on the iCub humanoid robot.


Title: A Dynamical System Approach for Adaptive Grasping, Navigation and Co-Manipulation with Humanoid Robots
Key Words: compliance control  dexterous manipulators  humanoid robots  position control  dynamical system approach  adaptive grasping  humanoid robots  iCub humanoid robot  state-dependent dynamical systems  robots hands  intermediate virtual object  motion generators  object moves  whole-body compliant control strategy  manipulation tasks  body manipulation  iCub robots walk-to-grasp objects  Robot kinematics  Legged locomotion  Grasping  Task analysis  Humanoid robots  Planning 
Abstract: We present an integrated approach that provides compliant control of an iCub humanoid robot and adaptive reaching, grasping, navigating and co-manipulating capabilities. We use state-dependent dynamical systems (DS) to (i) coordinate and drive the robots hands (in both position and orientation) to grasp an object using an intermediate virtual object, and (ii) drive the robot's base while walking/navigating. The use of DS as motion generators allows us to adapt smoothly as the object moves and to re-plan on-line motion of the arms and body to reach the object's new location. The desired motion generated by the DS are used in combination with a whole-body compliant control strategy that absorbs perturbations while walking and offers compliant behaviors for grasping and manipulation tasks. Further, the desired dynamics for the arm and body can be learned from demonstrations. By integrating these components, we achieve unprecedented adaptive behaviors for whole body manipulation. We showcase this in simulations and real-world experiments where iCub robots (i) walk-to-grasp objects, (ii) follow a human (or another iCub) through interaction and (iii) learn to navigate or comanipulate an object from human guided demonstrations; whilst being robust to changing targets and perturbations.


Title: Subspace Projectors for State-Constrained Multi-Robot Consensus
Key Words: distributed algorithms  distributed control  mobile robots  multi-robot systems  subspace projectors  state-constrained multirobot consensus  distributed algorithms  subspace projection methods  consensus value  constrained 2D rendezvous  single-integrator robots  discrete-time agreement protocol  Symmetric matrices  Eigenvalues and eigenfunctions  Protocols  Matrix decomposition  Laplace equations  Two dimensional displays  Robots 
Abstract: In this paper, we study the state-constrained consensus problem and introduce a new family of distributed algorithms based on subspace projection methods which are simple to implement and which preserve, under some suitable conditions, the consensus value of the original discrete-time agreement protocol. The proposed theory is supported by extensive numerical experiments for the constrained 2D rendezvous of single-integrator robots.


Title: Multi-Agent Task Allocation using Cross-Entropy Temporal Logic Optimization
Key Words: discrete systems  entropy  graph theory  multi-agent systems  multi-robot systems  optimisation  search problems  stochastic programming  temporal logic  task specification  discrete transition systems  finite linear temporal logic specifications  stochastic optimization  graph based search  cross entropy temporal logic optimization  multiagent task allocation cross entropy algorithm  robot team  Task analysis  Automata  Cost function  Planning  Resource management  Switches 
Abstract: In this paper, we propose a graph-based search method to optimally allocate tasks to a team of robots given a global task specification. In particular, we define these agents as discrete transition systems. In order to allocate tasks to the team of robots, we decompose finite linear temporal logic (LTL) specifications and consider agent specific cost functions. We propose to use the stochastic optimization technique, cross entropy, to optimize over this cost function. The multi-agent task allocation cross-entropy (MTAC-E) algorithm is developed to determine both when it is optimal to switch to a new agent to complete a task and minimize the costs associated with individual agent trajectories. The proposed algorithm is verified in simulation and experimental results are included.


Title: Adaptive Task Allocation for Heterogeneous Multi-Robot Teams with Evolving and Unknown Robot Capabilities
Key Words: adaptive systems  multi-robot systems  adaptive task allocation  task execution  robot capabilities  heterogeneous multirobot teams  Task analysis  Resource management  Cost function  Mobile robots  Real-time systems  Minimization 
Abstract: For multi-robot teams with heterogeneous capabilities, typical task allocation methods assign tasks to robots based on the suitability of the robots to perform certain tasks as well as the requirements of the task itself. However, in real-world deployments of robot teams, the suitability of a robot might be unknown prior to deployment, or might vary due to changing environmental conditions. This paper presents an adaptive task allocation and task execution framework which allows individual robots to prioritize among tasks while explicitly taking into account their efficacy at performing the tasks-the parameters of which might be unknown before deployment and/or might vary over time. Such a specialization parameter-encoding the effectiveness of a given robot towards a task-is updated on-the-fly, allowing our algorithm to reassign tasks among robots with the aim of executing them. The developed framework requires no explicit model of the changing environment or of the unknown robot capabilities-it only takes into account the progress made by the robots at completing the tasks. Simulations and experiments demonstrate the efficacy of the proposed approach during variations in environmental conditions and when robot capabilities are unknown before deployment.


Title: Mobile Wireless Network Infrastructure on Demand
Key Words: mobile ad hoc networks  mobile robots  multi-agent systems  multi-robot systems  optimisation  telecommunication network routing  mobile relay nodes  network team  wireless connectivity  task agents  Mobile wireless network infrastructure  multirobot teams  previous multiagent systems  communication infrastructure  end-to-end communication requirements  task team  arbitrary objective  joint optimization framework  optimal network routes  Task analysis  Ad hoc networks  Routing  Wireless networks  Hardware  Probabilistic logic 
Abstract: In this work, we introduce Mobile Wireless Infrastructure on Demand: a framework for providing wireless connectivity to multi-robot teams via autonomously reconfiguring ad-hoc networks. In many cases, previous multi-agent systems either assumed the availability of existing communication infrastructure or were required to create a network in addition to completing their objective. Instead our system explicitly assumes the responsibility of creating and sustaining a wireless network capable of satisfying end-to-end communication requirements of a team of agents, called the task team, performing an arbitrary objective. To accomplish this goal, we propose a joint optimization framework that alternates between finding optimal network routes to support data flows between the task agents and improving the performance of the network by repositioning a collection of mobile relay nodes referred to as the network team. We demonstrate our approach with simulations and experiments wherein wireless connectivity is provided to patrolling task agents.


Title: Monitoring Over the Long Term: Intermittent Deployment and Sensing Strategies for Multi-Robot Teams
Key Words: combinatorial mathematics  Gaussian processes  greedy algorithms  matrix algebra  Monte Carlo methods  multi-robot systems  optimisation  multirobot team  intermittent deployment problem  heterogeneous robots  environmental process  spatiotemporal process  intermittent deployment strategy  spatiotemporal Gaussian process  Monte Carlo simulations  greedy algorithm  submodular optimization  matroids  Robot sensing systems  Monitoring  Mutual information  Spatiotemporal phenomena  Kernel 
Abstract: In this paper, we formulate and solve the intermittent deployment problem, which yields strategies that couple when heterogeneous robots should sense an environmental process, with where a deployed team should sense in the environment. As a motivation, suppose that a spatiotemporal process is slowly evolving and must be monitored by a multi-robot team, e.g., unmanned aerial vehicles monitoring pasturelands in a precision agriculture context. In such a case, an intermittent deployment strategy is necessary as persistent deployment or monitoring is not cost-efficient for a slowly evolving process. At the same time, the problem of where to sense once deployed must be solved as process observations yield useful feedback for determining effective future deployment and monitoring decisions. In this context, we model the environmental process to be monitored as a spatiotemporal Gaussian process with mutual information as a criterion to measure our understanding of the environment. To make the sensing resource-efficient, we demonstrate how to use matroid constraints to impose a diverse set of homogeneous and heterogeneous constraints. In addition, to reflect the cost-sensitive nature of real-world applications, we apply budgets on the cost of deployed heterogeneous robot teams. To solve the resulting problem, we exploit the theories of submodular optimization and matroids and present a greedy algorithm with bounds on sub-optimality. Finally, Monte Carlo simulations demonstrate the correctness of the proposed method.


Title: Multi-Robot Coordination for Estimation and Coverage of Unknown Spatial Fields
Key Words: Bayes methods  computational geometry  Gaussian processes  mobile robots  multi-robot systems  optimisation  sampling methods  multirobot coordination  unknown spatial fields  multirobot coverage  initially unknown spatial scalar field  Bayesian optimization  control law  centroidal Voronoi tessellation  adaptive sequential sampling method  surrogate function  density function  Gaussian processes  Density functional theory  Estimation  Gaussian processes  Robot sensing systems  Prediction algorithms  Bayes methods 
Abstract: We present an algorithm for multi-robot coverage of an initially unknown spatial scalar field characterized by a density function, whereby a team of robots simultaneously estimates and optimizes its coverage of the density function over the domain. The proposed algorithm borrows powerful concepts from Bayesian Optimization with Gaussian Processes that, when combined with control laws to achieve centroidal Voronoi tessellation, give rise to an adaptive sequential sampling method to explore and cover the domain. The crux of the approach is to apply a control law using a surrogate function of the true density function, which is then successively refined as robots gather more samples for estimation. The performance of the algorithm is justified theoretically under slightly idealized assumptions, by demonstrating asymptotic no-regret with respect to the coverage obtained with a known density function. The performance is also evaluated in simulation and on the Robotarium with small teams of robots, confirming the good performance suggested by the theoretical analysis.


Title: Learning Robotic Assembly Tasks with Lower Dimensional Systems by Leveraging Physical Softness and Environmental Constraints
Key Words: force control  force sensors  industrial manipulators  learning (artificial intelligence)  mobile robots  robotic assembly  torque control  high frequency force-torque sensors  physical softness  data-driven approaches  hard robots  learning robotic assembly tasks  peg-in-hole tasks  model-based reinforcement learning method  lower dimensional systems  environmental constraints  soft robot  high frequency force-torque controllers  Task analysis  Soft robotics  Aerospace electronics  Force  Robotic assembly  Learning (artificial intelligence) 
Abstract: In this study, we present a novel control framework for assembly tasks with a soft robot. Typically, existing hard robots require high frequency controllers and precise force/torque sensors for assembly tasks. The resulting robot system is complex, entailing large amounts of engineering and maintenance. Physical softness allows the robot to interact with the environment easily. We expect soft robots to perform assembly tasks without the need for high frequency force/torque controllers and sensors. However, specific data-driven approaches are needed to deal with complex models involving nonlinearity and hysteresis. If we were to apply these approaches directly, we would be required to collect very large amounts of training data. To solve this problem, we argue that by leveraging softness and environmental constraints, a robot can complete tasks in lower dimensional state and action spaces, which could greatly facilitate the exploration of appropriate assembly skills. Then, we apply a highly efficient model-based reinforcement learning method to lower dimensional systems. To verify our method, we perform a simulation for peg-in-hole tasks. The results show that our method learns the appropriate skills faster than an approach that does not consider lower dimensional systems. Moreover, we demonstrate that our method works on a real robot equipped with a compliant module on the wrist.


Title: Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body Robotics using NVIDIA CUDA
Key Words: control engineering computing  graphics processing units  learning (artificial intelligence)  mobile robots  multi-agent systems  multi-robot systems  optimisation  parallel algorithms  parallel architectures  CUDA-based C++ robotics simulation library  multiagent robots  massively parallel integration scheme  reinforcement learning iterations  rapid topology optimization  simultaneous optimization  innovative GPU architecture design  robotics primitives  GPU-accelerated simulations  asynchronous computing model  GPU-accelerated interface  interacting bodies  multiagent robotics  intrinsically serial tasks  low-dimensional tasks  robotics simulation libraries  NVIDIA CUDA  soft-body robotics  parallel asynchronous library  Titan  Graphics processing units  Robots  Libraries  Springs  Computational modeling  Kernel  Acceleration 
Abstract: While most robotics simulation libraries are built for low-dimensional and intrinsically serial tasks, soft-body and multi-agent robotics have created a demand for simulation environments that can model many interacting bodies in parallel. Despite the increasing interest in these fields, no existing simulation library addresses the challenge of providing a unified, highly-parallelized, GPU-accelerated interface for simulating large robotic systems. Titan is a versatile CUDA-based C++ robotics simulation library that employs a novel asynchronous computing model for GPU-accelerated simulations of robotics primitives. The innovative GPU architecture design permits simultaneous optimization and control on the CPU while the GPU runs asynchronously, enabling rapid topology optimization and reinforcement learning iterations. Kinematics are solved with a massively parallel integration scheme that incorporates constraints and environmental forces. We report dramatically improved performance over CPU baselines, simulating as many as 300 million primitive updates per second, while allowing flexibility for a wide range of research applications. We present several applications of Titan to high-performance simulations of soft-body and multi-agent robots.


Title: Human-like Planning for Reaching in Cluttered Environments
Key Words: collision avoidance  decision making  dexterous manipulators  grippers  learning (artificial intelligence)  planning (artificial intelligence)  robot programming  robot vision  trajectory control  virtual reality  decision making  decision classifiers  cluttered environments  robot planners  random sampling  object manipulation plans  virtual reality  trajectory optimisation  physics based robot simulation  human-like planning  depth camera  Robotiq two finger gripper  Task analysis  Planning  Robots  Testing  Feature extraction  Trajectory  Standards 
Abstract: Humans, in comparison to robots, are remarkably adept at reaching for objects in cluttered environments. The best existing robot planners are based on random sampling of configuration space- which becomes excessively high-dimensional with large number of objects. Consequently, most planners often fail to efficiently find object manipulation plans in such environments. We addressed this problem by identifying high-level manipulation plans in humans, and transferring these skills to robot planners. We used virtual reality to capture human participants reaching for a target object on a tabletop cluttered with obstacles. From this, we devised a qualitative representation of the task space to abstract the decision making, irrespective of the number of obstacles. Based on this representation, human demonstrations were segmented and used to train decision classifiers. Using these classifiers, our planner produced a list of waypoints in task space. These waypoints provided a high-level plan, which could be transferred to an arbitrary robot model and used to initialise a local trajectory optimiser. We evaluated this approach through testing on unseen human VR data, a physics-based robot simulation, and a real robot (dataset and code are publicly available1). We found that the human-like planner outperformed a state-of-the-art standard trajectory optimisation algorithm, and was able to generate effective strategies for rapid planning- irrespective of the number of obstacles in the environment.


Title: Where to relocate?: Object rearrangement inside cluttered and confined environments for robotic manipulation
Key Words: collision avoidance  computational complexity  graph theory  industrial robots  manipulators  mobile robots  optimisation  robot vision  warehousing  object rearrangement  cluttered confined environments  robotic manipulation  cluttered confined space  motion planning  manipulator  nonmonotone arrangement problems  pick-and-place actions  baseline methods  Planning  Manipulators  Collision avoidance  Task analysis  Clutter  Kinematics 
Abstract: We present an algorithm determining where to relocate objects inside a cluttered and confined space while rearranging objects to retrieve a target object. Although methods that decide what to remove have been proposed, planning for the placement of removed objects inside a workspace has not received much attention. Rather, removed objects are often placed outside the workspace, which incurs additional laborious work (e.g., motion planning and execution of the manipulator and the mobile base, perception of other areas). Some other methods manipulate objects only inside the workspace but without a principle so the rearrangement becomes inefficient. In this work, we consider both monotone (each object is moved only once) and non-monotone arrangement problems which have shown to be NP-hard. Once the sequence of objects to be relocated is given by any existing algorithm, our method aims to minimize the number of pick-and-place actions to place the objects until the target becomes accessible. From extensive experiments, we show that our method reduces the number of pick-and-place actions and the total execution time (the reduction is up to 23.1% and 28.1% respectively) compared to baseline methods while achieving higher success rates.


Title: Autonomous Modification of Unstructured Environments with Found Material
Key Words: building materials  dexterous manipulators  friction  grippers  industrial manipulators  materials handling equipment  mechanical contact  path planning  sensors  unseen objects  adaptive ramp building algorithms  irregularly shaped stones  contact geometry  friction  high-level algorithm  physics-based planner  pickup  robotic system  complex grasp planning  autonomous modification  manipulation  found material  pickup  motion support structures  specialized construction algorithm  unstructured environments  Uncertainty  Robot sensing systems  Physics  Shape  Buildings  physics simulation  autonomous construction  robotics  irregular building materials. 
Abstract: The ability to autonomously modify their environment dramatically increases the capability of robots to operate in unstructured environments. We develop a specialized construction algorithm and robotic system that can autonomously build motion support structures with previously unseen objects. The approach is based on our prior work on adaptive ramp building algorithms, but it eliminates the assumption of having specialized building materials that simplify manipulation and planning for stability. Utilizing irregularly shaped stones makes the problem significantly more challenging since the outcome of individual placements is sensitive to details of contact geometry and friction, which are difficult to observe. To reuse the same high-level algorithm, we develop a new physics-based planner that explicitly considers the uncertainty produced by incomplete in-situ sensing and imprecision during pickup and placement. We demonstrate the approach on a robotic system that uses a newly developed gripper to reliably pick up stones with minimal additional sensors or complex grasp planning. The resulting system can build structures with more than 70 stones, which in turn provide traversable paths to previously inaccessible locations.


Title: LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery
Key Words: image matching  image resolution  optical radar  stereo image processing  dense depth maps  depth information  light detection and ranging  accurate depth map  stereo imagery  stereo systems  high-quality dense depth maps  stereo matching algorithms  sparse depth map  high-resolution LIDAR  Laser radar  Task analysis  Training  Feature extraction  Estimation  Convolution  Correlation 
Abstract: An accurate depth map of the environment is critical to the safe operation of autonomous robots and vehicles. Currently, either light detection and ranging (LIDAR) or stereo matching algorithms are used to acquire such depth information. However, a high-resolution LIDAR is expensive and produces sparse depth map at large range; stereo matching algorithms are able to generate denser depth maps but are typically less accurate than LIDAR at long range. This paper combines these approaches together to generate high-quality dense depth maps. Unlike previous approaches that are trained using ground-truth labels, the proposed model adopts a self-supervised training process. Experiments show that the proposed method is able to generate high-quality dense depth maps and performs robustly even with low-resolution inputs. This shows the potential to reduce the cost by using LIDARs with lower resolution in concert with stereo systems while maintaining high resolution.


Title: Monocular Visual-Inertial Odometry in Low-Textured Environments with Smooth Gradients: A Fully Dense Direct Filtering Approach
Key Words: calibration  distance measurement  gradient methods  image filtering  image texture  inertial systems  interpolation  vectors  low-textured environments  fully dense direct filtering approach  visual texture  direct photometric approaches  image information  information propagation  complexity reduction approach  state vector  monocular visual-inertial odometry approaches  higher order covariance propagation  state handling improvement  Cameras  Uncertainty  Estimation  Mathematical model  Motion estimation  Integrated circuits  Robots 
Abstract: State of the art visual-inertial odometry approaches suffer from the requirement of high gradients and sufficient visual texture. Even direct photometric approaches select a subset of the image with high-gradient areas and ignore smooth gradients or generally low-textured areas. In this work, we show that taking all image information (i.e. every single pixel) enables visual-inertial odometry even on areas with very low texture and smooth gradients, inherently interpolating and estimating the scene with no texture based on its informative surrounding. This information propagation is only possible as we estimate all states and their uncertainties (robot pose, extrinsic sensor calibration, and scene depth) jointly in a fully dense filter framework. Our complexity reduction approach enables real-time execution despite the large size of the state vector. Compared to our previous basic feasibility study on this topic, this work includes higher order covariance propagation and improved state handling for a significant performance gain, thorough comparisons to state-of-the-art algorithms, larger mapping components with uncertainty, self-calibration capability, and real-data tests.


Title: Interaction Stability Analysis from the Input-Output Viewpoints
Key Words: bond graphs  haptic interfaces  Hilbert spaces  human-robot interaction  input-output stability  robotic assembly  mechanical impedance  linear spatial impedance representation  bond graph theory  ideal model  idealized interaction  port functions  ideal interaction model  collaborative interactions  competitive interactions  interaction models  passivity indices  interaction stability analysis  input-output viewpoints  robot applications  haptic devices  parts assembly  interaction behaviours  interaction dynamics  DAlembert principle  nonsmooth mechanics  kinematic constraints  bond graph methodology  Hilbert function space  continuous function  input-output stability condition  Impedance  Admittance  Robots  Force  Kinematics  Stability criteria 
Abstract: Interaction with the environment is arguably one of the necessary actions for many robot applications such as haptic devices, manipulation, parts assembly, cooperation with humans, and the use of tools. Taxonomy of interaction behaviours is classified into three categories: cooperation, collaboration, and competition. In theory, interaction dynamics may be modelled by D'Alembert's principle or nonsmooth mechanics through seeking equality and/or inequality kinematic constraints. However, it is hard to gain these kinematic constraints in practice since they may be variable or be hardly described in a mathematical form. As a result, bond graph methodology is preferred in interaction dynamics modelling.In this paper, passivity and passivity indices with the differential operator are put forward by restricting its domain from the whole extended Hilbert function space to a set of all continuous function with finite derivative, and then the input-output stability condition, in this case, is derived. Next, mechanical impedance and admittance are defined, and a linear spatial impedance representation is given from the energetic point of view. Base on the bond graph theory, an ideal model is presented to model the idealized interaction, and invariance of port functions derived from the ideal interaction model is introduced; An interaction model is then proposed accounting for nonidealized factors and to describe cooperative, collaborative, and competitive interactions in a unified way. Finally, interaction stabilities are analyzed corresponding to different interaction models, and robustness of interaction stability is addressed based on the passivity indices.


Title: Improving the contact instant detection of sensing antennae using a Super-Twisting algorithm
Key Words: filtering theory  multi-robot systems  signal processing  variable structure systems  contact instant detection  antenna devices  mimic insect antennae  mammal whiskers  robotic systems  signal processing  flexible antenna  impact detection  impact instant estimation  super-twisting algorithm  time 5.0 ms  Antennas  Estimation  Vibrations  Robot sensing systems  Antenna measurements  Delays 
Abstract: Sensing antenna devices, that mimic insect antennae or mammal whiskers, is an active field of research that still needs new developments in order to become efficient and reliable components of robotic systems. This work reports a new result in the area of signal processing of these devices that allows to detect the instant of the impact of a flexible antenna with an object faster than other reported methods. Previous methods require the use of filters that introduce delays in the impact detection. A method based on the Super-Twisting algorithm is proposed here that avoids the use of these filters and reduces such delays improving the impact instant estimation. Experiments show that these delays can be reduced in more than 50%, allowing reliable estimation of the impact instant with an error of less than 5 ms in many cases requiring a limited computational effort.


Title: 6DFC: Efficiently Planning Soft Non-Planar Area Contact Grasps using 6D Friction Cones
Key Words: friction  grippers  manipulator dynamics  path planning  quadratic programming  robot vision  6D friction cone  approximate compliant contacts  soft point contact models  area contact model  6D friction limit surface  6DFC algorithm  soft nonplanar area contact grasp planning  3D friction cones  ABB YuMi robot  quadratic program  Friction  Grippers  Three-dimensional displays  Force  Solid modeling  Computational modeling  Ellipsoids 
Abstract: Analytic grasp planning algorithms typically approximate compliant contacts with soft point contact models to compute grasp quality, but these models are overly conservative and do not capture the full range of grasps available. While area contact models can reduce the number of false negatives predicted by point contact models, they have been restricted to a 3D analysis of the wrench applied at the contact and so are still overly conservative. We extend traditional 3D friction cones and present an efficient algorithm for calculating the 6D friction cone (6DFC) for a non-planar area contact between a compliant gripper and a rigid object. We introduce a novel sampling algorithm to find the 6D friction limit surface for a non-planar area contact and a linearization method for these ellipsoids that reduces the computation of 6DFC constraints to a quadratic program. We show that constraining the wrench applied at the contact in this way increases recall, a metric inversely related to the number of false negative predictions, by 17% and precision, a metric inversely related to the number of false positive predictions, by 2% over soft point contact models on results from 1500 physical grasps on 12 3D printed nonplanar objects with an ABB YuMi robot. The 6DFC algorithm also achieves 6% higher recall with similar precision and 85x faster runtime than a previously proposed area contact model.


Title: Long-Horizon Prediction and Uncertainty Propagation with Residual Point Contact Learners
Key Words: computer simulation  learning (artificial intelligence)  mobile robots  self-supervised approach  rigid-body simulators  contact models  predictive performance  horizon prediction  uncertainty propagation  residual point contact learners  robotic tasks  Predictive models  Analytical models  Uncertainty  Computational modeling  Trajectory  Dynamics  Stochastic processes 
Abstract: The ability to simulate and predict the outcome of contacts is paramount to the successful execution of many robotic tasks. Simulators are powerful tools for the design of robots and their behaviors, yet the discrepancy between their predictions and observed data limit their usability. In this paper, we propose a self-supervised approach to learning residual models for rigid-body simulators that exploits corrections of contact models to refine predictive performance and propagate uncertainty. We empirically evaluate the framework by predicting the outcomes of planar dice rolls and compare it's performance to state-of-the-art techniques.


Title: Versatile Trajectory Optimization Using a LCP Wheel Model for Dynamic Vehicle Maneuvers
Key Words: automobiles  friction  mechanical contact  mobile robots  optimisation  path planning  robot dynamics  trajectory control  tyres  vehicle dynamics  wheels  dynamic drift parking  discontinuous friction model  tire dynamics model  cost function  wheel skidding  versatile trajectory optimization framework  vehicle motion  anisotropic Coulomb friction cone  multirigid-body contact problems  linear complementarity problem  robotics community  contact dynamics  real world contact behavior  empirical friction model  aggressive maneuvers  car models  dynamic vehicle maneuvers  LCP wheel model  executing dynamic drift parking  planning horizon  Vehicle dynamics  Wheels  Friction  Planning  Dynamics  Trajectory optimization  Tires 
Abstract: Car models have been extensively studied at varying levels of abstraction, and planning and executing motions under ideal conditions is well researched and understood. For more aggressive maneuvers, for example when drifting or skidding, empirical and/or discontinuous friction models have been used to explain and approximate real world contact behavior. Separately, contact dynamics have been extensively studied by the robotics community, often times formulated as a linear complementarity problem (LCP) for dynamic multi-rigid-body contact problems with Coulomb friction cone approximations. In this work, we explore the validity of using such an anisotropic Coulomb friction cone to model tire dynamics to plan for vehicle motion, and present a versatile trajectory optimization framework using this model that can both avoid and/or exploit wheel skidding, depending on the cost function and planning horizon. Experimental evidence of planning and executing dynamic drift parking is shown on a 1/16 scale model car.


Title: Highly Parallelizable Plane Extraction for Organized Point Clouds Using Spherical Convex Hulls
Key Words: geometry  image colour analysis  pixelwise plane extraction  highly parallelizable plane extraction  organized point clouds  spherical convex hull  region growing algorithm  explicit plane parameterization  geometric constraints  GPU  RGB-D camera  Three-dimensional displays  Feature extraction  Robot sensing systems  Clustering algorithms  Real-time systems  Data mining 
Abstract: We present a novel region growing algorithm for plane extraction of organized point clouds using the spherical convex hull. Instead of explicit plane parameterization, our approach interprets potential underlying planes as a series of geometric constraints on the sphere that are refined during region growing. Unlike existing schemes relying on downsampling for sequential execution in real time, our approach enables pixelwise plane extraction that is highly parallelizable. We further test the proposed approach with a fully parallel implementation on a GPU. Evaluation based on public data sets has shown state-of-the-art extraction accuracy and superior speed compared to existing approaches, while guaranteeing real-time processing at full input resolution of a typical RGB-D camera.


Title: View-Invariant Loop Closure with Oriented Semantic Landmarks
Key Words: geometry  pose estimation  robot vision  SLAM (robots)  view-invariant loop closure  oriented semantic landmarks  simultaneous localization and mapping  monocular semantic SLAM system  object identity  inter-object geometry  view-invariant loop detection  ORB-SLAM  local appearance-based features  indoor scenes  object orientation estimation  geometrical detailed semantic maps  object translation  object scale  Cameras  Simultaneous localization and mapping  Semantics  Trajectory  Layout  Robustness  Estimation 
Abstract: Recent work on semantic simultaneous localization and mapping (SLAM) have shown the utility of natural objects as landmarks for improving localization accuracy and robustness. In this paper we present a monocular semantic SLAM system that uses object identity and inter-object geometry for view-invariant loop detection and drift correction. Our system's ability to recognize an area of the scene even under large changes in viewing direction allows it to surpass the mapping accuracy of ORB-SLAM, which uses only local appearance-based features that are not robust to large viewpoint changes. Experiments on real indoor scenes show that our method achieves mean drift reduction of 70% when compared directly to ORB-SLAM. Additionally, we propose a method for object orientation estimation, where we leverage the tracked pose of a moving camera under the SLAM setting to overcome ambiguities caused by object symmetry. This allows our SLAM system to produce geometrically detailed semantic maps with object orientation, translation, and scale.


Title: Active Acoustic Contact Sensing for Soft Pneumatic Actuators
Key Words: elastic constants  manipulator dynamics  pneumatic actuators  soft pneumatic actuators  active acoustic sensor  contact sensors  soft actuator  embedded speaker  PneuFlex actuator  active sensors  active acoustic contact sensing  Panda robot arm  embedded microphone  Actuators  Robot sensing systems  Acoustics  Microphones  Acoustic measurements 
Abstract: We present an active acoustic sensor that turns soft pneumatic actuators into contact sensors. The whole surface of the actuator becomes a sensor, rendering the question of where best to place a contact sensor unnecessary. At the same time, the compliance of the soft actuator remains unaffected. A small, embedded speaker emits a frequency sweep which travels through the actuator before it is recorded with an embedded microphone. The specific contact state of the actuator affects how the sound is modulated while traversing the structure. We learn to recognize these changes in the sound and map them to the corresponding contact locations. We demonstrate the method on the PneuFlex actuator. The active acoustic sensor achieves a classification rate of 93% and mean regression error of 3.7mm. It is robust against background noises and different objects. Finally, we test it on a Panda robot arm and show that it is unaffected by motor noises and other active sensors.


Title: Simultaneous Learning from Human Pose and Object Cues for Real-Time Activity Recognition
Key Words: feature extraction  human-robot interaction  image motion analysis  image recognition  image representation  learning (artificial intelligence)  optimisation  pose estimation  regression analysis  human activity categories  real-time human activity recognition  human pose  object cues  real-world human-centered robotics applications  assisted living  human-robot collaboration  frequency 104.0 Hz  Activity recognition  Real-time systems  Optimization  Object recognition  Feature extraction  Robot sensing systems 
Abstract: Real-time human activity recognition plays an essential role in real-world human-centered robotics applications, such as assisted living and human-robot collaboration. Although previous methods based on skeletal data to encode human poses showed promising results on real-time activity recognition, they lacked the capability to consider the context provided by objects within the scene and in use by the humans, which can provide a further discriminant between human activity categories. In this paper, we propose a novel approach to real-time human activity recognition, through simultaneously learning from observations of both human poses and objects involved in the human activity. We formulate human activity recognition as a joint optimization problem under a unified mathematical framework, which uses a regression-like loss function to integrate human pose and object cues and defines structured sparsity-inducing norms to identify discriminative body joints and object attributes. To evaluate our method, we perform extensive experiments on two benchmark datasets and a physical robot in a home assistance setting. Experimental results have shown that our method outperforms previous methods and obtains real-time performance for human activity recognition with a processing speed of 104 Hz.


Title: Demonstration of Hospital Receptionist Robot with Extended Hybrid Code Network to Select Responses and Gestures
Key Words: gesture recognition  human-robot interaction  interactive systems  learning (artificial intelligence)  recurrent neural nets  service robots  hospital receptionist robot  task-oriented dialogue system  human-robot interaction  pipeline  dialogue states  end-to-end learning  recurrent neural networks  social robot system  end-to-end dialogue system  RNN based gesture selector  dialogue efficiency  gestures  extended hybrid code network  Task analysis  Robot sensing systems  Pipelines  Face detection  Recurrent neural networks  Speech recognition 
Abstract: Task-oriented dialogue system has a vital role in Human-Robot Interaction (HRI). However, it has been developed based on conventional pipeline approach which has several drawbacks; expensive, time-consuming, and so on. Based on this approach, developers manually define a robot's behaviour such as gestures and facial expressions on the corresponding dialogue states. Recently, end-to-end learning of Recurrent Neural Networks (RNNs) is an attractive solution for the dialogue system. In this paper, we proposed a social robot system using end-to-end dialogue system in the context of hospital receptionist. We utilized Hybrid Code Network (HCN) as an end-to-end dialogue system and extended to select both response and gesture using RNN based gesture selector. We evaluate its performance with human users and compare the results with one of the conventional methods. Empirical result shows that the proposed method has benefits in terms of dialogue efficiency, which indicates how efficient users were in performing the given tasks with the help of the robot. Moreover, we achieved the same performance regarding the robot's gesture with the proposed method compared to manually defined gestures.


Title: Can I Trust You? A User Study of Robot Mediation of a Support Group
Key Words: human-robot interaction  robot mediation  socially assistive robots  group dynamics  social settings  trust dynamics  robot mediated support group  dyadic trust scale  general trust  average interpersonal trust  group interaction session  multiparty setting  Educational robots  Mediation  Sensitivity  Robot sensing systems  Atmospheric measurements  Particle measurements 
Abstract: Socially assistive robots have the potential to improve group dynamics when interacting with groups of people in social settings. This work contributes to the understanding of those dynamics through a user study of trust dynamics in the novel context of a robot mediated support group. For this study, a novel framework for robot mediation of a support group was developed and validated. To evaluate interpersonal trust in the multi-party setting, a dyadic trust scale was implemented and found to be uni-factorial, validating it as an appropriate measure of general trust. The results of this study demonstrate a significant increase in average interpersonal trust after the group interaction session, and qualitative post-session interview data report that participants found the interaction helpful and successfully supported and learned from one other. The results of the study validate that a robot-mediated support group can improve trust among strangers and allow them to share and receive support for their academic stress.


Title: Coronal Plane Spine Twisting Composes Shape To Adjust the Energy Landscape for Grounded Reorientation
Key Words: biomechanics  bone  orthopaedics  CPST  coronal plane spine twisting composes shape  energy landscape  grounded reorientation  animal locomotion  legged robots  self-righting mechanics  freedom coronal plane representation  body shape affordance  cross-sectional geometries  kinematic model predictions  elliptical bodies  rectangular shaped bodies  quasistatic reorientation maneuvers  Shape  Kinematics  Potential energy  Hip  Torso  Legged locomotion 
Abstract: Despite substantial evidence for the crucial role played by an active backbone or spine in animal locomotion, its adoption in legged robots remains limited because the added mechanical complexity and resulting dynamical challenges pose daunting obstacles to characterizing even a partial range of potential performance benefits. This paper takes a next step toward such a characterization by exploring the quasistatic terrestrial self-righting mechanics of a model system with coronal plane spine twisting (CPST). Reduction from a full 3D kinematic model of CPST to a two parameter, two degree of freedom coronal plane representation of body shape affordance predicts a substantial benefit to ground righting by lowering the barrier between stable potential energy basins. The reduced model predicts the most advantageous twist angle for several cross-sectional geometries, reducing the required righting torque by up to an order of magnitude depending on constituent shapes. Experiments with a three actuated degree of freedom physical mechanism corroborate the kinematic model predictions using two different quasistatic reorientation maneuvers for both elliptical and rectangular shaped bodies with a range of eccentricities or aspect ratios. More speculative experiments make intuitive use of the kinematic model in a highly dynamic maneuver to suggest still greater benefits of CPST achievable by coordinating kinetic as well as potential energy, for example as in a future multi-appendage system interacting with a contact-rich 3D environment.


Title: Motion Design for a Snake Robot Negotiating Complicated Pipe Structures of a Constant Diameter
Key Words: collision avoidance  mobile robots  motion control  path planning  pipes  motion design  snake robot  constant diameter  multiple pipe structures  target form  rolling motion  complicated pipe structures  Snake robots  Windings  Shape  Robots  Junctions  Modeling  Pins 
Abstract: A method for designing the motion of a snake robot negotiating complicated pipe structures having a constant diameter is presented. For such robots moving inside pipes, there are various "obstacles" such as junctions, bends, shears, and blockages. To surmount these obstacles, we propose a method that enables the robot to adapt to multiple pipe structures of a constant diameter. We designed the target form of the snake robot of two helices connected with an arbitrary shape. This method is applicable to various obstacles by designing a part of the target form conforming to the obstacle. The robot negotiates obstacles under shift control by employing a rolling motion. We demonstrated the effectiveness of the proposed method in various experiments.


Title: Single Actuator Peristaltic Robot for Subsurface Exploration and Device Emplacement
Key Words: actuators  cams (mechanical)  data acquisition  design engineering  geology  hydrocarbon reservoirs  mobile robots  oil reservoirs  autonomous robots  data acquisition  tool transportation  petroleum reservoirs  cam-follower configuration worm robot  peristaltic displacement  single actuator peristaltic robot  subsurface exploration  device emplacement  initial testing  single actuator peristaltic motion robot  subsurface geological exploration  design  nonconsolidated media  Robots  Hydrocarbons  Soil  Reservoirs  Actuators  Oils  Asphalt 
Abstract: In this work, we present the concept, design, and initial testing of a single actuator peristaltic motion robot for subsurface geological exploration and device emplacement. We are researching unconventional methods, including robotics, for the production of energy from oil reservoirs that do not liberate carbon to the atmosphere. For such application, we are developing autonomous robots for data acquisition and tool transportation inside petroleum reservoirs. The mechanism described in this work is a cam-follower configuration worm robot that utilizes peristaltic displacement. We confirmed that the mechanism works on a plane surface and in non-consolidated media.


Title: Dynamic modeling of robotic manipulators for accuracy evaluation
Key Words: finite element analysis  flexible manipulators  manipulator dynamics  robotic manipulators  industrial robots  mechanical stiffness  multibody models  finite element model  flexible link manipulator model  industrial robot  stiffness parameters  robot behavior  weight-reduced manipulator  Solid modeling  Manipulator dynamics  Mathematical model  Service robots  Finite element analysis 
Abstract: In order to fulfill conflicting requirements in the development of industrial robots, such as increased accuracy of a weightreduced manipulator with lower mechanical stiffness, the robot's dynamical behavior must be evaluated early in the development process. This leads to the need of accurate multibody models of the manipulator under development.This paper deals with multibody models that include flexible bodies, which are exported from the corresponding Finite Element model of the structural parts. It is shown that such a flexible link manipulator model, which is purely based on development and datasheet data, is suitable for an accurate description of an industrial robot's dynamic behavior. No stiffness parameters need to be identified by experimental methods, making this approach especially relevant during the development of new manipulators. This paper presents results of experiments in time and frequency domain for analyzing the modeling approach and for validating the model performance against real robot behavior.


Title: A Real-Robot Dataset for Assessing Transferability of Learned Dynamics Models
Key Words: learning (artificial intelligence)  learning systems  optimal control  robot dynamics  robust control  trajectory control  learned dynamics models  model based reinforcement learning  robust current dynamics learning  real robot dataset  transferability assessment  3 degrees of freedom robot trajectories  optimal control  robotic learning  Trajectory  Robots  Artificial neural networks  Heuristic algorithms  Mathematical model  Torque measurement 
Abstract: In the context of model-based reinforcement learning and control, a large number of methods for learning system dynamics have been proposed in recent years. The purpose of these learned models is to synthesize new control policies. An important open question is how robust current dynamics-learning methods are to shifts in the data distribution due to changes in the control policy. We present a real-robot dataset which allows to systematically investigate this question. This dataset contains trajectories of a 3 degrees-of-freedom (DOF) robot being controlled by a diverse set of policies. For comparison, we also provide a simulated version of the dataset. Finally, we benchmark a few widely-used dynamics-learning methods using the proposed dataset. Our results show that the iid test error of a learned model is not necessarily a good indicator of its accuracy under control policies different from the one which generated the training data. This suggests that it may be important to evaluate dynamics-learning methods in terms of their transfer performance, rather than only their iid error.


Title: Development of a Robotic System for Automated Decaking of 3D-Printed Parts
Key Words: force control  industrial robots  learning (artificial intelligence)  neural nets  path planning  production engineering computing  three-dimensional printing  industrial robots  robotic decaking  automated decaking  3D printed parts  3D printing based mass manufacturing  smart mechanical design  motion planning  force control  deep learning  Powders  Cleaning  Service robots  Three-dimensional displays  Task analysis  Force control  deep learning  manipulation  system design  3D-printing  decaking 
Abstract: With the rapid rise of 3D-printing as a competitive mass manufacturing method, manual "decaking" - i.e. removing the residual powder that sticks to a 3D-printed part - has become a significant bottleneck. Here, we introduce, for the first time to our knowledge, a robotic system for automated decaking of 3D-printed parts. Combining Deep Learning for 3D perception, smart mechanical design, motion planning, and force control for industrial robots, we developed a system that can automatically decake parts in a fast and efficient way. Through a series of decaking experiments performed on parts printed by a Multi Jet Fusion printer, we demonstrated the feasibility of robotic decaking for 3D-printing-based mass manufacturing.


Title: Design and Implementation of Hydraulic-Cable driven Manipulator for Disaster Response Operation
Key Words: emergency management  hydraulic actuators  manipulators  motion control  rescue robots  hydraulic-cable driven actuation modules  3DOF manipulator  hydraulic actuation system  disaster response mobile-manipulation  disaster response operation  hydraulic-cable driven manipulator  Manipulators  Blades  Actuators  Hydraulic systems  Torque  Wires 
Abstract: This paper introduces a new hydraulic manipulator with hydraulic-cable driven actuation (HCA) modules for disaster response mobile-manipulation. The hydraulic actuation system has the potential to apply disaster-response application, because it has a higher power-to-weight ratio and robustness to external impacts than electric motor actuation. However, using a conventional hydraulic manipulators is inappropriate because the revolute joint uses conventional actuators, such as linear cylinders and vanes, which have some limitations: 1) linear cylinder: small range of motion, 2) vane: low torque-toweight ratio. To overcome these limitations, we propose new 3DOF manipulator which has a larger workspace than the conventional hydraulic manipulator and comparable payloadto-weight ratio. To this end, we use hydraulic-cable driven actuation modules from our previous research. Experimental results verify the basic performance of the actuator modules and manipulator and their capability to perform various disaster response tasks.


Title: Designs for an Expressive Mechatronic Chordophone
Key Words: hearing  mechatronics  music  musical instruments  technical exploration  timbral exploration  mechatronic chordophones  musical robotics  stand-alone instruments  sound art installations  expressive potential  plucked strings  expressive mechatronic mono-chord  polystring chordophone  expressive mechatronic chordophone  sound generation model  Mechatronics  Instruments  Music  Actuators  Robots  Prototypes  Solenoids 
Abstract: Plucked strings are an exciting sound generation model for technical and timbral exploration. Mechatronic chordophones take advantage of this model and have been the focus of extensive research and exploration in musical robotics, often used as stand-alone instruments or as part of sound art installations. However, no existing chordophone designs have utilised the expressive potential of plucked strings to their full extent.In this paper, we introduce an expressive mechatronic mono-chord that serves as a prototyping platform for the construction of a polystring chordophone. This new chordophone has been developed to offer enhanced dynamic range, fast picking speeds, fast pitch shifter displacement, and additional expressive techniques compared to existing systems.


Title: OmBURo: A Novel Unicycle Robot with Active Omnidirectional Wheel
Key Words: mechanical stability  mobile robots  motion control  nonlinear control systems  pendulums  robot dynamics  wheels  human environments  ideal locomotion mechanism  omnidirectional balancing unicycle robot  mobility mechanism  OmBURo  agile mobility  compact structure  active omnidirectional wheel  Wheels  Mobile robots  Gears  Friction  Mathematical model  Energy loss 
Abstract: A mobility mechanism for robots to be used in tight spaces shared with people requires it to have a small footprint, to move omnidirectionally, as well as to be highly maneuverable. However, currently there exist few such mobility mechanisms that satisfy all these conditions well. Here we introduce Omnidirectional Balancing Unicycle Robot (OmBURo), a novel unicycle robot with active omnidirectional wheel. The effect is that the unicycle robot can drive in both longitudinal and lateral directions simultaneously. Thus, it can dynamically balance itself based on the principle of dual-axis wheeled inverted pendulum. This paper discloses the early development of this novel unicycle robot involving the overall design, modeling, and control, as well as presents some preliminary results including station keeping and path following. With its very compact structure and agile mobility, it might be the ideal locomotion mechanism for robots to be used in human environments in the future.


Title: Recognition and Reconfiguration of Lattice-Based Cellular Structures by Simple Robots
Key Words: mobile robots  path planning  robot vision  flexibility  cellular components  robust robots  cellular building materials  arbitrary cellular structures  cellular materials  lattice-based cellular structures  Tiles  Robot sensing systems  Lattices  Shape  Autonomous robots 
Abstract: We consider recognition and reconfiguration of lattice-based cellular structures by very simple robots with only basic functionality. The underlying motivation is the construction and modification of space facilities of enormous dimensions, where the combination of new materials with extremely simple robots promises structures of previously unthinkable size and flexibility; this is also closely related to the newly emerging field of programmable matter. Aiming for large-scale scalability, both in terms of the number of the cellular components of a structure, as well as the number of robots that are being deployed for construction requires simple yet robust robots and mechanisms, while also dealing with various basic constraints, such as connectivity of a structure during reconfiguration. To this end, we propose an approach that combines ultra-light, cellular building materials with extremely simple robots. We develop basic algorithmic methods that are able to detect and reconfigure arbitrary cellular structures, based on robots that have only constant-sized memory. As a proof of concept, we demonstrate the feasibility of this approach for specific cellular materials and robots that have been developed at NASA.


Title: A Fast Configuration Space Algorithm for Variable Topology Truss Modular Robots
Key Words: collision avoidance  mobile robots  path planning  topology  fast configuration space algorithm  VTT  self-reconfigurable robot  truss shape  motion planning  shape changing actions  topology reconfiguration  geometry reconfiguration actions  cell decomposition approach  collision-free space  simple shape-morphing method  variable topology truss modular robot  Topology  Robots  Planning  Geometry  Shape  Kinematics  Actuators 
Abstract: The Variable Topology Truss (VTT) is a new class of self-reconfigurable robot that can reconfigure its truss shape and topology depending on the task or environment requirements. Motion planning and avoiding self-collision are difficult as these systems usually have dozens of degrees-of-freedom with complex intersecting parallel actuation. There are two different types of shape changing actions for a VTT: geometry reconfiguration and topology reconfiguration. This paper focuses on the geometry reconfiguration actions. A new cell decomposition approach is presented based on a fast and complete method to compute the collision-free space of a node in a truss. A simple shape-morphing method is shown to quickly create motion paths for reconfiguration by moving one node at a time.


Title: ModQuad-DoF: A Novel Yaw Actuation for Modular Quadrotors
Key Words: actuators  aerospace robotics  attitude control  autonomous aerial vehicles  drag  helicopters  mobile robots  motion control  path planning  position control  vehicle dynamics  ModQuad-DoF  modular quadrotors  robotic structure  enhanced capabilities  module design  freedom relative motion  flying robot  cage  docking mechanism  structure control authority  structure yaw control  yaw actuation method 
Abstract: In this work we introduce ModQuad-DoF, a modular flying robotic structure with enhanced capabilities for yaw actuation. We propose a new module design that allows a one degree of freedom relative motion between the flying robot and the cage, with a docking mechanism allowing rigid connections between cages. A novel method of yaw actuation that increases the structure control authority is also presented. Our new method for the structure yaw control relies on the independent roll angles of each one of the modules, instead of the traditional drag moments from the propellers. In this paper, we propose a controller that allows the ModQuad-DoF to control its position and attitude. In our experiments, we tested a different number of modules flying in cooperation and validated the novel yaw actuation method.


Title: An Actuation Fault Tolerance Approach to Reconfiguration Planning of Modular Self-folding Robots
Key Words: actuators  fault tolerance  motion control  robots  fault tolerant reconfiguration  self-folding robots  modular system  complete actuation failure  active modules  imprecise robotic motion  reconfiguration failure  intra-module connection  reconfiguration schemes  user-specified fault tolerant capability  arbitrary input initial pattern  robotic platform  modular origami robot  fault tolerant initial patterns  actuation fault tolerance approach  reconfiguration planning  modular self-folding  Fault tolerance  Fault tolerant systems  Robots  Three-dimensional displays  Circuit faults  Shape  Planning 
Abstract: This paper presents a novel approach to fault tolerant reconfiguration of modular self-folding robots. Among various types of faults that probably occur in the modular system, we focus on the tolerance of complete actuation failure of active modules that might cause imprecise robotic motion and even reconfiguration failure. Our approach is to utilize the reconfigurability of modular self-folding robots and investigate intra-module connection to determine initial patterns that are inherently fault tolerant. We exploit the redundancy of actuation and distribute active modules in both layout-based and target-based scenarios, such that reconfiguration schemes with user-specified fault tolerant capability can be generated for an arbitrary input initial pattern or 3D configuration. Our methods are demonstrated in computer-aided simulation on the robotic platform of Mori, a modular origami robot. The simulation results validate that the proposed algorithms yield fault tolerant initial patterns and distribution schemes of active modules for several 2D and 3D configurations with Mori, while retaining generalizability for a large number of modular self-folding robots.


Title: Parallel Permutation for Linear Full-resolution Reconfiguration of Heterogeneous Sliding-only Cubic Modular Robots
Key Words: computational complexity  evolutionary computation  motion control  robots  studied cubic modules  convex motion primitives  rotating motion primitives  heterogeneous reconfiguration algorithm  parallel heterogeneous permutation method  full-resolution reconfiguration algorithm  heterogeneous operations  space saving  module hardware  sliding-only motion primitive  cubic modular robot  cubic module  sliding-only cubic modular robots  parallel permutation algorithm  heterogeneous sliding-only cubic modular  linear full-resolution reconfiguration  linear operating-time cost  sliding-only cubic modules  robot structure  linear operating time cost  Navigation  Hardware  Robot kinematics  Shape  Cameras  Robot vision systems 
Abstract: This paper presents a parallel permutation algorithm that achieves linear full-resolution reconfiguration of sliding-only cubic modular robots. We assume the use of a cubic module that can only slide across other modules' surfaces. The idea of a cubic modular robot with sliding-only motion primitive is a new concept that has advantages in simplifying the mechanisms of module hardware and space saving in its heterogeneous operations compared with previously studied cubic modules, such as those with sliding and convex motion primitives, or rotating motion primitives. However, because of its limited mobility, there are difficulties in managing the connectivity and scalability of the heterogeneous reconfiguration algorithm for it. To overcome these disadvantages, we introduce a parallel heterogeneous permutation method with linear operating time cost that can be incorporated into our previous full-resolution reconfiguration algorithm. We prove the correctness and completeness of the proposed algorithm. Simulation results show that the full-resolution reconfiguration algorithm that incorporates the proposed permutation algorithm reconfigures the robot structure with sliding-only cubic modules in linear operating-time cost.


Title: Determining and Improving the Localization Accuracy of AprilTag Detection
Key Words: edge detection  feature extraction  image colour analysis  image matching  location based services  object detection  robot vision  software libraries  localization accuracy  AprilTag detection  fiducial markers  freely available libraries  AprilTag 3  ArUco  OpenCV algorithm  AprilTags C++  robotics  edge refinement  grayscale camera image  template matching  Cameras  C++ languages  Libraries  Robot vision systems  Calibration  Image edge detection 
Abstract: Fiducial markers like AprilTags play an important role in robotics, e.g., for the calibration of cameras or the localization of robots. One of the most important properties of an algorithm for detecting such tags is its localization accuracy.In this paper, we present the results of an extensive comparison of four freely available libraries capable of detecting AprilTags, namely AprilTag 3, AprilTags C++, ArUco as standalone libraries, and the OpenCV algorithm based on ArUco. The focus of the comparison is on localization accuracy, but the processing time is also examined. Besides working with pure tags, their extension to checkerboard corners is investigated.In addition, we present two new post-processing techniques. Firstly, a method that can filter out very inaccurate detections resulting from partial border occlusion, and secondly a new highly accurate method for edge refinement. With this we achieve a median pixel error of 0.017 px, compared to 0.17 px for standard OpenCV corner refinement.The dataset used for the evaluation, as well as the developed post-processing techniques, are made publicly available to encourage further comparison and improvement of the detection libraries.


Title: Change of Optimal Values: A Pre-calculated Metric
Key Words: decision making  optimisation  linear least norm optimization problem  linear least distance optimization problems  nonlinear least distance optimization  optimal value  minimum norm optimization  Optimization  Measurement  Manifolds  Mathematical model  Robots  Covariance matrices  Gaussian distribution 
Abstract: A variety of optimization problems takes the form of a minimum norm optimization. In this paper, we study the change of optimal values between two incrementally constructed least norm optimization problems, with new measurements included in the second one. We prove an exact equation to calculate the change of optimal values in the linear least norm optimization problem. With the result in this paper, the change of the optimal values can be pre-calculated as a metric to guide online decision makings, without solving the second optimization problem as long the solution and covariance of the first optimization problem are available. The result can be extended to linear least distance optimization problems, and nonlinear least distance optimization with (nonlinear) equality constraints through linearizations. This derivation in this paper provides a theoretically sound explanation to the empirical observations shown in [1]. As an additional contribution, we propose another optimization problem, i.e. aligning two trajectories at given poses, to further demonstrate how to use the metric. The accuracy of the metric is validated with numerical examples, which is quite satisfactory in general (see the experiments in [1] as well), unless in some extremely adverse scenarios. Last but not least, calculating the optimal value by the proposed metric is at least one magnitude faster than solving the corresponding optimization problems directly.


Title: A Flexible Method for Performance Evaluation of Robot Localization
Key Words: image motion analysis  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  robot localization  research issue  mobile robotics  performance assessment  robot SLAM algorithms  localization accuracy  SLAM algorithm  benchmark datasets  motion capture  environment-specific  spatial coverage  SLAM performance evaluation  distinctive markers  robot navigation environment  generative latent optimization problem  local robot-to-marker  global robot  Simultaneous localization and mapping  Navigation  Cameras  Performance evaluation  Robot localization 
Abstract: An important research issue in mobile robotics is performance assessment of robot SLAM algorithms in terms of their localization accuracy. Typically, SLAM algorithms are evaluated with the help of benchmark datasets or expensive equipment such as motion capture. Benchmark datasets however, are environment-specific, and use of motion capture constrains spatial coverage and affordability. In this paper, we present a novel method for SLAM performance evaluation, which only uses distinctive markers (such as AR tags), randomly placed in the robot navigation environment at arbitrary locations, and observes these markers with a camera onboard of the robot. Formulated as a generative latent optimization (GLO) problem, our method uses the local robot-to-marker poses to evaluate the global robot pose estimates by a SLAM algorithm and therefore its performance. Through extensive experiments on two robots, three localization/SLAM algorithms and both LiDAR and RGB-D sensors, we demonstrate the feasibility and accuracy of our proposed method.


Title: Quantifying Good Seamanship For Autonomous Surface Vessel Performance Evaluation
Key Words: collision avoidance  decision making  marine safety  marine vehicles  mobile robots  remotely operated vehicles  ships  performance metrics  ASV decision-making  collision risk  ASV planning strategies  International Regulations for Prevention of Collisions at Sea  quantified good seamanship  COLREGS compliance  vessel interactions  autonomous surface vehicle decision-making  autonomous surface vessel performance evaluation  seamanship performance criteria  Marine vehicles  Safety  Geometry  Decision making  Navigation  Risk management  Planning 
Abstract: The current state-of-the-art for testing and evaluation of autonomous surface vehicle (ASV) decision-making is currently limited to one-versus-one vessel interactions by determining compliance with the International Regulations for Prevention of Collisions at Sea, referred to as COLREGS. Strict measurement of COLREGS compliance, however, loses value in multi-vessel encounters, as there can be conflicting rules which make determining compliance extremely subjective. This work proposes several performance metrics to evaluate ASV decision-making based on the concept of "good seamanship," a practice which generalizes to multi-vessel encounters. Methodology for quantifying good seamanship is presented based on the criteria of reducing the overall collision risk of the situation and taking early, appropriate actions. Case study simulation results are presented to showcase the seamanship performance criteria against different ASV planning strategies.


Title: Action-conditioned Benchmarking of Robotic Video Prediction Models: a Comparative Study
Key Words: Bayes methods  image sequences  mobile robots  path planning  robot vision  video coding  video signal processing  action-conditioned benchmarking  robotic video prediction models  intelligent systems  video prediction systems  robot actions  video prediction models  frame quality  robot performs  action inference system  robot planning systems  Predictive models  Measurement  Visualization  Stochastic processes  Planning  Robot sensing systems 
Abstract: A defining characteristic of intelligent systems is the ability to make action decisions based on the anticipated outcomes. Video prediction systems have been demonstrated as a solution for predicting how the future will unfold visually, and thus, many models have been proposed that are capable of predicting future frames based on a history of observed frames (and sometimes robot actions). However, a comprehensive method for determining the fitness of different video prediction models at guiding the selection of actions is yet to be developed.Current metrics assess video prediction models based on human perception of frame quality. In contrast, we argue that if these systems are to be used to guide action, necessarily, the actions the robot performs should be encoded in the predicted frames. In this paper, we are proposing a new metric to compare different video prediction models based on this argument. More specifically, we propose an action inference system and quantitatively rank different models based on how well we can infer the robot actions from the predicted frames. Our extensive experiments show that models with high perceptual scores can perform poorly in the proposed action inference tests and thus, may not be suitable options to be used in robot planning systems.


Title: LyRN (Lyapunov Reaching Network): A Real-Time Closed Loop approach from Monocular Vision
Key Words: cameras  closed loop systems  convolutional neural nets  image colour analysis  learning (artificial intelligence)  Lyapunov methods  manipulator dynamics  neurocontrollers  pose estimation  robot vision  velocity control  visual servoing  GTX 1080Ti GPU  grasping mugs  multiinstance control  real-time closed loop  LyRN  single shot RGB 6D pose estimation  complex multiinstance task  reaching action  control Lyapunov function  learning principles  visually guided reaching  Lyapunov reaching network  pose-based-visual-servo grasping system  closed-loop control  over-the-shoulder monocular RGB camera  multiinstance capability  visual control  velocity control  deep convolution neural network  manipulator joint angles  monocular vision  reaching points  frequency 85.0 Hz  Lyapunov methods  Grasping  Feature extraction  Computer architecture  Task analysis  Pose estimation  Velocity control 
Abstract: We propose a closed-loop, multi-instance control algorithm for visually guided reaching based on novel learning principles. A control Lyapunov function methodology is used to design a reaching action for a complex multi-instance task in the case where full state information (poses of all potential reaching points) is available. The proposed algorithm uses monocular vision and manipulator joint angles as the input to a deep convolution neural network to predict the value of the control Lyapunov function (cLf) and corresponding velocity control. The resulting network output is used in real-time as visual control for the grasping task with the multi-instance capability emerging naturally from the design of the control Lyapunov function. We demonstrate the proposed algorithm grasping mugs (textureless and symmetric objects) on a table-top from an over-the-shoulder monocular RGB camera. The manipulator dynamically converges to the best-suited target among multiple identical instances from any random initial pose within the workspace. The system trained with only simulated data is able to achieve 90.3% grasp success rate in the real-world experiments with up to 85Hz closed-loop control on one GTX 1080Ti GPU and significantly outperforms a Pose-Based-Visual-Servo (PBVS) grasping system adapted from a state-of-the-art single shot RGB 6D pose estimation algorithm. A key contribution of the paper is the inclusion of a first-order differential constraint associated with the cLf as a regularisation term during learning, and we provide evidence that this leads to more robust and reliable reaching/grasping performance than vanilla regression on general control inputs.


Title: Object Finding in Cluttered Scenes Using Interactive Perception
Key Words: cameras  image colour analysis  learning (artificial intelligence)  manipulators  object recognition  robot vision  search problems  object finding  cases physical interaction  target object  complex environment  object search  cluttered scenes interactions  reinforcement learning based active perception system  reinforcement learning based interactive perception system  robotic manipulator  RGB  depth camera  Cameras  Task analysis  Robot sensing systems  Search problems  Clutter  Detectors 
Abstract: Object finding in clutter is a skill that requires perception of the environment and in many cases physical interaction. In robotics, interactive perception defines a set of algorithms that leverage actions to improve the perception of the environment, and vice versa use perception to guide the next action. Scene interactions are difficult to model, therefore, most of the current systems use predefined heuristics. This limits their ability to efficiently search for the target object in a complex environment. In order to remove heuristics and the need for explicit models of the interactions, in this work we propose a reinforcement learning based active and interactive perception system for scene exploration and object search. We evaluate our work both in simulated and in real-world experiments using a robotic manipulator equipped with an RGB and a depth camera, and compare our system to two baselines. The results indicate that our approach, trained in simulation only, transfers smoothly to reality and can solve the object finding task efficiently and with more than 88% success rate.


Title: CCAN: Constraint Co-Attention Network for Instance Grasping
Key Words: dexterous manipulators  feature extraction  grippers  learning (artificial intelligence)  robot vision  query image  soft constraints  workspace image  grasp configuration  CCAN  instance grasping  learning-based method  constraint co-attention module  constraint co-attention network  robotic grasping task  end-to-end instance grasping method  grasp affordance predictor  Feature extraction  Grasping  Training  Task analysis  Robots  Data mining  Correlation 
Abstract: Instance grasping is a challenging robotic grasping task when a robot aims to grasp a specified target object in cluttered scenes. In this paper, we propose a novel end-to-end instance grasping method using only monocular workspace and query images, where the workspace image includes several objects and the query image only contains the target object. To effectively extract discriminative features and facilitate the training process, a learning-based method, referred to as Constraint Co-Attention Network (CCAN), is proposed which consists of a constraint co-attention module and a grasp affordance predictor. An effective co-attention module is presented to construct the features of a workspace image from the extracted features of the query image. By introducing soft constraints into the co-attention module, it highlights the target object's features while trivializes other objects' features in the workspace image. Using the features extracted from the co-attention module, the cascaded grasp affordance interpreter network only predicts the grasp configuration for the target object. The training of the CCAN is totally based on simulated self-supervision. Extensive qualitative and quantitative experiments show the effectiveness of our method both in simulated and real-world environments even for totally unseen objects.


Title: The Relative Confusion Matrix, a Tool to Assess Classifiablility in Large Scale Picking Applications
Key Words: image classification  industrial robots  logistics  materials handling equipment  robot vision  warehousing  relative confusion matrix  mixed-product bin  robot workstation  manual picking station  bin picking robot  logistics installations  warehouse  image dataset  Robots  Measurement  Task analysis  Reliability  Tools  Logistics  Workstations 
Abstract: For bin picking robots in real logistics installations, the certainty of picking the correct product out of a mixed-product bin is essential. This paper proposes an approach for the robot to efficiently decide whether it can robustly distinguish the product to pick from the others in the bin. If not, the pick has to be routed not to the robot workstation but to a manual picking station. For this, we introduce a modified version of the confusion matrix, which we call the relative confusion matrix. We show how this matrix can be used to make the required decision, taking into account that all other products in the warehouse can be logically ruled out as they are not contained in the bin. Considering only this subset of products would require a re-computation of the standard confusion matrix. With the relative confusion matrix, no such re-computation is needed, which makes our approach more efficient. We show the usefulness of our approach in extensive experiments with a real bin picking robot, on simulated data, and on a publicly available image dataset.


Title: PrimiTect: Fast Continuous Hough Voting for Primitive Detection
Key Words: computational geometry  feature extraction  Hough transforms  object detection  data abstraction  geometric primitives  compact representation  PrimiTect  fast continuous Hough voting  primitive detection  3D point sets  semiglobal Hough voting scheme  robotics applications  local low-dimensional parameterization  Three-dimensional displays  Feature extraction  Interpolation  Two dimensional displays  Transforms  Computational modeling  Shape 
Abstract: This paper tackles the problem of data abstraction in the context of 3D point sets. Our method classifies points into different geometric primitives, such as planes and cones, leading to a compact representation of the data. Being based on a semi-global Hough voting scheme, the method does not need initialization and is robust, accurate, and efficient. We use a local, low-dimensional parameterization of primitives to determine type, shape and pose of the object that a point belongs to. This makes our algorithm suitable to run on devices with low computational power, as often required in robotics applications. The evaluation shows that our method outperforms state-of-the-art methods both in terms of accuracy and robustness.


Title: FarSee-Net: Real-Time Semantic Segmentation by Efficient Multi-scale Context Aggregation and Feature Space Super-resolution
Key Words: convolutional neural nets  feature extraction  image resolution  image sampling  image segmentation  object detection  real-time systems  robot vision  feature space superresolution  FarSee-Net  real time semantic segmentation  cascaded factorized atrous spatial pyramid pooling  feature maps  convolutional neural networks  multiscale context aggregation  object scale variations  robotic applications  subsampled image  Semantics  Convolution  Image segmentation  Feature extraction  Real-time systems  Spatial resolution 
Abstract: Real-time semantic segmentation is desirable in many robotic applications with limited computation resources. One challenge of semantic segmentation is to deal with the object scale variations and leverage the context. How to perform multi-scale context aggregation within limited computation budget is important. In this paper, firstly, we introduce a novel and efficient module called Cascaded Factorized Atrous Spatial Pyramid Pooling (CF-ASPP). It is a lightweight cas-caded structure for Convolutional Neural Networks (CNNs) to efficiently leverage context information. On the other hand, for runtime efficiency, state-of-the-art methods will quickly decrease the spatial size of the inputs or feature maps in the early network stages. The final high-resolution result is usually obtained by non-parametric up-sampling operation (e.g. bilinear interpolation). Differently, we rethink this pipeline and treat it as a super-resolution process. We use optimized super-resolution operation in the up-sampling step and improve the accuracy, especially in sub-sampled input image scenario for real-time applications. By fusing the above two improvements, our methods provide better latency-accuracy trade-off than the other state-of-the-art methods. In particular, we achieve 68.4% mIoU at 84 fps on the Cityscapes test set with a single Nivida Titan X (Maxwell) GPU card. The proposed module can be plugged into any feature extraction CNN and benefits from the CNN structure development.


Title: C-3PO: Cyclic-Three-Phase Optimization for Human-Robot Motion Retargeting based on Reinforcement Learning
Key Words: human-robot interaction  learning (artificial intelligence)  mobile robots  motion control  multi-robot systems  human-robot motion retargeting  kinematic configurations  kinematic independent general solution  three-phase optimization method  deep reinforcement learning  motion retargeting learning  motion retargeting policy  motion retargeting skill  human skeleton  cyclic-three-phase optimization  NAO robot  Pepper robot  Baxter robot  C-3PO robot  Skeleton  Robot motion  Robot kinematics  Kinematics  Zirconium  Torso 
Abstract: Motion retargeting between heterogeneous polymorphs with different sizes and kinematic configurations requires a comprehensive knowledge of (inverse) kinematics. Moreover, it is non-trivial to provide a kinematic independent general solution. In this study, we developed a cyclic three-phase optimization method based on deep reinforcement learning for human-robot motion retargeting. The motion retargeting learning is performed using refined data in a latent space by the cyclic and filtering paths of our method. In addition, the human- in-the-loop based three-phase approach provides a framework for the improvement of the motion retargeting policy by both quantitative and qualitative manners. Using the proposed C- 3PO method, we were successfully able to learn the motion retargeting skill between the human skeleton and motion of the multiple robots such as NAO, Pepper, Baxter and C-3PO.


Title: AP-MTL: Attention Pruned Multi-task Learning Model for Real-time Instrument Detection and Segmentation in Robot-assisted Surgery
Key Words: endoscopes  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  robot vision  surgery  attention pruned multitask learning model  real-time instrument detection  robot-assisted surgery  image-guided robotic surgery  real-time robotic system  weight-shared encoder  task-aware detection  asynchronous task-aware optimization  robotic instrument segmentation dataset  end-to-end trainable realtime multitask learning model  global attention dynamic pruning  skip squeeze and excitation module  Task analysis  Instruments  Decoding  Real-time systems  Computational modeling  Optimization  Surgery 
Abstract: Surgical scene understanding and multi-tasking learning are crucial for image-guided robotic surgery. Training a real-time robotic system for the detection and segmentation of high-resolution images provides a challenging problem with the limited computational resource. The perception drawn can be applied in effective real-time feedback, surgical skill assessment, and human-robot collaborative surgeries to enhance surgical outcomes. For this purpose, we develop a novel end-to-end trainable real-time Multi-Task Learning (MTL) model with weight-shared encoder and task-aware detection and segmentation decoders. Optimization of multiple tasks at the same convergence point is vital and presents a complex problem. Thus, we propose an asynchronous task-aware optimization (ATO) technique to calculate task-oriented gradients and train the decoders independently. Moreover, MTL models are always computationally expensive, which hinder real-time applications. To address this challenge, we introduce a global attention dynamic pruning (GADP) by removing less significant and sparse parameters. We further design a skip squeeze and excitation (SE) module, which suppresses weak features, excites significant features and performs dynamic spatial and channel-wise feature re-calibration. Validating on the robotic instrument segmentation dataset of MICCAI endoscopic vision challenge, our model significantly outperforms state-of-the-art segmentation and detection models, including best-performed models in the challenge.


Title: Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search
Key Words: gesture recognition  image classification  image segmentation  learning (artificial intelligence)  medical robotics  neural nets  surgery  tree searching  video signal processing  video surveillance  joint surgical gesture segmentation  tree search algorithm  neural networks design  reinforcement learning framework  surgical robotic applications  surgical video classification  baseline methods  JIGSAWS dataset  surgery surveillance  automatic surgical gesture recognition  robot-assisted surgery  Surgery  Gesture recognition  Robots  Hidden Markov models  Learning (artificial intelligence)  Feature extraction  Task analysis  Surgical gesture recognition  Deep reinforcement learning in robotics  Tree search  Robotic surgery 
Abstract: Automatic surgical gesture recognition is fundamental for improving intelligence in robot-assisted surgery, such as conducting complicated tasks of surgery surveillance and skill evaluation. However, current methods treat each frame individually and produce the outcomes without effective consideration on future information. In this paper, we propose a framework based on reinforcement learning and tree search for joint surgical gesture segmentation and classification. An agent is trained to segment and classify the surgical video in a human-like manner whose direct decisions are re-considered by tree search appropriately. Our proposed tree search algorithm unites the outputs from two designed neural networks, i.e., policy and value network. With the integration of complementary information from distinct models, our framework is able to achieve the better performance than baseline methods using either of the neural networks. For an overall evaluation, our developed approach consistently outperforms the existing methods on the suturing task of JIGSAWS dataset in terms of accuracy, edit score and F1 score. Our study highlights the utilization of tree search to refine actions in reinforcement learning framework for surgical robotic applications.


Title: ACNN: a Full Resolution DCNN for Medical Image Segmentation
Key Words: biomedical MRI  computerised tomography  convolutional neural nets  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  neural nets  surgery  medical image segmentation  robot-assisted Minimally Invasive Surgeries  current DCNNs  sampling layer  receptive field  spatial dimension  feature maps  atrous convolutional layers  ACNN  magnetic resonance imaging  computed tomography image segmentation  segmentation Intersection  Atrous convolutional neural network  full-resolution DCNN  U-Net  Deeplabv3+  Image segmentation  Convolution  Biomedical imaging  Image resolution  Convolutional neural networks  Three-dimensional displays  Robots 
Abstract: Deep Convolutional Neural Networks (DCNNs) are used extensively in medical image segmentation and hence 3D navigation for robot-assisted Minimally Invasive Surgeries (MISs). However, current DCNNs usually use down sampling layers for increasing the receptive field and gaining abstract semantic information. These down sampling layers decrease the spatial dimension of feature maps, which can be detrimental to image segmentation. Atrous convolution is an alternative for the down sampling layer. It increases the receptive field whilst maintains the spatial dimension of feature maps. In this paper, a method for effective atrous rate setting is proposed to achieve the largest and fully-covered receptive field with a minimum number of atrous convolutional layers. Furthermore, a new and full resolution DCNN - Atrous Convolutional Neural Network (ACNN), which incorporates cascaded atrous II-blocks, residual learning and Instance Normalization (IN) is proposed. Application results of the proposed ACNN to Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) image segmentation demonstrate that the proposed ACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net and Deeplabv3+, but with reduced trainable parameters.


Title: Hyperproperties for Robotics: Planning via HyperLTL
Key Words: formal specification  path planning  robots  temporal logic  hyperproperties  formal methods  temporal logic objectives  hyper-temporal logics  multiple paths  HyperLTL specifications  planning strategies  robotic planning  discrete transition systems  Planning  Robustness  Privacy  Automata  Model checking  Task analysis 
Abstract: There is a growing interest on formal methods-based robotic planning for temporal logic objectives. In this work, we extend the scope of existing synthesis methods to hyper-temporal logics. We are motivated by the fact that important planning objectives, such as optimality, robustness, and privacy, (maybe implicitly) involve the interrelation between multiple paths. Such objectives are thus hyperproperties, and cannot be expressed with usual temporal logics like the linear temporal logic (LTL). We show that such hyperproperties can be expressed by HyperLTL, an extension of LTL to multiple paths. To handle the complexity of planning with HyperLTL specifications, we introduce a symbolic approach for synthesizing planning strategies on discrete transition systems. Our planning method is evaluated on several case studies.


Title: Abstractions for computing all robotic sensors that suffice to solve a planning problem
Key Words: computational complexity  control engineering computing  data structures  graph theory  planning (artificial intelligence)  robots  search problems  sensors  robotic sensors  planning problem  search algorithms  sensor designs  design trade-offs  sensor maps  potential sensors  outer limits  search space  data structures  single special representative  task domain knowledge  sensor technology  particular problem instances  sensor characterization pairs  yielding solutions  Robot sensing systems  Planning  Sensor phenomena and characterization  Uncertainty  Collision avoidance 
Abstract: Whether a robot can perform some specific task depends on several aspects, including the robot's sensors and the plans it possesses. We are interested in search algorithms that treat plans and sensor designs jointly, yielding solutions-i.e., plan and sensor characterization pairs-if and only if they exist. Such algorithms can help roboticists explore the space of sensors to aid in making design trade-offs. Generalizing prior work where sensors are modeled abstractly as sensor maps on p-graphs, the present paper increases the potential sensors which can be sought significantly. But doing so enlarges a problem currently on the outer limits of being considered tractable. Toward taming this complexity, two contributions are made: (1) we show how to represent the search space for this more general problem and describe data structures that enable whole sets of sensors to be summarized via a single special representative; (2) we give a means by which other structure (either task domain knowledge, sensor technology or fabrication constraints) can be incorporated to reduce the sets to be enumerated. These lead to algorithms that we have implemented and which suffice to solve particular problem instances, albeit only of small scale. Nevertheless, the algorithm aids in helping understand what attributes sensors must possess and what information they must provide in order to ensure a robot can achieve its goals despite non-determinism.


Title: T* : A Heuristic Search Based Path Planning Algorithm for Temporal Logic Specifications
Key Words: collision avoidance  graph theory  mobile robots  navigation  search problems  temporal logic  trajectory control  obstacle avoidance  heuristic search based path planning algorithm  temporal logic path planning  graph search problem  point-to-point navigation  temporal logic specifications  temporal logic query  optimal trajectory  Dijkstra's shortest path algorithm  Automata  Heuristic algorithms  Trajectory  Task analysis  Robot kinematics 
Abstract: The fundamental path planning problem for a mobile robot involves generating a trajectory for point-to-point navigation while avoiding obstacles. Heuristic-based search algorithms like A* have been shown to be efficient in solving such planning problems. Recently, there has been an increased interest in specifying complex path planning problem using temporal logic. In the state-of-the-art algorithm, the temporal logic path planning problem is reduced to a graph search problem, and Dijkstra's shortest path algorithm is used to compute the optimal trajectory satisfying the specification. The A* algorithm, when used with an appropriate heuristic for the distance from the destination, can generate an optimal path in a graph more efficiently than Dijkstra's shortest path algorithm. The primary challenge for using A* algorithm in temporal logic path planning is that there is no notion of a single destination state for the robot. We present a novel path planning algorithm T* that uses the A* search procedure opportunistically to generate an optimal trajectory satisfying a temporal logic query. Our experimental results demonstrate that T* achieves an order of magnitude improvement over the state-of-the-art algorithm to solve many temporal logic path planning problems in 2-D as well as 3-D workspaces.


Title: Global/local motion planning based on Dynamic Trajectory Reconfiguration and Dynamical Systems for Autonomous Surgical Robots
Key Words: collision avoidance  medical robotics  mobile robots  motion control  splines (mathematics)  surgery  robotic minimally invasive surgery  geometric constraints  desired task  final target  moving obstacles  developed motion planner  two-layer architecture  global level computes smooth spline-based trajectories  collision free connections  realistic surgical scenario  autonomous surgical  collision-free trajectories  autonomous execution  assistive tasks  dynamical systems based obstacle avoidance  Trajectory  Tools  Robots  Collision avoidance  Task analysis  Surgery  Splines (mathematics) 
Abstract: This paper addresses the generation of collision-free trajectories for the autonomous execution of assistive tasks in Robotic Minimally Invasive Surgery (R-MIS). The proposed approach takes into account geometric constraints related to the desired task, like for example the direction to approach the final target and the presence of moving obstacles. The developed motion planner is structured as a two-layer architecture: a global level computes smooth spline-based trajectories that are continuously updated using virtual potential fields; a local level, exploiting Dynamical Systems based obstacle avoidance, ensures collision free connections among the spline control points. The proposed architecture is validated in a realistic surgical scenario.


Title: Deep Imitative Reinforcement Learning for Temporal Logic Robot Motion Planning with Noisy Semantic Observations
Key Words: Markov processes  mobile robots  motion control  neurocontrollers  path planning  probability  temporal logic  noisy semantic observations  mobile robots  linear temporal logic specifications  robot sensing error  probabilistic labels  labeled transition system  robot mobility  labeled Markov decision process  unknown transition probabilities  product-based model checkers  probabilistic labeling functions  Q-learning agent  deep imitative reinforcement learning  temporal logic robot motion planning  deep imitative Q-learning method  DIQL  control policies synthesis  LTL  LMDP  suboptimal instructions  Robot sensing systems  Labeling  Uncertainty  Semantics  Probabilistic logic 
Abstract: In this paper, we propose a Deep Imitative Q-learning (DIQL) method to synthesize control policies for mobile robots that need to satisfy Linear Temporal Logic (LTL) specifications using noisy semantic observations of their surroundings. The robot sensing error is modeled using probabilistic labels defined over the states of a Labeled Transition System (LTS) and the robot mobility is modeled using a Labeled Markov Decision Process (LMDP) with unknown transition probabilities. We use existing product-based model checkers (PMCs) as experts to guide the Q-learning algorithm to convergence. To the best of our knowledge, this is the first approach that models noise in semantic observations using probabilistic labeling functions and employs existing model checkers to provide suboptimal instructions to the Q-learning agent.


Title: AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
Key Words: autonomous aerial vehicles  cameras  computer vision  image annotation  image capture  image colour analysis  object detection  video signal processing  video surveillance  multimodal unmanned aerial vehicle dataset  low altitude traffic surveillance  UAVs  mounted cameras  aerial image capture  aerial visual data  object detection algorithms  computer vision community  object annotations  flying-cameras  multipurpose aerial dataset  multimodal sensor data  AU-AIR dataset  meta-data  traffic-related object category  mobile object detectors  real-time object detection  robotics  real-world outdoor environments  bounding box annotation  RGB videos recording  YOLOv3-Tiny  MobileNetv2-SSDLite  on-board computers  bird-view image  data types recording  Object detection  Videos  Visualization  Cameras  Detectors  Surveillance 
Abstract: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images. As a result of this, several aerial datasets have been introduced, including visual data with object annotations. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors). In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for traffic-related object category) from recorded RGB videos. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics. The dataset is available at https://bozcani.github.io/auairdataset.


Title: Design and Autonomous Stabilization of a Ballistically-Launched Multirotor
Key Words: aerodynamics  autonomous aerial vehicles  ballistics  helicopters  mobile robots  robot vision  aircraft  drones  emergency response  space exploration  critical situational data  onboard sensors  multirotor prototype  onboard sensor suite  autonomy pipeline  aerodynamic stability  active stabilization  ballistic launch  streamlined quick unfolding investigation drone  vision-based autonomous transition  SQUID  SQUIDs  Electron tubes  Aerodynamics  Drones  Prototypes  Thermal stability  Aircraft 
Abstract: Aircraft that can launch ballistically and convert to autonomous, free-flying drones have applications in many areas such as emergency response, defense, and space exploration, where they can gather critical situational data using onboard sensors. This paper presents a ballistically-launched, autonomously-stabilizing multirotor prototype (SQUID - Streamlined Quick Unfolding Investigation Drone) with an onboard sensor suite, autonomy pipeline, and passive aerodynamic stability. We demonstrate autonomous transition from passive to vision-based, active stabilization, confirming the multirotor's ability to autonomously stabilize after a ballistic launch in a GPS-denied environment.


Title: Asynchronous event-based clustering and tracking for intrusion monitoring in UAS
Key Words: autonomous aerial vehicles  cameras  image sensors  object tracking  pattern clustering  robot vision  surveillance  feature tracking  intrusion monitoring  UAS  unmanned aerial systems  perception systems  illumination conditions  event cameras  neuromorphic sensors  illumination changes  event based vision  event stream  intruder monitoring  event clustering  event-by-event processing  asynchronous event-based clustering  automatic surveillance  on-board hardware computational constraints  Cameras  Robot vision systems  Tracking  Surveillance  Robustness  event camera  asynchronous  intrusion monitoring  surveillance  UAS  clustering  feature tracking 
Abstract: Automatic surveillance and monitoring using Unmanned Aerial Systems (UAS) require the development of perception systems that robustly work under different illumination conditions. Event cameras are neuromorphic sensors that capture the illumination changes in the scene with very low latency and high dynamic range. Although recent advances in eventbased vision have explored the use of event cameras onboard UAS, most techniques group events in frames and, therefore, do not fully exploit the sequential and asynchronous nature of the event stream. This paper proposes a fully asynchronous scheme for intruder monitoring using UAS. It employs efficient event clustering and feature tracking modules and includes a sampling mechanism to cope with the computational cost of event-by-event processing adapting to on-board hardware computational constraints. The proposed scheme was tested on a real multirotor in challenging scenarios showing significant accuracy and robustness to lighting conditions.


Title: Flydar: Magnetometer-based High Angular Rate Estimation during Gyro Saturation for SLAM
Key Words: gyroscopes  Kalman filters  magnetometers  mobile robots  nonlinear filters  optical radar  SLAM (robots)  Flydar  magnetometer-based high angular rate estimation  SLAM  simultaneous localisation and mapping  Flying Li-DAR  EKF-based algorithm  sinusoidal magnetometer measurement  continuously rotating airframe  IMU sensors  gyro measurement  gyro bias  gyro saturation condition  rotating locomotion  robot hovering angular velocity  Robots  Magnetometers  Sensors  Estimation  Frequency measurement  Saturation magnetization  Angular velocity 
Abstract: In this paper, the high angular rate estimation for simultaneous localisation and mapping (SLAM) of a Flying Li-DAR (Flydar) is presented. The proposed EKF-based algorithm exploits the sinusoidal magnetometer measurement generated by the continuously rotating airframe for estimation of the robot hovering angular velocity. Significantly, the proposed method does not rely on additional sensors other than existing IMU sensors already being used for flight stabilization. The gyro measurement and the gyro bias are incorporated as a control input and a filter state respectively to enable estimation even under gyro saturation condition. Additionally, this work proposes leveraging on the inherently rotating locomotion to generate a planar lidar scan using only a single-point laser for possible lightweight autonomy. The proposed estimation method was experimentally evaluated on a ground rotating rig up to twice the gyro saturation limit with an effective rms error of 0.0045Hz; and on the proposed aerial platform - Flydar - hovering beyond the saturation limit with a rms error of 0.0056Hz. Lastly, the proposed method for SLAM using the rotating dynamics of Flydar was demonstrated with a localisation accuracy of 0.11m.


Title: Map-Predictive Motion Planning in Unknown Environments
Key Words: mobile robots  navigation  path planning  predictive control  trajectory control  map prediction  data-driven method  autonomous navigation  hallway environments  naïve frontier pursuit method  heuristic methods  map-predictive motion planning  dynamically-constrained robots  trajectory planning  robot position  frontier selection heuristics  Robots  Trajectory  Planning  Navigation  Collision avoidance  Safety  Cognition 
Abstract: Algorithms for motion planning in unknown environments are generally limited in their ability to reason about the structure of the unobserved environment. As such, current methods generally navigate unknown environments by relying on heuristic methods to choose intermediate objectives along frontiers. We present a unified method that combines map prediction and motion planning for safe, time-efficient au-tonomous navigation of unknown environments by dynamically-constrained robots. We propose a data-driven method for predicting the map of the unobserved environment, using the robot's observations of its surroundings as context. These map predictions are then used to plan trajectories from the robot's position to the goal without requiring frontier selection. We applied this map-predictive motion planning strategy to randomly generated winding hallway environments, yielding substantial improvement in trajectory duration over a naïve frontier pursuit method. We also experimentally demonstrate similar performance to methods using more sophisticated fron-tier selection heuristics while significantly reducing computation time.


Title: An Efficient and Continuous Approach to Information-Theoretic Exploration
Key Words: computational complexity  information theory  mobile robots  path planning  robot vision  SLAM (robots)  information-theoretic exploration  continuous occupancy map framework  |Θ| measurement beams  recursive structure  robotics applications  autonomous navigation task  Robot sensing systems  Mutual information  Distortion measurement  Gain measurement  Time measurement 
Abstract: Exploration of unknown environments is embedded and essential in many robotics applications. Traditional algorithms, that decide where to explore by computing the expected information gain of an incomplete map from future sensor measurements, are limited to very powerful computational platforms. In this paper, we describe a novel approach for computing this expected information gain efficiently, as principally derived via mutual information. The key idea behind the proposed approach is a continuous occupancy map framework and the recursive structure it reveals. This structure makes it possible to compute the expected information gain of sensor measurements across an entire map much faster than computing each measurements' expected gain independently. Specifically, for an occupancy map composed of |M| cells and a range sensor that emits |Θ| measurement beams, the algorithm (titled FCMI) computes the information gain corresponding to measurements made at each cell in O(|Θ||M|) steps. To the best of our knowledge, this complexity bound is better than all existing methods for computing information gain. In our experiments, we observe that this novel, continuous approach is two orders of magnitude faster than the state-of-the-art FSMI algorithm.


Title: A Feature-Based Underwater Path Planning Approach using Multiple Perspective Prior Maps
Key Words: autonomous underwater vehicles  bathymetry  maximum likelihood estimation  mobile robots  navigation  path planning  remotely operated vehicles  multiple perspective prior maps  path planning methodology  Autonomous Underwater Vehicles  AUV  shallow complex environments  coral reefs  aerial photographic survey  bathymetric information  prior map  navigation graph  test points  shortest paths  destination points  maximum likelihood function  misclassified objects  photo-realistic simulated environment  Navigation  Cameras  Sensors  Uncertainty  Image segmentation  Feature extraction  Robots 
Abstract: This paper presents a path planning methodology which enables Autonomous Underwater Vehicles (AUVs) to navigate in shallow complex environments such as coral reefs. The approach leverages prior information from an aerial photographic survey, and derived bathymetric information of the corresponding area. From these prior maps, a set of features is obtained which define an expected arrangement of objects and bathymetry likely to be perceived by the AUV when underwater. A navigation graph is then constructed by predicting the arrangement of features visible from a set of test points within the prior, which allows the calculation of the shortest paths from any pair of start and destination points. A maximum likelihood function is defined which allows the AUV to match its observations to the navigation graph as it undertakes its mission. To improve robustness, the history of observed features are retained to facilitate possible recovery from non-detectable or misclassified objects. The approach is evaluated using a photo-realistic simulated environment, and results illustrate the merits of the approach even when only a relatively small number of features can be identified from the prior map.


Title: Estimating Motion Uncertainty with Bayesian ICP
Key Words: Bayes methods  image fusion  iterative methods  Markov processes  Monte Carlo methods  motion estimation  pose estimation  motion uncertainty  accurate uncertainty estimation  pose transformation  autonomous navigation  data fusion  iterative closest point  point cloud pairs  motion estimation  deterministic algorithm  probabilistic manner  data association errors  sensor noise  overconfident transformation estimates  pose uncertainty  Markov Chain Monte Carlo algorithm  scalable Bayesian sampling  stochastic gradient Langevin dynamics  data association uncertainty  3D Kinect data  Bayesian ICP  Iterative closest point algorithm  Uncertainty  Bayes methods  Three-dimensional displays  Robot sensing systems  Standards  Stochastic processes 
Abstract: Accurate uncertainty estimation associated with the pose transformation between two 3D point clouds is critical for autonomous navigation, grasping, and data fusion. Iterative closest point (ICP) is widely used to estimate the transformation between point cloud pairs by iteratively performing data association and motion estimation. Despite its success and popularity, ICP is effectively a deterministic algorithm, and attempts to reformulate it in a probabilistic manner generally do not capture all sources of uncertainty, such as data association errors and sensor noise. This leads to overconfident transformation estimates, potentially compromising the robustness of systems relying on them. In this paper we propose a novel method to estimate pose uncertainty in ICP with a Markov Chain Monte Carlo (MCMC) algorithm. Our method combines recent developments in optimization for scalable Bayesian sampling such as stochastic gradient Langevin dynamics (SGLD) to infer a full posterior distribution of the pose transformation between two point clouds. We evaluate our method, called Bayesian ICP, in experiments using 3D Kinect data demonstrating that our method is capable of both quickly and accuractely estimating pose uncertainty, taking into account data association uncertainty as reflected by the shape of the objects.


Title: Actively Mapping Industrial Structures with Information Gain-Based Planning on a Quadruped Robot
Key Words: collision avoidance  image representation  legged locomotion  robot vision  industrial structure  mapping industrial structures  information gain-based planning  quadruped robot  online active mapping system  voxel representation  NBV  expected information gain  terrain map  ANYbotics ANYmal robot  Robot sensing systems  Service robots  Solid modeling  Planning  Laser radar 
Abstract: In this paper, we develop an online active mapping system to enable a quadruped robot to autonomously survey large physical structures. We describe the perception, planning and control modules needed to scan and reconstruct an object of interest, without requiring a prior model. The system builds a voxel representation of the object, and iteratively determines the Next-Best-View (NBV) to extend the representation, according to both the reconstruction itself and to avoid collisions with the environment. By computing the expected information gain of a set of candidate scan locations sampled on the as-sensed terrain map, as well as the cost of reaching these candidates, the robot decides the NBV for further exploration. The robot plans an optimal path towards the NBV, avoiding obstacles and un-traversable terrain. Experimental results on both simulated and real-world environments show the capability and efficiency of our system. Finally we present a full system demonstration on the real robot, the ANYbotics ANYmal, autonomously reconstructing a building facade and an industrial structure.


Title: Least-squares Optimal Relative Planar Motion for Vehicle-mounted Cameras
Key Words: calibration  computer vision  image motion analysis  least squares approximations  optimisation  polynomials  closed-form solver  point correspondences  camera movement  motion parameters  vehicle-mounted cameras  least-squares optimal relative planar motion  6th degree polynomial  Cameras  Transmission line matrix methods  Robot vision systems  Estimation  Automobiles  Geometry 
Abstract: A new closed-form solver is proposed minimizing the algebraic error optimally, in the least squares sense, to estimate the relative planar motion of two calibrated cameras. The main objective is to solve the over-determined case, i.e., when a larger-than-minimal sample of point correspondences is given - thus, estimating the motion from at least three correspondences. The algorithm requires the camera movement to be constrained to a plane, e.g. mounted to a vehicle, and the image plane to be orthogonal to the ground.1 The solver obtains the motion parameters as the roots of a 6th degree polynomial. It is validated both in synthetic experiments and on publicly available real-world datasets that using the proposed solver leads to results superior to the state-of-the-art in terms of geometric accuracy with no noticeable deterioration in the processing time.


Title: Relative planar motion for vehicle-mounted cameras from a single affine correspondence
Key Words: calibration  cameras  computer vision  image motion analysis  image sensors  relative planar motion  vehicle-mounted cameras  single affine correspondence  extrinsic camera parameters  general planar motion  camera movement  image plane  minimal solver  semicalibrated case  common focal length  fully calibrated case  Cameras  Transmission line matrix methods  Mathematical model  Estimation  Geometry  Robot vision systems  Robustness 
Abstract: Two solvers are proposed for estimating the extrinsic camera parameters from a single affine correspondence assuming general planar motion. In this case, the camera movement is constrained to a plane and the image plane is orthogonal to the ground. The algorithms do not assume other constraints, e.g. the non-holonomic one, to hold. A new minimal solver is proposed for the semi-calibrated case, i.e. the camera parameters are known except a common focal length. Another method is proposed for the fully calibrated case. Due to requiring a single correspondence, robust estimation, e.g. histogram voting, leads to a fast and accurate procedure. The proposed methods are tested in our synthetic environment and on publicly available real datasets consisting of videos through tens of kilometers. They are superior to the state-of-the-art both in terms of accuracy and processing time.


Title: Moving object detection for visual odometry in a dynamic environment based on occlusion accumulation
Key Words: cameras  distance measurement  image colour analysis  image segmentation  image sensors  mobile robots  object detection  pose estimation  regression analysis  robot vision  dynamic environment  simple moving object detection algorithm  dense visual odometry  VO algorithms  occlusion accumulation  color images  robotic navigation  real-time RGBD data  depth information  obstacle recognition  camera pose estimate  bi-square regression weight  segmentation accuracy  public datasets  Cameras  Heuristic algorithms  Object detection  Robustness  Trajectory  Visual odometry  Feature extraction 
Abstract: Detection of moving objects is an essential capability in dealing with dynamic environments. Most moving object detection algorithms have been designed for color images without depth. For robotic navigation where real-time RGBD data is often readily available, utilization of the depth information would be beneficial for obstacle recognition. Here, we propose a simple moving object detection algorithm that uses RGB-D images. The proposed algorithm does not require estimating a background model. Instead, it uses an occlusion model which enables us to estimate the camera pose on a background confused with moving objects that dominate the scene. The proposed algorithm allows to separate the moving object detection and visual odometry (VO) so that an arbitrary robust VO method can be employed in a dynamic situation with a combination of moving object detection, whereas other VO algorithms for a dynamic environment are inseparable. In this paper, we use dense visual odometry (DVO) as a VO method with a bi-square regression weight. Experimental results show the segmentation accuracy and the performance improvement of DVO in the situations. We validate our algorithm in public datasets and our dataset which also publicly accessible.


Title: A Parametric Grasping Methodology for Multi-Manual Interactions in Real-Time Dynamic Simulations
Key Words: biomechanics  control engineering computing  data visualisation  kinematics  manipulators  medical robotics  rendering (computer graphics)  surgery  telerobotics  parametric grasping methodology  multimanual interactions  interactive simulators  training simulators  teleoperated robotic laparoscopic surgery  stateof-art simulators  realistic visuals  accurate dynamics  kinematic simplification techniques  truly multimanual manipulation  actual task  realistic grasping  rigid-body dynamics  collision computation techniques  state-of-the-art physics libraries  parametric approach  multimanual grasping  real-time dynamic simulation  accomplishing multimanual tasks  screwdriver task  Friction  Grasping  Force  Sensors  Computational modeling  Mathematical model  Robots 
Abstract: Interactive simulators are used in several important applications which include the training simulators for teleoperated robotic laparoscopic surgery. While stateof-art simulators are capable of rendering realistic visuals and accurate dynamics, grasping is often implemented using kinematic simplification techniques that prevent truly multimanual manipulation, which is often an important requirement of the actual task. Realistic grasping and manipulation in simulation is a challenging problem due to the constraints imposed by the implementation of rigid-body dynamics and collision computation techniques in state-of-the-art physics libraries. We present a penalty based parametric approach to achieve multi-manual grasping and manipulation of complex objects at arbitrary postures in a real-time dynamic simulation. This approach is demonstrated by accomplishing multi-manual tasks modeled after realistic scenarios, which include the grasping and manipulation of a two-handed screwdriver task and the manipulation of a deformable thread.


Title: A methodology for the incorporation of arbitrarily-shaped feet in passive bipedal walking dynamics
Key Words: computational geometry  legged locomotion  motion control  pose estimation  public domain software  robot dynamics  stability  ankle trajectory  robot dynamics  shape dependent foot kinetics  OpenPose  open source pose estimation system  rigid foot passive robot  walking robot stability  foot shape optimization  exact foot geometry  dynamic model  biped robot  passive bipedal walking dynamics  Foot  Legged locomotion  Shape  Geometry  Mathematical model  Dynamics 
Abstract: A methodology for implementing arbitrary foot shapes in the passive walking dynamics of biped robots is developed. The dynamic model of a walking robot is defined in a way that allows shape-dependent foot kinetics to contribute to the robot's dynamics, for all convex foot shapes regardless of the exact foot geometry: for the developed method, only the set of points describing the foot profile curve is needed. The method is mathematically derived and then showcased with an application. The open-source pose estimation system OpenPose is used to determine the foot profile that enables the rigid-foot passive robot to reproduce the ankle trajectory of the actively powered, multi-DOF human foot complex. The passive gait of the biped robot walking on the specified foot shape is simulated and analyzed, and a stable walking cycle is found and evaluated. The proposed model enables the study of the effects of foot shape on the walking dynamics of biped robots, eliminating the necessity of solely using simple, and analytically defined geometric shapes as the walking robots' feet. The method can be used for foot shape optimization towards achieving any desired walking pattern in walking robots.


Title: Experimental Analysis of Structural Vibration Problems of a Biped Walking Robot
Key Words: closed loop systems  humanoid robots  legged locomotion  modal analysis  position control  robot dynamics  vibrations  biped using Experimental Modal Analysis  structural design  low level position control  walking control  control design  structural dynamics  LOLA's mechanical structure  biped walking robot  control algorithms  structural vibration problems  structural resonances  control loop resonances  closed-loop identification method  structural modes  Legged locomotion  Robot sensing systems  Foot  Gears  Harmonic analysis 
Abstract: Over the past decade we have been able to vastly improve the control algorithms of our biped walking robot LOLA. Further enhancements, however, are limited by vibration problems caused by the dynamics of LOLA's mechanical structure. In this work, we present small examples how structural dynamics limit our control design for walking control as well as low level position control of the joints. We also provide a procedure to identify weaknesses in the structural design of our biped using Experimental Modal Analysis. Using this method, we could successfully identify the structural modes of the system. Furthermore, we were able to use a closed-loop identification method to show a connection between the control loop resonances and the structural resonances of our robot.


Title: Dynamic Coupling as an Indicator of Gait Robustness for Underactuated Biped Robots
Key Words: legged locomotion  robot dynamics  robust control  trajectory control  dynamic coupling  gait robustness  velocity decomposition  underactuated mechanical systems  two link biped model  underactuated biped robots  trajectory optimization  Couplings  Mathematical model  Legged locomotion  Aerodynamics  Robustness 
Abstract: This paper employs velocity decomposition of underactuated mechanical systems to determine the degree of dynamic coupling in the gaits of a two-link biped model. The degree of coupling between controlled and uncontrolled directions quantifies the control authority the system has over its unactuated degree of freedom. This paper shows that the amount of coupling is directly correlated to gait robustness, as seen through the size of the gait's region of attraction. The analytical measure of coupling is applied in the context of trajectory optimization to generate two-link gaits that maximize or minimize coupling. Simulation studies show that gaits maximizing coupling exhibit significantly superior robustness, as measured by 1) stochastic performance on uneven terrain, 2) ability to maintain desired walking speed under non-vanishing disturbances, 3) size of the region of attraction, and 4) robustness to model uncertainties.


Title: ZMP Constraint Restriction for Robust Gait Generation in Humanoids
Key Words: humanoid robots  legged locomotion  motion control  predictive control  robot dynamics  robust control  stability  range amplitude  internal stability  IS-MPC method  constraint modification  ZMP constraint restriction  robust gait generation  humanoids  humanoid gait generation  robust performance  considered disturbance signals  mid-range value  sampling time  stability constraint  current mid-range disturbance  appropriate restriction  control horizon  Humanoid robots  Lips  Robustness  Stability criteria  Dynamics 
Abstract: We present an extension of our previously proposed IS-MPC method for humanoid gait generation aimed at obtaining robust performance in the presence of disturbances. The considered disturbance signals vary in a range of known amplitude around a mid-range value that can change at each sampling time, but whose current value is assumed to be available. The method consists in modifying the stability constraint that is at the core of IS-MPC by incorporating the current mid-range disturbance, and performing an appropriate restriction of the ZMP constraint in the control horizon on the basis of the range amplitude of the disturbance. We derive explicit conditions for recursive feasibility and internal stability of the IS-MPC method with constraint modification. Finally, we illustrate its superior performance with respect to the nominal version by performing dynamic simulations on the NAO robot.


Title: Hybrid Zero Dynamics Inspired Feedback Control Policy Design for 3D Bipedal Locomotion using Reinforcement Learning
Key Words: feedback  humanoid robots  learning systems  legged locomotion  robot dynamics  hybrid zero dynamics inspired feedback control policy design  3D bipedal locomotion  model-free reinforcement learning framework  feedback control policies  3D bipedal walking  RL algorithms  reference joint trajectories  policy structure  hybrid nature  walking dynamics  RL framework  lightweight network structure  short training time  3D bipedal robot  stable limit walking cycles  walking speed  Legged locomotion  Three-dimensional displays  Trajectory  Robustness  Torso  Robot kinematics 
Abstract: This paper presents a novel model-free reinforcement learning (RL) framework to design feedback control policies for 3D bipedal walking. Existing RL algorithms are often trained in an end-to-end manner or rely on prior knowledge of some reference joint trajectories. Different from these studies, we propose a novel policy structure that appropriately incorporates physical insights gained from the hybrid nature of the walking dynamics and the well-established hybrid zero dynamics approach for 3D bipedal walking. As a result, the overall RL framework has several key advantages, including lightweight network structure, short training time, and less dependence on prior knowledge. We demonstrate the effectiveness of the proposed method on Cassie, a challenging 3D bipedal robot. The proposed solution produces stable limit walking cycles that can track various walking speed in different directions. Surprisingly, without specifically trained with disturbances to achieve robustness, it also performs robustly against various adversarial forces applied to the torso towards both the forward and the backward directions.


Title: Optimal Reduced-order Modeling of Bipedal Locomotion
Key Words: legged locomotion  nonlinear control systems  pendulums  reduced order systems  robot dynamics  springs (mechanical)  five-link model  Cassie bipedal robot  optimal reduced-order modeling  bipedal locomotion  legged locomotion  LIP  spring-loaded inverted pendulum  SLIP  agile maneuvers  high-dimensional system  ground inclines  Task analysis  Legged locomotion  Reduced order systems  Lips  Trajectory optimization 
Abstract: State-of-the-art approaches to legged locomotion are widely dependent on the use of models like the linear inverted pendulum (LIP) and the spring-loaded inverted pendulum (SLIP), popular because their simplicity enables a wide array of tools for planning, control, and analysis. However, they inevitably limit the ability to execute complex tasks or agile maneuvers. In this work, we aim to automatically synthesize models that remain low-dimensional but retain the capabilities of the high-dimensional system. For example, if one were to restore a small degree of complexity to LIP, SLIP, or a similar model, our approach discovers the form of that additional complexity which optimizes performance. In this paper, we define a class of reduced-order models and provide an algorithm for optimization within this class. To demonstrate our method, we optimize models for walking at a range of speeds and ground inclines, for both a five-link model and the Cassie bipedal robot.


Title: CAPRICORN: Communication Aware Place Recognition using Interpretable Constellations of Objects in Robot Networks
Key Words: feature extraction  image colour analysis  image matching  image representation  mobile robots  multi-robot systems  object detection  robot vision  SLAM (robots)  particular communication bandwidth  limited communication bandwidth  relative object positions  2step decentralized loop closure verification  compact semantic descriptors  bandwidth requirements  communication aware place recognition  interpretable constellations  robot networks  multiple robots  mapping environments  CAPRICORN  exploring environments  3D points  compact spatial descriptors  matching robots  geometric information  global image descriptors  TUM RGB-D SLAM sequence  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Robustness  Visualization  Bandwidth 
Abstract: Using multiple robots for exploring and mapping environments can provide improved robustness and performance, but it can be difficult to implement. In particular, limited communication bandwidth is a considerable constraint when a robot needs to determine if it has visited a location that was previously explored by another robot, as it requires for robots to share descriptions of places they have visited. One way to compress this description is to use constellations, groups of 3D points that correspond to the estimate of a set of relative object positions. Constellations maintain the same pattern from different viewpoints and can be robust to illumination changes or dynamic elements. We present a method to extract from these constellations compact spatial and semantic descriptors of the objects in a scene. We use this representation in a 2step decentralized loop closure verification: first, we distribute the compact semantic descriptors to determine which other robots might have seen scenes with similar objects; then we query matching robots with the full constellation to validate the match using geometric information. The proposed method requires less memory, is more interpretable than global image descriptors, and could be useful for other tasks and interactions with the environment. We validate our system's performance on a TUM RGB-D SLAM sequence and show its benefits in terms of bandwidth requirements.


Title: Online Planning for Quadrotor Teams in 3-D Workspaces via Reachability Analysis On Invariant Geometric Trees
Key Words: aerospace control  helicopters  multi-robot systems  path planning  position control  reachability analysis  robot dynamics  trees (mathematics)  collision-free geometric solution guarantees  online planning  aerial robots  quadrotor teams  cluttered 3D workspaces  reachability analysis  kinodynamic multirobot planning problem  position invariant geometric trees  kinodynamically feasible trajectories  multirobot team  nonstationary initial states  Collision avoidance  Planning  Robot kinematics  Vegetation  Trajectory  Reachability analysis 
Abstract: We consider the kinodynamic multi-robot planning problem in cluttered 3-D workspaces. Reachability analysis on position invariant geometric trees is leveraged to find kino- dynamically feasible trajectories for the multi-robot team from potentially non-stationary initial states. The key contribution of our approach is that a collision-free geometric solution guarantees a kinodynamically feasible, safe solution without additional refinement. Simulation results with up-to 40 robots and hardware results with 5 robots suggest the viability of the proposed approach for online planning and replanning for large teams of aerial robots in cluttered 3-D workspaces.


Title: Decentralized Visual-Inertial-UWB Fusion for Relative State Estimation of Aerial Swarm
Key Words: autonomous aerial vehicles  decentralised control  mobile robots  robot vision  state estimation  decentralized visual-inertial-UWB fusion  unmanned aerial vehicles  multiple UAVs  visual-inertial-UWB fusion framework  extensive aerial swarm flight experiments  motion capture system  vision based method  Global Positioning System  estimation consistency  relative state estimation framework  aerial swarm applications  decentralized relative state estimation method  Drones  State estimation  Cameras  Sensors  Global Positioning System  Real-time systems 
Abstract: The collaboration of unmanned aerial vehicles (UAVs) has become a popular research topic for its practicability in multiple scenarios. The collaboration of multiple UAVs, which is also known as aerial swarm is a highly complex system, which still lacks a state-of-art decentralized relative state estimation method. In this paper, we present a novel fully decentralized visual-inertial-UWB fusion framework for relative state estimation and demonstrate the practicability by performing extensive aerial swarm flight experiments. The comparison result with ground truth data from the motion capture system shows the centimeter-level precision which outperforms all the Ultra-WideBand (UWB) and even vision based method. The system is not limited by the field of view (FoV) of the camera or Global Positioning System (GPS), meanwhile on account of its estimation consistency, we believe that the proposed relative state estimation framework has the potential to be prevalently adopted by aerial swarm applications in different scenarios in multiple scales.


Title: DC-CAPT: Concurrent Assignment and Planning of Trajectories for Dubins Cars
Key Words: collision avoidance  mobile robots  path planning  robot kinematics  trajectory control  Dubins curves  holonomic robots  separation distance  trajectory planning  collision-free trajectories  Dubins cars  concurrent assignment  DC-CAPT  Automobiles  Collision avoidance  Robots  Trajectory  Planning  Turning  Kinematics 
Abstract: We present an algorithm for the concurrent assignment and planning of collision-free trajectories (DC-CAPT) for robots whose kinematics can be modeled as Dubins cars, i.e., robots constrained in terms of their initial orientation and their minimum turning radius. Coupling the assignment and trajectory planning subproblems allows for a computationally tractable solution. This solution is guaranteed to be collision- free through the use of a single constraint: the start and goal locations have separation distance greater than some threshold. We derive this separation distance by extending a prior work that assumed holonomic robots. We demonstrate the validity of our approach, and show its efficacy through simulations and experiments where groups of robots executing Dubins curves travel to their assigned goal locations without collisions.


Title: On sensing-aware model predictive path-following control for a reversing general 2-trailer with a car-like tractor
Key Words: control system synthesis  mobile robots  motion control  path planning  position control  predictive control  road traffic control  road vehicles  stability  vehicle dynamics  sensing-aware model predictive path  car-like tractor  controller-design problem  joint-angle kinematics  backward motion  vehicle segments  jackknife state  joint-angle estimation problem  path-following controller  Agricultural machinery  Axles  Sensors  Kinematics  Reliability  Estimation  Trajectory 
Abstract: The design of reliable path-following controllers is a key ingredient for successful deployment of self-driving vehicles. This controller-design problem is especially challenging for a general 2-trailer with a car-like tractor due to the vehicle's structurally unstable joint-angle kinematics in backward motion and the car-like tractor's curvature limitations which can cause the vehicle segments to fold and enter a jackknife state. Furthermore, advanced sensors with a limited field of view have been proposed to solve the joint-angle estimation problem online, which introduce additional restrictions on which vehicle states that can be reliably estimated. To incorporate these restrictions at the level of control, a model predictive path-following controller is proposed. By taking the vehicle's physical and sensing limitations into account, it is shown in real-world experiments that the performance of the proposed path-following controller in terms of suppressing disturbances and recovering from non-trivial initial states is significantly improved compared to a previously proposed solution where the constraints have been neglected.


Title: Offline Practising and Runtime Training Framework for Autonomous Motion Control of Snake Robots
Key Words: adaptive control  biomimetics  feedback  mobile robots  motion control  regression analysis  robot dynamics  biomorphic hyperredundant robots  locomotion gait  autonomous motion control  runtime training framework  offline practising  snake robot  linear regression  dynamic feedback  Robots  Snake robots  Runtime  Training  Entropy  Motion control  Linear regression 
Abstract: This paper proposes an offline and runtime combined framework for the autonomous motion of snake robots. With the dynamic feedback of its state during runtime, the robot utilizes the linear regression to update its control parameters for better performance and thus adaptively reacts to the environment. To reduce interference from infeasible samples and improve efficiency, the data set for runtime training is chosen from one in several clusters categorized from samples collected in offline practice. Moreover, only the most sensitive control parameter is updated at one iteration for better robustness and efficiency. The effectiveness and efficiency of our approach are evaluated by a set of case studies of pole climbing. Experimental results demonstrate that with the proposed framework, the snake robot can adapt its locomotion gait to poles with different unknown diameters.


Title: Control of a differentially driven nonholonomic robot subject to a restricted wheels rotation
Key Words: controllability  feedback  geometry  position control  robot kinematics  motion task scenarios  virtual geometry constraint  transverse function  four-dimensional configuration manifold  small time local controllability  two-wheeled nonholonomic robot  nonstandard motion tasks  restricted wheels rotation  Wheels  Mobile robots  Kinematics  Task analysis  Manifolds  Trajectory 
Abstract: The paper deals with non-standard motion tasks specified for a two-wheeled nonholonomic robot. It is assumed that wheels cannot fully rotate which reduces a set of feasible movements significantly. In spite of these constraints, it is expected that position of the robot can be changed without violating nonholonomic constraints. Such a possibility comes from the small time local controllability (STLC) of the kinematics described on four-dimensional configuration manifold. In order to solve these specific tasks a feedback taking advantage of the transverse function approach is designed. Consequently, the system can be virtually released from non-holonomic constraints. The transverse function also defines a virtual geometry constraint which makes it possible to limit wheels rotation.Properties of the designed controller are illustrated by results of numerical simulations in various motion task scenarios.


Title: Inferring Task-Space Central Pattern Generator Parameters for Closed-loop Control of Underactuated Robots
Key Words: closed loop systems  graph theory  legged locomotion  motion control  neurocontrollers  optimisation  path planning  real-time systems  sampling methods  optimal behaviors  gradient free optimization  closed loop control  underactuated robots  legged robot control  real time applications  probabilistic graphical model  locomotive behaviors  task space central pattern generator  sampling based motion planner  neural oscillator network  Mathematical model  Robot kinematics  Adaptation models  Probability distribution  Oscillators  Legged locomotion 
Abstract: The complexity associated with the control of highly-articulated legged robots scales quickly as the number of joints increases. Traditional approaches to the control of these robots are often impractical for many real-time applications. This work thus presents a novel sampling-based planning approach for highly-articulated robots that utilizes a probabilistic graphical model (PGM) to infer in real-time how to optimally modify goal-driven, locomotive behaviors for use in closed-loop control. Locomotive behaviors are quantified in terms of the parameters associated with a network of neural oscillators, or rather a central pattern generator (CPG). For the first time, we show that the PGM can be used to optimally modulate different behaviors in real-time (i.e., to select of optimal choice of parameter values across the CPG model) in response to changes both in the local environment and in the desired control signal. The PGM is trained offline using a library of optimal behaviors that are generated using a gradient-free optimization framework.


Title: In-Hand Manipulation of Objects with Unknown Shapes
Key Words: dexterous manipulators  graph theory  learning (artificial intelligence)  unknown shape  grasp configurations  deep generative models  object shapes  partial visual sensing  object shape uncertainty  manipulation actions  in-hand manipulation tasks  unknown objects  dexterous manipulation graph method  Shape  Grippers  Three-dimensional displays  Task analysis  Planning  Robots  Uncertainty 
Abstract: This work addresses the problem of changing grasp configurations on objects with an unknown shape through in-hand manipulation. Our approach leverages shape priors, learned as deep generative models, to infer novel object shapes from partial visual sensing. The Dexterous Manipulation Graph method is extended to build incrementally and account for object shape uncertainty when planning a sequence of manipulation actions. We show that our approach successfully solves in-hand manipulation tasks with unknown objects, and demonstrate the validity of these solutions with robot experiments.


Title: Learning Hierarchical Control for Robust In-Hand Manipulation
Key Words: computational complexity  learning (artificial intelligence)  manipulators  hierarchical control  robotic in-hand manipulation  finger motion  complex manipulation sequences  low-level controllers  model-free deep reinforcement learning  hierarchical method  traditional model-based controllers  manipulation primitives  elongated objects  object models  Robustness  Task analysis  Force  Robot kinematics  Torque  Robot sensing systems 
Abstract: Robotic in-hand manipulation has been a longstanding challenge due to the complexity of modelling hand and object in contact and of coordinating finger motion for complex manipulation sequences. To address these challenges, the majority of prior work has either focused on model-based, low-level controllers or on model-free deep reinforcement learning that each have their own limitations. We propose a hierarchical method that relies on traditional, model-based controllers on the low-level and learned policies on the mid-level. The low-level controllers can robustly execute different manipulation primitives (reposing, sliding, flipping). The mid-level policy orchestrates these primitives. We extensively evaluate our approach in simulation with a 3-fingered hand that controls three degrees of freedom of elongated objects. We show that our approach can move objects between almost all the possible poses in the workspace while keeping them firmly grasped. We also show that our approach is robust to inaccuracies in the object models and to observation noise. Finally, we show how our approach generalizes to objects of other shapes.


Title: Tactile Dexterity: Manipulation Primitives with Tactile Feedback
Key Words: closed loop systems  dexterous manipulators  end effectors  force control  manipulator dynamics  path planning  perturbation techniques  robot vision  tactile sensors  robot trajectories  manipulation primitives  ABB YuMi dual-arm robot  tactile dexterity  tactile feedback  closed-loop tactile controllers  dexterous robotic manipulation  dual-palm robotic system  tactile control  tactile-based tracking  end-effector  Tactile sensors  Trajectory  Force  Friction  Perturbation methods 
Abstract: This paper develops closed-loop tactile controllers for dexterous robotic manipulation with a dual-palm robotic system. Tactile dexterity is an approach to dexterous manipulation that plans for robot/object interactions that render interpretable tactile information for control. We divide the role of tactile control into two goals: 1) control the contact state between the end-effector and the object (contact/no-contact, stick/slip) by regulating the stability of planned contact configurations and monitoring undesired slip events; and 2) control the object state by tactile-based tracking and iterative replanning of the object and robot trajectories. Key to this formulation is the decomposition of manipulation plans into sequences of manipulation primitives with simple mechanics and efficient planners. We consider the scenario of manipulating an object from an initial pose to a target pose on a flat surface while correcting for external perturbations and uncertainty in the initial pose of the object. We experimentally validate the approach with an ABB YuMi dual-arm robot and demonstrate the ability of the tactile controller to react to external perturbations.


Title: Design of a Roller-Based Dexterous Hand for Object Grasping and Within-Hand Manipulation
Key Words: control system synthesis  dexterous manipulators  grippers  manipulator kinematics  motion control  continuous rotation capability  fingertips  object manipulation  roller-based dexterous hand design  two-finger manipulation  nonholonomic spatial motion  robotic hands  three-finger manipulation  actively driven rollers  nonanthropomorphic robot hand  within-hand manipulation  object grasping  Grasping  Prototypes  Kinematics  Task analysis  Robot sensing systems  Thumb 
Abstract: This paper describes the development of a novel non-anthropomorphic robot hand with the ability to manipulate objects by means of articulated, actively driven rollers located at the fingertips. An analysis is conducted and systems of equations for two-finger and three-finger manipulation of a sphere are formulated to demonstrate full six degree of freedom nonholonomic spatial motion capability. A prototype version of the hand was constructed and used to grasp and manipulate a variety of objects. Tests conducted with the prototype confirmed the validity of the mathematical analysis. Unlike conventional approaches to within-hand manipulation using legacy robotic hands, the continuous rotation capability of our rolling fingertips allows for unbounded rotation of a grasped object without the need for finger gaiting.


Title: High-Resolution Optical Fiber Shape Sensing of Continuum Robots: A Comparative Study *
Key Words: bending  Bragg gratings  dexterous manipulators  fibre optic sensors  optical fibres  reflectometry  flexible medical instruments  continuum dexterous manipulators  minimally invasive surgery  accurate CDM shape reconstruction  fiber Bragg grating sensors  sensing locations  basic shapes  optical frequency domain reflectometry  higher spatial resolution  complex shapes  ultraviolet laser exposure  orthopedic surgeries  maximum tip position error  OFDR reconstruction  FBG reconstruction  more accurate alternative  FBG sensors  complex CDM shapes  continuum robots  high-resolution optical fiber shape sensing  random optical gratings  size 35.0 mm  size 3.4 mm  Shape  Fiber gratings  Optical sensors  Robot sensing systems  Spatial resolution 
Abstract: Flexible medical instruments, such as Continuum Dexterous Manipulators (CDM), constitute an important class of tools for minimally invasive surgery. Accurate CDM shape reconstruction during surgery is of great importance, yet a challenging task. Fiber Bragg grating (FBG) sensors have demonstrated great potential in shape sensing and consequently tip position estimation of CDMs. However, due to the limited number of sensing locations, these sensors can only accurately recover basic shapes, and become unreliable in the presence of obstacles or many inflection points such as s-bends. Optical Frequency Domain Reflectometry (OFDR), on the other hand, can achieve much higher spatial resolution, and can therefore accurately reconstruct more complex shapes. Additionally, Random Optical Gratings by Ultraviolet laser Exposure (ROGUEs) can be written in the fibers to increase signal to noise ratio of the sensors. In this comparison study, the tip position error is used as a metric to compare both FBG and OFDR shape reconstructions for a 35 mm long CDM developed for orthopedic surgeries, using a pair of stereo cameras as ground truth. Three sets of experiments were conducted to measure the accuracy of each technique in various surgical scenarios. The tip position error for the OFDR (and FBG) technique was found to be 0.32 (0.83) mm in free-bending environment, 0.41 (0.80) mm when interacting with obstacles, and 0.45 (2.27) mm in s-bending. Moreover, the maximum tip position error remains sub-millimeter for the OFDR reconstruction, while it reaches 3.40 mm for FBG reconstruction. These results propose a cost-effective, robust and more accurate alternative to FBG sensors for reconstructing complex CDM shapes.


Title: Local Trajectory Stabilization for Dexterous Manipulation via Piecewise Affine Approximations
Key Words: approximation theory  dexterous manipulators  feedback  linear programming  linearisation techniques  manipulator dynamics  nonlinear control systems  stability  piecewise affine approximations  dexterous robotic manipulation  nonsmooth nonlinear system  trajectory optimization  local multicontact dynamics  piecewise affine system  linearization  feedback controller  linear programs  local trajectory stabilization  dexterous manipulation  feedback policy design  Trajectory optimization  Manipulator dynamics  Task analysis  Planning 
Abstract: We propose a model-based approach to design feedback policies for dexterous robotic manipulation. The manipulation problem is formulated as reaching the target region from an initial state for some non-smooth nonlinear system. First, we use trajectory optimization to find a feasible trajectory. Next, we characterize the local multi-contact dynamics around the trajectory as a piecewise affine system, and build a funnel around the linearization of the nominal trajectory using polytopes. We prove that the feedback controller at the vicinity of the linearization is guaranteed to drive the nonlinear system to the target region. During online execution, we solve linear programs to track the system trajectory. We validate the algorithm on hardware, showing that even under large external disturbances, the controller is able to accomplish the task.


Title: Automated Eye-in-Hand Robot-3D Scanner Calibration for Low Stitching Errors
Key Words: calibration  industrial manipulators  robot kinematics  robot vision  DH parameters  high stitching errors  long-term routine industrial use  robot-scanner calibration approach  low data stitching error  long-term continuous measurement  2D standard calibration board  low stitching error  virtual arm-based robot-scanner kinematic model  trajectory-based robot-world transformation calculation  cumbersome marker-based method  lower system downtime  automated eye-in-hand robot-3D scanner calibration  industrial robot  complete measurement  data stitching process  single coordinate system  marker-free stitching  cumbersome traditional fiducial marker-based method  align multiple FOV  Calibration  DH-HEMTs  Robot kinematics  Three-dimensional displays  Kinematics  Optimization 
Abstract: A 3D measurement system consisting of a 3D scanner and an industrial robot (eye-in-hand) is commonly used to scan large object under test (OUT) from multiple fieldof-views (FOVs) for complete measurement. A data stitching process is required to align multiple FOVs into a single coordinate system. Marker-free stitching assisted by robot's accurate positioning becomes increasingly attractive since it bypasses the cumbersome traditional fiducial marker-based method. Most existing methods directly use initial Denavit-Hartenberg (DH) parameters and hand-eye calibration to calculate the transformations between multiple FOVs. Since accuracy of DH parameters deteriorates over time, such methods suffer from high stitching errors (e.g., 0.2 mm) in long-term routine industrial use. This paper reports a new robot-scanner calibration approach to realize such measurement with low data stitching errors. During long-term continuous measurement, the robot periodically moves towards a 2D standard calibration board to optimize kinematic model's parameters to maintain a low stitching error. This capability is enabled by several techniques including virtual arm-based robot-scanner kinematic model, trajectory-based robot-world transformation calculation, nonlinear optimization. Experimental results demonstrated a low data stitching error (<; 0.1 mm) similar to the cumbersome marker-based method and a lower system downtime (<; 60 seconds vs. 10-15 minutes by traditional DH and hand-eye calibration).


Title: A Robotics Inspection System for Detecting Defects on Semi-specular Painted Automotive Surfaces
Key Words: automobile industry  flaw detection  inspection  mobile robots  painting  quality control  vibrations  semispecular painted automotive surfaces  real-time robotics system  tolerate varying lighting conditions  inspected surface  defect tracking mechanism  robotics inspection system  detecting defects  painted surface defect detection  small inherent vibrations  manufacturing operations  topographical information  spectral analysis  quality control  Inspection  Surface treatment  Cameras  Surface topography  Robots  Automobiles  Surface reconstruction 
Abstract: This paper describes the design and implementation of a real-time robotics system for semi-specular/painted surface defect detection. The system can be used on moving parts, tolerate varying lighting conditions, and can accommodate small inherent vibrations of the inspected surface that is common in manufacturing operations. Topographical information of the inspected surface is first obtained by the analysis of reflections of a known pattern from this surface. Spectral analysis is then applied to identify defects through novelty detection. Finally, a defect tracking mechanism eliminates spurious defects. The proposed system operates continuously at 90 fps. The paper presents field testing results that show the system can be used as a consistent and cost-effective way of quality control.


Title: A Novel Underactuated End-Effector for Planar Sequential Grasping of Multiple Objects
Key Words: actuators  control system synthesis  dexterous manipulators  end effectors  human-robot interaction  motion control  torque control  underactuated end-effector  planar sequential grasping  multiple objects  underactuated end-effector design  autonomous grasp  circular objects  sequential grasps  human-robot hand-off interactions  torque control  Force  Grasping  Sensors  Estimation  Shape  Fasteners  Torque 
Abstract: We propose a serpentine type tendon driven underactuated end-effector design with a closing mechanism that is triggered upon contact with an object. This end-effector can grasp objects without knowing the size a priori and is able to grasp a new object while securing another one previously grasped, and so grasp multiple objects sequentially with a single DOF actuation. Design parameters based on the object dimensions are proposed. A low-cost prototype demonstrates two implementations (radius estimation and autonomous grasp of circular objects by torque control, and sequential grasps of multiple objects) of the end-effector through several experiments. A method for estimating applied internal forces is also proposed. This end-effector can benefit robotic manipulation in tasks such as fetching applications, industrial pick-and-place of single or multiple objects and human-robot hand-off interactions.


Title: Design and Analysis of a Synergy-Inspired Three-Fingered Hand
Key Words: dexterous manipulators  grippers  manipulator kinematics  motion control  synergy-inspired hands  biomechanical characteristics  human hand synergy  robot hands  synergy characteristics  anthropomorphic hands  synergy-inspired design  Thumb  Robots  Joints  Muscles  Grasping  Electronics packaging 
Abstract: Hand synergy from neuroscience provides an effective tool for anthropomorphic hands to realize versatile grasping with simple planning and control. This paper aims to extend the synergy-inspired design from anthropomorphic hands to multi-fingered robot hands. The synergy-inspired hands are not necessarily humanoid in morphology but perform primary characteristics and functions similar to the human hand. At first, the biomechanics of hand synergy is investigated. Three biomechanical characteristics of the human hand synergy are explored as a basis for the mechanical simplification of the robot hands. Secondly, according to the synergy characteristics, a three-fingered hand is designed, and its kinematic model is developed for the analysis of some typical grasping and manipulation functions. Finally, a prototype is developed and preliminary grasping experiments validate the effectiveness of the design and analysis.


Title: Multiplexed Manipulation: Versatile Multimodal Grasping via a Hybrid Soft Gripper
Key Words: compliant mechanisms  dexterous manipulators  grippers  motion control  multiplexed manipulation  hybrid soft gripper  hybrid suction  parallel jaw grippers  multimodal grippers  soft robotic manipulators  soft fingers  multimodal grasping  Amazon Robotics/Picking Challenge  complaint handed shearing auxetics actuators  Grasping  Grippers  Multiplexing  Force  Belts  Manipulators 
Abstract: The success of hybrid suction + parallel-jaw grippers in the Amazon Robotics/Picking Challenge have demonstrated the effectiveness of multimodal grasping approaches. However, existing multimodal grippers combine grasping modes in isolation and do not incorporate the benefits of compliance found in soft robotic manipulators. In this paper, we present a gripper that integrates three modes of grasping: suction, parallel jaw, and soft fingers. Using complaint handed shearing auxetics actuators as the foundation, this gripper is able to multiplex manipulation by creating unique grasping primitives through permutations of these grasping techniques. This gripper is able to grasp 88% of tested objects, 14% of which could only be grasped using a combination of grasping modes. The gripper is also able to perform in-hand object re-orientation of flat objects without the need for pre-grasp manipulation.


Title: Underactuated Gecko Adhesive Gripper for Simple and Versatile Grasp
Key Words: actuators  adhesion  adhesives  grippers  controllable activation  minimal disturbance  form closure  robotic grasping  versatile grasp  underactuated gecko adhesive gripper  resulting gripper grasp force  adhesive contact area  simple tendon-driven mechanism  underactuated gecko-inspired adhesive gripper  multiple activation steps  complex activation mechanism  grippers  Grippers  Force  Grasping  Pulleys  Adhesives  Actuators  Tendons 
Abstract: Gecko-inspired adhesives have several desirable characteristics in robotic grasping: controllable activation and deactivation of adhesion, ability to grasp and release with minimal disturbance, and grasping without the need of form closure. Previously proposed grippers with this technology either require a complex activation mechanism or multiple activation steps. In this paper, we present an underactuated gecko-inspired adhesive gripper that can grasp a wide range of curved surfaces using a single actuator through a simple tendon-driven mechanism that attaches and adheres in one step. We derive a theoretical model of the adhesive contact area and resulting gripper grasp force, which is verified experimentally. The actual performance of the proposed mechanism is demonstrated by successfully grasping several surfaces with different curvature diameters.


Title: Active Deformation through Visual Servoing of Soft Objects
Key Words: control engineering computing  deformation  eigenvalues and eigenfunctions  end effectors  Jacobian matrices  least squares approximations  robot vision  visual servoing  soft objects  online estimation  deformation Jacobian  robot end-effector  deformation behavior  ADVISEd method  model-free methods  marker-based active shaping task  shape preservation tasks  active deformation through visual servoing method  model-free deformation servoing method  weighted least-squares minimization  sliding window  eigenvalue-based confidence criterion  marker-less active shaping  model-based methods  Strain  Jacobian matrices  Deformable models  Shape  Task analysis  Adaptation models  Robots 
Abstract: In this paper, we propose the ADVISEd (Active Deformation through VIsual SErvoing) method, a novel model-free deformation servoing method able to deform a soft object towards a desired shape. ADVISEd relies on an online estimation of the deformation Jacobian that relates the motion of the robot end-effector to the deformation behavior of the object. The estimation is based on a weighted least-squares minimization with a sliding window. The robustness of the method to observation noise is ensured using an eigenvalue-based confidence criterion. The ADVISEd method is validated through comparisons with a model-based and a model-free state-of-the-art methods. Two experimental setups are proposed to compare the methods, one to perform a marker-based active shaping task and one to perform several marker-less active shaping and shape preservation tasks. Experiments showed that our approach can interactively control the deformations of an object in different tasks while ensuring better robustness to external perturbations than the state-of-the-art methods.


Title: Visual Geometric Skill Inference by Watching Human Demonstration
Key Words: control engineering computing  data visualisation  entropy  feature selection  graph theory  learning (artificial intelligence)  manipulators  regression analysis  video signal processing  visual geometric skill inference  manipulation skills  human demonstration video  association relationships  eye-hand coordination tasks  geometric control error  graph based kernel regression method  association constraints  human readable task definition  control errors  feature-based visual ser-voing  incremental maximum entropy inverse reinforcement learning  feature selection  robust feature trackers  Task analysis  Kernel  Robustness  Feature extraction  Robot kinematics  Three-dimensional displays 
Abstract: We study the problem of learning manipulation skills from human demonstration video by inferring the association relationships between geometric features. Motivation for this work stems from the observation that humans perform eye-hand coordination tasks by using geometric primitives to define a task while a geometric control error drives the task through execution. We propose a graph based kernel regression method to directly infer the underlying association constraints from human demonstration video using Incremental Maximum Entropy Inverse Reinforcement Learning (InMaxEnt IRL). The learned skill inference provides human readable task definition and outputs control errors that can be directly plugged into traditional controllers. Our method removes the need for tedious feature selection and robust feature trackers required in traditional approaches (e.g. feature-based visual ser-voing). Experiments show our method infers correct geometric associations even with only one human demonstration video and can generalize well under variance.


Title: DFVS: Deep Flow Guided Scene Agnostic Image Based Visual Servoing
Key Words: cameras  feature extraction  image sensors  image sequences  learning (artificial intelligence)  neural nets  pose estimation  robot vision  stereo image processing  visual servoing  optical flow  visual features  deep neural network  diverse scenes  visual servoing approaches  robust servoing performance  camera transformations  deep flow guided scene agnostic image  deep learning  relative camera pose  photo-realistic 3D simulation  aerial robot  DFVS  interaction matrix  Visual servoing  Cameras  Convergence  Visualization  Adaptive optics  Task analysis 
Abstract: Existing deep learning based visual servoing approaches regress the relative camera pose between a pair of images. Therefore, they require a huge amount of training data and sometimes fine-tuning for adaptation to a novel scene. Furthermore, current approaches do not consider underlying geometry of the scene and rely on direct estimation of camera pose. Thus, inaccuracies in prediction of the camera pose, especially for distant goals, lead to a degradation in the servoing performance. In this paper, we propose a two-fold solution: (i) We consider optical flow as our visual features, which are predicted using a deep neural network. (ii) These flow features are then systematically integrated with depth estimates provided by another neural network using interaction matrix. We further present an extensive benchmark in a photo-realistic 3D simulation across diverse scenes to study the convergence and generalisation of visual servoing approaches. We show convergence for over 3m and 40 degrees while maintaining precise positioning of under 2cm and 1 degree on our challenging benchmark where the existing approaches that are unable to converge for majority of scenarios for over 1.5m and 20 degrees. Furthermore, we also evaluate our approach for a real scenario on an aerial robot. Our approach generalizes to novel scenarios producing precise and robust servoing performance for 6 degrees of freedom positioning tasks with even large camera transformations without any retraining or fine-tuning.


Title: Photometric Path Planning for Vision-Based Navigation
Key Words: cameras  manipulators  navigation  path planning  robot vision  photometric path planning  vision-based navigation system  visual memory  topological map  virtual camera  navigability  visual path  navigation stage  onboard camera  top view image  learning stage  urban scene  Visualization  Navigation  Cameras  Visual servoing 
Abstract: We present a vision-based navigation system that uses a visual memory to navigate. Such memory corresponds to a topological map of key images created from moving a virtual camera over a model of the real scene. The advantage of our approach is that it provides a useful insight into the navigability of a visual path without relying on a traditional learning stage. During the navigation stage, the robot is controlled by sequentially comparing the images stored in the memory with the images acquired by the onboard camera.The evaluation is conducted on a robotic arm equipped with a camera and the model of the environment corresponds to a top view image of an urban scene.


Title: A memory of motion for visual predictive control tasks
Key Words: manipulators  optimal control  optimisation  predictive control  regression analysis  robot vision  visual servoing  visual predictive control tasks  regression techniques  control optimization process  7-axis manipulator  image-based visual servoing  Visualization  Microsoft Windows  Trajectory  Optimization  Task analysis  Computational modeling  Cameras 
Abstract: This paper addresses the problem of efficiently achieving visual predictive control tasks. To this end, a memory of motion, containing a set of trajectories built off-line, is used for leveraging precomputation and dealing with difficult visual tasks. Standard regression techniques, such as k-nearest neighbors and Gaussian process regression, are used to query the memory and provide on-line a warm-start and a way point to the control optimization process. The proposed technique allows the control scheme to achieve high performance and, at the same time, keep the computational time limited. Simulation and experimental results, carried out with a 7-axis manipulator, show the effectiveness of the approach.


Title: Design and Workspace Characterisation of Malleable Robots
Key Words: actuators  design engineering  elasticity  end effectors  manipulator dynamics  manipulator kinematics  position control  bin picking  variable stiffness link  low DOF serial robot  2-DOF malleable robot  workspace categories  serial robot arms  End effectors  Robot kinematics  Task analysis  Mathematical model  Topology 
Abstract: For the majority of tasks performed by traditional serial robot arms, such as bin picking or pick and place, only two or three degrees of freedom (DOF) are required for motion; however, by augmenting the number of degrees of freedom, further dexterity of robot arms for multiple tasks can be achieved. Instead of increasing the number of joints of a robot to improve flexibility and adaptation, which increases control complexity, weight, and cost of the overall system, malleable robots utilise a variable stiffness link between joints allowing the relative positioning of the revolute pairs at each end of the link to vary, thus enabling a low DOF serial robot to adapt across tasks by varying its workspace. In this paper, we present the design and prototyping of a 2-DOF malleable robot, calculate the general equation of its workspace using a parameterisation based on distance geometry-suitable for robot arms of variable topology, and characterise the workspace categories that the end effector of the robot can trace via reconfiguration. Through the design and construction of the malleable robot we explore design considerations, and demonstrate the viability of the overall concept. By using motion tracking on the physical robot, we show examples of the infinite number of workspaces that the introduced 2-DOF malleable robot can achieve.


Title: A Tri-Stable Soft Robotic Finger Capable of Pinch and Wrap Grasps
Key Words: bending  dexterous manipulators  elastomers  force control  grippers  pneumatic actuators  position control  springs (mechanical)  preprogrammed grasp  constant-curvature wrap  finger-sized round objects  flat objects  small objects  adaptable tri-stable  grasped object  bi-stable springs  stable positions  finger bending  grasping performance  control gripper  soft grippers  wrap grasps  soft robotic pneumatic grippers  delicate objects  fluidic elastomer grippers  inextensible gripping surface  extensible pneumatic chambers  extensibility results  finger curling  simple fingers  tri-stable soft robotic finger  pinch grasps  Grippers  Springs  Shape  Soft robotics  Grasping  Force  Mathematical model 
Abstract: Soft robotic pneumatic grippers have been shown to be versatile, robust to impacts, and safe for use on delicate objects. One type, fluidic elastomer grippers, are characterized by fingers with an inextensible gripping surface backed by extensible pneumatic chambers; when inflated, this mismatch in extensibility results in the finger curling. However, one drawback of these simple fingers is that they have one preprogrammed grasp, usually a simple constant-curvature wrap. While well-suited for finger-sized round objects, they do not grasp flat or small objects well. Here, we present an adaptable tri-stable soft robotic finger that can form either a pinch or wrap grasp based on the shape of the grasped object. We enable this by incorporating two bi-stable springs into the inextensible layer. The three stable positions are: i) open (unpressurized), ii) pinch (with only the proximal section bending), and iii) wrap (with the entire finger bending). We present a simple model of the behavior of our finger and experimental results verifying the model. Further, we apply forces and moments to grasped objects, and show that the tri-stable finger increases the grasping performance when compared to a control gripper with equal gripping force. Our work presents a novel design modification that is unobtrusive, simple, and passive. Our introduction of inexpensive programmable hardware advances the versatility and adaptability of soft grippers.


Title: A Dexterous Tip-extending Robot with Variable-length Shape-locking
Key Words: biomechanics  dexterous manipulators  mobile robots  position control  dexterous tip-extending robot  variable-length shape-locking  tip-extending vine robots  distal end  inextensible tip-extending  pressurized tip-extending  robot body  locked sections  free sections  shape-locking mechanism  shape-locking concept  soft robotics  dexterous workspace  Electron tubes  Tendons  Shape  Educational robots  Manipulators  Pneumatic systems 
Abstract: Soft, tip-extending "vine" robots offer a unique mode of inspection and manipulation in highly constrained environments. For practicality, it is desirable that the distal end of the robot can be manipulated freely, while the body remains stationary. However, in previous vine robots, either the shape of the body was fixed after growth with no ability to manipulate the distal end, or the whole body moved together with the tip. Here, we present a concept for shape-locking that enables a vine robot to move only its distal tip, while the body is locked in place. This is achieved using two inextensible, pressurized, tip-extending, chambers that "grow" along the sides of the robot body, preserving curvature in the section where they have been deployed. The length of the locked and free sections can be varied by controlling the extension and retraction of these chambers. We present models describing this shape-locking mechanism and workspace of the robot in both free and constrained environments. We experimentally validate these models, showing an increased dexterous workspace compared to previous vine robots. Our shape-locking concept allows improved performance for vine robots, advancing the field of soft robotics for inspection and manipulation in highly constrained environments.


Title: Compliant Electromagnetic Actuator Architecture for Soft Robotics
Key Words: dexterous manipulators  electromagnetic actuators  grippers  liquid metal ion sources  pneumatic actuators  compliant electromagnetic actuator architecture  soft robotics  soft materials  compliant actuation concepts  soft robotic systems  electromagnetic actuators  gallium-indium liquid metal conductors  soft actuator  Xenia soft corals  compliant permanent magnetic tips  robotic actuator  frequency 7.0 Hz  size 6.0 mm  Iron  Actuators  Magnetic cores  Powders  Electromagnetics  Magnetic liquids 
Abstract: Soft materials and compliant actuation concepts have generated new design and control approaches in areas from robotics to wearable devices. Despite the potential of soft robotic systems, most designs currently use hard pumps, valves, and electromagnetic actuators. In this work, we take a step towards fully soft robots by developing a new compliant electromagnetic actuator architecture using gallium-indium liquid metal conductors, as well as compliant permanent magnetic and compliant iron composites. Properties of the new materials are first characterized and then co-fabricated to create an exemplary biologically-inspired soft actuator with pulsing or grasping motions, similar to Xenia soft corals. As current is applied to the liquid metal coil, the compliant permanent magnetic tips on passive silicone arms are attracted or repelled. The dynamics of the robotic actuator are characterized using stochastic system identification techniques and then operated at the resonant frequency of 7 Hz to generate high-stroke (>6 mm) motions.


Title: Dynamically Reconfigurable Discrete Distributed Stiffness for Inflated Beam Robots
Key Words: beams (structures)  buckling  cantilevers  electromagnets  electromechanical actuators  jamming  motion control  rigidity  robot kinematics  valves  tip-everting robots  tendonsteering  robot kinematics  cantilevered loads  electromechanical device  electromagnet  passive valves  pressure layer jamming  buckle point locations  compressive loads  actuators  motion control  discrete distributed stiffness control  inflated beam robot body  inflated continuum robots  Valves  Jamming  Actuators  Laser beams  Soft robotics  Shape 
Abstract: Inflated continuum robots are promising for a variety of navigation tasks, but controlling their motion with a small number of actuators is challenging. These inflated beam robots tend to buckle under compressive loads, producing extremely tight local curvature at difficult-to-control buckle point locations. In this paper, we present an inflated beam robot that uses distributed stiffness changing sections enabled by positive pressure layer jamming to control or prevent buckling. Passive valves are actuated by an electromagnet carried by an electromechanical device that travels inside the main inflated beam robot body. The valves themselves require no external connections or wiring, allowing the distributed stiffness control to be scaled to long beam lengths. Multiple layer jamming elements are stiffened simultaneously to achieve global stiffening, allowing the robot to support greater cantilevered loads and longer unsupported lengths. Local stiffening, achieved by leaving certain layer jamming elements unstiffened, allows the robot to produce "virtual joints" that dynamically change the robot kinematics. Implementing these stiffening strategies is compatible with growth through tip eversion and tendonsteering, and enables a number of new capabilities for inflated beam robots and tip-everting robots.


Title: Data-Driven Reinforcement Learning for Walking Assistance Control of a Lower Limb Exoskeleton with Hemiplegic Patients
Key Words: adaptive control  artificial limbs  handicapped aids  iterative methods  learning (artificial intelligence)  medical robotics  neural nets  optimal control  patient rehabilitation  wearable robots  Data-driven reinforcement learning  lower limb exoskeleton  hemiplegic patient  rehabilitation scenario  affected leg  unaffected leg  exoskeleton system  DDRL strategy  optimal control  policy iteration algorithm  online adaptation control  walking assistance control  walking assistance scenario  strength augmentation scenario  Actor-Critic Neural Network  ACNN  Legged locomotion  Exoskeletons  Adaptation models  Learning (artificial intelligence)  Trajectory  Extremities  Optimal control  Data-driven Control  Reinforcement Learning  Leader-Follower Multi-Agent System  Lower Limb Exoskeleton  Hemiplegic Patients  Actor-Critic Neural Network 
Abstract: Lower limb exoskeleton (LLE) has received considerable interests in strength augmentation, rehabilitation and walking assistance scenarios. For walking assistance, the LLE is expected to have the capability of controlling the affected leg to track the unaffected leg's motion naturally. An important issue in this scenario is that the exoskeleton system needs to deal with unpredictable disturbance from the patient, which requires the controller of exoskeleton system to have the ability to adapt to different wearers. This paper proposes a novel Data-Driven Reinforcement Learning (DDRL) control strategy to adapt different hemiplegic patients with unpredictable disturbances. In the proposed DDRL strategy, the interaction between two lower limbs of LLE and the legs of hemiplegic patient are modeled in the context of leader-follower framework. The walking assistance control problem is transformed into a optimal control problem. Then, a policy iteration (PI) algorithm is introduced to learn optimal controller. To achieve online adaptation control for different patients, based on PI algorithm, an Actor-Critic Neural Network (ACNN) technology of the reinforcement learning (RL) is employed in the proposed DDRL. We conduct experiments both on a simulation environment and a real LLE system. Experimental results demonstrate that the proposed control strategy has strong robustness against disturbances and adaptability to different pilots.


Title: On the Effects of Visual Anticipation of Floor Compliance Changes on Human Gait: Towards Model-based Robot-Assisted Rehabilitation
Key Words: biomechanics  feedback  gait analysis  mechanoception  medical robotics  muscle  neurophysiology  patient rehabilitation  patient treatment  virtual reality  poststroke gait rehabilitation  variable stiffness treadmill  robot-assisted gait therapies  feedback mechanisms  visual feedback  surface stiffness changes  repeatable muscle activation patterns  predictable muscle activation patterns  surface changes  proprioceptive feedback  manipulated visual feedback  virtual environment  real-world compliant surfaces  walking surface stiffness  sensorimotor mechanisms  robotic rehabilitation device  virtual reality experience  robot-assisted interventions  rehabilitation method  robot assistance  model-based robot-assisted rehabilitation  human gait  floor compliance changes  visual anticipation  Legged locomotion  Visualization  Muscles  Robot sensing systems  Perturbation methods  Electromyography 
Abstract: The role of various types of robot assistance in post-stroke gait rehabilitation has gained much attention in recent years. Furthermore, there is increased popularity to use more than one rehabilitation method in order to utilize the different advantages of each. Naturally, this results in the need to study how the different robot-assisted interventions affect the various underlying sensorimotor mechanisms involved in rehabilitation. To answer this important question, this paper combines a virtual reality experience with a unique robotic rehabilitation device, the Variable Stiffness Treadmill (VST), as a way of understanding interactions across different sensorimotor mechanisms involved in gait. The VST changes the walking surface stiffness in order to simulate real-world compliant surfaces while seamlessly interacting with a virtual environment. Through the manipulated visual and proprioceptive feedback, this paper focuses on the muscle activation patterns before, during, and after surface changes that are both visually informed and uninformed. The results show that there are predictable and repeatable muscle activation patterns both before and after surface stiffness changes, and these patterns are affected by the perceived visual and proprioceptive feedback. The interaction of feedback mechanisms and their effect on evoked muscular activation can be used in future robot-assisted gait therapies, where the intended muscle responses are informed by deterministic models and are tailored to a specific patient's needs.


Title: A Visual Positioning System for Indoor Blind Navigation
Key Words: cameras  collision avoidance  distance measurement  graph theory  handicapped aids  mobile robots  navigation  particle filtering (numerical methods)  pose estimation  robot vision  visual positioning system  indoor blind navigation  VPS  robotic navigation aid  RNA  assistive navigation  depth-enhanced visual-inertial odometry  RGB-D camera  inertial measurement unit  DVIO method  geometric feature  floor plane  measurement residuals  inertial data  graph optimization framework  Sampson error  near-range visual features  known depth  far-range visual features  estimation accuracy  particle filter localization method  PFL  visually impaired person  heading error  accurate pose estimation  Cameras  RNA  Feature extraction  Visualization  Pose estimation  Navigation  Three-dimensional displays 
Abstract: This paper presents a visual positioning system (VPS) for real-time pose estimation of a robotic navigation aid (RNA) for assistive navigation. The core of the VPS is a new method called depth-enhanced visual-inertial odometry (DVIO) that uses an RGB-D camera and an inertial measurement unit (IMU) to estimate the RNA's pose. The DVIO method extracts the geometric feature (the floor plane) from the camera's depth data and integrates its measurement residuals with that of the visual features and the inertial data in a graph optimization framework for pose estimation. A new measure based on the Sampson error is introduced to describe the measurement residuals of the near-range visual features with a known depth and that of the far-range visual features whose depths are unknown. The measure allows for the incorporation of both types of visual features into graph optimization. The use of the geometric feature and the Sampson error improves pose estimation accuracy and precision. The DVIO method is paired with a particle filter localization (PFL) method to locate the RNA in a 2D floor plan and the information is used to guide a visually impaired person. The PFL reduces the RNA's position and heading error by aligning the camera's depth data with the floor plan map. Together, the DVIO and the PFL allow for accurate pose estimation for wayfinding and 3D mapping for obstacle avoidance. Experimental results demonstrate the usefulness of the RNA in assistive navigation in indoor spaces.


Title: An Outsole-Embedded Optoelectronic Sensor to Measure Shear Ground Reaction Forces During Locomotion
Key Words: closed loop systems  footwear  force measurement  force sensors  gait analysis  medical robotics  muscle  patient rehabilitation  foot-mounted sensors  outsole-embedded optoelectronic sensor configuration  biaxial shear GRFs  traditional strain-gauge based solutions  optoelectronic sensors  footwear structure  outsole-embedded sensor  shear ground reaction forces  online estimation  3D ground reaction forces  closed-loop control  lower-extremity robotic exoskeletons  in-verse dynamics  optimization models  net joint torques  muscle forces  instrumented footwear  vertical GRFs  Robot sensing systems  Footwear  Force  Force measurement  Instruments  Legged locomotion  wearable technology  optoelectronics  shear force sensor  instrumented footwear 
Abstract: Online estimation of 3D ground reaction forces (GRFs) is becoming increasingly important for closed-loop control of lower-extremity robotic exoskeletons. Through in-verse dynamics and optimization models, 3D GRFs can be used to estimate net joint torques and approximate muscle forces. Although instrumented footwear to measure vertical GRFs in out-of-the-lab environments is available, accurately measuring shear GRFs with foot-mounted sensors still remains a challenging task. In this paper, a new outsole-embedded optoelectronic sensor configuration that is able to measure biaxial shear GRFs is proposed. Compared with traditional strain-gauge based solutions, optoelectronic sensors allow for a more affordable design. To mitigate the risk of altering the wearer's natural gait, the proposed solution does not involve external modifications to the footwear structure. A preliminary validation of the outsole-embedded sensor was conducted against validated laboratory equipment. The test involved two sessions of treadmill walking at different speeds. Experimental results suggest that the proposed design may be a promising solution for measuring shear GRFs in unconstrained environments.


Title: Bump’em: an Open-Source, Bump-Emulation System for Studying Human Balance and Gait
Key Words: closed loop systems  force control  gait analysis  geriatrics  injuries  mechanoception  medical computing  medical control systems  open-source bump-emulation system  robotic rope-driven system  human gait  fall-inducing perturbations  laboratory-based perturbation systems  aging population  fall-related injury  human balance  open-loop system  closed-loop force control  open-loop force control  transverse plane  force-fields  Brushless motors  Perturbation methods  Force  Force sensors  Shafts  Force control  Legged locomotion 
Abstract: Fall-related injury is a significant health problem on a global scale and is expected to grow with the aging population. Laboratory-based perturbation systems have the capability of simulating various modes of fall-inducing perturbations in a repeatable way. These systems enable fundamental research on human gait and balance and facilitate the development of devices to assist human balance. We present a robotic, rope-driven system capable of rendering bumps and force-fields at a person's pelvis in any direction in the transverse plane with forces up to 200 N, and a 90% rise time of as little as 44 ms, which is faster than a human's ability to sense and respond to the force. These capabilities enable experiments that require stabilizing or destabilizing subjects as they stand or walk on a treadmill. To facilitate use by researchers from all backgrounds, we designed both a configuration with simpler open-loop force control, and another with higher-performance, closed-loop force control. Both configurations are modular, and the open-loop system is made entirely from 3D-printed and catalog components. The design files and assembly instructions for both are freely available in an online repository.


Title: A Hybrid, Soft Exoskeleton Glove Equipped with a Telescopic Extra Thumb and Abduction Capabilities
Key Words: dexterous manipulators  diseases  force feedback  grippers  handicapped aids  motion control  patient rehabilitation  quality assessment experiments  inflatable thumb  grasp stability  hybrid assistive glove  grasping capabilities  hand exoskeletons  neurological diseases  musculoskeletal diseases  wearable gloves  exoskeleton glove  pneumatic telescopic extra thumb  force exertion experiments  activities of daily living  Actuators  Thumb  Exoskeletons  Tendons  Robots  Pneumatic systems  Grasping 
Abstract: Over the last years, hand exoskeletons have become a popular and efficient technical solution for assisting people that suffer from neurological and musculoskeletal diseases and enhance the capabilities of healthy individuals. These devices can vary from rigid and complex structures to soft, lightweight, wearable gloves. Despite the significant progress in the field, most existing solutions do not provide the same dexterity as the healthy human hand. In this paper, we focus on the development of a hybrid (tendon-driven and pneumatic), lightweight, affordable, wearable exoskeleton glove equipped with abduction/adduction capabilities and a pneumatic telescopic extra thumb that increases grasp stability. The efficiency of the proposed device is experimentally validated through three different types of experiments: i) abduction/adduction tests, ii) force exertion experiments that capture the maximum forces that can be applied by the proposed device, and iii) grasp quality assessment experiments that focus on the effect of the inflatable thumb on enhancing grasp stability. The hybrid assistive glove considerably improves the grasping capabilities of the user, being able to exert the forces required to assist people in the execution of activities of daily living.


Title: Controlling an upper-limb exoskeleton by EMG signal while carrying unknown load
Key Words: biomechanics  electromyography  force sensors  human-robot interaction  medical robotics  medical signal processing  muscle  wireless EMG  movement direction  intensity estimation  gravity compensation  EMG armband  EMG signal  traditional gravity compensation  intuitive control law  human-robot collaboration  force sensors  freedom upper-limb exoskeleton  Exoskeletons  Electromyography  Torque  Robots  Muscles  Gravity  Task analysis 
Abstract: Implementing an intuitive control law for an upper-limb exoskeleton dedicated to force augmentation is a challenging issue in the field of human-robot collaboration. The aim of this study is to design an innovative approach to assist carrying an unknown load without using force sensors or specific handle. The method is based on user's intentions estimated through a wireless EMG armband allowing movement direction and intensity estimation along 1 Degree of Freedom. This control law aimed to behave like a gravity compensation except that the mass of the load does not need to be known. The proposed approach was tested on 10 participants during a lifting task with a single Degree of Freedom upper-limb exoskeleton. Participants performed it in three different conditions : without assistance, with an exact gravity compensation and with the proposed method based on EMG armband. The evaluation of the efficiency of the assistance was based on EMG signals captured on seven muscles (objective indicator) and a questionnaire (subjective indicator). Results showed a statically significant reduction of mean activity of the biceps, erector spinae and deltoid by 20%±14%, 18%±12% and 25% ± 16% respectively while comparing the proposed method with no assistance. In addition, similar muscle activities were found both in the proposed method and the traditional gravity compensation. Subjective evaluation showed better precision, efficiency and responsiveness of the proposed method compared to the traditional one.


Title: Learning Grasping Points for Garment Manipulation in Robot-Assisted Dressing
Key Words: assisted living  clothing  collision avoidance  dexterous manipulators  end effectors  grippers  handicapped aids  learning (artificial intelligence)  learning systems  neurocontrollers  position control  robot vision  robot-assisted dressing system  dressing activities  grasping point estimations  Baxter robot  robot-garment collision avoidance  orientation computation  grasping point prediction  depth images  supervised deep neural network  robotic manipulation  robot end-effector  robot configuration  elderly people  disabled people  assistive robots  garment manipulation  Clothing  Robots  Grasping  Collision avoidance  Rails  Neural networks  Training 
Abstract: Assistive robots have the potential to provide tremendous support for disabled and elderly people in their daily dressing activities. Recent studies on robot-assisted dressing usually simplify the setup of the initial robot configuration by manually attaching the garments on the robot end-effector and positioning them close to the user's arm. A fundamental challenge in automating such a process for robots is computing suitable grasping points on garments that facilitate robotic manipulation. In this paper, we address this problem by introducing a supervised deep neural network to locate a predefined grasping point on the garment, using depth images for their invariance to color and texture. To reduce the amount of real data required, which is costly to collect, we leverage the power of simulation to produce large amounts of labeled data. The network is jointly trained with synthetic datasets of depth images and a limited amount of real data. We introduce a robot-assisted dressing system that combines the grasping point prediction method, with a grasping and manipulation strategy which takes grasping orientation computation and robot-garment collision avoidance into account. The experimental results demonstrate that our method is capable of yielding accurate grasping point estimations. The proposed dressing system enables the Baxter robot to autonomously grasp a hospital gown hung on a rail, bring it close to the user and successfully dress the upper-body.


Title: TACTO-Selector: Enhanced Hierarchical Fusion of PBVS with Reactive Skin Control for Physical Human-Robot Interaction
Key Words: haptic interfaces  human-robot interaction  industrial robots  mobile robots  motion control  position control  robot vision  visual servoing  reactive skin control  6 DOF industrial robot  physical human-robot interaction  industrial scenarios  hierarchical task approaches  low priority tasks  standard hierarchical fusion  tactile interaction  safety task  6 DOF position-based visual servoing task  interactive task-reconfiguring approach  TACTO-selector  PBVS  Task analysis  Collision avoidance  Skin  Robot sensing systems  Safety 
Abstract: In a physical Human-Robot Interaction for industrial scenarios is paramount to guarantee the safety of the user while keeping the robot's performance. Hierarchical task approaches are not sufficient since they tend to sacrifice the low priority tasks in order to guarantee the consistency of the main task. To handle this problem, we enhance the standard hierarchical fusion by introducing a novel interactive task-reconfiguring approach (TACTO-Selector) that uses the information of the tactile interaction to adapt the dimension of the tasks, therefore guaranteeing the execution of the safety task while performing the other task as good as possible. In this work, we hierarchically combine a 6 DOF Position-Based Visual Servoing (PBVS) task with a reactive skin control. This approach was evaluated on a 6 DOF industrial robot showing an improvement of 36.37% on average in tracking error reduction compared with a standard approach.


Title: Towards an Intelligent Collaborative Robotic System for Mixed Case Palletizing
Key Words: graphical user interfaces  groupware  human-robot interaction  industrial robots  mobile robots  multi-robot systems  optimisation  palletising  production engineering computing  visual perception  collaborative palletizing tasks  intelligent collaborative robotic system  mixed case palletizing  visual perception algorithms  high-level optimisation  graphical user interface  Mobile COllaborative robotic Assistant  human-robot collaborative framework  MOCA  packing density maximisation  Pallets  Task analysis  Collaboration  Robots  Impedance  Torque  Resource management 
Abstract: In this paper, a novel human-robot collaborative framework for mixed case palletizing is presented. The framework addresses several challenges associated with the detection and localisation of boxes and pallets through visual perception algorithms, high-level optimisation of the collaborative effort through effective role-allocation principles, and maximisation of packing density. A graphical user interface (GUI) is additionally developed to ensure an intuitive allocation of roles and the optimal placement of the boxes on target pallets. The framework is evaluated in two conditions where humans operate with and without the support of a Mobile COllaborative robotic Assistant (MOCA). The results show that the optimised placement can improve up to the 20% with respect to a manual execution of the same task, and reveal the high potential of MOCA in increasing the performance of collaborative palletizing tasks.


Title: Treadmill Based Three Tether Parallel Robot for Evaluating Auditory Warnings While Running
Key Words: force control  gait analysis  human-robot interaction  medical robotics  patient rehabilitation  statistical testing  three-term control  virtual reality  walking running subjects  treadmill  T-test  auditory warnings  3 DoF parallel cable system  Utah's Treadport Active Wind Tunnel  gait algorithms  PID force controller  three tether parallel robot  Nexus VICON motion capture  sports related concussions  hemiparetic rehabilitation  immersive virtual reality locomotion system  Perturbation methods  Force  Legged locomotion  Muscles  Pulleys  Tracking 
Abstract: We design and test a 3 DoF parallel cable system capable of applying precise and accurate impulses to walking and running subjects for the University of Utah's Treadport Active Wind Tunnel (TPAWT). Using Nexus VICON motion capture and gait algorithms, perturbations can be applied at different points in the subject's gait. The use of a PID force controller allow the system to create omnidirectional perturbations with walking and running subjects while having the capability to vary amplitude and direction of perturbations. Analysis is presented of the workspace of the large treadmill to test whether the workspace available to activate these perturbations is safe. This paper reports the efficacy of the system and evaluates how warning a runner before impact may affect their displacement. Participants experienced 48 perturbations while running applied with a random combination of a front/back/left/right impact at either toe-off or mid-stance with or without warning. A two sample T-test reveals that warning a runner before impact significantly reduced the magnitude they were displaced for both toe-off (t(46) = 4.98 p<; .001) and mid-stance (t(46) = 3.44, p = .001).


Title: Evaluation of Human-Robot Object Co-manipulation Under Robot Impedance Control
Key Words: human-robot interaction  manipulator dynamics  trajectory control  object dynamical properties  robot impedance control  human-robot collaboration  human-robot object co-manipulation  pHRI  7-dof Kuka LBR iiwa 14 R820 robot  human trajectory  robot control law  human forces  interaction quality metrics  interaction comfort  human safety  physical human-robot interaction  continuous physical interaction  Collaboration  Task analysis  Impedance  Manipulators  Service robots  Force 
Abstract: The human-robot collaboration is a promising and challenging field of robotics research. One of the main collaboration tasks is the object co-manipulation where the human and robot are in a continuous physical interaction and forces exerted must be handled. This involves some issues known in robotics as physical Human-Robot Interaction (pHRI), where human safety and interaction comfort are required. Moreover, a definition of interaction quality metrics would be relevant. In the current work, the assessment of Human-Robot object co-manipulation task was explored through the proposed metrics of interaction quality, based on human forces throughout the movement. This analysis is based on co-manipulation of objects with different dynamical properties (weight and inertia), with and without including these properties knowledge in the robot control law. Here, the human is a leader of task and the robot the follower without any information of the human trajectory and movement profile. For the robot control law, a well-known impedance control was applied on a 7-dof Kuka LBR iiwa 14 R820 robot. Results show that the consideration of object dynamical properties in the robot control law is crucial for a good and more comfortable interaction. Besides, human efforts are more significant with a higher no-considered weight, whereas it remains stable when these weights were considered.


Title: Whole-Body Bilateral Teleoperation of a Redundant Aerial Manipulator
Key Words: autonomous aerial vehicles  delays  end effectors  force feedback  haptic interfaces  manipulator dynamics  mobile robots  position control  redundant manipulators  robot vision  telerobotics  video cameras  redundant aerial manipulator  robotic manipulator  flying base  reachability  manipulation task  human capabilities  telemanipulation tasks  visual feedback  task-dependent  video camera  end-effector motion  base position  stable bilateral teleoperation  time-delayed telemanipulation  whole-body bilateral teleoperation  null-space wall  haptic concept  kinematic structure  task-dependent optimal pose  Task analysis  Manipulators  Haptic interfaces  Cameras  Null space  Robot vision systems 
Abstract: Attaching a robotic manipulator to a flying base allows for significant improvements in the reachability and versatility of manipulation tasks. In order to explore such systems while taking advantage of human capabilities in terms of perception and cognition, bilateral teleoperation arises as a reasonable solution. However, since most telemanipulation tasks require visual feedback in addition to the haptic one, real-time (task-dependent) positioning of a video camera, which is usually attached to the flying base, becomes an additional objective to be fulfilled. Since the flying base is part of the kinematic structure of the robot, if proper care is not taken, moving the video camera could undesirably disturb the end-effector motion. For that reason, the necessity of controlling the base position in the null space of the manipulation task arises. In order to provide the operator with meaningful information about the limits of the allowed motions in the null space, this paper presents a novel haptic concept called Null-Space Wall. In addition, a framework to allow stable bilateral teleoperation of both tasks is presented. Numerical simulation data confirm that the proposed framework is able to keep the system passive while allowing the operator to perform time-delayed telemanipulation and command the base to a task-dependent optimal pose.


Title: Shared Autonomous Interface for Reducing Physical Effort in Robot Teleoperation via Human Motion Mapping
Key Words: humanoid robots  manipulators  mobile robots  motion control  telerobotics  mobile humanoid robot  general-purpose assistive tasks  motion mapping  human motion  robot teleoperation  autonomous interface  teleoperation interfaces  task completion time  assistance function  teleoperator  autonomous grasping function  Task analysis  Fatigue  Muscles  Grasping  Cameras  Robot vision systems 
Abstract: Motion mapping is an intuitive method of teleoperation with a low learning curve. Our previous study investigates the physical fatigue caused by teleoperating a robot to perform general-purpose assistive tasks and this fatigue affects the operator's performance. The results from that study indicate that physical fatigue happens more in the tasks which involve more precise manipulation and steady posture maintenance. In this paper, we investigate how teleoperation assistance in terms of shared autonomy can reduce the physical workload in robot teleoperation via motion mapping. Specifically, we conduct a user study to compare the muscle effort in teleoperating a mobile humanoid robot to (1) reach and grasp an individual object and (2) collect objects in a cluttered workspace with and without an autonomous grasping function that can be triggered manually by the teleoperator. We also compare the participants' task performance, subjective user experience, and change in attitude towards the usage of teleoperation assistance in the future based on their experience using the assistance function. Our results show that: (1) teleoperation assistance like autonomous grasping can effectively reduce the physical effort, task completion time and number of errors; (2) based on their experience performing the tasks with and without assistance, the teleoperators reported that they would prefer to use automated functions for future teleoperation interfaces.


Title: DexPilot: Vision-Based Teleoperation of Dexterous Robotic Hand-Arm System
Key Words: dexterous manipulators  robot vision  telerobotics  vision-based teleoperation  dexterous robotic hand-arm system  robotic systems  reasoning skills  depth-based teleoperation system  DoA robotic system  DexPilot  degree-of-actuation  multifingered robots  pick-and-place operations  Tracking  Three-dimensional displays  Task analysis  Robot sensing systems  Cameras  Neural networks 
Abstract: Teleoperation offers the possibility of imparting robotic systems with sophisticated reasoning skills, intuition, and creativity to perform tasks. However, teleoperation solutions for high degree-of-actuation (DoA), multi-fingered robots are generally cost-prohibitive, while low-cost offerings usually offer reduced degrees of control. Herein, a low-cost, depth-based teleoperation system, DexPilot, was developed that allows for complete control over the full 23 DoA robotic system by merely observing the bare human hand. DexPilot enabled operators to solve a variety of complex manipulation tasks that go beyond simple pick-and-place operations and performance was measured through speed and reliability metrics. DexPilot cost-effectively enables the production of high dimensional, multi-modality, state-action data that can be leveraged in the future to learn sensorimotor policies for challenging manipulation tasks. The videos of the experiments can be found at https://sites.google.com/view/dex-pilot.


Title: Distributed Winner-Take-All Teleoperation of A Multi-Robot System
Key Words: decision making  Lyapunov methods  multi-robot systems  protocols  stability  telerobotics  team cohesion  dynamic decision-making protocol  decision variable  slave robots  decision-making algorithm  3-masters-11-slaves teleoperation  distributed winner-take-all teleoperation  multirobot system  multimaster-multislave teleoperation system  Lyapunov stability analysis  Robots  Decision making  Protocols  Heuristic algorithms  Indexes  Force  Multi-robot systems 
Abstract: In a distributed multi-master-multi-slave teleoperation system, the human users may compete against each other for the control of the team of slave robots. To win the competition, one operator would send the largest command to the slave group. For the sake of team cohesion, the slave group should follow the command of the winning operator and ignore the commands of the other users. To enable (i) the slave team to identify the winning operator, and (ii) each slave to determine whether to admit or discard the command it receives from its operator, this paper proposes a dynamic decision-making protocol that distinguishes the decision variable of the slave commanded by the winner from the decision variables of all other slave robots. The protocol only requires the slaves to exchange and evaluate their decision variables locally. Lyapunov stability analysis proves the theoretical convergence of the proposed decision-making algorithm. An experimental distributed winner-take-all teleoperation in a 3-masters-11-slaves teleoperation testbed validates its practical efficacy.


Title: Enhanced Teleoperation Using Autocomplete
Key Words: autonomous aerial vehicles  learning (artificial intelligence)  mobile robots  telerobotics  enhanced teleoperation  Autocomplete  remote location  skilled teleoperators  training time  novice teleoperators  human input  desired motion  machine learning  motion primitives  unmanned aerial vehicle  Task analysis  Trajectory  Robots  Training  Support vector machines  Manuals  Drones 
Abstract: Controlling and manning robots from a remote location is difficult because of the limitations one faces in perception and available degrees of actuation. Although humans can become skilled teleoperators, the amount of training time required to acquire such skills is typically very high. In this paper, we propose a novel solution (named Autocomplete) to aid novice teleoperators in manning robots adroitly. At the input side, Autocomplete relies on machine learning to detect and categorize human inputs as one from a group of motion primitives. Once a desired motion is recognized, at the actuation side an automated command replaces the human input in performing the desired action. So far, Autocomplete can recognize and synthesize lines, arcs, full circles, 3-D helices, and sine trajectories. Autocomplete was tested in simulation on the teleoperation of an unmanned aerial vehicle, and results demonstrate the advantages of the proposed solution versus manual steering.


Title: Strategy for automated dense parking: how to navigate in narrow lanes*
Key Words: automobiles  Global Positioning System  Lyapunov methods  mobile robots  motion control  navigation  path planning  road traffic control  narrow lanes  high- density parking solution  car-like robots  hard constraints  robot motion  robot localization  navigation  configuration space formulation  Stanley Robotics robots  automated dense parking  Lyapunov- based control strategy  GPS orientation  Robots  Collision avoidance  Aerospace electronics  Navigation  Mathematical model  Kinematics  Automobiles 
Abstract: This paper presents the architecture of a high- density parking solution based on car-like robots specifically designed to move cars. The main difficulty is to park the vehicles close to one another which implies hard constraints on the robot motion and localization. In particular, this paper focuses on navigation in narrow lanes. We propose a Lyapunov- based control strategy that has been derived after expressing the problem in a Configuration Space formulation. The current solution has been implemented and tested on Stanley Robotics' robots and has been running in production for several months. Thanks to the Configuration Space formulation, we are able to guarantee the obstacles' integrity. Moreover, a method for calibrating the GPS orientation with a high-precision is derived from the present control strategy.


Title: Online optimal motion generation with guaranteed safety in shared workspace
Key Words: collision avoidance  industrial manipulators  motion control  occupational safety  path planning  predictive control  online optimal motion generation  guaranteed safety  shared workspace  safer manipulator robots  serious injury  equip robots  online motion generation  partially unknown dynamic environment  industrial manipulator robots  model predictive control scheme  Collision avoidance  Trajectory  Safety  Manipulators  Robot sensing systems  Service robots 
Abstract: With new, safer manipulator robots, the probability of serious injury due to collisions with humans remains low (5%), even at speeds as high as 2 m.s-1. Collisions would better be avoided nevertheless, because they disrupt the tasks of both the robot and the human. We propose in this paper to equip robots with exteroceptive sensors and online motion generation so that the robot is able to perceive and react to the motion of the human in order to reduce the occurrence of collisions. It's impossible to guarantee that no collision will ever take place in a partially unknown dynamic environment such as a shared workspace, but we can guarantee instead that, if a collision takes place, the robot is at rest at the time of collision, so that it doesn't inject its own kinetic energy in the collision. To do so, we adapt a Model Predictive Control scheme which has been demonstrated previously with two industrial manipulator robots avoiding collisions while sharing their workspace. The proposed control scheme is validated in simulation.


Title: Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing
Key Words: aircraft control  helicopters  learning (artificial intelligence)  nonlinear control systems  nonlinear dynamical systems  optimal control  predictive control  robot dynamics  model predictive control  nonlinear diffeomorphism  nonlinear dynamical systems  optimal control  multirotor landing  nonlinear robot dynamics  episodic Koopman learning  Eigenvalues and eigenfunctions  Nonlinear dynamical systems  Robots  Aerospace electronics  Heuristic algorithms  Vehicle dynamics 
Abstract: This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.


Title: Eye-in-Hand 3D Visual Servoing of Helical Swimmers Using Parallel Mobile Coils
Key Words: marine systems  microrobots  mobile robots  robot vision  visual servoing  refraction-rectified location algorithm  coil module  motor module  eye-in-hand stereo-vision module  medical applications  spacial movement  control methods  magnetic actuation systems  narrow space  magnetic field  magnetic helical microswimmers  parallel mobile coils  eye-in-hand 3D visual servoing  cylindrical workspace  prototype system  long-distance 3D path  triple-loop stereo visual servoing strategy  dynamic magnetic fields  mobile-coil system  Coils  Magnetic resonance imaging  Three-dimensional displays  Cameras  Magnetic devices  Visual servoing  Magnetic separation 
Abstract: Magnetic helical microswimmers can be propelled by rotating magnetic field and are adept at passing through narrow space. To date, various magnetic actuation systems and control methods have been developed to drive these microswimmers. However, steering their spacial movement in a large workspace is still challenging, which could be significant for potential medical applications. In this regard, this paper designs an eye-in-hand stereo-vision module and corresponding refraction-rectified location algorithm. Combined with the motor module and the coil module, the mobile-coil system is capable of generating dynamic magnetic fields in a large 3D workspace. Based on the system, a robust triple-loop stereo visual servoing strategy is proposed that operates simultaneous tracking, locating, and steering, through which the helical swimmer is able to follow a long-distance 3D path. A scaled-up magnetic helical swimmer is employed in the path following experiment. Our prototype system reaches a cylindrical workspace with a diameter more than 200 mm, and the mean error of path tracking is less than 2 mm.


Title: A Mobile Paramagnetic Nanoparticle Swarm with Automatic Shape Deformation Control
Key Words: control nonlinearities  deformation  fuzzy control  microrobots  mobile robots  multi-robot systems  nanoparticles  robot dynamics  EPNS  mobile paramagnetic nanoparticle swarm  automatic shape deformation control  swarm control  active shape deformation  elliptical rotating magnetic fields  swarm pattern  elliptical paramagnetic nanoparticle swarm  strength ratio  elliptical field  shape ratio  length ratio  deformation dynamics  fuzzy logic-based control  nanorobot  microrobotics  field ratio  nonlinearity  planar rotational locomotion  planar translational locomotion  Shape  Strain  Magnetic resonance imaging  Nanoparticles  Virtual private networks  Micromagnetics  Magnetic anisotropy 
Abstract: Recently, swarm control of micro-/nanorobots has drawn much attention in the field of microrobotics. This paper reports a mobile paramagnetic nanoparticle swarm with the capability of active shape deformation that can improve its environment adaptability. We show that, by applying elliptical rotating magnetic fields, a swarm pattern called the elliptical paramagnetic nanoparticle swarm (EPNS) would be formed. When changing the field ratio-α (i.e. the strength ratio between the minor axis and major axis of the elliptical field), the shape ratio-β of the EPNS (i.e. the length ratio between the major axis and minor axis) will change accordingly. However, automatically control this shape deformation process has difficulties because the deformation dynamics has strong nonlinearity, model variation and long time requirement. To solve this problem, we propose a fuzzy logic-based control scheme that utilizes the knowledge and control experience from skilled human operators. Experiments show that the proposed control scheme can stably maneuver the shape deformation of the EPNS with small overshoot, which cannot be achieved by conventional PI control. Moreover, experimental results show that, with the automatic shape deformation control, shape of the EPNS is controlled with high reversibility and also can be well maintained during the planar rotational and translational locomotion of the EPNS.


Title: Magnetic miniature swimmers with multiple rigid flagella
Key Words: biomechanics  microrobots  mobile robots  position control  propulsion  magnetic miniature swimmers  multiple rigid flagella  multiple rigid tails  rotating magnetic field  robot rotation  tail distribution  tail height  multitailed swimmer robots  spherical helices  2-tailed swimmer  angular position  Robots  Propulsion  Magnetosphere  Prototypes  Microorganisms  Force  Mathematical model 
Abstract: In this paper, we introduce novel miniature swimmers with multiple rigid tails based on spherical helices. The tail distribution of these prototypes enhances its swimming features as well as allowing to carry objects with it. The proposed swimmers are actuated by a rotating magnetic field, generating the robot rotation and thus producing a considerable thrust to start self-propelling. These prototypes achieved propulsion speeds up to 6 mm/s at 3.5 Hz for a 6-mm in size prototypes. We study the efficiency of different tail distribution for a 2-tailed swimmer by varying the angular position between both tails. Moreover, it is demonstrated that these swimmers experience great sensibility when changing their tail height. Besides, these swimmers demonstrate to be effective for cargo carrying tasks since they can displace objects up to 3.5 times their weight. Finally, wall effect is studied with multi-tailed swimmer robots considering 2 containers with 20 and 50-mm in width. Results showed speeds' increments up to 59% when swimmers are actuated in the smaller container.


Title: Design and Control of a Large-Range Nil-Stiffness Electro-Magnetic Active Force Sensor
Key Words: calibration  closed loop systems  electromagnetic actuators  force measurement  force sensors  microsensors  MicroElectro Mechanical Systems  force measurements  meso-scale robotics  meso-scale active force sensor  novel meso-scale sensor  nil-stiffness guidance  loop control  nil-stiffness characteristic  infinite stiffness  sensor architecture  low frequency forces  cutoff frequency  large-range nil-stiffness electro-magnetic active force sensor  active force sensors  measurement range  conventional passive force sensors  quasiinfinite stiffness  frequency 73.9 Hz  Probes  Force  Force measurement  Optical sensors  Force sensors  Sensor phenomena and characterization 
Abstract: Active force sensors are key instruments to get around the tradeoff between the sensitivity and the measurement range of conventional passive force sensors. Thanks to their quasi-infinite stiffness in closed loop, active sensors can be applied for force measurements on samples with a wide range of stiffness without interference with the mechanical parameters of the sensor. MEMS (Micro-Electro Mechanical Systems) active force sensors have been wildly developed in the literature but they are ill adapted for force measurements at the Newton level needed in meso-scale robotics. In this article, a novel structure for a meso-scale active force sensor is proposed for the measurement of forces from the milli-newton to the newton.This novel meso-scale sensor is based on a nil-stiffness guidance and an electromagnetic actuation. This paper deals with its design, identification, calibration and closed loop control. The sensor exhibits nil-stiffness characteristic in open loop and an almost infinite stiffness in closed loop. This allows measuring forces with a large range of gradients. First experiments shows the ability of this new sensor architecture to measure low frequency forces up to 0.8N with a precision of 0.03 N and a closed loop -20 dB cutoff frequency of 73.9Hz.


Title: Multispectral Domain Invariant Image for Retrieval-based Place Recognition
Key Words: image colour analysis  image recognition  image retrieval  image segmentation  infrared imaging  spectral analysis  multispectral place recognition task  multispectral semantic segmentation  multispectral domain invariant image  retrieval-based place recognition  multispectral recognition  thermal image  RGB domain-based tasks  multispectral domain invariant framework  unpaired image translation method  semantic image  discriminative invariant image  Image recognition  Robot sensing systems  Task analysis  Semantics  Thermal sensors  Feature extraction  Imaging 
Abstract: Multispectral recognition has attracted increasing attention from the research community due to its potential competence for many applications from day to night. However, due to the domain shift between RGB and thermal image, it has still many challenges to apply and to use RGB domain-based tasks. To reduce the domain gap, we propose multispectral domain invariant framework, which leverages the unpaired image translation method to generate a semantic and strongly discriminative invariant image by enforcing novel constraints in the objective function. We demonstrate the efficacy of the proposed method on mainly multispectral place recognition task and achieve significant improvement compared to previous works. Furthermore, we test on multispectral semantic segmentation and unsupervised domain adaptations to prove the scalability and generality of the proposed method. We will open our source code and dataset.


Title: Probabilistic Effect Prediction through Semantic Augmentation and Physical Simulation
Key Words: failure analysis  humanoid robots  mobile robots  planning (artificial intelligence)  probability  probabilistic effect prediction  semantic augmentation  exact outcome  failure situations  failure tolerance  robot actions  augmenting collected experience  semantic knowledge  realistic physics simulations  outcome probabilities  unknown tasks  simulated experience  action success probabilities  world experiments  humanoid robot  planning trials  Rollin Justin  Robots  Planning  Probabilistic logic  Cognition  Semantics  Task analysis  Predictive models 
Abstract: Nowadays, robots are mechanically able to perform highly demanding tasks, where AI-based planning methods are used to schedule a sequence of actions that result in the desired effect. However, it is not always possible to know the exact outcome of an action in advance, as failure situations may occur at any time. To enhance failure tolerance, we propose to predict the effects of robot actions by augmenting collected experience with semantic knowledge and leveraging realistic physics simulations. That is, we consider semantic similarity of actions in order to predict outcome probabilities for previously unknown tasks. Furthermore, physical simulation is used to gather simulated experience that makes the approach robust even in extreme cases. We show how this concept is used to predict action success probabilities and how this information can be exploited throughout future planning trials. The concept is evaluated in a series of real world experiments conducted with the humanoid robot Rollin' Justin.


Title: Anytime Integrated Task and Motion Policies for Stochastic Environments
Key Words: intelligent robots  mobile robots  multi-agent systems  multi-robot systems  path planning  planning (artificial intelligence)  stochastic processes  multiple execution-time contingencies  motion policies  stochastic settings  stochastic situations  abstract models  motion planning  abstract planning  intelligent robots  stochastic environments  anytime integrated task  Robots  Planning  Task analysis  Computational modeling  Collision avoidance  Stochastic processes  Trajectory 
Abstract: In order to solve complex, long-horizon tasks, intelligent robots need to carry out high-level, abstract planning and reasoning in conjunction with motion planning. However, abstract models are typically lossy and plans or policies computed using them can be unexecutable. These problems are exacerbated in stochastic situations where the robot needs to reason about, and plan for multiple contingencies. We present a new approach for integrated task and motion planning in stochastic settings. In contrast to prior work in this direction, we show that our approach can effectively compute integrated task and motion policies whose branching structures encoding agent behaviors handling multiple execution-time contingencies. We prove that our algorithm is probabilistically complete and can compute feasible solution policies in an anytime fashion so that the probability of encountering an unresolved contingency decreases over time. Empirical results on a set of challenging problems show the utility and scope of our methods.


Title: CCRobot-III: a Split-type Wire-driven Cable Climbing Robot for Cable-stayed Bridge Inspection*
Key Words: bridges (structures)  cables (mechanical)  grippers  inspection  mobile robots  robot dynamics  wires  palm-based gripper  CCRobot-III  cable climbing robot  cable-stayed bridge inspection  mainbody frame  steel wires  climbing precursor  Split-type Wire-driven design  mass 40.0 kg  size 90.0 mm to 110.0 mm  Bridges  Payloads  Force  Wheels  Winches  Robots  Inspection 
Abstract: This paper presents a novel Cable Climbing Robot CCRobot-III, which is the third version designed for bridge cable inspection tasks, aiming at surpassing previous versions in terms of climbing speed and payload capacity. Benefiting from Split-type Wire-driven design, CCRobot-III can climb along a 90-110mm diameter bridge cable in inchworm-like gait at a speed of up to 12m/min, and carrying more than 40kg payload at the same time. CCRobot-III consists of a climbing precursor and a main-body frame. The two parts are connected and driven by steel wires. The climbing precursor, acting as a mobile anchor, moves quickly on a bridge cable. The mainbody frame, acting as a mobile winch, carries payload and pulls itself to a certain position with steel wires. Both parts have one or two pairs of palm-based gripper, which is the key component for providing strong adhesion to support the robot climbing. Experimental results have shown that CCRobotIII possesses outstanding climbing performance, high payload capacity, and good adaptability to complex conditions of cable surface. Moreover, it has potential engineering applications on the cable-stayed bridge for fieldwork.


Title: Omnidirectional Tractable Three Module Robot
Key Words: control engineering computing  mobile robots  motion control  pipes  robot kinematics  omnidirectional tractable three module robot  omnidirectional modules  holonomic motion  motion singularity region  motion capabilities  closed-form kinematic model  MSC ADAMS  Robots  Angular velocity  Turning  Shafts  Elbow  Crawlers  Kinematics 
Abstract: This paper introduces the Omnidirectional Tractable Three Module Robot for traversing inside complex pipe networks. The robot consists of three omnidirectional modules fixed 120° apart circumferentially which can rotate about their own axis allowing holonomic motion of the robot. The holonomic motion enables the robot to overcome motion singularity when negotiating T-junctions and further allows the robot to arrive in a preferred orientation while taking turns inside a pipe. We have developed a closed-form kinematic model for the robot in the paper and propose the `Motion Singularity Region' that the robot needs to avoid while negotiating T-junction. The design and motion capabilities of the robot are demonstrated both by conducting simulations in MSC ADAMS on a simplified lumped-model of the robot and with experiments on its physical embodiment.


Title: A Practical Climbing Robot for Steel Bridge Inspection
Key Words: bridges (structures)  design engineering  inspection  maintenance engineering  mobile robots  steel  structural engineering  ARA lab robot  steel structures  practical climbing robot  steel bridge inspection  cutting edge steel inspection robots  unified design  advanced robotic and automation lab  Bridges  Steel  Mobile robots  Inspection  Adhesives  Force 
Abstract: The advanced robotic and automation (ARA) lab has developed and successfully implemented a design inspired by many of the various cutting edge steel inspection robots to date. The combination of these robots concepts into a unified design came with its own set of challenges since the parameters for these features sometimes conflicted. An extensive amount of design and analysis work was performed by the ARA lab in order to find a carefully tuned balance between the implemented features on the ARA robot and general functionality. Having successfully managed to implement this conglomerate of features represents a breakthrough to the industry of steel inspection robots as the ARA lab robot is capable of traversing most complex geometries found on steel structures while still maintaining its ability to efficiently travel along these structures; a feat yet to be done until now.


Title: Development of a Wheeled Wall-Climbing Robot with a Shape-Adaptive Magnetic Adhesion Mechanism
Key Words: adhesion  elasticity  industrial manipulators  manipulator kinematics  mobile robots  permanent magnets  robotic welding  steel  wheels  nonelastic suspension mechanism  arbitrary curved shape  wheeled wall-climbing robot  curved ferromagnetic surfaces  magnetic force direction  magnetic wheels  spherical shape  2 DOF rotational magnetic adhesion  shape-adaptive magnetic adhesion  Wheels  Mobile robots  Magnetic levitation  Magnetic forces  Welding  Adhesives 
Abstract: This paper presents a wheeled wall-climbing robot with a shape-adaptive magnetic adhesion mechanism for large steel structures. To travel up and down various curved ferromagnetic surfaces, we developed a 2 DOF rotational magnetic adhesion mechanism installed on each wheel that can change the orientation of the magnets to keep the magnetic force direction always normal to the contact surface. These magnetic wheels have a spherical shape and can move relative to the main body by a non-elastic suspension mechanism so that the robot can climb up small obstacles on the ground and find contact points for each wheel on a wall with an arbitrary curved shape. Being geometrically stable is important for the robot because this robot is intended to be a mobile base for a welding manipulator. The detailed design of the mechanism and the results of climbing tests are presented.


Title: Algebraic Fault Detection and Identification for Rigid Robots
Key Words: fault diagnosis  manipulators  nonlinear control systems  polynomial approximation  SCARA  polynomial approximation  orthonormal Jacobi polynomials  nonlinear mechanical systems  rigid robots  algebraic fault detection  Fault detection  Jacobian matrices  Mathematical model  Kernel  Time measurement  Robot kinematics 
Abstract: This paper presents a method for algebraic fault detection and identification of nonlinear mechanical systems, describing rigid robots, by using an approximation with orthonormal Jacobi polynomials. An explicit expression is derived for the fault from the equation of motion, which is decoupled from disturbances and only depends on measurable signals and their time derivatives. Fault detection and identification is then achieved by polynomial approximation of the determined fault term. The results are illustrated by a simulation for a faulty SCARA.


Title: Detecting Execution Anomalies As an Oracle for Autonomy Software Robustness
Key Words: fault diagnosis  pattern clustering  robot programming  system monitoring  execution anomaly detection  autonomy software robustness  system monitoring  clustering algorithm  autonomy systems  robotics systems  real-world industrial system  autonomous systems  fault identification  Testing  Tools  Instruments  Clustering algorithms  Service robots  Safety 
Abstract: We propose a method for detecting execution anomalies in robotics and autonomy software. The algorithm uses system monitoring techniques to obtain profiles of executions. It uses a clustering algorithm to create clusters of those executions, representing nominal execution. A distance metric determines whether additional execution profiles belong to the existing clusters or should be considered anomalies. The method is suitable for identifying faults in robotics and autonomy systems. We evaluate the technique in simulation on two robotics systems, one of which is a real-world industrial system. We find that our technique works well to detect possibly unsafe behavior in autonomous systems.


Title: Real-Time, Highly Accurate Robotic Grasp Detection using Fully Convolutional Neural Network with Rotation Ensemble Module
Key Words: convolutional neural nets  image classification  learning (artificial intelligence)  object detection  robot vision  highly accurate robotic grasp detection  fully convolutional neural network  rotation ensemble module  rotation invariance  computer vision tasks  rotation anchor box  multiple objects  4-axis robot arm  Cornell dataset  REM  Proposals  Grasping  Task analysis  Robot sensing systems  Feature extraction  Kernel 
Abstract: Rotation invariance has been an important topic in computer vision tasks. Ideally, robot grasp detection should be rotation-invariant. However, rotation-invariance in robotic grasp detection has been only recently studied by using rotation anchor box that are often time-consuming and unreliable for multiple objects. In this paper, we propose a rotation ensemble module (REM) for robotic grasp detection using convolutions that rotates network weights. Our proposed REM was able to outperform current state-of-the-art methods by achieving up to 99.2% (image-wise), 98.6% (object-wise) accuracies on the Cornell dataset with real-time computation (50 frames per second). Our proposed method was also able to yield reliable grasps for multiple objects and up to 93.8% success rate for the real-time robotic grasping task with a 4-axis robot arm for small novel objects that was significantly higher than the baseline methods by 11-56%.


Title: Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly
Key Words: CAD  learning (artificial intelligence)  object recognition  pose estimation  robotic assembly  kit assembly task  shape matching problem  shape descriptor  object surfaces  self-supervised data-collection pipeline  robotic assembly  3D CAD models  task-specific training data  Task analysis  Shape  Visualization  Training data  Three-dimensional displays  Solid modeling  Training 
Abstract: Is it possible to learn policies for robotic assembly that can generalize to new objects? We explore this idea in the context of the kit assembly task. Since classic methods rely heavily on object pose estimation, they often struggle to generalize to new objects without 3D CAD models or task-specific training data. In this work, we propose to formulate the kit assembly task as a shape matching problem, where the goal is to learn a shape descriptor that establishes geometric correspondences between object surfaces and their target placement locations from visual input. This formulation enables the model to acquire a broader understanding of how shapes and surfaces fit together for assembly - allowing it to generalize to new objects and kits. To obtain training data for our model, we present a self-supervised data-collection pipeline that obtains ground truth object-to-placement correspondences by disassembling complete kits. Our resulting real-world system, Form2Fit, learns effective pick and place strategies for assembling objects into a variety of kits - achieving 90% average success rates under different initial conditions (e.g. varying object and kit poses), 94% success under new configurations of multiple kits, and over 86% success with completely new objects and kits. Code, videos, and supplemental material are available at https://form2fit.github.io.


Title: Learning Rope Manipulation Policies Using Dense Object Descriptors Trained on Synthetic Depth Data
Key Words: intelligent robots  learning (artificial intelligence)  manipulators  robot vision  ropes  rope manipulation policies  dense object descriptors  synthetic depth data  robotic manipulation  deformable 1D objects  high-fidelity analytic models  configuration spaces  end-to-end manipulation policies  physical interaction  interpretable deep visual representations  robot manipulation  interpretable policies  transferable geometric policies  visual reasoning  point-pair correspondences  initial goal rope configurations  geometric structure  synthetic depth images  dense depth object descriptors  ABB YuMi Robot  interpretable geometric policies  Task analysis  Visualization  Robots  Strain  Training  Videos  Data models 
Abstract: Robotic manipulation of deformable 1D objects such as ropes, cables, and hoses is challenging due to the lack of high-fidelity analytic models and large configuration spaces. Furthermore, learning end-to-end manipulation policies directly from images and physical interaction requires significant time on a robot and can fail to generalize across tasks. We address these challenges using interpretable deep visual representations for rope, extending recent work on dense object descriptors for robot manipulation. This facilitates the design of interpretable and transferable geometric policies built on top of the learned representations, decoupling visual reasoning and control. We present an approach that learns point-pair correspondences between initial and goal rope configurations, which implicitly encodes geometric structure, entirely in simulation from synthetic depth images. We demonstrate that the learned representation - dense depth object descriptors (DDODs) - can be used to manipulate a real rope into a variety of different arrangements either by learning from demonstrations or using interpretable geometric policies. In 50 trials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66% knot-tying success rate from previously unseen configurations. See https://tinyurl.com/rope-learning for supplementary material and videos.


Title: Efficient two step optimization for large embedded deformation graph based SLAM
Key Words: computational complexity  embedded systems  graph theory  Hessian matrices  robot vision  SLAM (robots)  stereo image processing  parameter estimation  computation complexity  two step optimization  deformable geometry  stereo camera  SLAM applications  large scale embedded deformation graph  Hessian matrix  Simultaneous localization and mapping  Strain  Jacobian matrices  Optimization  Cameras  Deformable models  Geometry 
Abstract: Embedded deformation graph is a widely used technique in deformable geometry and graphical problems. Although the technique has been transmitted to stereo (or RGB-D) camera based SLAM applications, it remains challenging to compromise the computational cost as the model grows. In practice, the processing time grows rapidly in accordance with the expansion of maps. In this paper, we propose an approach to decouple the nodes of deformation graph in large scale dense deformable SLAM and keep the estimation time to be constant. We observe that only partial deformable nodes in the graph are connected to visible points. Based on this fact, the sparsity of the original Hessian matrix is utilized to split the parameter estimation into two independent steps. With this new technique, we achieve faster parameter estimation with amortized computation complexity reduced from O(n2) to almost O(1). As a result, the computational cost barely increases as the map keeps growing. Based on our strategy, the computational bottleneck in large scale embedded deformation graph based applications will be greatly mitigated. The effectiveness is validated by experiments, featuring large scale deformation scenarios.


Title: Camera-to-Robot Pose Estimation from a Single Image
Key Words: cameras  image colour analysis  image sensors  manipulators  neural nets  pose estimation  robot vision  camera-to-robot pose estimation  single RGB image  deep neural network  perspective-n-point  robot manipulator  classic hand-eye calibration systems  camera sensors  classic off-line hand-eye calibration  robot sensors  Cameras  Robot vision systems  Robot kinematics  Calibration  Two dimensional displays  Training 
Abstract: We present an approach for estimating the pose of an external camera with respect to a robot using a single RGB image of the robot. The image is processed by a deep neural network to detect 2D projections of keypoints (such as joints) associated with the robot. The network is trained entirely on simulated data using domain randomization to bridge the reality gap. Perspective-n-point (PnP) is then used to recover the camera extrinsics, assuming that the camera intrinsics and joint configuration of the robot manipulator are known. Unlike classic hand-eye calibration systems, our method does not require an off-line calibration step. Rather, it is capable of computing the camera extrinsics from a single frame, thus opening the possibility of on-line calibration. We show experimental results for three different robots and camera sensors, demonstrating that our approach is able to achieve accuracy with a single frame that is comparable to that of classic off-line hand-eye calibration using multiple frames. With additional frames from a static pose, accuracy improves even further. Code, datasets, and pretrained models for three widely-used robot manipulators are made available.


Title: PST900: RGB-Thermal Calibration, Dataset and Segmentation Network
Key Words: calibration  control engineering computing  convolutional neural nets  image colour analysis  image segmentation  infrared imaging  learning (artificial intelligence)  mobile robots  PST900  segmentation network  long wave infrared imagery  RGB-thermal camera calibration  thermal image pairs  DARPA Subterranean Challenge  RGB imagery  semantic segmentation  CNN architecture  Cameras  Calibration  Image segmentation  Semantics  Heating systems  Aluminum  Three-dimensional displays 
Abstract: In this work we propose long wave infrared (LWIR) imagery as a viable supporting modality for semantic segmentation using learning-based techniques. We first address the problem of RGB-thermal camera calibration by proposing a passive calibration target and procedure that is both portable and easy to use. Second, we present PST900, a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge. Lastly, we propose a CNN architecture for fast semantic segmentation that combines both RGB and Thermal imagery in a way that leverages RGB imagery independently. We compare our method against the state-of-the-art and show that our method outperforms them in our dataset.


Title: Generation of Object Candidates Through Simply Looking Around
Key Words: cameras  image segmentation  image sequences  mobile robots  object detection  object recognition  robot vision  video signal processing  mobile robot  pan-tilt monocular camera  camera movements  robot operating indoors  object candidates  Cameras  Robot vision systems  Image segmentation  Visualization  Coherence  Tracking 
Abstract: In this paper, we consider the generation of generic object candidates by a mobile robot that is endowed with a pan-tilt monocular camera. This is an important problem because these candidates serve as basis for the robot to categorize and/or recognize the objects in its surroundings. The previously proposed methods either do not have a means of enabling the robot to look around through moving its camera or do not take advantage of the temporal coherence of the video data. We present a novel approach that enables the robot to achieve both of these capabilities simultaneously. In this approach, the robot's camera movements are governed by a family of controllers whose constructions depend on the set of object candidates that have been hitherto generated, but not directly looked at. In parallel, the robot discovers the object candidates through tracking segments and determining spatio-temporally coherent ones. The advantage of the proposed approach is that while the robot can explore its surroundings by simply looking around prior to more sophisticated exploration behavior involving possibly bodily locomotion the generated object candidates turn out to be consolidated across the visual stream in comparison to single-shot methods. This is demonstrated in extensive experimental results with a robot operating indoors varying in clutter as well as outdoors.


Title: A water-obstacle separation and refinement network for unmanned surface vehicles
Key Words: collision avoidance  edge detection  feature extraction  image fusion  image segmentation  inertial navigation  marine vehicles  mobile robots  neural net architecture  remotely operated vehicles  robot vision  refinement network  unmanned surface vehicles  obstacle detection  water reflections  wakes  inertial information fusion  visual features  deep encoder decoder architecture  water obstacle separation  semantic segmentation  autonomous navigation  water edge estimation  inertial measurement unit  loss function  water features  Decoding  Visualization  Feature extraction  Image segmentation  Semantics  Image edge detection  Task analysis  obstacle detection  semantic segmentation  sensor fusion  unmanned surface vehicles  separation function 
Abstract: Obstacle detection by semantic segmentation shows a great promise for autonomous navigation in unmanned surface vehicles (USV). However, existing methods suffer from poor estimation of the water edge in presence of visual ambiguities, poor detection of small obstacles and high false-positive rate on water reflections and wakes. We propose a new deep encoder-decoder architecture, a water-obstacle separation and refinement network (WaSR), to address these issues. Detection and water edge accuracy are improved by a novel decoder that gradually fuses inertial information from inertial measurement unit (IMU) with the visual features from the encoder. In addition, a novel loss function is designed to increase the separation between water and obstacle features early on in the network. Subsequently, the capacity of the remaining layers in the decoder is better utilised, leading to a significant reduction in false positives and increased true positives. Experimental results show that WaSR outperforms the current state-of-the-art by a large margin, yielding a 14% increase in F-measure over the second-best method.


Title: Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar
Key Words: distance measurement  feature extraction  image colour analysis  image sensors  image sequences  mobile robots  motion estimation  object recognition  object tracking  radar computing  robot vision  SLAM (robots)  supervised learning  predict robust keypoints  odometry estimation  metric localisation  self-supervised framework  differentiable point-based motion estimator  localisation error  Oxford Radar RobotCar Dataset  point-based radar odometry  Radar  Measurement  Task analysis  Estimation  Robot sensing systems  Computer architecture 
Abstract: This paper presents a self-supervised framework for learning to detect robust keypoints for odometry estimation and metric localisation in radar. By embedding a differentiable point-based motion estimator inside our architecture, we learn keypoint locations, scores and descriptors from localisation error alone. This approach avoids imposing any assumption on what makes a robust keypoint and crucially allows them to be optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from the Oxford Radar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneously solving metric loop closures. Combining these outputs, we provide a framework capable of full mapping and localisation with radar in urban environments.


Title: Where and When: Event-Based Spatiotemporal Trajectory Prediction from the iCub’s Point-Of-View
Key Words: cameras  control engineering computing  human-robot interaction  image resolution  recurrent neural nets  robot vision  spatiotemporal phenomena  iCub robot  event-based spatiotemporal trajectory prediction  nonlinear trajectories  frame-based cameras  asynchronous information stream  low latency information stream  high temporal resolution  long short-term memory networks  event-cameras spatial sampling  encoder-decoder architecture  spatial trajectory points  human-to-robot handover trajectories  Trajectory  Robots  Predictive models  Cameras  Target tracking  Data models  Pipelines 
Abstract: Fast, non-linear trajectories have been shown to be more accurately visually measured, and hence predicted, when sampled spatially (that is when the target position changes) rather than temporally, i.e. at a fixed-rate as in traditional frame-based cameras. Event-cameras, with their asynchronous, low latency information stream, allow for spatial sampling with very high temporal resolution, improving the quality of the data and the accuracy of post-processing operations. This paper investigates the use of Long Short-Term Memory (LSTM) networks with event-cameras spatial sampling for trajectory prediction. We show the benefit of using an Encoder-Decoder architecture over parameterised models for regression on event-based human-to-robot handover trajectories. In particular, we exploit the temporal information associated to the events stream to predict not only the incoming spatial trajectory points, but also when these will occur in time. After having studied the proper LSTM input/output sequence length, the network performance are compared to other regression models. Then, prediction behavior and computational time are analysed for the proposed method. We carry out the experiment using an iCub robot equipped with event-cameras, addressing the problem from the robot perspective.


Title: A Data-driven Planning Framework for Robotic Texture Painting on 3D Surfaces
Key Words: geometry  image texture  industrial robots  mixing  painting  path planning  recurrent neural nets  robot vision  supervised learning  robotic texture painting  painting textures  surface geometry  paint mixing  self-supervised learning framework  painting process  paint simulation environment  robot executes  data-driven planning framework  paint delivery tool flow rate  3D surfaces  recurrent neural network  RNN  Painting  Paints  Robots  Three-dimensional displays  Solid modeling  Two dimensional displays 
Abstract: Painting textures on 3D surfaces requires an understanding of the surface geometry, paint flow and paint mixing. This work formulates automated painting as a planning problem and proposes a solution based on a self-supervised learning framework that enables a robot to paint monochromatic non-uniform textures on 3D surfaces. We developed a method that iteratively decides the actions to take based on constant feedback of the painting process. Inspired by recent results, we formulate our solution using a recurrent neural network (RNN) to decide where and what to paint on the surface at each time instant. Specifically, the paint delivery tool's flow rate, orientation and position relative to the surface at each time instant are evaluated. This data can then be processed by a robot's planner of choice for generating a painting mission that can achieve the desired end result. We evaluate the proposed approach by providing qualitative and quantitative results of the different components. Furthermore, we validate the effectiveness of the approach for the application by providing renderings from a paint simulation environment and show how a robot executes the planned painting mission on a generic 3D surface.


Title: Learned Critical Probabilistic Roadmaps for Robotic Motion Planning
Key Words: graph theory  mobile robots  motion control  path planning  sampling methods  implicit graph representation  state space  solution trajectories  graph-theoretic techniques  hierarchical graph  uniform sampling  sampling-based motion planning  robotic motion planning  motion planning techniques  learned critical probabilistic roadmaps  critical PRM  critical probabilistic roadmaps  Planning  Robots  Trajectory  Probabilistic logic  Training  Complexity theory  Neural networks 
Abstract: Sampling-based motion planning techniques have emerged as an efficient algorithmic paradigm for solving complex motion planning problems. These approaches use a set of probing samples to construct an implicit graph representation of the robot's state space, allowing arbitrarily accurate representations as the number of samples increases to infinity. In practice, however, solution trajectories only rely on a few critical states, often defined by structure in the state space (e.g., doorways). In this work we propose a general method to identify these critical states via graph-theoretic techniques (betweenness centrality) and learn to predict criticality from only local environment features. These states are then leveraged more heavily via global connections within a hierarchical graph, termed Critical Probabilistic Roadmaps. Critical PRMs are demonstrated to achieve up to three orders of magnitude improvement over uniform sampling, while preserving the guarantees and complexity of sampling-based motion planning. A video is available at https://youtu.be/AYoD-pGd9ms.


Title: Learning Heuristic A*: Efficient Graph Search using Neural Network
Key Words: graph theory  learning (artificial intelligence)  neural nets  optimisation  path planning  search problems  path planning problem  computation load  neural network  optimal paths  optimal cost  global optimality  admissible heuristic function  efficient graph search  learning heuristic A*  LHA*  suboptimality bound  maze-like map  Heuristic algorithms  Training  Path planning  Biological neural networks  Robots  Navigation 
Abstract: In this paper, we consider the path planning problem on a graph. To reduce computation load by efficiently exploring the graph, we model the heuristic function as a neural network, which is trained by a training set derived from optimal paths to estimate the optimal cost between a pair of vertices on the graph. As such heuristic function cannot be proved to be an admissible heuristic to guarantee the global optimality of the path, we adapt an admissible heuristic function for the terminating criteria. Thus, proposed Learning Heuristic A* (LHA*) guarantees the bounded suboptimality of the path. The performance of LHA* was demonstrated by simulations in a maze-like map and compared with the performance of weighted A* with the same suboptimality bound.


Title: 3D-CNN Based Heuristic Guided Task-Space Planner for Faster Motion Planning
Key Words: collision avoidance  convolutional neural nets  learning (artificial intelligence)  manipulators  mobile robots  sampling methods  stereo image processing  trees (mathematics)  TS-RRT  task-space rapidly-exploring random trees  3D-CNN  heuristic guided task-space planner  fully convolutional neural networks  heuristic map  Random Trees  sampling-based planner  collision-free path  robotic manipulation  motion planning  Planning  Task analysis  Robots  Collision avoidance  Heuristic algorithms  Feature extraction  Convolution 
Abstract: Motion planning is important in a wide variety of applications such as robotic manipulation. However, it is still challenging to reliably find a collision-free path within a reasonable time. To address the issue, this paper proposes a novel framework which combines a sampling-based planner and deep learning for faster motion planning, focusing on heuristics. The proposed method extends Task-Space Rapidly-exploring Random Trees (TS-RRT) to guide the trees with a "heuristic map" where every voxel has a cost-to-go value toward the goal. It also utilizes fully convolutional neural networks (CNNs) for producing more appropriate heuristic maps, rather than manually-designed heuristics. To verify the effectiveness of the proposed method, experiments for motion planning using a real environment and mobile manipulator are carried out. The results indicate that it outperforms the existing planners, especially in terms of the average planning time with smaller variance.


Title: Learned Sampling Distributions for Efficient Planning in Hybrid Geometric and Object-Level Representations
Key Words: geometry  image representation  learning (artificial intelligence)  mobile robots  multi-agent systems  path planning  robot vision  linear regression  efficiency planning  sampling distribution learning  sampling-based planners  object-level semantics  myopic behavior  geometric information  navigation  robotic agent  object-level representations  hybrid geometric  Navigation  Planning  Semantics  Trajectory  Mathematical model  Optimization  Robots 
Abstract: We would like to enable a robotic agent to quickly and intelligently find promising trajectories through structured, unknown environments. Many approaches to navigation in unknown environments are limited to considering geometric information only, which leads to myopic behavior. In this work, we show that learning a sampling distribution that incorporates both geometric information and explicit, object-level semantics for sampling-based planners enables efficient planning at longer horizons in partially-known environments. We demonstrate that our learned planner is up to 2.7 times more likely to find a plan than the baseline, and can result in up to a 16% reduction in traversal costs as calculated by linear regression. We also show promising qualitative results on real-world data.


Title: Deep Visual Heuristics: Learning Feasibility of Mixed-Integer Programs for Manipulation Planning
Key Words: integer programming  learning (artificial intelligence)  manipulators  neurocontrollers  path planning  robot vision  deep visual heuristics  mixed-integer program  deep neural network  visual input  robot manipulation planning  motion planning  learning algorithm  goal encoding  optimization problems  Planning  Task analysis  Robot sensing systems  Neural networks  Grasping  Search problems 
Abstract: In this paper, we propose a deep neural network that predicts the feasibility of a mixed-integer program from visual input for robot manipulation planning. Integrating learning into task and motion planning is challenging, since it is unclear how the scene and goals can be encoded as input to the learning algorithm in a way that enables to generalize over a variety of tasks in environments with changing numbers of objects and goals. To achieve this, we propose to encode the scene and the target object directly in the image space.Our experiments show that our proposed network generalizes to scenes with multiple objects, although during training only two objects are present at the same time. By using the learned network as a heuristic to guide the search over the discrete variables of the mixed-integer program, the number of optimization problems that have to be solved to find a feasible solution or to detect infeasibility can greatly be reduced.


Title: Fast Frontier-based Information-driven Autonomous Exploration with an MAV
Key Words: autonomous aerial vehicles  collision avoidance  entropy  microrobots  mobile robots  navigation  octrees  probability  robot vision  MAV  collision-free navigation  autonomous robots  microaerial vehicles  map entropy  occupancy probabilities  utility function  frontier extraction  frontier-based exploration  frontier voxels  map frontiers  frontier-based information-driven autonomous exploration  exploration planner  octree map representation  visual-based navigation  motion planning  octree-based occupancy mapping  sampling-based exploration  Planning  Octrees  Entropy  Robot sensing systems  Measurement  Task analysis  Aerial Systems: Perception and Autonomy  Visual-Based Navigation 
Abstract: Exploration and collision-free navigation through an unknown environment is a fundamental task for autonomous robots. In this paper, a novel exploration strategy for Micro Aerial Vehicles (MAVs) is presented. The goal of the exploration strategy is the reduction of map entropy regarding occupancy probabilities, which is reflected in a utility function to be maximised. We achieve fast and efficient exploration performance with tight integration between our octree-based occupancy mapping approach, frontier extraction, and motion planning-as a hybrid between frontier-based and sampling-based exploration methods. The computationally expensive frontier clustering employed in classic frontier-based exploration is avoided by exploiting the implicit grouping of frontier voxels in the underlying octree map representation. Candidate next-views are sampled from the map frontiers and are evaluated using a utility function combining map entropy and travel time, where the former is computed efficiently using sparse raycasting. These optimisations along with the targeted exploration of frontier-based methods result in a fast and computationally efficient exploration planner. The proposed method is evaluated using both simulated and real-world experiments, demonstrating clear advantages over state-of-the-art approaches.


Title: Dynamic Landing of an Autonomous Quadrotor on a Moving Platform in Turbulent Wind Conditions
Key Words: aircraft landing guidance  autonomous aerial vehicles  Global Positioning System  helicopters  Kalman filters  mobile robots  nonlinear filters  robot dynamics  robot vision  robust control  turbulence  variable structure systems  vehicle dynamics  dynamic landing  autonomous quadrotor  moving platform  turbulent wind conditions  autonomous landing  fast trajectory planning  wind disturbance  fully autonomous vision-based system  quadrotor-platform distance  landing trajectory  receding horizon control  boundary layer sliding controller  extended Kalman filter  GPS measurements  robust control  precise control  Trajectory  Cameras  Vehicle dynamics  Planning  Global Positioning System  Visualization  Acceleration  Unmanned aerial vehicles  autonomous vehicles  landing on a moving platform  disturbance compensation 
Abstract: Autonomous landing on a moving platform presents unique challenges for multirotor vehicles, including the need to accurately localize the platform, fast trajectory planning, and precise/robust control. Previous works studied this problem but most lack explicit consideration of the wind disturbance, which typically leads to slow descents onto the platform. This work presents a fully autonomous vision-based system that addresses these limitations by tightly coupling the localization, planning, and control, thereby enabling fast and accurate landing on a moving platform. The platform's position, orientation, and velocity are estimated by an extended Kalman filter using simulated GPS measurements when the quadrotor-platform distance is large, and by a visual fiducial system when the platform is nearby. The landing trajectory is computed online using receding horizon control and is followed by a boundary layer sliding controller that provides tracking performance guarantees in the presence of unknown, but bounded, disturbances. To improve the performance, the characteristics of the turbulent conditions are accounted for in the controller. The landing trajectory is fast, direct, and does not require hovering over the platform, as is typical of most stateof-the-art approaches. Simulations and hardware experiments are presented to validate the robustness of the approach.


Title: Direct NMPC for Post-Stall Motion Planning with Fixed-Wing UAVs
Key Words: aerodynamics  aerospace components  autonomous aerial vehicles  feedback  mobile robots  motion control  nonlinear control systems  path planning  position control  predictive control  vehicle dynamics  rotary-wing UAVs  nonlinear control approach  fixed-wing UAVs  full-state direct trajectory optimization  representative aircraft model  nonlinear aircraft model  fixed-wing trajectories  randomized motion planning  local-linear feedback  direct NMPC  post-stall motion planning  fixed-wing unmanned aerial vehicles  frequency 5.0 Hz  Aerodynamics  Trajectory  Atmospheric modeling  Real-time systems  Computational modeling  Planning  Splines (mathematics) 
Abstract: Fixed-wing unmanned aerial vehicles (UAVs) offer significant performance advantages over rotary-wing UAVs in terms of speed, endurance, and efficiency. However, these vehicles have traditionally been severely limited with regards to maneuverability. In this paper, we present a nonlinear control approach for enabling aerobatic fixed-wing UAVs to maneuver in constrained spaces. Our approach utilizes full-state direct trajectory optimization and a minimalistic, but representative, nonlinear aircraft model to plan aggressive fixed-wing trajectories in real-time at 5 Hz across high angles-of-attack. Randomized motion planning is used to avoid local minima and local-linear feedback is used to compensate for model inaccuracies between updates. We demonstrate our method in hardware and show that both local-linear feedback and re-planning are necessary for successful navigation of a complex environment in the presence of model uncertainty.


Title: A Volumetric Albedo Framework for 3D Imaging Sonar Reconstruction
Key Words: autonomous underwater vehicles  image reconstruction  optimisation  robot vision  solid modelling  sonar  sonar imaging  stereo image processing  3D imaging sonar reconstruction  object-level 3D underwater reconstruction  imaging sonar sensors  nonline-of-sight reconstruction  convex linear optimization problem  alternating direction method of multipliers  ADMM  sonar elevation apertures  autonomous underwater vehicles  volumetric Albedo framework  Sonar  Image reconstruction  Imaging  Three-dimensional displays  Sensors  Nonlinear optics  Surface reconstruction 
Abstract: We present a novel framework for object-level 3D underwater reconstruction using imaging sonar sensors. We demonstrate that imaging sonar reconstruction is analogous to the problem of confocal non-line-of-sight (NLOS) reconstruction. Drawing upon this connection, we formulate the problem as one of solving for volumetric albedo, where the scene of interest is modeled as a directionless albedo field. After discretization, reconstruction reduces to a convex linear optimization problem, which we can augment with a variety of priors and regularization terms. We show how to solve the resulting regularized problems using the alternating direction method of multipliers (ADMM) algorithm. We demonstrate the effectiveness of the proposed approach in simulation and on real-world datasets collected in a controlled, test tank environment with several different sonar elevation apertures.


Title: Map Management Approach for SLAM in Large-Scale Indoor and Outdoor Areas
Key Words: image registration  iterative methods  mobile robots  navigation  robot vision  SLAM (robots)  link-points  multiple indoor areas  outdoor areas  map quality  single map approaches  semantic map management approach  multiple maps  modular map structure  utilized SLAM method  laser scan data  appropriate SLAM configuration  single independent maps  appearance-based method  iterative closest point registration  point clouds  simultaneous localization and mapping configurations  Simultaneous localization and mapping  Lasers  Navigation  Feature extraction  Three-dimensional displays 
Abstract: This work presents a semantic map management approach for various environments by triggering multiple maps with different simultaneous localization and mapping (SLAM) configurations. A modular map structure allows to add, modify or delete maps without influencing other maps of different areas. The hierarchy level of our algorithm is above the utilized SLAM method. Evaluating laser scan data (e.g. the detection of passing a doorway) triggers a new map, automatically choosing the appropriate SLAM configuration from a manually predefined list. Single independent maps are connected by link-points, which are located in an overlapping zone of both maps, enabling global navigation over several maps. Loop- closures between maps are detected by an appearance-based method, using feature matching and iterative closest point (ICP) registration between point clouds. Based on the arrangement of maps and link-points, a topological graph is extracted for navigation purpose and tracking the global robot's position over several maps. Our approach is evaluated by mapping a university campus with multiple indoor and outdoor areas and abstracting a metrical-topological graph. It is compared to a single map running with different SLAM configurations. Our approach enhances the overall map quality compared to the single map approaches by automatically choosing predefined SLAM configurations for different environmental setups.


Title: A Hierarchical Framework for Collaborative Probabilistic Semantic Mapping
Key Words: Bayes methods  expectation-maximisation algorithm  geometry  mobile robots  multi-robot systems  robot vision  single robot semantic mapping  collaborative geometry mapping  semantic point cloud  heterogeneous sensor fusion model  collaborative robots level  3D semantic map fusion algorithm  hierarchical collaborative probabilistic semantic mapping framework  Bayesian rule  probability  expectation-maximization  mathematical modeling  Semantics  Collaboration  Three-dimensional displays  Robot sensing systems  Geometry  Robot kinematics 
Abstract: Performing collaborative semantic mapping is a critical challenge for cooperative robots to maintain a comprehensive contextual understanding of the surroundings. Most of the existing work either focus on single robot semantic mapping or collaborative geometry mapping. In this paper, a novel hierarchical collaborative probabilistic semantic mapping framework is proposed, where the problem is formulated in a distributed setting. The key novelty of this work is the mathematical modeling of the overall collaborative semantic mapping problem and the derivation of its probability decomposition. In the single robot level, the semantic point cloud is obtained based on heterogeneous sensor fusion model and is used to generate local semantic maps. Since the voxel correspondence is unknown in collaborative robots level, an Expectation-Maximization approach is proposed to estimate the hidden data association, where Bayesian rule is applied to perform semantic and occupancy probability update. The experimental results show the high quality global semantic map, demonstrating the accuracy and utility of 3D semantic map fusion algorithm in real missions.


Title: Autonomous Navigation in Unknown Environments using Sparse Kernel-based Occupancy Mapping
Key Words: collision avoidance  image classification  learning (artificial intelligence)  mobile robots  path planning  robot vision  trajectory control  decision boundary  kernel perceptron classifier  online training algorithm  piecewise-polynomial robot trajectories  autonomous navigation  Ackermann-drive robot  sparse kernel-based occupancy  real-time occupancy mapping  autonomous robot  map representation  piecewise-polynomial robot trajectory  piecewise-linear robot trajectory  collision checking algorithm  Support vector machines  Kernel  Training  Robot sensing systems  Collision avoidance  Trajectory 
Abstract: This paper focuses on real-time occupancy mapping and collision checking onboard an autonomous robot navigating in an unknown environment. We propose a new map representation, in which occupied and free space are separated by the decision boundary of a kernel perceptron classifier. We develop an online training algorithm that maintains a very sparse set of support vectors to represent obstacle boundaries in configuration space. We also derive conditions that allow complete (without sampling) collision-checking for piecewise-linear and piecewise-polynomial robot trajectories. We demonstrate the effectiveness of our mapping and collision checking algorithms for autonomous navigation of an Ackermann-drive robot in unknown environments.


Title: Hybrid Topological and 3D Dense Mapping through Autonomous Exploration for Large Indoor Environments
Key Words: image representation  indoor navigation  mobile robots  path planning  robot vision  SLAM (robots)  stereo image processing  topology  indoor environments  topological global representations  3D dense submaps  hybrid global map  autonomous exploration  autonomous navigation  path planning  dense 3D maps  3D dense representations  3D dense mapping systems  hybrid topological mapping  metric 3D maps  standard CPU  Three-dimensional displays  Measurement  Robots  Semantics  Two dimensional displays  Indoor environments  Path planning 
Abstract: Robots require a detailed understanding of the 3D structure of the environment for autonomous navigation and path planning. A popular approach is to represent the environment using metric, dense 3D maps such as 3D occupancy grids. However, in large environments the computational power required for most state-of-the-art 3D dense mapping systems is compromising precision and real-time capability. In this work, we propose a novel mapping method that is able to build and maintain 3D dense representations for large indoor environments using standard CPUs. Topological global representations and 3D dense submaps are maintained as hybrid global map. Submaps are generated for every new visited place. A place (room) is identified as an isolated part of the environment connected to other parts through transit areas (doors). This semantic partitioning of the environment allows for a more efficient mapping and path-planning. We also propose a method for autonomous exploration that directly builds the hybrid representation in real time.We validate the real-time performance of our hybrid system on simulated and real environments regarding mapping and path-planning. The improvement in execution time and memory requirements upholds the contribution of the proposed work.


Title: Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints*
Key Words: computer vision  object detection  optimisation  path planning  pose estimation  robot vision  SLAM (robots)  lifted algorithm  combinatorial complexity  heuristic criterion  planar pose estimation  marker-based mapping  highly ambiguous inputs  PPE ambiguities  possible marker orientation solutions  rotation averaging formulation  marker corners  computer vision  planar markers  clique constraints  robust rotation averaging  marker pose ambiguity  Cameras  Pipelines  Machine-to-machine communications  Image edge detection  Pose estimation  Simultaneous localization and mapping  Histograms 
Abstract: Planar markers are useful in robotics and computer vision for mapping and localisation. Given a detected marker in an image, a frequent task is to estimate the 6DOF pose of the marker relative to the camera, which is an instance of planar pose estimation (PPE). Although there are mature techniques, PPE suffers from a fundamental ambiguity problem, in that there can be more than one plausible pose solutions for a PPE instance. Especially when localisation of the marker corners is noisy, it is often difficult to disambiguate the pose solutions based on reprojection error alone. Previous methods choose between the possible solutions using a heuristic criterion, or simply ignore ambiguous markers.We propose to resolve the ambiguities by examining the consistencies of a set of markers across multiple views. Our specific contributions include a novel rotation averaging formulation that incorporates long-range dependencies between possible marker orientation solutions that arise from PPE ambiguities. We analyse the combinatorial complexity of the problem, and develop a novel lifted algorithm to effectively resolve marker pose ambiguities, without discarding any marker observations. Results on real and synthetic data show that our method is able to handle highly ambiguous inputs, and provides more accurate and/or complete marker-based mapping and localisation.


Title: Anticipating the Start of User Interaction for Service Robot in the Wild
Key Words: cameras  face recognition  feature extraction  image colour analysis  image motion analysis  image sensors  image sequences  learning (artificial intelligence)  pose estimation  service robots  user interaction  service robot  proactive service  potential visitors  visitor  early anticipation  human pose information  publicly available JPL interaction dataset  accurate anticipation performance  CNN-LSTM-based model  human verification 
Abstract: A service robot is expected to provide proactive service for visitors who require its help. In contrast to passive service, e.g., providing service only after being spoken to, proactive service initiates an interaction at an early stage, e.g., talking to potential visitors who need the robot's help in advance. This paper addresses how to anticipate the start of user interaction. We propose an approach using only a single RGB camera that anticipates whether a visitor will come to the robot for interaction or just pass it by. In the proposed approach, we (i) utilize the visitor's pose information from captured images incorporating facial information, (ii) train a CNN-LSTM-based model in an end-to-end manner with an exponential loss for early anticipation, and (iii) during the training, the network branch for facial keypoints acquired as the part of the human pose information is taught to mimic the branch trained with the face image from a specialized face detector with a human verification. By virtue of (iii), at the inference, we can run our model in an embedded system processing only the pose information without an additional face detector and typical accuracy drop. We evaluated the proposed approach on our collected real world data with a real service robot and publicly available JPL interaction dataset and found that it achieved accurate anticipation performance.


Title: Spin Detection in Robotic Table Tennis*
Key Words: learning (artificial intelligence)  mobile robots  object detection  sport  spin detection  robotic table tennis  rotation  table tennis match  human player  return stroke  high-speed camera  frame rate  circular brand logo  background difference  spin types  table tennis rally  frequency 380.0 Hz  Three-dimensional displays  Cameras  Robot vision systems  Trajectory  Quaternions 
Abstract: In table tennis, the rotation (spin) of the ball plays a crucial role. A table tennis match will feature a variety of strokes. Each generates different amounts and types of spin. To develop a robot that can compete with a human player, the robot needs to detect spin, so it can plan an appropriate return stroke. In this paper we compare three methods to estimate spin. The first two approaches use a high-speed camera that captures the ball in flight at a frame rate of 380 Hz. This camera allows the movement of the circular brand logo printed on the ball to be seen. The first approach uses background difference to determine the position of the logo. In a second alternative, we train a CNN to predict the orientation of the logo. The third method evaluates the trajectory of the ball and derives the rotation from the effect of the Magnus force. This method gives the highest accuracy and is used for a demonstration. Our robot successfully copes with different spin types in a real table tennis rally against a human opponent.


Title: Look, Listen, and Act: Towards Audio-Visual Embodied Navigation
Key Words: acoustic signal processing  audio signal processing  audio-visual systems  human computer interaction  mobile agents  navigation  path planning  audio-visual embodied navigation  mobile intelligent agents  multiple sensory inputs  sound source  indoor environment  raw egocentric visual data  audio sensory data  audio signal  visual environment  visual pieces  audio pieces  visual perception mapper module  sound perception module  audio-visual observations  simulated multimodal environment  visual-audio-room dataset  Navigation  Visualization  Task analysis  Robot sensing systems  Visual perception  Acoustics  Feature extraction 
Abstract: A crucial ability of mobile intelligent agents is to integrate the evidence from multiple sensory inputs in an environment and to make a sequence of actions to reach their goals. In this paper, we attempt to approach the problem of Audio-Visual Embodied Navigation, the task of planning the shortest path from a random starting location in a scene to the sound source in an indoor environment, given only raw egocentric visual and audio sensory data. To accomplish this task, the agent is required to learn from various modalities, i.e., relating the audio signal to the visual environment. Here we describe an approach to audio-visual embodied navigation that takes advantage of both visual and audio pieces of evidence. Our solution is based on three key ideas: a visual perception mapper module that constructs its spatial memory of the environment, a sound perception module that infers the relative location of the sound source from the agent, and a dynamic path planner that plans a sequence of actions based on the audio-visual observations and the spatial memory of the environment to navigate toward the goal. Experimental results on a newly collected Visual-Audio-Room dataset using the simulated multi-modal environment demonstrate the effectiveness of our approach over several competitive baselines.


Title: Autonomous Tool Construction with Gated Graph Neural Network
Key Words: convolutional neural nets  graph theory  learning (artificial intelligence)  robot vision  tools  gated graph neural network  autonomous tool construction  reference tool  robotics  GGNN  RCNN-like structure  TC-GRCNN  large-scale training data generation  large-scale testing data generation  tool construction graph RCNN  region based convolutional neural network  candidate part pairs  Tools  Data models  Robots  Task analysis  Training  Solid modeling  Neural networks 
Abstract: Autonomous tool construction is a significant but challenging task in robotics. This task can be interpreted as when given a reference tool, selecting some available candidate parts to reconstruct it. Most of the existing works perform tool construction in the form of action part and grasp part, which is only a specific construction pattern and limits its application to some extent. In general scenarios, a tool can be constructed in various patterns with different part pairs. Therefore, whether a part pair is most suitable for constructing the tool depends not only on itself, but on other parts in the same scene. To solve this problem, we construct a Gated Graph Neural Network (GGNN) to model the relations between all part pairs, so that we can select the candidate parts in consideration of the global information. Afterwards, we embed the constructed GGNN into a RCNN-like structure to finally accomplish tool construction. The whole model will be named Tool Construction Graph RCNN (TC-GRCNN). In addition, we develop a mechanism that can generate large-scale training and testing data in simulation environments, by which we can save the time of data collection and annotation. Finally, the proposed model is deployed on the physical robot. The experiment results show that TC-GRCNN can perform well in the general scenarios of tool construction.


Title: Training-Set Distillation for Real-Time UAV Object Tracking
Key Words: autonomous aerial vehicles  correlation methods  image filtering  image motion analysis  minimisation  mobile robots  object detection  object tracking  robot vision  training-set distillation  UAV object tracking  correlation filter  visual object tracking  unmanned aerial vehicle  energy minimization function  scoring process  time slot-based distillation approach  Training  Unmanned aerial vehicles  Correlation  Reliability  Real-time systems  Optimization  Visualization 
Abstract: Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set's quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accelerate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on single CPU.


Title: CNN-Based Simultaneous Dehazing and Depth Estimation
Key Words: computer vision  convolutional neural nets  correlation methods  image coding  image colour analysis  image denoising  image representation  image sensors  learning (artificial intelligence)  spatial variables measurement  single hazy RGB input  single dense encoder  encoded image representation  dehazing image depth estimation  single image depth estimation  image dehazing  computer vision  convolutional neural networks  CNN  dehazing depth estimation algorithms  traditional haze modeling  depth estimation network  fully scaled depth map  depth-transmission consistency loss  separate decoders  Decoding  Propagation losses  Estimation  Training  Image reconstruction  Task analysis  Scattering 
Abstract: It is difficult for both cameras and depth sensors to obtain reliable information in hazy scenes. Therefore, image dehazing is still one of the most challenging problems to solve in computer vision and robotics. With the development of convolutional neural networks (CNNs), lots of dehazing and depth estimation algorithms using CNNs have emerged. However, very few of those try to solve these two problems at the same time. Focusing on the fact that traditional haze modeling contains depth information in its formula, we propose a CNN-based simultaneous dehazing and depth estimation network. Our network aims to estimate both a dehazed image and a fully scaled depth map from a single hazy RGB input with end-to-end training. The network contains a single dense encoder and four separate decoders; each of them shares the encoded image representation while performing individual tasks. We suggest a novel depth-transmission consistency loss in the training scheme to fully utilize the correlation between the depth information and transmission map. To demonstrate the robustness and effectiveness of our algorithm, we performed various ablation studies and compared our results to those of state-of-the-art algorithms in dehazing and single image depth estimation, both qualitatively and quantitatively. Furthermore, we show the generality of our network by applying it to some real-world examples.


Title: Internet of Things (IoT)-based Collaborative Control of a Redundant Manipulator for Teleoperated Minimally Invasive Surgeries
Key Words: control engineering computing  end effectors  human-robot interaction  Internet of Things  medical robotics  motion control  phantoms  redundant manipulators  surgery  telerobotics  Things-based collaborative control  teleoperated Minimally Invasive surgeries  Robot-assisted Minimally Invasive Surgery scenario  hierarchical operational space formulation  7-DoFs redundant manipulator  multiple operational tasks  motion constraint  collision avoidance  undergoing surgical operation  Internet of Robotic Things  human-robot interaction  compliant swivel motion  HTC VIVE PRO controllers  robot elbow  smooth swivel motion  KUKA LWR4+ slave robot  SIGMA 7 master manipulator  Internet of Things-based human-robot collaborative control scheme  priority levels  Collision avoidance  Manipulators  Surgery  Task analysis  Force  Tools 
Abstract: In this paper, an Internet of Things-based human-robot collaborative control scheme is developed in Robot-assisted Minimally Invasive Surgery scenario. A hierarchical operational space formulation is designed to exploit the redundancies of the 7-DoFs redundant manipulator to handle multiple operational tasks based on their priority levels, such as guaranteeing a remote center of motion constraint and avoiding collision with a swivel motion without influencing the undergoing surgical operation. Furthermore, the concept of the Internet of Robotic Things is exploited to facilitate the best action of the robot in human-robot interaction. Instead of utilizing compliant swivel motion, HTC VIVE PRO controllers, used as the Internet of Things technology, is adopted to detect the collision. A virtual force is applied to the robot elbow, enabling a smooth swivel motion for human-robot interaction. The effectiveness of the proposed strategy is validated using experiments performed on a patient phantom in a lab setup environment, with a KUKA LWR4+ slave robot and a SIGMA 7 master manipulator. By comparison with previous works, the results show improved performances in terms of the accuracy of the RCM constraint and surgical tip.


Title: Passive Dynamic Balancing and Walking in Actuated Environments
Key Words: gait analysis  legged locomotion  mechanical stability  pendulums  robot dynamics  passive dynamic balancing  passive dynamic systems  dynamic behaviors  actuated robots  robotic assistive devices  robotic systems  passive system  passive bipedal robot  dynamically stable periodic walking gaits  passive dynamic walking  Legged locomotion  Dynamics  Nonlinear dynamical systems  Robot sensing systems  Hardware 
Abstract: The control of passive dynamic systems remains a challenging problem in the field of robotics, and insights from their study can inform everything from dynamic behaviors on actuated robots to robotic assistive devices. In this work, we explore the use of flat actuated environments for realizing passive dynamic balancing and locomotion. Specifically, we utilize a novel omnidirectional actuated floor to dynamically stabilize two robotic systems. We begin with an inverted pendulum to demonstrate the ability to control a passive system through an active environment. We then consider a passive bipedal robot wherein dynamically stable periodic walking gaits are generated through an optimization that leverages the actuated floor. The end result is the ability to demonstrate passive dynamic walking experimentally through the use of actuated environments.


Title: Biped Stabilization by Linear Feedback of the Variable-Height Inverted Pendulum Model
Key Words: feedback  humanoid robots  legged locomotion  nonlinear control systems  pendulums  robot dynamics  stability  stabilization  linear feedback  variable-height  pendulum model  balancing strategy  height variations  well-known ankle strategy  biped stabilizer  input feasibility  state viability constraints  resulting stabilizer  Mathematical model  Lips  Three-dimensional displays  Feedback control  Legged locomotion  Trajectory 
Abstract: The variable-height inverted pendulum (VHIP) model enables a new balancing strategy by height variations of the center of mass, in addition to the well-known ankle strategy. We propose a biped stabilizer based on linear feedback of the VHIP that is simple to implement, coincides with the state-of-the-art for small perturbations and is able to recover from larger perturbations thanks to this new strategy. This solution is based on "best-effort" pole placement of a 4D divergent component of motion for the VHIP under input feasibility and state viability constraints. We complement it with a suitable whole-body admittance control law and test the resulting stabilizer on the HRP-4 humanoid robot.


Title: Stability Criteria of Balanced and Steppable Unbalanced States for Full-Body Systems with Implications in Robotic and Human Gait
Key Words: gait analysis  humanoid robots  legged locomotion  position control  robot dynamics  stability  steppable unbalanced state boundary  full-body systems  double support contact configurations  steppable states  biped system  full-order nonlinear system dynamics  DARwIn-OP humanoid robot  balance stability boundaries  DS contact configuration  human gait  Conferences  Automation 
Abstract: Biped walking involves a series of transitions between single support (SS) and double support (DS) contact configurations that can include both balanced and unbalanced states. The new concept of steppability is introduced to partition the set of unbalanced states into steppable states and falling (unsteppable) states based on the ability of a biped system to respond to forward velocity perturbations by stepping. In this work, a complete system-specific analysis of the stepping process including full-order nonlinear system dynamics is presented for the DARwIn-OP humanoid robot and a human subject in the sagittal plane with respect to both balance stability and steppability. The balance stability and steppability of each system are analyzed by numerical construction of its balance stability boundaries (BSB) for the initial SS and final DS contact configuration and the steppable unbalanced state boundary (SUB). These results are presented with center of mass (COM) trajectories obtained from walking experiments to benchmark robot controller performance and analyze the variation of balance stability and steppability with COM and swing foot position along the progression of a step cycle. For each system, DS BSBs were obtained with both constrained and unconstrained arms in order to demonstrate the ability of this approach to incorporate the effects of angular momentum and system-specific characteristics such as actuation torque, velocity, and angle limits.


Title: Material Handling by Humanoid Robot While Pushing Carts Using a Walking Pattern Based on Capture Point
Key Words: friction  humanoid robots  legged locomotion  materials handling  motion control  robot dynamics  robot self balance  zero moment point pattern  walking pattern dynamic model  arm compliance  friction compensation  capture point method  cart pushing  humanoid robot  material handling  Humanoid robots  Legged locomotion  Friction  Robot sensing systems  Wrist  Force 
Abstract: This paper presents a study that evaluates the effects on the walking pattern of a full-sized humanoid robot as it pushes different carts. Furthermore, it discuss a modified Zero Moment Point (ZMP) pattern based on a capture point method, and a friction compensation method for the arms. Humanoid researchers have demonstrated that robots can perform a wide range of tasks including handling tools, climbing ladders, and patrolling rough terrain. However, when it comes to handling objects while walking, humanoids are relatively limited; it becomes more apparent when humanoids have to push a cart. Many challenges become evident under such circumstances; for example, the walking pattern will be severely affected by the external force opposed by the cart. Therefore, an appropriate walking pattern dynamic model and arm compliance are needed to mitigate external forces. This becomes crucial in order to ensure the robot's self-balance and minimize external disturbances.


Title: Interconnection and Damping Assignment Passivity-Based Control for Gait Generation in Underactuated Compass-Like Robots
Key Words: control system synthesis  damping  gait analysis  legged locomotion  motion control  robot dynamics  robot kinematics  robust control  interconnection and damping assignment passivity-based control  gait generation  compass-like biped robot  dynamic parameter  port-Hamiltonian framework  controller discretization  parametric uncertainties  Legged locomotion  Potential energy  Damping  Transmission line matrix methods  Mathematical model  Robustness 
Abstract: A compass-like biped robot can go down a gentle slope without the need of actuation through a proper choice of its dynamic parameter and starting from a suitable initial condition. Addition of control actions is requested to generate additional gaits and robustify the existing one. This paper designs an interconnection and damping assignment passivity-based control, rooted within the port-Hamiltonian framework, to generate further gaits with respect to state-of-the-art methodologies, enlarge the basin of attraction of existing gaits, and further robustify the system against controller discretization and parametric uncertainties. The performance of the proposed algorithm is validated through numerical simulations and comparison with existing passivity-based techniques.


Title: Multi-Robot Path Deconfliction through Prioritization by Path Prospects
Key Words: mobile robots  multi-robot systems  path planning  path prospects  prioritization rule  heterogeneous robot teams  multirobot path deconfliction  conflict-free path planning  mobile robots  conflict-free path plans  prioritization heuristics  Robot kinematics  Collision avoidance  Planning  Trajectory  Heuristic algorithms  Couplings 
Abstract: This work deals with the problem of planning conflict-free paths for mobile robots in cluttered environments. Since centralized, coupled planning algorithms are computationally intractable for large numbers of robots, we consider decoupled planning, in which robots plan their paths sequentially in order of priority. Choosing how to prioritize the robots is a key consideration. State-of-the-art prioritization heuristics, however, do not model the coupling between a robot's mobility and its environment. This is particularly relevant when prioritizing between robots with different degrees of mobility. In this paper, we propose a prioritization rule that can be computed online by each robot independently, and that provides consistent, conflict-free path plans. Our innovation is to formalize a robot's path prospects to reach its goal from its current location. To this end, we consider the number of homology classes of trajectories, which capture distinct prospects of paths for each robot. This measure is used as a prioritization rule, whenever any robots enter negotiation to deconflict path plans. We perform simulations with heterogeneous robot teams and compare our method to five benchmarks. Our method achieves the highest success rate, and strikes a good balance between makespan and flowtime objectives.


Title: A Connectivity-Prediction Algorithm and its Application in Active Cooperative Localization for Multi-Robot Systems
Key Words: Markov processes  mobile robots  motion control  multi-robot systems  path planning  probability  infinite power series expansion theorem  finite-term approximation  computational feasibility  adverse impacts  higher order series terms  active CL  leader-follower architecture  Markov decision process  one-step planning horizon  optimal motion strategy  MDP model  connectivity-prediction algorithm  multirobot systems  future connectivity  mobile robots  range-limited communication  active motion planning  quadratic forms  random normal variables  Prediction algorithms  Robot sensing systems  Planning  Uncertainty  Computational modeling  Gaussian distribution 
Abstract: This paper presents a method for predicting the probability of future connectivity between mobile robots with range-limited communication. In particular, we focus on its application to active motion planning for cooperative localization (CL). The probability of connection is modeled by the distribution of quadratic forms in random normal variables and is computed by the infinite power series expansion theorem. A finite-term approximation is made to realize the computational feasibility and three more modifications are designed to handle the adverse impacts introduced by the omission of the higher order series terms. On the basis of this algorithm, an active and CL problem with leader-follower architecture is then reformulated into a Markov Decision Process (MDP) with a one-step planning horizon, and the optimal motion strategy is generated by minimizing the expected cost of the MDP. Extensive simulations and comparisons are presented to show the effectiveness and efficiency of both the proposed prediction algorithm and the MDP model.


Title: Behavior Mixing with Minimum Global and Subgroup Connectivity Maintenance for Large-Scale Multi-Robot Systems
Key Words: collision avoidance  distributed algorithms  mobile robots  multi-robot systems  trees (mathematics)  large-scale multirobot systems  robot team  connected communication graph  minimum inter-robot connectivity constraints  activated connectivity constraints  behavior mixing controllers  distributed minimum connectivity constraint spanning tree algorithm  provably minimum connectivity maintenance  subgroup connectivity maintenance  minimum global connectivity maintenance  collision avoidance  distributed MCCST algorithm  Collision avoidance  Robot kinematics  Task analysis  Safety  Multi-robot systems  Real-time systems 
Abstract: In many cases the multi-robot systems are desired to execute simultaneously multiple behaviors with different controllers, and sequences of behaviors in real time, which we call behavior mixing. Behavior mixing is accomplished when different subgroups of the overall robot team change their controllers to collectively achieve given tasks while maintaining connectivity within and across subgroups in one connected communication graph. In this paper, we present a provably minimum connectivity maintenance framework to ensure the subgroups and overall robot team stay connected at all times while providing the highest freedom for behavior mixing. In particular, we propose a real-time distributed Minimum Connectivity Constraint Spanning Tree (MCCST) algorithm to select the minimum inter-robot connectivity constraints preserving subgroup and global connectivity that are least likely to be violated by the original controllers. With the employed safety and connectivity barrier certificates for the activated connectivity constraints and collision avoidance, the behavior mixing controllers are thus minimally modified from the original controllers. We demonstrate the effectiveness and scalability of our approach via simulations of up to 100 robots with multiple behaviors.


Title: Energy-Optimal Cooperative Manipulation via Provable Internal-Force Regulation
Key Words: cooperative systems  decentralised control  force control  manipulator dynamics  mobile robots  multi-robot systems  position control  internal-force regulation  optimal cooperative robotic manipulation problem  energy resources  rigid cooperative manipulation systems  rigid grasping contacts  energy-optimal conditions  arising internal forces  inter-agent forces  closed form expression  standard inverse dynamics  force distribution  robotic agents  nonzero inter-agent internal force vector  internal force minimization  Grasping  Force  Robots  Dynamics  Acceleration  Jacobian matrices  Task analysis 
Abstract: This paper considers the optimal cooperative robotic manipulation problem in terms of energy resources. In particular, we consider rigid cooperative manipulation systems, i.e., with rigid grasping contacts, and study energy-optimal conditions in the sense of minimization of the arising internal forces, which are inter-agent forces that do not contribute to object motion. Firstly, we use recent results to derive a closed form expression for the internal forces. Secondly, by using a standard inverse dynamics control protocol, we provide novel conditions on the force distribution to the robotic agents for provable internal force minimization. Moreover, we derive novel results on the provable achievement of a desired non-zero inter-agent internal force vector. Extensive simulation results in a realistic environment verify the theoretical analysis.


Title: Robot Telekinesis: Application of a Unimanual and Bimanual Object Manipulation Technique to Robot Control
Key Words: end effectors  haptic interfaces  human-robot interaction  industrial manipulators  industrial robots  sensors  robot control  dangerous industrial robots  collaborative robots  teaching pendant  direct teaching  novel robot interaction technique  robot arm  unimanual hand gestures  bimanual hand gestures  robot telekinesis  bimanual object manipulation technique  unimanual object manipulation technique  production lines  Robot sensing systems  End effectors  Task analysis  Education  Service robots  Collaboration 
Abstract: Unlike large and dangerous industrial robots at production lines in factories that are strictly fenced off, collaborative robots are smaller and safer and can be installed adjacent to human workers and collaborate with them. However, controlling and teaching new moves to collaborative robots can be difficult and time-consuming when using existing methods, such as pressing buttons on a teaching pendant and physically grabbing and moving the robot via direct teaching. We present Robot Telekinesis, a novel robot interaction technique that lets the user remotely control the movement of the end effector of a robot arm with unimanual and bimanual hand gestures that closely resemble handling a physical object. Through formal evaluation, we show that using a teaching pendant is slow and confusing and that direct teaching is fast and intuitive but physically demanding. Robot Telekinesis is as fast and intuitive as direct teaching without the need for physical contact or physical effort.


Title: A Set-Theoretic Approach to Multi-Task Execution and Prioritization
Key Words: optimisation  redundant manipulators  safety-critical software  set theory  task analysis  time-varying systems  constrained optimization problem  redundant robotic manipulator  set theoretic approach  multitask execution  safety critical tasks  robotic system  optimization based task execution  set based tasks  time varying priorities  multitask prioritization  control barrier functions  Task analysis  Robot kinematics  Jacobian matrices  Aerospace electronics  Manipulators  Safety 
Abstract: Executing multiple tasks concurrently is important in many robotic applications. Moreover, the prioritization of tasks is essential in applications where safety-critical tasks need to precede application-related objectives, in order to protect both the robot from its surroundings and vice versa. Furthermore, the possibility of switching the priority of tasks during their execution gives the robotic system the flexibility of changing its objectives over time. In this paper, we present an optimization-based task execution and prioritization framework that lends itself to the case of time-varying priorities as well as variable number of tasks. We introduce the concept of extended set-based tasks, encode them using control barrier functions, and execute them by means of a constrained-optimization problem, which can be efficiently solved in an online fashion. Finally, we show the application of the proposed approach to the case of a redundant robotic manipulator.


Title: Variable Impedance Control in Cartesian Latent Space while Avoiding Obstacles in Null Space
Key Words: collision avoidance  feedback  human-robot interaction  learning (artificial intelligence)  manipulator dynamics  mobile robots  motion control  variable impedance control  cartesian latent space  null space  human-robot interaction  assistive robots  Cartesian impedance control  joint control  desired interaction  environmental feedback  robot arm  operational space  variable stiffness  precision requirements  dimensionality reduction  freedom relevant  redundant ones  task-redundant DoF  human head  Aerospace electronics  Task analysis  Trajectory  Jacobian matrices  Manipulators  Service robots 
Abstract: Human-robot interaction is one of the keys of assistive robots. Robots are expected to be compliant with people but at the same time correctly perform the tasks. In such applications, Cartesian impedance control is preferred over joint control, as the desired interaction and environmental feedback can be described more naturally, and the force to be exerted by the robot can be readily adjusted.This paper addresses the problem of controlling a robot arm in the operational space with variable stiffness so as to continuously adapt the force exerted in each phase of motion according to the precision requirements. Moreover, performing dimensionality reduction we can separate the degrees of freedom (DoF) relevant for the task from the redundant ones. The stiffness of the former can be adjusted constantly to achieve the required accuracy, while task-redundant DoF can be used to achieve other goals such as avoiding obstacles by moving in the directions where accuracy is not critical. The designed method is tested teaching the robot to give water to drink to a model of human head. Our empirical results demonstrate that the robot can learn precision requirements from demonstration. Furthermore, dimensionality reduction is proved to be useful to avoid obstacles.


Title: MagicHand: Context-Aware Dexterous Grasping Using an Anthropomorphic Robotic Hand
Key Words: dexterous manipulators  infrared spectra  object detection  robot vision  target object  grasping poses  grasp strategies  MagicHand system  context-aware dexterous grasping  robotic grasping  context-aware anthropomorphic robotic hand grasping system  NIR spectra  molecular level  RGB-D images  Grasping  Three-dimensional displays  Neurons  Cameras  Robot vision systems  Dexterous Grasping  Characteristics of Objects Recognition  NIR Spectrum  RGB-D Images 
Abstract: Understanding of characteristics of objects such as fragility, rigidity, texture and dimensions facilitates and innovates robotic grasping. In this paper, we propose a context- aware anthropomorphic robotic hand (MagicHand) grasping system which is able to gather various information about its target object and generate grasping strategies based on the perceived information. In this work, NIR spectra of target objects are perceived to recognize materials on a molecular level and RGB-D images are collected to estimate dimensions of the objects. We selected six most used grasping poses and our system is able to decide the most suitable grasp strategies based on the characteristics of an object. Through multiple experiments, the performance of the MagicHand system is demonstrated.


Title: Learning Pregrasp Manipulation of Objects from Ungraspable Poses
Key Words: feedback  humanoid robots  learning (artificial intelligence)  manipulators  mobile robots  neural nets  robot vision  ungraspable poses  robotic grasping  model-free deep reinforcement learning  feedback control policies  visual information  robot arm  object-table clearance  pregrasp manipulation learning  human bimanual manipulation  Grasping  Robustness  Training  Grippers  Sensors  End effectors 
Abstract: In robotic grasping, objects are often occluded in ungraspable configurations such that no feasible grasp pose can be found, e.g. large flat boxes on the table that can only be grasped once lifted. Inspired by human bimanual manipulation, e.g. one hand to lift up things and the other to grasp, we address this type of problems by introducing pregrasp manipulation - push and lift actions. We propose a model-free Deep Reinforcement Learning framework to train feedback control policies that utilize visual information and proprioceptive states of the robot to autonomously discover robust pregrasp manipulation. The robot arm learns to push the object first towards a support surface and then lift up one side of the object, creating an object-table clearance for possible grasping solutions. Furthermore, we show the robustness of the proposed learning framework in training pregrasp policies that can be directly transferred to a real robot. Lastly, we evaluate the effectiveness and generalization ability of the learned policy in real-world experiments, and demonstrate pregrasp manipulation of objects with various sizes, shapes, weights, and surface friction.


Title: Picking Thin Objects by Tilt-and-Pivot Manipulation and Its Application to Bin Picking
Key Words: dexterous manipulators  end effectors  grippers  industrial manipulators  manipulator kinematics  robot vision  convex polygonal objects  bin picking scenarios  manipulation process  robotic dexterous in-hand manipulation  tilt-and-pivot manipulation  thin object picking  tilt-and-pivot kinematics  tilt-and-pivot planning  Grippers  Shape  Solids  Surface treatment  Hardware  Robot sensing systems 
Abstract: This paper introduces the technique of tilt-and-pivot manipulation, a new method for picking thin, rigid objects lying on a flat surface through robotic dexterous in-hand manipulation. During the manipulation process, the gripper is controlled to reorient about the contact with the object such that its finger can get in the space between the object and the supporting surface, which is formed by tilting up the object, with no relative sliding motion at the contact. As a result, a pinch grasp can be obtained on the faces of the thin object with ease. We discuss issues regarding the kinematics and planning of tilt-and-pivot, effector shape design, and the overall practicality of the manipulation technique, which is general enough to be applicable to any rigid convex polygonal objects. We also present a set of experiments in a range of bin picking scenarios.


Title: Attention-Guided Lightweight Network for Real-Time Segmentation of Robotic Surgical Instruments
Key Words: convolutional neural nets  edge detection  image capture  image coding  image segmentation  learning (artificial intelligence)  medical image processing  medical robotics  robot vision  surgery  robotic surgical instruments  robot-assisted surgery  deep learning models  attention-guided lightweight network  LWANet  lightweight network MobileNetV2  depthwise separable convolution  transposed convolution  surgical instrument  encoder-decoder architecture  attention fusion block  Instruments  Convolution  Semantics  Computational efficiency  Decoding  Surgery  Real-time systems 
Abstract: The real-time segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, it is still a challenging task to implement deep learning models to do real-time segmentation for surgical instruments due to their high computational costs and slow inference speed. In this paper, we propose an attention-guided lightweight network (LWANet), which can segment surgical instruments in real-time. LWANet adopts encoder-decoder architecture, where the encoder is the lightweight network MobileNetV2, and the decoder consists of depthwise separable convolution, attention fusion block, and transposed convolution. Depthwise separable convolution is used as the basic unit to construct the decoder, which can reduce the model size and computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to emphasize target regions, contributing to locating the surgical instrument. Transposed convolution is performed to upsample feature maps for acquiring refined edges. LWANet can segment surgical instruments in real-time while takes little computational costs. Based on 960x544 inputs, its inference speed can reach 39 fps with only 3.39 GFLOPs. Also, it has a small model size and the number of parameters is only 2.06 M. The proposed network is evaluated on two datasets. It achieves state-of-the- art performance 94.10% mean IOU on Cata7 and obtains a new record on EndoVis 2017 with a 4.10% increase on mean IOU.


Title: Automated robotic breast ultrasound acquisition using ultrasound feedback
Key Words: biological tissues  biomedical ultrasonics  feedback  image registration  medical image processing  medical robotics  phantoms  surgery  visual servoing  robotic 3D breast US acquisitions  US feedback  visual servoing algorithm  patient specific scans  tissue deformations  US probe  ultrasound feedback  automated robotic breast ultrasound acquisition  Breast  Probes  Trajectory  Safety  Robot kinematics  Skin 
Abstract: Current challenges in automated robotic breast ultrasound (US) acquisitions include keeping acoustic coupling between the breast and the US probe, minimizing tissue deformations and safety. In this paper, we present how an autonomous 3D breast US acquisition can be performed utilizing a 7DOF robot equipped with a linear US transducer. Robotic 3D breast US acquisitions would increase the diagnostic value of the modality since they allow patient specific scans and have a high reproducibility, accuracy and efficiency. Additionally, 3D US acquisitions allow more flexibility in examining the breast and simplify registration with preoperative images like MRI. To overcome the current challenges, the robot follows a reference- based trajectory adjusted by a visual servoing algorithm. The reference trajectory is a patient specific trajectory coming from e.g. an MRI. The visual servoing algorithm commands in-plane rotations and corrects the probe contact based on confidence maps. A safety aware, intrinsically passive framework is utilised to actuate the robot. The approach is illustrated with experiments on a phantom, which show that the robot only needs minor pre-procedural information to consistently image the phantom while relying mainly on US feedback.


Title: Robust and Accurate 3D Curve to Surface Registration with Tangent and Normal Vectors
Key Words: biomechanics  image reconstruction  image registration  medical image processing  surgery  image-guided surgery  pre-to-intraoperative registration  intra-operative 3D data  tangent vectors  sparse intraoperative data points  pre-operative model points  probabilistic distribution  multidimensional point  maximum likelihood problem  rigid registration  intraoperative point  mean target registration error value  size 0.6795 mm  Three-dimensional displays  Surgery  Robustness  Probes  Probabilistic logic  Robots  Biomedical imaging 
Abstract: This paper presents a robust and accurate approach for the rigid registration of pre-operative and intraoperative point sets in image-guided surgery (IGS). Three challenges are identified in the pre-to-intraoperative registration: the intra-operative 3D data (usually forms a 3D curve in space) (1) is often contaminated with noise and outliers; (2) usually only covers a partial region of the whole pre-operative model; (3) is usually sparse. To tackle those challenges, we utilize the tangent vectors extracted from the sparse intraoperative data points and the normal vectors extracted from the pre-operative model points. Our first contribution is to formulate a novel probabilistic distribution of the error between a pair of corresponding tangent and normal vectors. The second contribution is, based on the novel distribution, we formulate the registration of two multi-dimensional (6D) point sets as a maximum likelihood (ML) problem and solve it under the expectation maximization (EM) framework. Our last contribution is, in order to facilitate the computation process, the derivatives of the objective function with respect to desired parameters are presented. We conduct extensive experiments to demonstrate that our approach outperforms the state-of-the-art methods. Importantly, in the context of anteriro cruciate ligament (ACL) reconstruction, our method can achieve as low as 0.6795 mm mean target registration error (TRE) value with considerable noises and very limited overlapping ratios.


Title: Single-Shot Pose Estimation of Surgical Robot Instruments’ Shafts from Monocular Endoscopic Images
Key Words: collision avoidance  endoscopes  learning (artificial intelligence)  medical image processing  medical robotics  neural net architecture  pose estimation  robot vision  surgery  single-shot pose estimation  monocular endoscopic images  minimally invasive surgery  collision-avoidance algorithm  online estimation  monocular endoscope  art vision-based marker-less  position estimation  surgical robot instrument shafts  annotated training dataset  improved pose-estimation deep-learning architecture  Instruments  Robots  Pose estimation  Surgery  Shafts  Endoscopes  Collision avoidance 
Abstract: Surgical robots are used to perform minimally invasive surgery and alleviate much of the burden imposed on surgeons. Our group has developed a surgical robot to aid in the removal of tumors at the base of the skull via access through the nostrils. To avoid injuring the patients, a collision-avoidance algorithm that depends on having an accurate model for the poses of the instruments' shafts is used. Given that the model's parameters can change over time owing to interactions between instruments and other disturbances, the online estimation of the poses of the instrument's shaft is essential. In this work, we propose a new method to estimate the pose of the surgical instruments' shafts using a monocular endoscope. Our method is based on the use of an automatically annotated training dataset and an improved pose-estimation deep-learning architecture. In preliminary experiments, we show that our method can surpass state of the art vision-based marker-less pose estimation techniques (providing an error decrease of 55% in position estimation, 64% in pitch, and 69% in yaw) by using artificial images.


Title: End-to-End Real-time Catheter Segmentation with Optical Flow-Guided Warping during Endovascular Intervention
Key Words: catheters  feature extraction  image segmentation  image sequences  learning (artificial intelligence)  medical image processing  medical robotics  neural nets  surgery  deep learning framework  segmentation network  encoder-decoder architecture  flow network  optical flow information  frame-to-frame temporal continuity  catheter segmentation  robot-assisted endovascular intervention  flow-guided warping function  optical flow-guided warping  Catheters  Image segmentation  X-ray imaging  Real-time systems  Machine learning  Motion segmentation 
Abstract: Accurate real-time catheter segmentation is an important pre-requisite for robot-assisted endovascular intervention. Most of the existing learning-based methods for catheter segmentation and tracking are only trained on smallscale datasets or synthetic data due to the difficulties of ground-truth annotation. Furthermore, the temporal continuity in intraoperative imaging sequences is not fully utilised. In this paper, we present FW-Net, an end-to-end and real-time deep learning framework for endovascular intervention. The proposed FW-Net has three modules: a segmentation network with encoder-decoder architecture, a flow network to extract optical flow information, and a novel flow-guided warping function to learn the frame-to-frame temporal continuity. We show that by effectively learning temporal continuity, the network can successfully segment and track the catheters in real-time sequences using only raw ground-truth for training. Detailed validation results confirm that our FW-Net outperforms stateof-the-art techniques while achieving real-time performance.


Title: Pathological Airway Segmentation with Cascaded Neural Networks for Bronchoscopic Navigation
Key Words: computerised tomography  image segmentation  lung  medical image processing  neural nets  cascaded 2D-3D model  pathological CT scans  3D adversarial training model  novel 2D neural network  airway tree  pathological abnormalities  preoperative chest CT scans  patient-specific airway maps  peripheral airways  enhanced visualisation  3D airway maps  robotic bronchoscopic intervention  bronchoscopic navigation  cascaded neural networks  pathological airway segmentation  Three-dimensional displays  Atmospheric modeling  Training  Two dimensional displays  Computed tomography  Image segmentation  Solid modeling 
Abstract: Robotic bronchoscopic intervention requires detailed 3D airway maps for both localisation and enhanced visualisation, especially at peripheral airways. Patient-specific airway maps can be generated from preoperative chest CT scans. Due to pathological abnormalities and anatomical variations, automatically delineating the airway tree with distal branches is a challenging task. In the paper, we propose a cascaded 2D+3D model that has been tailored for airway segmentation from pathological CT scans. A novel 2D neural network is developed to generate the initial predictions where the peripheral airways are refined by a 3D adversarial training model. A sampling strategy based on a sequence of morphological operations is employed for the concatenation of the 2D and 3D models. The method has been validated on 20 pathological CT scans with results demonstrating improved segmentation accuracy and consistency, especially in peripheral airways.


Title: Design of 3D-printed assembly mechanisms based on special wooden joinery techniques and its application to a robotic hand
Key Words: assembling  design engineering  plastics  printers  robots  three-dimensional printing  part-joining quality  design method  robotic hand  3D-printed assembly mechanisms  robotic systems  plastic materials  Japanese wooden joinery techniques  assembling 3D-printed parts  Gears  Robots  Pins  Shape  Three-dimensional displays  Thumb  Printers 
Abstract: Recently, it has become possible to easily design and fabricate robotic systems in the laboratory and at home due to the recent development of 3D printer technology. On the other hand, the strength of the plastic materials used in reasonably priced 3D printers and the accuracy of the printed parts are generally low. These problems affect the part-joining quality. Therefore, this paper describes a design method inspired by ancient Japanese wooden joinery techniques for assembling 3D-printed parts and presents the design of a robotic hand as its application. The joinery techniques use special shapes to assemble components and allow us to assemble the robotic hand without glue, screws or nails and to easily disassemble it.


Title: A Shape Memory Polymer Adhesive Gripper For Pick-and-Place Applications
Key Words: adhesion  adhesives  control system synthesis  grippers  manipulators  polymers  shape memory effects  shape memory polymer adhesive gripper  smart adhesive applications  pick-and-place applications  reversible dry adhesion  gecko grippers  high adhesion strength  SMP adhesive mechanics  reversible dry adhesive properties  single surface contact grippers  SMP adhesive gripper  Switched mode power supplies  Grippers  Adhesives  Heating systems  Shape  Rough surfaces  Surface roughness 
Abstract: Over the past few years, shape memory polymer (SMP) has been extensively studied in terms of its remarkable reversible dry adhesive properties and related smart adhesive applications. However, its exceptional properties have not been exploited for further opportunities such as pick-and-place applications, which would otherwise advance the robotic manipulation. This work explores the use of an SMP to design an adhesive gripper that picks and places a target solid object employing the reversible dry adhesion of an SMP. Compared with other single surface contact grippers including vacuum, electromagnetic, electroadhesion, and gecko grippers, the SMP adhesive gripper interacts with not only flat and smooth dry surfaces but also moderately rough and even wet surfaces for pick-and-place with high adhesion strength (> 2 atmospheres). In this work, associated physical mechanisms, SMP adhesive mechanics, and thermal conditions are studied. In particular, the numerical and experimental study elucidates that the optimal compositional and topological SMP design may substantially enhance its adhesion strength and reversibility, which leads to a strong grip force simultaneously with a minimized releasing force. Finally, the versatility and utility of the SMP adhesive gripper are highlighted through diverse pick-and-place demonstrations.


Title: Simultaneous Tracking and Elasticity Parameter Estimation of Deformable Objects
Key Words: deformation  finite element analysis  image colour analysis  manipulators  parameter estimation  robot vision  visual information  simulated object  elasticity parameter estimation  tracked object  soft objects  deformable object  simultaneous tracking  interactive finite element method simulations  RGB-D sensor  robotic manipulation  Strain  Elasticity  Estimation  Deformable models  Force measurement  Force  Force sensors 
Abstract: In this paper, we propose a novel method to simultaneously track the deformation of soft objects and estimate their elasticity parameters. The tracking of the deformable object is performed by combining the visual information captured by a RGB-D sensor with interactive Finite Element Method simulations of the object. The visual information is more particularly used to distort the simulated object. In parallel, the elasticity parameter estimation minimizes the error between the tracked object and a simulated object deformed by the forces that are measured using a force sensor. Once the elasticity parameters are estimated, our tracking algorithm can be used to estimate the deformation forces applied to an object without the use of a force sensor. We validated our method on several soft objects with different shape complexities. Our evaluations show the ability of our method to estimate the elasticity parameters as well as its use to estimate the forces applied to a deformable object without any force sensor. These results open novel perspectives to better track and control deformable objects during robotic manipulations.


Title: AVOT: Audio-Visual Object Tracking of Multiple Objects for Robotics
Key Words: audio signal processing  audio-visual systems  learning (artificial intelligence)  object detection  object tracking  robot vision  tracking  multiple objects  visually based trackers  object collisions  audio-visual object tracking neural network  tracking error  AVOT end  audio-visual inputs  visually based object detection  tracking methods  OpenCV object tracking implementations  deep learning method  audio-visual dataset  single-modality deep learning methods  audio onset  multimodal object tracking  state-of-the-art object tracking  Object tracking  Object detection  Visualization  Neural networks  Machine learning  Feature extraction  Streaming media 
Abstract: Existing state-of-the-art object tracking can run into challenges when objects collide, occlude, or come close to one another. These visually based trackers may also fail to differentiate between objects with the same appearance but different materials. Existing methods may stop tracking or incorrectly start tracking another object. These failures are uneasy for trackers to recover from since they often use results from previous frames. By using audio of the impact sounds from object collisions, rolling, etc., our audio-visual object tracking (AVOT) neural network can reduce tracking error and drift. We train AVOT end to end and use audio-visual inputs over all frames. Our audio-based technique may be used in conjunction with other neural networks to augment visually based object detection and tracking methods. We evaluate its runtime frames-per-second (FPS) performance and intersection over union (IoU) performance against OpenCV object tracking implementations and a deep learning method. Our experiments, using the synthetic Sound-20K audio-visual dataset, demonstrate that AVOT outperforms single-modality deep learning methods, when there is audio from object collisions. A proposed scheduler network to switch between AVOT and other methods based on audio onset maximizes accuracy and performance over all frames in multimodal object tracking.


Title: Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering
Key Words: agricultural robots  cameras  convolutional neural nets  distributed processing  farming  image filtering  image matching  image sensors  object detection  object tracking  robot vision  efficient pig counting  large-scale pig farming  automated pig counting method  pig movements  pig grouping houses  real-time automated pig counting system  pig detection algorithm  deformable pig shapes  pig body part  keypoints tracking  spatial-aware temporal response filtering  pig occlusion  pig overlapping  monocular fisheye camera  inspection robot  deep convolution neural network  keypoints association  efficient on-line tracking method  tracking failures  edge computing device  Cameras  Tracking  Robot vision systems  Inspection  Skeleton  Pipelines 
Abstract: Pig counting is a crucial task for large-scale pig farming. Pigs are usually visually counted by human. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. The existing works only focused on pig counting using single image, and its level of accuracy faced challenges due to pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the needs of pig counting for large pig grouping houses. Towards addressing these challenges, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it achieved performance superior to human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformable pig shapes. This detection included a deep convolution neural network (CNN) for pig body part keypoints detection and the keypoints association method to identify individual pigs. It then employed an efficient on-line tracking method to associate pigs across image frames. Finally, pig counts were estimated by a novel spatial-aware temporal response filtering (STRF) method to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.


Title: 6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints
Key Words: closed loop systems  image colour analysis  learning (artificial intelligence)  object detection  object tracking  pose estimation  robot vision  stereo image processing  https://sites.google.com/view/6packtracking  physical robot  interframe motion  3D keypoints  real time novel object instances  RGB-D data  NOCS category-level 6D pose estimation benchmark  keypoint matching  object instance  known object categories  deep learning approach  anchor-based keypoints  category-level 6D pose tracker  6-PACK  simple vision-based closed-loop manipulation tasks  Three-dimensional displays  Pose estimation  Robustness  Robots  Real-time systems  Tracking  Visualization 
Abstract: We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.


Title: Designing Ferromagnetic Soft Robots (FerroSoRo) with Level-Set-Based Multiphysics Topology Optimization
Key Words: actuators  deformation  design engineering  elastomers  grippers  optimisation  sensitivity analysis  topology  shape sensitivity analysis  gripper  flytrap structure  material layout  innovative structures  bionic medical devices  compliant actuators  level-set-based multiphysics topology optimization method  adjoint variable method  material time derivative  sub-objective function  architect ferromagnetic soft active structures  design domain  structural topology optimization  ferromagnetic soft elastomers  external magnetic field  shift morphology  soft elastomer matrix  ferromagnetic particles  flexible electronics  soft machines  external environmental stimulus  change configurations  flexible locomotion  soft active materials  FerroSoRo  designing ferromagnetic soft robots  Soft magnetic materials  Optimization  Topology  Magnetic domains  Magnetoacoustic effects  Level set  Shape 
Abstract: Soft active materials can generate flexible locomotion and change configurations through large deformations when subjected to an external environmental stimulus. They can be engineered to design 'soft machines' such as soft robots, compliant actuators, flexible electronics, or bionic medical devices. By embedding ferromagnetic particles into soft elastomer matrix, the ferromagnetic soft matter can generate flexible movement and shift morphology in response to the external magnetic field. By taking advantage of this physical property, soft active structures undergoing desired motions can be generated by tailoring the layouts of the ferromagnetic soft elastomers. Structural topology optimization has emerged as an attractive tool to achieve innovative structures by optimizing the material layout within a design domain, and it can be utilized to architect ferromagnetic soft active structures. In this paper, the level-set-based topology optimization method is employed to design ferromagnetic soft robots (FerroSoRo). The objective function comprises a sub-objective function for the kinematics requirement and a sub-objective function for minimum compliance. Shape sensitivity analysis is derived using the material time derivative and adjoint variable method. Three examples, including a gripper, an actuator, and a flytrap structure, are studied to demonstrate the effectiveness of the proposed framework.


Title: Exoskeleton-covered soft finger with vision-based proprioception and tactile sensing
Key Words: actuators  convolutional neural nets  dexterous manipulators  grippers  intelligent sensors  learning (artificial intelligence)  mobile robots  robot vision  tactile sensors  high-resolution proprioceptive sensing  rich tactile sensing  highly underactuated exoskeleton  robotic gripper  tactile information  proprioception CNN  human finger proprioception  proprioceptive state  peripheral environment  vision-based proprioception  rigid-body robots  soft robots  accurate proprioception  elasticity  tactile sensor  previous GelSight sensing techniques  exoskeleton-covered soft finger  size 0.77 mm  Soft robotics  Cameras  Robot vision systems  Ink 
Abstract: Soft robots offer significant advantages in adaptability, safety, and dexterity compared to conventional rigid-body robots. However, it is challenging to equip soft robots with accurate proprioception and tactile sensing due to their high flexibility and elasticity. In this work, we describe the development of a vision-based proprioceptive and tactile sensor for soft robots called GelFlex, which is inspired by previous GelSight sensing techniques. More specifically, we develop a novel exoskeleton-covered soft finger with embedded cameras and deep learning methods that enable high-resolution proprioceptive sensing and rich tactile sensing. To do so, we design features along the axial direction of the finger, which enable high-resolution proprioceptive sensing, and incorporate a reflective ink coating on the surface of the finger to enable rich tactile sensing. We design a highly underactuated exoskeleton with a tendon-driven mechanism to actuate the finger. Finally, we assemble 2 of the fingers together to form a robotic gripper and successfully perform a bar stock classification task, which requires both shape and tactile information. We train neural networks for proprioception and shape (box versus cylinder) classification using data from the embedded sensors. The proprioception CNN had over 99% accuracy on our testing set (all six joint angles were within 1° of error) and had an average accumulative distance error of 0.77 mm during live testing, which is better than human finger proprioception. These proposed techniques offer soft robots the high-level ability to simultaneously perceive their proprioceptive state and peripheral environment, providing potential solutions for soft robots to solve everyday manipulation tasks. We believe the methods developed in this work can be widely applied to different designs and applications.


Title: Tuning the Energy Landscape of Soft Robots for Fast and Strong Motion
Key Words: actuators  deformation  design engineering  flexible manipulators  grippers  mobile robots  pneumatic actuators  energy landscape  soft body structures  fast motion  strong motion  soft module  soft bistable module  fast robots  strong soft robots  soft gripper  soft jumping robot  soft actuator  twisted-and-coiled actuator  Soft robotics  Springs  Potential energy  Shape  Actuators  Plastics 
Abstract: Soft robots demonstrate great potential compared with traditional rigid robots owing to their inherently soft body structures. Although researchers have made tremendous progress in recent years, existing soft robots are in general plagued by a main issue: slow speeds and small forces. In this work, we aim to address this issue by actively designing the energy landscape of the soft body: the total strain energy with respect to the robot's deformation. With such a strategy, a soft robot's dynamics can be tuned to have fast and strong motion. We introduce the general design principle using a soft module with two stable states that can rapidly switch from one state to the other under external forces. We characterize the required triggering (switching) force with respect to design parameters (e.g., the initial shape of the module). We then apply the soft bistable module to develop fast and strong soft robots, whose triggering forces are generated by a soft actuator - twisted-and-coiled actuator (TCA). We demonstrate a soft gripper that can hold weights more than 8 times its own weight, and a soft jumping robot that can jump more than 5 times its body height. We envision our strategies will overcome the weakness of soft robots to unleash their potential for diverse applications.


Title: REBOund: Untethered Origami Jumping Robot with Controllable Jump Height
Key Words: deformation  finite element analysis  legged locomotion  pneumatic actuators  shear modulus  springs (mechanical)  custom release mechanisms  quick compression  fold pattern  controllable jump height  jumping maneuvers  control strategies  robot body  model fold patterns  potential energy storage  parametric origami tessellation  face deformations  nonlinear spring  pseudorigid-body model  mechanical testing system  stored potential energy  reconfigurable expanding bistable origami pattern  untethered origami jumping robot  REBOund robot 
Abstract: Origami robots are well-suited for jumping maneuvers because of their light weight and ability to incorporate actuation and control strategies directly into the robot body. However, existing origami robots often model fold patterns as rigidly foldable and fail to take advantage of deformation in an origami sheet for potential energy storage. In this paper, we consider a parametric origami tessellation, the Reconfigurable Expanding Bistable Origami (REBO) pattern, which leverages face deformations to act as a nonlinear spring. We present a pseudo-rigid-body model for the REBO for computing its energy stored when compressed to a given displacement and compare that model to experimental measurements taken on a mechanical testing system. This stored potential energy, when released quickly, can cause the pattern to jump. Using our model and experimental data, we design and fabricate a jumping robot, REBOund, that uses the spring-like REBO pattern as its body. Four lightweight servo motors with custom release mechanisms allow for quick compression and release of the origami pattern, allowing the fold pattern to jump over its own height even when carrying 5 times its own weight in electronics and power. We further demonstrate that small geometric changes to the pattern allow us to change the jump height without changing the actuation or control mechanism.


Title: Validation of a Forward Kinematics Based Controller for a mobile Tethered Pelvic Assist Device to Augment Pelvic Forces during Walking
Key Words: force control  gait analysis  medical robotics  mobile robots  motion control  patient rehabilitation  robot kinematics  mobile TPAD frame  treadmill walking  motor control patterns  open loop controller  treadmill based robotic trainer  pelvic force augmentation  mobile tethered pelvic assist device  forward kinematics controller  mobile device controller  rehabilitation robotic devices  overground gait training robotic devices  Belts  Legged locomotion  Pelvis  Kinematics  Training  Force 
Abstract: For those with irregular gait, re-calibration of motor control strategies and retraining of coordination are key goals. Thoughtful external forces or resistances during repetitive tasks can reprogram motor control patterns and strategies. Prior work in our lab has utilized this theory to improve gait in various patient groups using the Tethered Pelvic Assist Device (TPAD), a treadmill-based robotic trainer. In this paper, we propose a new, portable extension of the TPAD, which relies on an open-loop, forward kinematics based controller to remove the restriction of walking in the laboratory on a treadmill, and therefore accommodates overground ambulation. To evaluate the effects of this new control scheme and the effects of the users holding the mobile TPAD frame, a dataset of walking in four conditions was collected from eight healthy individuals. When applying a constant pelvic loading force of 10% body weight, the mean ground reaction force increased by 8.2±7.7% when the individual holds the walker frame and 11.1±7.8% when no hand contact is made. The mobile TPAD was shown to still induce a targeted loading on individuals during treadmill walking. The validation of this mobile device's controller and characterization of holding the frame allow overground studies to be conducted, and now opens the door to new training paradigms for overground gait training.


Title: Model Learning for Control of a Paralyzed Human Arm with Functional Electrical Stimulation
Key Words: biomechanics  biomedical electrodes  feedback  feedforward  Gaussian processes  handicapped aids  learning (artificial intelligence)  medical control systems  medical robotics  neuromuscular stimulation  neurophysiology  patient rehabilitation  regression analysis  paralyzed human arm  functional electrical stimulation  restoring reaching ability  tetraplegia  shoulder-arm complex  full-arm 3D reaching tasks  Gaussian process regression model  feedforward-feedback control structure  paralyzed upper limb  FES-driven 3D reaching controller  Muscles  Wrist  Iron  Force  Robots  Ground penetrating radar  Data models 
Abstract: Functional electrical stimulation (FES) is a promising technique for restoring reaching ability to individuals with tetraplegia. To this point, the complexities of goal-directed reaching motions and the shoulder-arm complex have prevented the realization of this potential in full-arm 3D reaching tasks. We trained a Gaussian process regression model to form the basis of a feedforward-feedback control structure capable of achieving reaching motions with a paralyzed upper limb. Over a series of 95 reaches of at least 10 cm in length, the controller achieved an average accuracy (measured by the Euclidean distance of the wrist to the final target position) of 3.8 cm and an average error along the path of 3.5 cm. This controller is the first demonstration of an accurate, complete-arm, FES-driven 3D reaching controller to be implemented with an individual with tetraplegia.


Title: Transient Behavior and Predictability in Manipulating Complex Objects
Key Words: feedback  feedforward  haptic interfaces  human-robot interaction  manipulator dynamics  motion control  virtual reality  robot control  internal dynamics  transient behavior  predictable dynamics  virtual object  robotic manipulandum  predictable steady state  feedforward controller  inverse dynamics  haptic feedback  Transient analysis  Robots  Steady-state  Task analysis  Dynamics  Force  Haptic interfaces 
Abstract: Relatively little work in human and robot control has examined the control of underactuated objects with internal dynamics, such as transporting a cup of coffee, a task that presents little problems for humans. This study examined how humans move a `cup of coffee' with a view to identify principles that may be useful for robot control. The specific focus was on how humans choose initial conditions to safely reach a steady state. We hypothesized that subjects choose initial conditions that minimized the transient duration to reach the steady state faster, as it presented more predictable dynamics. In the experiment, the cup of coffee was reduced to a 2-D cup with a sliding ball inside which was simulated in a virtual environment. Human subjects interacted with this virtual object via a robotic manipulandum that provided haptic feedback. Participants moved the cup between two targets without losing the ball; they were instructed to explore different initial conditions before initiating the continuous interaction. Results showed that subjects converged to a small set of initial conditions that decreased their transient durations and achieved a predictable steady state faster. Simulations with a simple feedforward controller and inverse dynamics calculations confirmed that these initial conditions indeed led to shorter transients and less complex interaction forces. These results may inform robot control of objects with internal dynamics where the effects of initial conditions need further investigation.


Title: A Variable-Fractional Order Admittance Controller for pHRI
Key Words: augmented reality  control engineering computing  drilling  groupware  human-robot interaction  industrial robots  production engineering computing  user interfaces  pHRI  labor intensive tasks  fractional order control  fractional order variable admittance controller  integer order variable admittance controller  realistic drilling task  transparent interaction  augmented reality headset  human sensory capabilities  variable-fractional order admittance controller  automation driven manufacturing environments  collaborative robots  augmented reality interfaces  production workflow  cognitive skills  physical human robot interaction  cobots  stability  drilling depth  Admittance  Task analysis  Robot sensing systems  Collaboration  Stability criteria 
Abstract: In today's automation driven manufacturing environments, emerging technologies like cobots (collaborative robots) and augmented reality interfaces can help integrating humans into the production workflow to benefit from their adaptability and cognitive skills. In such settings, humans are expected to work with robots side by side and physically interact with them. However, the trade-off between stability and transparency is a core challenge in the presence of physical human robot interaction (pHRI). While stability is of utmost importance for safety, transparency is required for fully exploiting the precision and ability of robots in handling labor intensive tasks. In this work, we propose a new variable admittance controller based on fractional order control to handle this trade-off more effectively. We compared the performance of fractional order variable admittance controller with a classical admittance controller with fixed parameters as a baseline and an integer order variable admittance controller during a realistic drilling task. Our comparisons indicate that the proposed controller led to a more transparent interaction compared to the other controllers without sacrificing the stability. We also demonstrate a use case for an augmented reality (AR) headset which can augment human sensory capabilities for reaching a certain drilling depth otherwise not possible without changing the role of the robot as the decision maker.


Title: Assistive Gym: A Physics Simulation Framework for Assistive Robotics
Key Words: human-robot interaction  learning (artificial intelligence)  manipulators  medical robotics  mobile robots  robot programming  service robots  physics simulation framework  autonomous robots  physical interaction  physics simulations  physical assistance  open source physics  assistive robots  simulated environments  robotic manipulator  assistive gym models  commercial robots  assistive robotics research  ADL  activities of daily living  Task analysis  Manipulators  Physics  Tools  Human-robot interaction  Mobile robots 
Abstract: Autonomous robots have the potential to serve as versatile caregivers that improve quality of life for millions of people worldwide. Yet, conducting research in this area presents numerous challenges, including the risks of physical interaction between people and robots. Physics simulations have been used to optimize and train robots for physical assistance, but have typically focused on a single task. In this paper, we present Assistive Gym, an open source physics simulation framework for assistive robots that models multiple tasks. It includes six simulated environments in which a robotic manipulator can attempt to assist a person with activities of daily living (ADLs): itch scratching, drinking, feeding, body manipulation, dressing, and bathing. Assistive Gym models a person's physical capabilities and preferences for assistance, which are used to provide a reward function. We present baseline policies trained using reinforcement learning for four different commercial robots in the six environments. We demonstrate that modeling human motion results in better assistance and we compare the performance of different robots. Overall, we show that Assistive Gym is a promising tool for assistive robotics research.


Title: Learning Whole-Body Human-Robot Haptic Interaction in Social Contexts
Key Words: control engineering computing  force sensors  haptic interfaces  human-robot interaction  learning (artificial intelligence)  telerobotics  whole-body human-robot haptic interaction  learning-from-demonstration framework  human-robot social interactions  human-robot contact  LfD framework  teleoperated bimanual robot  force sensors  Robot sensing systems  Haptic interfaces  Force  Spatiotemporal phenomena  Training 
Abstract: This paper presents a learning-from-demonstration (LfD) framework for teaching human-robot social interactions that involve whole-body haptic interaction, i.e. direct human-robot contact over the full robot body. The performance of existing LfD frameworks suffers in such interactions due to the high dimensionality and spatiotemporal sparsity of the demonstration data. We show that by leveraging this sparsity, we can reduce the data dimensionality without incurring a significant accuracy penalty, and introduce three strategies for doing so. By combining these techniques with an LfD framework for learning multimodal human-robot interactions, we can model the spatiotemporal relationship between the tactile and kinesthetic information during whole-body haptic interactions. Using a teleoperated bimanual robot equipped with 61 force sensors, we experimentally demonstrate that a model trained with 121 sample hugs from 4 participants generalizes well to unseen inputs and human partners.


Title: Human Preferences in Using Damping to Manage Singularities During Physical Human-Robot Collaboration
Key Words: damping  human-robot interaction  manipulator kinematics  mobile robots  kinematic singularities  damping-based strategy  human operator  human preferences  physical human-robot collaboration  robot manipulator  kinematic singular configuration  double-blind A/B pairwise comparison testing protocol  singularities handling  Damping  Manipulators  Collaboration  Jacobian matrices  Kinematics  Collision avoidance 
Abstract: When a robot manipulator approaches a kinematic singular configuration, control strategies need to be employed to ensure safe and robust operation. If this manipulator is being controlled by a human through physical human-robot collaboration, the choice of strategy for handling singularities can have a significant effect on the feelings and impressions of the user. To date the preferences of humans during physical human-robot collaboration regarding strategies for managing kinematic singularities have yet to be thoroughly explored.This work presents an empirical study of a damping-based strategy for handling singularities with regard to the preferences of the human operator. Two different parameters, damping rate and damping asymmetry, are tested using a double-blind A/B pairwise comparison testing protocol. Participants included two cohorts made up of the general public (n=51) and people working within a robotic research centre (n=18). In total 105 individual trials were performed. Results indicate a preference for a faster, asymmetric damping behavior that slows motions towards singularities whilst allowing for faster motions away.


Title: MOCA-MAN: A MObile and reconfigurable Collaborative Robot Assistant for conjoined huMAN-robot actions
Key Words: gesture recognition  human-robot interaction  manipulators  medical robotics  mobile robots  multi-robot systems  mobile manipulators  supernumerary limbs  reconfiguration potential  MObile Collaborative robot Assistant  supernumerary body  MOCA-MAN  hand gesture recognition system  mobile base  long distance co-carrying tasks  manipulating tools  conjoined actions  performing heavy manipulation tasks  prolonged manipulation tasks  close-proximity manipulation  mobile robot assistant  reconfigurable collaborative robot assistant  conjoined huMAN-robot actions  collaborative robotic system  Admittance  Task analysis  Collaboration  Robot sensing systems  Clamps  Manipulators 
Abstract: The objective of this paper is to create a new collaborative robotic system that subsumes the advantages of mobile manipulators and supernumerary limbs. By exploiting the reconfiguration potential of a MObile Collaborative robot Assistant (MOCA), we create a collaborative robot that can function autonomously, in close proximity to humans, or be physically coupled to the human counterpart as a supernumerary body (MOCA-MAN). Through an admittance interface and a hand gesture recognition system, the controller can give higher priority to the mobile base (e.g., for long distance co-carrying tasks) or the arm movements (e.g., for manipulating tools), when performing conjoined actions. The resulting system has a high potential not only to reduce waste associated with the equipment waiting and setup times, but also to mitigate the human effort when performing heavy or prolonged manipulation tasks. The performance of the proposed system, i.e., MOCA-MAN, is evaluated by multiple subjects in two different use-case scenarios, which require large mobility or close-proximity manipulation.


Title: Closing the Force Loop to Enhance Transparency in Time-delayed Teleoperation
Key Words: delays  force control  human-robot interaction  stability  telerobotics  master-slave teleoperation system  bilateral controllers  force transparency  force loop  force control  time delayed teleoperation  KUKA lightweight robots  time domain passivity  Force  Iron  Force measurement  Robots  Stability analysis  Delays  Task analysis 
Abstract: In the present paper, we first adopt explicit force control from general robotics and embed it into teleoperation systems to enhance the transparency by reducing the effect of the perceived inertia to the human operator and simultaneously improve contact perception. To ensure stability of the proposed teleoperation system considering time-delays, we propose a sequential design procedure based on time domain passivity approach. Experimental results of master-slave teleoperation system, based on KUKA light-weight-robots, for different values of delays are presented. Comparative analysis is conducted considering two existing approaches, namely 2-channel and 4-channel architecture based bilateral controllers, and its results clearly indicate significant improvement in force transparency owing to the proposed method. The proposed system is finally validated considering a real industrial assembly scenario.


Title: Evaluation of an Exoskeleton-based Bimanual Teleoperation Architecture with Independently Passivated Slave Devices
Key Words: delays  motion control  stability  telerobotics  simulated time delay  control loop frequency  multiDoFs devices  TDPA  time domain passivity approach  exoskeletal master  bimanual teleoperation system  communication delay  bilateral teleoperation  haptic feedback  robotic platforms  rescue robotics  independently passivated slave devices  exoskeleton-based bimanual teleoperation architecture  Exoskeletons  Task analysis  Computer architecture  Stability analysis  Robots  Delays  Haptic interfaces 
Abstract: Search and rescue robotics is becoming a relevant topic in the last years and the growing number of robotic platforms and dedicated projects is the evidence of the interest in this area. In this context, the possibility to drive a remote robot with an exoskeleton is a promising strategy to enhance dexterity, reduce operator effort and save time. However, the use of haptic feedback (bilateral teleoperation) may lead to instability in the presence of communication delay and more complex is the case of bimanual teleoperation where the two arms can exchange energy. In this work, we present a bimanual teleoperation system based on an exoskeletal master, where multi-degrees of freedom (multi-DoFs) and kinematically different devices are involved. In the implemented architecture the two slaves are managed in parallel and independently passivated using the Time Domain Passivity Approach (TDPA) extended for multi-DoFs devices. To investigate the stability of the architecture we designed two tasks highly related to real disaster scenarios: the first one was useful to verify the system behavior in case of small movements and constrained configurations, whereas the second experiment was designed to involve larger contact forces and movements. Moreover, we compared the effect of both delay and low control loop frequency on the stability of the system when TDPA was applied. From the results, it was evident that the overall system exhibited a stable behavior with the use of the TDPA, even passivating the two slaves independently, under simulated time delay and in presence of a low control loop frequency.


Title: Hand-worn Haptic Interface for Drone Teleoperation
Key Words: autonomous aerial vehicles  data gloves  human-robot interaction  mobile robots  motion control  robot vision  telerobotics  trajectory control  drone teleoperation  remote radio controllers  wearable interface  drone trajectory  hand motion  haptic system  robotic systems  teleoperation performance  remote controllers  human-robot interfaces  hand-worn haptic interface  data glove  line of sight  search-and-rescue missions  Haptic interfaces  Drones  Task analysis  Robot sensing systems  Hardware 
Abstract: Drone teleoperation is usually accomplished using remote radio controllers, devices that can be hard to master for inexperienced users. Moreover, the limited amount of information fed back to the user about the robot's state, often limited to vision, can represent a bottleneck for operation in several conditions. In this work, we present a wearable interface for drone teleoperation and its evaluation through a user study. The two main features of the proposed system are a data glove to allow the user to control the drone trajectory by hand motion and a haptic system used to augment their awareness of the environment surrounding the robot. This interface can be employed for the operation of robotic systems in line of sight (LoS) by inexperienced operators and allows them to safely perform tasks common in inspection and search-and-rescue missions such as approaching walls and crossing narrow passages with limited visibility conditions. In addition to the design and implementation of the wearable interface, we performed a systematic study to assess the effectiveness of the system through three user studies (n = 36) to evaluate the users' learning path and their ability to perform tasks with limited visibility. We validated our ideas in both a simulated and a real-world environment. Our results demonstrate that the proposed system can improve teleoperation performance in different cases compared to standard remote controllers, making it a viable alternative to standard Human-Robot Interfaces.


Title: Toward Human-like Teleoperated Robot Motion: Performance and Perception of a Choreography-inspired Method in Static and Dynamic Tasks for Rapid Pose Selection of Articulated Robots
Key Words: control engineering computing  human-robot interaction  mobile robots  service robots  telerobotics  virtual reality  remotely-operated robot  remote telepresence  Baxter robot  Xbox One controller  JBJ  limb  multiple joints  successfully completed tasks  joint-by-joint method  choreography-inspired method  performance data  static tasks  RCC method  dynamic tasks  human-likeness  robotic motion  teleoperated robot motion  rapid pose selection  articulated robots  robot choreography center  Task analysis  Training  Dynamics  Joints  Robot motion  Manipulators 
Abstract: In some applications, operators may want to create fluid, human-like motion on a remotely-operated robot, for example, a device used for remote telepresence. This paper examines two methods of controlling the pose of a Baxter robot via an Xbox One controller. The first method is a joint- by-joint (JBJ) method in which one joint of each limb is specified in sequence. The second method of control, named Robot Choreography Center (RCC), utilizes choreographic abstractions in order to simultaneously move multiple joints of the limb of the robot in a predictable manner. Thirty-eight users were asked to perform four tasks with each method. Success rate and duration of successfully completed tasks were used to analyze the performances of the participants. Analysis of the preferences of the users found that the joint-by-joint (JBJ) method was considered to be more precise, easier to use, safer, and more articulate, while the choreography-inspired (RCC) method of control was perceived as faster, more fluid, and more expressive. Moreover, performance data found that while both methods of control were over 80% successful for the two static tasks, the RCC method was an average of 11.85% more successful for the two more difficult, dynamic tasks. Future work will leverage this framework to investigate ideas of fluidity, expressivity, and human-likeness in robotic motion through online user studies with larger participant pools.


Title: Helping Robots Learn: A Human-Robot Master-Apprentice Model Using Demonstrations via Virtual Reality Teleoperation
Key Words: control engineering computing  human-robot interaction  learning (artificial intelligence)  multi-robot systems  robot programming  telerobotics  virtual reality  grasping task  human perception  human-robot master-apprentice model  virtual reality teleoperation  artificial intelligence  self-supervised learning  Robots  Grasping  Task analysis  Three-dimensional displays  Solid modeling  Virtual reality  Pipelines 
Abstract: As artificial intelligence becomes an increasingly prevalent method of enhancing robotic capabilities, it is important to consider effective ways to train these learning pipelines and to leverage human expertise. Working towards these goals, a master-apprentice model is presented and is evaluated during a grasping task for effectiveness and human perception. The apprenticeship model augments self-supervised learning with learning by demonstration, efficiently using the human's time and expertise while facilitating future scalability to supervision of multiple robots; the human provides demonstrations via virtual reality when the robot cannot complete the task autonomously. Experimental results indicate that the robot learns a grasping task with the apprenticeship model faster than with a solely self-supervised approach and with fewer human interventions than a solely demonstration-based approach; 100% grasping success is obtained after 150 grasps with 19 demonstrations. Preliminary user studies evaluating workload, usability, and effectiveness of the system yield promising results for system scalability and deployability. They also suggest a tendency for users to overestimate the robot's skill and to generalize its capabilities, especially as learning improves.


Title: A Framework for Interactive Virtual Fixture Generation for Shared Teleoperation in Unstructured Environments
Key Words: feature selection  haptic interfaces  telecontrol  virtual reality  unstructured environments  human operator performance  remote environment  task execution  user interface  camera images  interactive selection  feature selection  interactive virtual fixture generation  shared teleoperation  6-DOF haptic feedback  Tools  Feature extraction  Task analysis  Robots  Detectors  Three-dimensional displays  Haptic interfaces 
Abstract: Virtual fixtures (VFs) improve human operator performance in teleoperation scenarios. However, the generation of VFs is challenging, especially in unstructured environments. In this work, we introduce a framework for the interactive generation of VF. The method is based on the observation that a human can easily understand just by looking at the remote environment which VF could help in task execution. We propose a user interface that detects features on camera images and permits interactive selection of the features. We demonstrate how the feature selection can be used for designing VF, providing 6-DOF haptic feedback. In order to make the proposed framework more generally applicable to a wider variety of applications, we formalize the process of virtual fixture generation (VFG) into the specification of features, geometric primitives, and constraints. The framework can be extended further by the introduction of additional components. Through the human subject study, we demonstrate the proposed framework is intuitive, easy to use while effective, especially for performing hard contact tasks.


Title: Local Obstacle-Skirting Path Planning for a Fast Bi-steerable Rover using Bézier Curves
Key Words: automatic guided vehicles  collision avoidance  curve fitting  mobile robots  navigation  off-road vehicles  predictive control  stability  steering systems  vehicle dynamics  local obstacle-skirting path planning  obstacle avoidance  off-road mobile robots  global reference path  smooth path  lateral stability  double-steering rover  online cubic Bezier curves  bi-steerable rover  constrained model predictive control  navigation  autonomous guided vehicles  Safety  Collision avoidance  Mobile robots  Lead  Path planning  Robot kinematics 
Abstract: This paper focuses on local path planning for obstacle avoidance tasks dedicated to off-road mobile robots. This approach calculates a new local path for the vehicle using a set of cubic Bezier curves once the safety distance is not respected; otherwise, the vehicle follows the global reference path which is defined off-line. Two basic steps are used to determine this new path. Firstly, some significant points that should belong to the planned path are extracted on-line according to the obstacle's sizes and the current state of the vehicle, these points are approved as waypoints. Secondly, on-line cubic Bezier curves are computed to create a smooth path for these points such that the safety and lateral stability of the vehicle are ensured (i.e., preventing huge curvatures and wide-variation in steering angles). This path will be used as a reference to be performed by the vehicle using a constrained model predictive control. The validation of our navigation strategy is performed via numerical simulations and experiments using a fast double-steering rover.


Title: Collision Avoidance with Proximity Servoing for Redundant Serial Robot Manipulators
Key Words: collision avoidance  human-robot interaction  manipulators  motion control  quadratic programming  repulsive motions  instantaneous optimal joint velocities  quadratic optimization problem  proximity sensing skins  collision avoidance  low-latency perception  proximity sensors  fastreacting motions  safe human-robot interaction  redundant serial robot manipulators  proximity servoing  Collision avoidance  Robot sensing systems  Task analysis  Manipulators  Skin 
Abstract: Collision avoidance is a key technology towards safe human-robot interaction, especially on-line and fastreacting motions are required. Skins with proximity sensors mounted on the robot's outer shell provide an interesting approach to occlusion-free and low-latency perception. However, collision avoidance algorithms which make extensive use of these properties for fast-reacting motions have not yet been fully investigated. We present an improved collision avoidance algorithm for proximity sensing skins by formulating a quadratic optimization problem with inequality constraints to compute instantaneous optimal joint velocities. Compared to common repulsive force methods, our algorithm confines the approach velocity to obstacles and keeps motions pointing away from obstacles unrestricted. Since with repulsive motions the robot only moves in one direction, opposite to obstacles, our approach has better exploitation of the redundancy space to maintain the task motion and gets stuck less likely in local minima. Furthermore, our method incorporates an active behaviour for avoiding obstacles and evaluates all potentially colliding obstacles for the whole arm, rather than just the single nearest obstacle. We demonstrate the effectiveness of our method with simulations and on real robot manipulators in comparison with commonly used repulsive force methods and our prior proposed approach.


Title: Predicting Obstacle Footprints from 2D Occupancy Maps by Learning from Physical Interactions
Key Words: collision avoidance  convolutional neural nets  laser ranging  learning (artificial intelligence)  mobile robots  navigation  robot vision  indoor robot localization  obstacle avoidance  laser scanners  collision events  2D occupancy maps  obstacle footprint prediction  physical interaction learning  horizontal scanning 2D laser range finders  convolutional neural network  Two dimensional displays  Collision avoidance  Image segmentation  Training  Robot sensing systems 
Abstract: Horizontally scanning 2D laser rangefinders are a popular approach for indoor robot localization because of the high accuracy of the sensors and the compactness of the required 2D maps. As the scanners in this configuration only provide information about one slice of the environment, the measurements typically do not capture the full extent of a large variety of obstacles, including chairs or tables. Accordingly, obstacle avoidance based on laser scanners mounted in such a fashion is likely to fail. In this paper, we propose a learning-based approach to predict collisions in 2D occupancy maps. Our approach is based on a convolutional neural network which is trained on a 2D occupancy map and collision events recorded with a bumper while the robot is navigating in its environment. As the network operates on local structures only, it can generalize to new environments. In addition, the robot can collect and integrate new collision examples after an initial training phase. Extensive experiments carried out in simulation and a realistic real-world environment confirm that our approach allows robots to learn from collision events to avoid collisions in the future.


Title: Path Planning in Dynamic Environments using Generative RNNs and Monte Carlo Tree Search
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  Monte Carlo methods  recurrent neural nets  tree searching  Monte Carlo tree search  generative recurrent neural networks  integrated path  motion models  traffic  robotic path planning  dynamic environments  effective path planning  motion prediction accuracy  planned robotic actions  generative RNNs  action space  crowd dynamics  social response  learnt model  Robots  Predictive models  Path planning  Decoding  Collision avoidance  Training  Dynamics 
Abstract: State of the art methods for robotic path planning in dynamic environments, such as crowds or traffic, rely on hand crafted motion models for agents. These models often do not reflect interactions of agents in real world scenarios. To overcome this limitation, this paper proposes an integrated path planning framework using generative Recurrent Neural Networks within a Monte Carlo Tree Search (MCTS). This approach uses a learnt model of social response to predict crowd dynamics during planning across the action space. This extends our recent work using generative RNNs to learn the relationship between planned robotic actions and the likely response of a crowd. We show that the proposed framework can considerably improve motion prediction accuracy during interactions, allowing more effective path planning. The performance of our method is compared in simulation with existing methods for collision avoidance in a crowd of pedestrians, demonstrating the ability to control future states of nearby individuals. We also conduct preliminary real world tests to validate the effectiveness of our method.


Title: Safety-Critical Rapid Aerial Exploration of Unknown Environments
Key Words: autonomous aerial vehicles  collision avoidance  helicopters  mobile robots  sensors  uncertain systems  high-speed flight  uncertain environments  controller level  state uncertainty  nonlinear system dynamics  high-fidelity simulation  cave environment  safety-critical rapid aerial exploration  collision avoidance  aerial vehicles  unknown environments  quadrotor  onboard sensors  Safety  Collision avoidance  Three-dimensional displays  Drones  Trajectory  Vehicle dynamics  Planning 
Abstract: This paper details a novel approach to collision avoidance for aerial vehicles that enables high-speed flight in uncertain environments. This framework is applied at the controller level and provides safety regardless of the planner that is used. The method is shown to be robust to state uncertainty and disturbances, and is computed entirely online utilizing the full nonlinear system dynamics. The effectiveness of this method is shown in a high-fidelity simulation of a quadrotor with onboard sensors rapidly and safely exploring a cave environment utilizing a simple planner.


Title: Reconfigurable Magnetic Microswarm for Thrombolysis under Ultrasound Imaging
Key Words: biochemistry  biological tissues  biomedical materials  biomedical ultrasonics  blood  magnetic particles  microrobots  nanomedicine  nanoparticles  patient treatment  reconfigurable magnetic microswarm  thrombolysis  ultrasound imaging  magnetic nanoparticle microswarm  tissue plasminogen activator  oscillating magnetic field  aspect ratio  out-of-plane fluid convection  lysis rate  microswarm-induced fluid convection  Ultrasonic imaging  Magnetic resonance imaging  Convection  Coagulation  Coils  Micro/nanorobot  magnetic control  collective behavior  thrombolysis  ultrasound imaging 
Abstract: We propose thrombolysis using a magnetic nanoparticle microswarm with tissue plasminogen activator (tPA) under ultrasound imaging. The microswarm is generated in blood using an oscillating magnetic field and can be navigated with locomotion along both the long and short axis. By modulating the input field, the aspect ratio of the microswarm can be reversibly tuned, showing the ability to adapt to different confined environments. Simulation results indicate that both in-plane and out-of-plane fluid convection are induced around the microswarm, which can be further enhanced by tuning the aspect ratio of the microswarm. Under ultrasound imaging, the microswarm is navigated in a microchannel towards a blood clot and deformed to obtain optimal lysis. Experimental results show that the lysis rate reaches -0.1725 ± 0.0612 mm3/min in the 37°C blood environment under the influence of the microswarm-induced fluid convection and tPA. The lysis rate is enhanced 2.5-fold compared to that without the microswarm (-0.0681 ± 0.0263 mm3/min). Our method provides a new strategy to increase the efficiency of thrombolysis by applying microswarm-induced fluid convection, indicating that swarming micro/nanorobots have the potential to act as effective tools towards targeted therapy.


Title: Improving Optical Micromanipulation with Force-Feedback Bilateral Coupling
Key Words: control engineering computing  force feedback  haptic interfaces  micromanipulators  radiation pressure  robot vision  telerobotics  haptic device  transparent force feedback  user dexterity  microsized shapes  contact forces  optical micromanipulation  force-feedback bilateral coupling  interactive approaches  visual feedback  haptic feedback teleoperation systems  optical tweezers platform  2D image  Optical feedback  Optical sensors  Force  Haptic interfaces  Optical imaging  Three-dimensional displays  High-speed optical techniques 
Abstract: Micromanipulation is challenging due to the specific physical effects governing the microworld. Interactive approaches using only visual feedback are limited to the 2D image of the microscope, and have forcibly lower bandwidth. Recently, haptic feedback teleoperation systems have been developed to try to overcome those difficulties. This paper explores the case of an optical tweezers platform coupled to an haptic device providing transparent force feedback. The impact of haptic feedback regarding user dexterity on tactile exploration tasks is studied using 3 μm microbeads and a test bench with micro sized shapes. The results reveal a consistent improvement in both users' trajectory tracking and their control of the contact forces. This also validates the experimental setup which performed reliably on 140 different trials of the evaluation.


Title: Maneuver at Micro Scale: Steering by Actuation Frequency Control in Micro Bristle Robots*
Key Words: microactuators  microrobots  piezoelectric actuators  steering systems  vibrations  resonance-based steering mechanism  differential steering  on-board piezoelectric actuator  frequency-controlled locomotion  steering mechanism  microbristle robots  actuation frequency control  principal actuation frequency components  size 6.0 mm  size 400.0 mum  size 12.0 mm  size 8.0 mm  Robots  Resonant frequency  Actuators  Vibrations  Steady-state  Wires  Mathematical model 
Abstract: This paper presents a novel steering mechanism, which leads to frequency-controlled locomotion demonstrated for the first time in micro bristle robots. The miniaturized robots are 3D-printed, 12 mm × 8 mm × 6 mm in size, with bristle feature sizes down to 400 μm. The robots can be steered by utilizing the distinct resonance behaviors of the asymmetrical bristle sets. The left and right sets of the bristles have different diameters, and thus different stiffnesses and resonant frequencies. The unique response of each bristle side to the vertical vibrations of a single on-board piezoelectric actuator causes differential steering of the robot. The robot can be modeled as two coupled uniform bristle robots, representing the left and the right sides. At distinct frequencies, the robots can move in all four principal directions: forward, backward, left and right. Furthermore, the full 360° 2D plane can be covered by superimposing the principal actuation frequency components with desired amplitudes. In addition to miniaturized robots, the presented resonance-based steering mechanism can be applied over multiple scales and to other mechanical systems.


Title: Scaling down an insect-size microrobot, HAMR-VI into HAMR-Jr
Key Words: gait analysis  legged locomotion  microrobots  motion control  insect-size microrobot  mechanically dexterous legged robot  HAMR-Jr's open-loop locomotion  HAMR-VI microrobot  design process  fabrication process  independently actuated degrees of freedom  mass 320.0 mg  frequency 1.0 Hz to 200.0 Hz  size 22.5 mm  Legged locomotion  Actuators  Resonant frequency  Heat-assisted magnetic recording  Fabrication  Robot sensing systems 
Abstract: Here we present HAMR-Jr, a 22.5mm, 320mg quadrupedal microrobot. With eight independently actuated degrees of freedom, HAMR-Jr is, to our knowledge, the most mechanically dexterous legged robot at its scale and is capable of high-speed locomotion (13.91bodylengthss-1) at a variety of stride frequencies (1-200Hz) using multiple gaits. We achieved this using a design and fabrication process that is flexible, allowing scaling with minimum changes to our workflow. We further characterized HAMR-Jr's open-loop locomotion and compared it with the larger scale HAMR-VI microrobot to demonstrate the effectiveness of scaling laws in predicting running performance.


Title: Reality as a simulation of reality: robot illusions, fundamental limits, and a physical demonstration
Key Words: collision avoidance  human-robot interaction  mobile robots  multi-robot systems  fundamental limits  physical demonstration  robot behavior  potential mismatches  convincing illusion  system simulation  simulated systems  simple multirobot experiment  robot navigating  robot illusions  Robot sensing systems  Software  Sensor systems  Mobile robots  Emulation 
Abstract: We consider problems in which robots conspire to present a view of the world that differs from reality. The inquiry is motivated by the problem of validating robot behavior physically despite there being a discrepancy between the robots we have at hand and those we wish to study, or the environment for testing that is available versus that which is desired, or other potential mismatches in this vein. After formulating the concept of a convincing illusion, essentially a notion of system simulation that takes place in the real world, we examine the implications of this type of simulability in terms of infrastructure requirements. Time is one important resource: some robots may be able to simulate some others but, perhaps, only at a rate that is slower than real-time. This difference gives a way of relating the simulating and the simulated systems in a form that is relative. We establish some theorems, including one with the flavor of an impossibility result, and providing several examples throughout. Finally, we present data from a simple multi-robot experiment based on this theory, with a robot navigating amid an unbounded field of obstacles."Truth is beautiful, without doubt; but so are lies."-Ralph Waldo Emerson.


Title: Finding Missing Skills for High-Level Behaviors
Key Words: robots  temporal logic  KUKA IIWA arm  Baxter robot  LTL specifications  correct-by-construction robot control  LTL synthesis  high-level robot tasks  linear temporal logic  Task analysis  Maintenance engineering  Games  Robot control  Manipulators  Safety 
Abstract: Recently, Linear Temporal Logic (LTL) has been used as a formalism for defining high-level robot tasks, and LTL synthesis has been used to automatically create correct-by-construction robot control. The underlying premise of this approach is that the robot has a set of actions, or skills, that can be composed to achieve the high- level task. In this paper we consider LTL specifications that cannot be synthesized into robot control due to lack of appropriate skills; we present algorithms for automatically suggesting new or modified skills for the robot that will guarantee the task will be achieved. We demonstrate our approach with a physical Baxter robot and a simulated KUKA IIWA arm.


Title: Near-Optimal Reactive Synthesis Incorporating Runtime Information
Key Words: control system synthesis  mobile robots  optimisation  path planning  suboptimal control  mission specification  dynamic environment  performance metric  task-critical information  strategy synthesis  time-varying information  online re-synthesis  pre-specified representative information scenarios  performance suboptimality  runtime information  near-optimal reactive synthesis  switching mechanism  robotic motion planning  Runtime  Switches  Games  Robots  Measurement  Safety  Planning 
Abstract: We consider the problem of optimal reactive synthesis - compute a strategy that satisfies a mission specification in a dynamic environment, and optimizes a given performance metric. We incorporate task-critical information, that is only available at runtime, into the strategy synthesis in order to improve performance. Existing approaches to utilising such time-varying information require online re-synthesis, which is not computationally feasible in real-time applications. In this paper, we presynthesize a set of strategies corresponding to candidate instantiations (pre-specified representative information scenarios). We then propose a novel switching mechanism to dynamically switch between the strategies at runtime while guaranteeing all safety and liveness goals are met. We also characterize bounds on the performance suboptimality. We demonstrate our approach on two examples - robotic motion planning where the likelihood of the position of the robot's goal is updated in real-time, and an air traffic management problem for urban air mobility.


Title: Control Synthesis from Linear Temporal Logic Specifications using Model-Free Reinforcement Learning
Key Words: control system synthesis  decision theory  learning (artificial intelligence)  Markov processes  mobile robots  path planning  probability  temporal logic  motion planning  MDP  RL-based synthesis approach  discount factors  model-free RL algorithm  total discounted reward  optimal policy  transition probabilities  LTL formula  Markov decision process  unknown stochastic environment  control policy synthesis  reinforcement learning frame-work  model-free reinforcement learning  linear temporal logic specifications  control synthesis  Learning (artificial intelligence)  Automata  Planning  Markov processes  Computational modeling  Task analysis 
Abstract: We present a reinforcement learning (RL) frame-work to synthesize a control policy from a given linear temporal logic (LTL) specification in an unknown stochastic environment that can be modeled as a Markov Decision Process (MDP). Specifically, we learn a policy that maximizes the probability of satisfying the LTL formula without learning the transition probabilities. We introduce a novel rewarding and discounting mechanism based on the LTL formula such that (i) an optimal policy maximizing the total discounted reward effectively maximizes the probabilities of satisfying LTL objectives, and (ii) a model-free RL algorithm using these rewards and discount factors is guaranteed to converge to such a policy. Finally, we illustrate the applicability of our RL-based synthesis approach on two motion planning case studies.


Title: R-Min: a Fast Collaborative Underactuated Parallel Robot for Pick-and-Place Operations
Key Words: collision avoidance  end effectors  human-robot interaction  springs (mechanical)  trajectory control  parallel manipulator  pick-and-place operations  human operator  acceleration  planar five-bar mechanism  passive joints  planar seven-bar mechanism  supplementary passive leg  collaborative parallel robot  pick-and-place trajectory  R-Min  collaborative underactuated parallel robot  tension spring  end-effector  degrees of freedom  impact force  Collision avoidance  Collaboration  Prototypes  Parallel robots  Robot sensing systems  Springs 
Abstract: This paper introduces an intrinsically safe parallel manipulator dedicated to fast pick-and-place operations, called R-Min. It has been designed to reduce the risk of injury during a collision with a human operator, while maintaining high speed and acceleration. The proposed architecture is based on a modification of the well-known planar five-bar mechanism, where additional passive joints are introduced to the distal links in order to create a planar seven-bar mechanism with two degrees of underactuation, so that it can passively reconfigure in case of collision. A supplementary passive leg, in which a tension spring is mounted, is added between the base and the end-effector in order to constrain the additional degrees of freedom. A prototype of this new collaborative parallel robot is designed and its equilibrium configurations under several types of loadings are analyzed. Its dynamics is also studied. We analyze the impact force occurring during a collision between our prototype and the head of an operator and compare these results with those that would have been obtained with a rigid five-bar mechanism. Simulation results of impact during a standard pick-and-place trajectory of duration 0.3 s show that a regular five-bar mechanism would injure a human, while our robot would avoid the trauma.


Title: High-Flexibility Locomotion and Whole-Torso Control for a Wheel-Legged Robot on Challenging Terrain*
Key Words: mobile robots  motion control  robot kinematics  wheels  high-flexibility locomotion  whole-torso control  challenging terrain  six-wheel-legged robot  irregular terrain  heavy-duty work  Stewart platforms  wheels  diverse degrees  traversability  rough terrain  sand-gravel terrain  parallel suspension system  Legged locomotion  Wheels  Torso  Force  Kinematics 
Abstract: In this paper, we propose a parallel six-wheel-legged robot that can traverse irregular terrain while carrying objectives to do heavy-duty work. This robot is equipped with six Stewart platforms as legs and tightly integrates the additional degrees of freedom introduced by the wheels. The presented control strategy with physical system used to adapt the diverse degrees of each leg to irregular terrain such that robot increases the traversability, and simultaneously to maintain the horizontal whole-torso pose. This strategy makes use of Contact Scheduler (CS) and Whole-Torso Control (WTC) to control the multiple degrees of freedom (DOF) leg for performing high-flexibility locomotion and adapting the rough terrain like actively parallel suspension system. We conducted experiments on flat, slope, soft and sand-gravel surface, which validate the proposed control method and physical system. Especially, we attempt to traverse over sand-gravel terrain with 3 people about 240kg payload.


Title: The Prince’s tears, a large cable-driven parallel robot for an artistic exhibition
Key Words: exhibitions  mobile robots  trajectory control  cable length estimation  robot position  glass powder  cable-driven parallel robot  artistic exhibition  positioning control  CDPR geometry  exhibition place  Prince tears  time 3.0 d  time 174.0 hour  time 32.0 d  mass 1.5 ton  Meters  Length measurement  Winches  Powders  Kinematics  Robot kinematics  cable-driven parallel robot  kinematics  art 
Abstract: This paper presents the development and results of a large 3 d.o.f cable-driven parallel robot (CDPR) that has been extensively used between June and August 2019 for an artistic exhibition. The purpose of the exhibition was to 3D print a wall of glass powder, which will slowly collapse after the deposit of each layer. Positioning control on the assigned trajectory was an issue because of the CDPR geometry imposed by the specific configuration of the exhibition place. We describe how this problem was solved using a combination of cable length estimation based on the winch rotation measured by encoder, together with 3 on-board lidars that were used to provide a measure of the robot position. To the best of our knowledge this is the first time that such method was used for controlling a large CDPR. This CDPR has run for 174 hours since 6/18/2019, averaging a run time of 4h15mn per day. The 3D printing of the wall started on 7/18/2019 and stops on 8/31/2019. During this period the robot was used for 32 days with an average of 2h18mn run-time per day. The robot has traveled on a total distance of 4757 meters, of which 3893 meters on the assigned trajectory. During the period 76 layers have been deposited, representing a mass of 1.5 tons of glass powder.


Title: Singularity analysis and reconfiguration mode of the 3-CRS parallel manipulator
Key Words: manipulator kinematics  reconfiguration mode  3-CRS parallel manipulator  original parallel mechanism  motorized cylindrical joint  parallel robotics community  dimensional synthesis  geometric approach  relative geometric configurations  singularity analysis problem  Manipulators  Kinematics  Transmission line matrix methods  Force  Geometry  Prototypes 
Abstract: The 3-CRS manipulator is an original parallel mechanism having 6 degrees of freedom (DOFs) with only 3 limbs. This mechanism uses a motorized cylindrical joint per limb. This new paradigm of actuation opens research fields on new families of robots that should particularly interest the parallel robotics community. According to its dimensional synthesis, this mechanism can have remarkable kinematic properties such as a large orientation workspace or reconfiguration capabilities. In this paper, we introduce this mechanism and we study its singularities by using a geometric approach. This approach simplifies considerably singularity analysis problem by considering the relative geometric configurations of three planes defined by the distal links of the limbs. Thanks to that, a reconfiguration mode of the 3-CRS, that doubles its reachable workspace, is highlighted. This property is illustrated on a physical prototype of the robot.


Title: Trajectory optimization for a class of robots belonging to Constrained Collaborative Mobile Agents (CCMA) family
Key Words: actuators  collision avoidance  end effectors  manipulator kinematics  mobile robots  multi-robot systems  optimisation  position control  constrained collaborative mobile agents family  ground mobile bases  mobile robots  closed-loop kinematic chains  revolute joints  closed- loop kinematic chains  standalone trajectory optimization method  CCMA system  fixed design parameters  control policy optimization  manipulation capabilities  tracked mobile bases  Kinematics  Mobile robots  Trajectory optimization  Mobile agents  Task analysis  Parallel Robots  Optimization and Optimal Control  Multi-Robot Systems 
Abstract: We present a novel class of robots belonging to Constrained Collaborative Mobile Agents (CCMA) family which consists of ground mobile bases with non-holonomic constraints. Moreover, these mobile robots are constrained by closed-loop kinematic chains consisting of revolute joints which can be either passive or actuated. We also describe a novel trajectory optimization method which is general with respect to number of mobile robots, topology of the closed- loop kinematic chains and placement of the actuators at the revolute joints. We also extend the standalone trajectory optimization method to optimize concurrently the design parameters and the control policy. We describe various CCMA system examples, in simulation, differing in design, topology, number of mobile robots and actuation space. The simulation results for standalone trajectory optimization with fixed design parameters is presented for CCMA system examples. We also show how this method can be used for tasks other than end-effector positioning such as internal collision avoidance and external obstacle avoidance. The concurrent design and control policy optimization is demonstrated, in simulations, to increase the CCMA system workspace and manipulation capabilities. Finally, the trajectory optimization method is validated in experiments through two 4-DOF prototypes consisting of 3 tracked mobile bases.


Title: Development of Body Rotational Wheeled Robot and its Verification of Effectiveness
Key Words: collision avoidance  friction  mobile robots  traction  wheels  step-obstacle climbing  body rotational wheeled robot  scattered obstacles  driving environment  step-type obstacle  main body rotation mechanism  robot wheels  wheel-drive robot  body mass  slope traveling  downhill wheel  mechanical effect  robot platform  Mobile robots  Wheels  Force  Gears  Friction  Robot sensing systems 
Abstract: A wheeled robot operating on various terrains such as scattered obstacles and slopes is required to cope with and overcome the driving environment. In this paper, in order to overcome a step-type obstacle and to steadily ascend on the slope, the main body rotation mechanism, which controls the load distribution on the robot wheels was proposed for a wheel-drive robot. By rotating the center of the body mass, the friction/traction force required for climbing step obstacles can be reduced. In the case of slope traveling, the slip can be suppressed, and the traveling ability improved by controlling the load distribution excessively increased on the downhill wheel due to the attitude change of the robot's body. The mechanical effect of the proposed body rotation mechanism was analyzed. In addition, based on the design and manufacture of the robot platform, the effectiveness of the proposed mechanism was convincingly demonstrated by indoor test for step-obstacle climbing and slope-traveling.


Title: Radar Sensors in Collaborative Robotics: Fast Simulation and Experimental Validation
Key Words: collision avoidance  CW radar  FM radar  frequency modulation  image sensors  learning (artificial intelligence)  mobile robots  object detection  radar computing  road vehicle radar  sensors  collaborative robotics  radar systems  robot systems  optimization  machine learning approaches  realistic simulation models  radar sensor simulations  relative velocities  Lambertian reflectance model  reflection estimates  frequency modulated continuous wave radar  simulation environments  Radar  Robot sensing systems  Radar antennas  Chirp  Computational modeling 
Abstract: With the availability of small system in package realizations, radar systems become more and more attractive for a variety of applications in robotics, in particular also for collaborative robotics. As the simulation of robot systems in realistic scenarios has become an important tool, not only for design and optimization, but also e.g. for machine learning approaches, realistic simulation models are needed. In the case of radar sensor simulations, this means providing more realistic results than simple proximity sensors, e.g. in the presence of multiple objects and/or humans, objects with different relative velocities and differentiation between background and foreground movement. Due to the short wavelength in the millimeter range, we propose to utilize methods known from computer graphics (e.g. z-buffer, Lambertian reflectance model) to quickly acquire depth images and reflection estimates. This information is used to calculate an estimate of the received signal for a Frequency Modulated Continuous Wave (FMCW) radar by superposition of the corresponding signal contributions. Due to the moderate computational complexity, the approach can be used with various simulation environments such as V-Rep or Gazebo. Validity and benefits of the approach are demonstrated by means of a comparison with experimental data obtained with a radar sensor on a UR10 arm in different scenarios.


Title: Transferable Task Execution from Pixels through Deep Planning Domain Learning
Key Words: learning systems  manipulators  neurocontrollers  robot vision  symbolic operators  manipulation tasks  transferable task execution  visual input  symbolic planning methods  partially-observable world  hierarchical model  high-level model  deep planning domain learning  symbolic world state  DPDL  STRIPS  logical predicates  low-level policy learning  photorealistic kitchen scenario  Task analysis  Planning  Robot sensing systems  Grounding  Robustness  Feature extraction 
Abstract: While robots can learn models to solve many manipulation tasks from raw visual input, they cannot usually use these models to solve new problems. On the other hand, symbolic planning methods such as STRIPS have long been able to solve new problems given only a domain definition and a symbolic goal, but these approaches often struggle on the real world robotic tasks due to the challenges of grounding these symbols from sensor data in a partially-observable world. We propose Deep Planning Domain Learning (DPDL), an approach that combines the strengths of both methods to learn a hierarchical model. DPDL learns a high-level model which predicts values for a large set of logical predicates consisting of the current symbolic world state, and separately learns a low-level policy which translates symbolic operators into executable actions on the robot. This allows us to perform complex, multistep tasks even when the robot has not been explicitly trained on them. We show our method on manipulation tasks in a photorealistic kitchen scenario.


Title: Depth by Poking: Learning to Estimate Depth from Self-Supervised Grasping
Key Words: end effectors  image colour analysis  manipulator kinematics  mean square error methods  mobile robots  neural nets  optical radar  robot vision  unsupervised learning  robotic manipulation  neural network  RGB-D images  physical interactions  autonomous grasping policy  end effector position labels  forward kinematics  manipulation systems  structured light sensors  unsupervised deep learning  self-supervised grasping  depth estimation  LiDAR sensors  root mean squared error  Estimation  Uncertainty  Robot sensing systems  Training  Grasping 
Abstract: Accurate depth estimation remains an open problem for robotic manipulation; even state of the art techniques including structured light and LiDAR sensors fail on reflective or transparent surfaces. We address this problem by training a neural network model to estimate depth from RGB-D images, using labels from physical interactions between a robot and its environment. Our network predicts, for each pixel in an input image, the z position that a robot's end effector would reach if it attempted to grasp or poke at the corresponding position. Given an autonomous grasping policy, our approach is self-supervised as end effector position labels can be recovered through forward kinematics, without human annotation. Although gathering such physical interaction data is expensive, it is necessary for training and routine operation of state of the art manipulation systems. Therefore, this depth estimator comes for free while collecting data for other tasks (e.g., grasping, pushing, placing). We show our approach achieves significantly lower root mean squared error than traditional structured light sensors and unsupervised deep learning methods on dif cult, industry-scale jumbled bin datasets.


Title: Online Learning of Object Representations by Appearance Space Feature Alignment
Key Words: image representation  learning (artificial intelligence)  online learning  object representations  appearance space feature alignment  monocular videos  self-supervised model  OCN  leverage self-supervision  online adaptation  online model  object identification error  offline baseline  robotic pointing task  object adaptation  object-contrastive network  Videos  Robots  Training  Measurement  Object recognition  Video sequences  Task analysis 
Abstract: We propose a self-supervised approach for learning representations of objects from monocular videos and demonstrate it is particularly useful for robotics. The main contributions of this paper are: 1) a self-supervised model called Object-Contrastive Network (OCN) that can discover and disentangle object attributes from video without using any labels; 2) we leverage self-supervision for online adaptation: the longer our online model looks at objects in a video, the lower the object identification error, while the offline baseline remains with a large fixed error; 3) we show the usefulness of our approach for a robotic pointing task; a robot can point to objects similar to the one presented in front of it. Videos illustrating online object adaptation and robotic pointing are provided as supplementary material.


Title: Visual Prediction of Priors for Articulated Object Interaction
Key Words: feature extraction  intelligent robots  mobile robots  object detection  visual servoing  exploratory behavior  visual features learning  contextual multiarmed bandit  parameterized action space  articulated object interaction  contextual prior prediction  Robots  Visualization  Training  Kinematics  Gaussian processes  Optimization  Kernel 
Abstract: Exploration in novel settings can be challenging without prior experience in similar domains. However, humans are able to build on prior experience quickly and efficiently. Children exhibit this behavior when playing with toys. For example, given a toy with a yellow and blue door, a child will explore with no clear objective, but once they have discovered how to open the yellow door, they will most likely be able to open the blue door much faster. Adults also exhibit this behaviour when entering new spaces such as kitchens. We develop a method, Contextual Prior Prediction, which provides a means of transferring knowledge between interactions in similar domains through vision. We develop agents that exhibit exploratory behavior with increasing efficiency, by learning visual features that are shared across environments, and how they correlate to actions. Our problem is formulated as a Contextual Multi-Armed Bandit where the contexts are images, and the robot has access to a parameterized action space. Given a novel object, the objective is to maximize reward with few interactions. A domain which strongly exhibits correlations between visual features and motion is kinemetically constrained mechanisms. We evaluate our method on simulated prismatic and revolute joints1.


Title: MT-DSSD: Deconvolutional Single Shot Detector Using Multi Task Learning for Object Detection, Segmentation, and Grasping Detection
Key Words: convolutional neural nets  image segmentation  learning (artificial intelligence)  manipulators  object detection  MT-DSSD  object detection  semantic object segmentation  grasping point detection  multitask learning  grasping operation  multitask deconvolutional single shot detector  robot manipulation  Amazon Robotics Challenge dataset  Grasping  Robots  Object detection  Semantics  Task analysis  Deconvolution  Feature extraction 
Abstract: This paper presents the multi-task Deconvolutional Single Shot Detector (MT-DSSD), which runs three tasks-object detection, semantic object segmentation, and grasping detection for a suction cup-in a single network based on the DSSD. Simultaneous execution of object detection and segmentation by multi-task learning improves the accuracy of these two tasks. Additionally, the model detects grasping points and performs the three recognition tasks necessary for robot manipulation. The proposed model can perform fast inference, which reduces the time required for grasping operation. Evaluations using the Amazon Robotics Challenge (ARC) dataset showed that our model has better object detection and segmentation performance than comparable methods, and robotic experiments for grasping show that our model can detect the appropriate grasping point.


Title: Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object Grasping
Key Words: convolutional neural nets  dexterous manipulators  image segmentation  learning (artificial intelligence)  mobile robots  object recognition  path planning  robot vision  shape recognition  synthetic data  deep networks  primitive shape  object grasping  segmentation-based architecture  monocular depth input  backbone deep network  parametrized grasp families  shape primitive region  task-free grasping  shape primitives  task-relevant grasp prediction  ranking algorithm  task-free grasp prediction  Shape  Grasping  Image segmentation  Three-dimensional displays  Task analysis  Robot sensing systems 
Abstract: A segmentation-based architecture is proposed to decompose objects into multiple primitive shapes from monocular depth input for robotic manipulation. The backbone deep network is trained on synthetic data with 6 classes of primitive shapes generated by a simulation engine. Each primitive shape is designed with parametrized grasp families, permitting the pipeline to identify multiple grasp candidates per shape primitive region. The grasps are priority ordered via proposed ranking algorithm, with the first feasible one chosen for execution. On task-free grasping of individual objects, the method achieves a 94% success rate. On task-oriented grasping, it achieves a 76% success rate. Overall, the method supports the hypothesis that shape primitives can support task-free and task-relevant grasp prediction.


Title: Stillleben: Realistic Scene Synthesis for Deep Learning in Robotics
Key Words: cameras  image segmentation  iterative methods  learning (artificial intelligence)  neural nets  pose estimation  rendering (computer graphics)  robot vision  realistic scene synthesis  robotics  training data  deep learning  synthesis pipeline  cluttered scene perception tasks  semantic segmentation  object detection  physically realistic scenes  high-quality rasterization  material parameters  camera sensors  deep neural network  training frames  iterative render-and-compare approaches  YCB-Video dataset  Stillleben  Training  Rendering (computer graphics)  Robots  Engines  Pipelines  Task analysis  Cameras 
Abstract: Training data is the key ingredient for deep learning approaches, but difficult to obtain for the specialized domains often encountered in robotics. We describe a synthesis pipeline capable of producing training data for cluttered scene perception tasks such as semantic segmentation, object detection, and correspondence or pose estimation. Our approach arranges object meshes in physically realistic, dense scenes using physics simulation. The arranged scenes are rendered using high-quality rasterization with randomized appearance and material parameters. Noise and other transformations introduced by the camera sensors are simulated. Our pipeline can be run online during training of a deep neural network, yielding applications in life-long learning and in iterative render-and-compare approaches. We demonstrate the usability by learning semantic segmentation on the challenging YCB-Video dataset without actually using any training frames, where our method achieves performance comparable to a conventionally trained model. Additionally, we show successful application in a real-world regrasping system.


Title: A Generative Approach Towards Improved Robotic Detection of Marine Litter
Key Words: image classification  learning (artificial intelligence)  object detection  support vector machines  data scarcity problems  underwater image datasets  visual detection  marine debris  two-stage variational autoencoder  generated imagery  two-stage VAE  binary classifier  multiclass classifier  augmentation process  trash images  underwater trash classification problem  data-dependent task  quality images  Training  Image color analysis  Plastics  Gallium nitride  Task analysis  Shape  Decoding 
Abstract: This paper presents an approach to address data scarcity problems in underwater image datasets for visual detection of marine debris. The proposed approach relies on a two-stage variational autoencoder (VAE) and a binary classifier to evaluate the generated imagery for quality and realism. From the images generated by the two-stage VAE, the binary classifier selects "good quality" images and augments the given dataset with them. Lastly, a multi-class classifier is used to evaluate the impact of the augmentation process by measuring the accuracy of an object detector trained on combinations of real and generated trash images. Our results show that the classifier trained with the augmented data outperforms the one trained only with the real data. This approach will not only be valid for the underwater trash classification problem presented in this paper, but it will also be useful for any data-dependent task for which collecting more images is challenging or infeasible.


Title: Spatiotemporal Representation Learning with GAN Trained LSTM-LSTM Networks
Key Words: convolutional neural nets  learning (artificial intelligence)  learning systems  robots  unstructured environments  unsupervised representation learning architecture  underlying representation  high-dimensional raw video inputs  spatiotemporal representation learning  lower-dimensional latent space  two-stage learning approach  convolutional neural network  Long Short-Term Network  LSTM-LSTM cells  hierarchical representation learning  low-dimensional representation  video prediction task  GAN trained LSTM-LSTM networks  robot behavior learning  layered spatiotemporal memory long short-term memory  generative adversarial network  ConvNet  Spatiotemporal phenomena  Gallium nitride  Task analysis  Training  Generative adversarial networks  Robots  Feature extraction 
Abstract: Learning robot behaviors in unstructured environments often requires handcrafting the features for a given task. In this paper, we present and evaluate an unsupervised representation learning architecture, Layered Spatiotemporal Memory Long Short-Term Memory (LSTM-LSTM), that learns the underlying representation without knowledge of the task. The goal of this architecture is to learn the dynamics of the environment from high-dimensional raw video inputs. Using a Generative Adversarial Network (GAN) framework with the proposed network, this architecture is able to learn a spatiotemporal representation in its lower-dimensional latent space directly from raw input sequences. We show that our approach learns the spatial and temporal information simultaneously as opposed to a two-stage learning approach of alternating between training a Convolutional Neural Network (ConvNet) and a Long Short-Term Network (LSTM). Furthermore, by using LSTM-LSTM cells that shrink in size with the increase in the number of layers, the network learns a hierarchical representation with a low-dimensional representation at the top layer. We show that this architecture achieves state-of-the-art results with a substantially lower-dimensional representation than existing methods. We evaluate our approach on a video prediction task with standard benchmark datasets like Moving MNIST and KTH Action, as well as a simulated robot dataset.


Title: Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects
Key Words: backpropagation  graph theory  neural nets  robot programming  groups of articulated objects  learning action effects  complex robotic systems  graph neural networks  Belief Regulated Dual Propagation nets  object interaction  object trajectory level  belief regulator  physics predictor  PropNets  general-purpose learnable physics engine  BRDPN  robotics domain  Robots  Physics  Engines  Predictive models  History  Neural networks  Trajectory 
Abstract: In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot's knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online.


Title: Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory Predictions
Key Words: intelligent transportation systems  learning (artificial intelligence)  mobile robots  neural nets  road safety  road traffic  road vehicles  traffic engineering computing  trajectory control  vehicle dynamics  deep learning  deep convnets  deep kinematic models  kinematically feasible vehicle trajectory predictions  self driving vehicles  traffic safety  autonomous technology  kinematically feasible motion prediction  vehicle kinematics  physically grounded vehicle motion models  Predictive models  Kinematics  Trajectory  Hidden Markov models  Radar tracking  Data models  Interpolation 
Abstract: Self-driving vehicles (SDVs) hold great potential for improving traffic safety and are poised to positively affect the quality of life of millions of people. To unlock this potential one of the critical aspects of the autonomous technology is understanding and predicting future movement of vehicles surrounding the SDV. This work presents a deep-learning- based method for kinematically feasible motion prediction of such traffic actors. Previous work did not explicitly encode vehicle kinematics and instead relied on the models to learn the constraints directly from the data, potentially resulting in kinematically infeasible, suboptimal trajectory predictions. To address this issue we propose a method that seamlessly combines ideas from the AI with physically grounded vehicle motion models. In this way we employ best of the both worlds, coupling powerful learning models with strong feasibility guarantees for their outputs. The proposed approach is general, being applicable to any type of learning method. Extensive experiments using deep convnets on real-world data strongly indicate its benefits, outperforming the existing state-of-the-art.


Title: Human Driver Behavior Prediction based on UrbanFlow*
Key Words: decision making  driver information systems  mobile robots  road safety  road traffic  road vehicles  traffic engineering computing  transportation  human driver behavior prediction  public transportation systems  fully automatic transportation environments  autonomous vehicle decision making  planning  LSTM-based trajectory prediction method  urban scenario  Trajectory  Autonomous vehicles  Data collection  Automobiles  Roads  Drones 
Abstract: How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Understanding human drivers' behavior can be beneficial for autonomous vehicle decision making and planning, especially when the autonomous vehicle is surrounded by human drivers who have various driving behaviors and patterns of interaction with other vehicles. In this paper, we propose an LSTM-based trajectory prediction method for human drivers which can help the autonomous vehicle make better decisions, especially in urban intersection scenarios. Meanwhile, in order to collect human drivers' driving behavior data in the urban scenario, we describe a system called UrbanFlow which includes the whole procedure from raw bird's-eye view data collection via drone to the final processed trajectories. The system is mainly intended for urban scenarios but can be extended to be used for any traffic scenarios.


Title: Environment Prediction from Sparse Samples for Robotic Information Gathering
Key Words: handwritten character recognition  learning (artificial intelligence)  mobile robots  neural net architecture  robot vision  environment prediction  sparse samples  robotics applications  neural network architecture  spatially correlated data fields  spatially continuous samples  biased loss functions  reconstruction error  robotic information gathering trials  MNIST hand written digits dataset  ocean monitoring  regional ocean modeling system ocean dataset  ROMS  Robots  Data models  Oceans  Neural networks  Convolution  Network architecture  Logic gates 
Abstract: Robots often require a model of their environment to make informed decisions. In unknown environments, the ability to infer the value of a data field from a limited number of samples is essential to many robotics applications. In this work, we propose a neural network architecture to model these spatially correlated data fields based on a limited number of spatially continuous samples. Additionally, we provide a method based on biased loss functions to suggest future areas of exploration to minimize reconstruction error. We run simulated robotic information gathering trials on both the MNIST hand written digits dataset and a Regional Ocean Modeling System (ROMS) ocean dataset for ocean monitoring. Our method outperforms Gaussian process regression in both environments for modeling the data field and action selection.


Title: Predicting Pushing Action Effects on Spatial Object Relations by Learning Internal Prediction Models
Key Words: graph theory  humanoid robots  learning (artificial intelligence)  manipulators  spatial object relations  learning internal prediction models  robot tasks  possible action consequences  action parameters  desired goal states  parametrizing pushing actions  high-level planner  object-centric graphs  synthetic data set  goal state  possible pushing action candidates  high prediction accuracy  humanoid robot ARMAR-6  learned internal model  pushing action effects  Predictive models  Two dimensional displays  Robots  Data models  Physics  Three-dimensional displays  Analytical models 
Abstract: Understanding the effects of actions is essential for planning and executing robot tasks. By imagining possible action consequences, a robot can choose specific action parameters to achieve desired goal states. We present an approach for parametrizing pushing actions based on learning internal prediction models. These pushing actions must fulfill constraints given by a high-level planner, e. g., after the push the brown box must be to the right of the orange box. In this work, we represent the perceived scenes as object-centric graphs and learn an internal model, which predicts object pose changes due to pushing actions. We train this internal model on a large synthetic data set, which was generated in simulation, and record a smaller data set on the real robot for evaluation. For a given scene and goal state, the robot generates a set of possible pushing action candidates by sampling the parameter space and then evaluating the candidates by internal simulation, i. e., by comparing the predicted effect resulting from the internal model with the desired effect provided by the high-level planner. In the evaluation, we show that our model achieves high prediction accuracy in scenes with a varying number of objects and, in contrast to state-of-the-art approaches, is able to generalize to scenes with more objects than seen during training. In experiments on the humanoid robot ARMAR-6, we validate the transfer from simulation and show that the learned internal model can be used to manipulate scenes into desired states effectively.


Title: Learning of Key Pose Evaluation for Efficient Multi-contact Motion Planner
Key Words: humanoid robots  learning (artificial intelligence)  legged locomotion  motion control  neural nets  path planning  pose estimation  robot vision  robust control  transfer functions  locomotion  uneven terrain  multicontact motion planning  pose evaluation  neural network  activation function  robust robotics system  humanoid robots  deep learning  depth image  Planning  Trajectory  Torque  Legged locomotion  Jacobian matrices  Knee  Collision avoidance 
Abstract: It is necessary to use not only foot but also hand, knee and other body parts to support body weight for locomotion in uneven terrain. Such multi-contact motion planning is an important research topic including lots of previous works; however, a problem of computational speed of planning is still remaining. In this paper, we propose a learning-based algorithm to speed up the planning. The algorithm reduces replanning of contact states by learning an evaluation function of key pose to reach goal. We investigated the learning performance by comparing three neural network configurations and two activation function. This research aims at achieving robust robotics system in unknown environments.


Title: Differentiable Gaussian Process Motion Planning
Key Words: collision avoidance  Gaussian processes  learning (artificial intelligence)  mobile robots  motion control  optimisation  trajectory optimization  robotics tasks  trajectory optimization algorithms  differentiable extension  GPMP2 algorithm  learning-based approach  Gaussian process motion planning algorithm  motion planning  Planning  Conferences  Automation  Gaussian processes  Artificial intelligence  Trajectory optimization  Robots 
Abstract: Modern trajectory optimization based approaches to motion planning are fast, easy to implement, and effective on a wide range of robotics tasks. However, trajectory optimization algorithms have parameters that are typically set in advance (and rarely discussed in detail). Setting these parameters properly can have a significant impact on the practical performance of the algorithm, sometimes making the difference between finding a feasible plan or failing at the task entirely. We propose a method for leveraging past experience to learn how to automatically adapt the parameters of Gaussian Process Motion Planning (GPMP) algorithms. Specifically, we propose a differentiable extension to the GPMP2 algorithm, so that it can be trained end-to-end from data. We perform several experiments that validate our algorithm and illustrate the benefits of our proposed learning-based approach to motion planning.


Title: Learn and Link: Learning Critical Regions for Efficient Planning
Key Words: convolutional neural nets  learning (artificial intelligence)  neurocontrollers  path planning  sampling methods  convolutional neural networks  sampling-based planners  planning time  motion planning problems  open motion planning library  sampling-based algorithms  uniform sampling  sampling-based motion planners  Planning  Probabilistic logic  Robots  Density measurement  Task analysis  Buildings  Convolutional neural networks 
Abstract: This paper presents a new approach to learning for motion planning (MP) where critical regions of an environment are learned from a given set of motion plans and used to improve performance on new environments and problem instances. We introduce a new suite of sampling-based motion planners, Learn and Link. Our planners leverage critical regions to overcome the limitations of uniform sampling, while still maintaining guarantees of correctness inherent to sampling-based algorithms. We also show that convolutional neural networks (CNNs) can be used to identify critical regions for motion planning problems. We evaluate Learn and Link against planners from the Open Motion Planning Library (OMPL) using an extensive suite of experiments on challenging motion planning problems. We show that our approach requires far less planning time than existing sampling-based planners.


Title: Pose-Estimate-Based Target Tracking for Human-Guided Remote Sensor Mounting with a UAV
Key Words: autonomous aerial vehicles  image sequences  pose estimation  SLAM (robots)  target tracking  pose-estimate-based target tracking  human-guided remote sensor mounting  autonomous aerial manipulation  unstructured environments  UAV localization  PBTT method  target point  fully on-board computation  RGB-D camera  downward-facing optical flow camera  horizontal localization  autonomous flight tests  interacting-boomcopter UAV platform  UAV position estimator  Target tracking  Cameras  Unmanned aerial vehicles  Visualization  Task analysis  Surface cleaning  Three-dimensional displays 
Abstract: In this paper, we present a method for pose-estimate-based target tracking (PBTT) that enables the performance of autonomous aerial manipulation operations in unstructured environments using fully on-board computation for both UAV localization and target tracking. The PBTT method does not depend on extracting traditional visual features (e.g. using SIFT, SURF, ORB, etc.) on or near the target. Instead, the algorithm combines input from an RGB-D camera and the UAV's position estimator (which utilizes a downward-facing optical flow camera for horizontal localization) to track a target point selected by a human operator. The effectiveness of the PBTT method is evaluated through several autonomous flight tests performed with the Interacting-Boomcopter (I-BC) UAV platform in unstructured environments and in the presence of light wind disturbances.


Title: EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras
Key Words: cameras  collision avoidance  control engineering computing  helicopters  learning (artificial intelligence)  neural nets  object detection  robot vision  deep dynamic obstacle dodging  dynamic obstacle avoidance  quadrotor  deep learning  single event camera  shallow neural networks  ego-motion  low light testing scenario  EVDodgeNet  Cameras  Collision avoidance  Motion segmentation  Machine learning  Optical imaging  Robot vision systems  Image segmentation 
Abstract: Dynamic obstacle avoidance on quadrotors requires low latency. A class of sensors that are particularly suitable for such scenarios are event cameras. In this paper, we present a deep learning based solution for dodging multiple dynamic obstacles on a quadrotor with a single event camera and on-board computation. Our approach uses a series of shallow neural networks for estimating both the ego-motion and the motion of independently moving objects. The networks are trained in simulation and directly transfer to the real world without any fine-tuning or retraining. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with obstacles of different shapes and sizes, achieving an overall success rate of 70% including objects of unknown shape and a low light testing scenario. To our knowledge, this is the first deep learning - based solution to the problem of dynamic obstacle avoidance using event cameras on a quadrotor. Finally, we also extend our work to the pursuit task by merely reversing the control policy, proving that our navigation stack can cater to different scenarios.


Title: On training datasets for machine learning-based visual relative localization of micro-scale UAVs
Key Words: autonomous aerial vehicles  image classification  learning (artificial intelligence)  microrobots  mobile robots  object detection  training datasets  machine learning-based visual relative localization  microscale UAVs  relative Microscale Unmanned Aerial Vehicle localization sensor UVDAR  automatically annotated dataset MIDGARD  MAVs  visual object detection  carefully crafted training dataset  annotated camera footage  Cameras  Training  Observers  Visualization  Image color analysis  Global navigation satellite system  Position measurement 
Abstract: By leveraging our relative Micro-scale Unmanned Aerial Vehicle localization sensor UVDAR, we generated an automatically annotated dataset MIDGARD, which the community is invited to use for training and testing their machine learning systems for the detection and localization of Microscale Unmanned Aerial Vehicles (MAVs) by other MAVs. Furthermore, we provide our system as a mechanism for rapidly generating custom annotated datasets specifically tailored for the needs of a given application. The recent literature is rich in applications of machine learning methods in automation and robotics. One particular subset of these methods is visual object detection and localization, using means such as Convolutional Neural Networks, which nowadays enable objects to be detected and classified with previously inconceivable precision and reliability. Most of these applications, however, rely on a carefully crafted training dataset of annotated camera footage. These must contain the objects of interest in environments similar to those where the detector is expected to operate. Notably, the positions of the objects must be provided in annotations. For non-laboratory settings, the construction of such datasets requires many man-hours of manual annotation, which is especially the case for use onboard Micro-scale Unmanned Aerial Vehicles. In this paper, we are providing for the community a practical alternative to that kind of approach.


Title: Dynamic Actor-Advisor Programming for Scalable Safe Reinforcement Learning
Key Words: learning (artificial intelligence)  mobile robots  scalable safe reinforcement learning  real-world robots  complex strict constraints  safe reinforcement learning algorithms  high-dimensional systems  DAAP  sample efficiency  dynamic actor-advisor programming  dynamic policy programming framework  constraint violation risk  Dynamic programming  Programming  Robots  Learning (artificial intelligence)  Heuristic algorithms  Task analysis  Training 
Abstract: Real-world robots have complex strict constraints. Therefore, safe reinforcement learning algorithms that can simultaneously minimize the total cost and the risk of constraint violation are crucial. However, almost no algorithms exist that can scale to high-dimensional systems to the best of our knowledge. In this paper, we propose Dynamic Actor-Advisor Programming (DAAP), as an algorithm for sample-efficient and scalable safe reinforcement learning. DAAP employs two control policies, actor and advisor. They are updated to minimize total cost and risk of constraint violation intertwiningly and smoothly towards each other's direction by using the other as the baseline policy in the Kullback-Leibler divergence of Dynamic Policy Programming framework. We demonstrate the scalability and sample efficiency of DAAP through its application on simulated robot arm control tasks with performance comparisons to baselines.


Title: Discrete Deep Reinforcement Learning for Mapless Navigation
Key Words: discrete systems  gradient methods  learning (artificial intelligence)  mobile robots  navigation  optimisation  state-space methods  mapless navigation  discrete state space algorithms  continuous alternatives  double deep Q-network  parallel asynchronous training  training time  proximal policy optimization algorithms  original discrete algorithm  continuous algorithms  continuous deep deterministic policy gradient  multibatch priority experience replay  discrete deep reinforcement  Training  Navigation  Robot kinematics  Robot sensing systems  Optimization  Machine learning 
Abstract: Our goal is to investigate whether discrete state space algorithms are a viable solution to continuous alternatives for mapless navigation. To this end we present an approach based on Double Deep Q-Network and employ parallel asynchronous training and a multi-batch Priority Experience Replay to reduce the training time. Experiments show that our method trains faster and outperforms both the continuous Deep Deterministic Policy Gradient and Proximal Policy Optimization algorithms. Moreover, we train the models in a custom environment built on the recent Unity learning toolkit and show that they can be exported on the TurtleBot3 simulator and to the real robot without further training. Overall our optimized method is 40% faster compared to the original discrete algorithm. This setting significantly reduces the training times with respect to the continuous algorithms, maintaining a similar level of success rate hence being a viable alternative for mapless navigation.


Title: Learning Multi-Robot Decentralized Macro-Action-Based Policies via a Centralized Q-Net
Key Words: decentralised control  learning (artificial intelligence)  mobile robots  multi-agent systems  multi-robot systems  recurrent neural nets  multirobot decentralized macro-action-based policies  centralized Q-net  decentralized control  decentralized multiagent reinforcement learning  decentralized Q-net  decentralized exploration  macro-action based decentralized multiagent double deep recurrent Q-net  Parallel-MacDec-MADDRQN  Robot kinematics  Training  Tools  Task analysis  Machine learning  History 
Abstract: In many real-world multi-robot tasks, high-quality solutions often require a team of robots to perform asynchronous actions under decentralized control. Decentralized multi-agent reinforcement learning methods have difficulty learning decentralized policies because of the environment appearing to be non-stationary due to other agents also learning at the same time. In this paper, we address this challenge by proposing a macro-action-based decentralized multi-agent double deep recurrent Q-net (MacDec-MADDRQN) which trains each decentralized Q-net using a centralized Q-net for action selection. A generalized version of MacDec-MADDRQN with two separate training environments, called Parallel-MacDec-MADDRQN, is also presented to leverage either centralized or decentralized exploration. The advantages and the practical nature of our methods are demonstrated by achieving near-centralized results in simulation and having real robots accomplish a warehouse tool delivery task in an efficient way.


Title: Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps
Key Words: cameras  graph theory  image colour analysis  image sensors  mobile robots  normal distribution  robot vision  SLAM (robots)  camera origin  pose graph  NDT-OM  keyframe-based dense mapping  keyframe-based mapping system  RGB-D sensor  2D view-dependent structures  uncertainty model  RGB-D cameras  view-dependent local maps  normal distribution transform maps  global map  loop closure detection  autonomous robots  SLAM  Ellipsoids  Three-dimensional displays  Robot sensing systems  Cameras  Two dimensional displays  Uncertainty 
Abstract: In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.


Title: Informative Path Planning for Active Field Mapping under Localization Uncertainty
Key Words: Gaussian processes  mobile robots  path planning  informative path planning  active field mapping  localization uncertainty  information gathering algorithms  efficient data collection  fundamental problem  implicit requirement  high-quality maps  informative planning framework  active mapping  Gaussian process model  target environmental field  utility function  field mapping objectives  GP-based mapping scenarios  mean pose uncertainty  map error  indoor temperature mapping scenario  Uncertainty  Planning  Robot sensing systems  Trajectory  Manuals  Robot localization 
Abstract: Information gathering algorithms play a key role in unlocking the potential of robots for efficient data collection in a wide range of applications. However, most existing strategies neglect the fundamental problem of the robot pose uncertainty, which is an implicit requirement for creating robust, high-quality maps. To address this issue, we introduce an informative planning framework for active mapping that explicitly accounts for the pose uncertainty in both the mapping and planning tasks. Our strategy exploits a Gaussian Process (GP) model to capture a target environmental field given the uncertainty on its inputs. For planning, we formulate a new utility function that couples the localization and field mapping objectives in GP-based mapping scenarios in a principled way, without relying on manually-tuned parameters. Extensive simulations show that our approach outperforms existing strategies, reducing mean pose uncertainty and map error. We present a proof of concept in an indoor temperature mapping scenario.


Title: Ensemble of Sparse Gaussian Process Experts for Implicit Surface Mapping with Streaming Data
Key Words: Gaussian processes  mobile robots  path planning  regression analysis  robot vision  SLAM (robots)  sparse Gaussian process experts  implicit surface mapping  streaming data  creating maps  robotics  navigation  compact surface map  continuous implicit surface map  range data  approximate Gaussian process experts  GP models  model complexity  prediction error  real-world data sets  compact surface models  accurate implicit surface models  exact GP regression  subsampled data  Surface treatment  Data models  Predictive models  Covariance matrices  Gaussian processes  Computational modeling  Measurement uncertainty 
Abstract: Creating maps is an essential task in robotics and provides the basis for effective planning and navigation. In this paper, we learn a compact and continuous implicit surface map of an environment from a stream of range data with known poses. For this, we create and incrementally adjust an ensemble of approximate Gaussian process (GP) experts which are each responsible for a different part of the map. Instead of inserting all arriving data into the GP models, we greedily trade-off between model complexity and prediction error. Our algorithm therefore uses less resources on areas with few geometric features and more where the environment is rich in variety. We evaluate our approach on synthetic and real-world data sets and analyze sensitivity to parameters and measurement noise. The results show that we can learn compact and accurate implicit surface models under different conditions, with a performance comparable to or better than that of exact GP regression with subsampled data.


Title: Robust Method for Removing Dynamic Objects from Point Clouds
Key Words: image capture  image filtering  image registration  image representation  object detection  optical radar  robot vision  3D point cloud maps  dynamic object removal  laser scans  lidar scans  object detection  voxel traversal method  Three-dimensional displays  Vehicle dynamics  Octrees  Object detection  Laser modes  Feature extraction 
Abstract: 3D point cloud maps are an accumulation of laser scans obtained at different positions and times. Since laser scans represent a snapshot of the surrounding at the time of capture, they often contain moving objects which may not be observed at all times. Dynamic objects in point cloud maps decrease the quality of maps and affect localization accuracy, hence it is important to remove the dynamic objects from 3D point cloud maps. In this paper, we present a robust method to remove dynamic objects from 3D point cloud maps. Given a registered set of 3D point clouds, we build an occupancy map in which the voxels represent the occupancy state of the volume of space over an extended time period. After building the occupancy map, we use it as a filter to remove dynamic points in lidar scans before adding the points to the map. Furthermore, we accelerate the process of building occupancy maps using object detection and a novel voxel traversal method. Once the occupancy map is built, dynamic object removal can run in real-time. Our approach works well on wide urban roads with stopped or moving traffic and the occupancy maps get better with the inclusion of more lidar scans from the same scene.


Title: Real-Time Semantic Stereo Matching
Key Words: image matching  image segmentation  inference mechanisms  neural nets  semantic networks  stereo image processing  augmented reality  deep neural networks  semantic segmentation  inference  semantic stereo image matching  coarse-to-fine estimations  embedded devices  GPU  embedded Jetson TX2  Semantics  Feature extraction  Task analysis  Estimation  Three-dimensional displays  Image segmentation  Computer architecture 
Abstract: Scene understanding is paramount in robotics, self-navigation, augmented reality, and many other fields. To fully accomplish this task, an autonomous agent has to infer the 3D structure of the sensed scene (to know where it looks at) and its content (to know what it sees). To tackle the two tasks, deep neural networks trained to infer semantic segmentation and depth from stereo images are often the preferred choices. Specifically, Semantic Stereo Matching can be tackled by either standalone models trained for the two tasks independently or joint end-to-end architectures. Nonetheless, as proposed so far, both solutions are inefficient because requiring two forward passes in the former case or due to the complexity of a single network in the latter, although jointly tackling both tasks is usually beneficial in terms of accuracy. In this paper, we propose a single compact and lightweight architecture for real-time semantic stereo matching. Our framework relies on coarse-to-fine estimations in a multi-stage fashion, allowing: i) very fast inference even on embedded devices, with marginal drops in accuracy, compared to state-of-the-art networks, ii) trade accuracy for speed, according to the specific application requirements. Experimental results on high-end GPUs as well as on an embedded Jetson TX2 confirm the superiority of semantic stereo matching compared to standalone tasks and highlight the versatility of our framework on any hardware and for any application.


Title: Leveraging the Template and Anchor Framework for Safe, Online Robotic Gait Design
Key Words: control system synthesis  legged locomotion  predictive control  reduced order systems  robot dynamics  safety-preserving controllers  model predictive control  5-link RABBIT model  anchor framework  online robotic gait design  online control design  bipedal robot  reduced-order model  control synthesis  template framework  safe robotic gait design  Legged locomotion  Rabbits  Safety  Foot  Predictive models  Control systems  Bipeds  underactuated system  safety guarantee 
Abstract: Online control design using a high-fidelity, full-order model for a bipedal robot can be challenging due to the size of the state space of the model. A commonly adopted solution to overcome this challenge is to approximate the fullorder model (anchor) with a simplified, reduced-order model (template), while performing control synthesis. Unfortunately it is challenging to make formal guarantees about the safety of an anchor model using a controller designed in an online fashion using a template model. To address this problem, this paper proposes a method to generate safety-preserving controllers for anchor models by performing reachability analysis on template models by relying on functions that bound the difference between the two models. This paper describes how this reachable set can be incorporated into a Model Predictive Control framework to select controllers that result in safe walking on the anchor model in an online fashion. The method is illustrated on a 5-link RABBIT model, and is shown to allow the robot to walk safely while utilizing controllers designed in an online fashion.


Title: Unified Push Recovery Fundamentals: Inspiration from Human Study
Key Words: humanoid robots  legged locomotion  mechanical stability  motion control  position control  predictive control  time optimal control  unified push recovery fundamentals  humanoid robots  stepping strategies  balance strategies  minimum jerk controller  human behaviour  model-predictive control  recovery motions  robotic systems  human balance recovery  legged machines  time-optimal performance  Modulation  Robots  Hip  Stability criteria  Foot  Force 
Abstract: Currently for balance recovery, humans outperform humanoid robots which use hand-designed controllers in terms of the diverse actions. This study aims to close this gap by finding core control principles that are shared across ankle, hip, toe and stepping strategies by formulating experiments to test human balance recoveries and define criteria to quantify the strategy in use. To reveal fundamental principles of balance strategies, our study shows that a minimum jerk controller can accurately replicate comparable human behaviour at the Centre of Mass level. Therefore, we formulate a general Model-Predictive Control (MPC) framework to produce recovery motions in any system, including legged machines, where the framework parameters are tuned for time-optimal performance in robotic systems.


Title: DISCO: Double Likelihood-free Inference Stochastic Control
Key Words: Bayes methods  control system synthesis  differential equations  Monte Carlo methods  predictive control  probability  robust control  sampling methods  stochastic systems  transforms  uncertain systems  double likelihood-free inference stochastic control  complex physical systems  control strategies  analytical tractability  probabilistic inference  simulation parameters  likelihood function  modern simulators  nonanalytical model  classical control  model parameters  DISCO  differential equations  numerical solvers  uncertainty assessment  Bayesian statistics  likelihood-free inference  control framework design  unscented transform  information theoretical model predictive control  Monte Carlo sampling  robotics tasks  posterior distribution  Uncertainty  Trajectory  Mathematical model  Computational modeling  Numerical models  Stochastic processes  Cost function 
Abstract: Accurate simulation of complex physical systems enables the development, testing, and certification of control strategies before they are deployed into the real systems. As simulators become more advanced, the analytical tractability of the differential equations and associated numerical solvers incorporated in the simulations diminishes, making them difficult to analyse. A potential solution is the use of probabilistic inference to assess the uncertainty of the simulation parameters given real observations of the system. Unfortunately the likelihood function required for inference is generally expensive to compute or totally intractable. In this paper we propose to leverage the power of modern simulators and recent techniques in Bayesian statistics for likelihood-free inference to design a control framework that is efficient and robust with respect to the uncertainty over simulation parameters. The posterior distribution over simulation parameters is propagated through a potentially non-analytical model of the system with the unscented transform, and a variant of the information theoretical model predictive control. This approach provides a more efficient way to evaluate trajectory roll outs than Monte Carlo sampling, reducing the online computation burden. Experiments show that the controller proposed attained superior performance and robustness on classical control and robotics tasks when compared to models not accounting for the uncertainty over model parameters.


Title: Sufficiently Accurate Model Learning
Key Words: learning (artificial intelligence)  control algorithms  primal-dual method  sufficiently accurate models  traditional control  error characteristics  inaccurate physical measurements  planning algorithms  robot  accurate model learning  Task analysis  Optimization  Data models  Adaptation models  Heuristic algorithms  Neural networks  Planning 
Abstract: Modeling how a robot interacts with the environment around it is an important prerequisite for designing control and planning algorithms. In fact, the performance of controllers and planners is highly dependent on the quality of the model. One popular approach is to learn data driven models in order to compensate for inaccurate physical measurements and to adapt to systems that evolve over time. In this paper, we investigate a method to regularize model learning techniques to provide better error characteristics for traditional control and planning algorithms. This work proposes learning "Sufficiently Accurate" models of dynamics using a primal-dual method that can explicitly enforce constraints on the error in pre-defined parts of the state-space. The result of this method is that the error characteristics of the learned model is more predictable and can be better utilized by planning and control algorithms. The characteristics of Sufficiently Accurate models are analyzed through experiments on a simulated ball paddle system.


Title: Towards Plan Transformations for Real-World Mobile Fetch and Place
Key Words: manipulators  mobile robots  path planning  service robots  cleaning tasks  mobile manipulation plans  plan transformations  mobile fetch and place  robot behavior  table setting tasks  Task analysis  Planning  Runtime  Manipulators  Transforms  Complexity theory 
Abstract: In this paper, we present an approach and an implemented framework for applying plan transformations to real-world mobile manipulation plans, in order to specialize them to the specific situation at hand. The framework can improve execution cost and achieve better performance by autonomously transforming robot's behavior at runtime. To demonstrate the feasibility of our approach, we apply three example transformations to the plan of a PR2 robot performing simple table setting and cleaning tasks in the real world. Based on a large amount of experiments in a fast plan projection simulator, we make conclusions on improved execution performance.


Title: Planning an Efficient and Robust Base Sequence for a Mobile Manipulator Performing Multiple Pick-and-place Tasks
Key Words: collision avoidance  mobile robots  motion control  redundant manipulators  mobile manipulator  planned base positions  robust base sequence  precomputed reachability database  base positioning uncertainty  collision free inverse kinematics solutions  multiple pick and place tasks  kinematic redundancy  Manipulators  Databases  Task analysis  Robustness  Grasping  Uncertainty  Planning 
Abstract: In this paper, we address efficiently and robustly collecting objects stored in different trays using a mobile manipulator. A resolution complete method, based on precomputed reachability database, is proposed to explore collision-free inverse kinematics (IK) solutions and then a resolution complete set of feasible base positions can be determined. This method approximates a set of representative IK solutions that are especially helpful when solving IK and checking collision are treated separately. For real world applications, we take into account the base positioning uncertainty and plan a sequence of base positions that reduce the number of necessary base movements for collecting the target objects, the base sequence is robust in that the mobile manipulator is able to complete the part-supply task even there is certain deviation from the planned base positions. Our experiments demonstrate both the efficiency compared to regular base sequence and the feasibility in real world applications.


Title: Towards Mobile Multi-Task Manipulation in a Confined and Integrated Environment with Irregular Objects
Key Words: assembling  control engineering computing  industrial manipulators  machining  mobile robots  production engineering computing  software architecture  irregular objects  mechanical parts  complex task sets  integrated task sets  IEEE International Conference on Robots and Automation  FetchIt! Mobile Manipulation Challenge  mobile multitask manipulation  confined environment  integrated environment  confined space  machining  assembly  software architecture  Gears  Navigation  Task analysis  Manipulators  Three-dimensional displays  Robot sensing systems 
Abstract: The FetchIt! Mobile Manipulation Challenge, held at the IEEE International Conference on Robots and Automation (ICRA) in May 2019, offered an environment with complex and integrated task sets, irregular objects, confined space, and machining, introducing new challenges in the mobile manipulation domain. Here we describe our efforts to address these challenges by demonstrating the assembly of a kit of mechanical parts in a caddy. In addition to implementation details, we examine the issues in this task set extensively, and we discuss our software architecture in the hope of providing a base for other researchers. To evaluate performance and consistency, we conducted 20 full runs, then examined failure cases with possible solutions. We conclude by identifying future research directions to address the open challenges.


Title: Linear Time-Varying MPC for Nonprehensile Object Manipulation with a Nonholonomic Mobile Robot
Key Words: collision avoidance  friction  linear systems  manipulators  mobile robots  motion control  predictive control  time-varying systems  trajectory control  wheels  linear time-varying MPC  nonprehensile object manipulation  nonholonomic mobile robot  nonprehensile manipulation motion primitive  unilateral constraint  manipulated object  linear time-varying model predictive control  pushing manipulation  Mobile robots  Friction  Task analysis  Dynamics  Mathematical model  Force 
Abstract: This paper proposes a technique to manipulate an object with a nonholonomic mobile robot by pushing, which is a nonprehensile manipulation motion primitive. Such a primitive involves unilateral constraints associated with the friction between the robot and the manipulated object. Violating this constraint produces the slippage of the object during the manipulation, preventing the correct achievement of the task. A linear time-varying model predictive control is designed to include the unilateral constraint within the control action properly. The approach is verified in a dynamic simulation environment through a Pioneer 3-DX wheeled robot executing the pushing manipulation of a package.


Title: A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in Homes
Key Words: control engineering computing  force control  humanoid robots  learning (artificial intelligence)  manipulators  mobile robots  motion control  position control  virtual reality  mobile manipulation system  one-shot teaching  mobile manipulation hardware  software system  human-level tasks  single demonstration  virtual reality  highly capable mobile manipulation robot  parameterized primitives  robust learned dense visual embeddings representation  task graph  taught behaviors  Task analysis  Robot kinematics  Robustness  Visualization  Aerospace electronics  Education 
Abstract: We describe a mobile manipulation hardware and software system capable of autonomously performing complex human-level tasks in real homes, after being taught the task with a single demonstration from a person in virtual reality. This is enabled by a highly capable mobile manipulation robot, whole-body task space hybrid position/force control, teaching of parameterized primitives linked to a robust learned dense visual embeddings representation of the scene, and a task graph of the taught behaviors. We demonstrate the robustness of the approach by presenting results for performing a variety of tasks, under different environmental conditions, in multiple real homes. Our approach achieves 85% overall success rate on three tasks that consist of an average of 45 behaviors each. The video is available at: https://youtu.be/HSyAGMGikLk.


Title: 2D to 3D Line-Based Registration with Unknown Associations via Mixed-Integer Programming
Key Words: calibration  image registration  integer programming  iterative methods  mobile robots  robot vision  iterative nearest-neighbor  mixed-integer program  data association  integer variables  3D line-based registration  mixed-integer programming  rigid-body transformation  3D point cloud data  mobile robotics  sensor calibration  linear line-based 2D-3D registration  Three-dimensional displays  Two dimensional displays  Cameras  Cost function  Robot sensing systems  Transforms  Symmetric matrices 
Abstract: Determining the rigid-body transformation be-tween 2D image data and 3D point cloud data has applications for mobile robotics including sensor calibration and localizing into a prior map. Common approaches to 2D-3D registration use least-squares solvers assuming known associations often provided by heuristic front-ends, or iterative nearest-neighbor. We present a linear line-based 2D-3D registration algorithm formulated as a mixed-integer program to simultaneously solve for the correct transformation and data association. Our formulation is explicitly formulated to handle outliers, by modeling associations as integer variables. Additionally, we can constrain the registration to SE(2) to improve runtime and accuracy. We evaluate this search over multiple real-world data sets demonstrating adaptability to scene variation.


Title: End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera
Key Words: cameras  driver information systems  feature extraction  image sequences  learning (artificial intelligence)  mobile robots  neural nets  object detection  road safety  robot vision  video signal processing  end-to-end learning  inter-vehicle distance  ADAS  monocular camera  advanced driver-assistance systems  relative velocity estimation method  multiple visual clues  time-consecutive monocular frames  deep feature clue  scene geometry clue  temporal optical flow clue  vehicle-centric sampling mechanism  light-weight deep neural network  Estimation  Three-dimensional displays  Cameras  Optical imaging  Two dimensional displays  Feature extraction  Neural networks 
Abstract: Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.


Title: Learning an Action-Conditional Model for Haptic Texture Generation
Key Words: feedback  haptic interfaces  human-robot interaction  image texture  learning (artificial intelligence)  mobile robots  robot vision  tactile sensors  telerobotics  virtual reality  Haptic Texture generation  haptic sensory feedback  user interactions  immersive virtual reality  material properties  haptic vibration feedback  Penn Haptic Texture Toolkit  action-conditional model learning  GelSight measurements  teleoperation system  autonomous robot  GelSight image texture  Haptic interfaces  Autoregressive processes  Force  Acceleration  Predictive models  Solid modeling  Discrete Fourier transforms 
Abstract: Rich haptic sensory feedback in response to user interactions is desirable for an effective, immersive virtual reality or teleoperation system. However, this feedback depends on material properties and user interactions in a complex, non-linear manner. Therefore, it is challenging to model the mapping from material and user interactions to haptic feedback in a way that generalizes over many variations of the user's input. Current methodologies are typically conditioned on user interactions, but require a separate model for each material. In this paper, we present a learned action-conditional model that uses data from a vision-based tactile sensor (GelSight) and user's action as input. This model predicts an induced acceleration that could be used to provide haptic vibration feedback to a user. We trained our proposed model on a publicly available dataset (Penn Haptic Texture Toolkit) that we augmented with GelSight measurements of the different materials. We show that a unified model over all materials outperforms previous methods and generalizes to new actions and new instances of the material categories in the dataset.


Title: Multimodal tracking framework for visual odometry in challenging illumination conditions
Key Words: cameras  distance measurement  feature extraction  image matching  motion estimation  robot vision  stereo image processing  visible spectrum  electromagnetic spectrum  extreme illumination conditions  camera setups  multimodal monocular visual odometry solution  multimodal tracking framework  stereo matching techniques  long wave infrared spectral bands  LWIR  MMS-VO  windowed bundle adjustment framework  motion estimation process  visible-thermal datasets  feature tracking  visual odometry trajectory  Cameras  Feature extraction  Bundle adjustment  Visual odometry  Lighting  Tracking 
Abstract: Research on visual odometry and localisation is largely dominated by solutions developed in the visible spectrum, where illumination is a critical factor. Other parts of the electromagnetic spectrum are currently being investigated to generate solutions dealing with extreme illumination conditions. Multispectral setups are particularly interesting as they provide information from different parts of the spectrum at once. However, the main challenge of such camera setups is the lack of similarity between the images produced, which makes conventional stereo matching techniques obsolete.This work investigates a new way of concurrently processing images from different spectra for application to visual odometry. It particularly focuses on the visible and Long Wave InfraRed (LWIR) spectral bands where dissimilarity between pixel intensities is maximal. A new Multimodal Monocular Visual Odometry solution (MMS-VO) is presented. With this novel approach, features are tracked simultaneously, but only the camera providing the best tracking quality is used to estimate motion. Visual odometry is performed within a windowed bundle adjustment framework, by alternating between the cameras as the nature of the scene changes. Furthermore, the motion estimation process is robustified by selecting adequate keyframes based on parallax.The algorithm was tested on a series of visible-thermal datasets, acquired from a car with real driving conditions. It is shown that feature tracking could be performed in both modalities with the same set of parameters. Additionally, the MMS-VO provides a superior visual odometry trajectory as one camera can compensate when the other is not working.


Title: Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration
Key Words: autonomous underwater vehicles  control engineering computing  convolutional neural nets  feature extraction  human-robot interaction  mobile robots  object detection  object tracking  custom CNN  deep SORT algorithm  realtime tracking-by-detection  realtime diver detection  initial diver detection  appearance metric  simple online realtime tracking  human divers  autonomous underwater robots  underwater human-robot collaboration  realtime multidiver tracking re-identification  on-board tracking  on-board autonomous robot operations  multiperson tracking  Robots  Tracking  Feature extraction  Collaboration  Unmanned underwater vehicles  Task analysis 
Abstract: Autonomous underwater robots working with teams of human divers may need to distinguish between different divers, e.g., to recognize a lead diver or to follow a specific team member. This paper describes a technique that enables autonomous underwater robots to track divers in real time as well as to reidentify them. The approach is an extension of Simple Online Realtime Tracking (SORT) with an appearance metric (deep SORT). Initial diver detection is performed with a custom CNN designed for realtime diver detection, and appearance features are subsequently extracted for each detected diver. Next, realtime tracking-by-detection is performed with an extension of the deep SORT algorithm. We evaluate this technique on a series of videos of divers performing human-robot collaborative tasks and show that our methods result in more divers being accurately identified during tracking. We also discuss the practical considerations of applying multi-person tracking to on-board autonomous robot operations, and we consider how failure cases can be addressed during on-board tracking.


Title: Autonomous Tissue Scanning under Free-Form Motion for Intraoperative Tissue Characterisation
Key Words: biological tissues  biomedical optical imaging  medical image processing  medical robotics  surgery  visual servoing  autonomous tissue scanning  free-form motion  intraoperative tissue characterisation  imaging probes  tissue surface  robot-assisted local tissue scanning  motion stabilisation  periodic motion  free-form tissue motion  scanning trajectory  ultrasound tissue scanning  Three-dimensional displays  Probes  Tracking  Robots  Cameras  Trajectory 
Abstract: In Minimally Invasive Surgery (MIS), tissue scanning with imaging probes is required for subsurface visualisation to characterise the state of the tissue. However, scanning of large tissue surfaces in the presence of motion is a challenging task for the surgeon. Recently, robot-assisted local tissue scanning has been investigated for motion stabilisation of imaging probes to facilitate the capturing of good quality images and reduce the surgeon's cognitive load. Nonetheless, these approaches require the tissue surface to be static or translating with periodic motion. To eliminate these assumptions, we propose a visual servoing framework for autonomous tissue scanning, able to deal with free-form tissue motion. The 3D structure of the surgical scene is recovered, and a feature-based method is proposed to estimate the motion of the tissue in real-time. The desired scanning trajectory is manually defined on a reference frame and continuously updated using projective geometry to follow the tissue motion and control the movement of the robotic arm. The advantage of the proposed method is that it does not require the learning of the tissue motion prior to scanning and can deal with free-form motion. We deployed this framework on the da Vinci®surgical robot using the da Vinci Research Kit (dVRK) for Ultrasound tissue scanning. Our framework can be easily extended to other probe-based imaging modalities.


Title: 3D-Printed Electroactive Hydraulic Valves for Use in Soft Robotic Applications
Key Words: design engineering  electrorheology  human-robot interaction  hydraulic actuators  hydraulic systems  industrial robots  valves  3D-printed electroactive hydraulic valves  soft robotic applications  human-robot interaction  alternative locomotion techniques  open-source method  high-pressure electrorheological valves  electrorheological fluid-based control  deformable actuators  safety areas  design engineering  pressure 230.0 kPa  time 1.0 s to 3.0 s  Valves  Erbium  Three-dimensional displays  Soft robotics  Electrodes  Actuators 
Abstract: Soft robotics promises developments in the research areas of safety, bio-mimicry, manipulation, human-robot interaction, and alternative locomotion techniques. The research presented here is directed towards developing an improved, low-cost, and open-source method for soft robotic control using electrorheological fluids in compact, 3D-printed electroactive hydraulic valves. We construct high-pressure electrorheological valves and deformable actuators using only commercially available materials and accessible fabrication methods. The printed valves were characterized with industrial-grade electrorheological fluid (RheOil 3.0), but the design is generalizable to other electrorheological fluids. Valve performance was shown to be an improvement over comparable work with demonstrated higher yield pressures at lower voltages (up to 230 kPa), larger flow rates (up to 15 ml/min) and lower response times (1 to 3 seconds, depending on design). The resulting valve and actuator systems enable future novel applications of electrorheological fluid-based control and hydraulics in soft robotics and other disciplines.


Title: Variable Damping Control of a Robotic Arm to Improve Trade-off between Agility and Stability and Reduce User Effort
Key Words: damping  end effectors  human-robot interaction  stability  variable structure systems  stability  fixed damping controllers  variable robotic damping controller  robotic arm  physical human robot interaction  dual sided logistic function  end effector  7 degree-of-freedom robot  root mean squared interaction forces  Damping  Stability analysis  Service robots  Acceleration  Safety  Impedance 
Abstract: This paper presents a variable damping controller to improve the trade-off between agility and stability in physical human-robot interaction (pHRI), while reducing user effort. Variable robotic damping, defined as a dual-sided logistic function, was determined in real time throughout a range of negative to positive values based on the user's intent of movement. To evaluate the effectiveness of the proposed controller, we performed a set of human experiments with subjects interacting with the end-effector of a 7 degree-of-freedom robot. Twelve subjects completed target reaching tasks under three robotic damping conditions: fixed positive, fixed negative, and variable damping. On average, the variable damping controller significantly shortened the rise time by 22.4% compared to the fixed positive damping. It is also important to note that the rise time in the variable damping condition was as fast as that in the fixed negative damping condition and there was no statistical difference between the two conditions. The variable damping controller significantly decreased the percentage overshoot by 49.6% and shortened the settling time by 29.0% compared to the fixed negative damping. Both the maximum and mean root-mean-squared (RMS) interaction forces were significantly lower in the variable damping condition than the other two fixed damping conditions, i.e., the variable damping controller reduced user effort. The maximum and mean RMS interaction forces were at least 17.3% and 20.3% lower than any of the fixed damping conditions, respectively. The results of this study demonstrate that humans can extract the benefits of the variable damping controller in the context of pHRI, as it significantly improves the trade-off between agility and stability and reduces user effort in comparison to fixed damping controllers.


Title: Cognitive and motor compliance in intentional human-robot interaction
Key Words: cognitive systems  humanoid robots  human-robot interaction  mobile robots  torque feedback  humanoid Torobo  bio-inspired study  cognitive compliance  human environments  adaptive robotics  natural cognition  subjective experience  intentional human-robot interaction  motor compliance  Torque  Joints  Neural networks  Predictive models  Robot sensing systems  Stochastic processes 
Abstract: Embodiment and subjective experience in humanrobot interaction are important aspects to consider when studying both natural cognition and adaptive robotics to human environments. Although several researches have focused on nonverbal communication and collaboration, the study of autonomous physical interaction has obtained less attention. From the perspective of neurorobotics, we investigate the relation between intentionality, motor compliance, cognitive compliance, and behavior emergence. We propose a variational model inspired by the principles of predictive coding and active inference to study intentionality and cognitive compliance, and an intermittent control concept for motor deliberation and compliance based on torque feed-back. Our experiments with the humanoid Torobo portrait interesting perspectives for the bio-inspired study of developmental and social processes.


Title: Adaptive Authority Allocation in Shared Control of Robots Using Bayesian Filters
Key Words: Bayes methods  delays  mobile robots  stability  telerobotics  adaptive authority allocation  Bayesian filter  control framework  autonomous system  human operator  time-varying measurement noise characteristics  system-driven adaptive shared control framework  stability proof  teleoperation  Bayes methods  Robots  Uncertainty  Task analysis  Measurement uncertainty  Resource management  Noise measurement  Adaptive authority allocation  shared control  teleoperation  Kalman filter  Bayesian filters 
Abstract: In the present paper, we propose a novel system-driven adaptive shared control framework in which the autonomous system allocates the authority among the human operator and itself. Authority allocation is based on a metric derived from a Bayesian filter, which is being adapted online according to real measurements. In this way, time-varying measurement noise characteristics are incorporated. We present the stability proof for the proposed shared control architecture with adaptive authority allocation, which includes time delay in the communication channel between the operator and the robot. Furthermore, the proposed method is validated through experiments and a user-study evaluation. The obtained results indicate significant improvements in task execution compared with pure teleoperation.


Title: Tactile Telerobots for Dull, Dirty, Dangerous, and Inaccessible Tasks
Key Words: computational complexity  dexterous manipulators  haptic interfaces  telerobotics  first-generation telerobot  task complexity  tactile telerobots  inaccessible tasks  highly-dexterous bimanual tactile telerobot  bare human hands  anthropomorphic robot hands  biomimetic tactile sensors  in-hand manipulation  autonomous robotic hands  robotic dexterity  Robot sensing systems  Haptic interfaces  Task analysis  Force  Manipulators  Electrodes 
Abstract: The sense of touch, which is essential for human dexterity, is virtually absent from today's robotic hands. In this work we present progress in creating a highly-dexterous bimanual tactile telerobot, and evaluate its performance compared to bare human hands. The system, consisting of anthropomorphic robot hands, biomimetic tactile sensors, and advanced haptic gloves, enables a human operator to intuitively control and feel what the robotic hands are touching. Through carefully tuned tactile and kinematic mapping it was possible to intuitively perform dexterous operations, including pick and place tasks and even in-hand manipulation, a challenge for most autonomous robotic hands. Performance of the system was evaluated in standard measures of human and robotic dexterity such as the Box and Block test and other YCB benchmarks. This first-generation telerobot was found to have promising performance with the pilot able to do the same tasks in the telerobot between 1/4th to 1/12th the speed of their bare hands depending on the task complexity.


Title: A Novel Orientability Index and the Kinematic Design of the RemoT-ARM: A Haptic Master with Large and Dexterous Workspace
Key Words: control system synthesis  dexterous manipulators  haptic interfaces  manipulator kinematics  optimal systems  performance index  telerobotics  RemoT-ARM  kinematic design  dexterous workspace  performance index  relative orientability index  target workspace  performance indices  haptic master device dexterity  6-DOF haptic master device  workspace matching degree  Manipulators  Indexes  Haptic interfaces  Kinematics  Performance evaluation  Force  Phantoms 
Abstract: Orientability is an important performance index to evaluate the dexterity of haptic master devices. Currently, most of the existing haptic master devices have limited workspace and limited dexterity. In this paper, we present the RemoT-ARM, a 6 Degree-of-Freedom (DOF) haptic master device that can provide larger and more dexterous workspace for operators. To evaluate its reachability of orientations, we propose a novel orientability index. Furthermore, a relative orientability index is proposed to characterize the matching degree of the workspace of a given manipulator to its target workspace. The volume, the manipulability and the condition number are also introduced as performance indices to evaluate the size and the isotropy of the workspace. According to these performance indices, all possible configurations for the RemoT-ARM have been taken into consideration, analyzed, and compared to finalize its optimal configuration.


Title: RAVEN-S: Design and Simulation of a Robot for Teleoperated Microgravity Rodent Dissection Under Time Delay
Key Words: aerospace instrumentation  biocontrol  manipulators  medical robotics  mobile robots  space research  space vehicles  surgery  telerobotics  zero gravity experiments  teleoperated Microgravity Rodent dissection  International Space Station  ISS  biological effects  spaceflight  Rodent Habitat  Microgravity Science Glovebox  teleoperation  RAVEN II  rudimentary interaction force estimation  onboard dissection robot  RAVEN-S prototype design  communications time delay  robot design  robot simulation  Tools  Task analysis  Rodents  Delay effects  Delays  Force  Robots 
Abstract: The International Space Station (ISS) serves as a research lab for a wide variety of experiments including some that study the biological effects of microgravity and spaceflight using the Rodent Habitat and Microgravity Science Glovebox (MSG). Astronauts train for onboard dissections of rodents following basic training. An alternative approach for conducting these experiments is teleoperation of a robot located on the ISS from earth by a scientist who is proficient in rodent dissection. This pilot study addresses (1) the effects of extreme time delay on skill degradation during Fundamentals of Laparoscopic Surgery (FLS) tasks and rodent dissections using RAVEN II; (2) derivation and testing of rudimentary interaction force estimation; (3) elicitation of design requirements for an onboard dissection robot, RAVEN-S; and (4) simulation of the RAVEN-S prototype design with dissection data. The results indicate that the tasks' completion times increased by a factor of up to 9 for a 3 s time delay while performing manipulation and cutting tasks (FLS model) and by a factor of up to 3 for a 0.75 s time delay during mouse dissection tasks (animal model). Average robot forces/torques of 14N/0.1Nm (peak 90N/0.75Nm) were measured along with average linear/angular velocities of 0.02m/s/4rad/s (peak 0.1m/s/40rad/s) during dissection. A triangular configuration of three arms with respect to the operation site showed the best configuration given the MSG geometry and the dissection tasks. In conclusion, the results confirm the feasibility of utilizing a surgically-inspired RAVEN-S robot for teleoperated rodent dissection for successful completion of the predefined tasks in the presence of communications time delay between the ISS and ground control.


Title: Collision-free Navigation of Human-centered Robots via Markov Games
Key Words: collision avoidance  learning (artificial intelligence)  Markov processes  mobile robots  multi-agent systems  multi-robot systems  collision-free navigation  human-centered robots  Markov games  robot navigation  single-agent Markov decision process  static environment  multiagent formulation  primary agent  remaining auxiliary agents  path-following type adversarial training strategy  robust decentralized collision avoidance policy  real-world mobile robots  Collision avoidance  Robots  Markov processes  Navigation  Games  Robustness  Training  Collision-free navigation  human-centered robotics  deep reinforcement learning  multi-agent system  adversarial training 
Abstract: We exploit Markov games as a framework for collision-free navigation of human-centered robots. Unlike the classical methods which formulate robot navigation as a single-agent Markov decision process with a static environment, our framework of Markov games adopts a multi-agent formulation with one primary agent representing the robot and the remaining auxiliary agents form a dynamic or even competing environment. Such a framework allows us to develop a path-following type adversarial training strategy to learn a robust decentralized collision avoidance policy. Through thorough experiments on both simulated and real-world mobile robots, we show that the learnt policy outperforms the state-of-the-art algorithms in both sample complexity and runtime robustness.


Title: DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory Behaviors
Key Words: collision avoidance  learning (artificial intelligence)  mobile robots  pedestrians  trajectory control  DenseCAvoid  real-time navigation  dense crowds  anticipatory behaviors  pedestrian behaviors  visual sensors  pedestrian trajectory prediction algorithm  input frames  compute bounding boxes  pedestrian positions  future time  hybrid approach  deep reinforcement learning-based collision avoidance method  robust trajectories  static scenarios  dynamic scenarios  multiple pedestrians  robot freezing  trajectory lengths  mean arrival times  Collision avoidance  Navigation  Trajectory  Robot sensing systems  Robustness  Tracking 
Abstract: We present DenseCAvoid, a novel algorithm for navigating a robot through dense crowds and avoiding collisions by anticipating pedestrian behaviors. Our formulation uses visual sensors and a pedestrian trajectory prediction algorithm to track pedestrians in a set of input frames and compute bounding boxes that extrapolate to the pedestrian positions in a future time. Our hybrid approach combines this trajectory prediction with a Deep Reinforcement Learning-based collision avoidance method to train a policy to generate smoother, safer, and more robust trajectories during run-time. We train our policy in realistic 3-D simulations of static and dynamic scenarios with multiple pedestrians. In practice, our hybrid approach generalizes well to unseen, real-world scenarios and can navigate a robot through dense crowds (~1-2 humans per square meter) in indoor scenarios, including narrow corridors and lobbies. As compared to cases where prediction was not used, we observe that our method reduces the occurrence of the robot freezing in a crowd by up to 48%, and performs comparably with respect to trajectory lengths and mean arrival times to goal.


Title: Robotic Control of a Magnetic Swarm for On-Demand Intracellular Measurement
Key Words: biochemistry  biomedical materials  biomedical optical imaging  cellular biophysics  dyes  fluorescence  magnetic particles  medical robotics  micromanipulators  nanomedicine  nanoparticles  pH  fluorescent dyes  biochemical measurements  ion concentrations  signal-to-noise ratios  dye-coated magnetic nanoparticles  magnetic micromanipulation systems  generated swarm  magnetic micromanipulation system  position control accuracy  intracellular pH mapping  global dye treatment  fluorescent dye concentration  intracellular measurement results  robotic control  on-demand intracellular measurement  pH sensitive fluorescent dye-coated magnetic nanoparticles  Coils  Magnetic devices  Magnetic separation  Magnetic resonance imaging  Magnetic particles  Signal to noise ratio  Magnetic nanoparticles 
Abstract: In biology, fluorescent dyes are routinely used for biochemical measurements such as pH and ion concentrations. They, especially when used for detecting a low concentration of ions, suffer from low signal-to-noise ratios (SNR); and increasing the concentration of fluorescent dyes causes more sever cytotoxicity. We invented a new approach that uses a low amount of fluorescent dye-coated magnetic nanoparticles for on-demand, accurately aggregating the nanoparticles and thus fluorescent dyes in a local region inside a cell for intracellular measurement. Experiments proved this approach is capable of achieving a significantly higher SNR and lower cytotoxicity. Different from existing magnetic micromanipulation systems that generate large swarms (several microns and above) or cannot move the generated swarm to an arbitrary position, we developed a five-pole magnetic micromanipulation system and technique for generating a small swarm (e.g., 1 μm; capable of generating a magnetic swarm from 0.52 μm to 52.7 μm with an error <; 7.5 %) and accurately positioning the small swarm (position control accuracy: 0.76 μm). As an example, the system performed intracellular pH mapping using a 1 μm swarm of pH sensitive fluorescent dye-coated magnetic nanoparticles. The swarm had an SNR inside a cell 10 times that by the traditional method, i.e., global dye treatment, with both cases using the same fluorescent dye concentration. Our intracellular measurement results, for the first time, quantitatively revealed the existence of pH gradient and polarized pH distribution in live migrating cells.


Title: Acoustofluidic Tweezers for the 3D Manipulation of Microparticles
Key Words: acoustic streaming  hydrodynamics  microfabrication  microfluidics  micromanipulators  position control  transducers  high-speed acoustic streaming  manipulation velocity  trapped particle  particle manipulation  centimeter distance  transducer surface  streaming flow field  hydrodynamic force  microfabricated gigahertz transducer  three-dimensional space  dynamic position control  spatial distance  microscale objects  microrobotics  noncontact manipulation  microparticle  3D manipulation  acoustofluidic tweezers  Transducers  Acoustics  Force  Fluids  Three-dimensional displays  Streaming media  Hydrodynamics 
Abstract: Non-contact manipulation is of great importance in the actuation of micro-robotics. It is challenging to contactless manipulate micro-scale objects over large spatial distance in fluid. Here, we describe a novel approach for the dynamic position control of microparticles in three-dimensional (3D) space, based on high-speed acoustic streaming generated by a micro-fabricated gigahertz transducer. The hydrodynamic force generated by the streaming flow field has a vertical component against gravity and a lateral component towards the center, thus the microparticle is able to be stably trapped at a position far from the transducer surface, and to be manipulated over centimeter distance in 3D. Only the hydrodynamic force is utilized in the system for particle manipulation, making it a versatile tool regardless the material properties of the trapped particle. The system shows high reliability and manipulation velocity, revealing its potentials for the applications in robotics and automation at small scales.


Title: An online scheduling algorithm for human-robot collaborative kitting
Key Words: ergonomics  human-robot interaction  logistics  occupational health  occupational safety  productivity  robotic assembly  scheduling  warehousing  online scheduling algorithm  human-robot collaborative kitting  assembly line  key logistic task  human operators  work-related musculoskeletal disorders  picking operations  offline scheduler  warehouse  productivity analysis  Task analysis  Ergonomics  Robot kinematics  Strain  Collaboration  Scheduling algorithms 
Abstract: In manufacturing, kitting is the process of grouping separate items together to be supplied as one unit to the assembly line. This is a key logistic task, which is usually performed manually by human operators. However, picking objects from the warehouse implies a great repetitiveness in arm motion. Moreover, the weight and position of items may increase the physical strain and induce the development of work-related musculoskeletal disorders. The inclusion of a collaborative robot in the process may help to reduce the operator's effort and increase productivity. This paper introduces an online scheduling algorithm to guide the picking operations of the human and the robot. The proposed approach has been experimentally evaluated and compared with an offline scheduler, as well as with the baseline case of manual kitting.


Title: A Model-Free Approach to Meta-Level Control of Anytime Algorithms
Key Words: learning (artificial intelligence)  mobile robots  optimisation  autonomous system  real-time planning problems  model-free approach  anytime algorithms  computation time  meta-level control technique  meta-level control problem  reinforcement learning methods  mobile robot domain  Learning (artificial intelligence)  Autonomous systems  Planning  Heuristic algorithms  Computational modeling  Real-time systems  Uncertainty 
Abstract: Anytime algorithms offer a trade-off between solution quality and computation time that has proven to be useful in autonomous systems for a wide range of real-time planning problems. In order to optimize this trade-off, an autonomous system has to solve a challenging meta-level control problem: it must decide when to interrupt the anytime algorithm and act on the current solution. Prevailing meta-level control techniques, however, make a number of unrealistic assumptions that reduce their effectiveness and usefulness in the real world. Eliminating these assumptions, we first introduce a model-free approach to meta-level control based on reinforcement learning and prove its optimality. We then offer a general meta-level control technique that can use different reinforcement learning methods. Finally, we show that our approach is effective across several common benchmark domains and a mobile robot domain.


Title: Simultaneous task allocation and motion scheduling for complex tasks executed by multiple robots
Key Words: cutting  industrial manipulators  motion control  multi-robot systems  optimisation  rapid prototyping (industrial)  scheduling  spot welding  time-varying portion  generic optimization method  varying complexity  dual-arm robot  robot arm  motion scheduling  multiple robot coordination  simultaneous task allocation  additive manufacturing  cutting  spot welding  bolt tightening  bolt inserting  robot kinematics  Task analysis  Robot kinematics  Planning  Collision avoidance  Job shop scheduling  Resource management  task scheduling  dual-arm manipulation  motion planning  multi-robot systems 
Abstract: The coordination of multiple robots operating simultaneously in the same workspace requires the integration of task allocation and motion scheduling. We focus on tasks in which the robot's actions are not confined to small volumes, but can also occupy a large time-varying portion of the workspace, such as in welding along a line. The optimization of such tasks presents a considerable challenge mainly due to the fact that different variants of task execution exist, for instance, there can be multiple starting points of lines or closed curves, differentfilling patterns of areas, etc. We propose a generic and computationally efficient optimization method which is based on constraint programming. It takes into account the kinematics of the robots and guarantees that the motions of the robots are collision-free while minimizing the overall makespan. We evaluate our approach on several use-cases of varying complexity: cutting, additive manufacturing, spot welding, inserting and tightening bolts, performed by a dual-arm robot. In terms of the makespan, the result is superior to task execution by one robot arm as well as by two arms not working simultaneously.


Title: Efficient Planning for High-Speed MAV Flight in Unknown Environments Using Online Sparse Topological Graphs
Key Words: aerospace navigation  air safety  autonomous aerial vehicles  collision avoidance  graph theory  infinite horizon  microrobots  mobile robots  probability  robot vision  search problems  high-speed MAV flight  online sparse topological graphs  safe high-speed autonomous navigation  local planning grid  computationally-efficient planning architecture  safe high-speed operation  longer-term memory  motion primitive-based local receding horizon planner  memory-efficient sparse topological graph  planning system  complex simulation environments  robot decision making  probabilistic collision avoidance  safe rerouting  Planning  Collision avoidance  Robot sensing systems  Libraries  Safety  Trajectory 
Abstract: Safe high-speed autonomous navigation for MAVs in unknown environments requires fast planning to enable the robot to adapt and react quickly to incoming information about obstacles within the world. Furthermore, when operating in environments not known a priori, the robot may make decisions that lead to dead ends, necessitating global replanning through a map of the environment outside of a local planning grid. This work proposes a computationally-efficient planning architecture for safe high-speed operation in unknown environments that incorporates a notion of longer-term memory into the planner enabling the robot to accurately plan to locations no longer contained within a local map. A motion primitive-based local receding horizon planner that uses a probabilistic collision avoidance methodology enables the robot to generate safe plans at fast replan rates. To provide global guidance, a memory-efficient sparse topological graph is created online from a time history of the robot's path and a geometric notion of visibility within the environment to search for alternate pathways towards the desired goal if a dead end is encountered. The safety and performance of the proposed planning system is evaluated at speeds up to 10m/s, and the approach is tested in a set of large-scale, complex simulation environments containing dead ends. These scenarios lead to failure cases for competing methods; however, the proposed approach enables the robot to safely reroute and reach the desired goal.


Title: Evaluating Adaptation Performance of Hierarchical Deep Reinforcement Learning
Key Words: learning (artificial intelligence)  multi-agent systems  neural nets  differentiated sub-policies  hierarchical controller  adaptation performance  hierarchical deep reinforcement learning  policy performance  confidence- based training process  Training  Adaptation models  Trajectory  Games  Learning (artificial intelligence)  Robots  Switches 
Abstract: Deep Reinforcement Learning has been used to exploit specific environments, but has difficulty transferring learned policies to new situations. This issue poses a problem for practical applications of Reinforcement Learning, as real-world scenarios may introduce unexpected differences that drastically reduce policy performance. We propose the use of differentiated sub-policies governed by a hierarchical controller to support adaptation in such scenarios. We also introduce a confidence- based training process for the hierarchical controller which improves training stability and convergence times. We evaluate these methods in a new Capture the Flag environment designed to explore adaptation in autonomous multi-agent settings.


Title: Iterator-Based Temporal Logic Task Planning
Key Words: autonomous aerial vehicles  control system synthesis  discrete event systems  mobile robots  path planning  temporal logic  task specifications  universally quantified locations  constant time  hybrid control  discrete event controller  synthesised plan  iterator-based temporal logic task planning  robotic systems  state explosion  discrete locations  fixed-wing unmanned aerial vehicle  Task analysis  Robot sensing systems  Planning  Fires  Unmanned aerial vehicles 
Abstract: Temporal logic task planning for robotic systems suffers from state explosion when specifications involve large numbers of discrete locations. We provide a novel approach, particularly suited for task specifications with universally quantified locations, that has constant time with respect to the number of locations, enabling synthesis of plans for an arbitrary number of them. We propose a hybrid control framework that uses an iterator to manage the discretised workspace hiding it from a plan enacted by a discrete event controller. A downside of our approach is that it incurs in increased overhead when executing a synthesised plan. We demonstrate that the overhead is reasonable for missions of a fixed-wing Unmanned Aerial Vehicle in simulated and real scenarios for up to 700000 locations.


Title: Reactive Temporal Logic Planning for Multiple Robots in Unknown Environments
Key Words: mobile robots  multi-robot systems  path planning  robot dynamics  temporal logic  multiple robots  reactive mission  unknown environment  temporal logic planning approaches  robot dynamics  known environments  abstraction-free LTL planning algorithm  complex mission planning  complex planning tasks  co-safe linear temporal logic formulas  reactive temporal logic planning  Robot sensing systems  Planning  Task analysis  Heuristic algorithms  Automata 
Abstract: This paper proposes a new reactive mission planning algorithm for multiple robots that operate in unknown environments. The robots are equipped with individual sensors that allow them to collectively learn and continuously update a map of the unknown environment. The goal of the robots is to accomplish complex tasks, captured by global co-safe Linear Temporal Logic (LTL) formulas. The majority of existing temporal logic planning approaches rely on discrete abstractions of the robot dynamics operating in known environments and, as a result, they cannot be applied to the more realistic scenarios where the environment is initially unknown. In this paper, we address this novel challenge by proposing the first reactive, and abstraction-free LTL planning algorithm that can be applied for complex mission planning of multiple robots operating in unknown environments. Our algorithm is reactive in the sense that temporal logic planning is adapting to the updated map of the environment and abstraction-free as it does not rely on designing abstractions of robot dynamics. Our proposed algorithm is complete under mild assumptions on the structure of the environment and the sensor models. Our paper provides extensive numerical simulations and hardware experiments that illustrate the theoretical analysis and show that the proposed algorithm can address complex planning tasks in unknown environments.


Title: Higher Order Function Networks for View Planning and Multi-View Reconstruction
Key Words: image reconstruction  learning (artificial intelligence)  neural nets  object detection  robot vision  shape recognition  solid modelling  stereo image processing  Higher Order function networks  multiview reconstruction  visual inspection  neural network  shape information  deep learning  complete 3D reconstruction  Higher Order Functions  reconstruction quality  multiview HOF network  image acquisition  view planning  visibility quality  shape representation  Three-dimensional displays  Image reconstruction  Planning  Cameras  Surface reconstruction  Inspection  Shape 
Abstract: We consider the problem of planning views for a robot to acquire images of an object for visual inspection and reconstruction. In contrast to offline methods which require a 3D model of the object as input or online methods which rely on only local measurements, our method uses a neural network which encodes shape information for a large number of objects. We build on recent deep learning methods capable of generating a complete 3D reconstruction of an object from a single image. Specifically, in this work, we extend a recent method which uses Higher Order Functions (HOF) to represent the shape of the object. We present a new generalization of this method to incorporate multiple images as input and establish a connection between visibility and reconstruction quality. This relationship forms the foundation of our view planning method where we compute viewpoints to visually cover the output of the multiview HOF network with as few images as possible. Experiments indicate that our method provides a good compromise between online and offline methods: Similar to online methods, our method does not require the true object model as input. In terms of number of views, it is much more efficient. In most cases, its performance is comparable to the optimal offline case even on object classes the network has not been trained on.


Title: Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments
Key Words: learning (artificial intelligence)  mobile robots  path planning  trajectory control  learned navigation strategies  residual reinforcement learning framework  robotic manipulation literature  mobile robots  residual control effect  sub-optimal classical controller  data efficiency  cluttered indoor navigation tasks  residual reactive navigation  Navigation  Robots  Uncertainty  Training  Task analysis  Learning (artificial intelligence)  Machine learning 
Abstract: In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRRN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world. The code and training environment for this project is made publicly available at https://sites.google.com/view/srrn/home.


Title: Online Grasp Plan Refinement for Reducing Defects During Robotic Layup of Composite Prepreg Sheets
Key Words: dexterous manipulators  Gaussian processes  path planning  quality control  regression analysis  sheet materials  online grasp plan refinement  robotic layup  composite prepreg  high-performance composites  sheet layup  composite components  deformable sheets  robotic cell  layup process  manual layup  online refinement  environmental factors  Gaussian process regression model offline  grasp plans  GPR  Trajectory  Grasping  Grippers  Robot sensing systems  Computational modeling  Service robots 
Abstract: High-performance composites are increasingly being used in the industry. Sheet layup is a process of manufacturing composite components using deformable sheets. We have developed a robotic cell to automate the layup process and overcome the limitations of the manual layup. Generating offline trajectories for robots and executing them without online refinement can introduce defects in the process due to uncertainties in the model of the sheet and environmental factors. Our system computes layup and grasping trajectories for the robots and refines them during the layup process based on the sensor data. We use an approach that augments physical experiments with simulations to train a Gaussian process regression model offline. The use of GPR enables us to quickly refine grasp plans and perform a defect-free layup without slowing down the layup process. We present experimental results on two components.


Title: Learning Continuous 3D Reconstructions for Geometrically Aware Grasping
Key Words: dexterous manipulators  grippers  image reconstruction  learning (artificial intelligence)  mobile robots  neural nets  robot vision  shape recognition  solid modelling  continuous 3D reconstructions  geometrically aware grasping  deep learning  grasp synthesis  unseen objects  partial object views  indirect geometric reasoning  explicit geometric reasoning  grasping system  reconstruction network  grasp success classifier  continuous grasp optimization  grasp metrics  96 robot grasping trials  Three-dimensional displays  Optimization  Collision avoidance  Grasping  Geometry  Robot sensing systems 
Abstract: Deep learning has enabled remarkable improvements in grasp synthesis for previously unseen objects from partial object views. However, existing approaches lack the ability to explicitly reason about the full 3D geometry of the object when selecting a grasp, relying on indirect geometric reasoning derived when learning grasp success networks. This abandons explicit geometric reasoning, such as avoiding undesired robot object collisions. We propose to utilize a novel, learned 3D reconstruction to enable geometric awareness in a grasping system. We leverage the structure of the reconstruction network to learn a grasp success classifier which serves as the objective function for a continuous grasp optimization. We additionally explicitly constrain the optimization to avoid undesired contact, directly using the reconstruction. We examine the role of geometry in grasping both in the training of grasp metrics and through 96 robot grasping trials. Our results can be found on https://sites.google.com/view/reconstruction-grasp/.


