total paper: 212
Title: Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies
Key Words: control engineering computing  learning (artificial intelligence)  manipulators  mobile robots  neural nets  adversarial agent training  learned control policies  autonomous driving  deep neural network-driven  adversarial reinforcement learning agent  autonomous vehicle problem  automated black box testing  safety-critical applications  control policy  deep neural networks  autonomous vehicles  robot navigation  robotic arm manipulation  control problems  deep learning  deep control policies  Testing  Autonomous vehicles  Training  Learning (artificial intelligence)  Machine learning  Robots  Robustness 
Abstract: Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.


Title: Voxel-based General Voronoi Diagram for Complex Data with Application on Motion Planning
Key Words: assembly planning  CAD  computational geometry  data structures  graphics processing units  path planning  production engineering computing  rendering (computer graphics)  path planning  motion planning  assembly sequence planning  real-world CAD-scenarios  disassembly path  GVD  Voronoi voxel history  disassembly paths  general Voronoi diagram graph  hash table-based data structure  error-bounded wavefront propagation  error-bounded GPU render approach  voxel-based general Voronoi diagram  roadmap  representative vehicle data set  Octrees  Planning  Three-dimensional displays  Approximation algorithms  Task analysis  Runtime 
Abstract: One major challenge in Assembly Sequence Planning (ASP) for complex real-world CAD-scenarios is to find appropriate disassembly paths for all assembled parts. Such a path places demands on its length and clearance. In the past, it became apparent that planning the disassembly path based on the (approximate) General Voronoi Diagram (GVD) is a good approach to achieve these requirements. But for complex real-world data, every known solution for computing the GVD is either too slow or very memory consuming, even if only approximating the GVD.We present a new approach for computing the approximate GVD and demonstrate its practicability using a representative vehicle data set. We can calculate an approximation of the GVD within minutes and meet the accuracy requirement of some few millimeters for the subsequent path planning. This is achieved by voxelizing the surface with a common error-bounded GPU render approach. We then use an error-bounded wavefront propagation technique and combine it with a novel hash table-based data structure, the so-called Voronoi Voxel History (VVH). On top of the GVD, we present a novel approach for the creation of a General Voronoi Diagram Graph (GVDG) that leads to an extensive roadmap. For the later motion planning task this roadmap can be used to suggest appropriate disassembly paths.


Title: Stable Control in Climbing and Descending Flight under Upper Walls using Ceiling Effect Model based on Aerodynamics
Key Words: aerodynamics  aircraft control  autonomous aerial vehicles  helicopters  motion control  rotors (mechanical)  stability  wakes  ceiling effect model  flight control stability  multirotor unmanned aerial vehicles  rotor thrust  vertical flight tests  in unsteady state model based controller  aerodynamics based thrust model  vertical climbing  vertical descending  wake interaction  momentum theory  Rotors  Data models  Adaptation models  Aerodynamics  Steady-state  Atmospheric modeling  Sensors 
Abstract: Stable flight control under ceilings is difficult for multirotor Unmanned Aerial Vehicles (UAVs). The wake interaction between rotors and upper walls, called the "ceiling effect", causes an increase of rotor thrust. As a result of the thrust increase, multi-rotors are drawn upward abruptly and collide with ceilings. In previous work, several thrust models of the ceiling effect have been proposed for stable flight under ceilings, assuming that the airflow around rotors is in steady states. However, the airflow around rotors in vertical flight is not in steady states and each thrust model in previous work is skillfully determined based on large amounts of precise experimental data. In this paper, we introduce an aerodynamics-based thrust model and a stable control method under ceilings. This model is derived from the momentum theory and the relationship between vertical climbing/descending rates of rotors and an induced velocity. To confirm our proposed model, we collect thrust data at various vertical rates in flight. In addition, we use only onboard sensors to estimate selfstate for structural inspections. Consequently, we reveal that the proposed model is consistent with the experimental results. Based on an aerodynamic model, we need not collect large amounts of precise experimental data to realize stable flight. Furthermore, the vertical flight tests under ceilings demonstrate that our in-unsteady-state-model-based controller outperforms the conventional steady-state ones.


Title: Motion Primitives-based Path Planning for Fast and Agile Exploration using Aerial Robots
Key Words: aerospace robotics  collision avoidance  mobile robots  motion control  motion primitives-based path planning  agile exploration  aerial robots  path planning strategy  microaerial vehicles  volumetric representation  collision-tolerant flying robot  velocity 2.0 m/s  size 0.8 m  Vehicle dynamics  Collision avoidance  Path planning  Unmanned aerial vehicles  Educational robots  Dynamics 
Abstract: This paper presents a novel path planning strategy for fast and agile exploration using aerial robots. Tailored to the combined need for large-scale exploration of challenging and confined environments, despite the limited endurance of micro aerial vehicles, the proposed planner employs motion primitives to identify admissible paths that search the configuration space, while exploiting the dynamic flight properties of small aerial robots. Utilizing a computationally efficient volumetric representation of the environment, the planner provides fast collision-free and future-safe paths that maximize the expected exploration gain and ensure continuous fast navigation through the unknown environment. The new method is field-verified in a set of deployments relating to subterranean exploration and specifically, in both modern and abandoned underground mines in Northern Nevada utilizing a 0.55m-wide collision-tolerant flying robot exploring with a speed of up to 2m/s and navigating sections with width as small as 0.8m.


Title: Unsupervised Anomaly Detection for Self-flying Delivery Drones
Key Words: aerodynamics  autonomous aerial vehicles  control engineering computing  learning (artificial intelligence)  mobile robots  regression analysis  unsupervised anomaly detection  hybrid aerial vehicles  machine learning models  flight profiles  flight log measurements  sensor readings  predictive flight dynamics models  aircraft aerodynamics  self-flying delivery drones  Aerodynamics  Smoothing methods  Robustness  Training  Anomaly detection  Optimization  Aircraft 
Abstract: We propose a novel anomaly detection framework for a fleet of hybrid aerial vehicles executing high-speed package pickup and delivery missions. The detection is based on machine learning models of normal flight profiles, trained on millions of flight log measurements of control inputs and sensor readings. We develop a new scalable algorithm for robust regression which can simultaneously fit predictive flight dynamics models while identifying and discarding abnormal flight missions from the training set. The resulting unsupervised estimator has a very high breakdown point and can withstand massive contamination of training data to uncover what normal flight patterns look like, without requiring any form of prior knowledge of aircraft aerodynamics or manual labeling of anomalies upfront. Across many different anomaly types, spanning simple 3sigma statistical thresholds to turbulence and other equipment anomalies, our models achieve high detection rates across the board. Our method consistently outperforms alternative robust detection methods on synthetic benchmark problems. To the best of our knowledge, dynamics modeling of hybrid delivery drones for anomaly detection at the scale of 100 million measurements from 5000 real flight missions in variable flight conditions is unprecedented.


Title: Keyfilter-Aware Real-Time UAV Object Tracking
Key Words: autonomous aerial vehicles  image filtering  image motion analysis  learning (artificial intelligence)  mobile robots  object detection  object tracking  robot vision  SLAM (robots)  keyframe-based simultaneous localization and mapping  keyfilter restriction  visual tracking  background distraction  filter corruption  boundary effect  unmanned aerial vehicle  correlation filter-based tracking  keyfilter-aware real-time UAV object tracking  Unmanned aerial vehicles  Correlation  Visualization  Object tracking  Frequency-domain analysis  Real-time systems 
Abstract: Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.


Title: Aerial Regrasping: Pivoting with Transformable Multilink Aerial Robot
Key Words: aerospace robotics  dexterous manipulators  grippers  motion control  remotely operated vehicles  stability  aerial regrasping  aerial manipulator  dexterous manipulation  transformable multilink aerial robot  transformable multilink drone  grasping stability  thrust force  continous grasping force  admittance controller  impedance controller  contact aware regrasping  Task analysis  Force  Grasping  Unmanned aerial vehicles  Rotors  End effectors 
Abstract: Regrasping is one of the most common and important manipulation skills used in our daily life. However, aerial regrasping has not been seriously investigated yet, since most of the aerial manipulator lacks dexterous manipulation abilities except for the basic pick-and-place. In this paper, we focus on pivoting a long box, which is one of the most classical problems among regrasping researches, using a transformable multilink aerial robot. First, we improve our previous controller by compensating for the external wrench. Second, we optimize the joints configuration of our transformable multilink drone for stable grasping form under the constraints of thrust force and joints effort. Third, we sequentially optimize the grasping force in the pivoting process. The optimization goal is to generate continous grasping force whilst maximizing the friction force in case of the downwash, which would influence the grasped object and is difficult to model. Fourth, we develop the impedance controller in joint space and admittance controller in task space. As far as we know, it is the first research to achieve extrinsic contact-aware regrasping task on aerial robots.


Title: Deep Merging: Vehicle Merging Controller Based on Deep Reinforcement Learning with Embedding Network
Key Words: control engineering computing  learning (artificial intelligence)  road traffic control  road vehicles  traffic engineering computing  traffic conditions  traffic flow  Deep Merging  vehicle Merging controller  embedding network  highway merging sections  lane change  vehicle controller  merging efficiency  merging section  target vehicle speed  controlled vehicle speed  deep reinforcement learning network architecture  learning efficiency  merging behavior  Merging  Machine learning  Feature extraction  Acceleration  Road transportation  Vehicle dynamics  Network architecture 
Abstract: Vehicles at highway merging sections must make lane changes to join the highway. This lane change can generate congestion. To reduce congestion, vehicles should merge so as not to affect traffic flow as much as possible. In our study, we propose a vehicle controller called Deep Merging that uses deep reinforcement learning to improve the merging efficiency of vehicles while considering the impact on traffic flow. The system uses the images of a merging section as input to output the target vehicle speed. Moreover, an embedding network for estimating the controlled vehicle speed is introduced to the deep reinforcement learning network architecture to improve the learning efficiency. In order to show the effectiveness of the proposed method, the merging behavior and traffic conditions in several situations are verified by experiments using a traffic simulator. Through these experiments, it is confirmed that the proposed method enables controlled vehicles to effectively merge without adversely affecting to the traffic flow.


Title: Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels
Key Words: data analysis  object detection  radar imaging  road vehicles  supervised learning  traffic engineering computing  noisy labels  noise-aware training techniques  training data  weakly supervised vehicle detection  radar labels  object detector  image-based vehicle detection  Training  Noise measurement  Training data  Radar imaging  Labeling  Lenses 
Abstract: It has been demonstrated that the performance of an object detector degrades when it is used outside the domain of the data used to train it. However, obtaining training data for a new domain can be time consuming and expensive. In this work we demonstrate how a radar can be used to generate plentiful (but noisy) training data for image-based vehicle detection. We then show that the performance of a detector trained using the noisy labels can be considerably improved through a combination of noise-aware training techniques and relabelling of the training data using a second viewpoint. In our experiments, using our proposed process improves average precision by more than 17 percentage points when training from scratch and 10 percentage points when fine-tuning a pre-trained model.


Title: Robust Lane Detection with Binary Integer Optimization
Key Words: driver information systems  integer programming  object detection  road safety  road vehicles  vehicle dynamics  false positive cone detections  binary integer optimization  competition rules  average cone spacings  minimum track width  robust lane detection  Formula Student Driverless  FSD  student teams  autonomous racecar  main dynamic event  unknown track  Automobiles  Optimization  Robustness  Meters  Roads  Real-time systems  Pipelines 
Abstract: Formula Student Driverless (FSD) is a competition where student teams compete to build an autonomous racecar. The main dynamic event in FSD is trackdrive, where the racecar traverses an unknown track with lanes demarcated by cones. One major challenge of the event is to determine the boundaries of the lane from cones perceived online despite false positive cone detections and sharp turns. We present a binary integer optimization to address this problem by leveraging a priori knowledge from competition rules on parameters such as average cone spacings and minimum track width. In this paper, we describe our approach, and analyze its latency, accuracy, and robustness to false positive cone detections. This approach is used on-board to solve the lane detection problem during the competition in real-time.


Title: A Synchronization Approach for Achieving Cooperative Adaptive Cruise Control Based Non-Stop Intersection Passing
Key Words: adaptive control  control system synthesis  distributed control  Lyapunov methods  mobile robots  multi-robot systems  position control  road traffic control  road vehicles  stability  velocity control  synchronization approach  adaptive cruise control based nonstop intersection passing  intelligent vehicles  cruise control performance  traffic congestion  increasing traffic flow capacity  CACC problem  synchronization control  spatial-temporal synchronization mechanism  vehicle platoon control  robust CACC  cross-coupling based space synchronization mechanism  distributed control algorithm  single-lane CACC  vehicle-to-vehicle communications  autonomous vehicles  desired platoon trajectory  expected inter-vehicle distance  enter-time scheduling mechanism  high-level intersection control strategy  Lyapunov-based time-domain stability analysis approach  traditional string stability based approach  CACC system  Synchronization  Stability analysis  Cruise control  Robustness  Autonomous vehicles  Motion control  Acceleration 
Abstract: Cooperative adaptive cruise control (CACC) of intelligent vehicles contributes to improving cruise control performance, reducing traffic congestion, saving energy and increasing traffic flow capacity. In this paper, we resolve the CACC problem from the viewpoint of synchronization control, our main idea is to introduce the spatial-temporal synchronization mechanism into vehicle platoon control to achieve the robust CACC and to further realize the non-stop intersection control. Firstly, by introducing the cross-coupling based space synchronization mechanism, a distributed control algorithm is presented to achieve the single-lane CACC in the presence of vehicle-to-vehicle (V2V) communications, which enables autonomous vehicles to track the desired platoon trajectory while synchronizing their longitudinal velocities to keeping the expected inter-vehicle distance. Secondly, by designing the enter-time scheduling mechanism (temporal synchronization), a high-level intersection control strategy is proposed to command vehicles to form a virtual platoon to pass through the intersection without stopping. Thirdly, a Lyapunov-based time-domain stability analysis approach is presented. Compared with the traditional string stability based approach, the proposed approach guarantees the global asymptotical convergence of the proposed CACC system. Experiments in the small-scale simulated system demonstrate the effectiveness of the proposed approach.


Title: Urban Driving with Conditional Imitation Learning
Key Words: cameras  computer vision  decision making  driver information systems  learning (artificial intelligence)  mobile robots  road traffic  road vehicles  real-world urban autonomous driving  human driving demonstrations  user-defined route  single camera view  heavily cropped frames  lateral control  longitudinal control  real-world complexities  end-to-end conditional imitation learning approach  urban routes  simple traffic  autonomous vehicle  European urban streets  urban driving  hand-crafting generalised decision-making rules  Cameras  Sensor fusion  Autonomous vehicles  Roads  Computational modeling  Aerospace electronics  Training 
Abstract: Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.


Title: Vehicle Localization Based on Visual Lane Marking and Topological Map Matching
Key Words: image filtering  image fusion  image matching  iterative methods  Kalman filters  nonlinear filters  road vehicles  traffic engineering computing  visual lane marking  topological map matching  autonomous vehicle navigation  driver assistance systems  online vehicle localization  distinct map matching algorithms  visual lane tracker  map matching algorithm  grid map  iterative closest point based lane level map matching  Roads  Iterative closest point algorithm  Global Positioning System  Dead reckoning  Sensors  Visualization  Cameras  Map Relative Localization  Topological Map Matching  Lane Level Matching  Autonomous Vehicles 
Abstract: Accurate and reliable localization is crucial to autonomous vehicle navigation and driver assistance systems. This paper presents a novel approach for online vehicle localization in a digital map. Two distinct map matching algorithms are proposed: i) Iterative Closest Point (ICP) based lane level map matching is performed with visual lane tracker and grid map ii) decision-rule based approach is used to perform topological map matching. Results of both the map matching algorithms are fused together with GPS and dead reckoning using Extended Kalman Filter to estimate vehicle's pose relative to the map. The proposed approach has been validated on real life conditions on an equipped vehicle. Detailed analysis of the experimental results show improved localization using the two aforementioned map matching algorithms.


Title: Drone-aided Localization in LoRa IoT Networks
Key Words: autonomous aerial vehicles  Internet of Things  wide area networks  realistic simulated scenario  fully autonomous localization system  ten-fold improvement  localization precision  fixed network  UAV  localization accuracy  LoRa IoT networks  node localization  widespread IoT communication technologies  Long Range Wide Area Network  long communication distances  drone-aided localization system  communication system  search algorithm  Internet of Things  3D mobility  Logic gates  Drones  Estimation  Receivers  Servers  Internet of Things  Global navigation satellite system 
Abstract: Besides being part of the Internet of Things (IoT), drones can play a relevant role in it as enablers. The 3D mobility of UAVs can be exploited to improve node localization in IoT networks for, e.g., search and rescue or goods localization and tracking. One of the widespread IoT communication technologies is Long Range Wide Area Network (LoRaWAN), which allows achieving long communication distances with low power. In this work, we present a drone-aided localization system for LoRa networks in which a UAV is used to improve the estimation of a node's location initially provided by the network. We characterize the relevant parameters of the communication system and use them to develop and test a search algorithm in a realistic simulated scenario. We then move to the full implementation of a real system in which a drone is seamlessly integrated into Swisscom's LoRa network. The drone coordinates with the network with a two-way exchange of information which results in an accurate and fully autonomous localization system. The results obtained in our field tests show a ten-fold improvement in localization precision with respect to the estimation provided by the fixed network. Up to our knowledge, this is the first time a UAV is successfully integrated in a LoRa network to improve its localization accuracy.


Title: Iterative Learning based feedforward control for Transition of a Biplane-Quadrotor Tailsitter UAS
Key Words: aerodynamics  aircraft control  attitude control  autonomous aerial vehicles  error compensation  feedforward  helicopters  iterative learning control  learning systems  neurocontrollers  pitch control (position)  polynomials  propellers  robust control  wind tunnels  iterative learning based feedforward control  biplane-quadrotor tailsitter UAS  time on-board algorithm  forward transition maneuver  repeated flight trials  pitch angle  propeller thrust  polynomials  simplified aerodynamics  optimal coefficients  terminal conditions  air speed  modeling error compensation  geometric attitude controller  flight modes  feedforward law  high-fidelity thrust model  orientation angle  neural network model  experimental flight trials  learning algorithm  maneuver control  wind tunnel data  feedforward thrust  robustness  UAS  Aerodynamics  Propellers  Wind tunnels  Atmospheric modeling  Feedforward systems  Data models  Trajectory  VTOL UAS  transition maneuver  iterative learning 
Abstract: This paper provides a real time on-board algorithm for a biplane-quadrotor to iteratively learn a forward transition maneuver via repeated flight trials. The maneuver is controlled by regulating the pitch angle and propeller thrust according to feedforward control laws that are parameterized by polynomials. Based on a nominal model with simplified aerodynamics, the optimal coefficients of the polynomials are chosen through simulation such that the maneuver is completed with specified terminal conditions on altitude and air speed. In order to compensate for modeling errors, repeated flight trials are performed by updating the feedforward control parameters according to an iterative learning algorithm until the maneuver is perfected. A geometric attitude controller, valid for all flight modes is employed in order to track the pitch angle according to the feedforward law. Further, a high-fidelity thrust model of the propeller for varying advance-ratio and orientation angle is obtained from wind tunnel data which is captured using a neural network model. This facilitates accurate application of feedforward thrust for varying flow conditions during transition. Experimental flight trials are performed to demonstrate the robustness and rapid convergence of the proposed learning algorithm.


Title: UBAT: On Jointly Optimizing UAV Trajectories and Placement of Battery Swap Stations
Key Words: ant colony optimisation  autonomous aerial vehicles  battery powered vehicles  electric vehicles  optimal number  charging stations  UBAT  ant colony optimization  UAV trajectories  battery swap stations  unmanned aerial vehicles  UAVs  flight time  charging station deployment problem  NP-hard problem  Charging stations  Batteries  Trajectory  Optimization  Sensors  Unmanned aerial vehicles  Euclidean distance 
Abstract: Unmanned aerial vehicles (UAVs) have been widely used in many applications. The limited flight time of UAVs, however, still remains as a major challenge. Although numerous approaches have been developed to recharge the battery of UAVs effectively, little is known about optimal methodologies to deploy charging stations. In this paper, we address the charging station deployment problem with an aim to find the optimal number and locations of charging stations such that the system performance is maximized. We show that the problem is NP-Hard and propose UBAT, a heuristic framework based on the ant colony optimization (ACO) to solve the problem. Additionally, a suite of algorithms are designed to enhance the execution time and the quality of the solutions for UBAT. Through extensive simulations, we demonstrate that UBAT effectively performs multi-objective optimization of generation of UAV trajectories and placement of charging stations that are within 8.3% and 7.3% of the true optimal solutions, respectively.


Title: Bi-Convex Approximation of Non-Holonomic Trajectory Optimization
Key Words: approximation theory  convex programming  minimisation  quadratic programming  robot kinematics  nonholonomic trajectory optimization  nonholonomic kinematics  nonlinearly maps control input  nonconvex  bi-convex cost  constraint functions  bi-convex part  nonholonomic behavior  nonlinear penalty  nonlinear costs  bi-convex structure  bi-convex approximation  autonomous cars  fixed-wing aerial vehicles  computational tractability  alternating minimization  sequential quadratic programming  interior-point methods  Trajectory optimization  Minimization  Computational modeling  Collision avoidance  Robots  Mathematical model 
Abstract: Autonomous cars and fixed-wing aerial vehicles have the so-called non-holonomic kinematics which non-linearly maps control input to states. As a result, trajectory optimization with such a motion model becomes highly non-linear and non-convex. In this paper, we improve the computational tractability of non-holonomic trajectory optimization by reformulating it in terms of a set of bi-convex cost and constraint functions along with a non-linear penalty. The bi-convex part acts as a relaxation for the non-holonomic trajectory optimization while the residual of the penalty dictates how well its output obeys the non-holonomic behavior. We adopt an alternating minimization approach for solving the reformulated problem and show that it naturally leads to the replacement of the challenging non-linear penalty with a globally valid convex surrogate. Along with the common cost functions modeling goal-reaching, trajectory smoothness, etc., the proposed optimizer can also accommodate a class of non-linear costs for modeling goal-sets, while retaining the bi-convex structure. We benchmark the proposed optimizer against off-the-shelf solvers implementing sequential quadratic programming and interior-point methods and show that it produces solutions with similar or better cost as the former while significantly outperforming the latter. Furthermore, as compared to both off-the-shelf solvers, the proposed optimizer achieves more than 20x reduction in computation time.


Title: FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation using Monocular Fisheye Camera for Autonomous Driving
Key Words: cameras  distance measurement  feature extraction  image classification  image motion analysis  image sampling  learning (artificial intelligence)  mobile robots  object detection  road vehicles  robot vision  traffic engineering computing  video signal processing  autonomous driving  nonlinear distortions  complex algorithms  Euclidean distance estimation  fisheye cameras  automotive scenes  accurate depth supervision  dense depth supervision  self-supervised learning approaches  self-supervised scale-aware framework  raw monocular fisheye videos  applying rectification  piece-wise linear approximation  fisheye projection surface  re-sampling distortion  monocular methods  unseen fisheye video  self-supervised scale-aware distance estimation  monocular fisheye camera  Cameras  Estimation  Training  Euclidean distance  Image reconstruction  Three-dimensional displays  Robot vision systems 
Abstract: Fisheye cameras are commonly used in applications like autonomous driving and surveillance to provide a large field of view (> 180o). However, they come at the cost of strong non-linear distortions which require more complex algorithms. In this paper, we explore Euclidean distance estimation on fisheye cameras for automotive scenes. Obtaining accurate and dense depth supervision is difficult in practice, but self-supervised learning approaches show promising results and could potentially overcome the problem. We present a novel self-supervised scale-aware framework for learning Euclidean distance and ego-motion from raw monocular fisheye videos without applying rectification. While it is possible to perform piece-wise linear approximation of fisheye projection surface and apply standard rectilinear models, it has its own set of issues like re-sampling distortion and discontinuities in transition regions. To encourage further research in this area, we will release our dataset as part of the WoodScape project [1]. We further evaluated the proposed algorithm on the KITTI dataset and obtained state-of-the-art results comparable to other self-supervised monocular methods. Qualitative results on an unseen fisheye video demonstrate impressive performance1.


Title: Steerable Burrowing Robot: Design, Modeling and Experiments
Key Words: drag  impact (mechanical)  mobile robots  numerical analysis  robot dynamics  vehicle dynamics  thrusting mechanism  depth dependent model  steerable burrowing robot  vibro-impact mechanism  rotating bevel-tip head  nonholonomic model  steering mechanism  hybrid dynamics model  S-shaped trajectory  Robot kinematics  Needles  Numerical models  Solid modeling  Trajectory  Springs 
Abstract: This paper investigates a burrowing robot that can maneuver and steer while being submerged in a granular medium. The robot locomotes using an internal vibro-impact mechanism and steers using a rotating bevel-tip head. We formulate and investigate a non-holonomic model for the steering mechanism and a hybrid dynamics model for the thrusting mechanism. We perform a numerical analysis of the dynamics of the robot's thrusting mechanism using a simplified, orientation and depth dependent model for the drag forces acting on the robot. We first show, in simulation, that by carefully tuning various control input parameters, the thrusting mechanism can drive the robot both forward and backward. We present several experiments designed to evaluate and verify the simulative results using a proof-of-concept robot. We show that different input amplitudes indeed affect the direction of motion, as suggested by the simulation. We further demonstrate the ability of the robot to perform a simple S-shaped trajectory. These experiments demonstrate the feasibility of the robot's design and fidelity of the model.


Title: Using Manipulation to Enable Adaptive Ground Mobility
Key Words: adhesion  legged locomotion  manipulators  permanent magnets  propulsion  road vehicles  wheels  swappable propulsors  adhesion forces  wheeled locomotion  legged locomotion  autonomous ground vehicles  terrain  whegs  physical adaptation  multipurpose manipulators  propulsion system  adaptive ground mobility  permanent magnets  functional prototype robot  Legged locomotion  Wheels  Manipulators  Steel  Force  Mechanism design  mobile manipulation  wheeled robots 
Abstract: In order to accomplish various missions, autonomous ground vehicles must operate on a wide range of terrain. While many systems such as wheels and whegs can navigate some types of terrain, none are optimal across all. This creates a need for physical adaptation. This paper presents a broad new approach to physical adaptation that relies on manipulation. Specifically, we explore how multipurpose manipulators can enable ground vehicles to dramatically modify their propulsion system in order to optimize performance across various terrain. While this approach is general and widely applicable, this work focuses on physically switching between wheels and legs. We outline the design of "swappable propulsors" that combine the powerful adhesion forces of permanent magnets with geometric features for easy detachment. We provide analysis on how the swappable propulsors can be manipulated, and use these results to create a functional prototype robot. This robot can use its manipulator to change between wheeled and legged locomotion. Our experimental results illustrate how this approach can enhance energy efficiency and versatility.


Title: Cooperative Autonomy and Data Fusion for Underwater Surveillance With Networked AUVs
Key Words: autonomous underwater vehicles  mobile robots  sensor fusion  target tracking  underwater acoustic communication  AUV cooperative strategies  data fusion  realistic underwater surveillance scenarios  networked AUVs  data sharing  robotic networks  underwater surveillance applications  autonomous underwater vehicles  CMRE Anti-Submarine Warfare network  track management module  robot autonomy software  track classification  T2T association  Target tracking  Robot kinematics  Sonar  Receivers  Signal processing algorithms 
Abstract: Cooperative autonomy and data sharing can largely improve the mission performance of robotic networks in underwater surveillance applications. In this paper, we describe the cooperative autonomy used to control the Autonomous Underwater Vehicles (AUVs) acting as sonar receiver nodes in the CMRE Anti-Submarine Warfare (ASW) network. The paper focuses on a track management module that was integrated in the robot autonomy software for enabling the share of information. Track to track (T2T) associations are used for improving track classification and for creating a common tactical picture, necessary for AUV cooperative strategies. We also present a new cooperative data-driven AUV behaviour that exploits the spatial diversity of multiple robots for improving target tracking and for facilitating T2T associations. We report results with real data collected at sea that validate the approach. The reported results are one of the first examples that show the potential of cooperative autonomy and data fusion in realistic underwater surveillance scenarios characterised by limited communications.


Title: Bidirectional Resonant Propulsion and Localization for AUVs
Key Words: autonomous underwater vehicles  diaphragms  electromagnetic actuators  marine control  mobile robots  motion control  robot vision  SLAM (robots)  electromagnetic voice coil motor  bidirectional resonant propulsion  AUV localization  thrust vectors  diaphragm pump mechanism  resonant motion  actuator design  bidirectional resonant pump  autonomous underwater vehicles  Propulsion  Resonant frequency  Strain  Standards  Damping  Reliability engineering 
Abstract: Battery life, reliability, and localization are prominent challenges in the design of autonomous underwater vehicles (AUVs). This work aims to address facets of these challenges using a single system. We describe the design of a bidirectional resonant pump that uses a single electromagnetic voice coil motor (VCM) capable of rotation around a central two degree-of-freedom flexure stage axis. This actuator design produces highly efficient resonant motion that drives two orthogonally oriented diaphragms simultaneously. The operation of this diaphragm pump mechanism produces both adjustable thrust vectors at the aft surface of the AUV and a monotonic relationship between thrust vectors and operating frequency. We propose using the unique frequency to thrust relationship to enhance AUV localization capabilities. We construct a prototype and use it to experimentally demonstrate the feasibility of the directionally-tunable resonance concept.


Title: Hierarchical Planning in Time-Dependent Flow Fields for Marine Robots
Key Words: autonomous underwater vehicles  computational complexity  graph theory  marine vehicles  mobile robots  path planning  remotely operated vehicles  hierarchical planning  time-dependent flow fields  shortest paths  flow predictions  motion planning  slow marine robots  dynamic ocean currents  time-dependent graphs  polynomial-time algorithm  continuous trajectories  time-varying edge costs  underlying flow field  continuous algorithm  time complexity  path quality properties  autonomous marine vehicle  marine robotics  time-varying ocean predictions  East Australian Current  Planning  Robots  Vehicle dynamics  Oceans  Heuristic algorithms  Prediction algorithms  Trajectory 
Abstract: We present an efficient approach for finding shortest paths in flow fields that vary as a sequence of flow predictions over time. This approach is applicable to motion planning for slow marine robots that are subject to dynamic ocean currents. Although the problem is NP-hard in general form, we incorporate recent results from the theory of finding shortest paths in time-dependent graphs to construct a polynomial-time algorithm that finds continuous trajectories in time-dependent flow fields. The algorithm has a hierarchical structure where a graph is constructed with time-varying edge costs that are derived from sets of continuous trajectories in the underlying flow field. We show that the continuous algorithm retains the time complexity and path quality properties of the discrete graph solution, and demonstrate its application to surface and underwater vehicles including a traversal along the East Australian Current with an autonomous marine vehicle. Results show that the algorithm performs efficiently in practice and can find paths that adapt to changing ocean currents. These results are significant to marine robotics because they allow for efficient use of time-varying ocean predictions for motion planning.


Title: Navigation in the Presence of Obstacles for an Agile Autonomous Underwater Vehicle
Key Words: autonomous underwater vehicles  collision avoidance  feature extraction  mobile robots  navigation  optimisation  robot vision  stereo image processing  fly-overs  AUV  cluttered space  navigation framework  sampling-based correction procedure  obstacles detection  real-time 3D autonomous navigation  agile autonomous underwater vehicle  Trajopt  3D path-optimization planning  visual features detection  Planning  Three-dimensional displays  Navigation  Optimization  Robots  Trajectory  Cameras 
Abstract: Navigation underwater traditionally is done by keeping a safe distance from obstacles, resulting in "fly-overs" of the area of interest. Movement of an autonomous underwater vehicle (AUV) through a cluttered space, such as a shipwreck or a decorated cave, is an extremely challenging problem that has not been addressed in the past. This paper proposes a novel navigation framework utilizing an enhanced version of Trajopt for fast 3D path-optimization planning for AUVs. A sampling-based correction procedure ensures that the planning is not constrained by local minima, enabling navigation through narrow spaces. Two different modalities are proposed: planning with a known map results in efficient trajectories through cluttered spaces; operating in an unknown environment utilizes the point cloud from the visual features detected to navigate efficiently while avoiding the detected obstacles. The proposed approach is rigorously tested, both on simulation and in-pool experiments, proven to be fast enough to enable safe real-time 3D autonomous navigation for an AUV.


Title: Underwater Image Super-Resolution using Deep Residual Multipliers
Key Words: image resolution  learning (artificial intelligence)  neural nets  robot vision  underwater vehicles  single image super-resolution  autonomous underwater robots  adversarial training pipeline  perceptual quality  global content  local style information  USR-248  SISR  state-of-the-art models  deep residual multipliers  deep residual network-based generative model  underwater image super-resolution  noisy visual conditions  Training  Image resolution  Robots  Data models  Cameras  Pipelines  Generators 
Abstract: We present a deep residual network-based generative model for single image super-resolution (SISR) of underwater imagery for use by autonomous underwater robots. We also provide an adversarial training pipeline for learning SISR from paired data. In order to supervise the training, we formulate an objective function that evaluates the perceptual quality of an image based on its global content, color, and local style information. Additionally, we present USR-248, a large-scale dataset of three sets of underwater images of `high' (640×480) and `low' (80 × 60, 160 × 120, and 320×240) resolution. USR-248 contains paired instances for supervised training of 2×, 4×, or 8× SISR models. Furthermore, we validate the effectiveness of our proposed model through qualitative and quantitative experiments and compare the results with several state-of-the-art models' performances. We also analyze its practical feasibility for applications such as scene understanding and attention modeling in noisy visual conditions.


Title: Nonlinear Synchronization Control for Short-Range Mobile Sensors Drifting in Geophysical Flows
Key Words: actuators  mobile radio  oceanographic techniques  synchronisation  telecommunication control  wireless sensor networks  short-range mobile sensors drifting  geophysical flows  ocean monitoring applications  minimal actuation capabilities  active drifters  gyre flows  data exchange  nonlinear synchronization control strategy  rendezvous regions  large-scale mobile sensor networks  numerical simulations  small-scale experiments  Synchronization  Sensors  Vehicle dynamics  Orbits  Robots  Oscillators  Dynamics 
Abstract: This paper presents a synchronization controller for mobile sensors that are minimally actuated and can only communicate with each other over a very short range. This work is motivated by ocean monitoring applications where large-scale sensor networks consisting of drifters with minimal actuation capabilities, i.e., active drifters, are employed. We assume drifters are tasked to monitor regions consisting of gyre flows where their trajectories are periodic. As drifters in neighboring regions move into each other's proximity, it presents an opportunity for data exchange and synchronization to ensure future rendezvous. We present a nonlinear synchronization control strategy to ensure that drifters will periodically rendezvous and maximize the time they are in their rendezvous regions. Numerical simulations and small-scale experiments validate the efficacy of the control strategy and hint at extensions to large-scale mobile sensor networks.


Title: Real-time Simulation of Non-Deformable Continuous Tracks with Explicit Consideration of Friction and Grouser Geometry
Key Words: friction  mobile robots  motion control  tracked vehicles  trajectory control  velocity control  nondeformable continuous tracks  grouser geometry  real-time simulation  circular segments  robot body  segment link  track rotation  friction  rough terrain  track trajectory  velocity constraints  tracked vehicles  Robots  Trajectory  Tracking  Friction  Collision avoidance  Real-time systems  Wheels 
Abstract: In this study, we developed a real-time simulation method for non-deformable continuous tracks having grousers for rough terrain by explicitly considering the collision and friction between the tracks and the ground. In the proposed simulation method, an arbitrary trajectory of a track is represented with multiple linear and circular segments, each of which is a link connected to a robot body. The proposed method sets velocity constraints between each segment link and the robot body, to simulate the track rotation around the body. To maintain the shape of a track, it also restores the positions of the segment links when required. Experimental comparisons with other existing real-time simulation methods demonstrated that while the proposed method considered the grousers and the friction with the ground, it was comparable to them in terms of the computational speed. Experimental comparison of the simulations based on the proposed method and a physical robot exhibited that the former was comparable to the precise motion of the robot on rough or uneven terrain.


Title: Learning error models for graph SLAM
Key Words: autonomous aerial vehicles  graph theory  mobile robots  path planning  robot vision  SLAM (robots)  resistance distance  covisibility graph  simulated UAV coverage path  uncertainty models  monocular graph SLAM  topological features  error model learning  UAV coverage path planning trajectories  Simultaneous localization and mapping  Resistance  Uncertainty  Computational modeling  Computer architecture  Predictive models  Cameras 
Abstract: Following recent developments, this paper investigates the possibility to predict uncertainty models for monocular graph SLAM using topological features of the problem. An architecture to learn relative (i.e. inter-keyframe) uncertainty models using the resistance distance in the covisibility graph is presented. The proposed architecture is applied to simulated UAV coverage path planning trajectories and an analysis of the approaches strengths and shortcomings is provided.


Title: Real-Time UAV Path Planning for Autonomous Urban Scene Reconstruction
Key Words: autonomous aerial vehicles  computational geometry  image reconstruction  path planning  robot vision  SLAM (robots)  unmanned aerial vehicles  large-scale scene mapping  autonomous urban scene reconstruction  point cloud reconstruction  reconstruction quality  large-scale scene reconstruction  real-time UAV path planning  SLAM  Buildings  Image reconstruction  Three-dimensional displays  Path planning  Drones  Cameras  Layout 
Abstract: Unmanned aerial vehicles (UAVs) are frequently used for large-scale scene mapping and reconstruction. However, in most cases, drones are operated manually, which should be more effective and intelligent. In this article, we present a method of real-time UAV path planning for autonomous urban scene reconstruction. Considering the obstacles and time costs, we utilize the top view to generate the initial path. Then we estimate the building heights and take close-up pictures that reveal building details through a SLAM framework. To predict the coverage of the scene, we propose a novel method which combines information on reconstructed point clouds and possible coverage areas. The experimental results reveal that the reconstruction quality of our method is good enough. Our method is also more time-saving than the state-of-the-arts.


Title: Privacy-Aware UAV Flights through Self-Configuring Motion Planning
Key Words: aircraft control  autonomous aerial vehicles  collision avoidance  data privacy  decision making  mobile robots  motion control  privacy-aware UAV flights  unmanned aerial vehicle  uncertain obstacles  motion planning algorithms  privacy-preserving requirements  privacy risk aware motion planning method  privacy-sensitive sensor  safety  energy hard constraints  dynamically detected restricted areas  decision making method  test flights  DJI Matrice 100 UAV  self-configuring motion planning  Privacy  Planning  Cameras  Sensors  Trajectory  Safety  Unmanned aerial vehicles 
Abstract: During flights, an unmanned aerial vehicle (UAV) may not be allowed to move across certain areas due to soft constraints such as privacy restrictions. Current methods on self-adaption focus mostly on motion planning such that the trajectory does not trespass predetermined restricted areas. When the environment is cluttered with uncertain obstacles, however, these motion planning algorithms are not flexible enough to find a trajectory that satisfies additional privacy-preserving requirements within a tight time budget during the flights. In this paper, we propose a privacy risk aware motion planning method through the reconfiguration of privacy-sensitive sensors. It minimises environmental impact by re-configuring the sensor during flight, while still guaranteeing the safety and energy hard constraints such as collision avoidance and timeliness. First, we formulate a model for assessing privacy risks of dynamically detected restricted areas. In case the UAV cannot find a feasible solution to satisfy both hard and soft constraints from the current configuration, our decision making method can then produce an optimal reconfiguration of the privacy-sensitive sensor with a more efficient trajectory. We evaluate the proposal through various simulations with different settings in a virtual environment and also validate the approach through real test flights on DJI Matrice 100 UAV.


Title: Robust Real-time UAV Replanning Using Guided Gradient-based Optimization and Topological Paths
Key Words: autonomous aerial vehicles  gradient methods  helicopters  mobile robots  optimisation  path planning  search problems  trajectory control  quadrotor trajectory replanning  replanning method  GTO  path-guided optimization approach  topological path searching algorithm  independent trajectory optimization  output superior replanned trajectories  gradient-based trajectory optimization  UAV replanning  Splines (mathematics)  Linear programming  Trajectory optimization  Robustness  Safety 
Abstract: Gradient-based trajectory optimization (GTO) has gained wide popularity for quadrotor trajectory replanning. However, it suffers from local minima, which is not only fatal to safety but also unfavorable for smooth navigation. In this paper, we propose a replanning method based on GTO addressing this issue systematically. A path-guided optimization (PGO) approach is devised to tackle infeasible local minima, which improves the replanning success rate significantly. A topological path searching algorithm is developed to capture a collection of distinct useful paths in 3-D environments, each of which then guides an independent trajectory optimization. It activates a more comprehensive exploration of the solution space and output superior replanned trajectories. Benchmark evaluation shows that our method outplays state-of-the-art methods regarding replanning success rate and optimality. Challenging experiments of aggressive autonomous flight are presented to demonstrate the robustness of our method. We will release our implementation as an open-source package1.


Title: Learning-based Path Planning for Autonomous Exploration of Subterranean Environments
Key Words: autonomous aerial vehicles  graph theory  learning by example  mobile robots  optical radar  path planning  robot programming  sampled data systems  tunnels  autonomous exploration  subterranean environments  aerial robots  training expert  imitation learning  underground mine drifts  tunnels  graph based path planner  learning based path planning  LiDAR  range data sampling  Robot sensing systems  Path planning  Training  Training data  Planning  Robot kinematics 
Abstract: In this work we present a new methodology on learning-based path planning for autonomous exploration of subterranean environments using aerial robots. Utilizing a recently proposed graph-based path planner as a "training expert" and following an approach relying on the concepts of imitation learning, we derive a trained policy capable of guiding the robot to autonomously explore underground mine drifts and tunnels. The algorithm utilizes only a short window of range data sampled from the onboard LiDAR and achieves an exploratory behavior similar to that of the training expert with a more than an order of magnitude reduction in computational cost, while simultaneously relaxing the need to maintain a consistent and online reconstructed map of the environment. The trained path planning policy is extensively evaluated both in simulation and experimentally within field tests relating to the autonomous exploration of underground mines.


Title: Distributed Rotor-Based Vibration Suppression for Flexible Object Transport and Manipulation
Key Words: autonomous aerial vehicles  control engineering computing  controllability  helicopters  mechanical engineering computing  mobile robots  optimisation  path planning  rotors  vibration control  RVM design  optimal placement  flexible object transport  manipulated object  object size  quadrotor usage  distributed RVMs  constrained optimization problem  aerial-ground manipulator system  robot-based vibration suppression module  distributed rotor-based vibration suppression  controllability gramian  multiple aerial-ground manipulator system  Rotors  Vibrations  Mathematical model  Manipulators  Controllability  Torque 
Abstract: The RVM (Robot-based Vibration Suppression Modules) is proposed for the manipulation and transport of a large flexible object. Since the RVM is easily attachable/detachable to the object, this RVM allows distributing over the manipulated object so that it is scalable to the object size. The composition of the system is partly motivated by the MAGMaS (Multiple Aerial-Ground Manipulator System) [1]- [3], however, since the quadrotor usage is mechanically too complicated and its design is not optimized for manipulation, thus we overcome these limitations using distributed RVMs and newly developed theory. For this, we first provide a constrained optimization problem of RVM design with the minimum number of rotors, so that the feasible thrust force is maximized while it minimizes undesirable wrench and its own weight. Then, we derive the full dynamics and elucidate a controllability condition with multiple distributed RVMs and show that even if multiple, their structures turn out similar to [2] composed with a single quadrotor. We also elucidate the optimal placement of the RVM via the usage of controllability gramian which is not even alluded in [2] and established for the first time here. Experiments are performed to demonstrate the effectiveness of the proposed theory.


Title: Aerial Manipulation using Model Predictive Control for Opening a Hinged Door
Key Words: aerospace robotics  collision avoidance  control system synthesis  dynamic programming  manipulators  observers  position control  predictive control  robust control  three-term control  model predictive control  hinged door  environment interaction  aerial robot  multirotor-based aerial manipulator  daily-life moving structure  collision avoidance  differential dynamic programming  disturbance observer  robust controller  Manipulators  Vehicle dynamics  Mathematical model  Dynamics  Trajectory  Servomotors 
Abstract: Existing studies for environment interaction with an aerial robot have been focused on interaction with static surroundings. However, to fully explore the concept of an aerial manipulation, interaction with moving structures should also be considered. In this paper, a multirotor-based aerial manipulator opening a daily-life moving structure, a hinged door, is presented. In order to address the constrained motion of the structure and to avoid collisions during operation, model predictive control (MPC) is applied to the derived coupled system dynamics between the aerial manipulator and the door involving state constraints. By implementing a constrained version of differential dynamic programming (DDP), MPC can generate position setpoints to the disturbance observer (DOB)-based robust controller in real-time, which is validated by our experimental results.


Title: Integrated Motion Planner for Real-time Aerial Videography with a Drone in a Dense Environment
Key Words: autonomous aerial vehicles  collision avoidance  graph theory  mobile robots  object detection  quadratic programming  video recording  dense environment  drone  autonomous videography task  3-D obstacle environment  moving object  target motion prediction module  hierarchical chasing planner  covariant optimization  bi-level structure  smooth planner  graph-search method  chasing corridor  subsequent phase  smooth trajectory  dynamically feasible trajectory  integrated motion planner  real-time aerial videography  autonomous videography task  source code  quadratic programming  Drones  Trajectory  Safety  Optimization  Measurement  Shape  Real-time systems 
Abstract: This work suggests an integrated approach for a drone (or multirotor) to perform an autonomous videography task in a 3-D obstacle environment by following a moving object. The proposed system includes 1) a target motion prediction module which can be applied to dense environments and 2) a hierarchical chasing planner. Leveraging covariant optimization, the prediction module estimates the future motion of the target assuming it efforts to avoid the obstacles. The other module, chasing planner, is in a bi-level structure composed of preplanner and smooth planner. In the first phase, we exploit a graph-search method to plan a chasing corridor which incorporates safety and visibility of target. In the subsequent phase, we generate a smooth and dynamically feasible trajectory within the corridor using quadratic programming (QP). We validate our approach with multiple complex scenarios and actual experiments. The source code and the experiment video can be found in https://github.com/icsl-Jeon/traj_gen_vis and https://www.youtube.com/watch?v=_JSwXBwYRl8.


Title: FG-GMM-based Interactive Behavior Estimation for Autonomous Driving Vehicles in Ramp Merging Control *
Key Words: automobiles  Gaussian processes  graph theory  mobile robots  probability  road traffic control  road vehicles  traffic engineering computing  autonomous driving vehicles  autonomous driving cars  factor graph  human-designed models  FG-GMM-based interactive behavior estimation  ramp merging control  significant social interaction  probabilistic graphical model merging control model  Merging  Automobiles  Estimation  Autonomous vehicles  Probabilistic logic  Mathematical model  Roads 
Abstract: Interactive behavior is important for autonomous driving vehicles, especially for scenarios like ramp merging which require significant social interaction between autonomous driving vehicles and human-driven cars. This paper enhances our previous Probabilistic Graphical Model (PGM) merging control model for the interactive behavior of autonomous driving vehicles. To better estimate the interactive behavior for autonomous driving cars, a Factor Graph (FG) is used to describe the dependency among observations and estimate other cars' intentions. Real trajectories are used to approximate the model instead of human-designed models or cost functions. Forgetting factors and a Gaussian Mixture Model (GMM) are also applied in the intention estimation process for stabilization, interpolation and smoothness. The advantage of the factor graph is that the relationship between its nodes can be described by self-defined functions, instead of probabilistic relationships as in PGM, giving more flexibility. Continuity of GMM also provides higher accuracy than the previous discrete speed transition model. The proposed method enhances the overall performance of intention estimation, in terms of collision rate and average distance between cars after merging, which means it is safer and more efficient.


Title: Cooperative Perception and Localization for Cooperative Driving
Key Words: cooperative systems  Kalman filters  location based services  mobile robots  multi-robot systems  nonlinear filters  road vehicles  sensor fusion  vehicle sensors  extended Kalman filters  fully autonomous road vehicles  cooperative driving  cooperative perception  high fidelity sensors  low fidelity sensors  localization information  Sensor systems  Time measurement  Roads  Fuses  Current measurement  Bandwidth 
Abstract: Fully autonomous vehicles are expected to share the road with less advanced vehicles for a significant period of time. Furthermore, an increasing number of vehicles on the road are equipped with a variety of low-fidelity sensors which provide some perception and localization data, but not at a high enough quality for full autonomy. In this paper, we develop a perception and localization system that allows a vehicle with low-fidelity sensors to incorporate high-fidelity observations from a vehicle in front of it, allowing both vehicles to operate with full autonomy. The resulting system generates perception and localization information that is both low-noise in regions covered by high-fidelity sensors and avoids false negatives in areas only observed by low-fidelity sensors, while dealing with latency and dropout of the communication link between the two vehicles. At its core, the system uses a set of Extended Kalman filters which incorporate observations from both vehicles' sensors and extrapolate them using information about the road geometry. The perception and localization algorithms are evaluated both in simulation and on real vehicles as part of a full cooperative driving system.


Title: Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images
Key Words: collision avoidance  mobile robots  off-road vehicles  robot programming  robot vision  supervised learning  autonomous driving  vision based controllers  navigation learning  model robustmess  planning foresight  self supervised method  collision avoidance  sparse aerial images  off road driving  smooth terrain traversal  visual obstructions  on-board sensors  terrain roughness  model free reinforcement learning  unstructured outdoor environments  on-board camera  rough terrain  Predictive models  Navigation  Cameras  Planning  Computational modeling  Visualization  Robots 
Abstract: We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both first-person and overhead aerial image inputs to our model. We nd that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only first-person imagery. Video and project details can be found at www.cim.mcgill.ca/mrl/offroad_driving/.


Title: Ground Texture Based Localization Using Compact Binary Descriptors
Key Words: image matching  image texture  pose estimation  robot vision  ground texture based localization  compact binary descriptors  global localization  subsequent local localization updates  compact binary feature descriptors  localization success rates  self-contained method  matching strategy  identity matching  Feature extraction  Cameras  Robots  Latches  Task analysis  Asphalt  Pose estimation 
Abstract: Ground texture based localization is a promising approach to achieve high-accuracy positioning of vehicles. We present a self-contained method that can be used for global localization as well as for subsequent local localization updates, i.e. it allows a robot to localize without any knowledge of its current whereabouts, but it can also take advantage of a prior pose estimate to reduce computation time significantly. Our method is based on a novel matching strategy, which we call identity matching, that is based on compact binary feature descriptors. Identity matching treats pairs of features as matches only if their descriptors are identical. While other methods for global localization are faster to compute, our method reaches higher localization success rates, and can switch to local localization after the initial localization.


Title: Reliable Data Association for Feature-Based Vehicle Localization using Geometric Hashing Methods
Key Words: feature extraction  optical radar  reliability  road vehicles  sensor fusion  feature-based vehicle localization  data association  local environment  plausible feature associations  safe localization  localization features  geometric hashing methods  error propagation  cylindrical objects  LiDAR data  Feature extraction  Reliability  Data mining  Noise measurement  Standards  Visualization  Object recognition 
Abstract: Reliable data association represents a main challenge of feature-based vehicle localization and is the key to integrity of localization. Independent of the type of features used, incorrect associations between detected and mapped features will provide erroneous position estimates. Only if the uniqueness of a local environment is represented by the features that are stored in the map, the reliability of localization is enhanced. In this work, a new approach based on Geometric Hashing is introduced to the field of data association for feature-based vehicle localization. Without any information on a prior position, the proposed method allows to efficiently search large map regions for plausible feature associations. Therefore, odometry and GNSS-based inputs can be neglected, which reduces the risk of error propagation and enables safe localization. The approach is demonstrated on approximately 10min of data recorded in an urban scenario. Cylindrical objects without distinctive descriptors, which were extracted from LiDAR data, serve as localization features. Experimental results both demonstrate the feasibility as well as limitations of the approach.


Title: The OmniScape Dataset
Key Words: cameras  image segmentation  motorcycles  object detection  stereo image processing  traffic engineering computing  omnidirectional images  semantic segmentation  depth map  ground truth images  CARLA Simulator  open-source simulator  catadioptric images  OmniScape dataset  autonomous driving research  Grand Theft Auto V  two-wheeled vehicles  motorcycle  Cameras  Semantics  Vehicle dynamics  Motorcycles  Image segmentation  Virtual environments  Roads 
Abstract: Despite the utility and benefits of omnidirectional images in robotics and automotive applications, there are no datasets of omnidirectional images available with semantic segmentation, depth map, and dynamic properties. This is due to the time cost and human effort required to annotate ground truth images. This paper presents a framework for generating omnidirectional images using images that are acquired from a virtual environment. For this purpose, we demonstrate the relevance of the proposed framework on two well-known simulators: CARLA Simulator, which is an open-source simulator for autonomous driving research, and Grand Theft Auto V (GTA V), which is a very high quality video game. We explain in details the generated OmniScape dataset, which includes stereo fisheye and catadioptric images acquired from the two front sides of a motorcycle, including semantic segmentation, depth map, intrinsic parameters of the cameras and the dynamic parameters of the motorcycle. It is worth noting that the case of two-wheeled vehicles is more challenging than cars due to the specific dynamic of these vehicles.


Title: Reliable frame-to-frame motion estimation for vehicle-mounted surround-view camera systems
Key Words: cameras  distance measurement  mobile robots  motion estimation  optimisation  path planning  robot vision  frame-to-frame visual odometry  vehicle-mounted surround-view camera system  reliable frame-to-frame motion estimation  vehicle-mounted surround-view camera systems  surround-view multicamera system  autonomous driving  3D point related optimization variables  two-view optimization scheme  nonholonomic characteristics  relative displacements  nonholonomic vehicle motion  overly simplified assumptions  single camera  existing camera  relative vehicle displacement  Conferences  Automation  Reliability  Motion estimation  Cameras  Robot vision systems 
Abstract: Modern vehicles are often equipped with a surround-view multi-camera system. The current interest in autonomous driving invites the investigation of how to use such systems for a reliable estimation of relative vehicle displacement. Existing camera pose algorithms either work for a single camera, make overly simplified assumptions, are computationally expensive, or simply become degenerate under non-holonomic vehicle motion. In this paper, we introduce a new, reliable solution able to handle all kinds of relative displacements in the plane despite the possibly non-holonomic characteristics. We furthermore introduce a novel two-view optimization scheme which minimizes a geometrically relevant error without relying on 3D point related optimization variables. Our method leads to highly reliable and accurate frame-to-frame visual odometry with a full-size, vehicle-mounted surround-view camera system.


Title: Human-Centric Active Perception for Autonomous Observation
Key Words: aerospace robotics  Markov processes  mobile communication  mobile robots  optimisation  space vehicles  autonomous observation systems  human activity  multiobjective optimization  autonomous human observation problem  robot-centric costs  scalarization-based MultiObjective MDP methods  NASA Astrobee robot operating  human-centric active perception  robot autonomy  SemiMDP formulation  constrained MDP method  NASA Astrobee robot  Task analysis  Cameras  Robot vision systems  Collision avoidance  Cost function 
Abstract: As robot autonomy improves, robots are increasingly being considered in the role of autonomous observation systems - free-flying cameras capable of actively tracking human activity within some predefined area of interest. In this work, we formulate the autonomous observation problem through multi-objective optimization, presenting a novel Semi-MDP formulation of the autonomous human observation problem that maximizes observation rewards while accounting for both human- and robot-centric costs. We demonstrate that the problem can be solved with both scalarization-based Multi-Objective MDP methods and Constrained MDP methods, and discuss the relative benefits of each approach. We validate our work on activity tracking using a NASA Astrobee robot operating within a simulated International Space Station environment.


Title: Distance and Steering Heuristics for Streamline-Based Flow Field Planning
Key Words: computational fluid dynamics  flow simulation  marine robots  mobile robots  path planning  robot dynamics  artificial flow field  East Australian current  streamline-based flow field planning  motion planning  streamline-based planning  fluid dynamics  travel distance  incompressible flows  ocean currents  distance functions  Euclidean distance  stream function  steering heuristics  ocean prediction data  autonomous marine robots  Planning  Aerospace electronics  Vehicle dynamics  Two dimensional displays  Space exploration  Space vehicles  Oceans 
Abstract: Motion planning for vehicles under the influence of flow fields can benefit from the idea of streamline-based planning, which exploits ideas from fluid dynamics to achieve computational efficiency. Important to such planners is an efficient means of computing the travel distance and direction between two points in free space, but this is difficult to achieve in strong incompressible flows such as ocean currents. We propose two useful distance functions in analytical form that combine Euclidean distance with values of the stream function associated with a flow field, and with an estimation of the strength of the opposing flow between two points. Further, we propose steering heuristics that are useful for steering towards a sampled point. We evaluate these ideas by integrating them with RRT* and comparing the algorithm's performance with state-of-the-art methods in an artificial flow field and in actual ocean prediction data in the region of the dominant East Australian Current between Sydney and Brisbane. Results demonstrate the method's computational efficiency and ability to find high-quality paths outperforming state-of-the-art methods, and show promise for practical use with autonomous marine robots.


Title: Enhancing Coral Reef Monitoring Utilizing a Deep Semi-Supervised Learning Approach
Key Words: convolutional neural nets  feature extraction  marine engineering  object detection  supervised learning  deep neural network  sample extraction  coral object dataset  coral reef monitoring  deep semisupervised learning approach  coral species detection  convolutional neural network-based object detector  Detectors  Training  Object detection  Monitoring  Tracking  Predictive models  Semantics 
Abstract: Coral species detection underwater is a challenging problem. There are many cases when even the experts (marine biologists) fail to recognize corals, hence limiting ground truth annotation for training a robust detection system. Identifying coral species is fundamental for enabling the monitoring of coral reefs, a task currently performed by humans, which can be automated with the use of underwater robots. By employing temporal cues using a tracker on a high confidence prediction by a convolutional neural network-based object detector, we augment the collected dataset for the retraining of the object detector. However, using trackers to extract examples also introduces hard or mislabelled samples, which is counterproductive and will deteriorate the performance of the detector. In this work, we show that employing a simple deep neural network to filter out hard or mislabelled samples can help regulate sample extraction. We empirically evaluate our approach in a coral object dataset, collected via an Autonomous Underwater Vehicle (AUV) and human divers, that shows the benefit of incorporating extracted examples obtained from tracking. This work also demonstrates how controlling sample generation by tracking using a simple deep neural network can further improve an object detector.


Title: DOB-Net: Actively Rejecting Unknown Excessive Time-Varying Disturbances
Key Words: control system synthesis  feedback  learning (artificial intelligence)  neurocontrollers  observers  optimal control  position control  recurrent neural nets  robots  robot control capabilities  disturbance dynamics observer network  controller network  conventional DOB mechanisms  recurrent neural networks  optimal control signals  conventional feedback controllers  DOB-Net  disturbance OB-server network  observer-integrated reinforcement learning  Observers  History  Vehicle dynamics  Robots  Optimization  Optimal control  Dynamics 
Abstract: This paper presents an observer-integrated Reinforcement Learning (RL) approach, called Disturbance OB-server Network (DOB-Net), for robots operating in environments where disturbances are unknown and time-varying, and may frequently exceed robot control capabilities. The DOB-Net integrates a disturbance dynamics observer network and a controller network. Originated from conventional DOB mechanisms, the observer is built and enhanced via Recurrent Neural Networks (RNNs), encoding estimation of past values and prediction of future values of unknown disturbances in RNN hidden state. Such encoding allows the controller generate optimal control signals to actively reject disturbances, under the constraints of robot control capabilities. The observer and the controller are jointly learned within policy optimization by advantage actor critic. Numerical simulations on position regulation tasks have demonstrated that the proposed DOB-Net significantly outperforms conventional feedback controllers and classical RL policy.


Title: Demonstration of Autonomous Nested Search for Local Maxima Using an Unmanned Underwater Vehicle
Key Words: autonomous underwater vehicles  oceanographic equipment  oceanographic techniques  search problems  hydrothermal plume model  hydrothermal vent emissions  local maxima  autonomous nested search method  hydrothermal venting  sufficient autonomy  mission concept  solar system  extra-terrestrial life  Ocean World  unmanned underwater vehicle  Vents  Oceans  Vehicle dynamics  Underwater vehicles  Earth  Numerical models  Base stations 
Abstract: Ocean Worlds represent one of the best chances for extra-terrestrial life in our solar system. A new mission concept must be developed to explore these oceans. This mission would require traversing the 10s of km thick icy shell and releasing a submersible into the ocean below. During the transit of the icy shell and the exploration of the ocean, the vehicle(s) would be out of contact with Earth for weeks or potentially months at a time. During this time the vehicle must have sufficient autonomy to locate and study scientific targets of interest. One such target of interest is hydrothermal venting. We have previously developed an autonomous nested search method to locate and investigate sources of hydrothermal venting by locating local maxima in hydrothermal vent emissions. In this work we demonstrate this approach on board an OceanServer Iver2 AUV in Chesapeake Bay, MD using simulated sensor data from a hydrothermal plume model. This represents the first step towards the deployment of this approach in conditions analogous to those that we might expect on an Ocean World.


Title: VALID: A Comprehensive Virtual Aerial Image Dataset
Key Words: computer vision  feature extraction  image classification  image segmentation  object detection  stereo image processing  aerial imagery  unmanned aerial vehicle tasks  single ground truth type  virtual environment  high-resolution images  virtual scenes  comprehensive virtual aerial image dataset  visual ground truth data  Image segmentation  Semantics  Task analysis  Object detection  Image color analysis  Benchmark testing  Labeling 
Abstract: Aerial imagery plays an important role in land-use planning, population analysis, precision agriculture, and unmanned aerial vehicle tasks. However, existing aerial image datasets generally suffer from the problem of inaccurate labeling, single ground truth type, and few category numbers. In this work, we implement a simulator that can simultaneously acquire diverse visual ground truth data in the virtual environment. Based on that, we collect a comprehensive Virtual AeriaL Image Dataset named VALID, consisting of 6690 high-resolution images, all annotated with panoptic segmentation on 30 categories, object detection with oriented bounding box, and binocular depth maps, collected in 6 different virtual scenes and 5 various ambient conditions (sunny, dusk, night, snow and fog). To our knowledge, VALID is the first aerial image dataset that can provide panoptic level segmentation and complete dense depth maps. We analyze the characteristics of VALID and evaluate state-of-the-art methods for multiple tasks to provide reference baselines. The experiment results demonstrate that VALID is well presented and challenging. The dataset is available at https://sites.google.com/view/valid-dataset/.


Title: An Iterative Quadratic Method for General-Sum Differential Games with Feedback Linearizable Dynamics
Key Words: control system synthesis  convergence of numerical methods  differential games  feedback  game theory  iterative methods  linear quadratic control  linearisation techniques  nonlinear control systems  path planning  feedback linearizable dynamics  nonlinear optimal control community  multiplayer general-sum differential games  ILQ methods  local equilibria  interactive motion planning problems  iterative procedures  initial conditions  hyperparameter choices  unsafe trajectories  dynamical systems  algorithmic reliability  feedback linearizable structure  iterative linear-quadratic method  Games  Heuristic algorithms  Planning  Feedback linearization  Iterative methods  Vehicle dynamics  Optimal control 
Abstract: Iterative linear-quadratic (ILQ) methods are widely used in the nonlinear optimal control community. Recent work has applied similar methodology in the setting of multi-player general-sum differential games. Here, ILQ methods are capable of finding local equilibria in interactive motion planning problems in real-time. As in most iterative procedures, however, this approach can be sensitive to initial conditions and hyperparameter choices, which can result in poor computational performance or even unsafe trajectories. In this paper, we focus our attention on a broad class of dynamical systems which are feedback linearizable, and exploit this structure to improve both algorithmic reliability and runtime. We showcase our new algorithm in three distinct traffic scenarios, and observe that in practice our method converges significantly more often and more quickly than was possible without exploiting the feedback linearizable structure.


Title: A Morphable Aerial-Aquatic Quadrotor with Coupled Symmetric Thrust Vectoring
Key Words: autonomous aerial vehicles  autonomous underwater vehicles  design engineering  helicopters  mobile robots  stability  single design difficult  normal aerial vehicles  rotational acceleration  quadrotor based vehicle  vehicle body  design considerations  aerial-aquatic quadrotor  coupled symmetric thrust vectoring  aerial-aquatic vehicles  fluid resistance  energy efficient position  morphable aerial-aquatic quadrotor  mechanical actuation  static stability  Buoyancy  Robots  Force  Torque  Prototypes  Propellers 
Abstract: Hybrid aerial-aquatic vehicles have the unique ability of travelling in both air and water and can benefit from both lower fluid resistance in air and energy efficient position holding in water. However, they have to address the differing requirements which make optimising a single design difficult. While existing examples have shown the possibility of such vehicles, they are mostly structurally identical to normal aerial vehicles with minor adjustments to work underwater. Instead of using rotational acceleration to direct a component of thrust in surge and sway, we propose a quadrotor based vehicle that tilts its rotors about the respective arm so that a larger component of thrust can be directed in the lateral plane or in the opposite direction without rotating the vehicle body. A small-scale prototype of this design is presented here, detailing the design considerations including mechanical actuation, static stability and waterproofing.


Title: An Autonomous Intercept Drone with Image-based Visual Servo
Key Words: autonomous aerial vehicles  cameras  robot vision  visual servoing  autonomous intercept drone  unwanted drone  radio wave gun  image-based visual servo algorithm  Cameras  Visualization  Mathematical model  Drones  Channel models  Servomotors  Angular velocity 
Abstract: For most people on the ground, facing an unwanted drone buzzing around overhead, there is not a lot that we can do, especially if it is out of gun (radio wave gun or shotgun) range. A solution to this is to use intercept drones that seek out and bring down other drones. In order to make the interception autonomous, an image-based visual servo algorithm is designed with a forward-looking monocular camera. The control command, namely the angular velocity and thrust, is generated for intercept drones to implement accurate and fast interception. The proposed method is demonstrated in both hardware-in-the-loop simulation and demonstrative flight experiments.


Title: On the Human Control of a Multiple Quadcopters with a Cable-suspended Payload System
Key Words: actuators  attitude control  autonomous aerial vehicles  control system synthesis  helicopters  mobile robots  multi-robot systems  position control  cable-suspended payload system  human control  multiple quadcopters system  leader quadcopter  payload attitude controller  cable attitude controller  quadcopter-payload system  Payloads  Oscillators  Angular velocity  Attitude control  Vehicle dynamics  Trajectory  Quadcopters  Human control  Cable-suspended payload  Collaborative transportation  Multi-agents 
Abstract: A quadcopter is an under-actuated system with only four control inputs for six degrees of freedom, and yet the human control of a quadcopter is simple enough to be learned with some practice. In this work, we consider the problem of human control of a multiple quadcopters system to transport a cable-suspended payload. The coupled dynamics of the system, due to the inherent physical constraints, is used to develop a leader-follower architecture where the leader quadcopter is controlled directly by a human operator and the followers are controlled with the proposed Payload Attitude Controller and Cable Attitude Controller. Experiments, where a human operator flew a two quadcopters system to transport a cable-suspended payload, were conducted to study the performance of proposed controller. The results demonstrated successful implementation of human control in these systems. This work presents the possibility of enabling manual control for on-the-go maneuvering of the quadcopter-payload system which motivates aerial transportation in the unknown environments.


Title: A*3D Dataset: Towards Autonomous Driving in Challenging Environments
Key Words: image annotation  image colour analysis  mobile robots  object detection  optical radar  road vehicle radar  stereo image processing  traffic engineering computing  A*3D dataset  self-driving cars  3D object detection  3D object annotations  autonomous driving research  nuScenes dataset  KITTI dataset  high-density images  LiDAR data  RGB images  computer vision tasks  Three-dimensional displays  Laser radar  Autonomous vehicles  Cameras  Calibration  Object detection  Robot sensing systems 
Abstract: With the increasing global popularity of self-driving cars, there is an immediate need for challenging real-world datasets for benchmarking and training various computer vision tasks such as 3D object detection. Existing datasets either represent simple scenarios or provide only day-time data. In this paper, we introduce a new challenging A*3D dataset which consists of RGB images and LiDAR data with a significant diversity of scene, time, and weather. The dataset consists of high-density images (≈ 10 times more than the pioneering KITTI dataset), heavy occlusions, a large number of nighttime frames (≈ 3 times the nuScenes dataset), addressing the gaps in the existing datasets to push the boundaries of tasks in autonomous driving research to more challenging highly diverse environments. The dataset contains 39K frames, 7 classes, and 230K 3D object annotations. An extensive 3D object detection benchmark evaluation on the A*3D dataset for various attributes such as high density, day-time/night-time, gives interesting insights into the advantages and limitations of training and testing 3D object detection in real-world setting.


Title: SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud
Key Words: image denoising  image segmentation  object detection  optical radar  stereo image processing  traffic engineering computing  depth-aware features  3D vehicle detection  point cloud distribution  semantic context information  ambiguous vehicles  semantic context encoder  target detection range  emantic segmentation masks  depth-aware head  SegVoxelNet  LiDAR  noisy region suppression  Three-dimensional displays  Semantics  Feature extraction  Two dimensional displays  Vehicle detection  Head  Convolution 
Abstract: 3D vehicle detection based on point cloud is a challenging task in real-world applications such as autonomous driving. Despite significant progress has been made, we observe two aspects to be further improved. First, the semantic context information in LiDAR is seldom explored in previous works, which may help identify ambiguous vehicles. Second, the distribution of point cloud on vehicles varies continuously with increasing depths, which may not be well modeled by a single model. In this work, we propose a unified model SegVoxelNet to address the above two problems. A semantic context encoder is proposed to leverage the free-of-charge semantic segmentation masks in the bird's eye view. Suspicious regions could be highlighted while noisy regions are suppressed by this module. To better deal with vehicles at different depths, a novel depth-aware head is designed to explicitly model the distribution differences and each part of the depth-aware head is made to focus on its own target detection range. Extensive experiments on the KITTI dataset show that the proposed method outperforms the state-of-the-art alternatives in both accuracy and efficiency with point cloud as input only.


Title: Fine-Grained Driving Behavior Prediction via Context-Aware Multi-Task Inverse Reinforcement Learning
Key Words: behavioural sciences computing  driver information systems  learning (artificial intelligence)  road accidents  road safety  road traffic  unexpected VRU movements  residential roads  proficient acceleration  deceleration  road width  traffic direction  multilinear reward function  contextual information  long-term prediction  defensive driving strategy  context-aware multitask inverse reinforcement learning  advanced driver assistance systems  vulnerable road users  traffic accident reduction rate  multitask IRL approach  fine-grained driving behavior prediction  inverse reinforcement learning  Roads  Context modeling  Task analysis  Vehicles  Hidden Markov models  Safety  Predictive models 
Abstract: Research on advanced driver assistance systems for reducing risks to vulnerable road users (VRUs) has recently gained popularity because the traffic accident reduction rate for VRUs is still small. Dealing with unexpected VRU movements on residential roads requires proficient acceleration and deceleration. Although fine-grained prediction of driving behavior through inverse reinforcement learning (IRL) has been reported with promising results in recent years, learning of a precise model fails when driving strategies vary with contextual factors, i.e., weather, time of day, road width, and traffic direction. In this work, we propose a novel multi-task IRL approach with a multilinear reward function to incorporate contextual information into the model. This approach can provide precise long-term prediction of fine-grained driving behavior while adjusting to context. Experimental results using actual driving data over 141 km with various contexts and roads confirm the success of this approach in terms of predicting defensive driving strategy even in unknown situations.


Title: How to Keep HD Maps for Automated Driving Up To Date
Key Words: cartography  data privacy  road vehicles  traffic engineering computing  dedicated mapping vehicles  low traversal frequencies  anonymized data  up-to-dateness  crowdsourced data  automatically trigger map update jobs  map patches  date HD map  automated driving functions  HD maps  automotive high definition digital map generation  automated driving up to date  Roads  Vehicle dynamics  Topology  Robot sensing systems  Shape 
Abstract: The current state of the art in automotive high definition digital (HD) map generation based on dedicated mapping vehicles cannot reliably keep these maps up to date because of the low traversal frequencies. Anonymized data collected from the fleet of vehicles that is already on the road provides a huge potential to outperform such state of the art solutions in robustness, safety and up-to-dateness of the map while achieving comparable quality. We thus present a solution based on crowdsourced data to (i) detect changes in the map independent of the type of change, (ii) automatically trigger map update jobs for parts of the map, and (iii) create and integrate map patches to keep the map always up to date. The developed solution provides a crowdsourced up to date HD map to make reliable prior information on lane markings and road edges available to automated driving functions.


Title: Binary DAD-Net: Binarized Driveable Area Detection Network for Autonomous Driving
Key Words: embedded systems  field programmable gate arrays  image segmentation  inference mechanisms  learning (artificial intelligence)  traffic engineering computing  binarized driveable area detection network  autonomous driving  ground-plane detection  obstacle detection  maneuver planning  over-parameterized networks  slim binary networks  binary weights  binary dilated convolutions  binary DAD-Net  semantic segmentation networks  FPGA  memory size 0.9 MByte  Convolutional codes  Task analysis  Semantics  Decoding  Training  Autonomous vehicles  Computational modeling 
Abstract: Driveable area detection is a key component for various applications in the field of autonomous driving (AD), such as ground-plane detection, obstacle detection and maneuver planning. Additionally, bulky and over-parameterized networks can be easily forgone and replaced with smaller networks for faster inference on embedded systems. The driveable area detection, posed as a two class segmentation task, can be efficiently modeled with slim binary networks. This paper proposes a novel binarized driveable area detection network (binary DAD-Net), which uses only binary weights and activations in the encoder, the bottleneck, and the decoder part. The latent space of the bottleneck is efficiently increased (×32→×16 downsampling) through binary dilated convolutions, learning more complex features. Along with automatically generated training data, the binary DAD-Net outperforms state-of-the-art semantic segmentation networks on public datasets. In comparison to a full-precision model, our approach has a ×14.3 reduced compute complexity on an FPGA and it requires only 0.9MB memory resources. Therefore, commodity SIMD-based AD-hardware is capable of accelerating the binary DAD-Net.


Title: Joint Human Pose Estimation and Stereo 3D Localization
Key Words: neural nets  pose estimation  stereo image processing  3D localization task  KITTI dataset  stereo 3D localization  neural network architecture  stereo imaging  human body  joint human pose estimation  image stereo pair  stereo pose dataset  Three-dimensional displays  Correlation  Two dimensional displays  Pose estimation  Uncertainty  Decoding  Task analysis 
Abstract: We present an end-to-end trainable Neural Network architecture for stereo imaging that jointly locates and estimates human body poses in 3D. Our method defines a 2D pose for each human in a stereo pair of images and uses a correlation layer with a composite field to associate each left-right pair of joints. In absence of a stereo pose dataset, we show that we can train our method with synthetic data only and test it on real-world images (i.e., our training stage is domain invariant). Our method is particularly suitable for autonomous vehicles. We achieve state-of-the-art results for the 3D localization task on the challenging real-world KITTI dataset while running four times faster.


Title: Ultra-High-Accuracy Visual Marker for Indoor Precise Positioning
Key Words: attitude control  attitude measurement  autonomous aerial vehicles  indoor navigation  mobile robots  position control  robot vision  local positioning  marker coordinate system  high-accuracy global positioning  ultra-high-accuracy visual marker  indoor precise positioning  indoor positioning  indoor mobile robots  drones  general-purpose technology  attitude measurement  multiple dynamic moires  lenticular lens  attitude estimation error  marker position error  reprojection error  size 10.0 m  size 1.0 cm  size 10.0 cm  attitude accuracy  Visualization  Lenses  Position measurement  Cameras  Measurement uncertainty  Pose estimation 
Abstract: Indoor positioning technology is essential for indoor mobile robots and drones. However, there has never been a general-purpose technology or infrastructure that enables indoor positioning with an accuracy of less than 10 cm. We have developed an attitude measurement method using multiple dynamic moires with a lenticular lens and developed an ultra- high-accuracy visual marker with an attitude estimation error of less than 0.1°. We also developed a calculation method that minimizes the marker position error by reminimizing reprojection error using its good attitude accuracy. We proved that accurate local positioning with a position error of about 1 cm in a marker coordinate system is possible even when a marker is shot from a distance of 10 m. In addition, a demonstration test was performed in a public space, and it was shown that high-accuracy global positioning with a position error of about 10 cm is possible.


Title: BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking
Key Words: autonomous aerial vehicles  image motion analysis  learning (artificial intelligence)  object detection  object tracking  remotely operated vehicles  target tracking  video signal processing  unmanned aerial vehicle tracking scenarios  high computational efficiency  UAV tracking process  viewpoint variations  background appearance  CF-based trackers  ideal tracker  object position  response-based errors  forward errors  backward errors  current training sample  historical training samples  BiCF  response-based bidirectional incongruity error  UAV datasets  UAV123  bidirectional incongruity-aware correlation filter  Unmanned aerial vehicles  Training  Target tracking  Correlation  Robustness  Feature extraction 
Abstract: Correlation filters (CFs) have shown excellent performance in unmanned aerial vehicle (UAV) tracking scenarios due to their high computational efficiency. During the UAV tracking process, viewpoint variations are usually accompanied by changes in the object and background appearance, which poses a unique challenge to CF-based trackers. Since the appearance is gradually changing over time, an ideal tracker can not only forward predict the object position but also backtrack to locate its position in the previous frame. There exist response-based errors in the reversibility of the tracking process containing the information on the changes in appearance. However, some existing methods do not consider the forward and backward errors based on while using only the current training sample to learn the filter. For other ones, the applicants of considerable historical training samples impose a computational burden on the UAV. In this work, a novel bidirectional incongruity-aware correlation filter (BiCF) is proposed. By integrating the response-based bidirectional incongruity error into the CF, BiCF can Efficiently learn the changes in appearance and suppress the inconsistent error. Extensive experiments on 243 challenging sequences from three UAV datasets (UAV123, UAVDT, and DTB70) are conducted to demonstrate that BiCF favorably outperforms other 25 state-of-the-art trackers and achieves a real-time speed of 45.4 FPS on a single CPU, which can be applied in UAV Efficiently.


Title: Context-aware Cost Shaping to Reduce the Impact of Model Error in Receding Horizon Control
Key Words: mobile robots  predictive control  probability  remotely operated vehicles  robot dynamics  stochastic systems  model error  receding horizon control  repetitive path-following task  robot dynamics  simple learned dynamics model  MPC horizon  stochastic MPC  prediction horizon  online model learning  ground robot  context-aware cost shaping  stochastic model predictive control  Robots  Computational modeling  Predictive models  Aerodynamics  Cost function  Task analysis  Stochastic processes 
Abstract: This paper presents a method to enable a robot using stochastic Model Predictive Control (MPC) to achieve high performance on a repetitive path-following task. In particular, we consider the case where the accuracy of the model for robot dynamics varies significantly over the path-motivated by the fact that the models used in MPC must be computationally efficient, which limits their expressive power. Our approach is based on correcting the cost predicted using a simple learned dynamics model over the MPC horizon. This discourages the controller from taking actions that lead to higher cost than would have been predicted using the dynamics model. In addition, stochastic MPC provides a quantitative measure of safety by limiting the probability of violating state and input constraints over the prediction horizon. Our approach is unique in that it combines both online model learning and cost learning over the prediction horizon and is geared towards operating a robot in changing conditions. We demonstrate our algorithm in simulation and experiment on a ground robot that uses a stereo camera for localization.


Title: Enhancing Bilevel Optimization for UAV Time-Optimal Trajectory using a Duality Gap Approach
Key Words: autonomous aerial vehicles  bang-bang control  duality (mathematics)  mobile robots  Newton method  nonlinear programming  path planning  sensitivity analysis  time optimal control  sensitivity analysis  NLP solvers  parametric optimization problems  quasiNewton method  geometric path  time-optimal velocity profile  hierarchical optimization  bang-bang control structure  nonlinear programming solvers  dynamic robotic vehicles  time-optimal trajectories  duality gap approach  UAV time-optimal trajectory  gap-free bilevel optimization  interior-point method  Trajectory  Cost function  Switches  Acceleration  Robustness  Robots 
Abstract: Time-optimal trajectories for dynamic robotic vehicles are difficult to compute even for state-of-the-art nonlinear programming (NLP) solvers, due to nonlinearity and bang-bang control structure. This paper presents a bilevel optimization framework that addresses these problems by decomposing the spatial and temporal variables into a hierarchical optimization. Specifically, the original problem is divided into an inner layer, which computes a time-optimal velocity profile along a given geometric path, and an outer layer, which refines the geometric path by a Quasi-Newton method. The inner optimization is convex and efficiently solved by interior-point methods. The gradients of the outer layer can be analytically obtained using sensitivity analysis of parametric optimization problems. A novel contribution is to introduce a duality gap in the inner optimization rather than solving it to optimality; this lets the optimizer realize warm-starting of the interior-point method, avoids non-smoothness of the outer cost function caused by active inequality constraint switching. Like prior bilevel frameworks, this method is guaranteed to return a feasible solution at any time, but converges faster than gap-free bilevel optimization. Numerical experiments on a drone model with velocity and acceleration limits show that the proposed method performs faster and more robustly than gap-free bilevel optimization and general NLP solvers.


Title: Super-Pixel Sampler: a Data-driven Approach for Depth Sampling and Reconstruction
Key Words: image colour analysis  image reconstruction  image sampling  random processes  data-driven approach  depth sampling  depth acquisition  active illumination  autonomous navigation  robotic navigation  mechanical templates  sampling templates  autonomous vehicles  solid-state depth sensors  adaptive framework  simple reconstruction algorithm  generic reconstruction algorithm  sampling reconstruction algorithm  random sampling  depth completion algorithms  single-pixel prototype sampler  RGB sampling strategies  single depth sampling strategies  piecewise planar depth model  superpixel sampler  SPS  Image reconstruction  Adaptation models  Estimation  Cameras  Robot sensing systems  Navigation 
Abstract: Depth acquisition, based on active illumination, is essential for autonomous and robotic navigation. LiDARs (Light Detection And Ranging) with mechanical, fixed, sampling templates are commonly used in today's autonomous vehicles. An emerging technology, based on solid-state depth sensors, with no mechanical parts, allows fast and adaptive scans. In this paper, we propose an adaptive, image-driven, fast, sampling and reconstruction strategy. First, we formulate a piece-wise planar depth model and estimate its validity for indoor and outdoor scenes. Our model and experiments predict that, in the optimal case, adaptive sampling strategies with about 20-60 piece-wise planar structures can approximate well a depth map. This translates to requiring a single depth sample for every 1200 RGB samples (less than 0.1%), providing strong motivation to investigate an adaptive framework. Second, we introduce SPS (Super-Pixel Sampler), a simple, generic, sampling and reconstruction algorithm, based on super-pixels. Our sampling improves grid and random sampling, consistently, for a wide variety of reconstruction methods. Third, we propose an extremely simple and fast reconstruction for our sampler. It achieves state-of-the-art results, compared to complex image- guided depth completion algorithms, reducing the required sampling rate by a factor of 3-4. A single-pixel prototype sampler built in our lab illustrates the concept.


Title: Robust Vision-based Obstacle Avoidance for Micro Aerial Vehicles in Dynamic Environments
Key Words: collision avoidance  helicopters  mobile robots  predictive control  robot dynamics  robot vision  state estimation  robust vision-based obstacle avoidance  microaerial vehicle  dynamic environments  on-board vision-based approach  moving obstacle  efficient obstacle detection  tracking algorithm  depth image pairs  robust collision avoidance  chance-constrained model predictive controller  collision probability  account MAV dynamics  state estimation  obstacle sensing uncertainties  on-line collision avoidance  Collision avoidance  Uncertainty  Cameras  Robustness  Sensors  Predictive models  State estimation 
Abstract: In this paper, we present an on-board vision-based approach for avoidance of moving obstacles in dynamic environments. Our approach relies on an efficient obstacle detection and tracking algorithm based on depth image pairs, which provides the estimated position, velocity and size of the obstacles. Robust collision avoidance is achieved by formulating a chance-constrained model predictive controller (CC-MPC) to ensure that the collision probability between the micro aerial vehicle (MAV) and each moving obstacle is below a specified threshold. The method takes into account MAV dynamics, state estimation and obstacle sensing uncertainties. The proposed approach is implemented on a quadrotor equipped with a stereo camera and is tested in a variety of environments, showing effective on-line collision avoidance of moving obstacles.


Title: Proximity Estimation Using Vision Features Computed On Sensor
Key Words: collision avoidance  computer vision  feature extraction  image sensors  microcontrollers  mobile robots  recurrent neural nets  robot vision  sensor arrays  VLSI  neural network output  image capture  control system  trained neural network  fully connected layer-recurrent  training data  infrared proximity sensors  vision output  sparse feature description data  feature algorithms  pixel processor array chip  embedded 256×256 processor SIMD array  image sensor  RC model car  microcontroller  SCAMP-5 vision chip  vision system integrating  experimental vehicle  blobs  corner points  abstract features  monocular vision based proximity estimation system  vision features computed  velocity 0.64 m/s to 1.8 m/s  time 4.0 ms  Hardware  Estimation  Neural networks  Machine vision  Feature extraction  Registers  Image edge detection 
Abstract: This paper presents a monocular vision based proximity estimation system using abstract features, such as corner points, blobs and edges, as inputs to a neural network. An experimental vehicle was built using a vision system integrating the SCAMP-5 vision chip, a micro-controller, and an RC model car. The vision chip includes image sensor with embedded 256×256 processor SIMD array. The pixel processor array chip was programmed to capture images and run the feature algorithms directly on the focal plane, and then digest them so that only sparse feature description data were read-out in the form of 40 values. By logging the vision output and the output from three infrared proximity sensors, training data were obtained to train three fully connected layer-recurrent neural networks with fewer than 700 parameters each. The trained neural network was able to estimate the proximity to the level of accuracy sufficient for a reactive collision avoidance behaviour to be achieved. The latency of the control system, from image capture to neural network output, was under 4ms, enabling the vehicles to avoid obstacles while moving at 0.64m/s to 1.8m/s in the experiment.


Title: Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles
Key Words: cameras  distance measurement  feature extraction  image registration  mobile robots  motion estimation  optimisation  path planning  robot vision  steering systems  tree searching  efficient globally-optimal correspondence-less visual odometry  planar ground vehicles  2 DoF Ackermann steering model  downward facing camera  simple image registration problem  2-parameter planar homography  ground-plane features  plane-based Ackermann motion estimation  correspondence-based hypothesise  test schemes  fronto-parallel motion  branch-and-bound optimisation technique  low-dimensional parametrisation  Cameras  Optimization  Transmission line matrix methods  Land vehicles  Motion estimation  Image registration  Real-time systems 
Abstract: The motion of planar ground vehicles is often non-holonomic, and as a result may be modelled by the 2 DoF Ackermann steering model. We analyse the feasibility of estimating such motion with a downward facing camera that exerts fronto-parallel motion with respect to the ground plane. This turns the motion estimation into a simple image registration problem in which we only have to identify a 2-parameter planar homography. However, one difficulty that arises from this setup is that ground-plane features are indistinctive and thus hard to match between successive views. We encountered this difficulty by introducing the first globally-optimal, correspondence-less solution to plane-based Ackermann motion estimation. The solution relies on the branch-and-bound optimisation technique. Through the low-dimensional parametrisation, a derivation of tight bounds, and an efficient implementation, we demonstrate how this technique is eventually amenable to accurate real-time motion estimation. We prove its property of global optimality and analyse the impact of assuming a locally constant centre of rotation. Our results on real data finally demonstrate a significant advantage over the more traditional, correspondence-based hypothesise-and-test schemes.


Title: Bio-inspired Tensegrity Fish Robot
Key Words: autonomous underwater vehicles  biomechanics  bone  elasticity  mobile robots  muscle  robot dynamics  servomotors  underwater vehicles  fish robots  body stiffness  fish swimming  mechanical stiffness  tensegrity-based underwater robots  tensegrity class  elastic cables  rigid body segments  body shape  tensegrity systems  fish-like robots  bio-inspired tensegrity fish robot  Conferences  Automation 
Abstract: This paper presents a method to create fish-like robots with tensegrity systems and describes a prototype modeled on the body shape of the rainbow trout with a length of 400 mm and a mass of 102 g that is driven by a waterproof servomotor. The structure of the tensegrity robot consists of rigid body segments and elastic cables that represent bone/tissue and muscles of fish, respectively. This structural configuration employing the tensegrity class 2 is much simpler than other tensegrity-based underwater robots. It also allows the tuning of the mechanical stiffness, which is often said to be an important factor in fish swimming. In our robot, the body stiffness can be tuned by changing the cross-section of the cables and their pre-stretch ratio. We characterize the robot in terms of body stiffness, swimming speed, and thrust force while varying the body stiffness i.e., the cross-section of the elastic cables. The results show that the body stiffness of the robot can be designed to approximate that of the real fish and modulate its performance characteristics. The measured swimming speed of the robot is 0.23 m/s (0.58 BL/s), which is comparable to other fish robots of the same type. Strouhal number of the robot 0.54 is close to that of the natural counterpart, suggesting that the presented method is an effective engineering approach to realize the swimming characteristics of real fish.


Title: Ibex: A reconfigurable ground vehicle with adaptive terrain navigation capability
Key Words: friction  mobile robots  off-road vehicles  optimisation  remotely operated vehicles  robot dynamics  stability  vehicle dynamics  reconfigurable ground vehicle  adaptive terrain navigation capability  unmanned ground vehicle  dynamic wheelbase  adaptive thrust  friction optimization  steep slopes  slippery surfaces  surface topography  impedance-based stabilization module  mechanical oscillatory transients  Ibex  Force  Wheels  Surface topography  Surface impedance  Land vehicles  Kinematics  Drag 
Abstract: This paper presents a unique unmanned ground vehicle with a dynamic wheelbase and an adaptive thrust based friction optimization scheme that aids in the traversal of steep slopes and slippery surfaces. The vehicle is capable of adapting itself to the surface topography using an impedance-based stabilization module to minimize the mechanical oscillatory transients induced during its motion. A detailed analysis of its modules has been elucidated in this paper based on the vehicle parameters. The proposed methodologies have been integrated and tested on a customized prototype. Experimental validation and simulation for the proposed modules at various terrain conditions have been carried out to authenticate its performance.


Title: Generating Locomotion with Effective Wheel Radius Manipulation
Key Words: mobile robots  motion control  motor drives  road vehicles  robot dynamics  vehicle dynamics  wheels  motor drives  sloped terrain  wheel rotation  plain centre hub drive  active ride height selection  wheel radius manipulation  locomotion generation  slope traversability  wheel pose control  centre of gravity manipulation  Wheels  Gravity  Acceleration  Mathematical model  Torque  Axles  Actuators 
Abstract: Travel over sloped terrain is difficult as an incline changes the interaction between each wheel and the ground resulting in an unbalanced load distribution which can lead to loss of traction and instability. This paper presents a novel approach to generating wheel rotation for primary locomotion by only changing its centre of rotation, or as a complimentary locomotion source to increase versatility of a plain centre hub drive. This is done using linear actuators within a wheel to control the position of the centre hub and induce a moment on the wheel from gravity. In doing so our platform allows for active ride height selection and individual wheel pose control. We present the system with calculations outlining the theoretical properties and perform experiments to validate the concept under loading via multiple gaits to show motion on slopes, and sustained motion over extended distance. We envision applications in conjunction to assist current motor drives and increasing slope traversability by allowing body pose and centre of gravity manipulation, or as a primary locomotion system.


Title: A GNC Architecture for Planetary Rovers with Autonomous Navigation
Key Words: aerospace navigation  aerospace robotics  distance measurement  Mars  mobile robots  optimal control  path planning  planetary rovers  robot vision  SLAM (robots)  trajectory control  GNC architecture  planetary rovers  autonomous navigation  Mars exploration missions  sample fetching rover  autonomous capabilities  two-level architecture  terrain  local path replanning  trajectory control  global localization  planetary exploration  planetary analog field test campaigns  guidance navigation and control architecture  hazard detection  visual odometry  adaptive SLAM algorithm  optimal path planning  Hazards  Navigation  Space vehicles  Autonomous robots  Computer architecture  Cameras  Trajectory 
Abstract: This paper proposes a Guidance, Navigation, and Control (GNC) architecture for planetary rovers targeting the conditions of upcoming Mars exploration missions such as Mars 2020 and the Sample Fetching Rover (SFR). The navigation requirements of these missions demand a control architecture featuring autonomous capabilities to achieve a fast and long traverse. The proposed solution presents a two-level architecture where the efficient navigation (lower) level is always active and the full navigation (upper) level is enabled according to the difficulty of the terrain. The first level is an efficient implementation of the basic functionalities for autonomous navigation based on hazard detection, local path replanning, and trajectory control with visual odometry. The second level implements an adaptive SLAM algorithm that improves the relative localization, evaluates the traversability of the terrain ahead for a more optimal path planning, and performs global (absolute) localization that corrects the pose drift during longer traverses. The architecture provides a solution for long-range, low supervision, and fast planetary exploration. Both navigation levels have been validated on planetary analog field test campaigns.


Title: Uncertainty-Based Adaptive Sensor Fusion for Visual-Inertial Odometry under Various Motion Characteristics
Key Words: distance measurement  image fusion  Jacobian matrices  motion estimation  visual-inertial odometry  inertial measurement units  scale-aware estimation  sensor states  sensor motion  noninformative inertial measurements  estimation modes  motion characteristics  uncertainty-based adaptive sensor fusion  uncertainty-based sensor fusion  relative motion estimation  observability  Jacobian matrices  Estimation  Bundle adjustment  Motion measurement  Velocity measurement  Uncertainty  Sensor fusion 
Abstract: We propose an uncertainty-based sensor fusion framework for visual-inertial odometry, which is the task of estimating relative motion using images and measurements from inertial measurement units. Visual-inertial odometry enables robust and scale-aware estimation of motion by incorporating sensor states, such as metric scale, velocity, and the direction of gravity, into the estimation. However, the observability of the states depends on sensor motion. For example, if the sensor moves in a constant velocity, scale and velocity cannot be observed from inertial measurements. Under these degenerate motions, existing methods may produce inaccurate results because they incorporate erroneous states estimated from non-informative inertial measurements. Our proposed framework is able to avoid this situation by adaptively switching estimation modes, which represents the states that should be incorporated, based on their uncertainties. These uncertainties can be obtained at a small computational cost by reusing the Jacobian matrices computed in bundle adjustment. Our approach consistently outperformed conventional sensor fusion in datasets with different motion characteristics, namely, the KITTI odometry dataset recorded by a ground vehicle and the EuRoC MAV dataset captured from a micro aerial vehicle.


Title: Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV
Key Words: distance measurement  mobile robots  optical radar  path planning  robot vision  SLAM (robots)  FoV  autonomous vehicles  autonomous navigation  path planning  LOAM algorithm  LiDAR odometry and mapping  robot pose localization  Laser radar  Feature extraction  Three-dimensional displays  Measurement by laser beam  Laser noise  Real-time systems  Spinning 
Abstract: LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot's pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.


Title: Active SLAM using 3D Submap Saliency for Underwater Volumetric Exploration
Key Words: graph theory  mobile robots  navigation  path planning  robot vision  SLAM (robots)  underwater volumetric exploration  active SLAM framework  3D underwater environments  multibeam sonar  integrated SLAM  volumetric free-space information  informative loop closures  navigation policy  3D visual dictionary  submap saliency  sensor information  pose-graph SLAM formulation  global occupancy grid map  uncertainty-agnostic framework  Simultaneous localization and mapping  Three-dimensional displays  Uncertainty  Conferences  Automation  Sonar  Planning 
Abstract: In this paper, we present an active SLAM framework for volumetric exploration of 3D underwater environments with multibeam sonar. Recent work in integrated SLAM and planning performs localization while maintaining volumetric free-space information. However, an absence of informative loop closures can lead to imperfect maps, and therefore unsafe behavior. To solve this, we propose a navigation policy that reduces vehicle pose uncertainty by balancing between volumetric exploration and revisitation. To identify locations to revisit, we build a 3D visual dictionary from real-world sonar data and compute a metric of submap saliency. Revisit actions are chosen based on propagated pose uncertainty and sensor information gain. Loop closures are integrated as constraints in our pose-graph SLAM formulation and these deform the global occupancy grid map. We evaluate our performance in simulation and real-world experiments, and highlight the advantages over an uncertainty-agnostic framework.


Title: Temporally Consistent Horizon Lines
Key Words: computer vision  convolutional neural nets  image reconstruction  image sequences  learning (artificial intelligence)  recurrent neural nets  stereo image processing  video signal processing  geometric feature  image processing  scene understanding  computer vision  autonomous vehicle navigation  driver assistance  semantic interpretation  dynamic environments  convolutional neural networks  residual convolutional LSTM  temporally consistent horizon line estimation  video sequences  3D reconstruction  CNN architecture  adaptive loss function  Video sequences  Cameras  Three-dimensional displays  Observers  Task analysis  Training 
Abstract: The horizon line is an important geometric feature for many image processing and scene understanding tasks in computer vision. For instance, in navigation of autonomous vehicles or driver assistance, it can be used to improve 3D reconstruction as well as for semantic interpretation of dynamic environments. While both algorithms and datasets exist for single images, the problem of horizon line estimation from video sequences has not gained attention. In this paper, we show how convolutional neural networks are able to utilise the temporal consistency imposed by video sequences in order to increase the accuracy and reduce the variance of horizon line estimates. A novel CNN architecture with an improved residual convolutional LSTM is presented for temporally consistent horizon line estimation. We propose an adaptive loss function that ensures stable training as well as accurate results. Furthermore, we introduce an extension of the KITTI dataset which contains precise horizon line labels for 43699 images across 72 video sequences. A comprehensive evaluation shows that the proposed approach consistently achieves superior performance compared with existing methods.


Title: Full-Scale Continuous Synthetic Sonar Data Generation with Markov Conditional Generative Adversarial Networks*
Key Words: acoustic signal processing  autonomous underwater vehicles  data analysis  environmental factors  Markov processes  naval engineering computing  neural nets  object recognition  realistic images  sonar imaging  statistical analysis  bootstrapping ATR systems  autonomous underwater vehicles  autonomous target recognition systems  realistic synthetic sonar imagery  Markov conditional generative adversarial networks  continuous synthetic sonar data generation  Markov conditional pix2pix  environmental factors  acoustic sensors  Sonar  Training  Semantics  Data models  Gallium nitride  Training data  Markov processes 
Abstract: Deployment and operation of autonomous underwater vehicles is expensive and time-consuming. High-quality realistic sonar data simulation could be of benefit to multiple applications, including training of human operators for post-mission analysis, as well as tuning and validation of autonomous target recognition (ATR) systems for underwater vehicles. Producing realistic synthetic sonar imagery is a challenging problem as the model has to account for specific artefacts of real acoustic sensors, vehicle attitude, and a variety of environmental factors. We propose a novel method for generating realistic-looking sonar side-scans of full-length missions, called Markov Conditional pix2pix (MC-pix2pix). Quantitative assessment results confirm that the quality of the produced data is almost indistinguishable from real. Furthermore, we show that bootstrapping ATR systems with MC-pix2pix data can improve the performance. Synthetic data is generated 18 times faster than real acquisition speed, with full user control over the topography of the generated data.


Title: Hierarchical Coverage Path Planning in Complex 3D Environments
Key Words: autonomous aerial vehicles  hierarchical systems  image resolution  image sampling  mobile robots  path planning  robot vision  complex three-dimensional environment  nooks  crannies  coverage planning  multiresolution hierarchical framework  three-dimensional scenes  hierarchical coverage path planning  lightweight UAV  low-level sampling  complex 3D environments  Planning  Robot sensing systems  Cameras  Octrees  Three-dimensional displays  Surface treatment 
Abstract: State-of-the-art coverage planning methods perform well in simple environments but take an ineffectively long time to converge to an optimal solution in complex three-dimensional (3D) environments. As more structures are present in the same volume of workspace, these methods slow down as they spend more time searching for all of the nooks and crannies concealed in three-dimensional spaces. This work presents a method for coverage planning that employs a multi-resolution hierarchical framework to solve the problem at two different levels, producing much higher efficiency than the state-of-the-art. First, a high-level algorithm separates the environment into multiple subspaces at different resolutions and computes an order of the subspaces for traversal. Second, a low-level sampling-based algorithm solves for paths within the subspaces for detailed coverage. In experiments, we evaluate our method using real-world datasets from complex three-dimensional scenes. Our method finds paths that are constantly shorter and converges at least ten times faster than the state-of-the-art. Further, we show results of a physical experiment where a lightweight UAV follows the paths to realize the coverage.


Title: Perception-aware time optimal path parameterization for quadrotors
Key Words: autonomous aerial vehicles  helicopters  mobile robots  optimisation  path planning  perception-aware time optimal path parameterization  quadrotors  perception-aware time optimal path parametrization  quadrotor systems  on-board navigation  estimation algorithms  planning  efficient time optimal path parametrization algorithm  quadrotor platform  vision-driven vehicles  Trajectory  Cameras  Planning  Task analysis  Aerodynamics  Heuristic algorithms  Navigation 
Abstract: The increasing popularity of quadrotors has given rise to a class of predominantly vision-driven vehicles. This paper addresses the problem of perception-aware time optimal path parametrization for quadrotors. Although many different choices of perceptual modalities are available, the low weight and power budgets of quadrotor systems makes a camera ideal for on-board navigation and estimation algorithms. However, this does come with a set of challenges. The limited field of view of the camera can restrict the visibility of salient regions in the environment, which dictates the necessity to consider perception and planning jointly. The main contribution of this paper is an efficient time optimal path parametrization algorithm for quadrotors with limited field of view constraints. We show in a simulation study that a state-of-the-art controller can track planned trajectories, and we validate the proposed algorithm on a quadrotor platform in experiments.


Title: Generating Visibility-Aware Trajectories for Cooperative and Proactive Motion Planning
Key Words: mobile robots  motion estimation  object detection  path planning  road traffic  road vehicles  vehicles  autonomous vehicle  ego vehicle  visibility-aware planning  visibility-aware trajectories  proactive motion planning  cooperative motion planning  partially-occluded intersection  emergent behavior  Trajectory  Uncertainty  Planning  Safety  Autonomous vehicles  Splines (mathematics) 
Abstract: The safety of an autonomous vehicle not only depends on its own perception of the world around it, but also on the perception and recognition from other vehicles. If an ego vehicle considers the uncertainty other vehicles have about itself, then by reducing the estimated uncertainty it can increase its safety. In this paper, we focus on how an ego vehicle plans its trajectories through the blind spots of other vehicles. We create visibility-aware planning, where the ego vehicle chooses its trajectories such that it reduces the perceived uncertainty other vehicles may have about the state of the ego vehicle. We present simulations of traffic and highway environments, where an ego vehicle must pass another vehicle, make a lane change, or traverse a partially-occluded intersection. Emergent behavior shows that when using visibility-aware planning, the ego vehicle spends less time in a blind spot, and may slow down before entering the blind spot so as to increase the likelihood other vehicles perceive the ego vehicle.


Title: Distributed Consensus Control of Multiple UAVs in a Constrained Environment
Key Words: autonomous aerial vehicles  control system synthesis  decentralised control  distributed control  multi-robot systems  position control  tracking  trees (mathematics)  multiple UAVs  constrained environment  consensus problem  multiple unmanned aerial vehicles  environmental constraints  general communication topology  directed spanning tree  position transformation function  dynamic reference position  yaw angle  asymmetric topology  local tracking controller  distributed consensus control  Topology  Decentralized control  Protocols  Unmanned aerial vehicles  Tracking loops  Heuristic algorithms  Attitude control 
Abstract: In this paper, we investigate the consensus problem of multiple unmanned aerial vehicles (UAVs) in the presence of environmental constraints under a general communication topology containing a directed spanning tree. First, based on a position transformation function, we propose a novel dynamic reference position and yaw angle for each UAV to cope with both the asymmetric topology and the constraints. Then, the backstepping-like design methodology is presented to derive a local tracking controller for each UAV such that its position and yaw angle can converge to the reference ones. The proposed protocol is distributed in the sense that, the input update of each UAV dynamically relies only on local state information from its neighborhood set and the constraints, and it does not require any additional centralized information. It is demonstrated that under the proposed protocol, all UAVs reach consensus without violation of the environmental constraints. Finally, simulation and experimental results are provided to demonstrate the performance of the protocol.


Title: Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using Learned Interactions
Key Words: aerodynamics  aerospace robotics  control system synthesis  decentralised control  learning (artificial intelligence)  multi-robot systems  neurocontrollers  nonlinear control systems  particle swarm optimisation  stability  close-proximity multirotor control  learned interactions  Neural-Swarm  nonlinear decentralized stable controller  close-proximity flight  multirotor swarms  close-proximity control  complex aerodynamic interaction effects  safety distances  nominal dynamics model  regularized permutation-invariant Deep Neural Network  high-order multivehicle interactions  larger swarm sizes  baseline nonlinear  stable nonlinear tracking controller  Vehicle dynamics  Aerodynamics  Neural networks  Rotors  Stability analysis  Training  Robots 
Abstract: In this paper, we present Neural-Swarm, a nonlinear decentralized stable controller for close-proximity flight of multirotor swarms. Close-proximity control is challenging due to the complex aerodynamic interaction effects between multirotors, such as downwash from higher vehicles to lower ones. Conventional methods often fail to properly capture these interaction effects, resulting in controllers that must maintain large safety distances between vehicles, and thus are not capable of close-proximity flight. Our approach combines a nominal dynamics model with a regularized permutation-invariant Deep Neural Network (DNN) that accurately learns the high-order multi-vehicle interactions. We design a stable nonlinear tracking controller using the learned model. Experimental results demonstrate that the proposed controller significantly outperforms a baseline nonlinear tracking controller with up to four times smaller worst-case height tracking errors. We also empirically demonstrate the ability of our learned model to generalize to larger swarm sizes.


Title: Goal-Directed Occupancy Prediction for Lane-Following Actors
Key Words: mobile robots  motion estimation  prediction theory  road safety  road traffic  road vehicles  roads  robot vision  traffic engineering computing  complex road networks  mapped road topology  dynamic road actors  mapped lane geometry  mode collapse problem  goal-directed occupancy prediction  lane-following actors  shared roads  safe autonomous driving  possible vehicle behaviors  possible goal reasoning  local scene context multimodality  high-level action set  future spatial occupancy prediction  Roads  Predictive models  Trajectory  Topology  Geometry  Task analysis  Autonomous vehicles 
Abstract: Predicting the possible future behaviors of vehicles that drive on shared roads is a crucial task for safe autonomous driving. Many existing approaches to this problem strive to distill all possible vehicle behaviors into a simplified set of high-level actions. However, these action categories do not suffice to describe the full range of maneuvers possible in the complex road networks we encounter in the real world. To combat this deficiency, we propose a new method that leverages the mapped road topology to reason over possible goals and predict the future spatial occupancy of dynamic road actors. We show that our approach is able to accurately predict future occupancy that remains consistent with the mapped lane geometry and naturally captures multi-modality based on the local scene context while also not suffering from the mode collapse problem observed in prior work.


Title: Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching
Key Words: decision making  decision theory  Markov processes  multi-agent systems  road traffic  trees (mathematics)  dense traffic scenarios  automated vehicles  stochastic behaviors  traffic participants  perception uncertainties  partially observable Markov decision process  efficient uncertainty-aware decision-making  longitudinal behaviors  complex driving environments  automated driving  guided branching  domain-specific closed-loop policy tree structure  DCP-Tree  conditional focused branching mechanism  CFB  domain-specific expert knowledge  Planning  Decision making  Uncertainty  Semantics  Vegetation  Aerospace electronics  Safety 
Abstract: Decision-making in dense traffic scenarios is challenging for automated vehicles (AVs) due to potentially stochastic behaviors of other traffic participants and perception uncertainties (e.g., tracking noise and prediction errors, etc.). Although the partially observable Markov decision process (POMDP) provides a systematic way to incorporate these uncertainties, it quickly becomes computationally intractable when scaled to the real-world large-size problem. In this paper, we present an efficient uncertainty-aware decision-making (EUDM) framework, which generates long-term lateral and longitudinal behaviors in complex driving environments in real-time. The computation complexity is controlled to an appropriate level by two novel techniques, namely, the domain-specific closed-loop policy tree (DCP-Tree) structure and conditional focused branching (CFB) mechanism. The key idea is utilizing domain-specific expert knowledge to guide the branching in both action and intention space. The proposed framework is validated using both onboard sensing data captured by a real vehicle and an interactive multi-agent simulation platform. We also release the code of our framework to accommodate benchmarking.


Title: Imitative Reinforcement Learning Fusing Vision and Pure Pursuit for Self-driving
Key Words: generalisation (artificial intelligence)  intelligent robots  learning (artificial intelligence)  road vehicles  sensor fusion  steering systems  traffic engineering computing  pretrained IPP model  CARLA driving benchmark  generalization capability  pure pursuit  autonomous urban driving navigation  two-stage framework  visual information  pure-pursuit method  steering angle  imitation learning performance  driving data  reinforcement learning method  deep deterministic policy gradient  IPP-RL framework  Learning (artificial intelligence)  Meteorology  Robustness  Task analysis  Navigation  Training  Autonomous vehicles 
Abstract: Autonomous urban driving navigation is still an open problem and has ample room for improvement in unknown complex environments and terrible weather conditions. In this paper, we propose a two-stage framework, called IPP-RL, to handle these problems. IPP means an Imitation learning method fusing visual information with the additional steering angle calculated by Pure-Pursuit (PP) method, and RL means using Reinforcement Learning for further training. In our IPP model, the visual information captured by camera can be compensated by the calculated steering angle, thus it could perform well under bad weather conditions. However, imitation learning performance is limited by the driving data severely. Thus we use a reinforcement learning method-Deep Deterministic Policy Gradient (DDPG)-in the second stage training, which shares the learned weights from pretrained IPP model. In this way, our IPP-RL can lower the dependency of imitation learning on demonstration data and solve the problem of low exploration efficiency caused by randomly initialized weights in reinforcement learning. Moreover, we design a more reasonable reward function and use the n-step return to update the critic-network in DDPG. Our experiments on CARLA driving benchmark demonstrate that our IPP-RL is robust to lousy weather conditions and shows remarkable generalization capability in unknown environments on navigation task.


Title: Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving
Key Words: augmented reality  driver information systems  image segmentation  learning (artificial intelligence)  object detection  pedestrians  road vehicles  traffic engineering computing  adversarial appearance learning  augmented Cityscapes  pedestrian recognition  autonomous driving area synthetic data  traffic scenarios  autonomous vehicle  data augmentation  Cityscapes dataset  virtual pedestrians  augmentation realism  generative network architecture  data-set lighting conditions  VRU  Solid modeling  Semantics  Three-dimensional displays  Autonomous vehicles  Training  Cameras  Pipelines 
Abstract: In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.


Title: ROI-cloud: A Key Region Extraction Method for LiDAR Odometry and Localization
Key Words: Bayes methods  distance measurement  feature extraction  image filtering  image matching  image registration  image sampling  Monte Carlo methods  optical radar  pose estimation  robot vision  ROI-cloud  key region extraction method  LiDAR odometry  LiDAR scan  on-board IMU/odometry data  Bayes filtering  Monte Carlo sampling  autonomous robot  LiDAR localization  voxelized cube set  pose estimation  point set registration  massive point cloud data  Feature extraction  Three-dimensional displays  Laser radar  Heuristic algorithms  Vehicle dynamics  Robots  Urban areas 
Abstract: We present a novel key region extraction method of point cloud, ROI-cloud, for LiDAR odometry and localization with autonomous robots. Traditional methods process massive point cloud data in every region within the field of view. In dense urban environments, however, processing redundant and dynamic regions of point cloud is time-consuming and harmful to the results of matching algorithms. In this paper, a voxelized cube set, ROI-cloud, is proposed to solve this problem by exclusively reserving the regions of interest for better point set registration and pose estimation. 3D space is firstly voxelized into weighted cubes. The key idea is to update their weights continually and extract cubes with high importance as key regions. By extracting geometrical features of a LiDAR scan, the importance of each cube is evaluated as a new measurement. With the help of on-board IMU/odometry data as well as new measurements, the weights of cubes are updated recursively through Bayes filtering. Thus, dynamic and redundant point cloud inside cubes with low importance are discarded by means of Monte Carlo sampling. Our method is validated on various datasets, and results indicate that the ROI-cloud improves the existing method in both accuracy and speed.


Title: Vision-based Multi-MAV Localization with Anonymous Relative Measurements Using Coupled Probabilistic Data Association Filter
Key Words: aerospace robotics  microrobots  mobile robots  multi-robot systems  pose estimation  probability  robot vision  SLAM (robots)  target tracking  multiMAV system  robot team  vision based detection  distance measurements  coupled probabilistic data association filter  nonlinear measurements  visual based robot to robot detection  vision based multiMAV localization  robot localization  robot pose estimation  multiple microaerial vehicles  Robot kinematics  Robot sensing systems  Noise measurement  Probabilistic logic  Task analysis  Cameras 
Abstract: We address the localization of robots in a multi-MAV system where external infrastructure like GPS or motion capture systems may not be available. Our approach lends itself to implementation on platforms with several constraints on size, weight, and power (SWaP). Particularly, our framework fuses the onboard VIO with the anonymous, visual-based robot-to-robot detection to estimate all robot poses in one common frame, addressing three main challenges: 1) the initial configuration of the robot team is unknown, 2) the data association between each vision-based detection and robot targets is unknown, and 3) the vision-based detection yields false negatives, false positives, inaccurate, and provides noisy bearing, distance measurements of other robots. Our approach extends the Coupled Probabilistic Data Association Filter [1] to cope with nonlinear measurements. We demonstrate the superior performance of our approach over a simple VIO-based method in a simulation with the measurement models statistically modeled using the real experimental data. We also show how onboard sensing, estimation, and control can be used for formation flight.


Title: Distributed Multi-Target Tracking for Autonomous Vehicle Fleets
Key Words: cameras  Kalman filters  maximum likelihood estimation  target tracking  vehicular ad hoc networks  wireless sensor networks  Consensus Kalman Filter  fixed communication bandwidth  high fidelity urban driving simulator  autonomous cars  time-varying communication network  distributed multitarget tracking  autonomous vehicle fleets  scalable distributed target tracking algorithm  alternating direction method of multipliers  vehicle-to-vehicle network  sensing vehicle  Kalman filter-like update  centralized maximum a posteriori estimate  CARLA  on-board cameras  Sensors  Target tracking  Kalman filters  Microsoft Windows  Estimation  Trajectory  Optimization 
Abstract: We present a scalable distributed target tracking algorithm based on the alternating direction method of multipliers that is well-suited for a fleet of autonomous cars communicating over a vehicle-to-vehicle network. Each sensing vehicle communicates with its neighbors to execute iterations of a Kalman filter-like update such that each agent's estimate approximates the centralized maximum a posteriori estimate without requiring the communication of measurements. We show that our method outperforms the Consensus Kalman Filter in recovering the centralized estimate given a fixed communication bandwidth. We also demonstrate the algorithm in a high fidelity urban driving simulator (CARLA), in which 50 autonomous cars connected on a time-varying communication network track the positions and velocities of 50 target vehicles using on-board cameras.


Title: Flying batteries: In-flight battery switching to increase multirotor flight time
Key Words: aerospace control  helicopters  mobile robots  secondary cells  in-flight battery switching  multirotor flight time  mid-air docking  primary battery  quadcopter flight  docking platform  flying battery  secondary battery  arbitrary switching  Batteries  Switches  Legged locomotion  Switching circuits  Aerodynamics  Connectors  Propellers 
Abstract: We present a novel approach to increase the flight time of a multirotor via mid-air docking and in-flight battery switching. A main quadcopter flying using a primary battery has a docking platform attached to it. A `flying battery' - a small quadcopter carrying a secondary battery - is equipped with docking legs that can mate with the main quadcopter's platform. Connectors between the legs and the platform establish electrical contact on docking, and enable power transfer from the secondary battery to the main quadcopter. A custom-designed circuit allows arbitrary switching between the primary battery and secondary battery. We demonstrate the concept in a flight experiment involving repeated docking, battery switching, and undocking. This is shown in the video attachment. The experiment increases the flight time of the main quadcopter by a factor of 4.7× compared to solo flight, and 2.2× a theoretical limit for that given multirotor. Importantly, this increase in flight time is not associated with a large increase in overall vehicle mass or size, leaving the main quadcopter in fundamentally the same safety class.


Title: Tactile sensing based on fingertip suction flow for submerged dexterous manipulation
Key Words: dexterous manipulators  grippers  motion control  neurocontrollers  recurrent neural nets  tactile sensors  saltwater corrosion  low-light conditions  robust electrical parts  mechanical parts  underwater robots  mobile manipulation tasks  suction flow mechanism  orifice occlusion  ambient pressure  tactile sensing modality  automated robotic behaviors  fingertip suction flow  submerged dexterous manipulation  robotic systems  Electron tubes  Sea measurements  Tactile sensors  Oceans 
Abstract: The ocean is a harsh and unstructured environment for robotic systems; high ambient pressures, saltwater corrosion and low-light conditions demand machines with robust electrical and mechanical parts that are able to sense and respond to the environment. Prior work shows that the addition of gentle suction flow to the hands of underwater robots can aid in the handling of objects during mobile manipulation tasks. The current paper explores using this suction flow mechanism as a new modality for tactile sensing; by monitoring orifice occlusion we can get a sense of how objects make contact in the hand. The electronics required for this sensor can be located remotely from the hand and the signal is insensitive to large changes in ambient pressure associated with diving depth. In this study, suction is applied to the fingertips of a two-fingered compliant gripper and suction-based tactile sensing is monitored while an object is pulled out of a pinch grasp. As a proof of concept, a recurrent neural network model was trained to predict external force trends using only the suction signals. This tactile sensing modality holds the potential to enable automated robotic behaviors or to provide operators of remotely operated vehicles with additional feedback in a robust fashion suitable for ocean deployment.


Title: SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic
Key Words: control engineering computing  multi-agent systems  road traffic  road vehicles  telecommunication traffic  traffic engineering computing  SUMMIT  urban driving  massive mixed traffic  unregulated urban crowd  high-speed traffic participants  high-fidelity simulator  crowd-driving algorithms  open-source OpenStreetMap map database  multiagent motion prediction model  unregulated urban traffic  heterogeneous agents  autonomous driving simulation  realistic traffic behaviors  crowd-driving settings  Roads  Robot sensing systems  Context modeling  Planning  Automobiles  Geometry  Kinematics 
Abstract: Autonomous driving in an unregulated urban crowd is an outstanding challenge, especially, in the presence of many aggressive, high-speed traffic participants. This paper presents SUMMIT, a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physics and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control and planning, and end-to-end learning. We provide a context-aware planner together with benchmark scenarios and show that SUMMIT generates complex, realistic traffic behaviors in challenging crowd-driving settings.


Title: Avalanche victim search via robust observers
Key Words: adaptive control  autonomous aerial vehicles  emergency management  multi-robot systems  observers  rescue robots  robust control  sensor fusion  avalanche victim search  robust observers  victim localization  ARVA sensor  adaptive control  UAVs  least square identifier  Receivers  Transmitters  Drones  Observers  Trajectory  Electromagnetics  Adaptive control  Search and Rescue  Robust Control 
Abstract: This paper introduces a new approach for victim localization in avalanches that will be exploited by UAVs using the ARVA sensor. We show that the nominal ARVA measurement can be linearly related to a quantity that is sufficient to reconstruct the victim position. We explicitly deal with a robust scenario in which the measurement is actually perturbed by a noise that grows with the distance to the victim and we propose an adaptive control scheme made of a least-square identifier and a trajectory generator whose role is both to guarantee the persistence of excitation for the identifier and to steer the ARVA receiver towards the victim. We show that the system succeeds in localizing the victim in a domain where the ARVA output is sufficiently informative and illustrate its performance in simulation. This new approach could significantly reduce the searching time by providing an exploitable estimate before having reached the victim. The work is framed within the EU project AirBorne whose goals is to develop at TRL8 a drone for quick localization of victims in avalanche scenarios.


Title: 3D Scene Geometry-Aware Constraint for Camera Localization with Deep Learning
Key Words: cameras  convolutional neural nets  feature extraction  image classification  image motion analysis  image reconstruction  image segmentation  learning (artificial intelligence)  mobile robots  motion control  path planning  pose estimation  3D scene geometry-aware constraint  camera localization  deep learning  fundamental component  autonomous driving vehicles  path planning  motion control  end-to-end approaches  convolutional neural network  3D-geometry based traditional methods  compact network  absolute camera  image contents  image-level structural similarity loss  challenging scenes  Cameras  Three-dimensional displays  Training  Geometry  Robot vision systems  Transforms 
Abstract: Camera localization is a fundamental and key component of autonomous driving vehicles and mobile robots to localize themselves globally for further environment perception, path planning and motion control. Recently end-to-end approaches based on convolutional neural network have been much studied to achieve or even exceed 3D-geometry based traditional methods. In this work, we propose a compact network for absolute camera pose regression. Inspired from those traditional methods, a 3D scene geometry-aware constraint is also introduced by exploiting all available information including motion, depth and image contents. We add this constraint as a regularization term to our proposed network by defining a pixel-level photometric loss and an image-level structural similarity loss. To benchmark our method, different challenging scenes including indoor and outdoor environment are tested with our proposed approach and state-of-the-arts. And the experimental results demonstrate significant performance improvement of our method on both prediction accuracy and convergence efficiency.


Title: Balancing Actuation and Computing Energy in Motion Planning
Key Words: mobile robots  motion control  path planning  balancing actuation  motion planning problems  low-energy robotic vehicles  high-endurance autonomous blimps  computing hardware  actuation hardware  CEIMP  anytime planning algorithm  actuation energy  asymptotic computational complexity  sampling-based motion planning algorithms  compute energy included motion planning algorithm  Planning  Robots  Meters  Task analysis  Hardware  Buildings  Space exploration 
Abstract: We study a novel class of motion planning problems, inspired by emerging low-energy robotic vehicles, such as insect-size flyers, chip-size satellites, and high-endurance autonomous blimps, for which the energy consumed by computing hardware during planning a path can be as large as the energy consumed by actuation hardware during the execution of the same path. We propose a new algorithm, called Compute Energy Included Motion Planning (CEIMP). CEIMP operates similarly to any other anytime planning algorithm, except it stops when it estimates further computing will require more computing energy than potential savings in actuation energy. We show that CEIMP has the same asymptotic computational complexity as existing sampling-based motion planning algorithms, such as PRM*. We also show that CEIMP outperforms the average baseline of using maximum computing resources in realistic computational experiments involving 10 floor plans from MIT buildings. In one representative experiment, CEIMP outperforms the average baseline 90.6% of the time when energy to compute one more second is equal to the energy to move one more meter, and 99.7% of the time when energy to compute one more second is equal to or greater than the energy to move 3 more meters.


Title: Upset Recovery Control for Quadrotors Subjected to a Complete Rotor Failure from Large Initial Disturbances
Key Words: attitude control  autonomous aerial vehicles  cascade control  control system synthesis  helicopters  Monte Carlo methods  quadratic programming  upset recovery control  fault-tolerant controller  arbitrary initial orientations  angular velocities  control method  post-failure quadrotor  Monte-Carlo simulation  control allocator  quadratic programming  control allocation method  almost-global convergence attitude controller  Rotors  Attitude control  Angular velocity  Fault tolerance  Fault tolerant systems  Resource management  Drones 
Abstract: This study has developed a fault-tolerant controller that is able to recover a quadrotor from arbitrary initial orientations and angular velocities, despite the complete failure of a rotor. This cascaded control method includes a position/altitude controller, an almost-global convergence attitude controller, and a control allocation method based on quadratic programming. As a major novelty, a constraint of undesirable angular velocity is derived and fused into the control allocator, which significantly improves the recovery performance. For validation, we have conducted a set of Monte-Carlo simulation to test the reliability of the proposed method of recovering the quadrotor from arbitrary initial attitude/rate conditions. In addition, real-life flight tests have been performed. The results demonstrate that the post-failure quadrotor can recover after being casually tossed into the air.


Title: Identification and evaluation of a force model for multirotor UAVs*
Key Words: aerodynamics  autonomous aerial vehicles  blades  momentum  propellers  rotors (mechanical)  vehicle dynamics  force model  multirotor UAV  model identification method  propellers  blade element theories  aerodynamics  momentum theory  actuation dynamics  Propellers  Aerodynamics  Atmospheric modeling  Predictive models  Blades  Acceleration  Computational modeling 
Abstract: This paper proposes a model identification method and evaluation of a force model for multirotor UAVs. The model incorporates propellers' aerodynamics derived from momentum and blade element theories, as well as aerodynamics of the UAV's structure and actuation dynamics. A two-steps identification approach of the model parameters is proposed. The model is identified and evaluated from outdoor experiments with flight speeds exceeding 10m/s.


Title: Towards Low-Latency High-Bandwidth Control of Quadrotors using Event Cameras
Key Words: aircraft control  angular velocity control  attitude control  autonomous aerial vehicles  cameras  closed loop systems  feedback  helicopters  Hough transforms  image resolution  image sensors  Kalman filters  PD control  robot vision  state estimation  attitude tracking  state estimator  rotor thrusts  black-and-white disk  angular velocity  roll angle  Kalman filter  Hough transform  dualcopter platform  one-dimensional attitude tracking  drones  quadrotors  low-latency high-bandwidth control  event-based feedback  event-camera-driven closed loop control  proportional-derivative attitude control law  event-based state estimation  temporal resolution  sensor latency  high speed vision-based control  frequency 1.0 kHz  time 12.0 ms  Cameras  Robot vision systems  Transforms  Angular velocity  State estimation  Attitude control 
Abstract: Event cameras are a promising candidate to enable high speed vision-based control due to their low sensor latency and high temporal resolution. However, purely event-based feedback has yet to be used in the control of drones. In this work, a first step towards implementing low-latency high-bandwidth control of quadrotors using event cameras is taken. In particular, this paper addresses the problem of one-dimensional attitude tracking using a dualcopter platform equipped with an event camera. The event-based state estimation consists of a modified Hough transform algorithm combined with a Kalman filter that outputs the roll angle and angular velocity of the dualcopter relative to a horizon marked by a black-and-white disk. The estimated state is processed by a proportional-derivative attitude control law that computes the rotor thrusts required to track the desired attitude. The proposed attitude tracking scheme shows promising results of event-camera-driven closed loop control: the state estimator performs with an update rate of 1 kHz and a latency determined to be 12 ms, enabling attitude tracking at speeds of over 1600°/s.


Title: Perception-constrained and Motor-level Nonlinear MPC for both Underactuated and Tilted-propeller UAVS
Key Words: actuators  aircraft control  autonomous aerial vehicles  control system synthesis  helicopters  motion control  nonlinear control systems  predictive control  propellers  rotors (mechanical)  vehicle dynamics  motor-level Nonlinear MPC  tilted-propeller  Perception-constrained Nonlinear Model Predictive Control framework  real-time control  multirotor aerial vehicles  perceptive sensor  realistic actuator limitations  rotor minimum  maximum speeds  multirotor platforms  underactuated quadrotors  tilted-propellers hexarotors  motor-torque level  Propellers  Task analysis  Robot sensing systems  Real-time systems  Actuators  Vehicle dynamics  Torque 
Abstract: In this paper, we present a Perception-constrained Nonlinear Model Predictive Control (NMPC) framework for the real-time control of multi-rotor aerial vehicles. Our formulation considers both constraints from a perceptive sensor and realistic actuator limitations that are the rotor minimum and maximum speeds and accelerations. The formulation is meant to be generic and considers a large range of multi-rotor platforms (such as underactuated quadrotors or tilted-propellers hexarotors) since it does not rely on differential flatness for the dynamical equations, and a broad range of sensors, such as cameras, lidars, etc.... The perceptive constraints are expressed to maintain visibility of a feature point in the sensor's field of view, while performing a reference maneuver. We demonstrate both in simulation and real experiments that our framework is able to exploit the full capabilities of the multi-rotor, to achieve the motion under the aforementioned constraints, and control in real-time the platform at a motor-torque level, avoiding the use of an intermediate unconstrained trajectory tracker.


Title: Coordinate-Free Dynamics and Differential Flatness of a Class of 6DOF Aerial Manipulators
Key Words: control system synthesis  end effectors  manipulator dynamics  manipulator kinematics  path planning  position control  simulated aerial videography task  differential flatness  6DOF aerial manipulators  coordinate-free formulation  coupled dynamics  2DOF articulated manipulator  end effector frame  Manipulator dynamics  End effectors  Trajectory  Task analysis  Planning  Vehicle dynamics 
Abstract: In this work, we derive a coordinate-free formulation of the coupled dynamics of a class of 6DOF aerial manipulators consisting of an underactuated quadrotor equipped with a 2DOF articulated manipulator, and demonstrate that the system is differentially flat with respect to the end effector pose. In particular, we require the center of mass of the entire system to be fixed in the end effector frame, suggesting a reasonable mechanical design criterion. We make use of an inertial decoupling transformation to demonstrate differential flatness, allowing us to plan dynamically feasible trajectories for the system in the space of the 6DOF pose of the end effector, which is ideal for achieving precise manipulator tasks. Simulation results validate the flatness-based planning methodology for our dynamic model, and its usefulness is demonstrated in a simulated aerial videography task.


Title: CMTS: A Conditional Multiple Trajectory Synthesizer for Generating Safety-Critical Driving Scenarios
Key Words: Bayes methods  data analysis  mobile robots  road safety  road vehicles  safety-critical software  trajectory control  CMTS  conditional multiple trajectory synthesizer  safety-critical driving scenarios  naturalistic driving trajectory generation  autonomous driving algorithms  collision-free scenarios  safety-critical cases  near-miss scenarios  off-the-shelf datasets  generative model  conditional probability  trajectory predictions  autonomous vehicle safety  safety-critical data synthesizing framework  variational Bayesian methods  Trajectory  Interpolation  Roads  Training  Aerospace electronics  Data models  Autonomous vehicles 
Abstract: Naturalistic driving trajectory generation is crucial for the development of autonomous driving algorithms. However, most of the data is collected in collision-free scenarios leading to the sparsity of the safety-critical cases. When considering safety, testing algorithms in near-miss scenarios that rarely show up in off-the-shelf datasets and are costly to accumulate is a vital part of the evaluation. As a remedy, we propose a safety-critical data synthesizing framework based on variational Bayesian methods and term it as Conditional Multiple Trajectory Synthesizer (CMTS). We extend a generative model to connect safe and collision driving data by representing their distribution in the latent space and use conditional probability to adapt to different maps. Sampling from the mixed distribution enables us to synthesize the safety-critical data not shown in the safe or collision datasets. Experimental results demonstrate that the generated dataset covers many different realistic scenarios, especially the near-misses. We conclude that the use of data generated by CMTS can improve the accuracy of trajectory predictions and autonomous vehicle safety.


Title: LiDAR Inertial Odometry Aided Robust LiDAR Localization System in Changing City Scenes
Key Words: distance measurement  graph theory  inertial systems  maximum likelihood estimation  motion estimation  optical radar  localization estimation  inertial LiDAR intensity  matching estimation  LiDAR localization system  environmental change detection method  kinematic estimation  frame-to-frame motion estimation  multiresolution occupancy grid based LiDAR inertial odometry  pose graph fusion framework  Apollo-SouthBay dataset  MAP estimation problem  Laser radar  Estimation  Robustness  Roads  Windows  Optimization  Autonomous vehicles 
Abstract: Environmental fluctuations pose crucial challenges to a localization system in autonomous driving. We present a robust LiDAR localization system that maintains its kinematic estimation in changing urban scenarios by using a dead reckoning solution implemented through a LiDAR inertial odometry. Our localization framework jointly uses information from complementary modalities such as global matching and LiDAR inertial odometry to achieve accurate and smooth localization estimation. To improve the performance of the LiDAR odometry, we incorporate inertial and LiDAR intensity cues into an occupancy grid based LiDAR odometry to enhance frame-to-frame motion and matching estimation. Multi-resolution occupancy grid is implemented yielding a coarse-to-fine approach to balance the odometry's precision and computational requirement. To fuse both the odometry and global matching results, we formulate a MAP estimation problem in a pose graph fusion framework that can be efficiently solved. An effective environmental change detection method is proposed that allows us to know exactly when and what portion of the map requires an update. We comprehensively validate the effectiveness of the proposed approaches using both the Apollo-SouthBay dataset and our internal dataset. The results confirm that our efforts lead to a more robust and accurate localization system, especially in dynamically changing urban scenarios.


Title: Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning in Autonomous Driving
Key Words: convolutional neural nets  decision making  graph theory  image representation  image sequences  learning (artificial intelligence)  neural net architecture  traffic engineering computing  DeepScene-Q off-policy reinforcement learning algorithms  graph-Q  graph convolutional networks  multiple variable-length sequences  novel deep scene architecture  complex interaction-aware scene representations  traffic participants  traffic signs  object types  high-level decision making  deep reinforcement learning  high-level decision component  perception component  autonomous driving systems  dynamic interaction-aware scene understanding  traffic simulator SUMO  Computer architecture  Autonomous vehicles  Lenses  Learning (artificial intelligence)  Decision making  Predictive models  Neural networks 
Abstract: The common pipeline in autonomous driving systems is highly modular and includes a perception component which extracts lists of surrounding objects and passes these lists to a high-level decision component. In this case, leveraging the benefits of deep reinforcement learning for high-level decision making requires special architectures to deal with multiple variable-length sequences of different object types, such as vehicles, lanes or traffic signs. At the same time, the architecture has to be able to cover interactions between traffic participants in order to find the optimal action to be taken. In this work, we propose the novel Deep Scenes architecture, that can learn complex interaction-aware scene representations based on extensions of either 1) Deep Sets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q off-policy reinforcement learning algorithms, both outperforming state-ofthe-art methods in evaluations with the publicly available traffic simulator SUMO.


Title: Interacting Vehicle Trajectory Prediction with Convolutional Recurrent Neural Networks
Key Words: convolutional neural nets  feature extraction  feedback  learning (artificial intelligence)  path planning  recurrent neural nets  traffic engineering computing  car  convolutional recurrent neural networks  convolutional long short term memory  Conv-LSTM  interacting vehicle trajectory prediction  novel feedback scheme  path planning  interaction learning  temporal learning  motion learning  Automobiles  Trajectory  Feature extraction  Hidden Markov models  Predictive models  Computer architecture  Road transportation 
Abstract: Anticipating the future trajectories of surrounding vehicles is a crucial and challenging task in path planning for autonomy. We propose a novel Convolutional Long Short Term Memory (Conv-LSTM) based neural network architecture to predict the future positions of cars using several seconds of historical driving observations. This consists of three modules: 1) Interaction Learning to capture the effect of surrounding cars, 2) Temporal Learning to identify the dependency on past movements and 3) Motion Learning to convert the extracted features from these two modules into future positions. To continuously achieve accurate prediction, we introduce a novel feedback scheme where the current predicted positions of each car are leveraged to update future motion, encapsulating the effect of the surrounding cars. Experiments on two public datasets demonstrate that the proposed methodology can match or outperform the state-of-the-art methods for long-term trajectory prediction.


Title: Navigation Command Matching for Vision-based Autonomous Driving
Key Words: control engineering computing  image colour analysis  learning (artificial intelligence)  mobile robots  multi-agent systems  navigation  path planning  road traffic control  robot vision  robust control  traffic engineering computing  suboptimal policy  CARLA driving benchmark  vision-based autonomous driving  imitative reinforcement learning  robust driving policy  nonsmooth rewards  state-action pairs  smooth rewards  matching measurer  navigation rewards  navigation command matching  attention-guided agent  salient regions  RGB images  Navigation  Task analysis  Trajectory  Learning (artificial intelligence)  Autonomous vehicles  Smoothing methods  Current measurement 
Abstract: Learning an optimal policy for autonomous driving task to confront with complex environment is a long- studied challenge. Imitative reinforcement learning is accepted as a promising approach to learn a robust driving policy through expert demonstrations and interactions with environments. However, this model utilizes non-smooth rewards, which have a negative impact on matching between navigation commands and trajectory (state-action pairs), and degrade the generalizability of an agent. Smooth rewards are crucial to discriminate actions generated from sub-optimal policy. In this paper, we propose a navigation command matching (NCM) model to address this issue. There are two key components in NCM, 1) a matching measurer produces smooth navigation rewards that measure matching between navigation commands and trajectory; 2) attention-guided agent performs actions given states where salient regions in RGB images (i.e. roadsides, lane markings and dynamic obstacles) are highlighted to amplify their influence on the final model. We obtain navigation rewards and store transitions to replay buffer after an episode, so NCM is able to discriminate actions generated from suboptimal policy. Experiments on CARLA driving benchmark show our proposed NCM outperforms previous state-of-the- art models on various tasks in terms of the percentage of successfully completed episodes. Moreover, our model improves generalizability of the agent and obtains good performance even in unseen scenarios.


Title: GraphRQI: Classifying Driver Behaviors Using Graph Spectrums
Key Words: computational complexity  driver information systems  eigenvalues and eigenfunctions  graph theory  multi-agent systems  pattern classification  supervised learning  GraphRQI  graph spectrums  road-agent trajectories  driving traits  aggressive driving  conservative driving  nearby road-agents  interagent interactions  unweighted traffic graphs  undirected traffic graphs  supervised learning algorithm  traffic graph  eigenvalue algorithm  autonomous driving datasets  prior driver behavior classification algorithms  Vehicles  Trajectory  Heuristic algorithms  Eigenvalues and eigenfunctions  Laplace equations  Classification algorithms  Topology 
Abstract: We present a novel algorithm (GraphRQI) to identify driver behaviors from road-agent trajectories. Our approach assumes that the road-agents exhibit a range of driving traits, such as aggressive or conservative driving. Moreover, these traits affect the trajectories of nearby road-agents as well as the interactions between road-agents. We represent these inter-agent interactions using unweighted and undirected traffic graphs. Our algorithm classifies the driver behavior using a supervised learning algorithm by reducing the computation to the spectral analysis of the traffic graph. Moreover, we present a novel eigenvalue algorithm to compute the spectrum efficiently. We provide theoretical guarantees for the running time complexity of our eigenvalue algorithm and show that it is faster than previous methods by 2 times. We evaluate the classification accuracy of our approach on traffic videos and autonomous driving datasets corresponding to urban traffic. In practice, GraphRQI achieves an accuracy improvement of up to 25% over prior driver behavior classification algorithms. We also use our classification algorithm to predict the future trajectories of road-agents.


Title: LOL: Lidar-only Odometry and Localization in 3D point cloud maps*
Key Words: distance measurement  image enhancement  image matching  image segmentation  object detection  object recognition  optical radar  LOL system  3D point cloud maps  Lidar-equipped vehicles  3D point segment matching method  Lidar-only odometry and localization algorithm  Kitti datasets  Three-dimensional displays  Laser radar  Sensors  Iterative closest point algorithm  Image color analysis  Trajectory  Real-time systems 
Abstract: In this paper we deal with the problem of odometry and localization for Lidar-equipped vehicles driving in urban environments, where a premade target map exists to localize against. In our problem formulation, to correct the accumulated drift of the Lidar-only odometry we apply a place recognition method to detect geometrically similar locations between the online 3D point cloud and the a priori offline map. In the proposed system, we integrate a state-of-the-art Lidaronly odometry algorithm with a recently proposed 3D point segment matching method by complementing their advantages. Also, we propose additional enhancements in order to reduce the number of false matches between the online point cloud and the target map, and to refine the position estimation error whenever a good match is detected. We demonstrate the utility of the proposed LOL system on several Kitti datasets of different lengths and environments, where the relocalization accuracy and the precision of the vehicle's trajectory were significantly improved in every case, while still being able to maintain real-time performance.


Title: Set-membership state estimation by solving data association
Key Words: mobile robots  position control  robot vision  sensor fusion  SLAM (robots)  state estimation  deterministic approach  data association  underwater robot  sonar data  membership state estimation  localization problem  indistinguishable landmarks  diving phase  unknown initial position  Sonar  State estimation  Rocks  Trajectory  Robot sensing systems  Reliability 
Abstract: This paper deals with the localization problem of a robot in an environment made of indistinguishable landmarks, and assuming the initial position of the vehicle is unknown. This scenario is typically encountered in underwater applications for which landmarks such as rocks all look alike. Furthermore, the position of the robot may be lost during a diving phase, which obliges us to consider unknown initial position. We propose a deterministic approach to solve simultaneously the problems of data association and state estimation, without combinatorial explosion. The efficiency of the method is shown on an actual experiment involving an underwater robot and sonar data.


Title: Collaborative Multi-Robot Localization in Natural Terrain*
Key Words: autonomous underwater vehicles  filtering theory  mobile robots  Monte Carlo methods  multi-robot systems  path planning  sensor fusion  Monterey Bay  terrain relative navigation  filter architecture  collaborative multirobot localization  standard TRN  Monte Carlo simulation  inter-vehicle range measurements  autonomous underwater vehicle  multirobot information  TRN techniques  covariance intersection  Robots  Correlation  Atmospheric measurements  Particle measurements  Extraterrestrial measurements  Collaboration  Navigation 
Abstract: This paper presents a novel filter architecture that allows a team of vehicles to collaboratively localize using Terrain Relative Navigation (TRN). The work explores several causes of measurement correlation that preclude the use of traditional estimators, and proposes an estimator structure that eliminates one source of measurement correlation while properly incorporating others through the use of Covariance Intersection. The result is a consistent estimator that is able to augment proven TRN techniques with multi-robot information, significantly improving localization for vehicles in uninformative terrain. The approach is demonstrated using field data from an Autonomous Underwater Vehicle (AUV) navigating with TRN in Monterey Bay and simulated inter-vehicle range measurements. In addition, a Monte Carlo simulation was used to quantify the algorithm's performance on one example mission. Monte Carlo results show that a vehicle operating in uninformative terrain has 62% lower localization error when fusing range measurements to two converged AUVs than it would using standard TRN.


Title: Efficient Large-Scale Multi-Drone Delivery Using Transit Networks
Key Words: autonomous aerial vehicles  computational complexity  graph theory  multi-robot systems  optimisation  near-optimal polynomial-time task allocation algorithm  delivery sequences  two-layer approach  multifaceted complexity  maximum time  comprehensive algorithmic framework  public transit vehicles  efficient large-scale multidrone delivery  transit network  bounded-suboptimal multiagent pathfinding techniques  Drones  Task analysis  Resource management  Routing  Urban areas  Planning 
Abstract: We consider the problem of controlling a large fleet of drones to deliver packages simultaneously across broad urban areas. To conserve energy, drones hop between public transit vehicles (e.g., buses and trams). We design a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery. We address the multifaceted complexity of the problem through a two-layer approach. First, the upper layer assigns drones to package delivery sequences with a near-optimal polynomial-time task allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network while employing efficient bounded-suboptimal multi-agent pathfinding techniques tailored to our setting. Experiments demonstrate the efficiency of our approach on settings with up to 200 drones, 5000 packages, and transit networks with up to 8000 stops in San Francisco and Washington DC. Our results show that the framework computes solutions within a few seconds (up to 2 minutes at most) on commodity hardware, and that drones travel up to 450% of their flight range with public transit.


Title: Observability Analysis of Flight State Estimation for UAVs and Experimental Validation
Key Words: inertial systems  Kalman filters  magnetic sensors  nonlinear filters  observability  pressure sensors  remotely operated vehicles  singular value decomposition  state estimation  UAV  cost-efficient onboard flight state estimation  robustness  MEMS-based inertial system  static pressure sensors  dynamic pressure sensors  magnetic sensor  weak magnetic field  necessary condition  system state  in-depth observability analysis  sensor data  test flights  EKF  undisturbed estimates  wind state variable  observable spaces  multisensor extended Kalman filter  singular value decomposition  SVD  glider  Observability  Mathematical model  Aerodynamics  State estimation  Global Positioning System  Magnetometers  Pressure measurement 
Abstract: UAVs require reliable, cost-efficient onboard flight state estimation that achieves high accuracy and robustness to perturbation. We analyze a multi-sensor extended Kalman filter (EKF) based on the work by Leutenegger. The EKF uses measurements from a MEMS-based inertial system, static and dynamic pressure sensors as well as GPS. As opposed to other implementations we do not use a magnetic sensor because the weak magnetic field of the earth is subject to disturbances. Observability of the state is a necessary condition for the EKF to work. In this paper, we demonstrate that the system state is observable - which is in contrast to statements in the literature - if the random nature of the air mass is taken into account. Therefore, we carry out an in-depth observability analysis based on a singular value decomposition (SVD). The numerical SVD delivers a wealth of information regarding the observable (sub)spaces. We validated the theoretical findings based on sensor data recorded in test flights on a glider. Most importantly, we demonstrate that the EKF works. It is capable of absorbing large perturbations in the wind state variable converging to the undisturbed estimates.


Title: A Novel Calibration Method between a Camera and a 3D LiDAR with Infrared Images
Key Words: calibration  cameras  image filtering  infrared imaging  optical radar  3D LiDAR  infrared images  infrared filter  calibration method  simultaneous location and mapping  Velodyne VLP-16 sensor  Conferences  Automation 
Abstract: Fusions of LiDARs (light detection and ranging) and cameras have been effectively and widely employed in the communities of autonomous vehicles, virtual reality and mobile mapping systems (MMS) for different purposes, such as localization, high definition map or simultaneous location and mapping. However, the extrinsic calibration between a camera and a 3D LiDAR is a fundamental prerequisite to guarantee its performance. Some previous methods are inaccurate, have calibration error that is several times the beam divergence, and often require special calibration objects, thereby limiting their ubiquitous use for calibration. To overcome these shortcomings, we propose a novel and high-accuracy method for the extrinsic calibration between a camera and a 3D LiDAR. Our approach relies on the infrared images from a camera with an infrared filter, and the 2D-3D corresponding points in a scene with the corners of a wall can be extracted to calculate the six extrinsic parameters. Experiments using the Velodyne VLP-16 sensor show that the method can achieve an extrinsic accuracy at the level of the beam divergence, which is fully analyzed and validated from two different aspects. Therefore, the calibration method in this paper is highly accurate, effective and does not require special complicated calibration objects; thus, it meets the requirements of practical applications.


Title: Online Camera-LiDAR Calibration with Sensor Semantic Information
Key Words: calibration  cameras  computer vision  image colour analysis  image motion analysis  optical radar  optimisation  sensor fusion  RGB camera  light detection and ranging sensor  autonomous vehicles  outdoor environment  online calibration technique  optimal rigid motion transformation  mutual information  perceived data  optimization problem  semantic features  temporally synchronized camera  autonomous driving tasks  online camera-LiDAR calibration  sensor semantic information  sensor data fusion  sensor calibration  complex settings  suboptimal results  extrinsic calibration  edge feature based auto-calibration  cutting-edge machine vision  calibration quality metric  LiDAR sensor  Calibration  Cameras  Laser radar  Image edge detection  Robot sensing systems  Semantics  Robustness 
Abstract: As a crucial step of sensor data fusion, sensor calibration plays a vital role in many cutting-edge machine vision applications, such as autonomous vehicles and AR/VR. Existing techniques either require quite amount of manual work and complex settings, or are unrobust and prone to produce suboptimal results. In this paper, we investigate the extrinsic calibration of an RGB camera and a light detection and ranging (LiDAR) sensor, which are two of the most widely used sensors in autonomous vehicles for perceiving the outdoor environment. Specifically, we introduce an online calibration technique that automatically computes the optimal rigid motion transformation between the aforementioned two sensors and maximizes their mutual information of perceived data, without the need of tuning environment settings. By formulating the calibration as an optimization problem with a novel calibration quality metric based on semantic features, we successfully and robustly align pairs of temporally synchronized camera and LiDAR frames in real time. Demonstrated on several autonomous driving tasks, our method outperforms state-of-the-art edge feature based auto-calibration approaches in terms of robustness and accuracy.


Title: Online calibration of exterior orientations of a vehicle-mounted surround-view camera system
Key Words: calibration  cameras  computer vision  motion estimation  path planning  road vehicles  large-scale outdoor experiments  indoor experiments  exterior orientation parameters  highly practicable online optimisation strategy  complete online optimisation strategy  camera-to-camera transformations  camera-to-vehicle rotations  camera positions  exterior orientation calibration  vision-based vehicle motion estimation  neighbouring views  extrinsic calibration  intelligent vehicle behaviour  exterior perception modality  passenger vehicles  vehicle-mounted surround-view camera system  online calibration  Cameras  Calibration  Optimization  Motion estimation  Geometry  Automobiles  Mirrors 
Abstract: The increasing availability of surround-view camera systems in passenger vehicles motivates their use as an exterior perception modality for intelligent vehicle behaviour. An important problem within this context is the extrinsic calibration between the cameras, which is challenging due to the often reduced overlap between the fields of view of neighbouring views. Our work is motivated by two insights. First, we argue that the accuracy of vision-based vehicle motion estimation depends crucially on the quality of exterior orientation calibration, while design parameters for camera positions typically provide sufficient accuracy. Second, we demonstrate how planar vehicle motion related direction vectors can be used to accurately identify individual camera-to-vehicle rotations, which are more useful than the commonly and tediously derived camera-to-camera transformations. We present a complete and highly practicable online optimisation strategy to obtain the exterior orientation parameters and conclude with successful tests on simulated, indoor, and large-scale outdoor experiments.


Title: On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause Detection and Identification
Key Words: aerospace computing  autonomous aerial vehicles  convolutional neural nets  fault diagnosis  learning (artificial intelligence)  neural net architecture  pattern classification  real-time systems  recurrent neural nets  sensor fusion  raw sensor data  drone misoperations  unmanned aerial vehicle fault cause detection  deep learning architectures  fault cause identification  drone software cyberattack  deep convolutional neural network  long short term memory neural network  autoencoder  real time sensor data classification  Drones  Computer crashes  Real-time systems  Robot sensing systems  Computer architecture  Neural networks  Data models 
Abstract: With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor/actuator system, a physical damage/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on real-time sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and realworld experiments on a drone. Empirical results show that our solution is able to detect (with over 90% accuracy) and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 85% accuracy (experimental data)).


Title: Motion Planning and Task Allocation for a Jumping Rover Team
Key Words: collision avoidance  integer programming  linear programming  mobile robots  multi-robot systems  planetary rovers  travelling salesman problems  trees (mathematics)  jumping rover team  robotic team  unmanned ground vehicles  hybrid operational modes  multiple traveling salesman problem  mTSP  ground surface  jumping capability  optimal path  mixed-integer linear programming problem  RRT*  multiple UGV  optimized motion  customized jumping rovers  Planning  Task analysis  Smoothing methods  Resource management  Mobile robots  Wheels  Jumping Robots  Multiple Traveling Salesman Problem  Path Planning  Rapidly-exploring Random Tree  Mixed-Integer Linear Programming 
Abstract: This paper presents a cooperative robotic team composed of unmanned ground vehicles (UGVs) with hybrid operational modes to tackle the multiple traveling salesman problem (mTSP) with obstacles. The hybrid operational modes allow every UGV in the team to not only travel on a ground surface but also jump over obstacles. We name these UGVs jumping rovers. The jumping capability provides a flexible form of locomotion by leaping and landing on top of obstacles instead of navigating around obstacles. To solve the mTSP, an optimal path between any two objective points in an mTSP is determined by the optimized rapidly-exploring random tree method, named RRT*, and is further improved through a refined RRT* algorithm to find a smoother path between targets. We then formulate the mTSP as a mixed-integer linear programming (MILP) problem to search for the most cost-effective combination of paths for multiple UGVs. The effectiveness of the hybrid operational modes and optimized motion with assigned tasks is verified in an indoor, physical experimental environment using the customized jumping rovers.


Title: Nonlinear Vector-Projection Control for Agile Fixed-Wing Unmanned Aerial Vehicles
Key Words: aerospace components  aircraft control  attitude control  autonomous aerial vehicles  cascade control  helicopters  mobile robots  nonlinear control systems  trajectory control  nonlinear vector-projection control  agile fixed-wing aircraft  fixed-wing platforms  nonlinear control strategy  autonomous flight  cascaded control structure  inner attitude control loop  Special Orthornormal group  outer position control loop  thrust command  attitude references  lift forces  agile fixed-wing unmanned aerial vehicles  rotorcraft  Aircraft  Control systems  Attitude control  Aerodynamics  Position measurement  Aircraft propulsion 
Abstract: Agile fixed-wing aircraft integrate the efficient, high-speed capabilities of conventional fixed-wing platforms with the extreme maneuverability of rotorcraft. This work presents a nonlinear control strategy that harnesses these capabilities to enable autonomous flight through aggressive, time-constrained, three-dimensional trajectories. The cascaded control structure consists of two parts; an inner attitude control loop developed on the Special Orthornormal group that avoids singularities commonly associated with other parametrizations, and an outer position control loop that jointly determines the thrust command and attitude references by implementing a novel vector-projection algorithm. The objective of the algorithm is to decouple roll from the reference attitude to ensure that thrust and lift forces can always be pointed such that position errors converge to zero. The proposed control system represents a single, unified solution that remains effective throughout the aircraft's flight envelope, including aerobatic operation. Controller performance is verified through simulations and experimental flight tests; results show the unified control scheme is capable of performing a wide range of operations that would normally require multiple, single-purpose controllers, and their associated switching logic.


Title: The Reconfigurable Aerial Robotic Chain: Modeling and Control
Key Words: aerospace robotics  control system synthesis  mobile robots  position control  ARC-Alpha prototype  multilinked microaerial vehicles  reconfigurable aerial robotic chain  multiple parallel angular controllers  model predictive position control loop  controller design  connected aerial vehicles  system dynamics  system extendability  distributed sensing  Robot sensing systems  Robot kinematics  Payloads  Shape  Prototypes 
Abstract: This paper overviews the system design, modeling and control of the Aerial Robotic Chain. This new design corresponds to a reconfigurable robotic system of systems consisting of multilinked micro aerial vehicles that presents the ability to cross narrow sections, morph its shape, ferry significant payloads, offer the potential of distributed sensing and processing, and enable system extendability. We present the system dynamics for any number of connected aerial vehicles, followed by the controller design involving a model predictive position control loop combined with multiple parallel angular controllers on SO(3). Evaluation studies both in simulation and through experiments based on our ARC-Alpha prototype are depicted and involve coordinated maneuvering and shape configuration to cross narrow windows.


Title: Direct Acceleration Feedback Control of Quadrotor Aerial Vehicles
Key Words: acceleration control  aircraft control  autonomous aerial vehicles  control system synthesis  feedback  helicopters  motion control  position control  position measurement  regression analysis  three-term control  direct feedback  novel regression-based filter  commanded propeller speeds  low-latency acceleration measurements  control feedback  low frequency position measurements  PID strategy  noisy acceleration measurements  quadrotor aerial vehicles  direct acceleration feedback control  Acceleration  Accelerometers  Propellers  Robustness  Attitude control  Zirconium  Mathematical model 
Abstract: In this paper we propose to control a quadrotor through direct acceleration feedback. The proposed method, while simple in form, alleviates the need for accurate estimation of platform parameters such as mass and propeller effectiveness. In order to use efficaciously the noisy acceleration measurements in direct feedback, we propose a novel regression-based filter that exploits the knowledge on the commanded propeller speeds, and extracts smooth platform acceleration with minimal delay. Our tests show that the controller exhibits a few millimeter error when performing real world tasks with fast changing mass and effectiveness, e.g., in pick and place operation and in turbulent conditions. Finally, we benchmark the direct acceleration controller against the PID strategy and show the clear advantage of using high-frequency and low-latency acceleration measurements directly in the control feedback, especially in the case of low frequency position measurements that are typical for real outdoor conditions.


Title: Trajectory Tracking Nonlinear Model Predictive Control for an Overactuated MAV
Key Words: actuators  autonomous aerial vehicles  control system synthesis  microrobots  mobile robots  nonlinear control systems  observers  optimal control  predictive control  robot dynamics  robot kinematics  trajectory control  trajectory tracking nonlinear model predictive control  omnidirectional microaerial vehicles  free space  rigid body model based approach  receding horizon  optimal wrench commands  mechanical design  optimal actuator commands  disturbance observer  offset-free tracking  6DoF trajectories  Rotors  Resource management  Actuators  Trajectory  Force  Quaternions  Torque 
Abstract: This work presents a method to control omnidirectional micro aerial vehicles (OMAVs) for the tracking of 6-DoF trajectories in free space. A rigid body model based approach is applied in a receding horizon fashion to generate optimal wrench commands that can be constrained to meet limits given by the mechanical design and actuators of the platform. Allocation of optimal actuator commands is performed in a separate step. A disturbance observer estimates forces and torques that may arise from unmodeled dynamics or external disturbances and fuses them into the optimization to achieve offset-free tracking. Experiments on a fully overactuated MAV show the tracking performance and compare it against a classical PD-based controller.


Title: TUNERCAR: A Superoptimization Toolchain for Autonomous Racing
Key Words: automobiles  mobile robots  optimisation  path planning  random processes  search problems  autonomous vehicles  online planning  TUNERCAR  superoptimization toolchain  autonomous racing  vehicle parameters  autonomous racecar  systems infrastructure  parallel implementation  CMA-ES  lap time  naive random search  racing strategy  Optimization  Sociology  Statistics  Vehicle dynamics  Hardware  Robots  Planning 
Abstract: TUNERCAR is a toolchain that jointly optimizes racing strategy, planning methods, control algorithms, and vehicle parameters for an autonomous racecar. In this paper, we detail the target hardware, software, simulators, and systems infrastructure for this toolchain. Our methodology employs a parallel implementation of CMA-ES which enables simulations to proceed 6 times faster than real-world rollouts. We show our approach can reduce the lap times in autonomous racing, given a fixed computational budget. For all tested tracks, our method provides the lowest lap time, and relative improvements in lap time between 7-21%. We demonstrate improvements over a naive random search method with equivalent computational budget of over 15 seconds/lap, and improvements over expert solutions of over 2 seconds/lap. We further compare the performance of our method against hand-tuned solutions submitted by over 30 international teams, comprised of graduate students working in the field of autonomous vehicles. Finally, we discuss the effectiveness of utilizing an online planning mechanism to reduce the reality gap between our simulation and actual tests.


Title: Risk Assessment and Planning with Bidirectional Reachability for Autonomous Driving
Key Words: collision avoidance  mobile robots  path planning  risk analysis  road safety  road traffic control  road vehicles  autonomous driving  subsequent planning  risk assessment algorithms  ego vehicle  risk-inducing factors  bidirectional reachability  risk planning  Risk management  Planning  Prediction algorithms  Autonomous vehicles  Robot sensing systems  Navigation  Probabilistic logic 
Abstract: Risk assessment to quantify the danger associated with taking a certain action is critical to navigating safely through crowded urban environments during autonomous driving. Risk assessment and subsequent planning is usually done by first tracking and predicting trajectories of other agents, such as vehicles and pedestrians, and then choosing an action to avoid future collisions. However, few existing risk assessment algorithms handle occlusion and other sensory limitations effectively. One either assesses the risk in the worst-case scenario and thus makes the ego vehicle overly conservative, or predicts as many hidden agents as possible and thus makes the computation intensive. This paper explores the possibility of efficient risk assessment under occlusion via both forward and backward reachability. The proposed algorithm can not only identify the location of risk-inducing factors, but can also be used during motion planning. The proposed method is evaluated on various four-way highly occluded intersections with up to five other vehicles in the scene. Compared with other risk assessment algorithms, the proposed method shows better efficiency, meaning that the ego vehicle reaches the goal at a higher speed. In addition, it also lowers the median collision rate by 7.5× when compared to state of the art techniques.


Title: Game theoretic decision making based on real sensor data for autonomous vehicles’ maneuvers in high traffic
Key Words: decision making  game theory  learning (artificial intelligence)  mobile robots  Monte Carlo methods  road vehicles  sensors  sensor data  autonomous vehicles  iterative multiplayer game  game model  ego-vehicle  vehicle-to-vehicle communication  traffic simulator  game theoretic decision making  cognitive hierarchy reasoning  Monte Carlo reinforcement learning  Games  Automobiles  Mathematical model  Game theory  Autonomous vehicles  Robot sensing systems 
Abstract: This paper presents an approach for implementing game theoretic decision making in combination with realistic sensory data input so as to allow an autonomous vehicle to perform maneuvers, such as lane change or merge in high traffic scenarios. The main novelty of this work, is the use of realistic sensory data input to obtain the observations as input of an iterative multi-player game in a realistic simulator. The game model allows to anticipate reactions of additional vehicles to the movements of the ego-vehicle without using any specific coordination or vehicle-to-vehicle communication. Moreover, direct information from the simulator, such as position or speed of the vehicles is also avoided.The solution of the game is based on cognitive hierarchy reasoning and it uses Monte Carlo reinforcement learning in order to obtain a near-optimal policy towards a specific goal. Moreover, the game proposed is capable of solving different situations using a single policy. The system has been successfully tested and compared with previous techniques using a realistic hybrid simulator, where the ego-vehicle and its sensors are simulated on a 3D simulator and the additional vehicles' behavior is obtained from a traffic simulator.


Title: Driving in Dense Traffic with Model-Free Reinforcement Learning
Key Words: collision avoidance  learning (artificial intelligence)  predictive control  road traffic control  traffic engineering computing  dense traffic  model-free reinforcement learning  control methods  autonomous vehicle  obstacle-free volume  deep reinforcement learning  continuous control policy  target road lane  model-predictive control-based algorithms  Roads  Autonomous vehicles  Learning (artificial intelligence)  Task analysis  Benchmark testing  Trajectory 
Abstract: Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation. As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.


Title: Enhancing Game-Theoretic Autonomous Car Racing Using Control Barrier Functions
Key Words: collision avoidance  computer games  game theory  game-theoretic autonomous car  control barrier functions  two-player racing game  autonomous ego vehicle  opponent vehicle  two-car racing game  game-theoretic control method hinges  sensitivity-enhanced Nash equilibrium  Collision avoidance  Trajectory  Robots  Acceleration  Safety  Bicycles  Automobiles 
Abstract: In this paper, we consider a two-player racing game, where an autonomous ego vehicle has to be controlled to race against an opponent vehicle, which is either autonomous or human-driven. The approach to control the ego vehicle is based on a Sensitivity-ENhanced NAsh equilibrium seeking (SENNA) method, which uses an iterated best response algorithm in order to optimize for a trajectory in a two-car racing game. This method exploits the interactions between the ego and the opponent vehicle that take place through a collision avoidance constraint. This game-theoretic control method hinges on the ego vehicle having an accurate model and correct knowledge of the state of the opponent vehicle. However, when an accurate model for the opponent vehicle is not available, or the estimation of its state is corrupted by noise, the performance of the approach might be compromised. For this reason, we augment the SENNA algorithm by enforcing Permissive RObust SafeTy (PROST) conditions using control barrier functions. The objective is to successfully overtake or to remain in the front of the opponent vehicle, even when the information about the latter is not fully available. The successful synergy between SENNA and PROST-antithetical to the notable rivalry between the two namesake Formula 1 drivers-is demonstrated through extensive simulated experiments.


Title: Improving Generalisation in Learning Assistance by Demonstration for Smart Wheelchairs
Key Words: Gaussian processes  generalisation (artificial intelligence)  handicapped aids  human-robot interaction  learning (artificial intelligence)  neural nets  wheelchairs  Gaussian process  learning assistance by demonstration  human agent  machine learning models  dimensionality reduction techniques  learning system  custom teleoperation  LAD  generalisation capability  assistive power  learned assistive policy  customised assistance  smart wheelchairs  Wheelchairs  Training  Robots  Vehicles  Haptic interfaces  Sensors  Navigation 
Abstract: Learning Assistance by Demonstration (LAD) is concerned with using demonstrations of a human agent to teach a robot how to assist another human. The concept has previously been used with smart wheelchairs to provide customised assistance to individuals with driving difficulties. A basic premise of this technique is that the learned assistive policy should be able to generalise to environments different than the ones used for training; but this has not been tested before. In this work we evaluate the assistive power and the generalisation capability of LAD using our custom teleoperation and learning system for smart wheelchairs, while seeking to improve it by experimenting with different combinations of dimensionality reduction techniques and machine learning models. Using Autoencoders to reduce the dimension of laserscan data and a Gaussian Process as the learning model, we achieved a 23% improvement in prediction performance against the combination used by the latest work on the field. Using this model to assist a driver exposed to a simulated disability, we observed a 9.8% reduction in track completion times when compared to driving without assistance.


Title: Analyzing the Suitability of Cost Functions for Explaining and Imitating Human Driving Behavior based on Inverse Reinforcement Learning
Key Words: behavioural sciences computing  driver information systems  learning (artificial intelligence)  path planning  road safety  road traffic  road vehicles  vehicles  human drivers  interactive driving  cooperative behavior  dense traffic  autonomous vehicles  safer interaction  cost function selection  cost function structures  inverse reinforcement learning  human driven trajectories  human driving behavior imitation  human driving behavior explanation  theory of mind  Cost function  Trajectory  Vehicles  Roads  Learning (artificial intelligence)  Safety  Planning  Automated vehicles  cost function  inverse reinforcement learning  imitation learning  cooperative motion planning 
Abstract: Autonomous vehicles are sharing the road with human drivers. In order to facilitate interactive driving and cooperative behavior in dense traffic, a thorough understanding and representation of other traffic participants' behavior are necessary. Cost functions (or reward functions) have been widely used to describe the behavior of human drivers since they can not only explicitly incorporate the rationality of human drivers and the theory of mind (TOM), but also share similarity with the motion planning problem of autonomous vehicles. Hence, more human-like driving behavior and comprehensible trajectories can be generated to enable safer interaction and cooperation. However, the selection of cost functions in different driving scenarios is not trivial, and there is no systematic summary and analysis for cost function selection and learning from a variety of driving scenarios. In this work, we aim to investigate to what extent cost functions are suitable for explaining and imitating human driving behavior. Further, we focus on how cost functions differ from each other in different driving scenarios. Towards this goal, we first comprehensively review existing cost function structures in literature. Based on that, we point out required conditions for demonstrations to be suitable for inverse reinforcement learning (IRL). Finally, we use IRL to explore suitable features and learn cost function weights from human driven trajectories in three different scenarios.


Title: Radar-Inertial Ego-Velocity Estimation for Visually Degraded Environments
Key Words: inertial navigation  millimetre wave radar  mobile robots  optimisation  radar imaging  radar receivers  visual perception  millimeter-wave radar-on-a-chip sensor  inertial measurement unit  batch optimization  sliding window  radar-inertial ego-velocity estimation  visually degraded environments  mobile robot body-frame velocity  radar-inertial velocity estimates  visual-inertial approach  Robot sensing systems  Radar measurements  Doppler radar  Doppler effect  Estimation  Velocity measurement 
Abstract: We present an approach for estimating the body-frame velocity of a mobile robot. We combine measurements from a millimeter-wave radar-on-a-chip sensor and an inertial measurement unit (IMU) in a batch optimization over a sliding window of recent measurements. The sensor suite employed is lightweight, low-power, and is invariant to ambient lighting conditions. This makes the proposed approach an attractive solution for platforms with limitations around payload and longevity, such as aerial vehicles conducting autonomous exploration in perceptually degraded operating conditions, including subterranean environments. We compare our radar-inertial velocity estimates to those from a visual-inertial (VI) approach. We show the accuracy of our method is comparable to VI in conditions favorable to VI, and far exceeds the accuracy of VI when conditions deteriorate.


Title: Fast Local Planning and Mapping in Unknown Off-Road Terrain
Key Words: collision avoidance  graph theory  mobile robots  motion control  remotely operated vehicles  SLAM (robots)  trajectory control  off-road terrain  on-line mapping  planning solution  obstacle detection  terrain gradient map  simple cost map  adaptable cost map  optimal paths  control input space  kinematic forward simulation  generated feasible trajectories  optimal trajectory  time operation  frequency 10.0 Hz  frequency 30.0 Hz  Trajectory  Robots  Planning  Aerospace electronics  Microsoft Windows  Three-dimensional displays  Real-time systems 
Abstract: In this paper, we present a fast, on-line mapping and planning solution for operation in unknown, off-road, environments. We combine obstacle detection along with a terrain gradient map to make simple and adaptable cost map. This map can be created and updated at 10 Hz. An A* planner finds optimal paths over the map. Finally, we take multiple samples over the control input space and do a kinematic forward simulation to generated feasible trajectories. Then the most optimal trajectory, as determined by the cost map and proximity to A* path, is chosen and sent to the controller. Our method allows real time operation at rates of 30 Hz. We demonstrate the efficiency of our method in various off-road terrain at high speed.


Title: Barefoot Rover: a Sensor-Embedded Rover Wheel Demonstrating In-Situ Engineering and Science Extractions using Machine Learning
Key Words: aerospace computing  aerospace control  aerospace robotics  control engineering computing  learning (artificial intelligence)  mobile robots  planetary rovers  planetary surfaces  wheels  barefoot rover  machine learning  2D pressure grid  electrochemical impedance spectroscopy sensor  in-situ sensing  sensor-embedded rover wheel  in-situ engineering  planetary exploration missions  Wheels  Robot sensing systems  Rocks  Instruments  Measurement  Surface impedance  DC motors 
Abstract: In this work, we demonstrate an instrumented wheel concept which utilizes a 2D pressure grid, an electrochemical impedance spectroscopy (EIS) sensor and machine learning (ML) to extract meaningful metrics from the interaction between the wheel and surface terrain. These include continuous slip/skid estimation, balance, and sharpness for engineering applications. Estimates of surface hydration, texture, terrain patterns, and regolith physical properties such as cohesion and angle of internal friction are additionally calculated for science applications. Traditional systems rely on post-processing of visual images and vehicle telemetry to estimate these metrics. Through in-situ sensing, these metrics can be calculated in near real time and made available to onboard science and engineering autonomy applications. This work aims to provide a deployable system for future planetary exploration missions to increase science and engineering capabilities through increased knowledge of the terrain.


Title: Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering
Key Words: aerospace computing  image classification  learning (artificial intelligence)  mixture models  neural nets  pose estimation  rendering (computer graphics)  space vehicles  photorealistic rendering  on-orbit proximity operations  6D pose estimation  monocular pose estimation  Unreal Engine 4  neural networks  deep learning framework  orientation soft classification  orientation ambiguity  mixture model  URSO  spacecraft pose estimation  Space vehicles  Quaternions  Pose estimation  Earth  Cameras  Three-dimensional displays 
Abstract: On-orbit proximity operations in space rendezvous, docking and debris removal require precise and robust 6D pose estimation under a wide range of lighting conditions and against highly textured background, i.e., the Earth. This paper investigates leveraging deep learning and photorealistic rendering for monocular pose estimation of known uncooperative spacecraft. We first present a simulator built on Unreal Engine 4, named URSO, to generate labeled images of spacecraft orbiting the Earth, which can be used to train and evaluate neural networks. Secondly, we propose a deep learning framework for pose estimation based on orientation soft classification, which allows modelling orientation ambiguity as a mixture model. This framework was evaluated both on URSO datasets and the European Space Agency pose estimation challenge. In this competition, our best model achieved 3rd place on the synthetic test set and 2nd place on the real test set. Moreover, our results show the impact of several architectural and training aspects, and we demonstrate qualitatively how models learned on URSO datasets can perform on real images from space.


Title: AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM
Key Words: calibration  cameras  sensitivity analysis  SLAM (robots)  calibration parameters  calibration sensitivity analysis  joint angle noise  joint angle values  calibration code  dynamic camera clusters  visual SLAM  time-varying set  extrinsic calibration transformations  DCC calibration accuracy  configuration space  pixel re-projection error  fiducial target  dynamic camera cluster  pose-loop error optimization  Cameras  Calibration  Robot vision systems  Simultaneous localization and mapping  Vehicle dynamics  Optimization  Measurement uncertainty 
Abstract: In order to relate information across cameras in a Dynamic Camera Cluster (DCC), an accurate time-varying set of extrinsic calibration transformations need to be determined. Previous calibration approaches rely solely on collecting measurements from a known fiducial target which limits calibration accuracy as insufficient excitation of the gimbal is achieved. In this paper, we improve DCC calibration accuracy by collecting measurements over the entire configuration space of the gimbal and achieve a 10X improvement in pixel re-projection error. We perform a joint optimization over the calibration parameters between any number of cameras and unknown joint angles using a pose-loop error optimization approach, thereby avoiding the need for overlapping fields-of-view. We test our method in simulation and provide a calibration sensitivity analysis for different levels of camera intrinsic and joint angle noise. In addition, we provide a novel analysis of the degenerate parameters in the calibration when joint angle values are unknown, which avoids situations in which the calibration cannot be uniquely recovered. The calibration code will be made available at https://github.com/TRAILab/AC-DCC.


Title: Online Trajectory Planning for an Industrial Tractor Towing Multiple Full Trailers
Key Words: agricultural machinery  nonlinear dynamical systems  path planning  trajectory control  vehicle dynamics  vehicle dynamics model  online trajectory planning  car-like tractor  passive full trailers  motion planning  complex nonlinear dynamics  simulation based prediction  industrial tractor-trailers vehicle  obstacle free trajectories  Agricultural machinery  Vehicle dynamics  Planning  Trajectory  Dynamics  Wheels  Lead 
Abstract: This paper presents a novel solution for online trajectory planning of a full-size tractor-trailers vehicle composed of a car-like tractor and arbitrary number of passive full trailers. The motion planning problem for such systems was rarely addressed due to the complex nonlinear dynamics. A simulation-based prediction method is proposed to easily handle the complicated nonlinear dynamics and efficiently generate the obstacle-free and dynamically feasible trajectories. The vehicle dynamics model and a two-layer controller are used in the prediction. Implementation results on the real-world full-size industrial tractor-trailers vehicle are presented to validate the performance of the proposed methods.


Title: Slip-Based Nonlinear Recursive Backstepping Path Following Controller for Autonomous Ground Vehicles
Key Words: compensation  control nonlinearities  convergence  feedback  feedforward  mobile robots  motion control  nonlinear control systems  observers  path planning  position control  robot dynamics  robot kinematics  robust control  stability  steering systems  variable structure systems  kinematic controller  feedforward slip compensation  variable structure controller  graceful motion  yaw rate commands  backstepping dynamic controller  robust steering commands  output feedback control  autonomous ground vehicles  vehicle steering control  graceful lateral motion  couples yaw-rate based path  steering angle  heading error  slip-based nonlinear recursive backstepping path following controller  observer based sideslip estimates  path following accuracy  error convergence  slip-based kinematic model  dynamic model  path following error  robustness  high gain observer  stability analysis  Kinematics  Vehicle dynamics  Backstepping  Tracking  Dynamics  Tires  Convergence 
Abstract: Path following accuracy and error convergence with graceful motion in vehicle steering control is challenging due to the competing nature of these requirements, especially across a range of operating speeds. This work is founded upon slip-based kinematic and dynamic models, which allow derivation of controllers considering error due to sideslip and the mapping between steering commands and graceful lateral motion. A novel recursive backstepping steering controller is proposed that better couples yaw-rate based path following commands to steering angle and rate. Observer based sideslip estimates are combined with heading error in the kinematic controller to provide feedforward slip compensation. Path following error is compensated by a Variable Structure Controller (VSC) to balance graceful motion, path following error, and robustness. Yaw rate commands are used by a backstepping dynamic controller to generate robust steering commands. A High Gain Observer (HGO) estimates sideslip and yaw rate for output feedback control. Stability analysis is provided and peaking is addressed. Field experimental results evaluate the work and provide comparisons to MPC.


Title: MulRan: Multimodal Range Dataset for Urban Place Recognition
Key Words: geophysical image processing  geophysical techniques  image recognition  mobile robots  object recognition  optical radar  radar imaging  robot vision  multimodal range dataset  radio detection and ranging  light detection and ranging  urban environment  range sensor-based place recognition  6D baseline trajectories  place recognition ground truth  image-format data  time-stamped 1D intensity arrays  polar images  image data  radar place recognition method  LiDAR  longer-range measurements  urban place recognition  MulRan  Laser radar  Radar imaging  Three-dimensional displays  Urban areas  Simultaneous localization and mapping 
Abstract: This paper introduces a multimodal range dataset namely for radio detection and ranging (radar) and light detection and ranging (LiDAR) specifically targeting the urban environment. By extending our workshop paper [1] to a larger scale, this dataset focuses on the range sensor-based place recognition and provides 6D baseline trajectories of a vehicle for place recognition ground truth. Provided radar data support both raw-level and image-format data, including a set of time-stamped 1D intensity arrays and 360° polar images, respectively. In doing so, we provide flexibility between raw data and image data depending on the purpose of the research. Unlike existing datasets, our focus is at capturing both temporal and structural diversities for range-based place recognition research. For evaluation, we applied and validated that our previous location descriptor and its search algorithm [2] are highly effective for radar place recognition method. Furthermore, the result shows that radar-based place recognition outperforms LiDAR-based one exploiting its longer-range measurements. The dataset is available from https://sites.google.com/view/mulran-pr.


Title: Aggressive Online Control of a Quadrotor via Deep Network Representations of Optimality Principles
Key Words: aircraft control  autonomous aerial vehicles  control engineering computing  helicopters  mobile robots  neural nets  optimisation  time optimal control  power optimality  time optimality  deep neural network  robotic applications  optimality principles  deep network representations  aggressive online control  time-optimal maneuvers  offline optimal control method  aggressive quadrotor control  Trajectory  Optimal control  Stability analysis  Neural networks  Delays  Training  Drones 
Abstract: Optimal control holds great potential to improve a variety of robotic applications. The application of optimal control on-board limited platforms has been severely hindered by the large computational requirements of current state of the art implementations. In this work, we make use of a deep neural network to directly map the robot states to control actions. The network is trained offline to imitate the optimal control computed by a time consuming direct nonlinear method. A mixture of time optimality and power optimality is considered with a continuation parameter used to select the predominance of each objective. We apply our networks (termed G&CNets) to aggressive quadrotor control, first in simulation and then in the real world. We give insight into the factors that influence the `reality gap' between the quadrotor model used by the offline optimal control method and the real quadrotor. Furthermore, we explain how we set up the model and the control structure on-board of the real quadrotor to successfully close this gap and perform time-optimal maneuvers in the real world. Finally, G&CNet's performance is compared to state-of-the-art differential-flatness-based optimal control methods. We show, in the experiments, that G&CNets lead to significantly faster trajectory execution due to, in part, the less restrictive nature of the allowed state-to-input mappings.


Title: Robust quadcopter control with artificial vector fields*
Key Words: autonomous aerial vehicles  control system synthesis  helicopters  mobile robots  multi-robot systems  nonlinear control systems  path planning  position control  robust control  time-varying systems  robust quadcopter control  artificial vector fields  path tracking control strategy  control laws  vector field  controlled second order integrator  quadcopter model  input-to-state stable  control inputs  Robots  Vehicle dynamics  Robustness  Convergence  Level set  Mathematical model  Force 
Abstract: This article presents a path tracking control strategy for a quadcopter to follow a time varying curve. The control is based on artificial vector fields. The construction of the field is based on a well known technique in the literature. Next, control laws are developed to impose the behavior of the vector field to a second order integrator model. Finally, control laws are developed to impose the dynamics of the controlled second order integrator to a quadcopter model, which assumes the thrust and the angular rates as input commands. Asymptotic convergence of the whole system is proved by showing that the individual systems in cascade connection are input-to-state stable. We also analyze the influence of norm-bounded disturbances in the control inputs to evaluate the robustness of the controller. We show that bounded disturbances originate limited deviations from the target curve. Simulations and a real robot experiment exemplify and validate the developed theory.


Title: Simulation-Based Reinforcement Learning for Real-World Autonomous Driving
Key Words: image segmentation  learning (artificial intelligence)  road vehicles  traffic engineering computing  simulation-based reinforcement learning  real-world autonomous driving  driving system  real-world vehicle  driving policy  RGB images  single camera  semantic segmentation  synthetic data  real-world data  segmentation network  real-world experiments  sim-to-real policy transfer  real-world performance  Training  Visualization  Learning (artificial intelligence)  Semantics  Robots  Image segmentation  Predictive models 
Abstract: We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.


Title: Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions
Key Words: behavioural sciences computing  human-robot interaction  navigation  path planning  pedestrians  road traffic control  road vehicles  traffic engineering computing  virtual reality  automated vehicle interactions  safe navigation  automated vehicles  pedestrian interactions  human-driven vehicles  HDV  hybrid systems model  constant velocity dynamics  long-term pedestrian trajectory prediction  immersive virtual environment  AV interactions  pedestrian crosswalk behavior analysis  pedestrian crosswalk behavior prediction  gap acceptance behavior  AV motion planning  IVE  Predictive models  Trajectory  Legged locomotion  Vehicle dynamics  Virtual environments  Planning  Dynamics 
Abstract: For safe navigation around pedestrians, automated vehicles (AVs) need to plan their motion by accurately predicting pedestrians' trajectories over long time horizons. Current approaches to AV motion planning around crosswalks predict only for short time horizons (1-2 s) and are based on data from pedestrian interactions with human-driven vehicles (HDVs). In this paper, we develop a hybrid systems model that uses pedestrians' gap acceptance behavior and constant velocity dynamics for long-term pedestrian trajectory prediction when interacting with AVs. Results demonstrate the applicability of the model for long-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further, we compared measures of pedestrian crossing behaviors in the immersive virtual environment (when interacting with AVs) to that in the real world (results of published studies of pedestrians interacting with HDVs), and found similarities between the two. These similarities demonstrate the applicability of the hybrid model of AV interactions developed from an immersive virtual environment (IVE) for real-world scenarios for both AVs and HDVs.


Title: The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset
Key Words: CW radar  distance measurement  FM radar  Global Positioning System  millimetre wave radar  optical radar  road vehicle radar  Oxford Radar RobotCar dataset  radar extension  millimetre-wave FMCW scanning radar data  central Oxford route  truth optimised radar odometry  autonomous vehicles  environmental conditions  sensor modalities  AD 2019 01  urban driving  weather condition  traffic condition  lighting condition  Navtech CTS350-X radar  Velodyne HDL-32E 3D LIDAR  GPS-INS receiver  ori.ox.ac.uk/datasets/radar-robotear-dataset  size 280.0 km  memory size 4.7 TByte  Robot sensing systems  Laser radar  Three-dimensional displays  Azimuth  Calibration 
Abstract: In this paper we present The Oxford Radar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.(/P)(P)The data were gathered in January 2019 over thirty-two traversals of a central Oxford route spanning a total of 280 km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7 TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotear-dataset.


Title: Multi-modal Experts Network for Autonomous Driving
Key Words: computational complexity  control engineering computing  expert systems  inference mechanisms  learning (artificial intelligence)  mobile robots  road vehicles  sensory data  autonomous driving  autonomous vehicles  computational complexity  multistage training procedure  end-to-end learning  multimodal experts network architecture  inference time step  mixed discrete-continuous policy  Laser radar  Feature extraction  Cameras  Training  Autonomous vehicles  Robot sensing systems 
Abstract: End-to-end learning from sensory data has shown promising results in autonomous driving. While employing many sensors enhances world perception and should lead to more robust and reliable behavior of autonomous vehicles, it is challenging to train and deploy such network and at least two problems are encountered in the considered setting. The first one is the increase of computational complexity with the number of sensing devices. The other is the phenomena of network overfitting to the simplest and most informative input. We address both challenges with a novel, carefully tailored multi-modal experts network architecture and propose a multi-stage training procedure. The network contains a gating mechanism, which selects the most relevant input at each inference time step using a mixed discrete-continuous policy. We demonstrate the plausibility of the proposed approach on our 1/6 scale truck equipped with three cameras and one LiDAR.


Title: Visual Localization with Google Earth Images for Robust Global Pose Estimation of UAVs
Key Words: autonomous aerial vehicles  distance measurement  Global Positioning System  image filtering  image registration  image sensors  mobile robots  multi-robot systems  pose estimation  rendering (computer graphics)  robot vision  Google Earth images  georeferenced rendered images  dense mutual information technique  outdoor GPS-denied environments  image registrations  gimballed visual odometry pipeline  visual localization  robust global pose estimation  multirotor UAV  typical feature-based localizer  Cameras  Image registration  Three-dimensional displays  Visualization  Earth  Robustness  Google 
Abstract: We estimate the global pose of a multirotor UAV by visually localizing images captured during a flight with Google Earth images pre-rendered from known poses. We metrically localize real images with georeferenced rendered images using a dense mutual information technique to allow accurate global pose estimation in outdoor GPS-denied environments. We show the ability to consistently localize throughout a sunny summer day despite major lighting changes while demonstrating that a typical feature-based localizer struggles under the same conditions. Successful image registrations are used as measurements in a filtering framework to apply corrections to the pose estimated by a gimballed visual odometry pipeline. We achieve less than 1 m and 1° RMSE on a 303 m flight and less than 3 m and 3° RMSE on six 1132 m flights as low as 36 m above ground level conducted at different times of the day from sunrise to sunset.


Title: Force-based Control of Bipedal Balancing on Dynamic Terrain with the "Tallahassee Cassie" Robotic Platform
Key Words: force control  humanoid robots  legged locomotion  motion control  PD control  position control  springs (mechanical)  bipedal control  minimal model information  dynamic impacts  walking running controllers  modeling information  force-based control  bipedal balancing  Tallahassee Cassie robotic platform  bipedal robots  force-based double support balancing controller  dynamic terrain scenarios  robotic bipedal platform  minimal information  robot model  individual links  pelvis-centric pelvis positions  commanding pelvis positions  model-free PD controller  Pelvis  Legged locomotion  Foot  Dynamics  Vehicle dynamics  Robot kinematics 
Abstract: Out in the field, bipedal robots need to travel on terrain that is uneven, non-rigid, and sometimes moving beneath their feet. We present a force-based double support balancing controller for such dynamic terrain scenarios for bipedal robots, and test it on the robotic bipedal platform "Tallahassee Cassie." The presented controller relies on minimal information about the robot model, requiring its kinematics and overall weight, but not inertias of individual links or components. The controller is pelvis-centric, commanding pelvis positions in Cartesian space, which a model-free PD controller converts to motor torques in joint space. By commanding forces, torques, and a frontal center of pressure in this fashion, Tallahassee Cassie is capable of balancing on a variety of scenarios, from a lifting/sliding platform, to soft foam, to a sudden drop. These results show the potential for bipedal control to balance successfully despite minimal model information, the presence of large dynamic impacts-e.g., falling through trap door, and soft series-spring deflections. These results motivate future work for walking and running controllers on dynamic terrain with relatively low reliance on modeling information.


Title: CyPhyHouse: A programming, simulation, and deployment toolchain for heterogeneous distributed coordination
Key Words: control engineering computing  control system synthesis  learning (artificial intelligence)  middleware  mobile computing  mobile robots  multi-threading  path planning  program debugging  specification languages  heterogeneous distributed coordination  libraries  development tools  application development processes  mobile computing  machine learning  CyPhyHouse  debugging  distributed mobile robotic applications  distributed applications  Koord programming language  controller design  distributed network protocols  platform-independent middleware  path planning  multithreaded simulator  Koord applications  application code  heterogeneous agents  heterogeneous mobile platforms  design cycles  robotic testbed  distributed task allocation  deployment toolchain  hardware-agnostic application  Robot kinematics  Task analysis  Middleware  Collision avoidance  Python 
Abstract: Programming languages, libraries, and development tools have transformed the application development processes for mobile computing and machine learning. This paper introduces CyPhyHouse-a toolchain that aims to provide similar programming, debugging, and deployment benefits for distributed mobile robotic applications. Users can develop hardware-agnostic, distributed applications using the high-level, event driven Koord programming language, without requiring expertise in controller design or distributed network protocols. The modular, platform-independent middleware of CyPhyHouse implements these functionalities using standard algorithms for path planning (RRT), control (MPC), mutual exclusion, etc. A high-fidelity, scalable, multi-threaded simulator for Koord applications is developed to simulate the same application code for dozens of heterogeneous agents. The same compiled code can also be deployed on heterogeneous mobile platforms. The effectiveness of CyPhyHouse in improving the design cycles is explicitly illustrated in a robotic testbed through development, simulation, and deployment of a distributed task allocation application on in-house ground and aerial vehicles.


Title: Weakly Supervised Silhouette-based Semantic Scene Change Detection
Key Words: feature extraction  image segmentation  learning (artificial intelligence)  object detection  weakly supervised silhouette-based semantic scene change detection  novel semantic scene change detection scheme  semantic change detection network  large-scale dataset  specific dataset  semantic extraction  change detection task  siamese network structure  publicly available dataset  Semantics  Image segmentation  Cameras  Training  Task analysis  Satellites  Estimation 
Abstract: This paper presents a novel semantic scene change detection scheme with only weak supervision. A straightforward approach for this task is to train a semantic change detection network directly from a large-scale dataset in an end-to-end manner. However, a specific dataset for this task, which is usually labor-intensive and time-consuming, becomes indispensable. To avoid this problem, we propose to train this kind of network from existing datasets by dividing this task into change detection and semantic extraction. On the other hand, the difference in camera viewpoints, for example, images of the same scene captured from a vehicle-mounted camera at different time points, usually brings a challenge to the change detection task. To address this challenge, we propose a new siamese network structure with the introduction of correlation layer. In addition, we create a publicly available dataset for semantic change detection to evaluate the proposed method. The experimental results verified both the robustness to viewpoint difference in change detection task and the effectiveness for semantic change detection of the proposed networks. Our code and dataset are available at https://github.com/xdspacelab/sscdnet.


Title: Who2com: Collaborative Perception via Learnable Handshake Communication
Key Words: aircraft communication  autonomous aerial vehicles  image segmentation  learning (artificial intelligence)  multi-agent systems  multi-robot systems  neural nets  visual perception  degraded sensor data  compressed request  aerial robots  semantic segmentation task  collaborative perception  learnable handshake communication  local observations  neighboring agents  multiagent reinforcement learning  bandwidth-sensitive manner  scene understanding tasks  communication protocols  multistage handshake communication mechanism  neural network  Who2com  AirSim simulator  AirSim-CP dataset  Task analysis  Bandwidth  Semantics  Training  Collaboration  Robot sensing systems 
Abstract: In this paper, we propose the problem of collaborative perception, where robots can combine their local observations with those of neighboring agents in a learnable way to improve accuracy on a perception task. Unlike existing work in robotics and multi-agent reinforcement learning, we formulate the problem as one where learned information must be shared across a set of agents in a bandwidth-sensitive manner to optimize for scene understanding tasks such as semantic segmentation. Inspired by networking communication protocols, we propose a multi-stage handshake communication mechanism where the neural network can learn to compress relevant information needed for each stage. Specifically, a target agent with degraded sensor data sends a compressed request, the other agents respond with matching scores, and the target agent determines who to connect with (i.e., receive information from). We additionally develop the AirSim-CP dataset and metrics based on the AirSim simulator where a group of aerial robots perceive diverse landscapes, such as roads, grasslands, buildings, etc. We show that for the semantic segmentation task, our handshake communication method significantly improves accuracy by approximately 20% over decentralized baselines, and is comparable to centralized ones using a quarter of the bandwidth.


Title: Towards Proactive Navigation: A Pedestrian-Vehicle Cooperation Based Behavioral Model
Key Words: collision avoidance  mobile robots  motion control  path planning  remotely operated vehicles  road traffic control  road vehicles  trajectory control  pedestrian-vehicle cooperation  autonomous vehicle  intelligent transportation  autonomous navigation research  safe proactive navigation  pedestrian-vehicle interaction behavioral model  interaction scenario  quantitative time-varying function  cooperation estimation  cooperation-based trajectory planning model  Navigation  Space vehicles  Predictive models  Strain  Task analysis  Trajectory  Autonomous vehicles 
Abstract: Developing autonomous vehicles capable of navigating safely and socially around pedestrians is a major challenge in intelligent transportation. This challenge cannot be met without understanding pedestrians' behavioral response to an autonomous vehicle, and the task of building a clear and quantitative description of the pedestrian to vehicle interaction remains a key milestone in autonomous navigation research. As a step towards safe proactive navigation in a space shared with pedestrians, this work introduces a pedestrian-vehicle interaction behavioral model. The model estimates the pedestrian's cooperation with the vehicle in an interaction scenario by a quantitative time-varying function. Using this cooperation estimation the pedestrian's trajectory is predicted by a cooperation-based trajectory planning model. Both parts of the model are tested and validated using real-life recorded scenarios of pedestrian-vehicle interaction. The model is capable of describing and predicting agents' behaviors when interacting with a vehicle in both lateral and frontal crossing scenarios.


Title: A Linearized Model for an Ornithopter in Gliding Flight: Experiments and Simulations
Key Words: aerodynamics  aerospace components  autonomous aerial vehicles  linearisation techniques  position control  velocity control  longitudinal gliding flight configuration  aerodynamic forces  linearized potential theory  flat plate  flapping-wing episodes  linear potential theory  steady-state descent  terminal velocity  pitching  gliding angles  tail position  flapping-wing configuration  flight velocity  climbing episodes  realistic simulation tool  flapping frequencies  ornithopter flight  linearized model  flapping-wings UAV  Unreal Engine 4  Numerical models  Force  Mathematical model  Hardware  Steady-state  Computed tomography  Aerodynamics 
Abstract: This work studies the accuracy of a simple but effective analytical model for a flapping-wings UAV in longitudinal gliding flight configuration comparing it with experimental results of a real ornithopter. The aerodynamic forces are modeled following the linearized potential theory for a flat plate in gliding configuration, extended to flapping-wing episodes modeled also by the (now unsteady) linear potential theory, which are studied numerically. In the gliding configuration, the model reaches a steady-state descent at given terminal velocity and pitching and gliding angles, governed by the wings and tail position. In the flapping-wing configuration, it is noticed that the vehicle can increase its flight velocity and perform climbing episodes. A realistic simulation tool based on Unreal Engine 4 was developed to visualize the effect of the tail position and flapping frequencies and amplitudes on the ornithopter flight in real time. The paper also includes the experimental validation of the gliding flight and the data has been released for the community.


Title: Towards biomimicry of a bat-style perching maneuver on structures: the manipulation of inertial dynamics
Key Words: aerospace control  angular momentum  biomimetics  closed loop systems  gears  geometry  mobile robots  motion control  remotely operated vehicles  robot dynamics  bat-style perching maneuver  aerial drone designs  aerial flip turns  landing surface  zero-angular-momentum turns  detachable landing gear  closed-loop manipulations  biomimicry  inertial dynamics manipulation  bat flight characteristics  dynamical system  geometric conservation properties  Aerodynamics  Manipulator dynamics  Mathematical model  Robot sensing systems  Birds 
Abstract: The flight characteristics of bats remarkably have been overlooked in aerial drone designs. Unlike other animals, bats leverage the manipulation of inertial dynamics to exhibit aerial flip turns when they perch. Inspired by this unique maneuver, this work develops and uses a tiny robot called Harpoon to demonstrate that the preparation for upside-down landing is possible through: 1) reorientation towards the landing surface through zero-angular-momentum turns and 2) reaching to the surface through shooting a detachable landing gear. The closed-loop manipulations of inertial dynamics takes place based on a symplectic description of the dynamical system (body and appendage), which is known to exhibit an excellent geometric conservation properties.


Title: A ROS Gazebo plugin to simulate ARVA sensors
Key Words: aerospace communication  autonomous aerial vehicles  control engineering computing  operating systems (computers)  radio transceivers  rescue robots  robot programming  sensors  ROS Gazebo plugin  forefront technology  Search & Rescue operations  ARVA sensor simulation  transceiver sensor  Appareil de Recherche de Victims en Avalanche  Unmanned Aerial Vehicle  Receivers  Transmitters  Sensors  Electromagnetics  Antennas  Robots  Unmanned aerial vehicles 
Abstract: This paper addresses the problem to simulate ARVA sensors using ROS and Gazebo. ARVA is a French acronym which stands for Appareil de Recherche de Victims en Avalanche and represents the forefront technology adopted in Search & Rescue operations to localize victims of avalanches buried under the snow. The aim of this paper is to describe the mathematical and theoretical background of the transceiver, discussing its implementation and integration with ROS allowing researchers to develop faster and smarter Search &Rescue strategies based on ARVA receiver data. To assess the effectiveness of the proposed sensor model, We present a simulation scenario in which an Unmanned Aerial Vehicle equipped with the transceiver sensor performs a basic S&R pattern using the output of ARVA system.


Title: Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control
Key Words: Bayes methods  belief networks  collision avoidance  decision making  inference mechanisms  neurocontrollers  probability  road safety  Bayesian inference methods  uncertainty computation  pointwise uncertainty measures  end-to-end Bayesian controllers  autonomous driving scenarios  Bayesian neural networks  sensor noise  controller behaviour  safety guarantees  neural network controllers  end-to-end autonomous driving control  statistical guarantees  uncertainty quantification  Uncertainty  Safety  Autonomous vehicles  Neural networks  Automobiles  Probabilistic logic 
Abstract: Deep neural network controllers for autonomous driving have recently benefited from significant performance improvements, and have begun deployment in the real world. Prior to their widespread adoption, safety guarantees are needed on the controller behaviour that properly take account of the uncertainty within the model as well as sensor noise. Bayesian neural networks, which assume a prior over the weights, have been shown capable of producing such uncertainty measures, but properties surrounding their safety have not yet been quantified for use in autonomous driving scenarios. In this paper, we develop a framework based on a state-of-the-art simulator for evaluating end-to-end Bayesian controllers. In addition to computing pointwise uncertainty measures that can be computed in real time and with statistical guarantees, we also provide a method for estimating the probability that, given a scenario, the controller keeps the car safe within a finite horizon. We experimentally evaluate the quality of uncertainty computation by three Bayesian inference methods in different scenarios and show how the uncertainty measures can be combined and calibrated for use in collision avoidance. Our results suggest that uncertainty estimates can greatly aid decision making in autonomous driving.


Title: Learn-to-Recover: Retrofitting UAVs with Reinforcement Learning-Assisted Flight Control Under Cyber-Physical Attacks
Key Words: actuators  aerospace computing  autonomous aerial vehicles  control engineering computing  fault diagnosis  fault tolerant control  helicopters  learning (artificial intelligence)  position control  security of data  stability  fault-tolerant control policy  actuator  stabilizing controller  detection activation  sensor faults  position control  learn-to-recover  UAVs  reinforcement learning-assisted flight control  cyber-physical attacks  quadcopter unmanned aerial vehicles  sensor attack  Actuators  Fault tolerance  Fault tolerant systems  Vehicle dynamics  Learning (artificial intelligence)  Training  Solid modeling 
Abstract: In this paper, we present a generic fault-tolerant control (FTC) strategy via reinforcement learning (RL). We demonstrate the effectiveness of this method on quadcopter unmanned aerial vehicles (UAVs). The fault-tolerant control policy is trained to handle actuator and sensor fault/attack. Unlike traditional FTC, this policy does not require fault detection and diagnosis (FDD) nor tailoring the controller for specific attack scenarios. Instead, the policy is running simultaneously alongside the stabilizing controller without the need for on- detection activation. The effectiveness of the policy is compared with traditional active and passive FTC strategies against actuator and sensor faults. We compare their performance in position control tasks via simulation and experiments on quadcopters. The result shows that the strategy can effectively tolerate different types of attacks/faults and maintain the vehicle's position, outperforming the other two methods.


Title: Model Reference Adaptive Control of Multirotor for Missions with Dynamic Change of Payloads During Flight
Key Words: aerospace robotics  aircraft control  attitude control  helicopters  MIMO systems  model reference adaptive control systems  nonlinear control systems  robust control  stability  payloads  multirotor aerial robot  flight controller  flight stability  attitude control  aerial robot system  model reference adaptive control  nonlinear multiple-input and multiple-output  MRAC  Attitude control  Adaptation models  Payloads  MIMO communication  Unmanned aerial vehicles  Adaptive control  Gravity 
Abstract: Carrying payloads in air is a major mission for multirotor aerial robot. However, the presence of payloads on multirotor aerial robot has a risk of degrading the performance of the flight controller. This concern becomes obvious especially when carrying objects not securely attached to the body or performing aerial manipulation. Therefore, controller with the ability to adapt itself to the effects of payloads on flight stability is needed. This paper proposes a novel nonlinear multiple-input and multiple-output (MIMO) model reference adaptive control (MRAC) system for attitude control of multirotor aerial robots which can dynamically compensate change in the position of center of gravity and inertia caused by payloads. Stability and robustness of the controller are experimentally confirmed in quadrotor and transformable multirotor, and experiments modeling practical applications are conducted for each aerial robot system, proving the utility of the controller.


Title: The Tiercel: A novel autonomous micro aerial vehicle that can map the environment by flying into obstacles
Key Words: cameras  collision avoidance  image sensors  mobile robots  navigation  robot vision  SLAM (robots)  space vehicles  autonomous microaerial vehicle  autonomous Tiercel robots  collision detector design  fisheye camera  reflective obstacles  transparent obstacles  collision-resilient robot  Tiercel MAV  autonomous navigation  autonomous flight  Collision avoidance  Cameras  Robot vision systems  Planning 
Abstract: Autonomous flight through unknown environments in the presence of obstacles is a challenging problem for micro aerial vehicles (MAVs). A majority of the current state-of-art research assumes obstacles as opaque objects that can be easily sensed by optical sensors such as cameras or LiDARs. However in indoor environments with glass walls and windows, or scenarios with smoke and dust, robots (even birds) have a difficult time navigating through the unknown space.In this paper, we present the design of a new class of micro aerial vehicles that achieves autonomous navigation and are robust to collisions. In particular, we present the Tiercel MAV: a small, agile, light weight and collision-resilient robot powered by a cellphone grade CPU. Our design exploits contact to infer the presence of transparent or reflective obstacles like glass walls, integrating touch with visual perception for SLAM. The Tiercel is able to localize using visual-inertial odometry (VIO) running on board the robot with a single downward facing fisheye camera and an IMU. We show how our collision detector design and experimental set up enable us to characterize the impact of collisions on VIO. We further develop a planning strategy to enable the Tiercel to fly autonomously in an unknown space, sustaining collisions and creating a 2D map of the environment. Finally we demonstrate a swarm of three autonomous Tiercel robots safely navigating and colliding through an obstacle field to reach their objectives.


Title: Adaptive Control of Variable-Pitch Propellers: Pursuing Minimum-Effort Operation
Key Words: adaptive control  aerospace propulsion  aircraft control  autonomous aerial vehicles  control system synthesis  electric propulsion  pitch control (position)  propellers  state-space methods  adaptive control  minimum-effort operation  unmanned aerial vehicles  UAV  electric propulsion systems  disparate flight modes  forward-moving flight  flight mode dissimilarity  fixed-geometry propulsion systems  variable-geometry systems  variable pitch propeller  propulsion performance  VPP system control  operation state space  hovering  near-minimum-electrical-effort propulsion system behavior  Propellers  Mathematical model  Servomotors  Geometry  Brushless DC motors  Blades 
Abstract: As Unmanned Aerial Vehicles (UAVs) become more commonly used in industry, their performance will continue to be challenged. A performance bottleneck that is crucial to overcome is the design of electric propulsion systems for UAVs that operate in disparate flight modes (e.g., hovering and forward-moving flight). While flight mode dissimilarity presents a fundamental design challenge for fixed-geometry propulsion systems, variable-geometry systems such as the Variable Pitch Propeller (VPP) ones are able to provide superior propulsion performance across a wide range of flight modes. This work builds on previous work by the authors and presents a VPP system control and estimation framework for safe, near-minimum-electrical-effort propulsion system behavior across the whole operation state space of any UAV. Multiple simulated validations are presented to support the feasibility of the approach.


Title: Predicting optimal value functions by interpolating reward functions in scalarized multi-objective reinforcement learning
Key Words: Gaussian processes  interpolation  learning (artificial intelligence)  optimisation  smooth interpolation  reward function weights  optimal value function  multiobjective reinforcement learning problems  Gaussian process  value function transforms  MORL problems  Interpolation  Training  Learning (artificial intelligence)  Gaussian processes  Mathematical model  Random variables  Optimization 
Abstract: A common approach for defining a reward function for multi-objective reinforcement learning (MORL) problems is the weighted sum of the multiple objectives. The weights are then treated as design parameters dependent on the expertise (and preference) of the person performing the learning, with the typical result that a new solution is required for any change in these settings. This paper investigates the relationship between the reward function and the optimal value function for MORL; specifically addressing the question of how to approximate the optimal value function well beyond the set of weights for which the optimization problem was actually solved, thereby avoiding the need to recompute for any particular choice. We prove that the value function transforms smoothly given a transformation of weights of the reward function (and thus a smooth interpolation in the policy space). A Gaussian process is used to obtain a smooth interpolation over the reward function weights of the optimal value function for three well-known examples: Gridworld, Objectworld and Pendulum. The results show that the interpolation can provide robust values for sample states and actions in both discrete and continuous domain problems. Significant advantages arise from utilizing this interpolation technique in the domain of autonomous vehicles: easy, instant adaptation of user preferences while driving and true randomization of obstacle vehicle behavior preferences during training.


Title: Integrated moment-based LGMD and deep reinforcement learning for UAV obstacle avoidance
Key Words: autonomous aerial vehicles  collision avoidance  control engineering computing  image motion analysis  image sequences  learning (artificial intelligence)  mobile robots  neural nets  object detection  robot vision  SLAM (robots)  visual perception  deep reinforcement learning  UAV obstacle avoidance  learning-based reaction local planner  microUAVs  image moment  illuminance variation  mapless navigation  moment-based LGMD  bioinspired monocular vision perception method  Navigation  Collision avoidance  Robustness  Lighting  Robots  Optical imaging  Machine learning 
Abstract: In this paper, a bio-inspired monocular vision perception method combined with a learning-based reaction local planner for obstacle avoidance of micro UAVs is presented. The system is more computationally efficient than other vision-based perception and navigation methods such as SLAM and optical flow because it does not need to calculate accurate distances. To improve the robustness of perception against illuminance change, the input image is remapped using image moment which is independent of illuminance variation. After perception, a local planner is trained using deep reinforcement learning for mapless navigation. The proposed perception and navigation methods are evaluated in some realistic simulation environments. The result shows that this light-weight monocular perception and navigation system works well in different complex environments without accurate depth information.


Title: Monitoring Over the Long Term: Intermittent Deployment and Sensing Strategies for Multi-Robot Teams
Key Words: combinatorial mathematics  Gaussian processes  greedy algorithms  matrix algebra  Monte Carlo methods  multi-robot systems  optimisation  multirobot team  intermittent deployment problem  heterogeneous robots  environmental process  spatiotemporal process  intermittent deployment strategy  spatiotemporal Gaussian process  Monte Carlo simulations  greedy algorithm  submodular optimization  matroids  Robot sensing systems  Monitoring  Mutual information  Spatiotemporal phenomena  Kernel 
Abstract: In this paper, we formulate and solve the intermittent deployment problem, which yields strategies that couple when heterogeneous robots should sense an environmental process, with where a deployed team should sense in the environment. As a motivation, suppose that a spatiotemporal process is slowly evolving and must be monitored by a multi-robot team, e.g., unmanned aerial vehicles monitoring pasturelands in a precision agriculture context. In such a case, an intermittent deployment strategy is necessary as persistent deployment or monitoring is not cost-efficient for a slowly evolving process. At the same time, the problem of where to sense once deployed must be solved as process observations yield useful feedback for determining effective future deployment and monitoring decisions. In this context, we model the environmental process to be monitored as a spatiotemporal Gaussian process with mutual information as a criterion to measure our understanding of the environment. To make the sensing resource-efficient, we demonstrate how to use matroid constraints to impose a diverse set of homogeneous and heterogeneous constraints. In addition, to reflect the cost-sensitive nature of real-world applications, we apply budgets on the cost of deployed heterogeneous robot teams. To solve the resulting problem, we exploit the theories of submodular optimization and matroids and present a greedy algorithm with bounds on sub-optimality. Finally, Monte Carlo simulations demonstrate the correctness of the proposed method.


Title: LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery
Key Words: image matching  image resolution  optical radar  stereo image processing  dense depth maps  depth information  light detection and ranging  accurate depth map  stereo imagery  stereo systems  high-quality dense depth maps  stereo matching algorithms  sparse depth map  high-resolution LIDAR  Laser radar  Task analysis  Training  Feature extraction  Estimation  Convolution  Correlation 
Abstract: An accurate depth map of the environment is critical to the safe operation of autonomous robots and vehicles. Currently, either light detection and ranging (LIDAR) or stereo matching algorithms are used to acquire such depth information. However, a high-resolution LIDAR is expensive and produces sparse depth map at large range; stereo matching algorithms are able to generate denser depth maps but are typically less accurate than LIDAR at long range. This paper combines these approaches together to generate high-quality dense depth maps. Unlike previous approaches that are trained using ground-truth labels, the proposed model adopts a self-supervised training process. Experiments show that the proposed method is able to generate high-quality dense depth maps and performs robustly even with low-resolution inputs. This shows the potential to reduce the cost by using LIDARs with lower resolution in concert with stereo systems while maintaining high resolution.


Title: Versatile Trajectory Optimization Using a LCP Wheel Model for Dynamic Vehicle Maneuvers
Key Words: automobiles  friction  mechanical contact  mobile robots  optimisation  path planning  robot dynamics  trajectory control  tyres  vehicle dynamics  wheels  dynamic drift parking  discontinuous friction model  tire dynamics model  cost function  wheel skidding  versatile trajectory optimization framework  vehicle motion  anisotropic Coulomb friction cone  multirigid-body contact problems  linear complementarity problem  robotics community  contact dynamics  real world contact behavior  empirical friction model  aggressive maneuvers  car models  dynamic vehicle maneuvers  LCP wheel model  executing dynamic drift parking  planning horizon  Vehicle dynamics  Wheels  Friction  Planning  Dynamics  Trajectory optimization  Tires 
Abstract: Car models have been extensively studied at varying levels of abstraction, and planning and executing motions under ideal conditions is well researched and understood. For more aggressive maneuvers, for example when drifting or skidding, empirical and/or discontinuous friction models have been used to explain and approximate real world contact behavior. Separately, contact dynamics have been extensively studied by the robotics community, often times formulated as a linear complementarity problem (LCP) for dynamic multi-rigid-body contact problems with Coulomb friction cone approximations. In this work, we explore the validity of using such an anisotropic Coulomb friction cone to model tire dynamics to plan for vehicle motion, and present a versatile trajectory optimization framework using this model that can both avoid and/or exploit wheel skidding, depending on the cost function and planning horizon. Experimental evidence of planning and executing dynamic drift parking is shown on a 1/16 scale model car.


Title: ModQuad-DoF: A Novel Yaw Actuation for Modular Quadrotors
Key Words: actuators  aerospace robotics  attitude control  autonomous aerial vehicles  drag  helicopters  mobile robots  motion control  path planning  position control  vehicle dynamics  ModQuad-DoF  modular quadrotors  robotic structure  enhanced capabilities  module design  freedom relative motion  flying robot  cage  docking mechanism  structure control authority  structure yaw control  yaw actuation method 
Abstract: In this work we introduce ModQuad-DoF, a modular flying robotic structure with enhanced capabilities for yaw actuation. We propose a new module design that allows a one degree of freedom relative motion between the flying robot and the cage, with a docking mechanism allowing rigid connections between cages. A novel method of yaw actuation that increases the structure control authority is also presented. Our new method for the structure yaw control relies on the independent roll angles of each one of the modules, instead of the traditional drag moments from the propellers. In this paper, we propose a controller that allows the ModQuad-DoF to control its position and attitude. In our experiments, we tested a different number of modules flying in cooperation and validated the novel yaw actuation method.


Title: Quantifying Good Seamanship For Autonomous Surface Vessel Performance Evaluation
Key Words: collision avoidance  decision making  marine safety  marine vehicles  mobile robots  remotely operated vehicles  ships  performance metrics  ASV decision-making  collision risk  ASV planning strategies  International Regulations for Prevention of Collisions at Sea  quantified good seamanship  COLREGS compliance  vessel interactions  autonomous surface vehicle decision-making  autonomous surface vessel performance evaluation  seamanship performance criteria  Marine vehicles  Safety  Geometry  Decision making  Navigation  Risk management  Planning 
Abstract: The current state-of-the-art for testing and evaluation of autonomous surface vehicle (ASV) decision-making is currently limited to one-versus-one vessel interactions by determining compliance with the International Regulations for Prevention of Collisions at Sea, referred to as COLREGS. Strict measurement of COLREGS compliance, however, loses value in multi-vessel encounters, as there can be conflicting rules which make determining compliance extremely subjective. This work proposes several performance metrics to evaluate ASV decision-making based on the concept of "good seamanship," a practice which generalizes to multi-vessel encounters. Methodology for quantifying good seamanship is presented based on the criteria of reducing the overall collision risk of the situation and taking early, appropriate actions. Case study simulation results are presented to showcase the seamanship performance criteria against different ASV planning strategies.


Title: Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph Convolutional Networks
Key Words: computer vision  convolutional neural nets  driver information systems  feature extraction  interactive systems  learning (artificial intelligence)  road traffic  road vehicles  graph convolutional networks  intelligent automated driving systems  complicated driving situations  spatial-temporal interaction framework  graph convolution networks  GCN  interaction modeling  ego-stuff interaction  Honda research institute driving dataset  3D-aware egocentric spatial-temporal interaction learning  tactical driver behavior annotations  ego-thing interactions  feature extraction  Feature extraction  Three-dimensional displays  Roads  Hidden Markov models  Vehicles  Two dimensional displays  Convolution 
Abstract: To enable intelligent automated driving systems, a promising strategy is to understand how human drives and interacts with road users in complicated driving situations. In this paper, we propose a 3D-aware egocentric spatial-temporal interaction framework for automated driving applications. Graph convolution networks (GCN) is devised for interaction modeling. We introduce three novel concepts into GCN. First, we decompose egocentric interactions into ego-thing and ego- stuff interaction, modeled by two GCNs. In both GCNs, ego nodes are introduced to encode the interaction between thing objects (e.g., car and pedestrian), and interaction between stuff objects (e.g., lane marking and traffic light). Second, objects' 3D locations are explicitly incorporated into GCN to better model egocentric interactions. Third, to implement ego-stuff interaction in GCN, we propose a MaskAlign operation to extract features for irregular objects.We validate the proposed framework on tactical driver behavior recognition. Extensive experiments are conducted using Honda Research Institute Driving Dataset, the largest dataset with diverse tactical driver behavior annotations. Our framework demonstrates substantial performance boost over baselines on the two experimental settings by 3.9% and 6.0%, respectively. Furthermore, we visualize the learned affinity matrices, which encode ego-thing and ego-stuff interactions, to showcase the proposed framework can capture interactions effectively.


Title: Minimal 3D Dubins Path with Bounded Curvature and Pitch Angle
Key Words: optimisation  path planning  vehicles  minimal 3D Dubins path  bounded curvature  pitch angle  cost-efficient three-dimensional paths  two-dimensional Dubins curves  closed-form solutions  local optimization  cost-efficient solution  lower bound estimation  optimal path  vehicle  Three-dimensional displays  Two dimensional displays  Turning  Path planning  Atmospheric modeling  Space vehicles  Optimization 
Abstract: In this paper, we address the problem of finding cost-efficient three-dimensional paths that satisfy the maximum allowed curvature and the pitch angle of the vehicle. For any given initial and final configurations, the problem is decoupled into finding the horizontal and vertical parts of the path separately. Although the individual paths are modeled as two-dimensional Dubins curves using closed-form solutions, the final 3D path is constructed using the proposed local optimization to find a cost-efficient solution. Moreover, based on the decoupled approach, we provide a lower bound estimation of the optimal path that enables us to determine the quality of the found heuristic solution. The proposed solution has been evaluated using existing benchmark instances and compared with state-of-the-art approaches. Based on the reported results and lower bounds, the proposed approach provides paths close to the optimal solution while the computational requirements are in hundreds of microseconds. Besides, the proposed method provides paths with fewer turns than others, which make them easier to be followed by the vehicle's controller.


Title: AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
Key Words: autonomous aerial vehicles  cameras  computer vision  image annotation  image capture  image colour analysis  object detection  video signal processing  video surveillance  multimodal unmanned aerial vehicle dataset  low altitude traffic surveillance  UAVs  mounted cameras  aerial image capture  aerial visual data  object detection algorithms  computer vision community  object annotations  flying-cameras  multipurpose aerial dataset  multimodal sensor data  AU-AIR dataset  meta-data  traffic-related object category  mobile object detectors  real-time object detection  robotics  real-world outdoor environments  bounding box annotation  RGB videos recording  YOLOv3-Tiny  MobileNetv2-SSDLite  on-board computers  bird-view image  data types recording  Object detection  Videos  Visualization  Cameras  Detectors  Surveillance 
Abstract: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images. As a result of this, several aerial datasets have been introduced, including visual data with object annotations. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors). In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for traffic-related object category) from recorded RGB videos. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics. The dataset is available at https://bozcani.github.io/auairdataset.


Title: Design and Autonomous Stabilization of a Ballistically-Launched Multirotor
Key Words: aerodynamics  autonomous aerial vehicles  ballistics  helicopters  mobile robots  robot vision  aircraft  drones  emergency response  space exploration  critical situational data  onboard sensors  multirotor prototype  onboard sensor suite  autonomy pipeline  aerodynamic stability  active stabilization  ballistic launch  streamlined quick unfolding investigation drone  vision-based autonomous transition  SQUID  SQUIDs  Electron tubes  Aerodynamics  Drones  Prototypes  Thermal stability  Aircraft 
Abstract: Aircraft that can launch ballistically and convert to autonomous, free-flying drones have applications in many areas such as emergency response, defense, and space exploration, where they can gather critical situational data using onboard sensors. This paper presents a ballistically-launched, autonomously-stabilizing multirotor prototype (SQUID - Streamlined Quick Unfolding Investigation Drone) with an onboard sensor suite, autonomy pipeline, and passive aerodynamic stability. We demonstrate autonomous transition from passive to vision-based, active stabilization, confirming the multirotor's ability to autonomously stabilize after a ballistic launch in a GPS-denied environment.


Title: Asynchronous event-based clustering and tracking for intrusion monitoring in UAS
Key Words: autonomous aerial vehicles  cameras  image sensors  object tracking  pattern clustering  robot vision  surveillance  feature tracking  intrusion monitoring  UAS  unmanned aerial systems  perception systems  illumination conditions  event cameras  neuromorphic sensors  illumination changes  event based vision  event stream  intruder monitoring  event clustering  event-by-event processing  asynchronous event-based clustering  automatic surveillance  on-board hardware computational constraints  Cameras  Robot vision systems  Tracking  Surveillance  Robustness  event camera  asynchronous  intrusion monitoring  surveillance  UAS  clustering  feature tracking 
Abstract: Automatic surveillance and monitoring using Unmanned Aerial Systems (UAS) require the development of perception systems that robustly work under different illumination conditions. Event cameras are neuromorphic sensors that capture the illumination changes in the scene with very low latency and high dynamic range. Although recent advances in eventbased vision have explored the use of event cameras onboard UAS, most techniques group events in frames and, therefore, do not fully exploit the sequential and asynchronous nature of the event stream. This paper proposes a fully asynchronous scheme for intruder monitoring using UAS. It employs efficient event clustering and feature tracking modules and includes a sampling mechanism to cope with the computational cost of event-by-event processing adapting to on-board hardware computational constraints. The proposed scheme was tested on a real multirotor in challenging scenarios showing significant accuracy and robustness to lighting conditions.


Title: SHIFT: Selective Heading Image for Translation An onboard monocular optical flow estimator for fast constantly rotating UAVs
Key Words: aerospace control  aircraft control  autonomous aerial vehicles  cameras  image sensors  image sequences  Kalman filters  motion estimation  nonlinear filters  pose estimation  remotely operated vehicles  onboard monocular optical flow estimator  UAV  pose estimation  flight control  unmanned aerial vehicles  autonomous operations  onboard sensors  monocular camera  free rotors  flight dynamics  falling samara seed  constantly rotating body frame  optical flow sensing  optimal images  rotation vectors  optical axis  translation vectors  flow field  SHIFT estimation  selective heading image for translation  Cameras  Optical imaging  Optical sensors  Adaptive optics  Optical filters  Tracking  Integrated optics 
Abstract: Pose estimation is of paramount importance for flight control as well as localization and navigation of Unmanned Aerial Vehicles (UAVs) to enable autonomous operations. In environments without GPS, such estimation can only be determined using onboard sensors; optical flow using a monocular camera is a popular approach. Monocopters are a class of nature inspired UAVs known as free rotors where their design and flight dynamics are inspired by the falling samara seed. With a constantly rotating body frame, free rotors introduces some unique challenges for visual perception required during optical flow sensing. This paper addresses these problems with the introduction of SHIFT (Selective Heading Image for Translation) that selects optimal images for determining translation with optical flow. It achieves this by decoupling rotation vectors about the optical axis from translation vectors in a flow field through the separate tracking of orientation and position using an Unscented Kalman Filter with phase correlation in the log-polar and spatial domain. The experiments show that SHIFT's estimation in orientation is stable even under sinusoidal excitation with a median absolute percentage errors of less than 1%. It is able to track position and orientation of a UAV accurately.


Title: Nonlinear MPC with Motor Failure Identification and Recovery for Safe and Aggressive Multicopter Flight
Key Words: actuators  autonomous aerial vehicles  helicopters  Kalman filters  nonlinear control systems  nonlinear filters  predictive control  precise reference tracking  crucial characteristic  microaerial vehicles  MAV  external disturbances  cluttered environments  nonlinear model predictive control  NMPC  fully physics  nonlinear dynamics  control inputs  feasible actuator commands  safe operation  potential loss  flight experiments  motor failures  nonlinear MPC  aggressive multicopter flight  extended Kalman filter based motor failure identification algorithm  Actuators  Resource management  Propellers  Aerodynamics  Quaternions  Angular velocity  Vehicle dynamics 
Abstract: Safe and precise reference tracking is a crucial characteristic of Micro Aerial Vehicles (MAVs) that have to operate under the influence of external disturbances in cluttered environments. In this paper, we present a Nonlinear Model Predictive Control (NMPC) that exploits the fully physics based non-linear dynamics of the system. We furthermore show how the moment and thrust control inputs can be transformed into feasible actuator commands. In order to guarantee safe operation despite potential loss of a motor under which we show our system keeps operating safely, we developed an Extended Kalman Filter (EKF) based motor failure identification algorithm. We verify the effectiveness of the developed pipeline in flight experiments with and without motor failures.


Title: Using multiple short hops for multicopter navigation with only inertial sensors
Key Words: accelerometers  closed loop systems  gyroscopes  helicopters  inertial navigation  Kalman filters  nonlinear filters  position control  state estimation  closed-loop control  mean absolute position estimation error  total flight distance  standard inertial navigation method  trajectory tracking error  multiple short hops  multicopter navigation  GPS systems  multicopter localization  direct integration  inertial navigation sensors  accelerometer  rate gyroscope  rapid error accumulation  motion strategy  inertial navigation state estimation error  long duration flight  multiple short duration hops  zero-velocity pseudomeasurements  extended Kalman filter  LiDAR  real-world environment  distance 5.0 m  distance 10.0 m  State estimation  Gyroscopes  Sensors  Inertial navigation  Accelerometers  Measurement uncertainty 
Abstract: In certain challenging environments, such as inside buildings on fire, the main sensors (e.g. cameras, LiDARs and GPS systems) used for multicopter localization can become unavailable. Direct integration of the inertial navigation sensors (the accelerometer and rate gyroscope), is however unaffected by external disturbances, but the rapid error accumulation quickly makes a naive application of such a strategy feasible only for very short durations. In this work we propose a motion strategy for reducing the inertial navigation state estimation error of multicopters. The proposed strategy breaks a long duration flight into multiple short duration hops between which the vehicle remains stationary on the ground. When the vehicle is stationary, zero-velocity pseudo-measurements are introduced to an extended Kalman Filter to reduce the state estimation error. We perform experiments for closed-loop control of a multicopter for evaluation. The mean absolute position estimation error was 3.4% over a total flight distance of 5m in the experiments. The results showed a 80% reduction compared to the standard inertial navigation method without using this strategy. In addition, an additional experiment with total flight distance of 10m is conducted to demonstrate the ability of this method to navigate a multicopter in real-world environment. The final trajectory tracking error was 3% of the total flight distance.


Title: A Feature-Based Underwater Path Planning Approach using Multiple Perspective Prior Maps
Key Words: autonomous underwater vehicles  bathymetry  maximum likelihood estimation  mobile robots  navigation  path planning  remotely operated vehicles  multiple perspective prior maps  path planning methodology  Autonomous Underwater Vehicles  AUV  shallow complex environments  coral reefs  aerial photographic survey  bathymetric information  prior map  navigation graph  test points  shortest paths  destination points  maximum likelihood function  misclassified objects  photo-realistic simulated environment  Navigation  Cameras  Sensors  Uncertainty  Image segmentation  Feature extraction  Robots 
Abstract: This paper presents a path planning methodology which enables Autonomous Underwater Vehicles (AUVs) to navigate in shallow complex environments such as coral reefs. The approach leverages prior information from an aerial photographic survey, and derived bathymetric information of the corresponding area. From these prior maps, a set of features is obtained which define an expected arrangement of objects and bathymetry likely to be perceived by the AUV when underwater. A navigation graph is then constructed by predicting the arrangement of features visible from a set of test points within the prior, which allows the calculation of the shortest paths from any pair of start and destination points. A maximum likelihood function is defined which allows the AUV to match its observations to the navigation graph as it undertakes its mission. To improve robustness, the history of observed features are retained to facilitate possible recovery from non-detectable or misclassified objects. The approach is evaluated using a photo-realistic simulated environment, and results illustrate the merits of the approach even when only a relatively small number of features can be identified from the prior map.


Title: Motion Estimation in Occupancy Grid Maps in Stationary Settings Using Recurrent Neural Networks
Key Words: image filtering  image motion analysis  motion estimation  optical radar  path planning  probability  radar imaging  recurrent neural nets  traffic engineering computing  occupancy grid maps  recurrent neural networks  grid cell  occupancy probability  measurement grid maps  occupancy probabilities  filtered occupancy  network architecture  Computer architecture  Microprocessors  Vehicle dynamics  Measurement by laser beam  Time measurement  Recurrent neural networks  Dynamics 
Abstract: In this work, we tackle the problem of modeling the vehicle environment as dynamic occupancy grid map in complex urban scenarios using recurrent neural networks. Dynamic occupancy grid maps represent the scene in a bird's eye view, where each grid cell contains the occupancy prob-ability and the two dimensional velocity. As input data, our approach relies on measurement grid maps, which contain occupancy probabilities, generated with lidar measurements. Given this configuration, we propose a recurrent neural net-work architecture to predict a dynamic occupancy grid map, i.e. filtered occupancy and velocity of each cell, by using a sequence of measurement grid maps. Our network architecture contains convolutional long-short term memories in order to sequentially process the input, makes use of spatial context, and captures motion. In the evaluation, we quantify improvements in estimating the velocity of braking and turning vehicles compared to the state-of-the-art. Additionally, we demonstrate that our approach provides more consistent velocity estimates for dynamic objects, as well as, less erroneous velocity estimates in static area.


Title: A Divide and Conquer Method for 3D Registration of Inhomogeneous, Partially Overlapping Scans with Fourier Mellin SOFT (FMS)
Key Words: divide and conquer methods  image registration  laser ranging  high-end laser range-finders  divide-and-conquer method  3D registration method  Fourier-Mellin-SOFT  FMS  partial overlaps  large-scale cultural heritage site  Three-dimensional displays  Frequency modulation  Robustness  Underwater vehicles  Two dimensional displays  Cultural differences  Nonhomogeneous media 
Abstract: High-end laser range-finders provide accurate 3D data over long ranges. But their scans are inhomogeneous, i.e., the environment is non-uniformly sampled, as there is denser data in the near range than in the far range. Furthermore, the generation of a scan is time-consuming. Thus, it is desirable to cover an area by as few scans as possible, i.e., scanning is more time-efficient if the overlap between scans is as small as possible. However, these factors pose significant challenges for state-of-the-art registration algorithms. In this work, we present a divide-and-conquer method that uses an efficient strategy to check for possible registrations between partitions of two scans. As underlying registration method, Fourier-Mellin-SOFT (FMS) is used. FMS is quite robust against partial overlaps but its performance is significantly boosted by the presented partitioning method. As concrete use case, results from the digitization of a WWII submarine bunker as a large-scale cultural heritage site are presented.


Title: Least-squares Optimal Relative Planar Motion for Vehicle-mounted Cameras
Key Words: calibration  computer vision  image motion analysis  least squares approximations  optimisation  polynomials  closed-form solver  point correspondences  camera movement  motion parameters  vehicle-mounted cameras  least-squares optimal relative planar motion  6th degree polynomial  Cameras  Transmission line matrix methods  Robot vision systems  Estimation  Automobiles  Geometry 
Abstract: A new closed-form solver is proposed minimizing the algebraic error optimally, in the least squares sense, to estimate the relative planar motion of two calibrated cameras. The main objective is to solve the over-determined case, i.e., when a larger-than-minimal sample of point correspondences is given - thus, estimating the motion from at least three correspondences. The algorithm requires the camera movement to be constrained to a plane, e.g. mounted to a vehicle, and the image plane to be orthogonal to the ground.1 The solver obtains the motion parameters as the roots of a 6th degree polynomial. It is validated both in synthetic experiments and on publicly available real-world datasets that using the proposed solver leads to results superior to the state-of-the-art in terms of geometric accuracy with no noticeable deterioration in the processing time.


Title: Relative planar motion for vehicle-mounted cameras from a single affine correspondence
Key Words: calibration  cameras  computer vision  image motion analysis  image sensors  relative planar motion  vehicle-mounted cameras  single affine correspondence  extrinsic camera parameters  general planar motion  camera movement  image plane  minimal solver  semicalibrated case  common focal length  fully calibrated case  Cameras  Transmission line matrix methods  Mathematical model  Estimation  Geometry  Robot vision systems  Robustness 
Abstract: Two solvers are proposed for estimating the extrinsic camera parameters from a single affine correspondence assuming general planar motion. In this case, the camera movement is constrained to a plane and the image plane is orthogonal to the ground. The algorithms do not assume other constraints, e.g. the non-holonomic one, to hold. A new minimal solver is proposed for the semi-calibrated case, i.e. the camera parameters are known except a common focal length. Another method is proposed for the fully calibrated case. Due to requiring a single correspondence, robust estimation, e.g. histogram voting, leads to a fast and accurate procedure. The proposed methods are tested in our synthetic environment and on publicly available real datasets consisting of videos through tens of kilometers. They are superior to the state-of-the-art both in terms of accuracy and processing time.


Title: Decentralized Visual-Inertial-UWB Fusion for Relative State Estimation of Aerial Swarm
Key Words: autonomous aerial vehicles  decentralised control  mobile robots  robot vision  state estimation  decentralized visual-inertial-UWB fusion  unmanned aerial vehicles  multiple UAVs  visual-inertial-UWB fusion framework  extensive aerial swarm flight experiments  motion capture system  vision based method  Global Positioning System  estimation consistency  relative state estimation framework  aerial swarm applications  decentralized relative state estimation method  Drones  State estimation  Cameras  Sensors  Global Positioning System  Real-time systems 
Abstract: The collaboration of unmanned aerial vehicles (UAVs) has become a popular research topic for its practicability in multiple scenarios. The collaboration of multiple UAVs, which is also known as aerial swarm is a highly complex system, which still lacks a state-of-art decentralized relative state estimation method. In this paper, we present a novel fully decentralized visual-inertial-UWB fusion framework for relative state estimation and demonstrate the practicability by performing extensive aerial swarm flight experiments. The comparison result with ground truth data from the motion capture system shows the centimeter-level precision which outperforms all the Ultra-WideBand (UWB) and even vision based method. The system is not limited by the field of view (FoV) of the camera or Global Positioning System (GPS), meanwhile on account of its estimation consistency, we believe that the proposed relative state estimation framework has the potential to be prevalently adopted by aerial swarm applications in different scenarios in multiple scales.


Title: Anti-Jackknifing Control of Tractor-Trailer Vehicles via Intrinsically Stable MPC
Key Words: feedback  linearisation techniques  optimal control  predictive control  road vehicles  stability  trajectory control  corrective term  tracking term  input-output linearization  nonminimum-phase systems  IS-MPC  antijackknifing control  feedback control law  reference Cartesian trajectory  trailer hitch angle  tractor-trailer vehicles  intrinsically stable MPC scheme  Trajectory  Agricultural machinery  Computational modeling  Vehicle dynamics  Tracking  Dynamics  Linear approximation 
Abstract: It is common knowledge that tractor-trailer vehicles are affected by jackknifing, a phenomenon that consists in the divergence of the trailer hitch angle and ultimately causes the vehicle to fold up. For the case of backwards motion, in which jackknifing can also occur at low speeds, we present a control method that drives the vehicle along a reference Cartesian trajectory while avoiding the divergence of the hitch angle. In particular, a feedback control law is obtained by combining two actions: a tracking term, computed using input-output linearization, and a corrective term, generated via IS-MPC, an intrinsically stable MPC scheme which is effective for stable inversion of nonminimum-phase systems. The proposed method has been verified in simulation and experimentally validated on a purposely built prototype.


Title: On sensing-aware model predictive path-following control for a reversing general 2-trailer with a car-like tractor
Key Words: control system synthesis  mobile robots  motion control  path planning  position control  predictive control  road traffic control  road vehicles  stability  vehicle dynamics  sensing-aware model predictive path  car-like tractor  controller-design problem  joint-angle kinematics  backward motion  vehicle segments  jackknife state  joint-angle estimation problem  path-following controller  Agricultural machinery  Axles  Sensors  Kinematics  Reliability  Estimation  Trajectory 
Abstract: The design of reliable path-following controllers is a key ingredient for successful deployment of self-driving vehicles. This controller-design problem is especially challenging for a general 2-trailer with a car-like tractor due to the vehicle's structurally unstable joint-angle kinematics in backward motion and the car-like tractor's curvature limitations which can cause the vehicle segments to fold and enter a jackknife state. Furthermore, advanced sensors with a limited field of view have been proposed to solve the joint-angle estimation problem online, which introduce additional restrictions on which vehicle states that can be reliably estimated. To incorporate these restrictions at the level of control, a model predictive path-following controller is proposed. By taking the vehicle's physical and sensing limitations into account, it is shown in real-world experiments that the performance of the proposed path-following controller in terms of suppressing disturbances and recovering from non-trivial initial states is significantly improved compared to a previously proposed solution where the constraints have been neglected.


Title: LINS: A Lidar-Inertial State Estimator for Robust and Efficient Navigation
Key Words: distance measurement  inertial navigation  iterative methods  Kalman filters  motion estimation  optical radar  state estimation  ground vehicles  6-axis IMU  iterated error-state Kalman filter  feature correspondences  filter divergence  LINS  state-of-the-art lidar-inertial odometry  lightweight lidar-inertial state estimator  real-time ego-motion estimation  robust navigation  3D lidar  robocentric formulation  Feature extraction  Laser radar  Three-dimensional displays  Kalman filters  Real-time systems  Optimization  Navigation 
Abstract: We present LINS, a lightweight lidar-inertial state estimator, for real-time ego-motion estimation. The proposed method enables robust and efficient navigation for ground vehicles in challenging environments, such as feature-less scenes, via fusing a 6-axis IMU and a 3D lidar in a tightly-coupled scheme. An iterated error-state Kalman filter (ESKF) is designed to correct the estimated state recursively by generating new feature correspondences in each iteration, and to keep the system computationally tractable. Moreover, we use a robocentric formulation that represents the state in a moving local frame in order to prevent filter divergence in a long run. To validate robustness and generalizability, extensive experiments are performed in various scenarios. Experimental results indicate that LINS offers comparable performance with the state-of-the-art lidar-inertial odometry in terms of stability and accuracy and has order-of-magnitude improvement in speed.


Title: Interaction Graphs for Object Importance Estimation in On-road Driving Videos
Key Words: decision making  driver information systems  graph theory  learning (artificial intelligence)  road traffic  road vehicles  traffic engineering computing  interaction graph  object importance estimation  on-road driving videos  human driving behavior  autonomous driving systems  ego-vehicle  Feature extraction  Automobiles  Videos  Convolution  Estimation  Visualization 
Abstract: A vehicle driving along the road is surrounded by many objects, but only a small subset of them influence the driver's decisions and actions. Learning to estimate the importance of each object on the driver's real-time decision-making may help better understand human driving behavior and lead to more reliable autonomous driving systems. Solving this problem requires models that understand the interactions between the ego-vehicle and the surrounding objects. However, interactions among other objects in the scene can potentially also be very helpful, e.g., a pedestrian beginning to cross the road between the ego-vehicle and the car in front will make the car in front less important. We propose a novel framework for object importance estimation using an interaction graph, in which the features of each object node are updated by interacting with others through graph convolution. Experiments show that our model outperforms state-of-the-art baselines with much less input and pre-processing.


Title: Whole-Body Bilateral Teleoperation of a Redundant Aerial Manipulator
Key Words: autonomous aerial vehicles  delays  end effectors  force feedback  haptic interfaces  manipulator dynamics  mobile robots  position control  redundant manipulators  robot vision  telerobotics  video cameras  redundant aerial manipulator  robotic manipulator  flying base  reachability  manipulation task  human capabilities  telemanipulation tasks  visual feedback  task-dependent  video camera  end-effector motion  base position  stable bilateral teleoperation  time-delayed telemanipulation  whole-body bilateral teleoperation  null-space wall  haptic concept  kinematic structure  task-dependent optimal pose  Task analysis  Manipulators  Haptic interfaces  Cameras  Null space  Robot vision systems 
Abstract: Attaching a robotic manipulator to a flying base allows for significant improvements in the reachability and versatility of manipulation tasks. In order to explore such systems while taking advantage of human capabilities in terms of perception and cognition, bilateral teleoperation arises as a reasonable solution. However, since most telemanipulation tasks require visual feedback in addition to the haptic one, real-time (task-dependent) positioning of a video camera, which is usually attached to the flying base, becomes an additional objective to be fulfilled. Since the flying base is part of the kinematic structure of the robot, if proper care is not taken, moving the video camera could undesirably disturb the end-effector motion. For that reason, the necessity of controlling the base position in the null space of the manipulation task arises. In order to provide the operator with meaningful information about the limits of the allowed motions in the null space, this paper presents a novel haptic concept called Null-Space Wall. In addition, a framework to allow stable bilateral teleoperation of both tasks is presented. Numerical simulation data confirm that the proposed framework is able to keep the system passive while allowing the operator to perform time-delayed telemanipulation and command the base to a task-dependent optimal pose.


Title: Enhanced Teleoperation Using Autocomplete
Key Words: autonomous aerial vehicles  learning (artificial intelligence)  mobile robots  telerobotics  enhanced teleoperation  Autocomplete  remote location  skilled teleoperators  training time  novice teleoperators  human input  desired motion  machine learning  motion primitives  unmanned aerial vehicle  Task analysis  Trajectory  Robots  Training  Support vector machines  Manuals  Drones 
Abstract: Controlling and manning robots from a remote location is difficult because of the limitations one faces in perception and available degrees of actuation. Although humans can become skilled teleoperators, the amount of training time required to acquire such skills is typically very high. In this paper, we propose a novel solution (named Autocomplete) to aid novice teleoperators in manning robots adroitly. At the input side, Autocomplete relies on machine learning to detect and categorize human inputs as one from a group of motion primitives. Once a desired motion is recognized, at the actuation side an automated command replaces the human input in performing the desired action. So far, Autocomplete can recognize and synthesize lines, arcs, full circles, 3-D helices, and sine trajectories. Autocomplete was tested in simulation on the teleoperation of an unmanned aerial vehicle, and results demonstrate the advantages of the proposed solution versus manual steering.


Title: Strategy for automated dense parking: how to navigate in narrow lanes*
Key Words: automobiles  Global Positioning System  Lyapunov methods  mobile robots  motion control  navigation  path planning  road traffic control  narrow lanes  high- density parking solution  car-like robots  hard constraints  robot motion  robot localization  navigation  configuration space formulation  Stanley Robotics robots  automated dense parking  Lyapunov- based control strategy  GPS orientation  Robots  Collision avoidance  Aerospace electronics  Navigation  Mathematical model  Kinematics  Automobiles 
Abstract: This paper presents the architecture of a high- density parking solution based on car-like robots specifically designed to move cars. The main difficulty is to park the vehicles close to one another which implies hard constraints on the robot motion and localization. In particular, this paper focuses on navigation in narrow lanes. We propose a Lyapunov- based control strategy that has been derived after expressing the problem in a Configuration Space formulation. The current solution has been implemented and tested on Stanley Robotics' robots and has been running in production for several months. Thanks to the Configuration Space formulation, we are able to guarantee the obstacles' integrity. Moreover, a method for calibrating the GPS orientation with a high-precision is derived from the present control strategy.


Title: Multimodal Trajectory Predictions for Urban Environments Using Geometric Relationships between a Vehicle and Lanes
Key Words: feature extraction  object detection  road traffic  road vehicles  traffic engineering computing  autonomous driving systems  traffic behavior  urban environments  road geometries  lane-based multimodal prediction network  arbitrary shapes  traffic lanes  future trajectory  lane geometry  lane feature  generalized geometric relationships  vehicle state  vehicle motion model constraint  prediction method  multimodal trajectory predictions  safe driving systems  LAMP-Net  Trajectory  Predictive models  Hidden Markov models  Acceleration  Geometry  Shape  Urban areas 
Abstract: Implementation of safe and efficient autonomous driving systems requires accurate prediction of the long-term trajectories of surrounding vehicles. High uncertainty in traffic behavior makes it difficult to predict trajectories in urban environments, which have various road geometries. To over-come this problem, we propose a method called lane-based multimodal prediction network (LAMP-Net), which can handle arbitrary shapes and numbers of traffic lanes and predict both the future trajectory along each lane and the probability of each lane being selected. A vector map is used to define the lane geometry and a novel lane feature is introduced to represent the generalized geometric relationships between the vehicle state and lanes. Our network takes this feature as the input and is trained to be versatile for arbitrarily shaped lanes. Moreover, we introduce a vehicle motion model constraint to our network. Our prediction method combined with the constraint significantly enhances prediction accuracy. We evaluate the prediction performance on two datasets which contain a wide variety of real-world traffic scenarios. Experimental results show that our proposed LAMP-Net outperforms state-of-the-art methods.


Title: Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing
Key Words: aircraft control  helicopters  learning (artificial intelligence)  nonlinear control systems  nonlinear dynamical systems  optimal control  predictive control  robot dynamics  model predictive control  nonlinear diffeomorphism  nonlinear dynamical systems  optimal control  multirotor landing  nonlinear robot dynamics  episodic Koopman learning  Eigenvalues and eigenfunctions  Nonlinear dynamical systems  Robots  Aerospace electronics  Heuristic algorithms  Vehicle dynamics 
Abstract: This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.


Title: Towards Adaptive Benthic Habitat Mapping
Key Words: bathymetry  geophysical image processing  neural nets  oceanographic techniques  remotely operated vehicles  seafloor phenomena  sonar  underwater vehicles  habitat model  AUV systems  seafloor imagery  efficient AUV surveys  visually-derived habitat classes  broad-scale bathymetric data  fewer samples  benthic surveys  adaptive benthic habitat  autonomous underwater vehicles  benthic habitat mapping  broadscale bathymetric data  remotely-sensed acoustic data  Feature extraction  Uncertainty  Biological system modeling  Bayes methods  Data models  Neural networks  Backscatter 
Abstract: Autonomous Underwater Vehicles (AUVs) are increasingly being used to support scientific research and monitoring studies. One such application is in benthic habitat mapping where these vehicles collect seafloor imagery that complements broadscale bathymetric data collected using sonar. Using these two data sources, the relationship between remotely-sensed acoustic data and the sampled imagery can be learned, creating a habitat model. As the areas to be mapped are often very large and AUV systems collecting seafloor imagery can only sample from a small portion of the survey area, the information gathered should be maximised for each deployment. This paper illustrates how the habitat models themselves can be used to plan more efficient AUV surveys by identifying where to collect further samples in order to most improve the habitat model. A Bayesian neural network is used to predict visually-derived habitat classes when given broad-scale bathymetric data. This network can also estimate the uncertainty associated with a prediction, which can be deconstructed into its aleatoric (data) and epistemic (model) components. We demonstrate how these structured uncertainty estimates can be utilised to improve the model with fewer samples. Such adaptive approaches to benthic surveys have the potential to reduce costs by prioritizing further sampling efforts. We illustrate the effectiveness of the proposed approach using data collected by an AUV on offshore reefs in Tasmania, Australia.


Title: Fault tolerance analysis of a hexarotor with reconfigurable tilted rotors
Key Words: aircraft control  attitude control  fault tolerance  helicopters  fault tolerance analysis  reconfigurable tilted rotor  multirotor vehicles  yaw maneuverability  attitude control  hexarotor vehicle  hexagon-shaped multirotor  altitude control  tilt angle  rotor reconfiguration  Rotors  Torque  Force  Servomotors  Fault tolerance  Fault tolerant systems  Attitude control 
Abstract: Tilted rotors in multirotor vehicles have shown to be useful for different practical reasons. For instance, increasing yaw maneuverability or enabling full position and attitude control of hexarotor vehicles. It has also been proven that a hexagon-shaped multirotor is capable of complete attitude and altitude control under failures of one of its rotors. However, when a rotor fails, the torque that can be reached in the worst- case direction decreases considerably.This work proposes to actively change the tilt angle of the rotors when a failure occurs. This rotor reconfiguration increases the maximum torque that can be achieved in the most stressful direction, reducing maneuverability limitations. Experimental validations are shown, where the proposed reconfigurable tilted rotor is used in order to control a hexarotor vehicle when a failure appears mid-flight. The impact of the delay in the reconfiguration when a failure occurs is also addressed.


Title: Reliability Validation of Learning Enabled Vehicle Tracking
Key Words: image filtering  image motion analysis  Kalman filters  learning (artificial intelligence)  neural nets  object tracking  real-world learning-enabled system  dynamic vehicle tracking  high-resolution wide-area motion imagery input  symbolic components  Kalman filter  neural networks  system-level reliability  coverage-guided neural network testing tool  vehicle tracking system  adversarial examples  deep learning components  validation methods  learning-enabled systems  neural network components  DeepConcolic tool  Testing  Tools  Tracking  Neurons  Cameras  Feature extraction  Reliability 
Abstract: This paper studies the reliability of a real-world learning-enabled system, which conducts dynamic vehicle tracking based on a high-resolution wide-area motion imagery input. The system consists of multiple neural network components - to process the imagery inputs - and multiple symbolic (Kalman filter) components - to analyse the processed information for vehicle tracking. It is known that neural networks suffer from adversarial examples, which make them lack robustness. However, it is unclear if and how the adversarial examples over learning components can affect the overall system-level reliability. By integrating a coverage-guided neural network testing tool, DeepConcolic, with the vehicle tracking system, we found that (1) the overall system can be resilient to some adversarial examples thanks to the existence of other components, and (2) the overall system presents an extra level of uncertainty which cannot be determined by analysing the deep learning components only. This research suggests the need for novel verification and validation methods for learning-enabled systems.


Title: A water-obstacle separation and refinement network for unmanned surface vehicles
Key Words: collision avoidance  edge detection  feature extraction  image fusion  image segmentation  inertial navigation  marine vehicles  mobile robots  neural net architecture  remotely operated vehicles  robot vision  refinement network  unmanned surface vehicles  obstacle detection  water reflections  wakes  inertial information fusion  visual features  deep encoder decoder architecture  water obstacle separation  semantic segmentation  autonomous navigation  water edge estimation  inertial measurement unit  loss function  water features  Decoding  Visualization  Feature extraction  Image segmentation  Semantics  Image edge detection  Task analysis  obstacle detection  semantic segmentation  sensor fusion  unmanned surface vehicles  separation function 
Abstract: Obstacle detection by semantic segmentation shows a great promise for autonomous navigation in unmanned surface vehicles (USV). However, existing methods suffer from poor estimation of the water edge in presence of visual ambiguities, poor detection of small obstacles and high false-positive rate on water reflections and wakes. We propose a new deep encoder-decoder architecture, a water-obstacle separation and refinement network (WaSR), to address these issues. Detection and water edge accuracy are improved by a novel decoder that gradually fuses inertial information from inertial measurement unit (IMU) with the visual features from the encoder. In addition, a novel loss function is designed to increase the separation between water and obstacle features early on in the network. Subsequently, the capacity of the remaining layers in the decoder is better utilised, leading to a significant reduction in false positives and increased true positives. Experimental results show that WaSR outperforms the current state-of-the-art by a large margin, yielding a 14% increase in F-measure over the second-best method.


Title: Any Motion Detector: Learning Class-agnostic Scene Dynamics from a Sequence of LiDAR Point Clouds
Key Words: computational geometry  computer vision  feature extraction  image sequences  intelligent transportation systems  learning (artificial intelligence)  motion compensation  motion estimation  object detection  optical radar  parameter estimation  real-time systems  road safety  traffic engineering computing  LiDAR point clouds  object detection  urban environment  motion detection  3D point cloud sequence  object categories  temporal context aggregation  class-agnostic scene dynamics  motion parameters estimation  self driving vehicle navigation safety  motion parameter estimation  real time inference  road participant motion  neural network  camera images  Three-dimensional displays  Tensile stress  Vehicle dynamics  Transforms  Laser radar  Estimation  Object detection 
Abstract: Object detection and motion parameters estimation are crucial tasks for self-driving vehicle safe navigation in a complex urban environment. In this work we propose a novel real-time approach of temporal context aggregation for motion detection and motion parameters estimation based on 3D point cloud sequence. We introduce an ego-motion compensation layer to achieve real-time inference with performance comparable to a naive odometric transform of the original point cloud sequence. Not only is the proposed architecture capable of estimating the motion of common road participants like vehicles or pedestrians but also generalizes to other object categories which are not present in training data. We also conduct an in-deep analysis of different temporal context aggregation strategies such as recurrent cells and 3D convolutions. Finally, we provide comparison results of our state-of-the-art model with existing solutions on KITTI Scene Flow dataset.


Title: Fast Frontier-based Information-driven Autonomous Exploration with an MAV
Key Words: autonomous aerial vehicles  collision avoidance  entropy  microrobots  mobile robots  navigation  octrees  probability  robot vision  MAV  collision-free navigation  autonomous robots  microaerial vehicles  map entropy  occupancy probabilities  utility function  frontier extraction  frontier-based exploration  frontier voxels  map frontiers  frontier-based information-driven autonomous exploration  exploration planner  octree map representation  visual-based navigation  motion planning  octree-based occupancy mapping  sampling-based exploration  Planning  Octrees  Entropy  Robot sensing systems  Measurement  Task analysis  Aerial Systems: Perception and Autonomy  Visual-Based Navigation 
Abstract: Exploration and collision-free navigation through an unknown environment is a fundamental task for autonomous robots. In this paper, a novel exploration strategy for Micro Aerial Vehicles (MAVs) is presented. The goal of the exploration strategy is the reduction of map entropy regarding occupancy probabilities, which is reflected in a utility function to be maximised. We achieve fast and efficient exploration performance with tight integration between our octree-based occupancy mapping approach, frontier extraction, and motion planning-as a hybrid between frontier-based and sampling-based exploration methods. The computationally expensive frontier clustering employed in classic frontier-based exploration is avoided by exploiting the implicit grouping of frontier voxels in the underlying octree map representation. Candidate next-views are sampled from the map frontiers and are evaluated using a utility function combining map entropy and travel time, where the former is computed efficiently using sparse raycasting. These optimisations along with the targeted exploration of frontier-based methods result in a fast and computationally efficient exploration planner. The proposed method is evaluated using both simulated and real-world experiments, demonstrating clear advantages over state-of-the-art approaches.


Title: Dynamic Landing of an Autonomous Quadrotor on a Moving Platform in Turbulent Wind Conditions
Key Words: aircraft landing guidance  autonomous aerial vehicles  Global Positioning System  helicopters  Kalman filters  mobile robots  nonlinear filters  robot dynamics  robot vision  robust control  turbulence  variable structure systems  vehicle dynamics  dynamic landing  autonomous quadrotor  moving platform  turbulent wind conditions  autonomous landing  fast trajectory planning  wind disturbance  fully autonomous vision-based system  quadrotor-platform distance  landing trajectory  receding horizon control  boundary layer sliding controller  extended Kalman filter  GPS measurements  robust control  precise control  Trajectory  Cameras  Vehicle dynamics  Planning  Global Positioning System  Visualization  Acceleration  Unmanned aerial vehicles  autonomous vehicles  landing on a moving platform  disturbance compensation 
Abstract: Autonomous landing on a moving platform presents unique challenges for multirotor vehicles, including the need to accurately localize the platform, fast trajectory planning, and precise/robust control. Previous works studied this problem but most lack explicit consideration of the wind disturbance, which typically leads to slow descents onto the platform. This work presents a fully autonomous vision-based system that addresses these limitations by tightly coupling the localization, planning, and control, thereby enabling fast and accurate landing on a moving platform. The platform's position, orientation, and velocity are estimated by an extended Kalman filter using simulated GPS measurements when the quadrotor-platform distance is large, and by a visual fiducial system when the platform is nearby. The landing trajectory is computed online using receding horizon control and is followed by a boundary layer sliding controller that provides tracking performance guarantees in the presence of unknown, but bounded, disturbances. To improve the performance, the characteristics of the turbulent conditions are accounted for in the controller. The landing trajectory is fast, direct, and does not require hovering over the platform, as is typical of most stateof-the-art approaches. Simulations and hardware experiments are presented to validate the robustness of the approach.


Title: Direct NMPC for Post-Stall Motion Planning with Fixed-Wing UAVs
Key Words: aerodynamics  aerospace components  autonomous aerial vehicles  feedback  mobile robots  motion control  nonlinear control systems  path planning  position control  predictive control  vehicle dynamics  rotary-wing UAVs  nonlinear control approach  fixed-wing UAVs  full-state direct trajectory optimization  representative aircraft model  nonlinear aircraft model  fixed-wing trajectories  randomized motion planning  local-linear feedback  direct NMPC  post-stall motion planning  fixed-wing unmanned aerial vehicles  frequency 5.0 Hz  Aerodynamics  Trajectory  Atmospheric modeling  Real-time systems  Computational modeling  Planning  Splines (mathematics) 
Abstract: Fixed-wing unmanned aerial vehicles (UAVs) offer significant performance advantages over rotary-wing UAVs in terms of speed, endurance, and efficiency. However, these vehicles have traditionally been severely limited with regards to maneuverability. In this paper, we present a nonlinear control approach for enabling aerobatic fixed-wing UAVs to maneuver in constrained spaces. Our approach utilizes full-state direct trajectory optimization and a minimalistic, but representative, nonlinear aircraft model to plan aggressive fixed-wing trajectories in real-time at 5 Hz across high angles-of-attack. Randomized motion planning is used to avoid local minima and local-linear feedback is used to compensate for model inaccuracies between updates. We demonstrate our method in hardware and show that both local-linear feedback and re-planning are necessary for successful navigation of a complex environment in the presence of model uncertainty.


Title: A Flight Envelope Determination and Protection System for Fixed-Wing UAVs
Key Words: aerospace components  aircraft control  autonomous aerial vehicles  convex programming  nonlinear control systems  predictive control  remotely operated vehicles  flight envelope determination  protection system  fixed-wing UAV  generic model  nonlinear numerical model  model predictive controller  flight envelope constraints  trim flight envelope  MPC  Iron  Aircraft  Numerical models  Atmospheric modeling  Approximation algorithms  Mathematical model  Aerospace control 
Abstract: In this work we present a novel, approximate, efficient algorithm for determining the Trim Flight Envelope of a fixed-wing UAV, based on a generic, nonlinear numerical model. The resulting Flight Envelope is expressed as a convex intersection of half-spaces. Subsequently, a Model Predictive Controller (MPC) is designed which takes into account the Flight Envelope constraints, to avoid Loss-of-Control. The overall system is shown to operate in real-time in a simulation environment.


Title: Multi-Head Attention for Multi-Modal Joint Vehicle Motion Forecasting
Key Words: position control  probability  road vehicles  tracking  joint forecast  multimodal probability density functions  vehicle position tracks  multimodal joint vehicle motion forecasting  maneuver definitions  road scene  long short-term memory layers  spatial grid  Forecasting  Predictive models  Roads  Uncertainty  Computer architecture  Tensile stress  Probability density function 
Abstract: This paper presents a novel vehicle motion forecasting method based on multi-head attention. It produces joint forecasts for all vehicles on a road scene as sequences of multi-modal probability density functions of their positions. Its architecture uses multi-head attention to account for interactions between all vehicles, and long short-term memory layers for encoding and forecasting. It relies solely on vehicle position tracks, does not need maneuver definitions, and does not rasterize the scene as a spatial grid. This allows it to be more versatile than similar model while combining many forecasting capabilities, namely joint forecast with interactions, uncertainty estimation, and multi-modality. The resulting prediction likelihood outperforms state-of-the-art models on the same dataset.


Title: A Volumetric Albedo Framework for 3D Imaging Sonar Reconstruction
Key Words: autonomous underwater vehicles  image reconstruction  optimisation  robot vision  solid modelling  sonar  sonar imaging  stereo image processing  3D imaging sonar reconstruction  object-level 3D underwater reconstruction  imaging sonar sensors  nonline-of-sight reconstruction  convex linear optimization problem  alternating direction method of multipliers  ADMM  sonar elevation apertures  autonomous underwater vehicles  volumetric Albedo framework  Sonar  Image reconstruction  Imaging  Three-dimensional displays  Sensors  Nonlinear optics  Surface reconstruction 
Abstract: We present a novel framework for object-level 3D underwater reconstruction using imaging sonar sensors. We demonstrate that imaging sonar reconstruction is analogous to the problem of confocal non-line-of-sight (NLOS) reconstruction. Drawing upon this connection, we formulate the problem as one of solving for volumetric albedo, where the scene of interest is modeled as a directionless albedo field. After discretization, reconstruction reduces to a convex linear optimization problem, which we can augment with a variety of priors and regularization terms. We show how to solve the resulting regularized problems using the alternating direction method of multipliers (ADMM) algorithm. We demonstrate the effectiveness of the proposed approach in simulation and on real-world datasets collected in a controlled, test tank environment with several different sonar elevation apertures.


Title: Training-Set Distillation for Real-Time UAV Object Tracking
Key Words: autonomous aerial vehicles  correlation methods  image filtering  image motion analysis  minimisation  mobile robots  object detection  object tracking  robot vision  training-set distillation  UAV object tracking  correlation filter  visual object tracking  unmanned aerial vehicle  energy minimization function  scoring process  time slot-based distillation approach  Training  Unmanned aerial vehicles  Correlation  Reliability  Real-time systems  Optimization  Visualization 
Abstract: Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set's quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accelerate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on single CPU.


Title: Hand-worn Haptic Interface for Drone Teleoperation
Key Words: autonomous aerial vehicles  data gloves  human-robot interaction  mobile robots  motion control  robot vision  telerobotics  trajectory control  drone teleoperation  remote radio controllers  wearable interface  drone trajectory  hand motion  haptic system  robotic systems  teleoperation performance  remote controllers  human-robot interfaces  hand-worn haptic interface  data glove  line of sight  search-and-rescue missions  Haptic interfaces  Drones  Task analysis  Robot sensing systems  Hardware 
Abstract: Drone teleoperation is usually accomplished using remote radio controllers, devices that can be hard to master for inexperienced users. Moreover, the limited amount of information fed back to the user about the robot's state, often limited to vision, can represent a bottleneck for operation in several conditions. In this work, we present a wearable interface for drone teleoperation and its evaluation through a user study. The two main features of the proposed system are a data glove to allow the user to control the drone trajectory by hand motion and a haptic system used to augment their awareness of the environment surrounding the robot. This interface can be employed for the operation of robotic systems in line of sight (LoS) by inexperienced operators and allows them to safely perform tasks common in inspection and search-and-rescue missions such as approaching walls and crossing narrow passages with limited visibility conditions. In addition to the design and implementation of the wearable interface, we performed a systematic study to assess the effectiveness of the system through three user studies (n = 36) to evaluate the users' learning path and their ability to perform tasks with limited visibility. We validated our ideas in both a simulated and a real-world environment. Our results demonstrate that the proposed system can improve teleoperation performance in different cases compared to standard remote controllers, making it a viable alternative to standard Human-Robot Interfaces.


Title: Local Obstacle-Skirting Path Planning for a Fast Bi-steerable Rover using Bézier Curves
Key Words: automatic guided vehicles  collision avoidance  curve fitting  mobile robots  navigation  off-road vehicles  predictive control  stability  steering systems  vehicle dynamics  local obstacle-skirting path planning  obstacle avoidance  off-road mobile robots  global reference path  smooth path  lateral stability  double-steering rover  online cubic Bezier curves  bi-steerable rover  constrained model predictive control  navigation  autonomous guided vehicles  Safety  Collision avoidance  Mobile robots  Lead  Path planning  Robot kinematics 
Abstract: This paper focuses on local path planning for obstacle avoidance tasks dedicated to off-road mobile robots. This approach calculates a new local path for the vehicle using a set of cubic Bezier curves once the safety distance is not respected; otherwise, the vehicle follows the global reference path which is defined off-line. Two basic steps are used to determine this new path. Firstly, some significant points that should belong to the planned path are extracted on-line according to the obstacle's sizes and the current state of the vehicle, these points are approved as waypoints. Secondly, on-line cubic Bezier curves are computed to create a smooth path for these points such that the safety and lateral stability of the vehicle are ensured (i.e., preventing huge curvatures and wide-variation in steering angles). This path will be used as a reference to be performed by the vehicle using a constrained model predictive control. The validation of our navigation strategy is performed via numerical simulations and experiments using a fast double-steering rover.


Title: Safety-Critical Rapid Aerial Exploration of Unknown Environments
Key Words: autonomous aerial vehicles  collision avoidance  helicopters  mobile robots  sensors  uncertain systems  high-speed flight  uncertain environments  controller level  state uncertainty  nonlinear system dynamics  high-fidelity simulation  cave environment  safety-critical rapid aerial exploration  collision avoidance  aerial vehicles  unknown environments  quadrotor  onboard sensors  Safety  Collision avoidance  Three-dimensional displays  Drones  Trajectory  Vehicle dynamics  Planning 
Abstract: This paper details a novel approach to collision avoidance for aerial vehicles that enables high-speed flight in uncertain environments. This framework is applied at the controller level and provides safety regardless of the planner that is used. The method is shown to be robust to state uncertainty and disturbances, and is computed entirely online utilizing the full nonlinear system dynamics. The effectiveness of this method is shown in a high-fidelity simulation of a quadrotor with onboard sensors rapidly and safely exploring a cave environment utilizing a simple planner.


Title: Radar Sensors in Collaborative Robotics: Fast Simulation and Experimental Validation
Key Words: collision avoidance  CW radar  FM radar  frequency modulation  image sensors  learning (artificial intelligence)  mobile robots  object detection  radar computing  road vehicle radar  sensors  collaborative robotics  radar systems  robot systems  optimization  machine learning approaches  realistic simulation models  radar sensor simulations  relative velocities  Lambertian reflectance model  reflection estimates  frequency modulated continuous wave radar  simulation environments  Radar  Robot sensing systems  Radar antennas  Chirp  Computational modeling 
Abstract: With the availability of small system in package realizations, radar systems become more and more attractive for a variety of applications in robotics, in particular also for collaborative robotics. As the simulation of robot systems in realistic scenarios has become an important tool, not only for design and optimization, but also e.g. for machine learning approaches, realistic simulation models are needed. In the case of radar sensor simulations, this means providing more realistic results than simple proximity sensors, e.g. in the presence of multiple objects and/or humans, objects with different relative velocities and differentiation between background and foreground movement. Due to the short wavelength in the millimeter range, we propose to utilize methods known from computer graphics (e.g. z-buffer, Lambertian reflectance model) to quickly acquire depth images and reflection estimates. This information is used to calculate an estimate of the received signal for a Frequency Modulated Continuous Wave (FMCW) radar by superposition of the corresponding signal contributions. Due to the moderate computational complexity, the approach can be used with various simulation environments such as V-Rep or Gazebo. Validity and benefits of the approach are demonstrated by means of a comparison with experimental data obtained with a radar sensor on a UR10 arm in different scenarios.


Title: Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory Predictions
Key Words: intelligent transportation systems  learning (artificial intelligence)  mobile robots  neural nets  road safety  road traffic  road vehicles  traffic engineering computing  trajectory control  vehicle dynamics  deep learning  deep convnets  deep kinematic models  kinematically feasible vehicle trajectory predictions  self driving vehicles  traffic safety  autonomous technology  kinematically feasible motion prediction  vehicle kinematics  physically grounded vehicle motion models  Predictive models  Kinematics  Trajectory  Hidden Markov models  Radar tracking  Data models  Interpolation 
Abstract: Self-driving vehicles (SDVs) hold great potential for improving traffic safety and are poised to positively affect the quality of life of millions of people. To unlock this potential one of the critical aspects of the autonomous technology is understanding and predicting future movement of vehicles surrounding the SDV. This work presents a deep-learning- based method for kinematically feasible motion prediction of such traffic actors. Previous work did not explicitly encode vehicle kinematics and instead relied on the models to learn the constraints directly from the data, potentially resulting in kinematically infeasible, suboptimal trajectory predictions. To address this issue we propose a method that seamlessly combines ideas from the AI with physically grounded vehicle motion models. In this way we employ best of the both worlds, coupling powerful learning models with strong feasibility guarantees for their outputs. The proposed approach is general, being applicable to any type of learning method. Extensive experiments using deep convnets on real-world data strongly indicate its benefits, outperforming the existing state-of-the-art.


Title: Human Driver Behavior Prediction based on UrbanFlow*
Key Words: decision making  driver information systems  mobile robots  road safety  road traffic  road vehicles  traffic engineering computing  transportation  human driver behavior prediction  public transportation systems  fully automatic transportation environments  autonomous vehicle decision making  planning  LSTM-based trajectory prediction method  urban scenario  Trajectory  Autonomous vehicles  Data collection  Automobiles  Roads  Drones 
Abstract: How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Understanding human drivers' behavior can be beneficial for autonomous vehicle decision making and planning, especially when the autonomous vehicle is surrounded by human drivers who have various driving behaviors and patterns of interaction with other vehicles. In this paper, we propose an LSTM-based trajectory prediction method for human drivers which can help the autonomous vehicle make better decisions, especially in urban intersection scenarios. Meanwhile, in order to collect human drivers' driving behavior data in the urban scenario, we describe a system called UrbanFlow which includes the whole procedure from raw bird's-eye view data collection via drone to the final processed trajectories. The system is mainly intended for urban scenarios but can be extended to be used for any traffic scenarios.


Title: Pose-Estimate-Based Target Tracking for Human-Guided Remote Sensor Mounting with a UAV
Key Words: autonomous aerial vehicles  image sequences  pose estimation  SLAM (robots)  target tracking  pose-estimate-based target tracking  human-guided remote sensor mounting  autonomous aerial manipulation  unstructured environments  UAV localization  PBTT method  target point  fully on-board computation  RGB-D camera  downward-facing optical flow camera  horizontal localization  autonomous flight tests  interacting-boomcopter UAV platform  UAV position estimator  Target tracking  Cameras  Unmanned aerial vehicles  Visualization  Task analysis  Surface cleaning  Three-dimensional displays 
Abstract: In this paper, we present a method for pose-estimate-based target tracking (PBTT) that enables the performance of autonomous aerial manipulation operations in unstructured environments using fully on-board computation for both UAV localization and target tracking. The PBTT method does not depend on extracting traditional visual features (e.g. using SIFT, SURF, ORB, etc.) on or near the target. Instead, the algorithm combines input from an RGB-D camera and the UAV's position estimator (which utilizes a downward-facing optical flow camera for horizontal localization) to track a target point selected by a human operator. The effectiveness of the PBTT method is evaluated through several autonomous flight tests performed with the Interacting-Boomcopter (I-BC) UAV platform in unstructured environments and in the presence of light wind disturbances.


Title: On training datasets for machine learning-based visual relative localization of micro-scale UAVs
Key Words: autonomous aerial vehicles  image classification  learning (artificial intelligence)  microrobots  mobile robots  object detection  training datasets  machine learning-based visual relative localization  microscale UAVs  relative Microscale Unmanned Aerial Vehicle localization sensor UVDAR  automatically annotated dataset MIDGARD  MAVs  visual object detection  carefully crafted training dataset  annotated camera footage  Cameras  Training  Observers  Visualization  Image color analysis  Global navigation satellite system  Position measurement 
Abstract: By leveraging our relative Micro-scale Unmanned Aerial Vehicle localization sensor UVDAR, we generated an automatically annotated dataset MIDGARD, which the community is invited to use for training and testing their machine learning systems for the detection and localization of Microscale Unmanned Aerial Vehicles (MAVs) by other MAVs. Furthermore, we provide our system as a mechanism for rapidly generating custom annotated datasets specifically tailored for the needs of a given application. The recent literature is rich in applications of machine learning methods in automation and robotics. One particular subset of these methods is visual object detection and localization, using means such as Convolutional Neural Networks, which nowadays enable objects to be detected and classified with previously inconceivable precision and reliability. Most of these applications, however, rely on a carefully crafted training dataset of annotated camera footage. These must contain the objects of interest in environments similar to those where the detector is expected to operate. Notably, the positions of the objects must be provided in annotations. For non-laboratory settings, the construction of such datasets requires many man-hours of manual annotation, which is especially the case for use onboard Micro-scale Unmanned Aerial Vehicles. In this paper, we are providing for the community a practical alternative to that kind of approach.


Title: Robust Method for Removing Dynamic Objects from Point Clouds
Key Words: image capture  image filtering  image registration  image representation  object detection  optical radar  robot vision  3D point cloud maps  dynamic object removal  laser scans  lidar scans  object detection  voxel traversal method  Three-dimensional displays  Vehicle dynamics  Octrees  Object detection  Laser modes  Feature extraction 
Abstract: 3D point cloud maps are an accumulation of laser scans obtained at different positions and times. Since laser scans represent a snapshot of the surrounding at the time of capture, they often contain moving objects which may not be observed at all times. Dynamic objects in point cloud maps decrease the quality of maps and affect localization accuracy, hence it is important to remove the dynamic objects from 3D point cloud maps. In this paper, we present a robust method to remove dynamic objects from 3D point cloud maps. Given a registered set of 3D point clouds, we build an occupancy map in which the voxels represent the occupancy state of the volume of space over an extended time period. After building the occupancy map, we use it as a filter to remove dynamic points in lidar scans before adding the points to the map. Furthermore, we accelerate the process of building occupancy maps using object detection and a novel voxel traversal method. Once the occupancy map is built, dynamic object removal can run in real-time. Our approach works well on wide urban roads with stopped or moving traffic and the occupancy maps get better with the inclusion of more lidar scans from the same scene.


Title: DirectShape: Direct Photometric Alignment of Shape Priors for Visual Vehicle Pose and Shape Estimation
Key Words: image reconstruction  image segmentation  neural nets  object detection  pose estimation  shape recognition  stereo image processing  state-of-the-art deep learning based 3D object detectors  previous geometric approach  adaptive sparse point selection scheme  silhouette alignment term  dense stereo reconstruction  stereo image pair  3D rigid-body poses  3D bounding boxes  instance segmentations  simple bounding boxes  object level  autonomous driving  scene understanding  shape estimation  visual vehicle pose  shape priors  direct photometric alignment  Shape  Three-dimensional displays  Two dimensional displays  Automobiles  Current measurement  Solid modeling  Image reconstruction 
Abstract: Scene understanding from images is a challenging problem encountered in autonomous driving. On the object level, while 2D methods have gradually evolved from computing simple bounding boxes to delivering finer grained results like instance segmentations, the 3D family is still dominated by estimating 3D bounding boxes. In this paper, we propose a novel approach to jointly infer the 3D rigid-body poses and shapes of vehicles from a stereo image pair using shape priors. Unlike previous works that geometrically align shapes to point clouds from dense stereo reconstruction, our approach works directly on images by combining a photometric and a silhouette alignment term in the energy function. An adaptive sparse point selection scheme is proposed to efficiently measure the consistency with both terms. In experiments, we show superior performance of our method on 3D pose and shape estimation over the previous geometric approach and demonstrate that our method can also be applied as a refinement step and significantly boost the performances of several state-of-the-art deep learning based 3D object detectors. All related materials and demonstration videos are available at the project page https://vision.in.tum.de/research/vslam/direct-shape.


Title: RoadText-1K: Text Detection & Recognition Dataset for Driving Videos
Key Words: image recognition  intelligent transportation systems  road traffic  text detection  traffic engineering computing  video signal processing  driver assistance  RoadText-1K dataset  text bounding boxes  driving videos  text detection  text recognition  intelligent systems  Videos  Text recognition  Vehicles  Image recognition  Task analysis  Roads  Semantics 
Abstract: Perceiving text is crucial to understand semantics of outdoor scenes and hence is a critical requirement to build intelligent systems for driver assistance and self-driving. Most of the existing datasets for text detection and recognition comprise still images and are mostly compiled keeping text in mind. This paper introduces a new "RoadText-1K" dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. Our dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. State of the art methods for text detection, recognition and tracking are evaluated on the new dataset and the results signify the challenges in unconstrained driving videos compared to existing datasets. This suggests that RoadText-1K is suited for research and development of reading systems, robust enough to be incorporated into more complex downstream tasks like driver assistance and self-driving. The dataset can be found at http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k.


Title: End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera
Key Words: cameras  driver information systems  feature extraction  image sequences  learning (artificial intelligence)  mobile robots  neural nets  object detection  road safety  robot vision  video signal processing  end-to-end learning  inter-vehicle distance  ADAS  monocular camera  advanced driver-assistance systems  relative velocity estimation method  multiple visual clues  time-consecutive monocular frames  deep feature clue  scene geometry clue  temporal optical flow clue  vehicle-centric sampling mechanism  light-weight deep neural network  Estimation  Three-dimensional displays  Cameras  Optical imaging  Two dimensional displays  Feature extraction  Neural networks 
Abstract: Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.


Title: Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration
Key Words: autonomous underwater vehicles  control engineering computing  convolutional neural nets  feature extraction  human-robot interaction  mobile robots  object detection  object tracking  custom CNN  deep SORT algorithm  realtime tracking-by-detection  realtime diver detection  initial diver detection  appearance metric  simple online realtime tracking  human divers  autonomous underwater robots  underwater human-robot collaboration  realtime multidiver tracking re-identification  on-board tracking  on-board autonomous robot operations  multiperson tracking  Robots  Tracking  Feature extraction  Collaboration  Unmanned underwater vehicles  Task analysis 
Abstract: Autonomous underwater robots working with teams of human divers may need to distinguish between different divers, e.g., to recognize a lead diver or to follow a specific team member. This paper describes a technique that enables autonomous underwater robots to track divers in real time as well as to reidentify them. The approach is an extension of Simple Online Realtime Tracking (SORT) with an appearance metric (deep SORT). Initial diver detection is performed with a custom CNN designed for realtime diver detection, and appearance features are subsequently extracted for each detected diver. Next, realtime tracking-by-detection is performed with an extension of the deep SORT algorithm. We evaluate this technique on a series of videos of divers performing human-robot collaborative tasks and show that our methods result in more divers being accurately identified during tracking. We also discuss the practical considerations of applying multi-person tracking to on-board autonomous robot operations, and we consider how failure cases can be addressed during on-board tracking.


Title: RAVEN-S: Design and Simulation of a Robot for Teleoperated Microgravity Rodent Dissection Under Time Delay
Key Words: aerospace instrumentation  biocontrol  manipulators  medical robotics  mobile robots  space research  space vehicles  surgery  telerobotics  zero gravity experiments  teleoperated Microgravity Rodent dissection  International Space Station  ISS  biological effects  spaceflight  Rodent Habitat  Microgravity Science Glovebox  teleoperation  RAVEN II  rudimentary interaction force estimation  onboard dissection robot  RAVEN-S prototype design  communications time delay  robot design  robot simulation  Tools  Task analysis  Rodents  Delay effects  Delays  Force  Robots 
Abstract: The International Space Station (ISS) serves as a research lab for a wide variety of experiments including some that study the biological effects of microgravity and spaceflight using the Rodent Habitat and Microgravity Science Glovebox (MSG). Astronauts train for onboard dissections of rodents following basic training. An alternative approach for conducting these experiments is teleoperation of a robot located on the ISS from earth by a scientist who is proficient in rodent dissection. This pilot study addresses (1) the effects of extreme time delay on skill degradation during Fundamentals of Laparoscopic Surgery (FLS) tasks and rodent dissections using RAVEN II; (2) derivation and testing of rudimentary interaction force estimation; (3) elicitation of design requirements for an onboard dissection robot, RAVEN-S; and (4) simulation of the RAVEN-S prototype design with dissection data. The results indicate that the tasks' completion times increased by a factor of up to 9 for a 3 s time delay while performing manipulation and cutting tasks (FLS model) and by a factor of up to 3 for a 0.75 s time delay during mouse dissection tasks (animal model). Average robot forces/torques of 14N/0.1Nm (peak 90N/0.75Nm) were measured along with average linear/angular velocities of 0.02m/s/4rad/s (peak 0.1m/s/40rad/s) during dissection. A triangular configuration of three arms with respect to the operation site showed the best configuration given the MSG geometry and the dissection tasks. In conclusion, the results confirm the feasibility of utilizing a surgically-inspired RAVEN-S robot for teleoperated rodent dissection for successful completion of the predefined tasks in the presence of communications time delay between the ISS and ground control.


Title: DeepCrashTest: Turning Dashcam Videos into Virtual Crash Tests for Automated Driving Systems
Key Words: cameras  Internet  public domain software  road safety  road vehicles  traffic engineering computing  video signal processing  real-world collision scenarios  autonomous vehicles  uncalibrated monocular camera source  DeepCrashTest  virtual crash tests  automated driving systems  dashcam crash videos  3D vehicle trajectories  open-source implementation  Internet  Three-dimensional displays  Trajectory  Cameras  Videos  Tracking  Vehicle crash testing  Data mining 
Abstract: The goal of this paper is to generate simulations with real-world collision scenarios for training and testing autonomous vehicles. We use numerous dashcam crash videos uploaded on the internet to extract valuable collision data and recreate the crash scenarios in a simulator. We tackle the problem of extracting 3D vehicle trajectories from videos recorded by an unknown and uncalibrated monocular camera source using a modular approach. A working architecture and demonstration videos along with the open-source implementation are provided with the paper.


Title: Efficient Planning for High-Speed MAV Flight in Unknown Environments Using Online Sparse Topological Graphs
Key Words: aerospace navigation  air safety  autonomous aerial vehicles  collision avoidance  graph theory  infinite horizon  microrobots  mobile robots  probability  robot vision  search problems  high-speed MAV flight  online sparse topological graphs  safe high-speed autonomous navigation  local planning grid  computationally-efficient planning architecture  safe high-speed operation  longer-term memory  motion primitive-based local receding horizon planner  memory-efficient sparse topological graph  planning system  complex simulation environments  robot decision making  probabilistic collision avoidance  safe rerouting  Planning  Collision avoidance  Robot sensing systems  Libraries  Safety  Trajectory 
Abstract: Safe high-speed autonomous navigation for MAVs in unknown environments requires fast planning to enable the robot to adapt and react quickly to incoming information about obstacles within the world. Furthermore, when operating in environments not known a priori, the robot may make decisions that lead to dead ends, necessitating global replanning through a map of the environment outside of a local planning grid. This work proposes a computationally-efficient planning architecture for safe high-speed operation in unknown environments that incorporates a notion of longer-term memory into the planner enabling the robot to accurately plan to locations no longer contained within a local map. A motion primitive-based local receding horizon planner that uses a probabilistic collision avoidance methodology enables the robot to generate safe plans at fast replan rates. To provide global guidance, a memory-efficient sparse topological graph is created online from a time history of the robot's path and a geometric notion of visibility within the environment to search for alternate pathways towards the desired goal if a dead end is encountered. The safety and performance of the proposed planning system is evaluated at speeds up to 10m/s, and the approach is tested in a set of large-scale, complex simulation environments containing dead ends. These scenarios lead to failure cases for competing methods; however, the proposed approach enables the robot to safely reroute and reach the desired goal.


Title: Iterator-Based Temporal Logic Task Planning
Key Words: autonomous aerial vehicles  control system synthesis  discrete event systems  mobile robots  path planning  temporal logic  task specifications  universally quantified locations  constant time  hybrid control  discrete event controller  synthesised plan  iterator-based temporal logic task planning  robotic systems  state explosion  discrete locations  fixed-wing unmanned aerial vehicle  Task analysis  Robot sensing systems  Planning  Fires  Unmanned aerial vehicles 
Abstract: Temporal logic task planning for robotic systems suffers from state explosion when specifications involve large numbers of discrete locations. We provide a novel approach, particularly suited for task specifications with universally quantified locations, that has constant time with respect to the number of locations, enabling synthesis of plans for an arbitrary number of them. We propose a hybrid control framework that uses an iterator to manage the discretised workspace hiding it from a plan enacted by a discrete event controller. A downside of our approach is that it incurs in increased overhead when executing a synthesised plan. We demonstrate that the overhead is reasonable for missions of a fixed-wing Unmanned Aerial Vehicle in simulated and real scenarios for up to 700000 locations.


